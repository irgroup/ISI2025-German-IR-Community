<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.79,115.96,273.77,12.62;1,142.66,133.89,330.03,12.62;1,201.60,151.82,212.16,12.62">Plant Identification System based on a Convolutional Neural Network for the LifeClef 2016 Plant Classification Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.15,189.69,55.21,8.74"><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName coords="1,202.02,189.69,83.99,8.74"><forename type="first">Yang</forename><forename type="middle">Loong</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName coords="1,296.68,189.69,45.39,8.74"><forename type="first">Seng</forename><surname>Chee</surname></persName>
						</author>
						<author>
							<persName coords="1,345.39,189.69,23.25,8.74;1,398.68,189.69,24.21,8.74"><forename type="first">Paolo</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName coords="1,426.21,189.69,48.84,8.74;1,475.06,188.11,1.83,6.12"><surname>Remagnino</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Dept</orgName>
								<orgName type="institution">Kingston University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre of Image &amp; Signal Processing</orgName>
								<orgName type="department" key="dep2">Fac. Comp. Sci. &amp; Info. Tech</orgName>
								<orgName type="institution">University of Malaya</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.79,115.96,273.77,12.62;1,142.66,133.89,330.03,12.62;1,201.60,151.82,212.16,12.62">Plant Identification System based on a Convolutional Neural Network for the LifeClef 2016 Plant Classification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72B3F6143A4E0AB0AABD3D270FE5EE98</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant classification</term>
					<term>deep learning</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the architecture of our plant classification system for the LifeClef 2016 challenge <ref type="bibr" coords="1,364.19,319.37,13.52,7.86" target="#b13">[14]</ref>. The objective of the task is to identify 1000 species of images of plants corresponding to 7 different plant organs, as well as automatically detecting invasive species from unknown classes. To address the challenge [10], we proposed a plant classification system that uses a convolutional neural network (CNN).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plant classification has received particular attention in the computer vision field due to its important implications in agriculture automation and also for environmental conservation. For instance, botanical knowledge of plants is essential to improve agricultural development. Researchers in computer vision <ref type="bibr" coords="1,436.31,488.75,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="1,453.47,488.75,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="1,467.86,488.75,12.73,8.74" target="#b15">16,</ref><ref type="bibr" coords="1,134.77,500.70,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="1,144.18,500.70,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="1,153.58,500.70,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="1,167.97,500.70,12.73,8.74" target="#b19">20]</ref> have used variations of leaf characteristics as a comparative tool to classify plant. The reason is because leaf characters have been used extensively in traditional text-based taxonomic; they have been keys for plant identification since the early days of botanical science <ref type="bibr" coords="1,312.72,536.57,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,324.90,536.57,7.01,8.74" target="#b3">4]</ref>. Although the structural features of a leaf play an important role in the plant identification task, for certain plants, such as deciduous plants or semi-evergreen plants, leaves are not available over different periods of the years. Moreover, some species are hard to be differentiated using only their leaf organ as leaves in nature might have very similar shape and colour <ref type="bibr" coords="1,214.28,596.34,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="1,231.45,596.34,7.01,8.74" target="#b8">9]</ref>. Therefore, botanists usually extend their observation to more than one organ such as stems, flowers, branches or fruits. Since 2013, the LifeClef challenge <ref type="bibr" coords="1,232.14,620.25,15.50,8.74" target="#b10">[11]</ref> has provided the first multi-organ plant dataset that not only covers leaf-based images but different organs of given individual plants. Such images of plants were collected in an unconstrained environment, at different periods of time during the year, by different users. The objective of the plant identification task <ref type="bibr" coords="2,245.36,118.99,15.50,8.74" target="#b9">[10]</ref> in the LifeClef 2016 <ref type="bibr" coords="2,359.16,118.99,15.50,8.74" target="#b13">[14]</ref> challenge is to identify 1000 species of images of plants corresponding to 7 different organs, as well as automatically detecting invasive species from unknown classes.</p><p>Inspired by the deep learning breakthrough in image classification, more researchers have started to use deep learning models such as the CNN, to learn a robust plant image representation <ref type="bibr" coords="2,282.75,178.98,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,294.93,178.98,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,309.32,178.98,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,318.73,178.98,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,328.14,178.98,11.62,8.74" target="#b23">24]</ref>. In this work, we employ a CNN model to build a plant classification system. We re-purpose the current stateof-the-art VGG net <ref type="bibr" coords="2,222.37,202.89,15.50,8.74" target="#b21">[22]</ref> to incorporate species and organ features and solve the multi-organ plant classification problem.</p><p>The rest of the paper is organized as follows. In Section 2, we present the methodology of our proposed architecture. Section 3 illustrates its training scheme. Section 4 shows the experiments and results for both the validation and testing set. Lastly, Section 5 presents conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Description</head><p>The CNN model was initially designed to process multiple arrays of data such as colour (RGB) images, signal or sequences as well as video. Due to the availability of large scale image datasets, such as ImageNet <ref type="bibr" coords="2,375.75,344.65,9.96,8.74" target="#b6">[7]</ref>, and, followed by the advancement of technology, such as Graphic Processing Units (GPUs), CNN is currently a commodity in the computer vision field. The VGG net <ref type="bibr" coords="2,437.77,368.56,15.50,8.74" target="#b21">[22]</ref> offers currently the best state-of-the-art result for image classification. In this work, we initialize our model architecture based on the VGG net and modify its higher level convolutional layer to learn the combined species and organ features. Fig. <ref type="figure" coords="2,193.23,548.31,4.98,8.74">2</ref> shows our proposed architecture. We do not handcraft any feature descriptor for the fusion features, but introduce convolution layers to learn the filters themselves. We could view these filters as the learned feature descriptors encoding the distinctive fusion structures.</p><p>Our architecture mainly comprises four components: (i) shared layers, (ii) organ layers, (iii) species layers, and (iv) fusion layers that handle those combinations of both species and organ features. We introduce shared layers for both species and organ components. The reasons are threefold. First, <ref type="bibr" coords="2,415.38,632.21,15.50,8.74" target="#b24">[25,</ref><ref type="bibr" coords="2,432.53,632.21,12.73,8.74" target="#b22">23]</ref> demonstrated that preceding layers in deep networks response to low-level features such as corners and edges. Since both higher level species or organ components Fig. <ref type="figure" coords="3,216.86,317.57,3.87,8.74">2</ref>: Organ-species high level fusion architecture require low-level features to build higher level features, we introduce shared preceding layer for both components. Second, according to <ref type="bibr" coords="3,383.96,365.75,14.61,8.74" target="#b22">[23]</ref>, the shared layers may reduce both floating point operations and the memory footprint of the network execution, which are of importance for real world application. Lastly, using shared layers helps to reduce the number of training parameters, which is beneficial to the architecture performance.</p><p>To incorporate both organ and species features, we firstly train the organ layers CNN based on organ classes. Then, we keep the shared and the organ layers unaltered, in order to reuse it to train the species layers. After we trained the species layers, we cascade both of them to learn the fusion features. Before cascading both features, a feature downsizing convolution layer is added in each layer to reduce the feature maps dimension and produce compact based features. This step is essential to reduce the number of training parameters, to compensate the overfitting issue. Last but not least, we train three fully connected layers as the classifier to classify input images to its corresponding species classes. To embed four components into one pipeline and jointly trained end-to-end, we employ the multiple steps training as outlined in Sec. 3.  Step 3: Initializing Species Layers</p><p>Step 4: Initializing Fusion Layers</p><p>Step 5: Finetuning Organ-species high level fusion</p><p>Step 1: Pre-Training Two-Path CNN We initially design a two-path CNN for the purpose of training two different components (species and organ) as shown in Fig. <ref type="figure" coords="5,195.48,307.80,8.49,8.74" target="#fig_2">3a</ref>. Each path of the CNN configuration is similar to the VGG-net 16 layer architecture <ref type="bibr" coords="5,227.18,319.75,14.61,8.74" target="#b21">[22]</ref>, except that each of them share the preceeding layers.</p><p>Step 2: Initializing Organ Layers After we have the two-path CNN pretrained with Imagenet dataset, we re-purpose one of the path to train an organ layer as shown in Fig. <ref type="figure" coords="5,253.11,368.11,8.86,8.74" target="#fig_2">3b</ref>. We perform fine-tuning with seven organ labels: branch, entire, flower, fruit, leaf, leafscan and stem. These organ labels are annotated together with species in PlantClef Dataset. We finetune the VGG-16 network by replacing the final fully connected layer with a total of seven neurons corresponding to seven classes.</p><p>Step 3: Initializing Species Layers After we obtained the organ layers, we train the species layers based on the species labeled dataset as shown in Fig. <ref type="figure" coords="5,134.77,464.30,8.12,8.74" target="#fig_2">3c</ref>. As mentioned in Sec. 2, we allow both species and organ layers to share the common proceeding layers. Hence, to make the sharing of the filters possible, we keep the first two convolutional layers' weights to be consistent by setting the learning rate to zero during the species layers training. In addition to that, we set the organ layers learning rate to zero in order to avoid having their filters altered throughout the species layers training.</p><p>Step 4: Initializing Fusion Layers After having both organ and species components fine-tuned on the two-path CNN, we first migrate its convolutional layers to a new architecture as shown in Fig 3d . Then, we add a convolution layer to each component to reduce the feature map dimension and follow by another two stacks of convolutional layers to learn the fusion features. Lastly, we assign three fully connected layers for species classification. When training, we set the predefined layers' learning rate to zero and only train the newly assigned convolutional layers with species labeled dataset. Typically, we could see this third step as the fusion feature learning stage.</p><p>Step 5: Finetuning Organ-species high level fusion Finally, we finetune the whole architecture end-to-end using the same learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We evaluate the architectures on the PlantClef2015 dataset <ref type="bibr" coords="6,406.12,190.21,14.61,8.74">[15]</ref>. The models are trained using the Caffe <ref type="bibr" coords="6,255.87,202.16,15.50,8.74" target="#b12">[13]</ref> software. For the parameter setting in training, we employ fixed learning policy. We set the learning rate to 0.01, and then decrease it by a factor of 10, when the validation set accuracy stops improving.</p><p>The momentum is set to 0.9 and the weight decay to 0.0001. The networks are trained with back-propagation, using stochastic gradient descent <ref type="bibr" coords="6,425.25,249.98,14.61,8.74" target="#b17">[18]</ref>. We run the experiments using multiple GPUs on two NVIDIA TitanX graphics cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data augmentation</head><p>The images used for training capture living plants, where every object in the image can be captured at different size. Multi-scale training is therefore proposed. We isotropically rescale the training images into three different sizes: 256, 385 and 512, then randomly crop 224 *224 pixels from the rescaled images to feed into the network for training. Thus, the crop from the larger scaled images will correspond to the small part of the image or particularly subpart of the organ that may be an important feature for recognition. Besides that, we also increase the data size by mirroring the input image during training. Finally, we have a new set of training images that contains 272892 images and a validation set of 66711 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results on the validation set</head><p>For the evaluation of our validation set, we directly employ the softmax output from the model, i.e. we assign each test image the label with maximum softmax output from the classifiers and measure the numbers of correctly assigned labels over all the testing images. Contribution of Imagenet pretraining. In this section, we evaluate the contribution of the Imagenet dataset <ref type="bibr" coords="6,301.82,536.22,10.52,8.74" target="#b6">[7]</ref> pretraining to the plant classification task. We perform transfer learning experiments on the PlantClef2015 dataset. We re-purpose a VGG-16 net by performing fine-tuning on the top fully connected layer. We compare it to the model trained directly from scratch using the PlantClef2015 dataset without any pretraining. The results of our analysis show that the VGG-16 net pretrained using Imagenet data improves by 9.5%, the original 61.7 % score. Hence, pretraining from a larger diversity dataset like Imagenet <ref type="bibr" coords="6,165.91,619.91,10.52,8.74" target="#b6">[7]</ref> is clearly beneficial, as it helps to improve the generalization accuracy of the model.</p><p>Contribution of data augmentation Table <ref type="table" coords="6,356.52,644.16,4.98,8.74" target="#tab_0">1</ref> demonstrates the results of data augmentation. We can observe that the VGG-16 net gains 14.8% while the proposed high level fusion gains 14.5%. Hence, it can be deduced that data augmentation is important for the plant classification task, especially when training a large CNN with limited amount of data. Indeed, this enables models to expose to larger amount of data with higher diversity. However, the proposed high level fusion method has lower classification accuracy compared to the finetuned VGG-16 net. This might be because the VGG net uses species features only, performing better than the fusion of features, robust enough to represent plant images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results on Test set</head><p>We have submitted four runs using the LifeClef 2016 -multi-organ plant dataset. The characteristics of each runs are stated as below: -UM Run 3: train the proposed high level fusion architecture with augmented PlantClef2015 training dataset -UM Run 4: finetune UM Run 1 architecture with validation set. Fig. <ref type="figure" coords="8,170.32,163.61,4.98,8.74" target="#fig_3">4</ref> shows the results of the mean average precision scores with the evaluation of the robustness of the system handling unseen categories. We observe that Run 4 is the best among the submitted Runs. The reason is because it was trained with a larger training dataset compared to other runs. In addition, Run 1 is better than Run 3 which is consistent with the results obtained in the validation set. Last but not least, Run 2 shows the lowest result among the submitted runs. This again shows the importance of the pre-training model with a large-scale dataset. Overall, all submitted runs obtain average results in the LifeClef2016 multi-organ plant classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future work</head><p>This paper proposed using the CNN model to incorporate species and organ features for the plant classification task. We described the methodology of our architecture and analyzed the results based on both validation and testing set. The results of the proposed high level fusion architecture is promising but still limited compared to the VGG net. In future, we will focus on exploring the CNN model for the plant classification task including its implication in the open-set recognition task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,224.59,475.75,166.18,8.74;2,134.77,412.35,345.83,51.87"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: VGG-net 16 layers architecture</figDesc><graphic coords="2,134.77,412.35,345.83,51.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,241.36,242.54,132.64,7.86;4,249.72,366.20,115.92,7.86;4,248.29,488.46,118.78,7.86;4,249.12,616.28,117.11,7.86"><head></head><label></label><figDesc>(a) Pre-Training Two-Path CNN (b) Initializing Organ Layers (c) Initializing Species Layers (d) Initializing Fusion Layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,149.66,637.22,316.04,8.74;4,209.12,498.86,197.12,109.89"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Multiple steps training scheme for Organ-species high level fusion</figDesc><graphic coords="4,209.12,498.86,197.12,109.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,154.42,573.89,306.52,8.74;7,134.77,372.16,345.82,190.20"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Results of the LifeClef2016 multi-organ plant classification task</figDesc><graphic coords="7,134.77,372.16,345.82,190.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.77,115.84,345.83,190.21"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.77,115.84,345.83,190.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,222.54,345.82,44.91"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison using augmented and non-augmented dataset</figDesc><table coords="7,163.64,234.88,285.01,32.57"><row><cell>Method</cell><cell cols="2">Non-Augmented (%) Augmented (%)</cell></row><row><cell>Finetuned VGG-16 top layer</cell><cell>56.4</cell><cell>71.2</cell></row><row><cell>High level fusion (proposed)</cell><cell>54.4</cell><cell>68.9</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,438.83,337.64,7.86;8,151.52,449.79,329.07,7.86;8,151.52,460.75,154.49,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,338.89,438.83,141.70,7.86;8,151.52,449.79,329.07,7.86;8,151.52,460.75,16.79,7.86">A comparative study of fine-grained classification methods in the context of the lifeclef plant identification challenge 2015</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,189.66,460.75,44.57,7.86">CLEF 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,471.50,337.63,7.86;8,151.52,482.46,329.07,7.86;8,151.52,493.42,324.62,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,378.82,471.50,101.77,7.86;8,151.52,482.46,246.03,7.86">Eagle: A novel descriptor for identifying plant species using leaf lamina vascular features</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Charters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,417.05,482.46,63.54,7.86;8,151.52,493.42,65.99,7.86;8,246.74,493.42,124.84,7.86">Multimedia and Expo Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>IEEE International Conference</note>
</biblStruct>

<biblStruct coords="8,142.96,504.18,337.64,7.86;8,151.52,515.14,329.07,7.86;8,151.52,526.10,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,189.11,504.18,291.49,7.86;8,151.52,515.14,151.31,7.86">Plant identification with deep convolutional neural network: Snumedinfo at lifeclef plant identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,322.36,515.14,158.23,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,536.85,337.63,7.86;8,151.52,547.81,329.07,7.86;8,151.52,558.77,116.26,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,165.82,547.81,162.52,7.86">Venation pattern analysis of leaf images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kirkup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,350.95,547.81,124.88,7.86">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,569.53,337.63,7.86;8,151.52,580.49,329.07,7.86;8,151.52,591.45,96.76,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,400.81,569.53,79.78,7.86;8,151.52,580.49,184.64,7.86">Plant species identification using digital morphometrics: A review</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Cope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,343.11,580.49,137.48,7.86">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7562" to="7573" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,602.20,337.64,7.86;8,151.52,613.16,329.07,7.86;8,151.52,624.12,280.84,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,354.37,602.20,126.22,7.86;8,151.52,613.16,266.29,7.86">The extraction of venation from leaf images by evolved vein classifiers and ant colony algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Cope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.17,613.16,39.43,7.86;8,151.52,624.12,157.11,7.86">Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,634.88,337.63,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,187.92,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,403.66,634.88,76.93,7.86;8,151.52,645.84,132.95,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,306.55,645.84,174.04,7.86;8,151.52,656.80,79.80,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.64,7.86;9,151.52,130.63,285.75,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,280.50,119.67,200.09,7.86;9,151.52,130.63,73.60,7.86">Content specific feature learning for fine-grained plant classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,245.82,130.63,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,141.54,337.64,7.86;9,151.52,152.50,329.07,7.86;9,151.52,163.46,329.07,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,228.74,152.50,126.93,7.86">Multi-organ plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,377.29,152.50,103.31,7.86;9,151.52,163.46,255.12,7.86">Proc. of the 1st ACM international workshop on Multimedia analysis for ecological data</title>
		<meeting>of the 1st ACM international workshop on Multimedia analysis for ecological data</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,174.37,337.97,7.86;9,151.52,185.33,147.90,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,277.55,174.37,198.94,7.86">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,185.33,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,196.24,337.97,7.86;9,151.52,207.19,271.28,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,173.03,207.19,175.40,7.86">The imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,369.42,207.19,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,218.10,337.97,7.86;9,151.52,229.06,329.07,7.86;9,151.52,240.02,287.44,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,409.09,218.10,71.50,7.86;9,151.52,229.06,204.65,7.86">Evaluation of features for leaf classification in challenging conditions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,376.68,229.06,103.92,7.86;9,151.52,240.02,61.38,7.86;9,242.96,240.02,100.78,7.86">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="797" to="804" />
		</imprint>
	</monogr>
	<note>IEEE Winter Conference</note>
</biblStruct>

<biblStruct coords="9,142.62,250.93,337.97,7.86;9,151.52,261.89,329.07,7.86;9,151.52,272.85,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,237.79,261.89,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.15,272.85,232.47,7.86">Proc. of the ACM International Conference on Multimedia</title>
		<meeting>of the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,283.76,337.97,7.86;9,151.52,294.71,329.07,7.86;9,151.52,305.67,126.37,7.86;9,134.77,316.58,7.85,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,360.12,294.71,120.47,7.86;9,151.52,305.67,126.37,7.86;9,134.77,316.58,7.85,7.86">Lifeclef 2016: multimedia life species identification challenges 15</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,316.58,337.97,7.86;9,151.52,327.54,329.07,7.86;9,151.52,338.50,329.07,7.86;9,151.52,349.46,246.58,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,378.80,327.54,101.79,7.86;9,151.52,338.50,143.37,7.86">Lifeclef 2015: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,318.07,338.50,162.52,7.86;9,151.52,349.46,123.36,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="462" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,360.37,337.97,7.86;9,151.52,371.33,329.07,7.86;9,151.52,382.28,176.04,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,359.58,360.37,121.01,7.86;9,151.52,371.33,106.46,7.86">Leaf classification using shape, color, and texture features</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Nugroho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">I</forename><surname>Santosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,265.19,371.33,215.40,7.86;9,151.52,382.28,26.37,7.86">International Journal of Computer Trends and Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="306" to="311" />
			<date type="published" when="2011-08">July to August 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,390.93,337.97,10.13;9,151.52,404.15,159.04,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,258.63,393.19,111.50,7.86">Geometric leaf classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kalyoncu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ö</forename><surname>Toygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.72,393.19,103.87,7.86;9,151.52,404.15,75.59,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,415.06,337.98,7.86;9,151.52,426.02,329.07,7.86;9,151.52,436.98,86.01,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,328.09,415.06,152.50,7.86;9,151.52,426.02,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,275.64,426.02,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,447.89,337.98,7.86;9,151.52,458.85,329.07,7.86;9,151.52,469.80,297.44,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,204.15,458.85,276.44,7.86;9,151.52,469.80,34.91,7.86">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.07,469.80,118.36,7.86">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,480.71,337.98,7.86;9,151.52,491.67,329.07,7.86;9,151.52,502.63,146.42,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,353.59,480.71,127.00,7.86;9,151.52,491.67,140.69,7.86">Deep-plant: Plant identification with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,314.26,491.67,166.33,7.86;9,151.52,502.63,42.49,7.86">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,513.54,337.97,7.86;9,151.52,524.50,319.81,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,333.89,513.54,146.70,7.86;9,151.52,524.50,107.28,7.86">Fine-tuning deep convolutional networks for plant recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,279.89,524.50,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,535.41,337.97,7.86;9,151.52,546.37,193.35,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="9,278.92,535.41,201.67,7.86;9,151.52,546.37,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,557.27,337.97,7.86;9,151.52,568.23,329.07,7.86;9,151.52,579.19,329.07,7.86;9,151.52,590.15,70.14,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,465.49,557.27,15.10,7.86;9,151.52,568.23,329.07,7.86;9,151.52,579.19,22.38,7.86">Hdcnn: Hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.04,579.19,264.06,7.86">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,601.06,337.97,7.86;9,151.52,612.02,329.07,7.86;9,151.52,622.98,70.42,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="9,384.38,601.06,96.21,7.86;9,151.52,612.02,181.37,7.86">Sabanci-okan system at lifeclef 2014 plant identification competition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tolga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fuencaglartes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,357.04,612.02,123.55,7.86;9,151.52,622.98,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,633.89,337.97,7.86;9,151.52,644.85,255.46,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,256.42,633.89,219.92,7.86">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,644.85,117.84,7.86">Computer vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
