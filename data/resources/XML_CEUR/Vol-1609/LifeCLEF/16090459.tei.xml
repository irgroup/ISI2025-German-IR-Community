<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,225.53,152.75,144.38,12.54;1,174.62,170.75,245.92,12.54">Bluefield (KDE TUT) at LifeCLEF 2016 Plant Identification Task</title>
				<funder ref="#_4e2ExyA #_QgmjtxC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.70,210.08,67.46,9.05"><forename type="first">Siang</forename><forename type="middle">Thye</forename><surname>Hang</surname></persName>
							<email>hang@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Knowledge Data Engineering&amp; Information Retrieval Laboratory (KDE Lab)</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.63,210.08,65.25,9.05"><forename type="first">Atsushi</forename><surname>Tatsuma</surname></persName>
							<email>tatsuma@cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Knowledge Data Engineering&amp; Information Retrieval Laboratory (KDE Lab)</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.05,210.08,54.02,9.05"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Knowledge Data Engineering&amp; Information Retrieval Laboratory (KDE Lab)</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,225.53,152.75,144.38,12.54;1,174.62,170.75,245.92,12.54">Bluefield (KDE TUT) at LifeCLEF 2016 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C7D257F885C9B1CFEAA3679E082089D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant identification</term>
					<term>deep learning</term>
					<term>sample rejection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an automatic approach for plant image identification. We enhanced the well-known VGG 16-layers Convolutional Neural Network model [1] by replacing the last pooling layer with a Spatial Pyramid Pooling layer <ref type="bibr" coords="1,241.49,337.97,9.70,8.18" target="#b1">[2]</ref>. Rectified Linear Units (ReLU) are also replaced with Parametric ReLUs <ref type="bibr" coords="1,240.65,349.01,9.70,8.18" target="#b2">[3]</ref>. The enhanced model is trained without external dataset. A post processing method is also proposed to reject irrelevant samples. We further improved identification performance using observation identity (ObservationId) provided in the dataset. Our methods showed outstanding performance in official evaluation results of the LifeCLEF 2016 Plant Identification Task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, conservation of biodiversity is becoming an important duty for us. To achieve this, accurate knowledge is essential. However, even for professionals, identifying a species can be a very difficult task. Convolutional Neural Networks (CNNs) are leading the best performance in various image retrieval tasks such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" coords="1,385.63,530.17,11.69,9.05" target="#b3">[4]</ref> [5] [1] <ref type="bibr" coords="1,428.83,530.17,10.69,9.05" target="#b5">[6]</ref>. CNNs learn filters of filters through multiple convolution layers, enabling higher level of abstraction than hand crafted features such as Scale Invariant Feature Transform (SIFT).</p><p>In recent years, CNN has gained popularity in various image retrieval tasks including the LifeCLEF Plant Identification Task (PlantCLEF). Over the past years, participants of PlantCLEF have adopted to use CNN in their works <ref type="bibr" coords="1,374.35,602.17,10.69,9.05" target="#b6">[7]</ref>. In PlantCLEF 2016 <ref type="bibr" coords="1,124.70,614.17,11.69,9.05" target="#b7">[8]</ref>  <ref type="bibr" coords="1,139.46,614.17,10.60,9.05" target="#b8">[9]</ref>, in addition to the 1000 class identification task, a new problem is introduced. The evaluation set consists of not only native plant images but also images of potentially invasive plants or irrelevant objects such as a table or a computer keyboard.</p><p>Considering the new setting, we propose a post processing method to reject these samples. In the next section we propose to enhance the VGG 16-layers model with Spatial Pyramid Pooling and Parametric Rectified Linear Units. We detail our model training strategy in Section 3, followed by irrelevant sample rejection algorithm in Section 4. Section 5 shows evaluation results and Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Enhancement</head><p>In the following sections, a few enhancements to the VGG 16-layers model will be described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatial Pyramid Pooling</head><p>The original VGG 16-layers model requires input image of spatial size 224 Ã— 224.</p><p>On the other hand, the images in the PlantCLEF 2016 dataset are arbitrarily sized, as shown in Figure <ref type="figure" coords="2,196.75,305.15,3.76,9.05">1</ref>. Following the spatial size restriction in VGG 16-layers model, input images has to be either cropped or warped. Both of these methods may lead to information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Aspect ratio of images in the PlantCLEF 2016 dataset</head><p>To circumvent such restriction, we have replaced the last pooling layer (pool5) with a Spatial Pyramid Pooling (SPP) layer <ref type="bibr" coords="2,279.29,531.01,10.69,9.05" target="#b1">[2]</ref>. The last convolution layer (conv5_3) produces feature map of 512 channels. With SPP, the feature map is spatially divided into 1 Ã— 1, 2 Ã— 2, 4 Ã— 4, a total of 21 regions. Each region is then average pooled, producing a vector of fixed size 21 Ã— 512 = 10752. Such conversion of arbitrarily sized feature map into a fixed size vector allows the model to accept input image of any size. Meanwhile, with such layer replacement, the number of parameters in the model is reduced from 138 million to 80 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parametric Rectified Linear Unit</head><p>We have also replaced all of the Rectified Linear Unit (ReLU) activations with a learnable version known as Parametric ReLU <ref type="bibr" coords="2,321.53,664.96,10.69,9.05" target="#b2">[3]</ref>, which consistently outperforms ReLU in empirical experiments by Bing et al. <ref type="bibr" coords="2,313.85,676.96,15.43,9.05" target="#b9">[10]</ref>. Before training, we initialize the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Training</head><p>The following strategies are used to train the enhanced model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data preparation</head><p>The PlantCLEF 2016 dataset consists of 113204 images. 2048 images are used for validation purpose, while the remaining 111156 images are used to train the model. We have augmented the training images as follows: while preserving aspect ratio, images are resized such that the shorter side becomes 224 and 336. Random cropping to 224 Ã— 224 and random flipping by y-axis are also applied to the resized images. For evaluation process, images are resized such that the shorter side becomes 224, and no cropping or flipping is applied. Cropping is not required during evaluation as the enhanced model accepts image of any size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Mean</head><p>As the enhanced model is trained from scratch i.e. without external resources, image mean is computed from the training set. Excluding LeafScan images (with high RGB values due to bright background), the computed mean values of red, green and blue channels are 105, 111, and 79 respectively. Images augmented in Section 3.1 are subtracted with these mean values before being used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We use the Caffe framework to train the enhanced model. To train such a deep model, we use Xavier's method <ref type="bibr" coords="3,230.09,513.13,16.72,9.05" target="#b10">[11]</ref> to initialize the weights of the convolution and fully connected layers. We train the model using Stochastic Gradient Descent with momentum 0.9, learning rate 0.01 through 0.0001, and batch size 50. The learning rate is multiplied by 0.1 when the validation accuracy stops improving. As the result, we have trained with learning rate 0.01 for 30 epochs, 0.001 for 15 epochs and finally 0.0001 for 8 epochs. Figure <ref type="figure" coords="3,238.76,573.13,4.98,9.05">2</ref> shows the normalized softmax loss of both training and validation. Figure <ref type="figure" coords="3,198.17,585.13,4.98,9.05">3</ref> shows the validation accuracy of the first prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Irrelevant Sample Rejection</head><p>The next section discusses the One-Versus-All method's limitation to reject irrelevant samples in multiclass classification. To overcome such problem, an algorithm is proposed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multiclass Classification with One Versus All Method</head><p>Assume a binary classifier ğ¹ of class ğ¶ mapping an input ğ‘¥ into a score ğ¹(ğ‘¥). ğ¹ can be optimized such that ğ¹(ğ‘¥) &gt; 0 when ğ‘¥ âˆˆ ğ¶, and ğ¹(ğ‘¥) &lt; 0 when ğ‘¥ âˆ‰ ğ¶.</p><p>In the case of multiclass classification, the One-Versus-All method can be used to </p><p>With (1), it is guaranteed a class will be assigned to ğ‘¥, regardless how high or low the scores produced by each classifier. Based on the scenario above, we realized that there are cases that a sample should be rejected by all of the binary classifiers. To identify such irrelevant sample, an algorithm is elaborated in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Irrelevant Sample Rejection Algorithm</head><p>After training the enhanced model as mentioned in Section 3, a matrix of raw (i.e. before softmax normalization) scores ğ‘º ğ‘€Ã—ğ‘ of ğ‘ classes and ğ‘€ training samples are extracted from the classifier layer (fc8). Incorrect predictions are omitted. With only correctly predicted class scores ğ‘º ğ‘€ â€² Ã—ğ‘ (ğ‘€ â€² â‰¤ ğ‘€), rejection threshold ğ’• = [ğ‘¡ 1 , â€¦ , ğ‘¡ ğ‘ ] of each class is computed from the class wise minima, as shown in <ref type="bibr" coords="5,393.83,433.19,10.66,9.05" target="#b1">(2)</ref>.</p><formula xml:id="formula_1" coords="5,262.85,452.14,207.82,15.65">ğ‘¡ ğ‘– = min ğ‘˜ğœ–ğ‘€ â€² ğ‘† ğ‘˜,ğ‘–<label>(2)</label></formula><p>Figure <ref type="figure" coords="5,153.23,477.13,4.98,9.05" target="#fig_4">5</ref> shows the threshold ğ’• of each class based on the PlantCLEF 2016 training set (ğ‘ = 1000), sorted in ascending order. During evaluation, any sample with score lower than ğ’• for all of the classes will be rejected as irrelevant. With the evaluation set, 195 out of 8000 images are rejected. Some of the images are shown in Figure <ref type="figure" coords="6,288.39,174.08,3.76,9.05">6</ref>.</p><p>Fig. <ref type="figure" coords="6,223.97,308.12,3.41,8.04">6</ref>. Subset of rejected samples with threshold ğ’•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Taking Validation Set into Account</head><p>The threshold ğ’• obtained in Section 4.2 is solely based on the training set. Due to factors such as overfitting, there is a possibility that lower threshold can be acquired from the validation set. Figure <ref type="figure" coords="6,255.71,381.11,4.98,9.05">7</ref> shows threshold obtained from the validation set, corresponding to the sorted classes shown in Figure <ref type="figure" coords="6,334.08,393.11,3.76,9.05" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 7. Rejection threshold of training and validation set</head><p>Although we expect lower thresholds from the validation set, as shown in Figure <ref type="figure" coords="6,462.79,555.61,3.76,9.05">7</ref>, majority of the thresholds are higher than that of the training set. This is due to the fact that there is too little sample (2048 â‰ª ğ‘€) in the validation set. Thus, only lower thresholds are considered. Ratios of each of these (seven) thresholds to its corresponding training set thresholds are computed, and then averaged into ğ‘„. As detailed in (3), the denominators are the thresholds of the training set, while the numerators are of the validation set. The values in (3) are based on Figure <ref type="figure" coords="6,334.62,627.61,3.76,9.05">7</ref>. ğ‘„ is then multiplied to the threshold ğ’• of the training set, as shown in ( <ref type="formula" coords="7,407.51,150.08,3.55,9.05">4</ref>), before using it to reject samples during evaluation.</p><formula xml:id="formula_2" coords="7,273.65,179.77,197.02,11.35">ğ’• â€² = ğ‘„ğ’• (4)</formula><p>Applying ğ’• â€² to the evaluation set, only 69 samples are rejected as it is lower compared to the original ğ’•. Some of the images are shown in Figure <ref type="figure" coords="7,355.99,214.16,3.77,9.05" target="#fig_6">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Observation Based Identification</head><p>Identification based on a single image may be insufficient. In the PlantCLEF2016 dataset, images based on the same observation share a unique ObservationId. One ObservationId may be assigned with multiple images of different organs. Specifically images of flower or fruit which have much characteristic features, their existence in an observation often improves identification performance.</p><p>To further improve the identification performance, after rejecting samples as explained in Section 4.2 and 4.3, we sum the raw (i.e. before softmax normalization) class scores of images with the same ObservationId. The summed scores are then softmax normalized. As the result, images with the same ObservationId share the same normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we described our approach to PlantCLEF 2016, focusing on model enhancements, data augmentations, and an irrelevant sample rejection strategy. We still leave some rooms for improvements, which are itemized as follows:</p><p>ï‚· As mentioned in Section 3.1, images for training process are resized to two scales. We should consider applying random scaling instead of a constant number of (two) scales. Other than scaling, random rotation should be applied as well.</p><p>ï‚· In Section 4.2 and 4.3, instead of taking minima as rejection threshold, we should consider using mean and standard deviation to obtain a more stable threshold.</p><p>ï‚· The PlantCLEF 2016 dataset includes rich metadata information such as Genus, Family, Date, Longitude, Latitude, Location and Content (organ type). However, in our work, only ClassId (class label) and ObservationId are used. More metadata information should be utilized to obtain more accurate identification performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,161.09,468.11,4.26,7.67;2,161.09,453.25,4.26,7.67;2,156.84,438.42,8.46,7.67;2,156.84,423.59,8.46,7.67;2,156.84,408.76,8.46,7.67;2,156.84,393.92,8.46,7.67;2,156.84,379.09,8.46,7.67;2,177.22,478.09,256.63,7.67;2,177.28,422.36,7.67,7.10;2,177.28,413.19,7.67,7.05;2,177.28,387.41,7.67,23.70;2,418.15,448.64,20.10,7.24;2,423.55,457.88,14.80,7.24;2,214.68,354.85,48.26,7.67;2,345.14,354.85,57.25,7.67;2,283.01,354.85,45.02,7.67;3,124.70,150.08,345.80,9.05;3,124.70,162.08,127.03,9.05"><head></head><label></label><figDesc>0.05. Weight decay of the learnable parameters is disabled throughout the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,234.29,278.93,126.70,8.18"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Training and validation loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,124.70,602.40,108.34,10.44;4,232.25,607.26,4.03,6.96;4,236.69,602.88,22.63,9.96;4,258.77,607.26,5.26,6.96;4,264.65,602.40,152.51,10.44;4,415.87,607.26,4.03,6.96;4,420.31,602.88,22.77,9.96;4,442.03,607.26,5.26,6.96;4,447.79,602.40,22.84,10.38;4,124.70,614.88,78.67,9.96;4,202.85,614.88,35.73,11.33;4,237.53,619.26,2.44,6.96;4,240.65,614.52,209.77,10.26;4,214.85,634.08,23.35,9.96;4,237.65,638.46,2.44,6.96;4,250.73,634.08,87.36,9.96;4,321.89,642.66,14.14,6.96;4,339.79,634.08,6.09,9.96;4,344.83,638.46,4.21,6.96;4,349.63,633.72,13.85,9.96"><head></head><label></label><figDesc>classify ğ‘ classes ğ‘ª = [ğ¶ 1 , â€¦ , ğ¶ ğ‘ ] using ğ‘ binary classifiers ğ‘­ = [ğ¹ 1 , â€¦ , ğ¹ ğ‘ ]. As shown in (1), ğ‘¥ âˆˆ ğ¶ ğ‘– when ğ¹ ğ‘– (ğ‘¥) is the highest among all of the binary classifiers. ğ‘¥ âˆˆ ğ¶ ğ‘– where ğ‘– = arg max ğ‘˜âˆˆğ‘ ğ¹ ğ‘˜ (ğ‘¥)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,124.70,150.08,345.66,9.05;5,124.70,161.23,345.66,9.96;5,124.70,173.23,345.53,9.96;5,124.70,186.08,36.18,9.05"><head>Figure 4 'Figure 4 Fig. 4 .</head><label>444</label><figDesc>Figure 4 belongs to the first class. Meanwhile, second and third examples have either weak or no positive score. With (1), input ğ›½ is classified to the third class, while input ğ›¾ is classified to the first class, despite that all of the scores are rather low or even negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,166.22,655.02,262.79,9.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Rejection threshold ğ’• of the training set, sorted in ascending order</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,205.73,347.17,183.78,9.00;7,207.90,289.40,90.00,50.00"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Subset of rejected samples with threshold ğ’•â€²</figDesc><graphic coords="7,207.90,289.40,90.00,50.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,126.22,276.74,11.21,8.10;9,126.22,262.55,11.21,8.10;9,126.22,248.37,11.21,8.10;9,126.22,234.19,11.21,8.10;9,126.22,220.00,11.21,8.10;9,126.22,205.82,11.21,8.10;9,126.22,191.63,11.21,8.10;9,126.22,177.45,11.21,8.10;9,126.22,163.27,11.21,8.10;9,126.22,149.06,11.21,8.10;9,147.92,306.13,7.24,29.80;9,147.92,290.78,7.24,13.37;9,147.92,284.78,7.24,4.02;9,159.15,306.13,7.24,29.80;9,159.15,290.78,7.24,13.37;9,159.15,284.78,7.24,4.02;9,170.38,306.13,7.24,29.80;9,170.38,290.78,7.24,13.37;9,170.38,284.78,7.24,4.02;9,181.61,306.13,7.24,29.80;9,181.61,290.78,7.24,13.37;9,181.61,284.78,7.24,4.02;9,192.85,327.66,7.24,17.39;9,192.85,306.18,7.24,19.55;9,192.85,290.77,7.24,13.37;9,192.85,284.77,7.24,4.02;9,204.08,327.66,7.24,17.39;9,204.08,306.18,7.24,19.55;9,204.08,290.77,7.24,13.37;9,204.08,284.77,7.24,4.02;9,215.31,327.66,7.24,17.39;9,215.31,306.18,7.24,19.55;9,215.31,290.77,7.24,13.37;9,215.31,284.77,7.24,4.02;9,226.54,327.66,7.24,17.39;9,226.54,306.18,7.24,19.55;9,226.54,290.77,7.24,13.37;9,226.54,284.77,7.24,4.02;9,237.77,306.11,7.24,16.95;9,237.77,290.84,7.24,13.27;9,237.77,284.85,7.24,4.02;9,249.01,306.11,7.24,16.95;9,249.01,290.84,7.24,13.27;9,249.01,284.85,7.24,4.02;9,260.24,306.11,7.24,16.95;9,260.24,290.84,7.24,13.27;9,260.24,284.85,7.24,4.02;9,271.47,306.11,7.24,26.72;9,271.47,290.80,7.24,13.37;9,271.47,284.80,7.24,4.02;9,282.70,306.11,7.24,26.72;9,282.70,290.80,7.24,13.37;9,282.70,284.80,7.24,4.02;9,293.93,306.11,7.24,26.72;9,293.93,290.80,7.24,13.37;9,293.93,284.80,7.24,4.02;9,305.19,324.59,7.24,15.56;9,305.19,306.10,7.24,16.54;9,305.19,290.81,7.24,13.37;9,305.19,284.81,7.24,4.02;9,316.42,324.59,7.24,15.56;9,316.42,306.10,7.24,16.54;9,316.42,290.81,7.24,13.37;9,316.42,284.81,7.24,4.02;9,327.65,324.59,7.24,15.56;9,327.65,306.10,7.24,16.54;9,327.65,290.81,7.24,13.37;9,327.65,284.81,7.24,4.02;9,338.89,306.18,7.24,16.43;9,338.89,290.88,7.24,13.27;9,338.89,284.89,7.24,4.02;9,350.12,306.18,7.24,16.43;9,350.12,290.88,7.24,13.27;9,350.12,284.89,7.24,4.02;9,361.35,306.18,7.24,16.43;9,361.35,290.88,7.24,13.27;9,361.35,284.89,7.24,4.02;9,372.58,306.18,7.24,16.43;9,372.58,290.88,7.24,13.27;9,372.58,284.89,7.24,4.02;9,383.81,306.18,7.24,62.17;9,383.81,290.81,7.24,13.37;9,383.81,284.81,7.24,4.02;9,395.05,306.18,7.24,62.17;9,395.05,290.81,7.24,13.37;9,395.05,284.81,7.24,4.02;9,406.28,306.18,7.24,62.17;9,406.28,290.81,7.24,13.37;9,406.28,284.81,7.24,4.02;9,417.51,306.18,7.24,62.17;9,417.51,290.81,7.24,13.37;9,417.51,284.81,7.24,4.02;9,428.74,306.16,7.24,12.90;9,428.74,290.81,7.24,13.38;9,428.74,284.81,7.24,4.02;9,439.97,306.16,7.24,12.90;9,439.97,290.81,7.24,13.38;9,439.97,284.81,7.24,4.02;9,451.21,306.16,7.24,12.90;9,451.21,290.81,7.24,13.38;9,451.21,284.81,7.24,4.02;9,462.44,306.16,7.24,12.90;9,462.44,290.81,7.24,13.38;9,462.44,284.81,7.24,4.02;9,262.13,243.51,63.11,7.24;9,262.13,254.57,197.86,7.24;9,262.13,265.61,137.09,7.24"><head></head><label></label><figDesc>MAP restricted to a black list of (potentially) invasive species MAP igoring unknown classes and queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,124.70,562.21,346.01,112.94"><head>Table 1 .</head><label>1</label><figDesc>our evaluation experiments, the enhanced CNN model is used to extract class scores. Four different post processing methods are then applied to the extracted class scores, detailed as follows. As mentioned in Section 4.2 and 4.3, ğ’• is the rejection threshold obtained from training set, while ğ’•â€² is the rejection threshold obtained by considering both training and validation sets. â”€ Run 1: Sample rejection with ğ’• , identification based on single sample â”€ Run 2: Sample rejection with ğ’• â€² , identification based on single sample â”€ Run 3: Sample rejection with ğ’• , observation based identification â”€ Run 4: Sample rejection with ğ’• â€² , observation based identificationIn our run files (Bluefield), we provide scores up to top 30 classes, and rejected samples are entirely excluded from our run files. Evaluation results compared with other participants are summarized in Table1 and Figure 9. Evaluation results sorted by official score MAP (MAP: Mean Average Precision) Evaluation results sorted by run name in alphabetical order Bluefield Run 4 yields the highest official score among all of the participants. Usage of ObservationId shows significant improvement. On the other hand, rejection threshold ğ’• â€² that takes validation set into account shows a slight improvement in official MAP.</figDesc><table coords="8,140.06,217.79,318.40,361.07"><row><cell>Run</cell><cell>Official score MAP</cell><cell>MAP restricted to potentially invasive species</cell><cell>MAP ignoring unknown classes and queries</cell></row><row><cell>Bluefield Run 4</cell><cell>0.742</cell><cell>0.717</cell><cell>0.827</cell></row><row><cell>SabanciUGebzeTU Run 1</cell><cell>0.738</cell><cell>0.704</cell><cell>0.806</cell></row><row><cell>SabanciUGebzeTU Run 3</cell><cell>0.737</cell><cell>0.703</cell><cell>0.807</cell></row><row><cell>Bluefield Run 3</cell><cell>0.736</cell><cell>0.718</cell><cell>0.820</cell></row><row><cell>SabanciUGebzeTU Run 2</cell><cell>0.736</cell><cell>0.683</cell><cell>0.807</cell></row><row><cell>SabanciUGebzeTU Run 4</cell><cell>0.735</cell><cell>0.695</cell><cell>0.802</cell></row><row><cell>CMP Run 1</cell><cell>0.710</cell><cell>0.653</cell><cell>0.790</cell></row><row><cell>LIIR KUL Run 3</cell><cell>0.703</cell><cell>0.674</cell><cell>0.761</cell></row><row><cell>LIIR KUL Run 2</cell><cell>0.692</cell><cell>0.667</cell><cell>0.744</cell></row><row><cell>LIIR KUL Run 1</cell><cell>0.669</cell><cell>0.652</cell><cell>0.708</cell></row><row><cell>UM Run 4</cell><cell>0.669</cell><cell>0.598</cell><cell>0.742</cell></row><row><cell>CMP Run 2</cell><cell>0.644</cell><cell>0.564</cell><cell>0.729</cell></row><row><cell>CMP Run 3</cell><cell>0.639</cell><cell>0.590</cell><cell>0.723</cell></row><row><cell>QUT Run 3</cell><cell>0.629</cell><cell>0.610</cell><cell>0.696</cell></row><row><cell>Floristic Run 3</cell><cell>0.627</cell><cell>0.533</cell><cell>0.693</cell></row><row><cell>UM Run 1</cell><cell>0.627</cell><cell>0.537</cell><cell>0.700</cell></row><row><cell>Floristic Run 1</cell><cell>0.619</cell><cell>0.541</cell><cell>0.694</cell></row><row><cell>Bluefield Run 1</cell><cell>0.611</cell><cell>0.600</cell><cell>0.692</cell></row><row><cell>Bluefield Run 2</cell><cell>0.611</cell><cell>0.600</cell><cell>0.693</cell></row><row><cell>Floristic Run 2</cell><cell>0.611</cell><cell>0.538</cell><cell>0.681</cell></row><row><cell>QUT Run 1</cell><cell>0.601</cell><cell>0.563</cell><cell>0.672</cell></row><row><cell>UM Run 3</cell><cell>0.589</cell><cell>0.509</cell><cell>0.652</cell></row><row><cell>QUT Run 2</cell><cell>0.564</cell><cell>0.562</cell><cell>0.641</cell></row><row><cell>UM Run 2</cell><cell>0.481</cell><cell>0.446</cell><cell>0.552</cell></row><row><cell>QUT Run 4</cell><cell>0.367</cell><cell>0.359</cell><cell>0.378</cell></row><row><cell>BME TMIT Run 4</cell><cell>0.174</cell><cell>0.144</cell><cell>0.213</cell></row><row><cell>BME TMIT Run 3</cell><cell>0.170</cell><cell>0.125</cell><cell>0.197</cell></row><row><cell>BME TMIT Run 1</cell><cell>0.169</cell><cell>0.125</cell><cell>0.196</cell></row><row><cell>BME TMIT Run 2</cell><cell>0.066</cell><cell>0.128</cell><cell>0.101</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="grantName">MEXT KAKENHI</rs>, <rs type="grantName">Grant-in-Aid for Challenging Exploratory Research</rs>, Grant Number <rs type="grantNumber">15K12027</rs> for partial support of our work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4e2ExyA">
					<orgName type="grant-name">MEXT KAKENHI</orgName>
				</org>
				<org type="funding" xml:id="_QgmjtxC">
					<idno type="grant-number">15K12027</idno>
					<orgName type="grant-name">Grant-in-Aid for Challenging Exploratory Research</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,128.47,259.76,342.08,9.05;10,140.18,271.27,172.39,9.06" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,279.49,259.76,191.05,9.05;10,140.18,271.28,98.42,9.05">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,260.09,271.27,21.96,9.06">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,285.83,341.62,9.05;10,140.18,297.34,269.13,9.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,323.95,285.83,146.13,9.05;10,140.18,297.35,190.59,9.05">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,352.39,297.34,26.58,9.06">TPAMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,311.75,341.62,9.05;10,140.18,323.26,293.01,9.06" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,301.81,311.75,168.27,9.05;10,140.18,323.27,219.73,9.05">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,380.71,323.26,21.96,9.06">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,337.79,341.82,9.05;10,140.18,349.30,148.87,9.06" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,349.49,337.79,120.79,9.05;10,140.18,349.31,78.81,9.05">ImageNet Classification with Deep Convolutional</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,240.41,349.30,18.89,9.06">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,363.83,341.72,9.05;10,140.18,375.34,330.41,9.06;10,140.18,386.75,22.61,9.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,283.26,375.35,133.33,9.05">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,443.11,375.34,21.98,9.06">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,401.27,341.98,9.05;10,140.18,412.78,120.78,9.06" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,320.11,401.27,150.33,9.05;10,140.18,412.79,46.76,9.05">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.37,412.78,21.96,9.06">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,427.30,342.13,9.06;10,140.18,438.83,22.61,9.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,281.83,427.31,139.73,9.05">LifeCLEF Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,443.71,427.30,21.50,9.06">CLEF</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,453.23,342.16,9.05;10,140.18,464.77,329.82,9.05;10,140.18,476.28,220.74,9.06" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,335.53,464.77,134.48,9.05;10,140.18,476.29,131.69,9.05">LifeCLEF 2016: Multimedia Life Species Identification Challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>LifeCLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,128.47,490.81,342.16,9.05;10,140.18,502.32,163.27,9.06" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,302.12,490.81,168.50,9.05;10,140.18,502.33,68.11,9.05">Plant Identification In An Open-World (LifeCLEF 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>PlantCLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.08,516.85,336.93,9.05;10,140.18,528.24,180.78,9.06" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,324.92,516.85,145.08,9.05;10,140.18,528.25,106.52,9.05">Empirical Evaluation of Rectified Activations in Convolution</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,268.37,528.24,21.96,9.06">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.08,542.77,337.06,9.05;10,140.18,554.28,199.03,9.06" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,270.07,542.77,200.08,9.05;10,140.18,554.29,113.25,9.05">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.97,554.28,34.31,9.06">AISTATS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
