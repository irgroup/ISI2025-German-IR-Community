<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62;1,224.19,135.80,166.97,10.52">LifeCLEF Bird Identification Task 2016 The arrival of Deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.19,171.83,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="department">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.29,171.83,56.56,8.74"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CNRS LSIS</orgName>
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">ENSAM</orgName>
								<orgName type="institution" key="instit3">Univ. Toulon</orgName>
								<orgName type="institution" key="instit4">Institut Univ. de France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.40,171.83,90.66,8.74"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.62,171.83,68.94,8.74"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.24,183.78,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62;1,224.19,135.80,166.97,10.52">LifeCLEF Bird Identification Task 2016 The arrival of Deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05494E05B258EBC2055FABD34B9450D0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>ecological monitoring 6 Scaled Acoustic Biodiversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF bird identification challenge provides a largescale testbed for the system-oriented evaluation of bird species identification based on audio recordings. One of its main strength is that the data used for the evaluation is collected through Xeno-Canto, the largest network of bird sound recordists in the world. This makes the task closer to the conditions of a real-world application than previous, similar initiatives. The main novelty of the 2016-th edition of the challenge was the inclusion of soundscape recordings in addition to the usual xeno-canto recordings that focus on a single foreground species. This paper reports the methodology of the conducted evaluation, the overview of the systems experimented by the 6 participating research groups and a synthetic analysis of the obtained results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. The general public as well as professionals like park rangers, ecological consultants and of course the ornithologists themselves are potential users of an automated bird identifying system, typically in the context of wider initiatives related to ecological surveillance or biodiversity conservation. The LifeCLEF Bird challenge proposes to evaluate the state-ofthe-art of audio-based bird identification systems at a very large scale. Before LifeCLEF started in 2014, three previous initiatives on the evaluation of acoustic bird species identification took place, including two from the SABIOD 6 group <ref type="bibr" coords="2,134.77,118.99,10.79,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,145.56,118.99,7.20,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,152.75,118.99,7.20,8.74" target="#b0">1]</ref>. In collaboration with the organizers of these previous challenges, Bird-CLEF 2014, 2015 and 2016 challenges went one step further by (i) significantly increasing the species number by an order of magnitude, (ii) working on realworld social data built from thousands of recordists, and (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of meta-data and defining information retrieval oriented metrics. Overall, the task is much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (different recording devices, contexts diversity, etc.). It therefore produces substantially lower scores and offers a better progression margin towards building real-world generalist identification tools. The main novelty of the 2016-th edition of the challenge with respect to the two previous years was the inclusion of soundscape recordings in addition to the usual xeno-canto recordings that focus on a single foreground species (usually thanks to mono-directional recording devices). Soundscapes, on the other hand, are generally based on omnidirectional recording devices that continuously monitor a specific environment over a long period. This new kind of recording fits better to the (possibly crowdsourced) passive acoustic monitoring scenario that could augment the number of collected records by several orders of magnitude. In this paper, we report the methodology of the conducted evaluation as well as the synthetic analysis of the results achieved by the 6 participating groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The training and test data of the challenge consists of audio recordings collected by Xeno-canto (XC) <ref type="foot" coords="2,249.22,421.03,3.97,6.12" target="#foot_0">7</ref> . Xeno-canto is a web-based community of bird sound recordists worldwide with about 3,000 active contributors that have already collected more than 300,000 recordings of about 9550 species (numbers for June 2016). Nearly 1000 (in fact 999) species were used in the BirdCLEF dataset, representing the 999 species with the highest number of recordings in October 2014 (14 or more) from the combined area of Brazil, French Guiana, Surinam, Guyana, Venezuela and Colombia, totalling 33,203 recordings produced by thousands of users. This dataset includes the entire dataset from the 2015 BirdCLEF challenge <ref type="bibr" coords="2,134.77,518.25,9.96,8.74" target="#b4">[5]</ref>, which contained about 33,000 recordings. The newly introduced test data in 2016, contains 925 soundscapes provided by 7 xeno-canto members, sometimes working in pairs. Most of the soundscapes have a length of (more or less) 10 minutes, each coming often from a set of 10-12 successive recording made at one location. The total duration of new testing data to process and analyse is thus equivalent to approximately 6 days of continuous sound recording. The number of known species (i.e. belonging to the 999 species in the training dataset) varies from 1 to 25 species, with an average of 10.1 species per soundscape.</p><p>To avoid any bias related to the used audio devices in the evaluation , each audio file was normalized to a constant bandwidth of 44.1 kHz and coded with 16 bits in wav mono format (the right channel is selected by default). The conversion from the original Xeno-canto data set was done using ffmpeg, sox and matlab scripts. The optimized 16 Mel Filter Cepstrum Coefficients for bird identification (according to an extended benchmark <ref type="bibr" coords="3,327.18,142.90,10.79,8.74" target="#b1">[2]</ref>) were computed with their first and second temporal derivatives on the whole set. They were used in the best systems run in ICML4B and NIPS4B challenges. However, due to some technical limitations, the soundscapes were not normalized and directly provided to the participants in mp3 format (shared on the xeno-canto website, the original raw files being not available).</p><p>All audio records are associated with various meta-data including the species name of the most active singing bird, the species of the other birds audible in the background, the type of sound (call, song, alarm, flight, etc.), the date and location of the observations (from which rich statistics on species distribution may be derived), some textual comments by the authors, multilingual common names and collaborative quality ratings. All of them were produced collaboratively by the Xeno-canto community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Participants were asked to determine all the active singing birds species in each query file. It was forbidden to correlate the test set of the challenge with the original annotated Xeno-canto database (or with any external content as many of them are circulating on the web). The whole data was split in two parts, one for training (and/or indexing) and one for testing. The test set was composed of (i) all the newly introduced soundscapes recordings and (ii), the entire test set used in 2015 (equal to about 1/3 of the observations in the whole 2015 dataset). The training set was exactly the same as the one used in 2015 (i.e. the remaining 2/3 of the observations). Note that recordings of the same species made by the same person on the same day are considered as being part of the same observation and cannot be split across the test and training set. The XML files containing the meta-data of the query recordings were purged so as to erase the taxon name (the ground truth), the vernacular name (common name of the bird) and the collaborative quality ratings (that would not be available at query stage in a real-world mobile application). Meta-data of the recordings in the training set were kept unaltered.</p><p>The groups participating in the task were asked to produce up to 4 runs containing a ranked list of the most probable species for each query records of the test set. Each species was associated with a normalized score in the range [0, 1] reflecting the likelihood that this species is singing in the sample. For each submitted run, participants had to say if the run was performed fully automatically or with human assistance in the processing of the queries, and if they used a method based only on audio analysis or with the use of the metadata.</p><p>The primary metric used was the mean Average Precision (mAP) averaged across all queries, considering each audio file of the test set as a query and computed as:</p><formula xml:id="formula_0" coords="4,254.31,129.29,106.74,26.77">mAP = Q q=1 AveP (q) Q ,</formula><p>where Q is the number of test audio files and AveP (q) for a given test file q is computed as</p><formula xml:id="formula_1" coords="4,213.72,184.76,187.91,25.41">AveP (q) = n k=1 (P (k) × rel(k)) number of relevant documents .</formula><p>Here k is the rank in the sequence of returned species, n is the total number of returned species, P (k) is the precision at cut-off k in the list and rel(k) is an indicator function equaling 1 if the item at rank k is a relevant species (i.e. one of the species in the ground truth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>84 research groups worldwide registered for the task and downloaded the data (from a total of 130 groups that registered for at least one of the three Life-CLEF tasks). This shows the high attractiveness of the challenge in both the multimedia community (presumably interested in several tasks) and in the audio and bioacoustics community (presumably registered only to the bird songs task). Finally, 6 of the registrants crossed the finish line by submitting runs and 5 of them submitted working notes explaining their runs in detail. We list them hereafter in alphabetical order and give a brief overview of the techniques they used in their runs. We would like to point out that the LifeCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested in the scientific and technical details of the implemented methods should refer to the LifeCLEF 2016 working notes or to the research papers of each participant (referenced below):</p><p>BME TMIT, Hungary, 4 runs <ref type="bibr" coords="4,289.82,476.76,16.81,8.77" target="#b10">[11]</ref>: BME TMIT is one of the three teams who used a Convolutional Neural Network with CUBE and WUT teams. As pre-processing, they first downsampled each audio file to 16 kHz frequency and applied a low-pass filter with cutoff frequency of 6250 Hz in order to reduce the size of the training data. Then they subdivided the spectograms into cells of 0.5 seconds x 10 bands of frequency, and removed the cells with few information (according to the mean and variance). After these preprocessing steps, they assembled and re-split the remaining parts of the spectrograms to five second long pieces, and obtained arrays of 200310 (where 310 samples corresponds to five seconds), used as input of the CNN. They used two distincts CNN architectures: the well-know AlexNet <ref type="bibr" coords="4,237.75,596.34,10.52,8.74" target="#b5">[6]</ref> with the addition of a batch normalisation (run 1 &amp; 2), and a CNN more inspired by audio recognition systems based on 4 convolutional layers, one full connected layer, ReLU activation functions and batch normalisation (run 3 &amp; 4).</p><p>CUBE, Switzerland, 4 runs: This system is based on a CNN architecture of 5 convolutional layers combined with the use of a rectify activation function followed by a max-pooling layer. Based on spectrogram analysis and morphological operations, silent and noisy parts were first detected and separated from the call and song parts. Spectrograms were then split into chunks of 3 seconds that were used as inputs of the CNN after several data augmentation techniques.</p><p>Each chunk identified as a singing bird was first concatenated with 3 randomly selected chunks of background noise. Time shift, pitch sift and mixes of audio files from the same species were then used as complementary data augmentation techniques. Considering one test record, all predictions from its distinct chunks are finally averaged. Run 1 was an intermediate result obtained after only one day of training. Run 2 differs from run 3 by using 50% smaller spectrograms in (pixel) size for doubling the batch size and thus allowing to have more iterations for the same training time (4 days). Run 4 is the average of predictions from run 2 and 3 and reaches the best performance, showing the benefit of bagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNI LSIS, France, 1 runs [10]:</head><p>The algorithm presented here is quite standard and was initially used on smaller datasets to improve, in a late fusion scheme, a classifier based on pairs of spectrogram peaks, described in the context of audio fingerprinting. The method is based on the bag-of-words approach: first the 44.1 kHz audio files were split in 0.2s segments with 50% overlap, and only the segments having energy values higher than a relative (to the whole audio file) value and spectral flatness values smaller than an absolute thresh-old were kept for Mel Frequency Cepstral Coefficient computation (MFCC). A k-means clustering was performed on all the MFCC and their derivatives with k=500, in order to extract for every files the normalized histogram of MFCC-based words (i.e. the 500 clusters), using only segments kept in step 2. The resulting feature vectors were then fed to a random forest classifier.</p><p>MNB TSA, Germany, 4 runs <ref type="bibr" coords="5,291.15,453.71,11.46,8.77" target="#b7">[8]</ref>: As in 2014 and 2015, this participant used two hand-crafted parametric acoustic features and probabilities of speciesspecific spectrogram segments in a template matching approach. Long segments extracted during BirdCLEF2015 were re-segmented with a more sensitive algorithm. The segments were then used to extract Segment-Probabilities for each file by calculating the maxima of the normalized cross-correlation between all segments and the target spectrogram image via template matching. Due to the very large amount of audio data, not all files were used as a source for segmentation (i.e. only good quality files without background species were used). The classification problem was then formulated as a multi-label regression task solved by training ensembles of randomized decision trees with probabilistic outputs. The training was performed in 2 passes, one selecting a small subset of the most discriminant features by optimizing the internal mAP score on the training set, and one training the final classifiers on the selected features. Run 1 used one single model on a small but highly optimized selection of Segment-Probabilities. A bagging approach was used consisting in calculating further Segment-Probabilities from additional segments and to combine them either by blending (24 models in Run 3). Run 4 also used blending to aggregate model predictions, but the predictions were included that after blending resulted in the highest possible mAP score calculated on the entire training set (13 models including the best model from 2015).</p><p>WUT, Poland, 4 runs <ref type="bibr" coords="6,252.49,178.74,11.46,8.77" target="#b8">[9]</ref>: as the Cube and the BME TMIT teams, they used a Convolutional Neural Network learning framework. Starting from denoised spectrograms, silent parts were removed with percentile thresholding, giving thus around 86,000 training segments varying in length and associated each with a single main species. As a data augmentation technique and for fitting the 5 seconds fixed input size of the CNN, segments were adjusted by either trimming or padding. The 3 first successive runs are produced by deeper and deeper, or/and, wider and wider filters. Run 4 is as an ensemble of neural networks averaging the predictions of the 3 first runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Figure <ref type="figure" coords="6,167.54,337.50,4.98,8.74" target="#fig_0">1</ref> and table <ref type="table" coords="6,224.16,337.50,4.98,8.74" target="#tab_0">1</ref> show the scores obtained by all the runs for the three distinct measured mean Average Precision (mAP) evaluation measures: Figure <ref type="figure" coords="7,181.70,118.99,4.98,8.74" target="#fig_0">1</ref> reports the performance measured for the 18 submitted runs. For each run (i.e. each evaluated system), we report the overall mean Average Precision (official metric) as well as the mAP for the two categories of queries: the soundscapes recordings (newly introduced) and the common observations (the same as the one used in 2015). To measure the progress over last year, we also plot on the graph the performance of the last year best system <ref type="bibr" coords="7,403.90,178.77,10.52,8.74" target="#b6">[7]</ref> (orange dotted line). The first noticeable conclusion is that, after two years of resistance of bird songs identification systems based on engineering features, convolutional neural networks finally managed to outperform them (as in many other domains). The best run based on CNN (Cube Run 4) actually reached an impressive mAP of 0.69 on the 2015 testbed to be compared to respectively 0.45 and 0.58 for the best systems based on hand-crafted features evaluated in 2015 and 2016. To our knowledge, BirdCLEF is the first comparative study reporting such an important performance gap in bioacoustic large-scale classification. A second important remark is that this performance of CNN's was achieved without any fine-tuning contrary to most computer vision challenges in which the CNN is generally pretrained on a large training data such as ImageNet. Thus, we could hope even better performance, e.g. by transferring knowledge from other bio-acoustic contexts or other domains. Now, it is important to notice that the other systems based on CNN (WUT and BME TMIT) did not perform as well as the Cube system and did not outperformed the system of TSA based on hand-crafted features. Looking at the detailed description of the three CNN architectures and their learning framework, it appears that the way in which audio segment extraction and data augmentation is performed does play a crucial role. Cube system does notably include a randomized background noise addition phase which makes it much more robust to the diversity of noise encountered in the test data. If we now look at the scores achieved by the evaluated systems on the soundscape recordings only (yellow plot), we can draw very different conclusions. First of all, we can observe that the performance on the soundscapes is much lower than on the classical queries, whatever the system. Although the classical recordings also include multiple species singing in the background, the soundscapes appear to be much more challenging. Several tens of species and even much more individual birds can actually be singing simultaneously. Separating all these sources seem to be beyond the scope of state-of-the-art audio representation learning methods. Interestingly, the best system on the soundscape queries was the one of TSA based on the extraction of very short species-specific spectrogram segments and a template matching approach. This very fine-grained approach allows the extracted audio patterns to be more robust to the species overlap problem. On the contrary, the CNN of Cube and WUT systems were optimized for the mono-species segment classification problem. The data augmentation method of the Cube system was in particular only designed for the single species case. It addressed the problem of several individual birds of the same species singing together (by mixing different segments of the same class) but it did not address the multi-label issue (i.e. several species singing simultaneously). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Complementary results</head><p>To study in more details the dynamic of the identification performance across the diversity of species, Figure <ref type="figure" coords="8,268.84,428.97,4.98,8.74" target="#fig_1">2</ref> presents the scores achieved by the best system of each team on a selection of 3x10 species: (i) the top-10 best recognized ones (according to the performance of the best system Cube Run 4 ), (ii) 10 species of intermediate difficulties and (iii) the worst-10 recognized ones (still based on the performance of Cube Run 4 ). For a better interpretation of the chart, we also included for each of the 30 selected species, the number of audio recordings in the training set (ranging from 10 to 37 recordings). The graph first shows that there is a huge performance gap between the best recognized species and the worst cases. Some species are actually perfectly classified by 4 of the 6 systems whereas some others are never recognized by none of the systems. Interestingly, one can see that the performance does not seem to be correlated to the number of training samples. In the same way, we did observed that it is not correlated to the average length of the recordings in the class. This means that the high variability in performance is more related to other factors such as (i) the bird sounds variability (some birds are more audible than others), (ii) the acquisition difficulty (some birds are easier to record than others), (iii) the degree of confusion across close species. Another interesting remark is that two of the species that are not recognized at all by the CNN are comparatively pretty well recognized by the template matching kernel approach of MNB TSA. Thus, it would be interesting to study in more details the kind of audio patterns that have been matched by their method so as to understand what the CNN missed and how such patterns could be automatically learned as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF bird identification challenge 2016. The main outcome was that after two years of resistance of bird song identification systems based on engineering features, convolutional neural networks finally managed to outperform them with a significant margin. It is noticeable that the best performing CNN did not used any fine-tuning so that it did not benefit from the transfer learning capacities of that techniques. We could thus expect even better performances. Also, the used CNN architecture was mostly inspired by the ones which perform the best on computer vision tasks. Our detailed analysis of the results tend to show that some audio patterns might not be learned accurately through such network whereas they are detected through template matching techniques. Anyway, it is obvious that, as in many domains beforehand, deep learning is redefining the boundaries of the state-of-the-art and opens the door to further progress in the next years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,159.33,358.14,296.70,7.89;8,134.77,115.84,345.82,216.57"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official scores of the LifeCLEF Bird Identification challenge 2016.</figDesc><graphic coords="8,134.77,115.84,345.82,216.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,134.77,385.66,345.83,7.89;9,134.77,396.64,345.82,7.86;9,134.77,407.60,345.82,7.86;9,134.77,418.56,114.51,7.86;9,134.77,165.37,345.83,194.56"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average Precision detailed on a selection of 3x10 species for the best run of each team, following the ranking given by the best overall system Cube Run 4 : (A) the top-10 species, (M) intermediate species (species ranked from 495 to 505), and (Z) species with the lowest APs.</figDesc><graphic coords="9,134.77,165.37,345.83,194.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,138.42,383.01,325.39,277.97"><head>Table 1 :</head><label>1</label><figDesc>Results of the LifeCLEF 2016 Bird Identification Task</figDesc><table coords="6,138.42,400.26,325.39,260.72"><row><cell>RunNameShort</cell><cell>Official score: mAP (with background species)</cell><cell>mAP (only main species. same queries BirdCLEF2015)</cell><cell>mAP with background species (only queries 2016 "Soundscape")</cell></row><row><cell>Cube Run 4</cell><cell>0.555 (1)</cell><cell>0.686 (1)</cell><cell>0.072 (5)</cell></row><row><cell>Cube Run 3</cell><cell>0.536 (2)</cell><cell>0.660 (2)</cell><cell>0.078 (4)</cell></row><row><cell>Cube Run 2</cell><cell>0.522 (3)</cell><cell>0.649 (3)</cell><cell>0.066</cell></row><row><cell>MarioTsaBerlin Run 1</cell><cell>0.519 (4)</cell><cell>0.585 (4)</cell><cell>0.137 (1)</cell></row><row><cell>MarioTsaBerlin Run 4</cell><cell>0.472 (5)</cell><cell>0.551 (5)</cell><cell>0.129 (3)</cell></row><row><cell>WUT Run 4</cell><cell>0.412</cell><cell>0.529</cell><cell>0.036</cell></row><row><cell>MarioTsaBerlin Run 3</cell><cell>0.396</cell><cell>0.456</cell><cell>0.130 (3)</cell></row><row><cell>WUT Run 2</cell><cell>0.376</cell><cell>0.483</cell><cell>0.032</cell></row><row><cell>WUT Run 3</cell><cell>0.352</cell><cell>0.455</cell><cell>0.029</cell></row><row><cell>WUT Run 1</cell><cell>0.35</cell><cell>0.453</cell><cell>0.027</cell></row><row><cell>BME TMIT Run 2</cell><cell>0.338</cell><cell>0.426</cell><cell>0.053</cell></row><row><cell>BME TMIT Run 3</cell><cell>0.337</cell><cell>0.426</cell><cell>0.059</cell></row><row><cell>MarioTsaBerlin Run 2</cell><cell>0.336</cell><cell>0.399</cell><cell>0.000</cell></row><row><cell>BME TMIT Run 4</cell><cell>0.335</cell><cell>0.424</cell><cell>0.053</cell></row><row><cell>BME TMIT Run 1</cell><cell>0.323</cell><cell>0.407</cell><cell>0.054</cell></row><row><cell>Cube Run 1</cell><cell>0.284</cell><cell>0.364</cell><cell>0.020</cell></row><row><cell>LSIS naive MFCC Run 1</cell><cell>0.149</cell><cell>0.183</cell><cell>0.037</cell></row><row><cell>BIG Run 1</cell><cell>0.021</cell><cell>0.021</cell><cell>0.004</cell></row><row><cell>Best run BirdCLEF 2015</cell><cell>-</cell><cell>0.454</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="2,144.73,657.44,122.88,7.47"><p>http://www.xeno-canto.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,142.59,337.64,7.86;10,151.52,153.55,329.07,7.86;10,151.52,164.51,329.07,7.86;10,151.52,175.46,127.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,388.47,142.59,92.13,7.86;10,151.52,153.55,329.07,7.86;10,151.52,164.51,84.33,7.86">The 9th mlsp competition: New methods for acoustic classification of multiple simultaneous bird species in noisy environment</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eftaxias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,258.97,164.51,221.63,7.86;10,151.52,175.46,61.10,7.86">IEEE Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,186.42,318.90,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,329.07,7.86;10,151.52,219.30,33.30,7.86;10,202.37,219.30,25.66,7.86;10,245.58,219.30,21.12,7.86;10,284.25,219.30,28.16,7.86;10,329.96,219.95,150.64,7.47;10,151.52,230.91,246.77,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,394.39,186.42,67.46,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,45.37,7.86">Clusterized mel ter cepstral coefficients and support vector machines for bird song idenfication</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Giraudet</surname></persName>
		</author>
		<ptr target="http://www.intechopen.com/books/soundscape-semiotics-localisation-and-categorisation" />
	</analytic>
	<monogr>
		<title level="m" coord="10,236.22,208.34,244.37,7.86;10,151.52,219.30,29.60,7.86">Soundscape Semiotics -Localization and Categorization</title>
		<imprint>
			<publisher>Glotin</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,241.22,337.64,7.86;10,151.52,252.18,329.07,7.86;10,151.52,263.14,329.07,8.12;10,151.52,274.74,122.39,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,441.76,241.22,38.83,7.86;10,151.52,252.18,99.54,7.86">Bioacoustic challenges in icml4b</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.org/ICML4B2013_proceedings.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,290.27,252.18,190.32,7.86;10,151.52,263.14,62.74,7.86">Proc. of 1st workshop on Machine Learning for Bioacoustics</title>
		<meeting>of 1st workshop on Machine Learning for Bioacoustics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,285.05,337.64,7.86;10,151.52,296.01,329.07,7.86;10,151.52,306.97,329.07,7.86;10,151.52,317.93,252.29,8.12" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,277.38,285.05,203.21,7.86;10,151.52,296.01,34.95,7.86">Overview of the 2nd challenge on acoustic bird classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/nips4b" />
	</analytic>
	<monogr>
		<title level="m" coord="10,207.81,296.01,272.78,7.86;10,151.52,306.97,34.46,7.86">Proc. Neural Information Processing Scaled for Bioacoustics. NIPS Int. Conf</title>
		<meeting>Neural Information essing Scaled for Bioacoustics. NIPS Int. Conf<address><addrLine>Halkias X., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,328.89,337.63,7.86;10,151.52,339.85,266.76,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,451.78,328.89,28.80,7.86;10,151.52,339.85,91.71,7.86">Lifeclef bird identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,284.45,339.85,83.66,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,350.81,337.63,7.86;10,151.52,361.77,329.07,7.86;10,151.52,372.72,86.01,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,328.09,350.81,152.50,7.86;10,151.52,361.77,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.64,361.77,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,383.68,337.64,7.86;10,151.52,394.64,328.74,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,205.58,383.68,275.01,7.86;10,151.52,394.64,115.92,7.86">Improved automatic bird identification through decision tree based feature selection and bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.82,394.64,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,405.60,337.64,7.86;10,151.52,416.56,329.07,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,202.62,405.60,277.97,7.86;10,151.52,416.56,143.19,7.86">Improving bird identification using multiresolution template matching and feature selection during training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,314.23,416.56,138.40,7.86">Working notes of CLEF conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,427.52,337.64,7.86;10,151.52,438.48,275.00,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,199.23,427.52,281.36,7.86;10,151.52,438.48,62.15,7.86">Recognizing bird species in audio recordings using deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.07,438.48,162.78,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,449.44,337.98,7.86;10,151.52,460.40,154.32,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,241.36,449.44,185.28,7.86">Bag of mfcc-based words for bird identification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ricard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.53,449.44,34.06,7.86;10,151.52,460.40,125.64,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,471.35,337.98,7.86;10,151.52,482.31,328.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,250.25,471.35,230.34,7.86;10,151.52,482.31,136.59,7.86">Convolutional neural networks for large-scale bird song classification in noisy environment</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P</forename><surname>Tóth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Czeba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,309.64,482.31,141.28,7.86">Working notes of CLEF conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
