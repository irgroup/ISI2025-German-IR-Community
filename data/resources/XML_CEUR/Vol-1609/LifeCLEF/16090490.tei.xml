<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.98,152.67,311.30,12.64;1,150.02,170.67,295.18,12.64;1,271.13,188.67,52.96,12.64">Improving Bird Identification using Multiresolution Template Matching and Feature Selection during Training</title>
				<funder>
					<orgName type="full">BMUB (Bundesministerium für Umwelt, Naturschutz, Bau und Reaktorsicherheit)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,248.22,59.04,8.96"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Animal Sound Archive Museum für Naturkunde</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.98,152.67,311.30,12.64;1,150.02,170.67,295.18,12.64;1,271.13,188.67,52.96,12.64">Improving Bird Identification using Multiresolution Template Matching and Feature Selection during Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A2032A91334710A112C6469EF3E634A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Identification</term>
					<term>Multimedia Information Retrieval</term>
					<term>Spectrogram Segmentation</term>
					<term>Multiresolution Template Matching</term>
					<term>Feature Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working note describes methods to automatically identify a large number of different bird species by their songs and calls. It focuses primarily on new techniques introduced for this year's task like advanced spectrogram segmentation and decision tree based feature selection during training. Considering the identification of dominant species, previous results of the LifeCLEF Bird Identification Task could be further improved by 29%, achieving a mean Average Precision of 59% (mAP). The proposed approach ranked second place among all participating teams and provided the best system to identify birds in soundscape recordings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated acoustic methods of species identification can serve as a useful tool for biodiversity assessments. Within the scope of the LifeCLEF 2016 Bird Identification Task researchers are challenged to identify 999 different species in a large and highly diverse set of audio files. The audio recordings forming the training and test data set are built from the Xeno-canto collaborative database (www.xeno-canto.org). A novelty in this year's challenge is the enrichment of the test data set by including a new set of soundscape recordings. These soundscapes are not targeting any specific species during recording and can contain an arbitrary number of singing birds. To establish reliable acoustic methods for assessing biodiversity it is essential to improve the automated identification of birds in general but especially within these soundscape recordings. An overview and further details about the Bird Identification Task are given in <ref type="bibr" coords="1,135.37,648.59,10.69,8.96" target="#b0">[1]</ref>. The task is among others part of the LifeCLEF 2016 evaluation campaign <ref type="bibr" coords="1,456.36,648.59,10.76,8.96" target="#b1">[2]</ref>. Some methods referred to in the following sections are further developments of approaches already successfully applied in previous identification tasks. A more detailed description of these approaches can be found in <ref type="bibr" coords="1,317.07,684.62,10.98,8.96" target="#b2">[3,</ref><ref type="bibr" coords="1,328.05,684.62,7.32,8.96" target="#b3">4,</ref><ref type="bibr" coords="1,335.37,684.62,7.32,8.96" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Engineering</head><p>Two main categories of features (the same as last year) were used for training and prediction: matching probabilities of species-specific 2D spectrogram segments (see 2.1 Segment-Probabilities) and acoustic features extracted with openSMILE (see 2.2 Parametric Acoustic Features). For this year's task a large number of new Segment-Probability features were added for training by extracting new sound segments from audio files using the following two methods.</p><p>Re-segmentation of large segments. Using the automated segmentation method of spectrograms described in <ref type="bibr" coords="2,234.06,279.21,11.83,8.96" target="#b2">[3]</ref> some of the extracted segments turned out to be quite large and in some cases not very useful for template matchingespecially when processing audio files with a lot of background noise, low signal to noise ratio or many overlapping sounds. To overcome this problem all segments having a duration longer than half a second or a frequency range greater than 6 kHz were treated as separate spectrogram images and re-segmented again with a slightly different image preprocessing technique. The preprocessing steps for these too large segments differed in the following ways from the original preprocessing of the spectrogram image:</p><p> transform spectrogram into a binary image via Median Clipping by setting each pixel to 1, if it is 3.5 (instead of 3) times the median of its corresponding row AND column, otherwise to 0  apply binary closing with structuring element of size 2x2 (instead of 6x10) pixel  no dilation (instead of binary dilation with structuring element of size 3x5 pixel)  apply median filter with window size of 4x4 (instead of 5x3) pixel  remove small objects if smaller then 10 (instead of 50) pixel  enlarge segments in each direction by 10 (instead of 12) pixels Basically the image preprocessing was adjusted to be more sensitive and to capture smaller sound components and species-specific sub-elements within larger song structures and call sequences. Figure <ref type="figure" coords="2,263.09,516.59,4.98,8.96" target="#fig_0">1</ref> visualizes an example of the new segmentation method. Extracting more features by segmenting files with low average precision. Besides re-segmentation of segmented files, additional files from the training set were chosen for segment extraction. However, instead of a random selection, a small number of files were chosen for each species (approx. 2 to 4) by selecting the ones having the lowest average precision score calculated during cross validation in previous training steps. This was done in two iterations (with new training and feature selection steps in between) increasing the number of features by additional 1,375,928 Segment-Probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segment-Probabilities</head><p>For each species an individual feature set was formed by sweeping all segments related to that particular species over the spectrogram representations of all training and test recordings. The features were extracted via multiresolution template matching followed by selecting the maxima of the normalized cross-correlation <ref type="bibr" coords="3,407.83,572.27,10.69,8.96" target="#b5">[6]</ref>. In this context, multiresolution has a double meaning. On one hand it is referring to the time and frequency resolution of the spectrogram image itself. For 492,753 segments (already used for the BirdCLEF 2014 identification task) a time resolution of Δt = 11.6 ms (approx. 86 pixel per seconds) and a frequency resolution of Δf = 43.07 Hz (approx. 23 pixel per kHz) was used. For all other and newly extracted segments both time and frequency resolution was halved through downsampling the spectrogram image by a factor of 2. On the other hand the template matching can be also interpreted as multiresolution in terms of time and frequency range or size of the different spectrogram patches. Because further re-segmenting large segments, matching is performed for both: larger sound combinations (song syllables, call sequences) and smaller, rather fine-grained sound sub-elements (song elements, single calls).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parametric Acoustic Features</head><p>Besides Segment-Probabilities, for some models also parametric acoustic features were used for prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training and Feature Selection</head><p>The classification task was transformed to 999 one-vs-rest multi-label regression tasks. This way the number of selected features could be optimized separately and independently for each species during training. For each audio file in the training set the target function was set to 1.0 for the dominant species and 0.5 for all background species. Ensembles of randomized decision trees (ExtraTreesRegressor <ref type="bibr" coords="4,422.11,569.27,16.09,8.96" target="#b6">[10]</ref>) of the scikit-learn machine learning library were used for training and prediction <ref type="bibr" coords="4,423.55,581.27,15.43,8.96" target="#b7">[11]</ref>.</p><p>Feature Selection during Training. Feature importance returned by the ensemble of decision trees was cumulated during training and used to rank individual features. The importance of each feature is determined by the total reduction of the mean squared error brought by that particular feature. After a complete training pass, including cross validation, the number of features was reduced by keeping only the N highest scoring and therefore most important features. The number N of features kept for the next training iteration was set to select 85% of the best features from the previous iteration.</p><p>Different percentages were tested (75% to 90%) to find a good compromise between time of training and finding the optimal number of features. After the time consuming feature reduction procedure (the number of training iterations was repeated until there were only 5 features left to predict each species) the optimal number and best performing features per species were selected by finding either the maximum of the Area Under the Curve (AUC) or alternatively the maximum mAP score calculated over the entire training set. Figure <ref type="figure" coords="5,230.21,222.18,4.98,8.96" target="#fig_1">2</ref> shows two examples of resulting AUC (with and without background species), mAP and R2 (coefficient of determination) score trajectories when successively discarding 15% of the least important features. The maximum of each evaluation criteria is marked with a red square. The features used in the corresponding training iteration (maximum of AUC or mAP score) were then chosen for predicting the test files. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission Results</head><p>In Table <ref type="table" coords="6,162.50,177.18,4.98,8.96" target="#tab_1">1</ref>  Run 1. For the best performing first run just a single model was used. This model was trained using only a small but highly optimized selection of Segment-Probabilities (as described in the previous section). For this run, features were selected per species by optimizing the mAP score on the training set. A total of 125,402 features (with a minimum of 20, a maximum of 1833 and an average of 126 features per species) were used to predict all species in the test files.</p><p>Run 2. The second run was submitted quite early as an interim result and is therefore not worthy of being discussed here. It was actually supposed to be replaced by the submission of another run averaging the predictions of several different models. Unfortunately uploading could not be completed before the submission deadline.</p><p>Run 3. For the third submitted run blending of different models followed by postprocessing was used as described in <ref type="bibr" coords="6,276.99,539.75,10.69,8.96" target="#b4">[5]</ref>. Predictions from all models created during training as well as predictions from the two best performing models submitted last year were included (in total 24 models). Some models used Segment-Probabilities or openSMILE features only, others a combination of both. Also different feature sets were used with the number of features included for training and prediction optimized regarding either AUC (with and without using background species) or mAP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4.</head><p>The fourth run also used blending to aggregate model predictions. But unlike run 3 only those predictions were included that after blending resulted in the highest possible mAP score calculated on the entire training set (13 models including the best model from 2015). The here proposed approach ranked second place among all teams (MarioTsaBerlin Run 1 &amp; 4) and provided the best system to identify birds in soundscape recordings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Interestingly, for the best performing submission (Run 1) just a single model was used with only one category of features. Although using a good selection of features for this model one would expect that blending several models with different feature sets would perform better than just a single one. One possible explanation for the comparatively weak results achieved with blending could be the inclusion of the best combined models from 2015. Those combined and post-processed predictions already showed a fairly high overfitting on the training set and blending was perhaps done in favor of these predictions at the cost of the maybe better generalizing new models. On the other hand achieving an improvement by almost 30% on the mAP score with a single model (25% if taking background species into account) clearly shows that the techniques introduced this year could be applied very successfully. They also seem to complement each other quite well. Extracting additional, fine-grained spectrogram segments for template matching by re-segmenting larger segments captures typical sub-elements of songs or call sequences. Matching these sub-elements can give better identification performance than matching larger song structures, especially if those show a high variability between different individuals of the same species. The downside of the new segmentation method is a collection of many redundant or even use-less segments e.g. when dealing with noisy recordings or overlapping sounds from other species or sources. However, the proposed feature selection method can compensate for that by successively discarding irrelevant features during training. This year also deep learning techniques were successfully applied to the BirdCLEF dataset <ref type="bibr" coords="8,156.86,198.18,15.39,8.96" target="#b8">[12]</ref>. By using convolutional neural networks (CNNs) the best performing system achieved a mAP score of almost 70% when ignoring background species. It outperformed the here described approach by 17%, or 7% when also identifying all background species (see Fig. <ref type="figure" coords="8,241.78,234.18,4.16,8.96" target="#fig_2">3</ref> Cube Run 4). For soundscape recordings, however, the technique proposed in this paper achieved a 76% better performance than the best run using CNNs. Although identification performance for the new introduced test set was generally low among all teams, in the case of soundscapes, template matching seems to be better suited. The matching of rather small templates is not so much affected by surrounding sound events (e.g. coming from many simultaneously vocalizing animals) and therefore can create features more robust to various background noises. Compared to the black box architecture of a neuronal network classifier, using template matching and decision tree based feature selection also has some additional advantages. By visually or acoustically examining the most important and best discriminating sound elements of a species (typical calls, syllables or song phrases) one can gain a better insight into its sound repertoire and learn more about its call or song characteristics. The following figures visualize sound elements most suitable to identify a certain species. Each spectrogram segment is positioned at its original frequency position within a box representing the frequency range of 0 to 11025 Hz. More figures and additional material can be found at [13]. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,132.02,297.02,331.10,8.10;3,131.90,308.06,331.52,8.10;3,258.05,319.10,79.19,8.10;3,124.85,217.90,345.60,63.85"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Spectrogram re-segmentation example (MediaID: 8). top: initial segmentation (large segments marked in yellow), bottom: via re-segmentation of large segments extracted additional segments in red</figDesc><graphic coords="3,124.85,217.90,345.60,63.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,136.22,566.20,322.84,8.10;5,149.66,577.24,295.85,8.10;5,125.05,327.40,172.60,230.13"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Progress of AUC, mAP and R2 scores during feature selection for left: Scytalopus latrans (SpeciesID: lzezgo) and right: Psarocolius decumanus (SpeciesID: cxyhrl)</figDesc><graphic coords="5,125.05,327.40,172.60,230.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,124.70,150.18,345.89,8.96;7,124.70,162.18,346.01,8.96;7,124.70,174.18,335.91,8.96;7,125.07,201.40,345.15,203.98"><head>Figure 3</head><label>3</label><figDesc>Figure 3 visualizes the official scores of the LifeCLEF 2016 Bird Identification Task.The here proposed approach ranked second place among all teams (MarioTsaBerlin Run 1 &amp; 4) and provided the best system to identify birds in soundscape recordings.</figDesc><graphic coords="7,125.07,201.40,345.15,203.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,132.02,420.02,331.11,8.10;7,197.93,431.06,199.31,8.10"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Official scores of the LifeCLEF 2016 Bird Identification Task. The above described methods and submitted runs belong to MarioTsaBerlin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,191.69,481.60,211.55,8.10"><head>Fig. 4 .Fig. 5 .Fig. 6 .Fig. 7 .Fig. 8 .Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .Fig. 13 .Fig. 14 .Fig. 15 .Fig. 16 .Fig. 17 .Fig. 19 .Fig. 20 .Fig. 21 .Fig. 22 .Fig. 23 .Fig. 24 .Fig. 25 .Fig. 27 .Fig. 28 .Fig. 29 .Fig. 30 .Fig. 31 .Fig. 32 .Fig. 33 .</head><label>45678910111213141516171920212223242527282930313233</label><figDesc>Fig. 4. Pallid Spinetail / Cranioleuca pallida, (ID: aiwvzm)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.70,224.22,346.12,248.99"><head></head><label></label><figDesc>To extract these features the openSMILE Feature Extractor Tool [7] was utilized again. The configuration file originally designed for emotion detection in speech signals was adapted to capture the characteristics of bird sounds. It first calculates 57 low-level descriptors (LLDs) per frame, adds delta (velocity) and delta-delta (acceleration) coefficients to each LLD and finally applies 39 statistical functionals on all, via moving average smoothened, feature trajectories.</figDesc><table /><note coords="4,124.70,296.25,345.62,8.96;4,124.70,308.25,346.09,8.96;4,124.70,320.25,345.88,8.96;4,124.70,332.25,345.86,8.96;4,124.70,344.25,345.89,8.96;4,124.70,356.25,345.75,8.96;4,124.70,368.25,345.91,8.96;4,124.70,380.25,346.12,8.96;4,124.70,392.25,346.00,8.96;4,124.70,404.25,346.00,8.96;4,124.70,416.25,346.00,8.96;4,124.70,428.25,345.69,8.96;4,124.70,440.25,345.77,8.96;4,124.70,452.25,345.54,8.96;4,124.70,464.25,14.21,8.96"><p><p><p>The all in all 73 LLDs consist of: 1 time domain signal feature (zero crossing rate), 39 spectral features (Mel-spectrum bins 0-25; 25%, 50%, 75% and 90% spectral roll-off points; spectral centroid, flux, entropy, variance, skewness, kurtosis and slope; relative position of spectral minimum and maximum), 17 cepstral features (MFCC 0-16), 6 pitch-related features (F0, F0 envelope, F0 raw, voicing probability, voice quality, log harmonics-to-noise ratio computed from the ACF) and 10 energy-related features (logarithmic energy as well as energy in frequency bands: 150-500 Hz, 400-1000 Hz, 800-1500 Hz, 1000-2000 Hz, 1500-4000 Hz, 3000-6000 Hz, 5000-8000 Hz, 7000-10000 Hz and 9000-11000 Hz). To summarize an entire recording, statistics are calculated from all LLD, velocity and acceleration trajectories by 39 functionals including e.g. means, extremes, moments, percentiles and linear as well as quadratic regression. In total this sums up to 8541 (73•3•39) features per recording. Further details regarding openSMILE and the features extracted for bird identification can be found in the openSMILE book [8] and the OpenSmileForBirds_v2.conf configuration file</p>[9]</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,124.70,177.18,346.00,161.03"><head>Table 1 .</head><label>1</label><figDesc>results of the submitted runs are summarized using two evaluation statistics: mean of the Area Under the Curve calculated per species and mean Average Precision on the public training and the private test sets. For all runs no external resources and only audio features (features extracted from audio files) were used for training and prediction. Performance of submitted runs (without | with background species)</figDesc><table coords="6,131.66,266.70,335.43,71.51"><row><cell></cell><cell cols="2">Public Training Set</cell><cell>Test Set</cell><cell>Test Soundscapes</cell></row><row><cell>Run</cell><cell>Mean AUC [%]</cell><cell>mAP [%]</cell><cell>mAP [%]</cell><cell>mAP [%]</cell></row><row><cell>1</cell><cell>96.4 | 93.5</cell><cell>64.1 | 61.9</cell><cell>58.5 | 51.9</cell><cell>13.7</cell></row><row><cell>2</cell><cell>96.2 | 92.1</cell><cell>62.4 | 58.2</cell><cell>39.9 | 33.6</cell><cell>-</cell></row><row><cell>3</cell><cell>96.9 | 92.2</cell><cell>74.5 | 70.1</cell><cell>45.6 | 39.6</cell><cell>13.0</cell></row><row><cell>4</cell><cell>97.1 | 92.5</cell><cell>74.7 | 70.2</cell><cell>55.1 | 47.2</cell><cell>12.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Glotin</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Willem-Pier Vellinga</rs>, <rs type="person">Alexis Joly</rs> and <rs type="person">Henning Müller</rs> for organizing this task and the <rs type="institution">Xeno-Canto</rs> foundation for nature sounds as well as the French projects <rs type="institution">Floris'Tic (INRIA</rs>, <rs type="institution">CIRAD</rs>, <rs type="institution">Tela Botanica)</rs> and <rs type="institution">SABIOD Mastodons</rs> for their support. I also want to thank the <rs type="funder">BMUB (Bundesministerium für Umwelt, Naturschutz, Bau und Reaktorsicherheit)</rs>, the <rs type="institution">Museum für Naturkunde</rs> and especially <rs type="person">Dr. Karl-Heinz Frommolt</rs> for supporting my research and <rs type="person">Wolfram Fritzsch</rs> for providing me with additional hardware resources.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,127.03,427.10,343.56,8.10;12,136.10,438.02,150.33,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,363.31,427.10,107.27,8.10;12,136.10,438.02,35.97,8.10">LifeCLEF Bird Identification Task 2016</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,190.73,438.02,75.33,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,449.06,343.71,8.10;12,136.10,460.10,169.20,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,284.45,449.06,186.29,8.10;12,136.10,460.10,52.86,8.10">LifeCLEF 2016: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,207.17,460.10,77.77,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,471.04,343.69,8.10;12,136.10,482.08,334.29,8.10;12,136.10,493.12,312.10,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,210.41,471.04,260.30,8.10;12,136.10,482.08,96.03,8.10">Bird Song Classification in Field Recordings: Winning Solution for NIPS4B 2013 Competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,331.78,482.08,138.62,8.10;12,136.10,493.12,111.68,8.10">Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</editor>
		<meeting>of int. symp. Neural Information Scaled for Bioacoustics, sabiod<address><addrLine>, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">2013. dec. 2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>.org/nips4b, joint to NIPS</note>
</biblStruct>

<biblStruct coords="12,127.03,504.04,343.83,8.10;12,136.10,515.08,255.70,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,211.73,504.04,259.13,8.10;12,136.10,515.08,29.63,8.10">Towards Automatic Large-Scale Identification of Birds in Audio Recordings</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,181.44,515.08,128.23,8.10">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9283</biblScope>
			<biblScope unit="page" from="364" to="375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,526.12,343.43,8.10;12,136.10,537.04,276.53,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,212.09,526.12,258.36,8.10;12,136.10,537.04,108.95,8.10">Improved Automatic Bird Identification through Decision Tree based Feature Selection and Bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,263.69,537.04,148.94,8.10">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,548.08,297.96,8.10;12,123.62,559.12,347.03,8.10;12,136.10,570.04,334.47,8.10;12,136.10,581.08,334.50,8.10;12,136.10,592.12,110.95,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,197.45,548.08,227.53,8.10;12,123.62,559.12,3.41,8.10;12,319.39,559.12,151.26,8.10;12,136.10,570.04,193.92,8.10">Recent Developments in openSMILE, the Munich Open-Source Multimedia Feature Extractor</title>
		<author>
			<persName coords=""><forename type="first">Jp ;</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/2502081.2502224</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,352.27,570.04,113.43,8.10">Proc. ACM Multimedia (MM)</title>
		<meeting>ACM Multimedia (MM)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995. 2013. October 2013</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
	<note>Fast Normalized Cross-Correlation, Industrial Light and Magic 7</note>
</biblStruct>

<biblStruct coords="12,126.64,625.12,306.33,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,215.69,625.12,100.46,8.10">Extremely randomized trees</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,321.65,625.12,64.63,8.10">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,636.04,344.10,8.10;12,136.10,647.08,18.05,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,235.29,636.04,150.53,8.10">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,393.55,636.04,22.92,8.10">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,658.15,344.10,8.10;12,136.10,669.07,189.53,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,210.65,658.15,260.09,8.10;12,136.10,669.07,22.09,8.10">Audio Based Bird Species Identifcation using Deep Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,176.38,669.07,149.25,8.10">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
