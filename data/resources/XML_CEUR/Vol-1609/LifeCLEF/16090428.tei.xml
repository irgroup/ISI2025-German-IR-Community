<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.41,116.75,340.54,12.62;1,288.40,134.69,38.56,12.62">Plant identification in an open-world (LifeCLEF 2016)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.25,172.36,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.36,172.36,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.23,172.36,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.41,116.75,340.54,12.62;1,288.40,134.69,38.56,12.62">Plant identification in an open-world (LifeCLEF 2016)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D13EDD59160201A92077279F6FC65CAC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flower</term>
					<term>fruit</term>
					<term>bark</term>
					<term>stem</term>
					<term>branch</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>species identification</term>
					<term>citizen-science</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF plant identification challenge aims at evaluating plant identification methods and systems at a very large scale, close to the conditions of a real-world biodiversity monitoring scenario. The 2016-th edition was actually conducted on a set of more than 110K images illustrating 1000 plant species living in West Europe, built through a large-scale participatory sensing platform initiated in 2011 and which now involves tens of thousands of contributors. The main novelty over the previous years is that the identification task was evaluated as an open-set recognition problem, i.e. a problem in which the recognition system has to be robust to unknown and never seen categories. Beyond the brute-force classification across the known classes of the training set, the big challenge was thus to automatically reject the false positive classification hits that are caused by the unknown classes. This overview presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-based plant identification is the most promising solution towards bridging the botanical taxonomic gap, as illustrated by the proliferation of research work on the topic <ref type="bibr" coords="1,218.37,548.52,9.96,8.74" target="#b6">[7]</ref>, <ref type="bibr" coords="1,235.81,548.52,9.96,8.74" target="#b3">[4]</ref>, <ref type="bibr" coords="1,253.24,548.52,14.61,8.74" target="#b12">[13]</ref>, <ref type="bibr" coords="1,275.66,548.52,14.61,8.74" target="#b9">[10]</ref>, <ref type="bibr" coords="1,298.08,548.52,10.52,8.74" target="#b0">[1]</ref> as well as the emergence of dedicated mobile applications such as LeafSnap <ref type="bibr" coords="1,306.12,560.48,15.50,8.74" target="#b13">[14]</ref> or Pl@ntNet <ref type="bibr" coords="1,386.01,560.48,14.61,8.74" target="#b11">[12]</ref>. As promising as these applications are, their performance is still far from the requirements of a fully automated ecological surveillance scenario. Allowing the mass of citizens to produce accurate plant observations requires to equip them with much more effective identification tools. As an illustration, in 2015, 2,328,502 millions queries have been submitted by the users of the Pl@ntNet mobile apps but only less than 3% of them were finally shared and collaboratively validated. Allowing the exploitation of the unvalidated observations could scale up the world-wide collection of plant records by several orders of magnitude. Measuring and boosting the performance of automated identification tools is therefore crucial. As a first step towards evaluating the feasibility of such an automated biodiversity monitoring paradigm, we created and shared a new testbed entirely composed of image search logs of the Pl@ntNet mobile application (contrary to the previous editions of the PlantCLEF benchmark that were based on explicitly shared and validated plant observations).</p><p>As a concrete scenario, we focused on the monitoring of invasive exotic plant species. These species represent today a major economic cost to our society (estimated at nearly 12 billion euros a year in Europe) and one of the main threats to biodiversity conservation <ref type="bibr" coords="2,292.31,226.59,14.61,8.74" target="#b21">[22]</ref>. This cost can even be more important at the country level, such as in China where it is evaluated to be about 15 billion US dollars annually <ref type="bibr" coords="2,222.02,250.50,14.61,8.74" target="#b22">[23]</ref>, and more than 34 billion US dollars in the US <ref type="bibr" coords="2,442.40,250.50,14.61,8.74" target="#b16">[17]</ref>. The early detection of the appearance of these species, as well as the monitoring of changes in their distribution and phenology, are key elements to manage them, and reduce the cost of their management. The analysis of Pl@ntNet search logs can provide a highly valuable response to this problem because the presence of these species is highly correlated with that of humans (and thus to the density of data occurrences produced through the mobile application).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training dataset</head><p>For the training set, we provided the PlantCLEF 2015 dataset enriched with the ground truth annotations of the test images (that were kept secret during the 2015 campaign). More precisely, PlantCLEF 2015 dataset is composed of 113,205 pictures belonging to 41,794 observations of 1000 species of trees, herbs and ferns living in Western European regions. This data was collected by 8,960 distinct contributors. Each picture belongs to one and only one of the 7 types of views reported in the meta-data (entire plant, fruit, leaf, flower, stem, branch, leaf scan) and is associated with a single plant observation identifier allowing to link it with the other pictures of the same individual plant (observed the same day by the same person). An originality of the PlantCLEF dataset is that its social nature makes it close to the conditions of a real-world identification scenario: (i) images of the same species are coming from distinct plants living in distinct areas, (ii) pictures are taken by different users that might not used the same protocol of image acquisition, (iii) pictures are taken at different periods in the year. Each image of the dataset is associated with contextual meta-data (author, date, locality name, plant id) and social data (user ratings on image quality, collaboratively validated taxon name, vernacular name) provided in a structured xml file. The gps geo-localization and device settings are available only for some of the images. More precisely, each image is associated with the followings meta-data:</p><p>-ObservationId: the plant observation ID from which several pictures can be associated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Test dataset</head><p>For the test set, we created a new annotated dataset based on the image queries that were submitted by authenticated users of the Pl@ntNet mobile application in 2015 (unauthenticated queries had to be removed for copyright issues). A fraction of that queries were already associated to a valid species name because they were explicitly shared by their authors and collaboratively revised. We included in the test set the 4633 ones that were associated to a species belonging to the 1000 species of the training set (populating the known classes). Remaining pictures were distributed to a pool of botanists in charge of manually annotating them either with a valid species name or with newly created tags of their choice (and shared between them). In the period of time devoted to this process, they were able to manually annotate 1821 pictures that were included in the test set. Therefore, 144 new tags were created to qualify the unknown classes such as for instance non-plant objects, legs or hands, UVO (Unidentified Vegetal Object), artificial plants, cactaceae, mushrooms, animals, food, vegetables or more precise names of horticultural plants such as roses, geraniums, ficus, etc. For privacy reasons, we had to remove all images tagged as people (about 1.1% of the tagged queries). Finally, to complete the number of test images belonging to unknown classes, we randomly selected a set of 1546 image queries that were associated to a valid species name that do not belong to the Western European flora (and thus, that do not belong to the 1000 species of the training set or to potentially highly similar species). In the end, the test set was composed of 8,000 pictures, 4633 labeled with one of the 1000 known classes of the training set, and 3367 labeled as new unknown classes. Among the 4633 images of known species, 366 were tagged as invasive according to a selected list of 26 potentially invasive species. This list was defined by aggregating several sources (such as the National Botanical conservatory, and the Global Invasive Species Programme) and by computing the intersection with the 1000 species of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Based on the previously described testbed, we conducted a system-oriented evaluation involving different research groups who downloaded the data and ran their system. To avoid participants tuning their algorithms on the invasive species scenario and keep our evaluation generalizable to other ones, we did not provide the list of species to be detected. Participants only knew that the targeted species were included in a larger set of 1000 species for which we provided the training set. Participants were also aware that (i) most of the test data does not belong to the targeted list of species (ii) a large fraction of them does not belong to the training set of the 1000 species, and (iii) a fraction of them might not even be plants. In essence, the task to be addressed is related to what is sometimes called open-set or open-world recognition problems <ref type="bibr" coords="4,357.57,405.06,11.15,8.74" target="#b2">[3,</ref><ref type="bibr" coords="4,368.72,405.06,11.15,8.74" target="#b17">18]</ref>, i.e. problems in which the recognition system has to be robust to unknown and never seen categories.</p><p>Beyond the brute-force classification across the known classes of the training set, a big challenge is thus to automatically reject the false positive classification hits that are caused by the unknown classes (i.e. by the distractors). To measure this ability of the evaluated systems, each prediction had to be associated with a confidence score in p ∈ [0, 1] quantifying the probability that this prediction is true (independently from the other predictions).</p><p>Each participating group was allowed to submit up to 4 runs built from different methods. Semi-supervised, interactive or crowdsourced approaches were allowed but compared independently from fully automatic methods. Any human assistance in the processing of the test queries had therefore to be signaled in the submitted runs.</p><p>Participants to the challenge were allowed to use external training data at the condition that the experiment is entirely re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants and methods</head><p>94 research groups registered to LifeCLEF plant challenge 2016 and downloaded the dataset. Among this large raw audience, 8 research groups succeeded in submitting runs, i.e. files containing the predictions of the system(s) they ran. Details of the methods and systems used in the runs are further developed in the individual working notes of the participants (Bluefield <ref type="bibr" coords="5,368.72,440.92,9.96,8.74" target="#b8">[9]</ref>, Sabanci <ref type="bibr" coords="5,420.85,440.92,9.96,8.74" target="#b4">[5]</ref>, CMP <ref type="bibr" coords="5,462.32,440.92,14.61,8.74" target="#b19">[20]</ref>, LIIR, Floristic <ref type="bibr" coords="5,202.15,452.88,9.96,8.74" target="#b5">[6]</ref>, UM <ref type="bibr" coords="5,239.70,452.88,14.61,8.74" target="#b14">[15]</ref>, QUT <ref type="bibr" coords="5,288.05,452.88,14.61,8.74" target="#b15">[16]</ref>, BME <ref type="bibr" coords="5,336.95,452.88,10.30,8.74" target="#b1">[2]</ref>). Table <ref type="table" coords="5,385.85,452.88,4.98,8.74" target="#tab_1">1</ref> provides the results achieved by each run as well as a brief synthesis of the methods used in each of them. Complementary, the following paragraphs give a few more details about the methods and the overall strategy employed by each participant.</p><p>Bluefield system, Japan, 4 runs, <ref type="bibr" coords="5,306.94,512.63,11.15,8.77" target="#b8">[9]</ref>: A VGGNet <ref type="bibr" coords="5,379.74,512.66,15.50,8.74" target="#b18">[19]</ref> based system with the addition of Spatial Pyramid Pooling, Parametric ReLU and unknown class rejection based on the minimal prediction score of training data (Run 1). Run 2 is the same as run 1 but with a slightly different rejection making use of a validation set. Run 3 and 4 are respectively the same as Run 1 and 2 but the scores of the images belonging to the same observation were summed and normalised.</p><p>BME TMIT system, Hungary, 4 runs, <ref type="bibr" coords="5,331.88,596.31,11.14,8.77" target="#b1">[2]</ref>: This team attempted to combine three classification methods: (i) one based on dense SIFT features, fisher vectors and SVM (Run 2), (ii) the second one based on AlexNet CNN (Run 1) and (iii), the last one based on a SVM trained on the meta-data. Run 3 corresponds to the combination of three classifiers (using a weighted average) and Run 4 added two rejection mechanisms to Run3 (a distance-based rejection for the fisher vectors and the minimal prediction score of training data for the CNN).</p><p>CMP system, Czech Republic, 3 runs: This team built their system with the very deep residual CNN approach ResNet with 152 layers <ref type="bibr" coords="7,381.79,154.86,15.50,8.74" target="#b10">[11]</ref> which achieved the best results in both ILSVRC 2015 and COCO 2015 (Common Objects in Context) challenges last year. They added a fully-connected layer with 512 neurons on top of the network, right before softmax classifier with an maxout activation function <ref type="bibr" coords="7,173.23,202.68,9.96,8.74" target="#b7">[8]</ref>. They obtained thus a first run (run 2) by using all the 2016 training dataset while they used only the 2015 training dataset in run 3. Run 1 achieved the best performances by using a bagging approach of 3 ResNet-152: the training dataset was divided into three folds, and each CNN was using a different fold for validation and the remaining two folds for fine tuning.</p><p>Floristic system, France, 3 runs, <ref type="bibr" coords="7,310.69,274.38,11.15,8.77" target="#b5">[6]</ref>: This participant used a modified GoogleNet architecture by adding batch normalisation and ReLU activation function instead of the PReLU ones (run 1). In Run 2, adaptive thresholds (one for each class) based on the prediction of the training images in the fine tuned CNN were estimated for removing too low prediction on test images. Run 3 used a visual similarity search for scaling down the initial CNN prediction when a test image gives inhomogeneous knns according to the metadata (organ tags, GPS, genus and family levels).</p><p>LIIR KUL system, Belgium, 3 runs: This team used a ensemble classifier of 5 fine-tuned models: one CaffeNet, one VGGNet16 and 3 GoogLeNet. They added 12k external training data from Oxford flowers set, LeafSnap and trunk12 and attempted to exploit information in the metadata, mostly range maps from GPS coordinates comparing predictions with content tags. As a rejection criteria, they used a threshold on confidence of best prediction, one different threshold for each run (run 1: 0.25, run 2: 0.2, run 3: 0.15).</p><p>QUT system, Australia, 4 runs, <ref type="bibr" coords="7,302.57,477.62,16.47,8.77" target="#b15">[16]</ref>: This participant compared a standard CNN fine tuned approach based on GoogleNet (run 1) with a bagging approach "mixDCNN" (run 2) built on the top of 6 fine tuned GoogleNet on the 6 training subsets corresponding to the 6 distinct organs ("leaf" and "leafscan" training images are actually merged into one subset). Outputs are weighted by "occupation probabilities" which give for each CNN a confidence about their prediction. Run 3 merged the two approaches, run 4 too but with a threshold attempting to remove false positives.</p><p>Sabanci system, Turkey, 4 runs, <ref type="bibr" coords="7,305.93,585.22,11.15,8.77" target="#b4">[5]</ref>: This team used a CNN-based system with 2 main configurations. Run 1: an ensemble of GoogleLeNet <ref type="bibr" coords="7,445.08,597.20,15.50,8.74" target="#b20">[21]</ref> and VGGNet <ref type="bibr" coords="7,177.43,609.16,15.50,8.74" target="#b18">[19]</ref> fine-tuned on LifeCLEF 2015 data (for recognizing the targeted species), as well as a second GoogleNet fine-tuned on the binary rejection problem (using 70k images of PlantCLEF2016 training set for the known class label and 70K external images from the ILSCVR dataset for the unknown class label).</p><p>Run 2 is the same than Run 1 but without rejection. Run 3 is the same than Run 1 but with manual rejection of 90 obviously non plant images.</p><p>UM system, Malaysia &amp; UK, 4 runs: This team used a CNN system based on a VGGNet 16 layers. They modified the higher convolutional level in order to learn at the same time combinations of species and organs. VGGNet16 with dedicated and combined organ &amp; species layers with seven organ labels: branch, entire, flower, fruit, leaf, leafscan and stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>We report in Figure <ref type="figure" coords="8,226.67,278.96,4.98,8.74" target="#fig_0">1</ref> the scores achieved by the 29 collected runs for the two official evaluation metrics (mAP-open and mAP-open-invasive). To better assess the impact of the distractors (i.e. the images in the test set belonging to unknown classes), we also report the mAP obtained when removing them (and denoted as mAP-closed). As a first noticeable remark, the top-26 runs which performed the best were based on Convolutional Neural Networks (CNN). This definitely confirms the supremacy of deep learning approaches over previous methods, in particular the one bases on hand-crafted features (such as BME TMIT Run 2). The different CNN-based systems mainly differed in (i) the architecture of the used CNN, (ii) the way in which the rejection of the unknown classes was managed and (iii), various system design improvements such as classifier ensembles, bagging or observation-level pooling. An impressive mAP of 0.718 (for the targeted invasive species monitoring scenario) was achieved by the best system configuration of Bluefield (run 3). The gain achieved by this run is however more related to the use of the observation-level pooling (looking at Bluefield run 1 for comparison) than to a good rejection of the distractors. Comparing the metric mAP-open with mAP-closed, the figure actually shows that the presence of the unknown classes degrades the performance of all systems in a roughly similar way. This difficulty of rejecting the unknown classes is confirmed by the very low difference between the runs of the participants who experimented their system with or without rejection (e.g. Sabanci Run 1 vs. Run 2 or FlorisTic Run 1 vs. Run 2). On the other side, one can remark that all systems are quite robust to the presence of unknown classes since the drop in performance is not too high.</p><p>Actually, as all the used CNNs were pre-trained on a large generalist data set beforehand (ImageNet), it is likely that they have learned a diverse enough set of visual patterns to avoid underfiting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Complementary Analysis: Impact of the degree of novelty</head><p>Within the conducted evaluation, the proportion of unknown classes in the test set was still reasonable (actually only 42%) because of the procedure used to create it. In a real mobile search data stream, the proportion of images belonging to unknown classes could actually be much higher. To simulate such a higher degree of novelty, we progressively down sampled the test images belonging to known classes and recomputed the mAP-open evaluation metric. Results of this experiment are provided in Figure . For clarity, we only reported the curves of the best systems (for various degrees of novelty). As a first conclusion, the chart clearly shows that the degree of novelty in the test set has a strong influence on the performance of all systems. Even when 25% of the queries still belong to a known class, none of the evaluated systems reach a mean average precision greater than 0.45 (to be compared to 0.83 in a closed world). Some systems do however better resist to the novelty than others. The performance of the best run of Bluefield on the official test set does for instance quickly degrade with higher novelty rates (despite the use of a rejection strategy). Looking at the best run of Sabanci, one can see that the use of a supervised rejection class is the most beneficial strategy for moderate novelty rates but then the performance also degrades for high rates. Interestingly, the comparison of LIIR KUL Run1 and LIIR KUL Run3 show that simply using a higher rejection threshold applied to the CNN probabilities is more beneficial in the context of high unknown class rates. Thus, we believe there is still rooms of improvements in the design of adaptive rejection methods that would allow to automatically adapt the strength of the rejection to the degree of novelty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF 2016 plant identification challenge following the five previous ones conducted within CLEF evaluation forum. The main novelty compared to the previous year was that the identification task was evaluated as an open-set recognition problem, i.e. a problem in which the recognition system has to be robust to unknown and never seen categories. The main conclusion was that CNNs appeared to be naturally quite robust to the presence of unknown classes in the test set but that none of the rejection methods additionally employed by the participants improved that robustness. Also, the proportion of novelty in the test was still moderate. We therefore conducted additional experiments showing that the preformance of CNNs is strongly affected by higher rates of images belonging to unknown classes and that the problem is clearly still open. In the end, our study shows that there is still some room of improvement before being able to share automatically identified plant observations within international biodiversity platforms. The proportion of false positives would actually be too high for being acceptable for biologists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.77,331.33,345.83,7.89;9,134.77,342.29,345.82,7.89;9,134.77,353.24,345.82,7.89;9,134.77,364.20,345.82,7.89;9,134.77,375.19,308.80,7.86;9,141.88,115.84,328.53,200.72"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Scores achieved by all systems evaluated within the plant identification task of LifeCLEF 2016, mAP-open: mean Average Precision on the 1000 species of the training set and distractors in the test set, mAP-open-invasive: mean Average Precision with distractors but restricted to 26 invasive species only, mAP-closed: mean Average Precision on the 1000 species but without distractors in the test set</figDesc><graphic coords="9,141.88,115.84,328.53,200.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,134.77,320.98,345.83,7.89;10,134.77,331.97,138.70,7.86;10,141.88,115.84,328.54,200.34"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Impact of the degree of novelty: Mean Average Precision vs. proportion of test images belonging to known classes</figDesc><graphic coords="10,141.88,115.84,328.54,200.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,596.34,345.83,68.79"><head></head><label></label><figDesc>The metric used to evaluate the performance of the systems is the classification mean Average Precision, called hereinafter "mAP-open", considering each class c i of the training set as a query. More concretely, for each class c i , we extract from the run file all predictions with P redictedClassId = c</figDesc><table coords="4,134.77,596.34,345.83,68.79"><row><cell>4 Metric</cell></row><row><cell>producible, i.e. that the used exter-</cell></row><row><cell>nal resource is clearly referenced and accessible to any other research group in</cell></row><row><cell>the world, and, the additional resource does not contain any of the test obser-</cell></row><row><cell>vations. It was in particular strictly forbidden to crawl training data from the</cell></row><row><cell>following domain names:</cell></row><row><cell>http://ds.plantnet-project.org/</cell></row></table><note coords="5,408.21,256.05,72.37,9.65;5,134.77,268.00,345.82,8.74;5,134.77,279.96,345.83,8.74;5,134.77,291.91,345.83,8.74;5,134.77,303.87,345.83,8.74;5,134.77,315.82,345.83,8.74;5,134.77,327.78,345.83,8.74;5,134.77,339.74,144.13,8.74"><p>i , rank them by decreasing probability p ∈ [0, 1] and compute the Average Precision for that class. The mean is then computed across all classes. Distractors associated to high probability values (i.e. false alarms) are likely to highly degrade the mAP, it is thus crucial to try rejecting them. To evaluate more specifically the targeted usage scenario (i.e. invasive species), a secondary mAP ("mAP-open-invasive") was computed by considering as queries only a subset of the species that belong to a black list of invasive species.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,115.95,352.94,547.01"><head>Table 1 :</head><label>1</label><figDesc>Results of the LifeCLEF 2016 Plant Identification Task. Column "Keywords" &amp; "Rejection" attempt to give the main idea of the method used.</figDesc><table coords="6,136.60,147.10,351.11,515.86"><row><cell>Run</cell><cell>Key-words</cell><cell>Rejection</cell><cell>mAP-open</cell><cell>mAP-open-invasive</cell><cell>mAP-closed</cell></row><row><cell>Bluefield Run4</cell><cell>VGGNet, combine outputs from a same observation</cell><cell>thresholds by class (train+validation)</cell><cell cols="3">0.742 0.717 0.827</cell></row><row><cell>SabanciU GebzeTU Run1</cell><cell>2x(VGGNet,GoogleNet) training images tuned with resp. 70k, 115k</cell><cell>GoogleNet Plant/ImageNet 70k/70k</cell><cell cols="3">0.738 0.704 0.806</cell></row><row><cell cols="2">SabanciU...Run3 SabanciUGebzeTU Run1</cell><cell>Manually removed 90 test images</cell><cell cols="3">0.737 0.703 0.807</cell></row><row><cell>Bluefield Run3</cell><cell>Bluefield Run 4</cell><cell cols="4">thresholds by class 0.736 0.718 0.82</cell></row><row><cell cols="2">SabanciU...Run2 SabanciUGebzeTU Run1</cell><cell>-</cell><cell cols="3">0.736 0.683 0.807</cell></row><row><cell cols="2">SabanciU...Run4 SabanciUGebzeTU Run1</cell><cell>-</cell><cell cols="3">0.735 0.695 0.802</cell></row><row><cell>CMP Run1</cell><cell>Bagging of 3xResNet-152</cell><cell>-</cell><cell cols="3">0.71 0.653 0.79</cell></row><row><cell></cell><cell>CaffeNet, VGGNet16,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIIR KUL Run3</cell><cell>3xGoogleNet, adding 12k</cell><cell>threshold</cell><cell cols="3">0.703 0.674 0.761</cell></row><row><cell></cell><cell>external plant images</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIIR KUL Run2</cell><cell>LIIR KUL Run 3</cell><cell>threshold</cell><cell cols="3">0.692 0.667 0.744</cell></row><row><cell>LIIR KUL Run1</cell><cell>LIIR KUL Run 3</cell><cell>threshold</cell><cell cols="3">0.669 0.652 0.708</cell></row><row><cell>UM Run4</cell><cell>VGGNet16</cell><cell>-</cell><cell cols="3">0.669 0.598 0.742</cell></row><row><cell>CMP Run2</cell><cell>ResNet-152</cell><cell>-</cell><cell cols="3">0.644 0.564 0.729</cell></row><row><cell>CMP Run3</cell><cell>ResNet-152 (2015training)</cell><cell>-</cell><cell cols="3">0.639 0.59 0.723</cell></row><row><cell></cell><cell>1 "general" GoogleNet, 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QUT Run3</cell><cell>"organ" GoogleNets,</cell><cell>-</cell><cell cols="3">0.629 0.61 0.696</cell></row><row><cell></cell><cell>observation combination</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Floristic Run3</cell><cell>GoogleNet, metadata</cell><cell>-</cell><cell cols="3">0.627 0.533 0.693</cell></row><row><cell>UM Run1</cell><cell>VGGNet16</cell><cell>-</cell><cell cols="2">0.627 0.537</cell><cell>0.7</cell></row><row><cell>Floristic Run1</cell><cell>GoogleNet</cell><cell>-</cell><cell cols="3">0.619 0.541 0.694</cell></row><row><cell>Bluefield Run1</cell><cell>VGGNet</cell><cell cols="2">thresholds by class 0.611</cell><cell>0.6</cell><cell>0.692</cell></row><row><cell>Bluefield Run2</cell><cell>VGGNet</cell><cell cols="2">thresholds by class 0.611</cell><cell>0.6</cell><cell>0.693</cell></row><row><cell>Floristic Run2</cell><cell>GoogleNet</cell><cell cols="4">thresholds by class 0.611 0.538 0.681</cell></row><row><cell>QUT Run1</cell><cell>GoogleNet</cell><cell>-</cell><cell cols="3">0.601 0.563 0.672</cell></row><row><cell></cell><cell>VGGNet16 with dedicated</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UM Run3</cell><cell>and combined organ &amp;</cell><cell>-</cell><cell cols="3">0.589 0.509 0.652</cell></row><row><cell></cell><cell>species layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QUT Run2</cell><cell>6 "organ" GoogleNets, observation combination</cell><cell></cell><cell cols="3">0.564 0.562 0.641</cell></row><row><cell>UM Run2</cell><cell>VGGNet16 from scratch (without ImageNet2012)</cell><cell>-</cell><cell cols="3">0.481 0.446 0.552</cell></row><row><cell>QUT Run4</cell><cell>QUT Run3</cell><cell>threshold</cell><cell cols="3">0.367 0.359 0.378</cell></row><row><cell>BMETMITRun4</cell><cell>AlexNet &amp; BVWs &amp; metadata</cell><cell>-</cell><cell cols="3">0.174 0.144 0.213</cell></row><row><cell>BMETMITRun3</cell><cell>AlexNet &amp; BVWs &amp; metadata</cell><cell>threshold by classifier</cell><cell cols="3">0.17 0.125 0.197</cell></row><row><cell>BMETMITRun1</cell><cell>AlexNet</cell><cell>-</cell><cell cols="3">0.169 0.125 0.196</cell></row><row><cell cols="2">BMETMITRun2 BVWs (fisher vectors)</cell><cell>-</cell><cell cols="3">0.066 0.128 0.101</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,613.60,337.64,7.86;10,151.52,624.56,307.29,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,261.82,613.60,214.85,7.86">Morphological features for leaf based plant recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.60,624.56,150.70,7.86">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,634.88,337.64,7.86;10,151.52,645.84,329.07,7.86;10,151.52,656.80,154.32,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,346.59,634.88,134.01,7.86;10,151.52,645.84,274.78,7.86">Deep learning and svm classification for plant recognition in content-based large scale image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Márton</forename><surname>Bálint Pál Tóth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Tóth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Szúcs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.53,645.84,34.06,7.86;10,151.52,656.80,125.64,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,119.67,337.64,8.11;11,151.52,131.28,117.68,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,259.02,119.67,129.71,7.86">Towards open world recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.5687" />
	</analytic>
	<monogr>
		<title level="j" coord="11,396.46,119.67,24.83,7.86">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,141.60,337.63,7.86;11,151.52,152.56,329.07,7.86;11,151.52,163.52,127.48,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,363.59,141.60,117.01,7.86;11,151.52,152.56,168.19,7.86">A parametric active polygon for leaf segmentation and shape estimation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,339.50,152.56,141.09,7.86;11,151.52,163.52,42.85,7.86">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,174.48,337.63,7.86;11,151.52,185.44,329.07,7.86;11,151.52,196.40,70.42,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,325.41,174.48,155.17,7.86;11,151.52,185.44,189.85,7.86">Open-set plant identification using an ensemble of deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,362.72,185.44,117.87,7.86;11,151.52,196.40,41.76,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,207.37,337.63,7.86;11,151.52,218.33,266.76,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,283.33,207.37,197.26,7.86;11,151.52,218.33,54.31,7.86">Floristic participation at lifeclef 2016 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,226.83,218.33,162.78,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,229.29,337.63,7.86;11,151.52,240.25,329.07,7.86;11,151.52,251.21,329.07,7.86;11,151.52,262.17,25.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,338.98,240.25,141.61,7.86;11,151.52,251.21,123.68,7.86">Visual-based plant species identification from crowdsourced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joyeux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bathelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,296.90,251.21,127.59,7.86">ACM conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="813" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,273.14,337.63,7.86;11,151.52,284.10,147.75,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,449.36,273.14,31.23,7.86;11,151.52,284.10,35.56,7.86">Maxout Networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-02">Feb 2013</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,142.96,295.06,337.63,7.86;11,151.52,306.02,266.76,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,298.97,295.06,181.62,7.86;11,151.52,306.02,54.31,7.86">Bluefield (kde tut) at lifeclef 2016 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,226.83,306.02,162.78,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,316.99,337.97,7.86;11,151.52,327.95,329.07,7.86;11,151.52,338.91,194.25,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,351.53,316.99,129.06,7.86;11,151.52,327.95,124.04,7.86">Shape oriented feature selection for tomato plant identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hazra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,283.31,327.95,197.29,7.86;11,151.52,338.91,102.09,7.86">International Journal of Computer Applications Technology and Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,349.87,337.98,7.86;11,151.52,360.83,159.05,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,299.05,349.87,177.61,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.62,371.80,337.97,7.86;11,151.52,382.76,329.07,7.86;11,151.52,393.72,209.08,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,298.24,382.76,182.35,7.86;11,151.52,393.72,43.01,7.86">Interactive plant identification based on social image data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,201.70,393.72,89.28,7.86">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,404.68,337.98,7.86;11,151.52,415.64,261.89,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,304.35,404.68,176.24,7.86;11,151.52,415.64,62.35,7.86">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kebapci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,220.74,415.64,92.84,7.86">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,426.61,337.98,7.86;11,151.52,437.57,329.07,7.86;11,151.52,448.53,325.61,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,215.91,437.57,264.68,7.86;11,151.52,448.53,51.37,7.86">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.71,448.53,169.57,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,459.49,337.98,7.86;11,151.52,470.45,329.07,7.86;11,151.52,481.41,227.85,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,370.71,459.49,109.88,7.86;11,151.52,470.45,329.07,7.86;11,151.52,481.41,15.40,7.86">Plant identification system based on a convolutional neural network for the lifeclef 2016 plant classification task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,187.92,481.41,162.78,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,492.38,337.98,7.86;11,151.52,503.33,318.54,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,283.10,492.38,197.50,7.86;11,151.52,503.33,106.38,7.86">Feature learning via mixtures of dcnns for finegrained plant classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,278.61,503.33,162.78,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,514.30,337.97,7.86;11,151.52,525.26,329.07,7.86;11,151.52,536.22,131.11,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,319.82,514.30,160.77,7.86;11,151.52,525.26,281.47,7.86">Update on the environmental and economic costs associated with alien-invasive species in the united states</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zuniga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,440.02,525.26,40.57,7.86;11,151.52,536.22,40.50,7.86">Ecological economics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="288" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,547.19,337.97,7.86;11,151.52,558.14,329.07,7.86;11,151.52,569.10,25.60,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,306.64,547.19,170.02,7.86">Probability models for open set recognition</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,558.14,315.73,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,580.07,337.97,7.86;11,151.52,591.03,190.79,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="11,278.92,580.07,201.67,7.86;11,151.52,591.03,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,599.73,337.98,10.13;11,151.52,612.95,329.07,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,292.96,602.00,187.63,7.86;11,151.52,612.95,119.89,7.86">Very deep residual networks with maxout for plant identification in the wild</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Śulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,291.63,612.95,160.71,7.86">Working notes of CLEF 2016 conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,623.92,337.98,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,25.60,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,282.96,634.88,127.48,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,432.52,634.88,48.07,7.86;11,151.52,645.84,290.18,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,254.23,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,230.61,119.67,249.98,7.86;12,151.52,130.63,25.25,7.86">Assessing the risk of potentially invasive plant species in central europe</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,184.06,130.63,131.07,7.86">Journal for Nature Conservation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="179" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,141.59,337.97,7.86;12,151.52,152.55,215.10,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,267.65,141.59,212.94,7.86;12,151.52,152.55,29.44,7.86">Invasive alien plants in china: diversity and ecological insights</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,187.72,152.55,79.07,7.86">Biological Invasions</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1411" to="1429" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
