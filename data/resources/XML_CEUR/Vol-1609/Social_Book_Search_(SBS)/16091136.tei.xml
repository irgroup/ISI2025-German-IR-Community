<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.22,116.95,318.92,12.62">Word Embedding for Social Book Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.94,154.83,79.18,8.74"><forename type="first">Nawal</forename><surname>Ould-Amer</surname></persName>
							<email>nawal.ould-amer@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.67,154.83,75.41,8.74"><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
							<email>philippe.mulhem@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.64,154.83,59.93,8.74"><forename type="first">Mathias</forename><surname>Géry</surname></persName>
							<email>mathias.gery@univ-st-etienne.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">UMR 5516</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Laboratoire Hubert Curien</orgName>
								<address>
									<postCode>F-42000</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Université de Saint-Étienne</orgName>
								<address>
									<addrLine>Jean-Monnet</addrLine>
									<postCode>F-42000</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.50,154.83,29.92,8.74;1,278.60,166.79,53.69,8.74"><forename type="first">Karam</forename><surname>Abdulahhad</surname></persName>
							<email>karam.abdulahhad@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.22,116.95,318.92,12.62">Word Embedding for Social Book Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">28CBA7F1279D9FEAE8564A4B78628C68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>social network</term>
					<term>word embedding</term>
					<term>probabilistic retrieval</term>
					<term>profile selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the joint work of the Universities of Grenoble and Saint-Étienne at CLEF 2016 Social Book Search Suggestion Track. The approaches studied are based on personalization, considering the user's profile in the ranking process. The profile is filtered using Word Embedding, by proposing several ways to handle the generated relationships between terms. We find that tackling the problem of "non-topical" only queries is a great challenge in this case. The official results show that Word Embedding methods are able to improve results in the SBS case.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the joint participation of the MRIM group from the Laboratoire d'Informatique de Grenoble and the Laboratoire Hubert Curien de Saint-Étienne at CLEF 2016 Social Book Search (SBS) Suggestion Track. The goal of this track is to evaluate book retrieval approaches using the metadata of the book and user-generated content <ref type="bibr" coords="1,282.00,524.08,9.96,8.74" target="#b5">[6]</ref>. Considering our previous participation to this track in 2015 <ref type="bibr" coords="1,216.51,536.04,9.96,8.74" target="#b8">[9]</ref>, our finding was that taking into account the whole user profile is not adequate. Hence, we believe that a selection of an adequate set of terms related both to the user's profile and to his query could enhance the results. Such selection of terms is then matched against the documents, and fused with the query/document matching values. The work described here relies on Word Embedding (word2vec <ref type="bibr" coords="1,260.19,595.82,10.79,8.74" target="#b6">[7]</ref>) as a mean to help to select those terms.</p><p>Recent works investigate the use of Word Embedding for enhancing IR effectiveness <ref type="bibr" coords="1,171.99,619.93,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,184.17,619.93,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="1,193.58,619.93,7.75,8.74" target="#b7">8]</ref> or classification <ref type="bibr" coords="1,275.18,619.93,9.96,8.74" target="#b4">[5]</ref>. Word Embedding techniques seek to embed representations of words. For example, two vectors -→ t 0 and -→ t 1 , corresponding to the words t 0 and t 1 , are close in a space of N dimensions if they have similar contexts and vice-versa, i.e. if the contexts in turn have similar words <ref type="bibr" coords="1,436.72,657.11,9.96,8.74" target="#b3">[4]</ref>. In this vector space of embedded words, the cosine similarity measure is classically used to identify words occurring in similar contexts. In the work described here, we used this approach to select words from the user's profile that occur in the same context as the query terms. These selected words represent a user sub-profile according to the context of his query.</p><p>More precisely, we investigate here several ways to filter these terms and their impact on the quality of the results. In a way to determine the impact of such term filtering, we also study the use of non-personalized word embeddings for the query terms, i.e., without using the user's profile.</p><p>The rest of the paper is organized as follows: Section 2 presents the proposed approach and its four steps. The experiments and the official results are presented in Section 3. We conclude this work in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Global Process</head><p>The task described here aims to suggest books according to a user's query and to his profile. It consists of four steps:</p><p>1. Learning of the Word Embedding: this step describes the learning of a set of word embeddings using word2vec <ref type="bibr" coords="2,298.23,377.16,10.52,8.74" target="#b6">[7]</ref> and the features applied in the training process; 2. User Model: this step describes the way user profile is modeled; 3. Profile Query-Filtering: this step describes the building process for the user sub-profile based on his query; 4. Ranking Model: this step depicts the ranking model using a Profile Query-Filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Notations</head><p>We present now the notations used in this paper:</p><p>u ∈ U : a user u from the whole set of users U ; q = {t 1 , t 2 , ..., t m }: the original query of a user u.</p><p>q f = {t 1 , t 2 , ..., t o }: the filtered query of a user u.</p><p>-P (u) = {t 1 : w 1 , t 2 : w 2 , ..., t n : w n }: the profile of a user u, composed of couples &lt;term : weight&gt;. -T erms(u) = {t 1 , t 2 , ..., t n }: a set of user terms belonging to the user profile P (u). -W ordEmbedding w2v (q): a set of word embeddings (i.e. output of the section 2.3). -P F iltered (q, u): a sub-profile of the user u according to the user query q.</p><p>-P W eighted f iltered (q, u): a weighted sub-profile of the user u according to the user query q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning of Word Embeddings</head><p>Word Embedding is the generic name of a set of NLP-related learning techniques to map words to low-dimensional (w.r.t. vocabulary size) vectors of real numbers. Word Embedding <ref type="bibr" coords="3,259.03,165.74,9.96,8.74" target="#b6">[7]</ref>, which falls in the same category as Latent Semantic Analysis (LSA) <ref type="bibr" coords="3,241.20,177.69,9.96,8.74" target="#b1">[2]</ref>, enables us to deal with a richer representation of words, namely words are represented as vectors of more elementary components or features. The similarity between vectors is supposed to be a good indicator to the 'closeness' between the respective words <ref type="bibr" coords="3,342.84,213.56,14.61,8.74" target="#b10">[11]</ref>. In addition, the arithmetic operations between vectors reflect a type of semantic-composition, e.g. bass + guitar = bass guitar <ref type="bibr" coords="3,225.98,237.47,14.61,8.74" target="#b10">[11]</ref>.</p><p>In this section, we describe the process of learning word-vectors. To this end, we train word2vec <ref type="bibr" coords="3,218.81,261.76,10.52,8.74" target="#b6">[7]</ref> on the Social Book Search corpus. Word2vec represents each word w of the training set as a vector of features, where this vector is supposed to capture the contexts in which w appears. Therefore, we chose the Social Book Search corpus to be the training set, where the training set is expected to be consistent with the data on which we want to test.</p><p>To construct the training set from the Social Book Search corpus, we simply concatenate the contents of all documents in one document, without any particular pre-processing such as stemming or stop-words removal, except some text normalization and cleaning operations such as lower-case normalization, removing HTML tags, etc. The obtained document is the input training set of word2vec. The size of the training set is ∼ 12.5GB, where it contains 2.3 billions words, and the vocabulary size is about 600K. The training parameters of word2vec are set as follows:</p><p>we used the continuous bag of words model instead of the skip-gram (word2vec options: cbow=1); the output vectors size or the number of features of resulting word-vectors is set to 500 (word2vec options: size=500); the width of the word-context window is set to 8 (word2vec options: win-dow=8); the number of negative samples is set to 25 (word2vec options: negative=25).</p><p>For example, based on the output of word2vec, the 10 most similar (using cosine similarity) words of "novel " in the Social Book Search corpus are: story, novella, thriller, tale, storyline, masterpiece, plot, debut, engrossing, and genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">User Modeling</head><p>We model each user u by a set of terms that describe his interests. These terms are the tags that a user u used to describe or annotate the documents (books present in his catalogue). Each user u is represented by his profile P (u) as a vector over the set of tags as follow:</p><formula xml:id="formula_0" coords="3,231.74,657.11,248.86,9.65">P (u) = {t 1 : w 1 , t 2 : w 2 , ..., t n : w n }<label>(1)</label></formula><p>Where t i is a term and w i a weight of this term, corresponding to his importance in the user profile. This weight w i is the tf of t i : the number of times the user u used this tag to annotate books.</p><p>2.5 Profile Query-Filtering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Term Filtering</head><p>As stated before, Word Embedding can potentially be helpful in the selection of terms related to the query. Despite the effectiveness of Word Embedding to select embedded terms, it could happen that they decrease the effectiveness of the system. Usually, embedded terms are used as extensions of a term, as query expansion. In fact, if an extended term is a noisy term (because of its ambiguity for instance, or because it is not a topical term of the query), then the set of its resulting word embeddings will increase the noise in the extended query. For example, in the queries (taken from the topics of Social Book Search 2016) "Help! i Need more books", "Can you recommend a good thesaurus?", "New releases from authors that literally make you swoon..." or "Favorite Christmas Books to read to young children", the majority of words are not useful for expansion, like "new, good, recommend, make, you ...". Therefore, we need to filter the words of the queries before the expansion process. We chose to remove from the words to be expanded all the words that are adjectives. To do that, we use an English stop-adjective list in addition to the standard English stop-word. Then, this process generates, from initial query q, a filtered query q f = {t 1 , t 2 , ..., t o } that is expected to contain more relevant topical elements as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Word Embedding Selection</head><p>From each of the terms in the filtered query q f obtained by the previous step 2.5.1, we then select their set of word embeddings. For each term as an input, we obtain the top-k most related terms as an output. The metric used is the cosine similarity between the query term and all words in the training corpus. From these top-k terms, we do not want to over-emphasize variations of the query term. So, we filter out from the top-k terms the ones that have the same stem as the initial query term. Such stems are generated using the well know Porter stemmer for English.</p><p>The expanded query generated as output of this step is: </p><formula xml:id="formula_1" coords="4,186.40,574.33,159.45,45.23">W ordEmbedding w2v (q f ) =        emt 11 ,</formula><formula xml:id="formula_2" coords="4,420.10,574.33,60.49,45.23">      <label>(2)</label></formula><p>Where W ordEmbedding w2v (q f ) denotes the function that returns a set of word embedding for a given filtered query q f , and em t ij denotes the j th element of the top-k word embeddings of t i .</p><p>Table <ref type="table" coords="7,281.84,116.91,4.13,7.89">1</ref>. Runs Parameters Run α Score(q f , d) Score(u, d) β γ λ δ profile RUN1 1.0 0.6 0.2 0.2 0.0 -RUN2 1.0 0.6 0.2 0.0 0.2 -RUN3 0.7 0.6 0.2 0.2 0.0 filtered RUN4 0.7 0.6 0.2 0.2 0.0 filtered RUN5 0.7 0.6 0.2 0.2 0.0 filtered, weighted RUN6 0.7 0.6 0.2 0.2 0.0 filtered, weighted</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Official Results</head><p>According to table 2, we see that all of our results are quite close to each other. Even the weighted profiles compared to unweighted ones does not change results: runs RU N 6 and RU N 3 on one side and runs RU N 4 and RU N 5 lead to exactly the same result lists, and therefore the same evaluation results. This may be explained by the fact that the top-10 terms are usually very close (according to the cosine similarity) to the query terms, leading to weights close to 1.0. This has to be studied more carefully in the future. We remark that our two best runs (RU N 2 and RU N 6) include word embeddings. Both of them use Word Embedding applied on Language Models. It seems so that the integration of word embeddings behaves a bit better on language models than on BM25. Our best run, RU N 2, is not personalized. This shows that using word embeddings in the context of personalization must be further studied. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unofficial Results</head><p>We depict here a strange behavior that we faced after the release of the official relevance judgments: the reranking (described in Section 3.1) that removes the documents that belong to the user catalogue does lower the evaluations results. Table <ref type="table" coords="7,162.83,621.25,4.98,8.74" target="#tab_2">3</ref> presents the N DCG@10 values with and without reranking, with the relative improvement. This is explained by the fact that we chose to remove all the ISBN documents that correspond to the documents from the user catalogue, and not only the documents that correspond to the LT id of the books. This approach increased the quality of the results in 2015, but is clearly not adequate for this year. This may come from a different removal, in the relevance judgments, of the documents in the user's catalogue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented in this paper a joint work between the universities of Grenoble and Saint-Étienne. Our focus was to study the integration of word embeddings for the Social Book Suggestion task. We find that the nature of the queries pose a great challenge to an effective use of word embeddings in this context. Weighting the expansion terms does not lead to better results. Our future works may help to understand this behavior. One last point is related to the removal of the documents that belong to the user's catalogue: removing all the ISBN ids and not only the LT ids from the results, assuming that is a user already bought one edition of a book he does not want another edition, is clearly not the right choice, as such removal decreased substantially our official results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,317.65,576.30,101.06,45.52"><head></head><label></label><figDesc>emt 12 , ..., emt 1k , emt 21 , emt 22 , ..., emt 2k , ... emt o1 , emt o2 , ..., emt ok</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,213.64,429.33,188.07,93.07"><head>Table 2 .</head><label>2</label><figDesc>Official Results</figDesc><table coords="7,213.64,448.39,188.07,74.02"><row><cell>Rank Run NDCG@10 MRR MAP R@1000</cell></row><row><cell>18 RUN2 0.0889 0.1889 0.0518 0.3491</cell></row><row><cell>19 RUN6 0.0872 0.1914 0.0538 0.3652</cell></row><row><cell>20 RUN3 0.0872 0.1914 0.0538 0.3652</cell></row><row><cell>21 RUN1 0.0864 0.1858 0.0529 0.3654</cell></row><row><cell>22 RUN5 0.0861 0.1866 0.0525 0.3652</cell></row><row><cell>23 RUN4 0.0861 0.1866 0.0524 0.3652</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,196.72,173.80,221.92,93.07"><head>Table 3 .</head><label>3</label><figDesc>Unofficial Results</figDesc><table coords="8,196.72,192.85,221.92,74.02"><row><cell cols="3">Run NDCG@10 official NDCG@10 non reranked (%)</cell></row><row><cell>RUN1</cell><cell>0.0864</cell><cell>0.1073 (+24.2%)</cell></row><row><cell>RUN2</cell><cell>0.0889</cell><cell>0.1092 (+22.8%)</cell></row><row><cell>RUN3</cell><cell>0.0872</cell><cell>0.1073 (+23.1%)</cell></row><row><cell>RUN4</cell><cell>0.0861</cell><cell>0.1063 (+23.5%)</cell></row><row><cell>RUN5</cell><cell>0.0861</cell><cell>0.1063 (+23.(%)</cell></row><row><cell>RUN6</cell><cell>0.0872</cell><cell>0.1073 (+23.1%)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Generation of user's sub-profile</head><p>The goal of our proposal is to construct a sub-profile using a set of word embeddings based on a user query. Our approach here is to apply a naive method based on the Intersection between a) the query word embeddings set and b) the user profile, as follows:</p><p>We propose two variations of such a user query sub-profile:</p><p>-Non-Weighted Sub-Profile (P F iltered ): this user sub-profile is represented by only the terms without weights, then all terms in the query have a same weight (i.e. corresponds to P F iltered (q f , u) defined in Eq. 3). -Weighted Sub-Profile (P W eighted f iltered ): for this user sub-profile, the weight for each term in the profile is the cosine between of the embeddings of the filtered term and its respective query term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Ranking</head><p>In this part, we present the ranking used to suggest books to the user according to his query and to his profile. We take into account two elements: if the document is related to the user query, and if the document is related to the user profile, as follows:</p><p>Where Score(q f , d) is the matching score between the filtered query q f and the document d, Score(u, d) is the matching score between the user u and the document d, and α ∈ [0, 1] determines the relative importance of each part of formula.</p><p>The Score(q f , d) is a fusion of four parts:</p><p>+ λ Score BM 25 (W ordEmbedding w2v (q f ), d)</p><p>Where Score BM 25 (q f , d) (resp. Score LM (q f , d)) denotes a matching using the classical BM25 model (resp. language model), and β, γ, λ and δ are parameters that determine the importance of each part of the document overall score (β + γ + λ + δ = 1).</p><p>The function Score(u, d) in equation ( <ref type="formula" coords="5,315.78,633.20,4.24,8.74">4</ref>) is computed as a classical matching function (BM25 or Language Model) between the document d and the filtered user's profile (weighted: P W eighted f iltered , or not weighted: P F iltered ) of u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameters and post-processing</head><p>For all the runs described below, for the BM25 runs the parameters are the defaults of Terrier <ref type="bibr" coords="6,216.60,184.08,14.61,8.74" target="#b9">[10]</ref>, except for one parameter b = 0.5, according to previous experiments on SBS 2015. For the Language Models with Dirichlet smoothing, we use the default Terrier parameter µ = 2500.</p><p>For the re-ranking post process, we used the same protocol used in <ref type="bibr" coords="6,446.69,220.77,9.96,8.74" target="#b8">[9]</ref>. The documents that belong to the user's catalogue are removed for the result. This process is applied for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted runs</head><p>We submitted the 6 following runs:</p><p>1. RUN1: This run is the non-personalized approach (α = 1 in the Eq. 4). The Score(q f , d) is computed using Eq. 5 with the following parameters: β = 0.6, γ = 0.2, λ = 0.2, δ = 0.0. 2. RUN2: This run is also a non-personalized approach (α = 1 in the Eq. 4), very similar to the run RUN1. The Score(q f , d) is computed using Eq. 5 with the following parameters: β = 0.6, γ = 0.2, λ = 0, δ = 0.2. 3. RUN3: This run is a personalized approach where the user u is represented by his filtered profile P F iltered in the Score(u, d) function (Eq.4). The Score(q f , d) represents the score of d according to the RUN1 above. The score Score(u, d) of equation ( <ref type="formula" coords="6,285.92,438.11,4.24,8.74">4</ref>) is equal to Score BM 25 (P F iltered (q f , u), d).</p><p>The parameter α is equal to 0.7. 4. RUN4: This run is a personalized approach where the user is represented by his filtered profile P F iltered in the Score(u, d) function (Eq.4). The Score(q f , d) represents the score of d according to the RUN1 above. The score Score(u, d) of equation ( <ref type="formula" coords="6,208.03,498.71,4.24,8.74">4</ref>) is equal to Score LM (P F iltered (q f , u), d). The parameter α is equal to 0.7. 5. RUN5: This run is a personalized approach where the user is represented by his weighted profile P W eighted f iltered in the Score(u, d) function (Eq.4). The Score(q f , d) represents the score of d according to the RUN1 above. The score Score(u, d) of equation ( <ref type="formula" coords="6,256.28,559.32,4.24,8.74">4</ref>) is equal to Score BM 25 (P W eighted F iltered (q f , u), d).</p><p>The parameter α is equal to 0.7. 6. RUN6: This run is a personalized approach where the user is represented by his weighted profile P W eighted f iltered in the Score(u, d) function (Eq.4). The Score(q f , d) represents the score of d according to the RUN1 above. The score Score(u, d) of equation ( <ref type="formula" coords="6,259.05,619.92,4.24,8.74">4</ref>) is equal to Score LM (P W eighted F iltered (q f , u), d).</p><p>The parameter α is equal to 0.7.</p><p>All these runs parameters are described in table 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,486.44,337.64,7.86;8,151.52,497.40,329.07,7.86;8,151.52,508.36,232.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,307.45,486.44,173.15,7.86;8,151.52,497.40,268.40,7.86">A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Almasri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berrut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.79,497.40,38.80,7.86;8,151.52,508.36,107.46,7.86;8,320.07,508.36,34.80,7.86">European Conference on IR Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
	<note>ECIR&apos;16</note>
</biblStruct>

<biblStruct coords="8,142.96,518.52,337.63,7.86;8,151.52,529.48,329.07,7.86;8,151.52,540.44,119.80,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,455.00,518.52,25.59,7.86;8,151.52,529.48,124.48,7.86">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,283.21,529.48,197.38,7.86;8,151.52,540.44,29.19,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,550.60,337.63,7.86;8,151.52,561.56,329.07,7.86;8,151.52,572.52,326.51,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,338.33,550.60,142.26,7.86;8,151.52,561.56,163.51,7.86">Word embedding based generalized language model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,336.54,561.56,144.06,7.86;8,151.52,572.52,129.41,7.86;8,341.63,572.52,35.66,7.86">Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="795" to="798" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;15</note>
</biblStruct>

<biblStruct coords="8,142.96,582.68,337.63,7.86;8,151.52,593.64,258.42,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,253.79,582.68,226.80,7.86;8,151.52,593.64,136.67,7.86">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno>CoRR abs/1402.3722</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,603.80,337.63,7.86;8,151.52,614.76,86.06,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,197.88,603.80,247.15,7.86">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno>CoRR abs/1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,624.92,337.63,7.86;8,151.52,635.88,329.07,7.86;8,151.52,646.84,329.07,7.86;8,151.52,657.79,55.45,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,255.36,635.88,207.48,7.86">Overview of the CLEF 2015 Social Book Search lab</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gäde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Huurdeman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Toms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.52,646.84,186.02,7.86;8,400.01,646.84,34.16,7.86">Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="545" to="564" />
		</imprint>
	</monogr>
	<note>CLEF&apos;15</note>
</biblStruct>

<biblStruct coords="9,142.96,120.67,337.63,7.86;9,151.52,131.63,222.94,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,341.33,120.67,139.26,7.86;9,151.52,131.63,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,142.59,337.64,7.86;9,151.52,153.55,329.07,7.86;9,151.52,164.51,260.62,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,362.88,142.59,117.71,7.86;9,151.52,153.55,109.54,7.86">Improving document ranking with dual word embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.78,153.55,176.80,7.86;9,180.20,164.51,87.03,7.86">Conference Companion on World Wide Web</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="83" to="84" />
		</imprint>
	</monogr>
	<note>WWW&apos;16 Companion</note>
</biblStruct>

<biblStruct coords="9,142.96,175.46,337.64,7.86;9,151.52,186.42,283.65,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,306.66,175.46,109.48,7.86">LIG at CLEF 2015 SBS Lab</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ould-Amer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Géry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,436.28,175.46,44.32,7.86;9,151.52,186.42,178.98,7.86">Conference and Labs of the Evaluation Forum. CLEF&apos;15</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,197.38,337.97,7.86;9,151.52,208.34,329.07,7.86;9,151.52,219.30,321.13,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,440.94,197.38,39.64,7.86;9,151.52,208.34,254.24,7.86">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.41,208.34,53.19,7.86;9,151.52,219.30,179.27,7.86">Workshop on Open Source Information Retrieval. OSIR&apos;06</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,230.26,337.98,7.86;9,151.52,241.22,96.14,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,212.31,230.26,96.25,7.86">Geometry and Meaning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.78,230.26,162.81,7.86;9,151.52,241.22,67.47,7.86">Center for the Study of Language and Information/SRI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
