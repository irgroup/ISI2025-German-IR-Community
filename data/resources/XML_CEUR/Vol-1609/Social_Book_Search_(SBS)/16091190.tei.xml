<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.62,115.96,320.13,12.62;1,262.76,133.89,95.22,12.62">KNOW At The Social Book Search Lab 2016 Mining Track</title>
				<funder ref="#_GyE2hCE">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Research Promotion Agency FFG</orgName>
				</funder>
				<funder ref="#_Mwtav8t">
					<orgName type="full">Austrian Federal Ministry of Transport, Innovation and Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">State of Styria</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Federal Ministry of Economy, Family and Youth</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.58,171.96,62.57,8.74"><forename type="first">Hermann</forename><surname>Ziak</surname></persName>
							<email>hziak@know-center.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Know-Center GmbH</orgName>
								<address>
									<addrLine>Inffeldgasse 13</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.85,171.96,52.16,8.74"><forename type="first">Andi</forename><surname>Rexha</surname></persName>
							<email>arexha@know-center.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Know-Center GmbH</orgName>
								<address>
									<addrLine>Inffeldgasse 13</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.71,171.96,56.06,8.74"><forename type="first">Roman</forename><surname>Kern</surname></persName>
							<email>rkern@know-center.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Know-Center GmbH</orgName>
								<address>
									<addrLine>Inffeldgasse 13</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.62,115.96,320.13,12.62;1,262.76,133.89,95.22,12.62">KNOW At The Social Book Search Lab 2016 Mining Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47EAF034E7547FD7ABF909FF11809158</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text Mining, Classification,</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our system for the mining task of the Social Book Search Lab in 2016. The track consisted of two task, the classification of book request postings and the task of linking book identifiers with references mentioned within the text. For the classification task we used text mining features like n-grams and vocabulary size, but also included advanced features like average spelling errors found within the text. Here two datasets were provided by the organizers for this task which were evaluated separately. The second task, the linking of book titles to a work identifier, was addressed by an approach based on lookup tables. For the dataset of the first task our approach was ranked third, following two baseline approaches of the organizers with an accuracy of 91 percent. For the second dataset we achieved second place with an accuracy of 82 percent. Our approach secured the first place with an F-score of 33.50 for the second task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Social Book Search Lab on the CLEF 2016 conference consisted of three tracks: suggestion, mining and interactive track. Within this work we describe our approach on the mining track. The tracks cover challenges that relate to the field of Just in Time Information Retrieval <ref type="bibr" coords="1,344.59,541.30,10.52,8.74" target="#b5">[6]</ref> which is also closely related to the field of recommender systems. In particular this includes challenges like automated query formulation, document ranking, and relevant context identification. The mining track is most relevant for the last of these challenges. The task itself was organised via two tasks. Within the classification task a dataset consisting of postings from LibraryThing 1 and Reddit 2 were given. Here the task was to identify the postings that contained requests to book recommendations. The LibraryThing postings were therefore labelled to be either a request or a normal thread posting. The Reddit threads were selected from two Subreddits: "suggestmeabook" and "books".</p><p>The second task was linking in the reverse direction, thus linking books with postings. Here the goal was to identify a reference to a book within a thread. The threads were again taken from LibraryThing containing the about 200 initial postings with about five to fifty replies each. The task was not about highlighting the exact title and location within the text but stating the according work ID. For the classification task we applied traditional text mining feature engineering methods, like stemming and according feature extraction. We submitted different runs, which represent different classification algorithms. The first three runs were conducted using well known machine learning algorithms. The results submitted as fourth run was based on the idea of a Vote/Veto ensemble classifier <ref type="bibr" coords="2,449.09,250.50,9.96,8.74" target="#b3">[4]</ref>. For the linking task we followed an approach based on a lookup table. Here we made use of the provided Amazon and LibraryThing book dataset <ref type="bibr" coords="2,397.26,274.41,9.96,8.74" target="#b0">[1]</ref>. We managed to be ranked on the third and second place on the LibraryThing and Reddit dataset for the classification task and to placed on the first place for the linking task. This is particularly encouraging as we did not conduct extensive optimisation upon the basic algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The base of the two tracks, "Mining" and "Suggestion" tracks, are the provided book data collections from Amazon and LibraryThing, with about 2.7 million books and according meta-data. We decided to transform the given structured data into a data structure, which should be quicker to access, thus making use of an indexed format. Consequently the dataset was parsed and indexed with Apache Solr<ref type="foot" coords="2,187.37,434.23,3.97,6.12" target="#foot_0">3</ref> , which is based on the Apache Lucene<ref type="foot" coords="2,361.18,434.23,3.97,6.12" target="#foot_1">4</ref> search-engine library.</p><p>The "Mining Track" of the SBS challenge consisted of two task: The "Classification Task" with the goal of classifying forum entries as book recommendation requests and the "Linking Task" where the task was to identify books within the text and report the according LiberyThing internal book ID. Within both approaches we used our Solr search index containing the Amazon book dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification Task</head><p>For the classification task two datasets were provided by the organizers. The Reddit training set containing about 250 threads from the "suggestmeabook" and threads from the "books" Subreddit. The "suggestmeabook" threads were the positive examples and the "books" threads were considered to be the negative examples. A similar but smaller testing set was provided as well were the category field were masked.</p><p>The second, more comprehensive dataset was extracted from LibararyThing itself. Here 2,000 labelled threads were provided for training and another 2,000 threads for testing. About 10 percent of the training threads were labelled as positive examples. Initially we started by parsing both datasets and uniting the given data within one data structure. We shuffled the entries within this data structure and split it two separate sets: the first part were used for training and the second part for validation, whereas the validation part was only a small fraction of the whole set.</p><p>The only preprocessing step, which we applied on the dataset, was stop word removal. To train the classifiers we extracted several types of features. The first types features are found in many text mining and natural language processing system, like n-grams. Although it is common to use TF/IDF based weighting scheme, for reasons of simplicity we decided to just use the sheer frequency of features within the text. Based on these basic features, we introduced a number of other features.</p><p>The first of the custom features are the number of terms within the text. Next we extracted the tags and browse nodes from the Amazon Dataset. The found tags and browse nodes within the user's text were higher than the basic features. Finally we extracted a feature based on the count of average spelling errors within the posting. We decided to introduce this feature based on our assumption that user asking for book recommendations might be more literate than the average user and therefore might as well make fewer spelling errors. This feature has the additional benefit that postings not containing decent text at all would be penalized further. Some of the classification algorithms we initially intended to use could not cope with missing features within the dataset. Therefore all missing features had to be added to the each single feature vector with zero weight.</p><p>For the very first test run we used only a single, dedicated feature: The quantity of question marks with in the entry. To our surprise with this simple feature the Naive Bayes approach already reached an accuracy of over 80 percent. Since we considered this to be an error at our end, we investigated this issue more closely. The final conclusion was that the imbalance of positive and negative examples has led to this result. Therefore we further separated the validation data into the positive and negative examples to get a more detailed information about the performance of the approach and features. We also created a more balanced training set by keeping all positives examples but using only a fraction of the negative examples for some of the classification algorithms.</p><p>With our feature extraction pipeline and the individually balanced training sets we could finally train the three chosen classification algorithms: A Random Forest classifier <ref type="bibr" coords="3,205.98,572.43,9.96,8.74" target="#b1">[2]</ref>, a Naive Bayes classifier <ref type="bibr" coords="3,330.06,572.43,10.52,8.74" target="#b2">[3]</ref> and finally a Decision Tree <ref type="bibr" coords="3,467.31,572.43,9.96,8.74" target="#b4">[5]</ref>. The parameters like maximum depth of the Random Forest classifier or amount of negative postings within the training set of the Naive Bayes approach where chosen by manually optimizing on the accuracy on our custom validation dataset. For example, we obtained the best results for the Random Forest classifier by sticking with the default of 10 as a target tree depth. Additionally we also worked an approach that was based on the idea of a Vote/Veto ensemble classifier, where we implemented a dedicated voting schema. Only if the majority of the algorithms decided that the posting contained a book recommendation request the posting was labelled as such.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linking Task</head><p>As basis for the linking task a dataset extracted from the LibraryThing website was provided by the lab organizers. This dataset contained of about 500 threads from users discussing about books while often mentioning book titles. Furthermore those threads included the replies to the initial posting and also contained potential candidates.</p><p>Our initial approach to tackle this task was to implement a lookup table. To generate our initial lookup table we extracted all titles and selected parts of the metadata (e.g. authors, creator, International Standard Book Number (ISBN)) from the Amazon dataset. To reduce the size and clean the data we conducted a number of preprocessing steps. We removed English stop words, removed or replaced special characters, removed additional information about the book provided within the title (e.g. binding information) and stemmed the title terms. The same preprocessing steps were applied upon the text of the posting entries.</p><p>Finally we implemented a lookup algorithm to match the potential candidates ISBN to the LibraryThing work IDs which had to be reported. Basically all the preprocessed book titles from the Amazon dataset were used for a simple string matching algorithm on each sentence in the posting.</p><p>The biggest issue with this kind of approach is the high amount of false positives, i.e. matches, which do not refer to any books. Most of in the following described approaches we tried were not included in the final results. Nevertheless we briefly describe our strategies how to resolve this problem. To reduce the amount of false candidates one strategy is to introduce a weight to all candidates and then remove all those, which falling below a certain threshold. As potential factors for such weighting scheme we considered the occurrence of the author's name within the same sentence as the corresponding book title. Often this cooccurrences of the author's name within the same sentences are either stated directly ahead of the book title (e.g. Stephen King's The Dark Tower) or directly following the title (e.g. The Dark Tower by Stephen King).</p><p>Furthermore we experimented with a supervised approach, to train a classifier to distinguish between sentences containing books and those, which do not mention books. The basic idea was to lower the weight for the book candidate if the book titles were found in sentences potentially not containing a book. This was made possible as parts of the dataset consisted of texts were the titles of the book were annotated. We extracted each of this sentences and applied the same feature extraction pipeline than in the classification task. Although both of the approaches may appear valid, we decided against using them, because of these reasons: First of all the classifier did not work on a satisfying accuracy level, with only about 60 to 65 percent on average. Secondly, even though the co-occurrence of an author's name within the text might validate the book title candidate, it might not necessarily mean that the other candidates are less likely correct. And finally it is hard to estimate the amount of actual titles within the text, i.e. it is hard to find an appropriate threshold for the weights. Finally, to reduce the false positives at least to a certain degree we decided to just remove book titles from the dataset that consisted only of one non stop word term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section we describe the results of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Task</head><p>In Table <ref type="table" coords="5,174.26,264.23,4.98,8.74" target="#tab_0">1</ref> we present the results of our approach on the classification task with the validation dataset created out of the original training dataset. The figures represent the accuracy of each approach. Here the Random Forest approach and the Naive Bayes classifier performed on the same level. The Vote/Veto ensemble classifier inspired algorithm achieved slightly lower results, about one percent, whereas the Decision Tree achieved the lowest results with about eight percent lower than the top algorithms. Table <ref type="table" coords="5,177.60,452.00,4.98,8.74" target="#tab_1">2</ref> shows the result of the official test run where the two datasets are evaluated separately. Here on the LibraryThing dataset the Naive Bayes classifier has the best accuracy, ranking on place three, followed by the Vote/Veto classifier ranking on place six. Table <ref type="table" coords="5,176.68,600.92,4.98,8.74" target="#tab_2">3</ref> shows parts of the official results stated on the SBS Lab website <ref type="foot" coords="5,462.99,599.35,3.97,6.12" target="#foot_2">5</ref> . It contains a comparison of our top performing approach, based on Naive Bayes, versus the top performing baseline approach based on a Linear Support Vector Classifier. The third results originate from the baseline based on Naive Bayes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linking Task</head><p>Table <ref type="table" coords="6,161.84,235.62,4.98,8.74" target="#tab_3">4</ref> presents the figures of the linking task. Here our system performed with an accuracy and recall of 41.14 and a precision of 28.26 resulting in the F-score of 33.50 and was ranked on the first place. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Given that the Naive Bayes approach is of low complexity compared to the best performing system, the baseline with an Linear Support Vector Classifier, it appears that our selected features worked well. This is especially apparent, when comparing our Naive Bayes approach with the provided baseline, see Table <ref type="table" coords="6,472.84,432.79,3.87,8.74" target="#tab_2">3</ref>. Within the official run both the Decision Tree and the Random Forest approach fared behind the others. Interestingly within our preliminary tests upon our own validation set, the Random Forest based approach achieved nearly the best results. This could be based on the fact that we did not apply any further optimization, like pruning on the tree based algorithms. Given the simplicity of our approach for the linking task it seemed to work, especially well in regards to the recall. As expected the precision is low in comparison. The datasets and results indicate that users tend to be quite accurate when it comes to stating book titles within written text. A bigger issue, than to identify the titles itself, seems to be the identification of false positives within the candidate list. Many book titles have the tendency to be short or use phrases that occur often within natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Given we trained only one set of classifiers for both datasets it seems that our approach generalizes well. For future work we want to investigate the performance of our selected feature set by applying different classification algorithms.</p><p>We expect the linking task to allow the most room for further improvement. In particular, we plan to rise the precision of the approach. Investing in a novel approach to detect sentences containing books, might be associated with the biggest gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,367.83,345.83,56.47"><head>Table 1 .</head><label>1</label><figDesc>Results of the first run with only a small validation dataset created out of the training data. The results represent the accuracy on the combined datasets of "LibraryThing" and "Reddit".</figDesc><table coords="5,139.11,409.79,307.14,14.50"><row><cell></cell><cell>Naive Bayes</cell><cell>Decision Tree</cell><cell>Random Forest</cell><cell>Vote/Veto</cell></row><row><cell>Accuracy</cell><cell>84.10</cell><cell>78.12</cell><cell>84.09</cell><cell>83.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,519.74,345.83,53.48"><head>Table 2 .</head><label>2</label><figDesc>Official results on the testing data. The accuracy on the "Reddit" and "LibraryThing" data are reported separately.</figDesc><table coords="5,137.69,550.75,308.49,22.48"><row><cell></cell><cell>Naive Bayes</cell><cell>Decision Tree</cell><cell>Random Forest</cell><cell>Vote-Veto</cell></row><row><cell cols="2">LibraryThing 91.59</cell><cell>83.38</cell><cell>74.82</cell><cell>90.63</cell></row><row><cell>Reddit</cell><cell>82.02</cell><cell>76.40</cell><cell>74.16</cell><cell>76.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,115.91,345.83,72.41"><head>Table 3 .</head><label>3</label><figDesc>Official results on the baseline versus our approach. The baseline provided used 4-grams as features classified by Linear Support Vector classifier and a Naive Bayes classifier.</figDesc><table coords="6,153.26,157.88,308.84,30.45"><row><cell></cell><cell cols="2">Naive Bayes KNOW Naive Bayes Baseline</cell><cell>Linear SVC Baseline</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>4-gram</cell></row><row><cell cols="2">LibraryThing 91.59</cell><cell>87.59</cell><cell>94.17</cell></row><row><cell>Reddit</cell><cell>82.02</cell><cell>76.40</cell><cell>78.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,166.04,291.05,273.62,34.55"><head>Table 4 .</head><label>4</label><figDesc>Official results on the testing data for the linking task.</figDesc><table coords="6,166.04,311.10,255.22,14.50"><row><cell>Accuracy</cell><cell>Recall</cell><cell>Precision</cell><cell>F-score</cell></row><row><cell>Linking Task 41.14</cell><cell>41.14</cell><cell>28.26</cell><cell>33.50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.48,141.22,7.47"><p>http://lucene.apache.org/solr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,657.44,122.39,7.47"><p>https://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,144.73,657.44,255.19,7.47"><p>http://social-book-search.humanities.uva.nl/#/mining16</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The presented work was developed within the <rs type="projectName">EEXCESS</rs> project funded by the <rs type="funder">European Union</rs> <rs type="programName">Seventh Framework Programme</rs> <rs type="grantNumber">FP7/2007-2013</rs> under grant agreement number <rs type="grantNumber">600601</rs>. The <rs type="projectName">Know-Center</rs> is funded within the <rs type="programName">Austrian COMET Program -Competence Centers for Excellent Technologies</rs> -under the auspices of the <rs type="funder">Austrian Federal Ministry of Transport, Innovation and Technology</rs>, the <rs type="funder">Austrian Federal Ministry of Economy, Family and Youth</rs> and by the <rs type="funder">State of Styria</rs>. COMET is managed by the <rs type="funder">Austrian Research Promotion Agency FFG</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GyE2hCE">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="project" subtype="full">EEXCESS</orgName>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_Mwtav8t">
					<idno type="grant-number">600601</idno>
					<orgName type="project" subtype="full">Know-Center</orgName>
					<orgName type="program" subtype="full">Austrian COMET Program -Competence Centers for Excellent Technologies</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,349.81,342.25,7.86;7,146.91,360.77,333.67,7.86;7,146.91,371.73,157.44,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,386.23,349.81,94.37,7.86;7,146.91,360.77,122.29,7.86">Overview and results of the inex 2009 interactive track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pharo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nordlie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Fachry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,290.51,360.77,190.08,7.86;7,146.91,371.73,34.30,7.86">Research and Advanced Technology for Digital Libraries</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,382.69,276.88,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,200.70,382.69,61.96,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,269.30,382.69,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,393.65,342.24,7.86;7,146.91,404.61,333.68,7.86;7,146.91,415.56,50.68,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,246.34,393.65,230.91,7.86">Estimating continuous distributions in bayesian classifiers</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,161.09,404.61,315.86,7.86">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="338" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,426.52,342.24,7.86;7,146.91,437.48,333.68,7.86;7,146.91,448.44,333.68,7.86;7,146.91,459.40,207.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,350.05,426.52,130.54,7.86;7,146.91,437.48,87.32,7.86">Vote/veto meta-classifier for authorship identification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,254.12,437.48,226.48,7.86;7,146.91,448.44,333.68,7.86;7,146.91,459.40,52.08,7.86">CLEF 2011: Proceedings of the 2011 Conference on Multilingual and Multimodal Information Access Evaluation (Lab and Workshop Notebook Papers)</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,470.36,284.86,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,207.06,470.36,148.84,7.86">C4. 5: programs for machine learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,481.32,342.24,7.86;7,146.91,492.28,103.93,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,204.02,481.32,133.73,7.86">Just-in-time information retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Rhodes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Massachusetts Institute of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
