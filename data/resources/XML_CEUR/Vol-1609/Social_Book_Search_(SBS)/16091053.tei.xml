<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.89,115.96,285.58,12.62">Overview of the SBS 2016 Mining Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.60,153.75,56.68,8.74"><forename type="first">Toine</forename><surname>Bogers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aalborg University Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.83,153.75,61.83,8.74"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CLS/CLST</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,291.21,153.75,63.40,8.74"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<email>marijn.koolen@uva.nl</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Netherlands Institute for Sound and Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.88,153.75,68.40,8.74"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CLS/CLST</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.89,115.96,285.58,12.62">Overview of the SBS 2016 Mining Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F5AB6088C69BDDFE354DA31549AABA81</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an overview of the mining track in the Social Book Search (SBS) lab 2016. The mining track addressed two tasks: (1) classifying forum posts as book search requests, and (2) linking book title mentions in forum posts to unique book IDs in a database. Both tasks are important steps in the process of solving complex search tasks within online reader communities. We prepared two data collections for the classification task: posts from the LibraryThing (LT) forum and a smaller number of posts from Reddit. For the linking task we used annotated LT threads. We found that the classification task was relatively straightforward, achieving up to 94% classification accuracy. The book linking task on the other hand turned out to be a difficult task: here the best system achieved an accuracy of 41% and F-score of 33.5%. Both the automatic classification of book search requests as the automatic linking of book mentions could next year be part of the pipeline for processing complex book searches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Mining track <ref type="foot" coords="1,209.26,503.84,3.97,6.12" target="#foot_0">1</ref> is a new addition to the Social Book Search (SBS) Lab in 2016. For the past five years, the Suggestion Track has explored techniques to deal with complex information needs that go beyond topical relevance and can include other aspects, such as genre, recency, engagement, interestingness, and quality of writing. In addition, it has investigated the value of complex information sources, such as user profiles, personal catalogues, and book descriptions containing both professional metadata and user-generated content.</p><p>So far, examples of such complex search tasks have been taken from the LibraryThing (LT) discussion fora. Book search requests were manually separated from other book-related discussion threads by human annotators, and the suggestions provided by other LT users were used as relevance judgments in the automatic evaluation of retrieval algorithms that were applied to the book search requests. If we wish to move further towards fully supporting complex book search behavior, then we should not just support the retrieval and recommendation stage of the process, but also the automatic detection of complex search needs and the analysis of these needs and the books and authors contained therein. This is the goal of the Mining Track.</p><p>The SBS 2016 Mining Track focuses on automating two text mining tasks in particular:</p><p>1. Book search request classification, in which the goal is to identify which threads on online forums are book search requests. That is, given a forum thread, the system should determine whether the opening post contains a request for book suggestions (i.e., binary classification of opening posts) 2. Book linking, in which the goal is to recognize book titles in forum posts and link them to the corresponding metadata record through their unique book ID. The task is not to mark each entity mention in the post text, but to label the post as a whole with the IDs of the mentioned books. That is, the system does not have to identify the exact phrase that refers to book, but only has to identify which book is mentioned on a per-post basis.</p><p>The suggestions that LT users provide in response to book search requests are often linked to official book metadata records using so-called Touchstones. Touchstones offer a wiki-like syntax for linking books (and authors) mentioned in LT threads to their official LT pages (and thereby the books' metadata records). All books mentioned in a thread are shown in a sidebar, so other LT users can see at a glance which books have already been suggested. Or, to quote a LT user: "The main reason I like Touchstones to work is that they allow me to scan the sidebar to see what books have already been discussed in a thread. This is particularly useful in a thread like this (in which somebody is asking for recommendations) because I can take care to mention something new without reading all previous threads (which I won't necessarily do if the thread gets really really long)."</p><p>However, not every book mentioned in LT threads is marked up using Touchstones; previous preliminary work has shown that around 16% of all books are not linked by LT users <ref type="bibr" coords="2,238.97,570.39,9.96,8.74" target="#b2">[3]</ref>, which has an as-of-yet unknown effect on their use as relevance assessments in the Suggestion Track.</p><p>In this paper, we report on the setup and the results of the 2016 Mining Track as part of the SBS Lab at CLEF 2016. First, in Section 2, we give a brief summary of the participating organisations. Section 3 describes the two tasks in the Mining Track in more detail, along with the data used and the evaluation process. Section 4 presents the results of the participating organisations on the two tasks. We close in Section 5 with a summary and plans for 2017.</p><p>A total of 28 organisations registered for the Mining Track and 4 organisations ended up submitting a total of 34 runs. The active organisations are listed in Table <ref type="table" coords="3,162.16,166.29,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mining Track setup and data</head><p>In the following sections we describe the data collection and annotation process for both tasks in the 2016 text mining track, as well as the evaluation procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 1: Book search request classification</head><p>Data collection For the task of classifying forum threads we created two data sets for training: one based on the LibraryThing (LT) forums and one based on Reddit. For the LT forums, we randomly sampled 4,000 threads and extracted their opening posts. We split them into a training and a test set, each containing 2,000 threads. These threads contained both positive and negative examples of book requests.</p><p>The Reddit training data was sampled from three months of Reddit threads collected in September, October, and November 2014. The set of positive book request examples comprises all threads from the suggestmeabook subreddit, whereas the negative examples comprises all threads from the books subreddit. The training set contained 248 threads in total. The Reddit test data was sampled from December 2014 and comprises 89 threads in total. Figure <ref type="figure" coords="3,392.53,413.28,4.98,8.74" target="#fig_0">1</ref> shows an example of the training data format for the classification task.</p><p>Annotation The labels of the Reddit training data were not annotated manually, as they were already categorized as positive and negative by virtue of the subreddit they originated from. In the annotation process for the LT threads, positive examples of book requests consisted of all posts where the user described an explicit foreground or background information need and was searching for books to read. Examples include known-item requests, where a user is looking for a specific book by describing plot elements, but cannot remember the title;  &lt;thread id="2nw0um"&gt; &lt;category&gt;suggestmeabook&lt;/category&gt; &lt;title&gt;can anyone suggest a modern fantasy series....?&lt;/title&gt; &lt;posts&gt; &lt;post id="2nw0um"&gt; &lt;author&gt;blackbonbon&lt;/author&gt; &lt;timestamp&gt;1417392344&lt;/timestamp&gt; &lt;parentid&gt;&lt;/parentid&gt; &lt;body&gt; .... where the baddy turns good, or a series similar to the broken empire trilogy. I thoroughly enjoyed reading it along with skullduggery pleasant, the saga of darren shan, the saga of lartern crepsley and the inheritance cycle. So whatever you got helps :D cheers lads, and lassses. &lt;/body&gt; &lt;upvotes&gt;8&lt;/upvotes&gt; &lt;downvotes&gt;0&lt;/downvotes&gt; &lt;/post&gt; ... &lt;/posts&gt; &lt;/thread&gt; users asking for books covering a specific topic; and users asking for books that are similar to another book they mention. Posts where users ask for new authors to explore or where they list their favorite books and ask others to do the same were not classified as explicit book requests.</p><p>The manual annotation of the LT data was performed by the four organizers of the task. To get an impression of the inter-annotator agreement, a small sample of 432 posts was labeled by two annotators. Average agreement according to Cohen's κ was 0.84, averaged over the pairs of annotators, which represents almost perfect agreement according to Cohen <ref type="bibr" coords="4,336.09,531.66,9.96,8.74" target="#b0">[1]</ref>.</p><p>For evaluation, 1,974 out of the 2,000 threads in the LibraryThing test set were used. For the 26 remaining threads, judges were unsure whether the first post was a request or not. The Reddit test set consisted of 89 threads with the subreddit names (books and suggestmeabook) as labels. In order to create a ground truth for the test set, two judges (track organizers) manually classified the 89 test threads. They discussed all disagreements and reached consensus on all 89 threads. 81 of the labels were the same as the original Reddit label; the other 8 were different. We used the manual labels as ground truth. Table <ref type="table" coords="4,447.69,632.21,4.98,8.74" target="#tab_1">2</ref> shows the proportion of positive and negative examples in the training and test sets of both data sets. For the book linking task we created a data set based on the touchstones in the LT forum. The training data consisted of 200 threads with 3619 posts in total. The training data contains only those touchstones that had been added by the LT authors; we did not enrich the posts with more annotations. Figure <ref type="figure" coords="5,475.61,377.39,4.98,8.74" target="#fig_1">2</ref> shows an example of the training data format for the linking task. In the example, Insomnia is the title of a book. The task is to identify the LT work ID of the corresponding book and link it to that specific post ID.</p><p>Participants used the Amazon/LT collection for linking the book mentions to a database record. This collection originated with the Suggestion track and contains 2.8 million book metadata records along with their LT work IDs. The test data for the linking task comprised 200 LT threads. As opposed to the training data, we did make the annotations in the test data more complete by manually annotating book mentions and linking them to the book database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation of test data</head><p>In the annotation process, we linked books manually at the post level by their unique LT work ID. Many books are published in different editions throughout the years with different unique ISBNs, but all of these versions are connected to the same unique LT work ID. If a book occurred multiple times in the same post, only the first occurrence was linked, so participants only need to specify each of the work IDs found in a post once. If a post mentioned a series of books, we linked this series to the first book in the series, e.g., the "Harry Potter series" was linked to "Harry Potter and the Philosopher's Stone". In some cases, a book title was mentioned, but no suitable work ID was found in the Amazon/LT collection. In this cases, we labeled that book title as UNKNOWN.</p><p>We did not link book authors. When a book was referred to as "the Stephen King book", we did not mark this as a book title. Similarly, if a series was referred   to by the name of the author, e.g., "the Stieg Larson trilogy", then the series was not labeled. We do consider these cases where the author is mentioned as borderline cases, because they point to both the author and the books that they wrote at the same time. In this data set we decided not to include them in the annotation, but we are aware that they fall in the 'grey' area of unclear cases.</p><p>Another source of annotation confusion were the forum threads about short stories and collections of stories. In these cases we did not label the individual short stories (they also do not have existing LT work IDs), but only the actual book with the collection.</p><p>Other difficult cases for the manual annotation were the cases where it was not immediately clear where the book title begins and ends. For example, in (1) below, the alternative book title could have been "Bujold's Sharing Knife" instead of "Sharing Knife". Vague or partial matches were also difficult to annotate sometimes. For example, the post containing fragment (2) was not linked to a work ID because it deviated significantly from the actual title of the book that was mentioned (and linked) correctly in the follow up post as being "Fifteen Decisive Battles of the World from 1851". In order to assess the difficulty and subjectivity of the book linking annotation task we had 28 threads (155 posts) annotated by 2 assessors and we analyzed the differences in annotation. We found that there was quite some disagreement between the assessors: 71 books were linked by both assessors, and 247 by only one of the two. This implies that absolute agreement is only 22%. <ref type="foot" coords="7,429.26,165.24,3.97,6.12" target="#foot_1">2</ref> There are two types of disagreement: (a) a book mention was linked by one assessor and missed/skipped by the other, and (b) a book mention was linked by both, but to different work ids. The most difficult were the mentions of book series. These should be linked to the first book of the series, which is not always trivial. For example, consider this post text:</p><p>Well, I could recommend some great Batman graphic novels, only one problem. They're written for adults, and are pretty dark. Year One is an amazing version of his origin story, but it isn't exactly appropriate for a second grader. You might try some of the Tintin graphic novels. There are dozens of them, and they're great stories. I second Louis Sachar as well. You might want to try Holes. Its a great, inventive story. Plus, you can watch the movie together once he finishes the book.</p><p>Both assessors linked two series in this post. These were linked by assessor 1:</p><p>-David Mazzuchelli -Batman: Year One -Deluxe Edition: Year One -Herge -Tintin in America (Tintin) and these were linked by assessor 2:</p><p>-Lewis Richmond -Batman: Year One (Batman) -Herge -Tintin in the Land of the Soviets The difficulty of the annotation for the linking task is a topic that should be addressed in future editions of the SBS lab. One recommendation would be to write more explicit annotation guidelines, and share those with the participants.s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>For the book request classification task, we computed and report only accuracy, as these are binary decisions. For the linking task, we computed accuracy, precision, recall, and F-score.</p><p>Both tasks were performed and evaluated at the level of forum posts. We detected whether a forum post was a book request in the classification task, and whether a certain book title occurred in a post. In case the same book title was mentioned multiple times in the same post, we only counted and evaluated on one occurrence of this particular book title. Each book title is mapped to a LibraryThing work ID that links together different editions of the same book (with different ISBNs).</p><p>During manual annotation, we came across several book titles for which we were unable to find the correct LT work ID (labeled as UNKNOWN). These cases were problematic in the evaluation: just because the annotator could not find the correct work ID does not mean that it does not exist. For that reason, we decided to discard these examples in the evaluation of the test set results. In total, 180 out of the 5097 book titles in the test set were discarded for this reason.</p><p>Similarly, during the book request classification task, we also found some cases in the LT data where we were unsure about categorizing them as book search requests or not. We discarded 26 such cases from the test set in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>A total of 3 teams submitted 15 runs, 2 teams submitted 9 runs for the Classification task and 2 teams submitted 6 runs for the Linking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: Classifying forum threads</head><p>Baselines For the baseline system of the classification task, we trained separate classifiers for the two data sets (LT and Reddit) using scikit-learn. <ref type="foot" coords="8,416.56,366.81,3.97,6.12" target="#foot_2">3</ref> We extracted bag-of-words-features (either words or character 4-grams) from the title and the body of the first post, and for LT also from the category (for Reddit, the category was the label). We used tf-idf weights for the words and the character 4-grams from these fields. We ran 3 classifiers on these data: Multinonial Naive Bayes (MNB), Linear Support Vector Classification (LinearSVC) and KNN, all with their default hyperparameter settings in scikit-learn. The results are in Table <ref type="table" coords="8,472.84,440.11,3.87,8.74">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of submitted runs</head><p>The Know team reported an interesting experiment on the LT training data of the classification task <ref type="bibr" coords="8,398.32,482.03,9.96,8.74" target="#b4">[5]</ref>. A Naive Bayes classifier trained on a single feature, namely the quantified presence of question marks within the post, already achieved an accuracy of 80% on the LT training material. This gives us some insight into the skewed nature of this domain specific data set from a dedicated book forum: a post containing a question is likely to express a question with a book search request.</p><p>The LIPAH team compared two types of features for the classification task: (a) all nouns and verbs in the posts, and (b) compound nouns and phrases extracted using syntactic patterns. They found that the addition of syntactic phrases improves the classification accuracy <ref type="bibr" coords="8,328.84,589.65,9.96,8.74" target="#b1">[2]</ref>.</p><p>Table <ref type="table" coords="8,178.07,601.62,4.98,8.74">3</ref> shows that for the LT data, the submitted runs did not beat the LinearSVC baselines. For the Reddit data however, runs by both teams were able to beat the best baseline system by a large margin. Since the Reddit dataset was much smaller than the LT dataset, the best strategy seems to be to add the LT Table <ref type="table" coords="9,167.53,191.37,4.46,8.77">3</ref>. Results for the classification task for the two datasets in terms of accuracy on the 1974 LibraryThing and 89 Reddit posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LibraryThing Rank Team</head><p>Run Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2: Book linking</head><p>Evaluation of submitted runs The results of the book linking task can be found in Table <ref type="table" coords="10,204.84,348.60,3.87,8.74" target="#tab_3">4</ref>. The Know team used a list look-up system combined with a weighting threshold in their sbs16classificationlinking run to prevent the overgeneration of potential book titles <ref type="bibr" coords="10,286.50,372.51,9.96,8.74" target="#b4">[5]</ref>. The LSIS team <ref type="bibr" coords="10,220.04,384.52,10.52,8.74" target="#b3">[4]</ref> first tried to detect book titles and author names at the phrase level (using SVM and CRF) inside posts and used Levenshtein distance to match titles to LT work IDs. Each detected unique book title was assigned to the larger post unit. They submitted 5 runs that varied in the way work IDs were matched against the potential book titles and the feature representation.</p><p>Both teams investigated the usage of author names in the proximity of potential book titles to disambiguate between potential titles and show that this is indeed a helpful feature. Both teams use complementary strategies for the book linking task as the Know systems has a higher recall while the LSIS run all achieves a better precision (as well as the highest F-score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Plans</head><p>This was the first year of the Social Book Search Mining Track. Our goal was create a benchmark data set for text mining of book related discussion forum. In this first edition we focused on two tasks. The first task was to automatically identify which posts in a book forum tread are actual book search requests, and the second task was to detect which book titles are mentioned in a forum post and link the correct unique book ID to the post. We had three active participants who submitted a total of 15 runs. The book search classification task turned out to be a relatively straightforward task, both in manual annotation and in automatic prediction. A rather simple bag-of-words baseline classifier achieved an accuracy up to 94% on the LibraryThing data. The book linking task on the other hand turned out to be a difficult task and here the best system achieved an accuracy of 41% and F-score of 33.5%.</p><p>Developing effective algorithms for automatically detecting and linking these book mentions would be a boon to the process of supporting complex search needs. Moreover, other book discussion websites, such as GoodReads or even dedicated Reddit threads may not have Touchstone-like functionality. Here, the need for automatic book linking algorithms is even more pressing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,125.88,345.83,8.77;4,134.77,137.87,56.23,8.74"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of the training data format for the Book search request classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,125.88,345.82,8.77;6,134.77,137.87,345.83,8.74;6,134.77,149.82,164.24,8.74"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the training data thread format for the Book title linking task. The corresponding label file contains three columns: threadid, postid, LT work id. In this case: 122992, 1, 5812.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,172.07,37.66,7.47"><head></head><label></label><figDesc>&lt;thread&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,151.70,583.89,279.51,8.74;6,151.70,601.73,311.95,8.74;6,151.70,613.69,92.49,8.74;6,149.71,632.21,330.88,8.74;6,134.77,644.16,345.82,8.74;6,134.77,656.12,280.67,8.74"><head>( 1 )</head><label>1</label><figDesc>have you read Lois McMaster Bujold's Sharing Knife books? (2) I think there was a book called something like Ten Decisive Battles by a General Creasey During manual annotation, 3 of the 220 threads were removed from the test set because of long lists of titles without context. The final test set consists of 217 threads comprising 5097 book titles identified in 2117 posts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,557.32,345.82,91.58"><head>Table 1 .</head><label>1</label><figDesc>Active participants of the Mining Track of the CLEF 2016 Social Book Search Lab and number of contributed runs or users.</figDesc><table coords="3,208.72,592.17,197.92,56.73"><row><cell>Institute</cell><cell cols="2">Acronym Runs</cell></row><row><cell cols="2">Aix-Marseille Université CNRS LSIS</cell><cell>8</cell></row><row><cell>Tunis EL Manar University</cell><cell>LIPAH</cell><cell>6</cell></row><row><cell>Know-Center</cell><cell>Know</cell><cell>8</cell></row><row><cell cols="2">Radboud University Nijmegen RUN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,115.92,345.83,222.39"><head>Table 2 .</head><label>2</label><figDesc>Overview of number of positive and negative instances in the training and test sets for the book request classification task. Book linking through the use of Touchstones is an striking characteristic of the LT forum, and an important feature for the forum community. A Touchstone is a link created by a forum member between a book mention in a forum post and a unique LT work ID in the LT database. A single post can have zero or more different touchstones linked to it. Touchstones allow readers of a forum thread to quickly see which books are mentioned in the thread.</figDesc><table coords="5,134.77,152.71,263.19,125.82"><row><cell></cell><cell></cell><cell cols="2">LibraryThing Reddit</cell></row><row><cell>Training</cell><cell>Positive Negative</cell><cell>272 1688</cell><cell>43 205</cell></row><row><cell>Test</cell><cell>Positive Negative</cell><cell>245 1729</cell><cell>13 76</cell></row><row><cell cols="2">3.2 Task 2: Book linking</cell><cell></cell><cell></cell></row><row><cell>Data collection</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,183.03,357.75,149.93"><head></head><label></label><figDesc>&lt;message&gt; &lt;date&gt;Sep 1, 2011, 9:56am &lt;/date&gt; &lt;text&gt;This month's read is Insomnia. Odd that I'm posting this I am yawning and blinking my eyes because I didn't sleep well last night. I remember not really caring for this one on my first read. The synopsis sounded excellent. But I was disappointed to find that it was basically a Dark tower spin-off. We'll see how it goes this time I guess.</figDesc><table coords="6,134.77,259.74,164.76,73.22"><row><cell>&lt;/text&gt;</cell></row><row><cell>&lt;postid&gt;1&lt;/postid&gt;</cell></row><row><cell>&lt;username&gt;jseger9000&lt;/username&gt;</cell></row><row><cell>&lt;threadid&gt;122992&lt;/threadid&gt;</cell></row><row><cell>&lt;/message&gt;</cell></row><row><cell>...</cell></row><row><cell>&lt;/thread&gt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,136.16,255.11,278.71,328.30"><head>Table 4 .</head><label>4</label><figDesc>Results for the linking task for the LibraryThing data set in terms of accuracy. Reddit training data for classifying the Reddit test threads. The best run for the Reddit data is LIPAH-submission6, which uses sequences of words and verbs as features.</figDesc><table coords="9,136.16,255.11,278.71,328.30"><row><cell>1</cell><cell cols="2">baseline character 4-grams.LinearSVC</cell><cell>94.17</cell></row><row><cell>2</cell><cell cols="2">baseline Words.LinearSVC</cell><cell>93.92</cell></row><row><cell>3</cell><cell>Know</cell><cell>Classification-Naive-Results</cell><cell>91.59</cell></row><row><cell>4</cell><cell cols="2">baseline character 4-grams.KNeighborsClassifier</cell><cell>91.54</cell></row><row><cell>5</cell><cell cols="2">baseline Words.KNeighborsClassifier</cell><cell>91.39</cell></row><row><cell>6</cell><cell cols="2">LIPAH submission2-librarything</cell><cell>90.98</cell></row><row><cell>7</cell><cell cols="2">LIPAH submission3-librarything</cell><cell>90.93</cell></row><row><cell>8</cell><cell cols="2">LIPAH submission4-librarything</cell><cell>90.83</cell></row><row><cell>9</cell><cell>Know</cell><cell>Classification-Veto-Results</cell><cell>90.63</cell></row><row><cell>10</cell><cell cols="2">LIPAH submission1-librarything</cell><cell>90.53</cell></row><row><cell>11</cell><cell cols="2">baseline character 4-grams.MultinomialNB</cell><cell>87.59</cell></row><row><cell>12</cell><cell cols="2">baseline Words.MultinomialNB</cell><cell>87.59</cell></row><row><cell>13</cell><cell>Know</cell><cell>Classification-Tree-Results</cell><cell>83.38</cell></row><row><cell>14</cell><cell>Know</cell><cell>Classification-Forest-Results</cell><cell>74.82</cell></row><row><cell></cell><cell></cell><cell>Reddit</cell><cell></cell></row><row><cell cols="2">Rank Team</cell><cell>Run</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell cols="2">LIPAH submission6-reddit</cell><cell>82.02</cell></row><row><cell>2</cell><cell>Know</cell><cell>Classification-Naive-Results</cell><cell>82.02</cell></row><row><cell>3</cell><cell cols="2">LIPAH submission5-reddit</cell><cell>80.90</cell></row><row><cell>4</cell><cell cols="2">baseline Words.KNeighborsClassifier</cell><cell>78.65</cell></row><row><cell>5</cell><cell cols="2">baseline Words.LinearSVC</cell><cell>78.65</cell></row><row><cell>6</cell><cell cols="2">baseline character 4-grams.LinearSVC</cell><cell>78.65</cell></row><row><cell>7</cell><cell cols="2">baseline character 4-grams.KNeighborsClassifier</cell><cell>78.65</cell></row><row><cell>8</cell><cell>Know</cell><cell>Classification-Tree-Results</cell><cell>76.40</cell></row><row><cell>9</cell><cell>Know</cell><cell>Classification-Veto-Results</cell><cell>76.40</cell></row><row><cell>10</cell><cell cols="2">baseline Words.MultinomialNB</cell><cell>76.40</cell></row><row><cell>11</cell><cell cols="2">baseline character 4-grams.MultinomialNB</cell><cell>76.40</cell></row><row><cell>12</cell><cell>Know</cell><cell>Classification-Forest-Results</cell><cell>74.16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.80,262.17,8.12"><p>See http://social-book-search.humanities.uva.nl/#/mining</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,144.73,645.84,335.86,7.86;7,144.73,656.80,308.16,7.86"><p>Note that Cohen's κ is undefined for these data because we the number of book titles for which the assessors agree that they should not be linked is infinite.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,144.73,657.44,113.47,7.47"><p>http://scikit-learn.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,138.64,246.52,341.95,8.74;11,147.50,258.47,190.42,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,195.34,246.52,204.56,8.74">A Coefficient of Agreement for Nominal Scales</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,409.77,246.52,70.82,8.74;11,147.50,258.47,115.66,8.74">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.64,270.43,341.95,8.74;11,147.50,282.38,333.09,8.74;11,147.50,294.34,93.02,8.74" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ettaleb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Latiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Douar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,370.08,270.43,110.51,8.74;11,147.50,282.38,208.54,8.74">Proceedings of the 7th International Conference of the CLEF Association</title>
		<title level="s" coord="11,420.01,282.38,60.58,8.74;11,147.50,294.34,88.73,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 7th International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.64,306.29,341.95,8.74;11,147.50,318.25,333.09,8.74;11,147.50,330.20,333.10,8.74;11,147.50,342.16,333.10,8.74;11,147.50,354.11,333.09,8.74;11,147.50,366.07,333.09,8.74;11,147.50,378.02,333.09,8.74;11,147.50,389.98,171.66,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,319.23,318.25,161.36,8.74;11,147.50,330.20,70.51,8.74">Overview of the CLEF 2015 social book search lab</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gäde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Huurdeman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Skov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Toms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,449.23,342.16,31.37,8.74;11,147.50,354.11,333.09,8.74;11,147.50,366.07,193.92,8.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction -6th International Conference of the CLEF Association</title>
		<title level="s" coord="11,365.39,378.02,115.20,8.74;11,147.50,389.98,30.49,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pinel-Sauvagnat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-09-08">2015. September 8-11, 2015. 9283. 2015</date>
			<biblScope unit="page" from="545" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.64,401.93,341.95,8.74;11,147.50,413.89,333.09,8.74;11,147.50,425.84,333.09,8.74;11,147.50,437.80,34.32,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,334.20,401.93,146.39,8.74;11,147.50,413.89,149.75,8.74">Linking task: Identifying authors and book titles in verbose queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ollagnier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,321.82,413.89,158.77,8.74;11,147.50,425.84,154.85,8.74">Proceedings of the 7th International Conference of the CLEF Association</title>
		<title level="s" coord="11,364.11,425.84,116.47,8.74;11,147.50,437.80,30.03,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 7th International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.64,449.75,341.95,8.74;11,147.50,461.71,333.09,8.74;11,147.50,473.66,34.32,8.74" xml:id="b4">
	<analytic>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ziak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rexha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,320.37,449.75,160.22,8.74;11,147.50,461.71,154.85,8.74">Proceedings of the 7th International Conference of the CLEF Association</title>
		<title level="s" coord="11,364.11,461.71,116.47,8.74;11,147.50,473.66,30.03,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 7th International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
