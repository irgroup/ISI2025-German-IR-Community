<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.97,116.95,305.41,12.62;1,156.61,134.89,302.14,12.62;1,263.43,152.82,88.50,12.62">SBS 2016 Track mining: Classification with linguistic features for book search requests classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.89,191.23,78.45,8.74"><forename type="first">Mohamed</forename><surname>Ettaleb</surname></persName>
							<email>mouhamed.taleb@hotmail.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Sciences of Tunis</orgName>
								<orgName type="laboratory">LIPAH research Laboratory</orgName>
								<orgName type="institution" key="instit1">Tunis EL Manar University</orgName>
								<orgName type="institution" key="instit2">Campus Universitaire Farhat Hached</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.90,191.23,56.66,8.74"><forename type="first">Chiraz</forename><surname>Latiri</surname></persName>
							<email>chiraz.latiri@gnet.tn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Sciences of Tunis</orgName>
								<orgName type="laboratory">LIPAH research Laboratory</orgName>
								<orgName type="institution" key="instit1">Tunis EL Manar University</orgName>
								<orgName type="institution" key="instit2">Campus Universitaire Farhat Hached</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.11,191.23,62.88,8.74"><forename type="first">Brahim</forename><surname>Douar</surname></persName>
							<email>b.douar@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Sciences of Tunis</orgName>
								<orgName type="laboratory">LIPAH research Laboratory</orgName>
								<orgName type="institution" key="instit1">Tunis EL Manar University</orgName>
								<orgName type="institution" key="instit2">Campus Universitaire Farhat Hached</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.92,191.23,60.07,8.74"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@univ-amu.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LSIS UMR 7296</orgName>
								<orgName type="institution" key="instit1">Aix-Marseille Universit√©</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>13397</postCode>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.97,116.95,305.41,12.62;1,156.61,134.89,302.14,12.62;1,263.43,152.82,88.50,12.62">SBS 2016 Track mining: Classification with linguistic features for book search requests classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2256569E61EF4E38800CA14340510D66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>classification</term>
					<term>noun phrases extraction</term>
					<term>sequences mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe text mining approaches dedicated to the classification track in Social Book Search Track Lab 2016. This track aims to exploit social knowledge extracted from LibraryThing and Reddit collections to identify which threads on online forums are book search requests. Our proposed classification model is based on combination of different textual features, namely : (i) basic linguistic features such as nouns and verbs; and, (ii) composed features such term sequences and noun phrases generated. Then, we applied a NaiveBayes classifier to specify the user's intentions in the requests.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Social Book Search (SBS) Lab investigates book search where the users information needs are complex, looking for more than objective metadata. In this respect, SBS Lab aims to research and develop techniques in order to support users in complex book search tasks. It consists of three tracks:</p><p>1. Interactive Track : a user-oriented interactive task investigating systems that support users in each of multiple stages of a complex search tasks. The track offers participants a complete experimental interactive IR setup and an exciting new multistage search interface to investigate how users move through search stages. 2. Suggestion Track : a system-oriented task for systems to suggest books based on rich search requests combining several topical and contextual relevance signals, as well as user profiles and real-world relevance judgements. 3. Mining Track : an NLP/Text Mining track focusing on detecting and linking book titles in online book discussion forums, as well as detecting book search request in forum posts for automatic book recommendation.</p><p>In this paper, we only consider the mining track which is a new one in SBS 2016 edition and investigates two tasks : (i ) Classification task : how Information Retrieval Systems can automatically identify book search requests in online forums, and; (ii ) Linking task : how to detect and link books mentioned in online book discussions.</p><p>Our contribution deals only with the classification task. The final objective of this task is to identify which threads on online forums are book search requests. Thereby, given a forum thread with one or more posts, the system should determine whether the opening post contains a request for book suggestions (i.e., binary classification of opening posts).</p><p>In this respect, we propose to use two types of approaches, namely : an approach based on textual sequences mining, and an NLP method which relies on nouns, verbs and noun phrases extraction (i.e., compound nouns), to improve the classification efficiency. Then, we use the NaiveBayes classifier with Weka to specify the user's intentions in the requests.</p><p>The remainder of this paper is organized as follows: Section 2 describes the mining track and the test data. Then, section 3 recalls the basic definition for textual sequences mining and details our proposed approaches for book search requests classification. Next, Section 4 details our different submitted runs for the mining track as the official obtained results. The conclusion is given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SBS 2016 mining Track</head><p>The SBS 2016 mining Track investigates how systems can automatically identify book search requests in online forums and how to detect and link books mentioned in online book discussions. Often, users can have information needs that are difficult to express while considering a classical search engine and they rely in this case to online forums, in order to get recommendations from others users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SBS requests classification task</head><p>Classification task identifies which threads on online forums are book search requests. That is, given a forum thread with one or more posts, the system should determine whether the opening post contains a request for book suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of Data collections</head><p>The test SBS 2016 collections contains:</p><p>1. A collection of 2 780 300 book records from Amazon, extended with social metadata from LibraryThing. This set represents the books available through Amazon. The records contain title information as well as a Dewey Decimal Classification (DDC) code (for 61% of the books) and category and subject information supplied by Amazon. Each book is identified by an ISBN. Note that since different editions of the same work have different ISBNs, there can be multiple records for a single intellectual work. Each book record is an XML file with fields like ISBN, title, author, publisher, dimensions, number of pages and publication date. Curated metadata comes in the form of a Dewey Decimal Classification in the dewey field, Amazon subject headings in the subject field, and Amazon category labels in the browseNode fields.</p><p>The social metadata from Amazon and LibraryThing is stored in the tag, rating, and review fields. 2. Two data collections for the classification task: LibraryThing and Reddit:</p><p>-Reddit training data: the training data contains threads from the suggestmeabook subreddit as positive examples and threads from the books subreddit as negative examples. In the test data, the subreddit has been removed (cf. Table <ref type="table" coords="3,253.23,263.63,3.87,8.74" target="#tab_0">1</ref>). -LibraryThing: 2,000 labelled threads for training, and 2,000 labelled threads for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">linguistic feature extraction</head><p>In the linguistic feature model, we begin with making the simplifying assumption about a text in the request that it can be represented as collections of words in which syntactic information a negligible and even the word order is unimportant. Text features extraction is the process of transforming what is essentially a bag of terms into a feature set that is usable by a classifier. We employed TreeTagger for annotating text with part-of-speech and lemma information <ref type="bibr" coords="4,419.42,254.80,9.96,8.74" target="#b5">[3]</ref>. We notice that the linguistic feature model is the simplest method; it constructs a word presence feature set from all the words of an instance. This method doesn't care about the order of the words, or how many times a word occurs, all that matters is whether the word is present in a list of words. In our approach, we chose to keep only the nouns and verbs for each request of the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compound nouns feature extraction</head><p>Earlier works in the literature proved that the use of simple terms features in classification is not accurate enough to represent the documents contents due to the words ambiguity. A solution to this problem is to use compound nouns 3  instead of simple words. The assumption is that compound nouns are more likely to identify semantic entities than simple words. We propose to perform a linguistic approach to extract compound nouns from the request content of the mining track 2016. The goal is to identify the dependencies and relationships between words through language phenomena. The linguistic approach for compound nouns extraction is based on two steps:</p><p>1. A complex-syntactic with a tagger (i.e., Treetagger). Each word is associated to a tag corresponding to the syntactic category of the word, example: noun, adjective, preposition, proper noun, determiner, etc. 2. The tagged corpus is used to extract a set of compound nouns by the identification of syntactic patterns as detailed in <ref type="bibr" coords="4,348.10,547.67,9.96,8.74" target="#b3">[1]</ref>. We adopt the definition of syntactic patterns given in <ref type="bibr" coords="4,393.31,560.95,9.96,8.74" target="#b3">[1]</ref>, where a pattern is a syntactic rule on the order of concatenation of grammatical categories which form a noun phrase, i.e., a compound noun.</p><p>For the English language, We choose to define 12 syntactic patterns: 4 syntactic patterns of size two (for example: Noun Noun, Adjective Noun, etc.), 6 syntactic patterns of size three (for example: Adjective Noun Noun, Adjective Noun Gerundive, etc.) and 2 syntactic patterns of size 4.</p><p>3 By compound nouns, we refer to complex terms and noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequences feature mining</head><p>Most methods in text classification rely on contiguous sequences of words as features. Indeed, if we want to take non-contiguous (gappy) patterns into account, the number of features increases exponentially with the size of the text. Furthermore, most of these patterns will be more noisy. To overcome both issues, sequential pattern mining can be used to efficiently extract a smaller number of the most frequent features. Sequential pattern mining problem was first proposed in <ref type="bibr" coords="5,405.22,212.44,9.96,8.74" target="#b6">[4]</ref>, and then improved in <ref type="bibr" coords="5,178.76,224.39,9.96,8.74" target="#b7">[5]</ref>. It is worth noting that many methods used to discover sequential patterns are usually extension of approaches dedicated to mining frequent itemsets. Most of these approaches proceed on a bottom-up way. First, the frequent sets, or sequences, of size 1 are found, then longer frequent sequences are iteratively obtained starting from the shorter ones <ref type="bibr" coords="5,353.48,272.21,9.96,8.74" target="#b7">[5]</ref>. Finally, all the sequences fulfilling the required conditions are found. In our work, we use the LCM seq algorithm <ref type="bibr" coords="5,181.10,296.12,10.52,8.74" target="#b4">[2]</ref> <ref type="foot" coords="5,191.62,294.55,3.97,6.12" target="#foot_0">4</ref> which is a variation of LCM<ref type="foot" coords="5,330.22,294.55,3.97,6.12" target="#foot_1">5</ref> for sequences mining. The algorithm follows the scheme so called prefix span, but the data structures and processing method are LCM based.</p><p>We adapt to our purpose the basic definitions of the theoretical framework for frequent sequential patterns discovery introduced in <ref type="bibr" coords="5,379.71,344.08,9.96,8.74" target="#b6">[4]</ref>. Definition 1. A sequence S = t 1 , . . . , t j , . . . , t n , such that t k ‚àà vacabulary V and n is its length, is a n-termset for which the position of each term in the sentence is maintained. S is called a n-sequence. Definition 2. Given S a sequence discovered from the collection. The support of S is the number of sentences in P that contain S, S is said to be frequent if and only if its support is greater than or equal to the minimum support threshold minsupp.</p><p>Interestingly enough, to address book search requests classification in an efficient and effective manner, we claim that a synergy with some advanced text mining methods, especially sequence mining <ref type="bibr" coords="5,348.19,492.24,9.96,8.74" target="#b6">[4]</ref>, is particularly appropriate. However, applying the frequent sequences of terms in the context of requests classification can help select good features and improve classification accuracy, mostly because of the huge number of potentially interesting frequent sequences that can be drawn from a request collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mining and learning process</head><p>The thread classification system serves to identify which threads on online forums are book search requests. Our proposed text mining based approaches are depicted in Figure <ref type="figure" coords="5,217.71,615.10,3.87,8.74" target="#fig_1">1</ref>. The classification threads process is performed on the following steps:</p><p>1. Annotating the selected threads with part-of-speech and lemma information using TreeTagger. 2. Extracting linguistic features, i.e., verbs and compound nouns from the annotated threads. 3. Generating the term sequence features using the efficient algorithm LCM seq.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Runs description</head><p>We conducted six runs according to the approaches described in Section 3, namely: four runs on the LibraryThing data collection and two runs on the Reddit data collection.</p><p>Runs on the LibraryThing data collection 1. Run1 (ID = Classification-NV): We used in this run, only Bag of linguistic features (i.e., nouns and verbs) to generate the classification model, using the NaiveBayes classifier under Weka using the default configurations<ref type="foot" coords="7,460.17,162.51,3.97,6.12" target="#foot_4">8</ref> . 2. Run2 (ID = Classification-NVC): We extracted first, Bag of linguistic features (i.e., nouns and verbs) and compound nouns from a set of 2000 threads. Then, we used these features to generate the classification model, using the NaiveBayes classifier. 3. Run3 (ID = Classification-NVSeq): We used the nouns and verbs as in Run1, then, we extracted the sequences of words using LCM seq algorithm with a threshold of minsupp =5, we noticed after series of experiments with differents threshold values that the minsupp =5 give the best results and had abvious clear impact on this features extraction. Finally, we combined all features to extract the classification model, using the NaiveBayes classifier. 4. Run4 (ID = Classification-CSeq): In this run, we combined the compound nouns with sequences, using the NaiveBayes classifier.</p><p>Runs on the Runs Reddit data collection 1. Run5 (ID = Classification-V): In this run, we used only the verbs as features to extract the classification model, using the NaiveBayes classifier. 2. Run6 (ID = Classification-VSeq): In the second run on post Reddit, we extracted the sequences of words and the verbs as features using LCM seq algorithm with a threshold of minsupp =3, we chose a low value of minsupp due to the limited number of sequence extracted from the collection Reddit. Finally, we generated the classification model with the NaiveBayes classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metric and results</head><p>The results obtained by our runs conducted for the classification task requests are evaluated in a single metric, which is the Accuracy. It simply measures how often the classifier makes the correct prediction. It is the ratio between the number of correct predictions and the total number of predictions (the number of test data points), thus :</p><formula xml:id="formula_0" coords="7,230.12,548.58,250.47,22.31">accuracy = T P + T N T P + T N + F P + F N<label>(1)</label></formula><p>where :</p><p>- In the 2016 SBS Mining Track, a total of 3 teams submitted 20 runs, 2 teams submitted 14 runs for the Classification task and 2 teams submitted 6 runs for the Linking task.</p><p>Table <ref type="table" coords="8,176.50,192.95,4.98,8.74" target="#tab_1">2</ref> shows 2016 SBS track mining official results for our 4 runs conducted on the LibraryThing collection. Our runs are (Classification-NVC, Classification-NVSeq, Classification-CSeq, Classification-NV) ranked sixth, seventh, eighth and tenth, respectively, for the classification task. These results highlight that the combination of Bag of linguistic features (i.e., nouns and verbs) and compound nouns performs the best in term of accuracy, i.e., Classification-NVC. We note also that the combination of nouns, verbs and sequences of words, i.e., Classification-NVSeq increases accuracy compared to the use of only Bag of linguistic features (i.e., nouns and verbs). This is mainly due to the difference between users' descriptions of their needs. Table <ref type="table" coords="8,177.43,315.54,4.98,8.74">3</ref> describes 2016 SBS track mining official results for our 2 runs conducted on the Reddit collection (Classification-VSeq and Classification-V), which are ranked first and third, respectively, in the classification task. The best run is performed with the sequences of words and the verbs as features for classification. This result confirms that mining sequences is useful for classification task.</p><p>It's worth noting that the obtained classification evaluation results shed light that our proposed approaches, based on NLP techniques, offer interesting results and helps to identify book search requests in online forums . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,138.97,181.07,309.42,8.74;6,448.39,179.50,3.97,6.12;6,455.66,181.07,24.94,8.74;6,151.70,194.71,29.01,7.61;6,180.71,191.45,3.97,6.12;6,185.18,193.03,2.77,8.74;6,138.97,205.42,264.01,8.74"><head>4 .</head><label>4</label><figDesc>Generation of the classification model using the NaiveBayes classifier 6 under Weka 7 . 5. Applying the classification model to the supplied test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,152.14,505.69,311.09,7.89;6,181.12,247.55,253.13,243.38"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed approaches steps for book search requests classification</figDesc><graphic coords="6,181.12,247.55,253.13,243.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,151.70,603.29,132.62,8.74;7,140.99,615.28,145.47,8.77;7,140.99,627.30,148.33,8.77;8,140.99,119.96,146.53,8.77"><head></head><label></label><figDesc>T P : Number of True Positives -F P : Number of False Positives -T N : Number of True Negatives -F N : Number of False Negative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,328.33,365.22,337.53"><head>Table 1 .</head><label>1</label><figDesc>Example of data format RedditIn this work, as depicted in Figure1, we present two approaches for book search requests classification. The first one is based on the sequences mining technique to extract frequent sequences from textual content requests. While the second one is based on NLP techniques. It consists in exploring textual content requests, and extracting verbs, nouns and compound nouns.</figDesc><table coords="3,134.77,491.59,314.54,126.60"><row><cell>&lt;/body&gt;</cell></row><row><cell>&lt;upvotes&gt;8&lt;/upvotes&gt;</cell></row><row><cell>&lt;downvotes&gt;0&lt;/downvotes&gt;</cell></row><row><cell>&lt;/post&gt;</cell></row><row><cell>&lt;/posts&gt;</cell></row><row><cell>&lt;/thread&gt;</cell></row><row><cell>&lt;/forum&gt;</cell></row><row><cell>3 Approaches for book search requests classification</cell></row></table><note coords="3,136.16,349.12,97.60,7.86;3,136.16,360.08,99.37,7.86;3,136.16,371.04,101.14,7.86;3,136.16,382.00,166.80,7.86;3,136.16,392.96,246.83,7.86;3,136.16,403.92,35.17,7.86;3,136.16,414.88,92.21,7.86;3,136.16,425.84,137.50,7.86;3,136.16,436.79,164.45,7.86;3,136.16,447.75,103.47,7.86;3,136.16,458.71,357.24,7.86;3,136.16,469.67,363.82,7.86;3,136.16,480.63,340.88,7.86;3,136.16,491.59,102.96,7.86"><p>&lt;?xml version="1.0"?&gt; &lt;forum type="reddit"&gt; &lt;thread id="2nw0um"&gt; &lt;category&gt;suggestmeabook&lt;/category&gt; &lt;title&gt;can anyone suggest a modern fantasy series. &lt;/title&gt; &lt;posts&gt; &lt;post id="2nw0um"&gt; &lt;author&gt;blackbonbon&lt;/author&gt; &lt;timestamp&gt;1417392344&lt;/timestamp&gt; &lt;parentid&gt; &lt;/parentid&gt; &lt;body&gt;.... where the baddy turns good, or a series similar to the broken empire trilogy. I thoroughly enjoyed reading it along with skullduggery pleasant, the saga of darren shan, the saga of lartern crepsley and the inhe ritance cycle. So whatever you got helps :D cheers lads, and lassses.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,201.12,451.78,213.11,7.89"><head>Table 2 .</head><label>2</label><figDesc>Classification of the LibraryThing Threads</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,144.73,646.84,201.60,7.86"><p>http://research.nii.ac.jp/‚àºuno/code/lcm seq.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="5,144.73,657.79,166.76,8.37"><p>LCM : Linear time Closed itemset Miner</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="6,144.73,635.88,335.86,7.86;6,144.73,646.84,143.07,7.86"><p>The Bayesian Classification represents a supervised learning method as well as a statistical method for classification.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="6,144.73,657.79,159.02,7.86"><p>http://www.cs.waikato.ac.nz/ml/weka/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="7,144.73,646.84,335.87,8.37;7,144.73,657.79,45.13,7.86"><p>We used in all experiments the NaiveBayes classifier with Weka using default configurations.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented our contribution for the 2016 Social Book Search Track, especially for the SBS Mining track. In the 6 submitted runs dedicated for book search requests classification, we tested three approaches for features selection, namely : Bag of linguistic features (i.e., nouns and verbs), compound nouns and sequences, and their combination. We performed classification with Weka with NaiveBayes classifier. We showed that combining Bag of linguistic features (i.e., nouns and verbs) and compound nouns improves accuracy, and integrating sequences in classification process enhances the performance. So, the results confirmed that the synergy between the NLP techniques (textual sequences mining and nouns phrases extraction) and the classification system is fruitful.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,148.89,472.58,46.43,7.86;8,212.61,472.58,17.02,7.86;8,405.06,472.58,61.40,7.86;8,158.69,483.91,294.55,7.89;8,157.27,495.29,127.74,7.86;8,405.06,495.29,45.00,7.86;8,157.27,506.25,39.07,7.86;8,212.61,506.25,111.69,7.86;8,405.06,506.25,45.00,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,148.89,472.58,46.43,7.86;8,212.61,472.58,17.02,7.86;8,405.06,472.58,61.40,7.86;8,158.69,483.91,243.58,7.89">Rank Team Run posts Accuracy 1 baseline character 4-grams.LinearSVC (Best run)</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
	<note>baseline Words.LinearSVC 1974 93.92 3 Know Classification-Naive-Results 1974 91</note>
</biblStruct>

<biblStruct coords="8,157.27,517.21,214.10,7.86;8,405.06,517.21,45.00,7.86;8,157.27,528.17,167.47,7.86;8,405.06,528.17,45.00,7.86;8,156.93,539.50,144.01,7.89;8,405.06,539.50,48.18,7.89;8,156.93,550.46,153.00,7.89;8,405.06,550.46,48.18,7.89;8,156.93,561.42,144.37,7.89;8,405.06,561.42,48.18,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,157.27,517.21,214.10,7.86;8,405.06,517.21,45.00,7.86;8,157.27,528.17,167.47,7.86;8,405.06,528.17,45.00,7.86;8,156.93,539.50,144.01,7.89;8,405.06,539.50,48.18,7.89;8,156.93,550.46,153.00,7.89">4 baseline character 4-grams.KNeighborsClassifier 1974 91.54 5 baseline Words.KNeighborsClassifier 1974 91.39 6 LIPAH Classification-NVC 1974 90.98 7 LIPAH Classification-NVSeq</title>
		<idno>-CSeq 1974 90.83</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,173.05,561.42,104.90,7.89">LIPAH Classification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,157.27,572.80,39.07,7.86;8,212.61,572.80,107.34,7.86;8,405.06,572.80,45.00,7.86;8,154.27,584.14,139.29,7.89;8,405.06,584.14,48.18,7.89;8,154.97,595.52,195.33,7.86;8,405.06,595.52,45.00,7.86;8,154.97,606.48,148.71,7.86;8,405.06,606.48,45.00,7.86;8,154.97,617.44,41.37,7.86;8,212.61,617.44,106.60,7.86;8,405.06,617.44,45.00,7.86;8,154.97,628.40,41.37,7.86;8,212.61,628.40,113.68,7.86;8,405.06,628.40,45.00,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,157.27,572.80,39.07,7.86;8,212.61,572.80,107.34,7.86;8,405.06,572.80,45.00,7.86;8,154.27,584.14,139.29,7.89;8,405.06,584.14,48.18,7.89;8,154.97,595.52,130.52,7.86;8,173.05,606.48,63.52,7.86">9 Know Classification-Veto-Resutls 1974 90.63 10 LIPAH Classification-NV 1974 90.53 11 baseline character 4-grams</title>
		<idno>87.59 12</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,241.37,606.48,62.31,7.86;8,405.06,606.48,45.00,7.86;8,154.97,617.44,41.37,7.86;8,212.61,617.44,106.60,7.86;8,405.06,617.44,45.00,7.86;8,154.97,628.40,41.37,7.86;8,212.61,628.40,113.68,7.86">MultinomialNB 1974 87.59 13 Know Classification-Tree-Resutls 1974 83.38 14 Know Classification-Forest-Resutls</title>
		<imprint>
			<date type="published" when="1974">1974. 1974 74</date>
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
	<note>baseline Words</note>
</biblStruct>

<biblStruct coords="9,138.35,504.34,342.24,7.86;9,146.91,515.30,333.67,7.86;9,146.91,526.26,333.68,7.86;9,146.91,537.22,58.87,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,218.01,504.34,262.58,7.86;9,146.91,515.30,54.47,7.86">French noun phrase indexing and mining for an information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">Hatem</forename><surname>Haddad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,222.32,515.30,258.26,7.86;9,146.91,526.26,100.40,7.86">String Processing and Information Retrieval, 10th International Symposium, SPIRE 2003</title>
		<meeting><address><addrLine>Manaus, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">October 8-10, 2003. 2003</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,548.19,342.24,7.86;9,146.91,559.15,333.68,7.86;9,146.91,570.11,333.68,7.86;9,146.91,581.06,333.68,7.86;9,146.91,592.02,251.77,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,378.62,548.19,101.97,7.86;9,146.91,559.15,207.56,7.86">Knowledge-Based and Intelligent Information and Engineering Systems: 14</title>
		<author>
			<persName coords=""><forename type="first">Takanobu</forename><surname>Nakahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Takeaki</forename><surname>Uno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katsutoshi</forename><surname>Yada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,337.30,559.15,143.29,7.86;9,146.91,570.11,17.31,7.86;9,316.53,570.11,164.06,7.86;9,146.91,581.06,303.47,7.86">Proceedings, Part III, chapter Extracting Promising Sequential Patterns from RFID Data Using the LCM Sequence</title>
		<meeting>Part III, chapter Extracting Promising Sequential Patterns from RFID Data Using the LCM Sequence<address><addrLine>Cardiff, UK; Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">September 8-10, 2010. 2010</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
	<note>th International Conference, KES 2010</note>
</biblStruct>

<biblStruct coords="9,138.35,602.99,342.24,7.86;9,146.91,613.95,333.67,7.86;9,146.91,624.91,65.35,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,215.52,602.99,222.93,7.86">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,457.40,602.99,23.19,7.86;9,146.91,613.95,243.61,7.86">International Conference on New Methods in Language Processing</title>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,635.88,342.25,7.86;9,146.91,646.84,37.66,7.86;9,184.57,645.07,7.19,5.24;9,196.71,646.84,283.88,7.86;9,146.91,657.79,188.73,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,264.69,635.88,148.78,7.86">Mining generalised associations rules</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.71,635.88,45.88,7.86;9,146.91,646.84,37.66,7.86;9,184.57,645.07,7.19,5.24;9,196.71,646.84,252.37,7.86">Proceedings of the 21 th International Conference on Very Large Databases, VLDB&apos;95</title>
		<meeting>the 21 th International Conference on Very Large Databases, VLDB&apos;95<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-09">September 1995</date>
			<biblScope unit="page" from="407" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,120.67,342.24,7.86;10,146.91,131.63,177.81,7.86;10,324.72,129.86,7.19,5.24;10,335.01,131.63,145.58,7.86;10,146.91,142.59,333.68,7.86;10,146.91,153.55,150.25,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,257.54,120.67,223.05,7.86;10,146.91,131.63,81.87,7.86">Mining sequential patterns : Generalizations and performance improvements</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,246.90,131.63,77.82,7.86;10,324.72,129.86,7.19,5.24;10,335.01,131.63,145.58,7.86;10,146.91,142.59,142.09,7.86">Proceedings of the 5 th International Conference on Extending Database Technology, EDBT&apos;96</title>
		<meeting>the 5 th International Conference on Extending Database Technology, EDBT&apos;96<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996-03">March 1996</date>
			<biblScope unit="volume">1057</biblScope>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
