<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.23,115.96,310.90,12.62">Overview of the SBS 2016 Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.78,153.68,63.40,8.74"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<email>marijn.koolen@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Netherlands Institute of Sound and Vision</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble MRIM</orgName>
							</affiliation>
							<affiliation key="aff11">
								<orgName type="institution">University of Neuchtel</orgName>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">Zurich University of Applied Sciences UniNe-ZHAW</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.07,153.68,56.68,8.74"><forename type="first">Toine</forename><surname>Bogers</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Aalborg University Copenhagen</orgName>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Research Center on Scientific and Technical Information CERIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.68,153.68,54.43,8.74"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff11">
								<orgName type="institution">University of Neuchtel</orgName>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">Zurich University of Applied Sciences UniNe-ZHAW</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute Acronym</orgName>
								<orgName type="institution" key="instit1">Aix-Marseille</orgName>
								<orgName type="institution" key="instit2">Universit√© CNRS (LSIS-OpenEdition</orgName>
								<orgName type="institution" key="instit3">LSIS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Chaoyang University of Technology CYUT-CSIE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Dhanbad ISMD</orgName>
								<orgName type="institution">Indian School of Mines</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Know-Center Know</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">Oslo</orgName>
								<orgName type="institution" key="instit2">Akershus University College of Applied Sciences OAUC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Amsterdam UvA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">University of Science and Technology Beijing USTB-PRIR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.23,115.96,310.90,12.62">Overview of the SBS 2016 Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A3B3685C2399376E8FE92FC4B3A3593D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the SBS 2016 Suggestion Track is to evaluate approaches for supporting users in searching collections of books who express their information needs both in a query and through example books. The track investigates the complex nature of relevance in book search and the role of traditional and user-generated book metadata in retrieval. We consolidated last year's investigation into the nature of book suggestions from the LibraryThing forums and how they compare to book relevance judgements. Participants were encouraged to incorporate rich user profiles of both topic creators and other LibraryThing users to explore the relative value of recommendation and retrieval paradigms for book search. In terms of systems evaluation, the most effective systems include ...</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the Social Book Search 2016 Suggestion Track 4 is to investigate techniques to support users in searching for books in catalogues of professional metadata and complementary social media. Towards this goal the track is building appropriate evaluation benchmarks, complete with test collections for social, semantic and focused search tasks. The track provides opportunities to explore research questions around two key areas:</p><p>-Evaluation methodologies for book search tasks that combine aspects of retrieval and recommendation, -Information retrieval techniques for dealing with professional and user-generated metadata,</p><p>The Social Book Search (SBS) 2016 Suggestion Track, framed within the scenario of a user searching a large online book catalogue for a given topic of interest, aims at exploring techniques to deal with complex information needsthat go beyond topical relevance and can include aspects such as genre, recency, engagement, interestingness, and quality of writing-and complex information Table <ref type="table" coords="2,166.42,115.92,4.46,8.77">1</ref>. Active participants of the INEX 2014 Social Book Search Track and number of contributed runs sources that include user profiles, personal catalogues, and book descriptions containing both professional metadata and user-generated content.</p><p>The Suggestion Track has been part of the SBS Lab since 2015 and is a continuation of the INEX SBS Track that ran from 2011 up to 2014. The focus is on search requests that combine a natural language description of the information need as well as example books, combining traditional ad hoc retrieval with query-by-document. The information needs are derived from the LibraryThing (LT) discussion forums. LibraryThing forum requests for book suggestions, combined with annotation of these requests resulted in a topic set of 120 topics with graded relevance judgments. A test collection is constructed around these information needs and the Amazon/LibraryThing collection, consisting of 2.8 million documents. The Suggestion Track runs in close collaboration with the SBS Interactive Track,<ref type="foot" coords="2,203.57,509.21,3.97,6.12" target="#foot_0">5</ref> which is a user-centered track where interfaces are developed and evaluated and user interaction is analysed to investigate how book searchers make use of professional metadata and user-generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Participating Organisations</head><p>A total of 29 organisations registered for the track, with 10 teams submitting runs (see Table <ref type="table" coords="2,180.19,620.36,3.87,8.74">1</ref>). In 2015 25 teams registered and 10 submitted runs, so participation is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Suggestion Track Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Track Goals and Background</head><p>The goal of the Suggestion Track is to evaluate the value of professional metadata and user-generated content for book search on the Web and to develop and evaluate systems that can deal with both retrieval and recommendation aspects, where the user has a specific information need against a background of personal tastes, interests and previously seen books.</p><p>Through social media, book descriptions have extended far beyond what is traditionally stored in professional catalogues. Not only are books described in the users' own vocabulary, but are also reviewed and discussed online, and added to online personal catalogues of individual readers. This additional information is subjective and personal, and opens up opportunities to aid users in searching for books in different ways that go beyond the traditional editorial metadata based search scenarios, such as known-item and subject search. For example, readers use many more aspects of books to help them decide which book to read next <ref type="bibr" coords="3,157.97,334.50,62.39,8.74" target="#b3">(Reuter, 2007)</ref>, such as how engaging, fun, educational or well-written a book is. In addition, readers leave a trail of rich information about themselves in the form of online profiles, which contain personal catalogues of the books they have read or want to read, personally assigned tags and ratings for those books and social network connections to other readers. This results in a search task that may require a different model than traditional ad hoc search <ref type="bibr" coords="3,446.00,394.27,34.60,8.74;3,134.77,406.23,52.03,8.74" target="#b1">(Koolen et al., 2012)</ref> or recommendation.</p><p>The SBS track investigates book requests and suggestions from the Library-Thing (LT) discussion forums as a way to model book search in a social environment. The discussions in these forums show that readers frequently turn to others to get recommendations and tap into the collective knowledge of a group of readers interested in the same topic.</p><p>The track builds on the INEX Amazon/LibraryThing (A/LT) collection <ref type="bibr" coords="3,134.77,492.87,91.42,8.74" target="#b0">(Beckers et al., 2010)</ref>, which contains 2.8 million book descriptions from Amazon, enriched with content from LT. This collection contains both professional metadata and user-generated content.</p><p>The SBS Suggestion Track aims to address the following research questions:</p><p>-Can we build reliable and reusable test collections for social book search based on book requests and suggestions from the LT discussion forums? -Can user profiles provide a good source of information to capture personal, affective aspects of book search information needs? -How can systems incorporate both specific information needs and general user profiles to combine the retrieval and recommendation aspects of social book search? -What is the relative value of social and controlled book metadata for book search?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scenario</head><p>The scenario is that of a user turning to Amazon Books and LT to find books to read, to buy or to add to their personal catalogue. Both services host large collaborative book catalogues that may be used to locate books of interest.</p><p>On LT, users can catalogue the books they read, manually index them by assigning tags, and write reviews for others to read. Users can also post messages on discussion forums asking for help in finding new, fun, interesting, or relevant books to read. The forums allow users to tap into the collective bibliographic knowledge of hundreds of thousands of book enthusiasts. On Amazon, users can read and write book reviews and browse to similar books based on links such as "customers who bought this book also bought... ".</p><p>Users can search online book collections with different intentions. They can search for specific known books with the intention of obtaining them (buy, download, print). Such needs are addressed by standard book search services as offered by Amazon, LT and other online bookshops as well as traditional libraries. In other cases, users search for a specific, but unknown, book with the intention of identifying it. Another possibility is that users are not looking for a specific book, but hope to discover one or more books meeting some criteria. These criteria can be related to subject, author, genre, edition, work, series or some other aspect, but also more serendipitously, such as books that merely look interesting or fun to read or that are similar to a previously read book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task description</head><p>The task is to reply to a user request posted on a LT forum (see Section 4.1) by returning a list of recommended books matching the user's information need. More specifically, the task assumes a user who issues a query to a retrieval system, which then returns a (ranked) list of relevant book records. The user is assumed to inspect the results list starting from the top, working down the list until the information need has been satisfied or until the user gives up. The retrieval system is expected to order the search results by relevance to the user's information need.</p><p>The user's query can be a number of keywords, but also one or more book records as positive or negative examples. In addition, the user has a personal profile that may contain information on the user's interests, list of read books and connections with other readers. User requests may vary from asking for books on a particular genre, looking for books on a particular topic or period or books written in a certain style. The level of detail also varies, from a brief statement to detailed descriptions of what the user is looking for. Some requests include examples of the kinds of books that are sought by the user, asking for similar books. Other requests list examples of known books that are related to the topic, but are specifically of no interest. The challenge is to develop a retrieval method that can cope with such diverse requests.</p><p>The books must be selected from a corpus that consists of a collection of curated and social book metadata, extracted from Amazon Books and LT, extended with associated records from library catalogues of the Library of Congress and the British Library (see the next section). Participants of the Suggestion track are provided with a set of book search requests and user profiles and are asked to submit the results returned by their systems as ranked lists.</p><p>The track thus combines aspects from retrieval and recommendation. On the one hand the task is akin to directed search familiar from information retrieval, with the requirement that returned books should be topically relevant to the user's information need described in the forum thread. On the other hand, users may have particular preferences for writing style, reading level, knowledge level, novelty, unusualness, presence of humorous elements and possibly many other aspects. These preferences are to some extent reflected by the user's reading profile, represented by the user's personal catalogue. This catalogue contains the books already read or earmarked for future reading, and may contain personally assigned tags and ratings. Such preferences and profiles are typical in recommendation tasks, where the user has no specific information need, but is looking for suggestions of new items based on previous preferences and history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submission Format</head><p>Participants are asked to return a ranked list of books for each user query, ranked by order of relevance, where the query is described in the LT forum thread. We adopt the submission format of TREC, with a separate line for each retrieval result (i.e., book), consisting of six columns:</p><p>1. topic id: the topic number, which is based on the LT forum thread number. 2. Q0: the query number. Unused, so should always be Q0. 3. isbn: the ISBN of the book, which corresponds to the file name of the book description. 4. rank: the rank at which the document is retrieved. 5. rsv: retrieval status value, in the form of a score. For evaluation, results are ordered by descending score. 6. run id: a code to identify the participating group and the run.</p><p>Participants are allowed to submit up to six runs, of which at least one should use only the title field of the topic statements (the topic format is described in Section 4.1). For the other five runs, participants could use any field in the topic statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Test Collection</head><p>We use and extend the Amazon/LibraryThing (A/LT) corpus crawled by the University of Duisburg-Essen for the INEX Interactive Track <ref type="bibr" coords="5,413.28,608.30,67.31,8.74;5,134.77,620.25,22.14,8.74" target="#b0">(Beckers et al., 2010)</ref>. The corpus contains a large collection of book records with controlled subject headings and classification codes as well as social descriptions, such as tags and reviews. See https://inex.mmci.uni-saarland.de/data/nd-agreements.jsp for information on how to gain access to the corpus. The collection consists of 2.8 million book records from Amazon, extended with social metadata from LT. This set represents the books available through Amazon. Each book is identified by an ISBN. Note that since different editions of the same work have different ISBNs, there can be multiple records for a single intellectual work. Each book record is an XML file with formal metadata fields like isbn, title, author, publisher, dimensions, numberofpages and publicationdate. Curated metadata comes in the form of a Dewey Decimal Classification in the dewey field, Amazon subject headings in the subject field, and Amazon category labels in the browseNode fields. The social metadata from Amazon and LT is stored in the tag, rating, and review fields. The full list of fields is shown in Table <ref type="table" coords="6,162.16,506.52,3.87,8.74" target="#tab_0">2</ref>.</p><p>To ensure that there is enough high-quality metadata from traditional library catalogues, we extended the A/LT data set with library catalogue records from the Library of Congress (LoC) and the British Library (BL). We only use library records of ISBNs that are already in the A/LT collection. These records contain formal metadata such as title information (book title, author, publisher, etc.), classification codes (mainly DDC and LCC) and rich subject headings based on the Library of Congress Subject Headings (LCSH).<ref type="foot" coords="6,358.15,591.71,3.97,6.12" target="#foot_1">6</ref> Both the LoC records and the BL records are in MARCXML<ref type="foot" coords="6,284.90,603.67,3.97,6.12" target="#foot_2">7</ref> format. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Information needs</head><p>LT users discuss their books on the discussion forums. Many of the topic threads are started with a request from a member for interesting, fun new books to read. Users typically describe what they are looking for, give examples of what they like and do not like, indicate which books they already know and ask other members for recommendations. Members often reply with links to works catalogued on LT, which, in turn, have direct links to the corresponding records on Amazon. These requests for recommendations are natural expressions of information needs for a large collection of online book records. We use a sample of these forum topics to evaluate systems participating in the Suggestion Track.</p><p>Each topic has a title and is associated with a group on the discussion forums. For instance, topic 99309 in Figure <ref type="figure" coords="7,294.07,536.57,4.98,8.74" target="#fig_0">1</ref> has the title Politics of Multiculturalism Recommendations? and was posted in the group Political Philosophy. The books suggested by members in the thread are collected in a list on the side of the topic thread (see Figure <ref type="figure" coords="7,219.26,572.43,3.87,8.74" target="#fig_0">1</ref>). A feature called touchstone can be used by members to easily identify books they mention in the topic thread, giving other readers of the thread direct access to a book record in LT, with associated ISBNs and links to Amazon. We use these suggested books as initial relevance judgements for evaluation. In the rest of this paper, we use the term suggestion to refer to a book that has been identified in a touchstone list for a given forum topic. Since all suggestions are made by forum members, we assume they are valuable judgements on the relevance of books. Additional relevance information can be gleaned from the discussions on the threads. Consider, for example, topic 129939<ref type="foot" coords="8,427.97,117.42,3.97,6.12" target="#foot_3">8</ref> . The topic starter first explains what sort of books he is looking for, and which relevant books he has already read or is reading. Other members post responses with book suggestions. The topic starter posts a reply describing which suggestions he likes and which books he has ordered and plans to read. Later on, the topic starter provides feedback on the suggested books that he has now read. Such feedback can be used to estimate the relevance of a suggestion to the user.</p><p>In the following, we first describe the topic selection and annotation procedure, then how we used the annotations to assign relevance values to the suggestions, and finally the user profiles, which were then provided with each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic selection</head><p>The topic set of 2016 is a newly selected set of topics from the LibraryThing discussion forums. A total of 2000 topic threads were assessed on whether they contain a book search request by four judges, with 272 threads labelled as book search requests. To establish inter-annotator agreement, 453 threads were double-assessed, resulting a Cohen's Kappa of 0.83. Judges strongly agree on which posts are book search requests and which are not. Of these 272 book search requests, 124 (46%) are known-item searches from the Name that Book discussion group. Here, LT members start a thread to describe a book they know but cannot remember the title and author of and ask others for help. In earlier work we found that known-item topics behave very differently from the other topic types ( <ref type="bibr" coords="8,216.15,391.25,86.23,8.74" target="#b2">(Koolen et al., 2015)</ref>). We remove these topics from the topic set so that they do not dominate the performance comparison. Furthermore, we removed topics that have no book suggestions by other LT members and topics for which we have no user profile of the topic starter, resulting in a topic set of 120 topics for evaluation of the 2016 Suggestion Track.</p><p>Topics are distributed to participants in XML format with as richly annotated requests. As an example, the topic in Figure <ref type="figure" coords="8,327.05,461.53,4.98,8.74" target="#fig_0">1</ref>  Suggestion provided by other LT members are often marked up in touchstones so they are easy to extract and use as relevance judgements. As for the 2015 Suggestion Track, we manually annotated all book suggestions that were not marked up as touchstones by LT members to provide a more complete recall base.</p><p>Operationalisation of forum judgement labels In previous years the Suggestion Track used a complicated decision tree to derive a relevance value from a suggestion. To reduce the number of assumptions, we simplified the mapping of book suggestions to relevance values. By default a suggested book has a relevance value of 1. Books that the requester already has in her personal catalogue before starting the thread (pre-catalogued suggestions) have little additional value are assumed to have a relevance value of 0. On the other hand, suggestions that the requester subsequently adds to her catalogue (post-catalogued suggestions) are assumed to be the most relevant suggestions and receive a relevance value of 8, to keep that relevance level the same is in 2014 and 2015. Note that some of the books mentioned in the forums are not part of the 2.8 million books in our collection. These suggestions removed from the suggestions any books that are not in the INEX A/LT collection. The numbers reported in the previous section were calculated after this filtering step.</p><p>User profiles and personal catalogues From LT we can not only extract the information needs of social book search topics, but also the rich user profiles of the topic creators and other LT users, which contain information on which books they have in their personal catalogue on LT, which ratings and tags they assigned to them and a social network of friendship relations, interesting library relations and group memberships. These profiles may provide important signals on the user's topical and genre interests, reading level, which books they already know and which ones they like and don't like. These profiles were scraped from the LT site, anonymised and made available to participants. This allows Track participants to experiment with combinations of retrieval and recommender systems. One of the research questions of the SBS task is whether this profile information can help systems in identifying good suggestions.</p><p>Although the user expresses her information need in some detail in the discussion forum, she may not describe all aspects she takes into consideration when selecting books. This may partly be because she wants to explore different options along different dimensions and therefore leaves some room for different interpretations of her need. Another reason might be that some aspects are not related directly to the topic at hand but may be latent factors that she takes into account with selecting books in general.</p><p>To anonymise all user profiles, we first removed all friendship and group membership connections and replaced the user name with a randomly generated string. The cataloguing date of each book was reduced to the year and month. What is left is an anonymised user name, book ID, month of cataloguing, rating and tags. We distributed a set of 94,656 user profiles containing over 33 million transactions.</p><p>ISBNs and Intellectual Works Each record in the collection corresponds to an ISBN, and each ISBN corresponds to a particular intellectual work. An intellectual work can have different editions, each with their own ISBN. The ISBN-to-work relation is a many-to-one relation. In many cases, we assume the user is not interested in all the different editions, but in different intellectual works. For evaluation we collapse multiple ISBN to a single work. The highest ranked ISBN is evaluated and all lower ranked ISBNs of the same work ignored. Although some of the topics on LibraryThing are requests to recommend a particular edition of a work-in which case the distinction between different ISBNs for the same work are important-we ignore these distinctions to make evaluation easier. This turns edition-related topics into known-item topics.</p><p>However, one problem remains. Mapping ISBNs of different editions to a single work is not trivial. Different editions may have different titles and even have different authors (some editions have a foreword by another author, or a translator, while others have not), so detecting which ISBNs actually represent the same work is a challenge. We solve this problem by using mappings made by the collective work of LibraryThing members. LT members can indicate that two books with different ISBNs are actually different manifestations of the same intellectual work. Each intellectual work on LibraryThing has a unique work ID, and the mappings from ISBNs to work IDs is made available by LibraryThing.<ref type="foot" coords="11,476.12,129.37,3.97,6.12" target="#foot_4">9</ref> </p><p>The mappings are not complete and might contain errors. Furthermore, the mappings form a many-to-many relationship, as two people with the same edition of a book might independently create a new book page, each with a unique work ID. It takes time for members to discover such cases and merge the two work IDs, which means that at any time, some ISBNs map to multiple work IDs even though they represent the same intellectual work. LibraryThing can detect such cases but, to avoid making mistakes, leaves it to members to merge them. The fraction of works with multiple ISBNs is small so we expect this problem to have a negligible impact on evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>This year, 10 teams submitted a total of 46 runs (see Table <ref type="table" coords="11,388.00,300.56,3.87,8.74">1</ref>). The evaluation results shown in Table <ref type="table" coords="11,223.89,312.52,3.87,8.74" target="#tab_2">3</ref>. The official evaluation measure for this task is nDCG@10. It takes graded relevance values into account and is designed for evaluation based on the top retrieved results. In addition, P@10, MAP and MRR scores are also reported.</p><p>The best runs of the top 5 groups are described below:</p><p>1. USTB-PRIR -run1.keyQuery active combineRerank (rank 1): This run was made by a searching-re-ranking process where the ini-tial retrieval result was based on the selection of query keywords and a small index of active books, the re-ranking results based on a combination of several strategies (number of people who read the book from profile, similar-product from amazon.com, popularity from LT forum, etc.). At indexing time, the collection is enriched with book metadata from other sources, particularly books that have little metadata from Amazon and LibraryThing. The full topic starter post in the &lt;request&gt; field is filtered based key word lists from topics of the SBS Tracks of 2011-2015. The top 1000 retrieval results of a Language Model with default parameters is re-ranked using a number of query-independent features. 2. CERIST -all features (rank 7): The topic statement in the request field is treated as a verbose query and is reduced using several features based on term statistics, Part-Of-Speech tagging, and whether terms from the request field occur in the user profile and example books. 3. CYUT-CSIE -0.95Averageword2vecType2TGR (rank 11): This run uses query expansion based on word embeddings using word2vec, on top of a standard Lucene index and retrieval model. For this run, queries are represented by a combination of the title, group and request fields. Results are re-ranked using a linear combination of the original retrieval score and the average Amazon user ratings of the retrieved books. Codes are replaced by their textual representation. Default retrieval parameters are used, the query is a combination of the topic title, group and request fields. This is the same index that is used for the experimental system of the Interactive Track and serves as a baseline for the Suggestion Track. 5. MRIM -RUN2 (rank 18): This run is a weighted linear fusion of a BM25F run on all fields, an Language Model (LM) run on all fields, and two query expansion runs, based on the BM25 and LM run respectively, using as expansion terms an intersection of terms in the user profiles and word embeddings from the query terms.</p><p>Most of the top performing systems, including the top performing run preprocess the rich topic statement with the aim of reducing the request to a set of most relevant terms. Two of the top five teams use the user profiles to modify the topic statement. This is the first year that word embeddings are used for the Suggestion Track and not without success. Both CYUT-CSIE and MRIM found that word embeddings improved performance over configurations without them. From these results it seems clear that topic representation is an important aspect in social book search. The longer narrative of the request field as well as the metadata in the user profiles and example books contain important information regarding the information need, but many terms are noisy, so a filtering step is essential to focus on the user's specific needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Plans</head><p>This was the second year of the SBS Suggestion Track as part of the SBS Lab. The overall goal remains to investigate the relative value of professional metadata, user-generated content and user profiles, but the specific focus for this year is to construct a test collection to evaluate systems dealing with complex book search requests that combine an information need expressed in a natural language statement and through example books.</p><p>We kept the evaluation procedure the same. Like last year, we added annotated example books with each topic statement, so that participants can investigate the value of query-by-example techniques in combination with more traditional text-based queries. Whereas in 2015 we only provided the book ID of example books in the topic statement, this year we also provided book title and author name.</p><p>The evaluation has shown that the most effective systems either adopt a learning-to-rank approach or incorporate keywords from the example books in the textual query. The effectiveness of learning-to-rank approaches suggests the complexity of dealing with multiple sources of evidence-book descriptions by multiple authors, differing in nature from controlled vocabulary descriptors, freetext tags and full-text reviews and information needs and interests represented by both natural language statements and user profiles-requires optimizing parameters through observing users' interactions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,328.33,345.83,8.77;7,134.77,340.31,44.33,8.74;7,134.77,115.83,342.37,197.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A topic thread in LibraryThing, with suggested books listed on the right hand side.</figDesc><graphic coords="7,134.77,115.83,342.37,197.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,136.16,115.92,305.72,234.99"><head>Table 2 .</head><label>2</label><figDesc>A list of all element names in the book descriptions</figDesc><table coords="6,136.16,140.76,285.50,210.15"><row><cell></cell><cell cols="2">tag name</cell><cell></cell></row><row><cell>book</cell><cell cols="2">similarproducts title</cell><cell>imagecategory</cell></row><row><cell>dimensions</cell><cell>tags</cell><cell>edition</cell><cell>name</cell></row><row><cell>reviews</cell><cell>isbn</cell><cell>dewey</cell><cell>role</cell></row><row><cell cols="2">editorialreviews ean</cell><cell>creator</cell><cell>blurber</cell></row><row><cell>images</cell><cell>binding</cell><cell>review</cell><cell>dedication</cell></row><row><cell>creators</cell><cell>label</cell><cell>rating</cell><cell>epigraph</cell></row><row><cell>blurbers</cell><cell>listprice</cell><cell>authorid</cell><cell>firstwordsitem</cell></row><row><cell>dedications</cell><cell>manufacturer</cell><cell>totalvotes</cell><cell>lastwordsitem</cell></row><row><cell>epigraphs</cell><cell>numberofpages</cell><cell>helpfulvotes</cell><cell>quotation</cell></row><row><cell>firstwords</cell><cell>publisher</cell><cell>date</cell><cell>seriesitem</cell></row><row><cell>lastwords</cell><cell>height</cell><cell>summary</cell><cell>award</cell></row><row><cell>quotations</cell><cell>width</cell><cell>editorialreview</cell><cell>browseNode</cell></row><row><cell>series</cell><cell>length</cell><cell>content</cell><cell>character</cell></row><row><cell>awards</cell><cell>weight</cell><cell>source</cell><cell>place</cell></row><row><cell>browseNodes</cell><cell>readinglevel</cell><cell>image</cell><cell>subject</cell></row><row><cell>characters</cell><cell>releasedate</cell><cell cols="2">imageCategories similarproduct</cell></row><row><cell>places</cell><cell cols="2">publicationdate url</cell><cell>tag</cell></row><row><cell>subjects</cell><cell>studio</cell><cell>data</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,461.53,345.83,183.32"><head></head><label></label><figDesc>(topic 99309)  would look like this:The hyperlink markup, represented by the &lt;a&gt; tags, is added by the Touchstone technology of LT. The rest of the markup is generated specifically for the Suggestion Track. Above, the book with work ID 164382 is annotated as an example of what the requester is looking for.</figDesc><table coords="8,134.77,483.96,343.64,160.89"><row><cell>&lt;a href="/author/nussbaum"&gt;Nussbaum&lt;/a&gt;, still don't feel like I've</cell></row><row><cell>wrapped my little brain around the issue very well and would</cell></row><row><cell>appreciate any suggestions for further anyone might offer.</cell></row><row><cell>&lt;/narrative&gt;</cell></row><row><cell>&lt;examples&gt;</cell></row><row><cell>&lt;example&gt;</cell></row><row><cell>&lt;workid&gt;164382&lt;/workid&gt;</cell></row><row><cell>&lt;booktitle&gt;Rethinking Multiculturalism: Cultural Diversity and Political Theory&lt;/booktitle&gt;</cell></row><row><cell>&lt;author&gt;Bhikhu Parekh&lt;/author&gt;</cell></row><row><cell>&lt;/example&gt;</cell></row><row><cell>&lt;/examples&gt;</cell></row><row><cell>&lt;catalog&gt;</cell></row><row><cell>&lt;work&gt;</cell></row><row><cell>&lt;workid&gt;9036&lt;/workid&gt;</cell></row><row><cell>&lt;booktitle&gt;The Confessions of St. Augustine&lt;/booktitle&gt;</cell></row><row><cell>&lt;author&gt;Saint Augustine, Bishop of Hippo&lt;/author&gt;</cell></row><row><cell>&lt;publication-year&gt;397&lt;/publication-year&gt;</cell></row><row><cell>&lt;cataloguing-date&gt;2007-09&lt;/cataloguing-date&gt;</cell></row><row><cell>&lt;rating&gt;0.0&lt;/rating&gt;</cell></row><row><cell>&lt;tags&gt;&lt;/tags&gt;</cell></row><row><cell>&lt;/work&gt;</cell></row><row><cell>&lt;work&gt;</cell></row><row><cell>...</cell></row><row><cell>&lt;topic&gt;</cell></row><row><cell>&lt;topicid&gt;99309&lt;/topicid&gt;</cell></row><row><cell>&lt;query&gt;Politics of Multiculturalism&lt;/query&gt;</cell></row><row><cell>&lt;title&gt;Politics of Multiculturalism Recommendations?&lt;/title&gt;</cell></row><row><cell>&lt;group&gt;Political Philosophy&lt;/group&gt;</cell></row><row><cell>&lt;request&gt; I'm new, and would appreciate any recommended reading on</cell></row><row><cell>the politics of multiculturalism. &lt;a href="/author/parekh"&gt;Parekh</cell></row><row><cell>&lt;/a&gt;'s &lt;a href="/work/164382"&gt;Rethinking Multiculturalism: Cultural</cell></row><row><cell>Diversity and Political Theory&lt;/a&gt; (which I just finished) in the end</cell></row><row><cell>left me unconvinced, though I did find much of value I thought he</cell></row><row><cell>depended way too much on being able to talk out the details later. It</cell></row><row><cell>may be that I found his writing style really irritating so adopted a</cell></row><row><cell>defiant skepticism, but still... Anyway, I've read</cell></row><row><cell>&lt;a href="/author/sen"&gt;Sen&lt;/a&gt;, &lt;a href="/author/rawles"&gt;Rawls&lt;/a&gt;,</cell></row><row><cell>&lt;a href="/author/habermas"&gt;Habermas&lt;/a&gt;, and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,134.77,181.70,345.82,411.89"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for the official submissions. Best scores are in bold. Runs marked with * are manual runs. UvA-ILLC -base es (rank 17): This run is based on a full-text elasticsearch (Elasticsearch) index of the A/LT collection, where the Dewey Decimal</figDesc><table coords="12,136.16,215.85,338.69,377.74"><row><cell>Group</cell><cell>Run</cell><cell>ndcg@10 P@10</cell><cell>mrr</cell><cell>map</cell></row><row><cell cols="2">USTB-PRIR run1.keyQuery active combineRerank</cell><cell cols="3">0.2157 0.5247 0.1253 0.3474</cell></row><row><cell cols="2">USTB-PRIR run2.keyQuery active userNumRerank</cell><cell cols="3">0.2047 0.4700 0.1177 0.3474</cell></row><row><cell cols="2">USTB-PRIR run6.keyQuery AllRerank</cell><cell cols="3">0.2030 0.4868 0.1144 0.3146</cell></row><row><cell cols="2">USTB-PRIR run5.keyQuery readByOne</cell><cell cols="3">0.2009 0.4767 0.1128 0.3146</cell></row><row><cell cols="2">USTB-PRIR run3.keyQuery active readByOneReRank</cell><cell cols="3">0.1989 0.4923 0.1157 0.3474</cell></row><row><cell cols="2">USTB-PRIR run4.keyQuery active similarRerank</cell><cell cols="3">0.1935 0.4685 0.1106 0.3474</cell></row><row><cell>CERIST</cell><cell>all features</cell><cell cols="3">0.1567 0.3513 0.0838 0.4330</cell></row><row><cell>CERIST</cell><cell>all no field feature</cell><cell cols="3">0.1438 0.3275 0.0754 0.3993</cell></row><row><cell>CERIST</cell><cell>all with filter</cell><cell cols="3">0.1418 0.3335 0.0780 0.3961</cell></row><row><cell>CERIST</cell><cell>stat ling features</cell><cell cols="3">0.1290 0.2970 0.0816 0.4560</cell></row><row><cell cols="2">CYUT-CSIE 0.95Averageword2vecType2TGR</cell><cell cols="3">0.1158 0.2563 0.0563 0.1603</cell></row><row><cell cols="2">CYUT-CSIE 0.95AverageType2TGR</cell><cell cols="3">0.1137 0.2718 0.0572 0.1626</cell></row><row><cell cols="2">CYUT-CSIE word2vecType2TGR</cell><cell cols="3">0.1107 0.2479 0.0542 0.1614</cell></row><row><cell>CERIST</cell><cell>stat features</cell><cell cols="3">0.1082 0.2279 0.0749 0.4326</cell></row><row><cell>CERIST</cell><cell>topic profil features</cell><cell cols="3">0.1077 0.2635 0.0627 0.4368</cell></row><row><cell cols="2">CYUT-CSIE Type2TGR</cell><cell cols="3">0.1060 0.2545 0.0550 0.1635</cell></row><row><cell>UvA-ILLC</cell><cell>base es</cell><cell cols="3">0.0944 0.2272 0.0548 0.3122</cell></row><row><cell>MRIM</cell><cell>RUN2</cell><cell cols="3">0.0889 0.1889 0.0518 0.3491</cell></row><row><cell>MRIM</cell><cell>RUN6</cell><cell cols="3">0.0872 0.1914 0.0538 0.3652</cell></row><row><cell>MRIM</cell><cell>RUN3</cell><cell cols="3">0.0872 0.1914 0.0538 0.3652</cell></row><row><cell>MRIM</cell><cell>RUN1</cell><cell cols="3">0.0864 0.1858 0.0529 0.3654</cell></row><row><cell>MRIM</cell><cell>RUN5</cell><cell cols="3">0.0861 0.1866 0.0525 0.3652</cell></row><row><cell>MRIM</cell><cell>RUN4</cell><cell cols="3">0.0861 0.1866 0.0524 0.3652</cell></row><row><cell>ISMD</cell><cell>ISMD16allfieds</cell><cell cols="3">0.0765 0.1722 0.0342 0.2157</cell></row><row><cell cols="2">UniNe-ZHAW Pages INEXSBS2016 SUM SCORE</cell><cell cols="3">0.0674 0.1512 0.0472 0.2556</cell></row><row><cell cols="2">UniNe-ZHAW RatingsPagesPrice INEXSBS2016 SUM SCORE</cell><cell cols="3">0.0667 0.1499 0.0462 0.2556</cell></row><row><cell cols="2">UniNe-ZHAW PagesPrice INEXSBS2016 SUM SCORE</cell><cell cols="3">0.0665 0.1442 0.0461 0.2556</cell></row><row><cell>ISMD</cell><cell>ISMD16titlefield</cell><cell cols="3">0.0639 0.1197 0.0333 0.1933</cell></row><row><cell>ISMD</cell><cell>ISMD16requestfield</cell><cell cols="3">0.0613 0.1454 0.0287 0.1870</cell></row><row><cell cols="2">UniNe-ZHAW Ratings INEXSBS2016 SUM SCORE</cell><cell cols="3">0.0584 0.1332 0.0419 0.2556</cell></row><row><cell cols="2">UniNe-ZHAW INEXSBS2016</cell><cell cols="3">0.0561 0.1251 0.0396 0.2556</cell></row><row><cell cols="2">UniNe-ZHAW Price INEXSBS2016 SUM SCORE</cell><cell cols="3">0.0542 0.1114 0.0386 0.2556</cell></row><row><cell>ISMD</cell><cell>ISMD16titlewithoutreranking</cell><cell cols="3">0.0531 0.1329 0.0355 0.1933</cell></row><row><cell>LSIS</cell><cell>Run1 ExeOrNarrativeNSW Collection</cell><cell cols="3">0.0450 0.1166 0.0251 0.2050</cell></row><row><cell>ISMD</cell><cell>similaritytitlefieldreranked</cell><cell cols="3">0.0445 0.0966 0.0307 0.1933</cell></row><row><cell>CYUT</cell><cell>0.95RatingType2TGR</cell><cell cols="3">0.0392 0.1363 0.0145 0.1089</cell></row><row><cell>CYUT</cell><cell>0.95Ratingword2vecType2TGR</cell><cell cols="3">0.0373 0.1265 0.0136 0.1055</cell></row><row><cell>LSIS</cell><cell>Run2 ExeOrNarrativeNSW UserProfile</cell><cell cols="3">0.0239 0.1018 0.0144 0.1742</cell></row><row><cell>OAU</cell><cell>oauc reranked ownQueryModel</cell><cell cols="3">0.0228 0.0766 0.0127 0.1265</cell></row><row><cell>OAU</cell><cell>oauc basic</cell><cell cols="3">0.0217 0.0778 0.0118 0.1265</cell></row><row><cell>LSIS</cell><cell>Run3 ExeOrNarrativeNSW Collection AddData</cell><cell cols="3">0.0177 0.0533 0.0101 0.2050</cell></row><row><cell>LSIS</cell><cell>Run4 ExeOrNarrativeNSW UserProfile AddData</cell><cell cols="3">0.0152 0.0566 0.0079 0.1742</cell></row><row><cell>ISMD</cell><cell>ISMD16groupfield</cell><cell cols="3">0.0104 0.0527 0.0069 0.0564</cell></row><row><cell>know</cell><cell>sbs16suggestiontopicsresult2</cell><cell cols="3">0.0058 0.0227 0.0010 0.0013</cell></row><row><cell>OAU</cell><cell>oauc reranked attachedWorksModel</cell><cell cols="3">0.0044 0.0081 0.0021 0.1265</cell></row><row><cell>know</cell><cell>sbs16suggestiontopicsresult1</cell><cell cols="3">0.0018 0.0084 0.0004 0.0004</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,144.73,656.80,285.70,8.12"><p>See http://social-book-search.humanities.uva.nl/#/interactive</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="6,144.73,634.88,304.71,8.12"><p>For more information see: http://www.loc.gov/aba/cataloging/subject/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="6,144.73,645.84,335.87,8.12;6,144.73,657.44,122.39,7.47"><p>MARCXML is an XML version of the well-known MARC format. See: http://www. loc.gov/standards/marcxml/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="8,144.73,656.80,213.38,8.12"><p>URL: http://www.librarything.com/topic/129939</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4" coords="11,144.73,656.80,254.32,8.12"><p>See: http://www.librarything.com/feeds/thingISBN.xml.gz</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Next year, we continue this focus on complex topics with example books and consider including an recommender systems type evaluation. We are also thinking of a pilot task in which the system not only has to retrieve relevant and recommendable books, but also to select which part of the book descriptione.g. a certain set of reviews or tags-is most useful to show to the user, given her information need.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="14,134.77,234.56,345.83,8.74;14,144.73,246.52,335.87,8.74;14,144.73,258.47,335.86,8.74;14,144.73,270.43,335.86,8.74;14,134.77,282.38,206.52,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,419.61,234.56,60.98,8.74;14,144.73,246.52,169.83,8.74">Overview and results of the inex 2009 interactive track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pharo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nordlie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Fachry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,322.50,258.47,24.34,8.74">ECDL</title>
		<title level="s" coord="14,421.97,258.47,58.62,8.74;14,144.73,270.43,87.72,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Frommholz</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010. 2015</date>
			<biblScope unit="volume">6273</biblScope>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
	<note>Elasticsearch. Elasticsearch, version 2.1</note>
</biblStruct>

<biblStruct coords="14,134.77,294.34,345.82,8.74;14,144.73,306.29,335.86,8.74;14,144.73,318.25,335.86,8.74;14,144.73,330.20,118.04,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,303.69,294.34,176.90,8.74;14,144.73,306.29,261.78,8.74">Social Book Search: The Impact of Professional and User-Generated Content on Book Suggestions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,430.93,306.29,49.65,8.74;14,144.73,318.25,335.86,8.74;14,144.73,330.20,57.88,8.74">Proceedings of the International Conference on Information and Knowledge Management (CIKM 2012)</title>
		<meeting>the International Conference on Information and Knowledge Management (CIKM 2012)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,134.77,342.16,345.82,8.74;14,144.73,354.11,335.86,8.74;14,144.73,366.07,335.86,8.74;14,144.73,378.02,335.86,8.74;14,144.73,389.98,335.86,8.74;14,144.73,401.93,335.86,9.02;14,144.73,414.60,219.20,8.30" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,389.44,342.16,91.14,8.74;14,144.73,354.11,221.49,8.74">Looking for books in social media: An analysis of complex search requests</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kamps</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-16354-3_19</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-16354-3_19" />
	</analytic>
	<monogr>
		<title level="m" coord="14,296.99,366.07,183.60,8.74;14,144.73,378.02,221.00,8.74">Advances in Information Retrieval -37th European Conference on IR Research, ECIR 2015</title>
		<title level="s" coord="14,360.67,389.98,119.91,8.74;14,144.73,401.93,30.49,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04-02">March 29 -April 2, 2015. 9022. 2015</date>
			<biblScope unit="page" from="184" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,134.77,425.84,345.82,8.74;14,144.73,437.80,181.25,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,186.44,425.84,294.15,8.74;14,144.73,437.80,27.90,8.74">Assessing aesthetic relevance: Children&apos;s book selection in a digital library</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Reuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,181.03,437.80,32.21,8.74">JASIST</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1745" to="1763" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
