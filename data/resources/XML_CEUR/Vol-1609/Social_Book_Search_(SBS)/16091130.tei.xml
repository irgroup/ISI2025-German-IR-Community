<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.42,116.95,256.53,12.62;1,173.55,134.89,268.26,12.62">SOCIAL BOOK SEARCH TRACK: ISM@CLEF&apos;16 SUGGESTION TASK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.47,172.56,58.88,8.74"><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian School of Mines Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.22,172.56,81.57,8.74"><forename type="first">Guggilla</forename><surname>Bhanodai</surname></persName>
							<email>bhanodaig@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian School of Mines Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.48,172.56,76.41,8.74"><forename type="first">Rajendra</forename><surname>Pamula</surname></persName>
							<email>rajendrapamula@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian School of Mines Dhanbad</orgName>
								<address>
									<postCode>826004</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.42,116.95,256.53,12.62;1,173.55,134.89,268.26,12.62">SOCIAL BOOK SEARCH TRACK: ISM@CLEF&apos;16 SUGGESTION TASK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEAA523EE4C264A7F14130543761DACB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Book Search</term>
					<term>Social Book Search</term>
					<term>Language modeling</term>
					<term>Information Retrieval</term>
					<term>re-ranking</term>
					<term>Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the work that we did at Indian School of Mines towards Social Book Search Track for CLEF 2016. As per requirement of CLEF-2016 we submitted six runs in its Suggestion Task. We investigated individual effect of title, group, request, as well as combined effect of title, request and group fields of the topics in our runs. For all the runs we used language modeling technique with Dirichlet smoothing. The run using combined effect of title, request and group field was our best. Overall, our performance is good but it needs some improvement, our scores are encouraging enough to work for better results in future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With growing numbers of online portals and book catalogues, our current time sees a rapid evolution in the way we acquire, share and use books. In order to enable users for searching the relevant books, Social Book Seach Track at CLEF <ref type="bibr" coords="1,470.08,465.83,10.52,8.74" target="#b4">[5]</ref> provides a relevant experimental platform to investigate techniques of searching and navigating professional metadata. These metadata are provided by publishers/booksellers and user-generated content from social media <ref type="bibr" coords="1,423.63,501.70,9.96,8.74" target="#b0">[1]</ref>. In CLEF 2016 at Social book Search Lab, they offered three different tracks: Suggestion Track, Interactive Track and Mining Track. We participated in the suggestion track where we were supposed to recommend books based on user's request and her personal catalogue data (list of books with rating and tags maintained for the user in the social cataloguing site). We were also provided with a large set of anonymised user profiles from LibraryThing forum members, consisting of almost 93,976 anonymised user profiles from LibraryThing with over 33 million cataloguing transaction. Each user request is provided in the form of topics containing different fields like title, request, group, examples and catalogue information.</p><p>Our goal is to investigate the contribution of different topic fields as well as combining effect of some fields for book recommendation. We only considered title, request, group fields from each topic.We did not consider topic-creator's catalogue information nor did we consult the user profiles. We submitted six runs (ISMD16allfieds, ISMD16titlefield, ISMD16requestfield, ISMD16titlewithoutreranking, similaritytitlefieldreranked, ISMD16groupfield) in the Suggestion Task. For all the runs, Language modelling with Dirchlet smoothing was used in Lemur's Indri search system <ref type="bibr" coords="2,330.97,167.81,9.96,8.74" target="#b2">[3]</ref>. The organization of rest of the paper is as follows. Section 2 describes about dataset. we describe our methodology: field categories and indexing, which document and topic fields we used for retrieval in section 3. Section 4 describes what approaches we have used, Section 5 reports results. In Section 6 we analyse our results. Finally, we conclude in Section 7 with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The test collection provided by CLEF 2016 SBS orgainzers for Suggestion Task had a document collection and a topicset. The document collection consists of 2.8 million book description with metadata from Amazon and LibraryThing. In Amazon there is formal metadata like booktitle, author, publisher, publication year, library classification codes, Amazon categories, similar product information and user-generated content in the form of user ratings and reviews. In Amazon, there are user tags and user-provided metadata on awards, book characters, locations and blurbs. There are additional records from the British Library and the Library of Congress. The entire collection was 7.1 GB in size <ref type="bibr" coords="2,424.08,387.56,9.96,8.74" target="#b1">[2]</ref>.</p><p>The topic-set contains 120 topics each describing a user's request for suggestion of books. Each topic has a set of fields like title, request, group, example and user's personal catalogue at the time of topic creation. The catalogue contains a list of book-entries with information like LibraryThing id of the book, its entry-date, rating and tags.</p><p>The organizers also supplied 94,000 anonymised user profiles from Library-Thing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Field categories and Indexing</head><p>We are provided by Amazon/LibraryThing data collection(corpus) which consists of 2.8 million book descriptions with metadata. There are so many fields in the corpus, we took some of them for indexing which are as follow: Metadata In our metadata index, we used these metadata field: &lt;title&gt; &lt;cre-ator&gt;, &lt;firstwords&gt;, &lt;lastwords&gt;. Content In our content index, we used these metadata field:&lt;content&gt; of provided corpus containing, &lt;blurbs&gt;, &lt;epigraph&gt;, &lt;quotation&gt;. Tags In our tags index, we used &lt;tags&gt; field for indexing. Reviews In our reviews index, we used &lt;reviews&gt; field from corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>This year's Suggestion task has provided 120 topics, With help of these we built four set of queries which are: Topic-Title: Only the&lt;title&gt; field of each topic. Topic-Request: It contains only the &lt;request&gt; field. Topic-group: Only the &lt;group&gt; field. Topic-All-Fields: It contains &lt;title&gt;, &lt;request&gt;, &lt;group&gt; field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In our approach we analyzed two methods first one i.e. Content Based Retrieval and secondly re-ranking approach after rank normalization of the scores of the retrieved documents. For both retrieving approaches we used Language modeling with Dirichlet smoothing. The document collection provided was stopwordremoved using SMART stop word list and then stemmed using Krovetz stemmer. We did not remove stopwords from provided topics. For retrieving and indexing we used Lemur 5.9 search system. We also removed punctuation marks from all the textual content of these fields and used only free text queries in all the runs.We did not consider any other information like catalogue information and user profile during retrieval. For each topic, we submitted up to 1000 book suggestions in the form of ISBNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Content Based Retrieval</head><p>During retrieval, we tried to see the effect of different components of a topic one by one as well as combined contribution of all the topics except &lt;example&gt; field. It is simply based on adhoc retrieval. We can see the result given in Table <ref type="table" coords="3,134.77,498.97,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Re-ranking</head><p>In this method we are inspired by Social Feature Re-ranking Method proposed by Toine Bogers in 2012 <ref type="bibr" coords="4,251.36,156.50,9.96,8.74" target="#b5">[6]</ref>. In order to improve the initial ranking, we perform re-ranking by two different strategies after analyzing the structure of XML: Item-Rerank (I) and RatingReview-Rerank (R), For re-ranking we have used following stages:</p><p>Similarity Calculation: The similarity of two documents based on feature I is calculated by equation ( <ref type="formula" coords="4,250.59,229.16,4.24,8.74">1</ref>)</p><formula xml:id="formula_0" coords="4,149.46,284.25,47.29,9.65">sim ij (I) =</formula><p>1 : i is j s similar product or j is i s similar product 0 : otherwise (1)</p><formula xml:id="formula_1" coords="4,177.14,322.02,303.45,30.32">score (i) = α • score(i) + (1 -α) • N j=1 sim ij • score(j)(j = i)<label>(2)</label></formula><p>Re-Ranking: We re-rank the top 1000 list of initial ranking for the above mentioned features by Equation <ref type="bibr" coords="4,277.51,376.09,11.62,8.74" target="#b1">(2)</ref>.</p><formula xml:id="formula_2" coords="4,139.75,409.26,340.85,23.44">score (i) = α•score(i)+(1-α)×log(|reviews(i)|)× r∈Ri r |reviews(i)| ×score(i)<label>(3)</label></formula><p>For feature R, we use Equation (3) <ref type="bibr" coords="4,305.42,447.67,9.96,8.74" target="#b6">[7]</ref>.</p><p>Before re-ranking we apply rank normalization on the retrieved results to map the score into the range [0, 1] <ref type="bibr" coords="4,259.36,484.46,9.96,8.74" target="#b7">[8]</ref>. The balance between the original retrieval score, score(i) and the contributions of the other books in the results list is controlled by the α parameter, which takes values in the range [0, 1], but in our experiment we have taken fixed value i.e. α= 0.96. Due to lack of time, we couldn't try with any other value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The scores obtained by our six runs are given in Table <ref type="table" coords="4,373.56,597.34,3.87,8.74" target="#tab_1">2</ref>. The official evaluation measure provided by CLEF'16 is nDCG@10 [4]. The performance of our runs are in decreasing order. Our best performance is by ISMD16allfieds where we use title,request and group field. We also show the best score in the task demonstrated by run-id run1.keyQuery active combineRerank(*), for the sake of comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This year we participated in the Suggestion Task of Social Book Search. We tried to see the individual effect as well as combined effect of different topic-fields on book recommendation. We considered only a handful of fields like request, title, group etc from the topics. While there can be no denial of the fact that our overall performance is average, initial results are suggestive as to what should be done next. We need to consult other fields like book catalogue of the topic creators, ratings of the books in the catalogue during retrieval. We also need to take into account profiles of other users. It is also imperative to see the learning to rank for different fields, and taking the α parameter range between [0,1], this time we have taken fixed vale of α= 0.96. We will also use other fields in user catalogues and user profiles. We shall be exploring some of these tasks in the coming days.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,531.59,345.83,106.17"><head>Table 1 .</head><label>1</label><figDesc>Results of content based retrieval for different runs using nDCG@10. Best performing run for overall topic is given in bold letter</figDesc><table coords="3,159.37,563.35,270.18,74.41"><row><cell></cell><cell></cell><cell>Topic List</cell><cell></cell><cell></cell></row><row><cell>Documents</cell><cell>title</cell><cell>request</cell><cell>group</cell><cell>allfields</cell></row><row><cell>Field</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metadata</cell><cell>0.0531</cell><cell>0.0478</cell><cell>0.0201</cell><cell>0.0621</cell></row><row><cell>Content</cell><cell>0.0510</cell><cell>0.0432</cell><cell>0.0191</cell><cell>0.0423</cell></row><row><cell>Tags</cell><cell>0.0507</cell><cell>0.0457</cell><cell>0.0312</cell><cell>0.0542</cell></row><row><cell>Reviews</cell><cell>0.0478</cell><cell>0.0367</cell><cell>0.0010</cell><cell>0.0592</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,116.91,345.83,294.58"><head>Table 2 .</head><label>2</label><figDesc>Results -The official evaluation Measure by CLEF 2016Although our performance is not up to the mark, there are few take-home lessons. In our run id:ISMD16allfieds, ISMD16titlefield, ISMD16requestfield and ISMD16groupfield, we have reranked the retrieved score based on reviews(R) by taking α=0.96. In our top score i.e. ISMD16allfieds, we have taken combination of all the fields title, request, group except example field from topic, In ISMD16titlefield, we have taken onlytitle field, In ISMD16requestfield we have taken only field of the topic, For ISMD16groupfield we have taken only group field. For run id: ISMD16titlewithoutreranking we simply used as content based retrieval. For run id: similaritytitlefieldreranked we have used similarity as well as reranking by taking α = 0.96.</figDesc><table coords="5,134.77,139.67,326.05,128.41"><row><cell>RUN ID</cell><cell cols="3">Rank M RR nDCG@10 M AP R@1000</cell></row><row><cell>ISMD16allfieds</cell><cell cols="3">24 0.1722 0.0765 0.0342 0.2157</cell></row><row><cell>ISMD16titlefield</cell><cell>28 0.1197</cell><cell>0.0639</cell><cell>0.0333 0.1933</cell></row><row><cell>ISMD16requestfield</cell><cell>29 0.1454</cell><cell>0.0613</cell><cell>0.0287 0.1870</cell></row><row><cell cols="2">ISMD16titlewithoutreranking 33 0.1114</cell><cell>0.0542</cell><cell>0.0386 0.2556</cell></row><row><cell>similaritytitlefieldreranked</cell><cell>35 0.0966</cell><cell>0.0445</cell><cell>0.0307 0.1933</cell></row><row><cell>ISMD16groupfield</cell><cell>43 0.0527</cell><cell>0.0104</cell><cell>0.0069 0.0564</cell></row><row><cell>best*</cell><cell>1 0.5247</cell><cell>0.2157</cell><cell>0.1253 0.3474</cell></row><row><cell>6 Analysis</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,142.96,646.84,337.63,7.86;5,151.52,657.79,329.07,7.86;6,151.52,120.67,329.07,7.86;6,151.52,131.63,189.40,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,246.55,657.79,229.87,7.86">Overview of the INEX 2012 Social Book Search Track</title>
		<author>
			<persName coords=""><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Preminger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Monica</forename><surname>Landoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.52,120.67,146.17,7.86">INEX&apos;12 Workshop Pre-proceedings</title>
		<editor>
			<persName><forename type="first">Shlomo</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ralf</forename><surname>Schenkel</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome , Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,142.59,337.63,7.86;6,151.52,153.55,164.22,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,182.46,142.59,197.08,7.86">Initiative for the Evaluation of XML Retrieval</title>
		<author>
			<persName coords=""><surname>Inex</surname></persName>
		</author>
		<ptr target="https://inex.mmci.uni-saarland.de/data/documentcollection.jsp" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,164.51,337.63,7.86;6,151.52,175.46,146.26,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,151.53,164.51,255.96,7.86">INDRI: Language modeling meets inference networks</title>
		<ptr target="http://www.lemurproject.org/indri/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,186.42,337.63,7.86;6,151.52,197.38,264.16,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,267.57,186.42,208.76,7.86">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,151.52,197.38,175.08,7.86">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,208.34,337.63,7.86;6,151.52,219.30,91.91,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="http://clef2016.clef-initiative.eu/index.php" />
		<title level="m" coord="6,185.56,208.34,203.33,7.86">Conference and labs of the Evaluation Forum</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,230.26,337.64,7.86;6,151.52,241.22,230.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,255.18,230.26,172.31,7.86">Rslis at inex 2012: Social book search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,445.40,230.26,35.19,7.86;6,151.52,241.22,105.70,7.86">INEX&apos;12 Workshop Pre-proceedings</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,252.18,337.64,7.86;6,151.52,263.14,273.15,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,308.74,252.18,160.62,7.86">Do social information help book search?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.52,263.14,143.97,7.86">INEX&apos;12 Workshop Pre-proceedings</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,274.09,337.63,7.86;6,151.52,285.05,329.07,7.86;6,151.52,296.01,221.44,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,260.39,274.09,220.20,7.86;6,151.52,285.05,51.81,7.86">Web Metasearch: Rank vs. Score-based Rank Aggregation Methods</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,225.38,285.05,255.21,7.86;6,151.52,296.01,42.85,7.86">SAC 03: Proceedings of the 2003 ACM Symposium on Applied Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">841846</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
