<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.12,115.96,311.12,12.62;1,209.17,133.89,197.01,12.62">Recommender Systems Evaluations: Offline, Online, Time and A/A Test</title>
				<funder>
					<orgName type="full">COMMIT project Infiniti</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.05,171.56,126.92,8.74"><forename type="first">Gebrekirstos</forename><forename type="middle">G</forename><surname>Gebremeskel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Access</orgName>
								<orgName type="institution">CWI</orgName>
								<address>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.14,171.56,74.69,8.74"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Radboud University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.12,115.96,311.12,12.62;1,209.17,133.89,197.01,12.62">Recommender Systems Evaluations: Offline, Online, Time and A/A Test</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6311135086B4DA071768152AA4B94193</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a comparison of recommender systems algorithms along four dimensions. The first dimension is offline evaluation where we compare the performance of our algorithms in an offline setting. The second dimension is online evaluation where we deploy recommender algorithms online with a view to comparing their performance patterns. The third dimension is time, where we compare our algorithms in two different years: 2015 and 2016. The fourth dimension is the quantification of the effect of non-algorithmic factors on the performance of an online recommender system by using an A/A test. We then analyze the performance similarities and differences along these dimensions in an attempt to draw meaningful patterns and conclusions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems can be evaluated offline or online. The purpose of recommender system evaluation is to select algorithms for use in a production setting. Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B. The recommender system that achieves a higher score according to a chosen metric ( for example, Click-Through-Rate) is chosen as a better recommender system, given other factors such as latency and complexity are comparable.</p><p>The purpose of offline evaluation is to select recommender systems for deployment online. Offline evaluations are easier and reproducible. But do offline evaluations predict online performance behaviors and trends? Do the absolute performances of algorithms offline hold online too? Do the relative rankings of algorithms according to offline evaluation hold online too? How do offline evaluation compare and contrast with online evaluations?</p><p>CLEF NewsREEL <ref type="bibr" coords="1,231.02,608.30,9.96,8.74" target="#b4">[5]</ref>, a campaign-like news recommendation evaluation, provides opportunities to investigate recommender system performance from several angles. CLEF NewsREEL 2016 campaign, in particular, is focused on comparing recommender system performance in online and offline settings <ref type="bibr" coords="1,408.63,644.16,9.96,8.74" target="#b8">[9]</ref>. CLEF News-REEL 2016 provides two tasks: Benchmark News Recommendations in a Living Lab (Task 1) which enables evaluation of systems in a production setting <ref type="bibr" coords="2,467.31,118.99,9.96,8.74" target="#b5">[6]</ref>, and Benchmarking News Recommendations in a Simulated Environment (Task 2) which enables the evaluation of systems in a simulated (offline) setting using dataset collected from the online interactions.</p><p>In 2015, we participated in Task 1. In 2016, we participated in both CLEF NewsREEL tasks. In this working notes, we report both offline and online evaluations and how they relate to each other. We also present the challenges of online evaluation from the dimensions of time and non-algorithmic causes of performance differences. On the time dimension, we specifically investigate online performances behaviors in 2015 and 2016, and on the dimension of non-algorithmic causes of performance differences, we employ an A/A test where we run two instances of the same algorithm to gauge the extent of performance difference resulting from non-algorithmic causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Objectives</head><p>The objective of our participation this year is to investigate performance behaviors of recommender system along several dimensions. We are interested in differences and similarities in offline and online performances, the variations in performance over time, and the estimation of performance differences caused by non-algorithmic factors in the online recommender system evaluations. This work can be see as an extension of studies that have previously investigated the differences between offline and online recommender system evaluations <ref type="bibr" coords="2,443.51,392.43,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,455.69,392.43,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="2,465.09,392.43,11.62,8.74" target="#b10">11]</ref>.</p><p>In 2015, we participated in CLEF NewsREEL News Recommendations Evaluation, the task of Benchmark News Recommendations in a Living Lab <ref type="bibr" coords="2,467.31,416.57,9.96,8.74" target="#b5">[6]</ref>. We reported the presence of substantial non-algorithmic factors that cause two instances of the same algorithm to end up having statistically significant performance differences. The results are presented in <ref type="bibr" coords="2,354.60,452.43,9.96,8.74" target="#b3">[4]</ref>. This year, we run four of our 2015 recommender systems without change. This allows us to compare the performance of the systems in 2015 and 2016. In 2016, we participated also in Task 2, which allows us to evaluate the recommender systems in a simulated environment and then compare the offline performance measurements with the corresponding online performance measurements. In this report, we present the results of these evaluations along the four dimensions and highlight similarities, differences and patterns or the lack thereof.</p><p>For the study of the effect of non-algorithmic factors on online recommender system performances, we run two instances of the same news recommender algorithm with the view to quantifying the extent of performance differences. To compare the online and offline performance behaviors, we conduct offline evaluations on a snapshot of a dataset collected from the same system. To investigate performance in the dimension of time, we rerun last year's recommender systems. This means that we can compare the performance of the recommender systems in 2016 with their corresponding performance in 2015.</p><p>The four recommender systems are two instances of Recency, one instance of GeoRec and one instance of RecencyRandom. Recency keeps the 100 most recently viewed items for each publisher, and upon recommendation request, the most recently read (clicked) are recommended. GeoRec is a modification of the Recency recommender to diversify the recommendations by taking into account the users' geographic context and estimated interest in local news. RecencyRandom recommends items randomly selected from the 100 most recently viewed items. For a detailed description of the algorithms, refer to <ref type="bibr" coords="3,465.11,178.77,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussions</head><p>We present the results and analysis from the different dimensions here. In 2015, the recommender systems ran from 2015-04-12 to 2015-07-06, a total of 86 days. RecencyRandom started 12 days later in 2015. In 2016, the systems ran from 2016-02-22 to 2016-05-21, a total of 70 days. We present three types of results for 2016: the daily performances, the incremental performances, and the cumulative performances. The plot for the daily performance is presented in Figure <ref type="figure" coords="3,447.04,296.94,3.87,8.74">1</ref>. From the plot, we observe large variations between the maximum and minimum performance measurements of the tested recommender systems; the minimum value equals 0 in every test, while the maximum varies between 12.5% for Recency2, 5.6% for GeoRec, 4.3% for RecencyRandom, and 4.2% for Recency. The highest performance measurements all occurred between the 18 th day from the start of our participation (2016-03-10) and the 31 st day. The highest scores of Recency2, and GeoRec occurred on March 21 nd , for RecencyRandom on March 20 th and GeoRec on March 10 th . We do not have a plausible explanation why the evaluation resulted in increased performance during that period, nor why the highest scores for two systems occurred on those two days in March. We did however observe quite a reduction in the number of recommendation requests issued in the period when the systems showed increased performance scores (such that minor variations would lead to larger normalized performance differences than in the rest of the evaluation period). Some of the systems have no reported results between 2016-03-24 and 2016-04-05; if this reduction in the number of recommendation requests is the same for all teams and systems who participated, the lower number of recommendations paired by an increase in CTR could indicate that users are more likely to click on recommendations when recommendations are offered sparsely. If this is the case, it might suggest further investigation into the relationship of the number of recommendations and user responses.</p><p>Figure <ref type="figure" coords="3,180.18,548.26,4.98,8.74" target="#fig_1">2</ref> for 2015 and in Figure <ref type="figure" coords="3,283.60,548.26,4.98,8.74" target="#fig_2">3</ref> for 2016 plot the performance measurements as the systems progress on a daily basis, which we call incremental performance. The cumulative number of requests, clicks and CTR scores of the systems in both years are presented in Table <ref type="table" coords="3,262.83,584.13,3.87,8.74" target="#tab_0">1</ref>. The cumulative performance measurements remain below 1% for all systems. The maximum performance differences observed between the systems equal 0.16% in 2015 and 0.07% in 2016.</p><p>From the plots in Figure <ref type="figure" coords="3,265.41,620.25,4.98,8.74" target="#fig_1">2</ref> and Figure <ref type="figure" coords="3,327.46,620.25,4.98,8.74" target="#fig_2">3</ref> and the cumulative performance measurements in Table <ref type="table" coords="3,237.13,632.21,3.87,8.74" target="#tab_0">1</ref>, we observe that the performance measurements of the different systems vary. Are these performance variations between the different systems also statistically significant? We look at statistical significance on a q qqq q q q q q q q q q q qq q q q q q q q q q q q q q qqq qq q q qq q q q q q q q q q q q q q q q q qqq q q q q q q q q q q q q q 0 10 20 Days CTR q q q q q q q qq q q q q q q qq q q q q q q q q q qqq q qq qq q q q q q q q qq q qq q q q q q q q qq q qq q qqqqq q qq qqq qq qq q q q qq q q q q q q q q q q q q q q q q q q qq q q q q q q q qq q q q q q q q q q q q q q q q qq qq q qqqq q q q q q q qq q q q qq qq q q q q q q qq q q q qq q q qq q q q q q q q q q q q qq q q q q q q q q q q q q q q q q qq qqq qqqqqq q q q q q q Legend Recency Recency2 RecencyRandom GeoRec Fig. <ref type="figure" coords="4,154.40,437.66,4.13,7.89">1</ref>. Daily CTR performance measurements of the four online recommender systems in 2016. Notice the large differences between the days and the unusual increase in CTR between the 18 th and 31 st day.   daily basis after the 14 th day, which is considered the average time within which industry A/B tests are conducted. To compute statistical significance, we used Python module of Thumbtack's Abba, a test for binomial experiments <ref type="bibr" coords="7,443.14,142.90,10.52,8.74" target="#b6">[7]</ref> (for a description of the implementation, please refer to <ref type="bibr" coords="7,352.75,154.86,10.79,8.74" target="#b7">[8]</ref>)</p><p>We perform statistical significance tests on a daily basis to simulate the notion of an experimenter checking whether one system is better than the other at the end of every day. In testing for statistical significance on a daily basis, we seek an answer to the question: 'On how many days would an experimenter seeking to select the better system find out that one system is significantly different from the chosen baseline?' We investigate this under two baselines: Recency2, and RecencyRandom. Tables <ref type="table" coords="7,313.73,238.55,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="7,342.80,238.55,4.98,8.74" target="#tab_2">3</ref> present the actual number of days and the percentage of days on which significant performance differences were observed. Next, we looked into the error notifications received by our recommender systems in the 2016 period. The error types and counts for each system are presented in Table <ref type="table" coords="7,217.46,517.50,3.87,8.74" target="#tab_3">4</ref>. Three types of errors occurred, the highest number for the RecencyRandom recommender. According to the ORP documentation <ref type="foot" coords="7,447.86,527.88,3.97,6.12" target="#foot_0">3</ref> , error code 408 corresponds to connection timeouts, error code 442 to invalid format of recommendation responses, while error 455 is not described.</p><p>We aggregated error messages by day. Out of the 70 days, Recency received error notifications on 16 days, Recency2 on 19 days, GeoRec on 24 days, and RecencyRandom on 51 days. All systems received high number of error messages on specific days, especially on 2016-04-07 and 2016-04-08. While we do not know the explanation for errors on especially those days, we did observe that most of the high-error days seem to be those that correspond to the beginning of the start of the systems, or at the beginning of a change of load (from low to high).</p><p>Why the RecencyRandom recommender received a high number of 'invalidly formatted' responses is not clear, because the format is the same as for other systems. The main difference between RecencyRandom and the other systems we deployed is that it has a lower response time, and we suspect the high number of errors to be related to its lower response rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online Evaluation</head><p>In the online evaluation, or Benchmark News Recommendations in a Living Lab (Task 1) as it is called in CLEF NewsREEL, we investigate recommender systems in two dimensions. One dimension is time where we compare and contrast the performances of our systems in 2015 and 2016. The second dimension is an A/A test where we attempt to study non-algorithmic effects on the performance of systems. Each of the dimensions are discussed in the following subsections.</p><p>Time Dimension: Performances in 2015 and 2016 Participation in the Lab in 2015 and in 2016 gives us the opportunity to study the evaluation results from a time dimension. We compare the systems both in terms of their absolute and relative performance (in terms of their rankings). To compare the absolute performances, we used the 2015 instances of the recommender systems as baselines, and the corresponding 2016 instances as alternatives. The performance measurements of the Recency and GeoRec instances of 2016 were significantly different from the performance measurements of the Recency and GeoRec instances of 2015 with a P-values of 0 .0001 and 0 .0009 respectively. The 2015 instances of Recency2 and RecencyRandom were not significantly different from their corresponding instances in 2016.</p><p>In 2015, Recency2 ranked third, but in 2016, it ranked first. In 2015, almost all systems started from a lower CTR performance, and slowly increased towards the end where the performance measurements stabilized (see Figure <ref type="figure" coords="8,431.16,620.25,3.87,8.74" target="#fig_2">3</ref>). In 2016, however, the evaluation results of the systems reached its high at the beginning, and then decreased steadily towards the end, except for recommender Recency2, which showed an increase after the first half of its deployment and then decreased (see Figure <ref type="figure" coords="9,187.44,118.99,3.87,8.74" target="#fig_2">3</ref>). In 2016, the performance measurements seemed to continue to decrease, and not converge to a stable result like in 2015.</p><p>When we compare the number of days for which the results are significantly different according to the statistical test (see Table <ref type="table" coords="9,362.52,154.94,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="9,390.68,154.94,31.51,8.74" target="#tab_2">Table 3</ref>), we observe that there is no consistency. In 2015, there were two days (2.7%) on which significant performance differences were observed between Recency and Recency2 while there are 25 days (34.3%) on which significant performance difference between GeoRec and Recency2. In 2016, Recency has shown 47.4% of the time significant performance, and GeoRec only 14%. When using RecencyRandom as a baseline, Recency has registered significant performance differences 27.4% of the time in 2015, and 0% in 2016. GeoRec has 56.2% in 2015 and 8.8% in 2016. <ref type="foot" coords="9,476.12,237.05,3.97,6.12" target="#foot_1">4</ref>We conclude that it is different to generalize the performance measurements over time. The patterns observed in 2015 and 2016 vary widely, both in terms of absolute and relative performance, irrespective of the baseline considered. The implication is that one can not rely on the absolute and relative rankings of recommender systems at one time for a similar job in another time. The systems have not changed between the two evaluations. The differences in evaluation results, therefore, can only be attributed to the setting in which the systems are deployed. It is possible that the presentation of recommendation items by the publishers, the users and content of the news publishers might have undergone changes which can then affect the performances in the two years, but we cannot be certain without more in-depth analysis.</p><p>A/A Testing In both 2015 and 2016, two of our systems were instances of the same algorithm. The two instances were run from the same computer; the only differences between them were the port numbers by which they communicated with CLEF NewsREEL's ORP <ref type="foot" coords="9,271.53,434.78,3.97,6.12" target="#foot_2">5</ref> . The purpose of running two instances of the same algorithm is to quantify the level of performance differences due to nonalgorithmic causes. From the participant's perspective, performance variation between two instances of the same algorithm can be seen pure luck. The extent of performance differences between the instances can be seen as also happening between the performances of the other systems. We can consider that he performance difference due to the effectiveness of the algorithms is therefore the overall performance minus the maximum performance difference between the performances of the two instances.</p><p>The results of the two instances (Recency and recency2) can be seen in 1, the incremental plots (Figure <ref type="figure" coords="9,266.42,555.98,4.98,8.74" target="#fig_1">2</ref> and Figure <ref type="figure" coords="9,326.11,555.98,3.87,8.74" target="#fig_2">3</ref>. The cumulative performances on the 86 th day of the deployment in 2015 showed no significant difference. In 2016, however, Recency2 showed a significant performance over Recency with a Pvalue of 0 .0005 . Checking for statistical significance on a daily basis after the 14 th day (see 2), in 2015, there were 2 days (2.7%) on which the two instances differed significantly. In 2016, however, the number of days was extremely higher, a total of 27 days (47.4%). This is interesting for two reasons: 1) the fact that two instances can end up having statistically significant performance differences and 2) that the significant difference occurred. In 2016, one instance achieved significant performance differences over the other instance for almost half of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Evaluation</head><p>We present evaluations conducted offline, or in Benchmarking News Recommendations in a Simulated Environment (Task 2), as it is called in CLEF NewsREEL. Evaluation in Task 2 differs from other offline evaluation setups in that Task 2 actually simulates the online evaluation setting for each of the systems. Usually, systems are selected on the basis of offline evaluation and deployed online. Other things such as complexity and latency being equal, there is this implicit assumption that the relative offline performances of systems holds online too. That is that if System one has performed better than system two in an offline evaluation, it is assumed that the same rank holds when the two algorithms are deployed online. In this section, we investigate whether this assumption holds by comparing the offline evaluation results of the algorithms in Task 1 with their online results.</p><p>Task 2 of CLEF NewsREEL provides a reproducible environment for participants to evaluate their algorithms in a simulated environment that uses user-item interaction dataset recorded from the online interactions <ref type="bibr" coords="10,386.59,393.10,14.61,8.74" target="#b9">[10]</ref>. In the simulated environment, a recommendation is successful if the user has viewed or clicked on the recommendations. This is different from Task 1 (online evaluation) where a recommendation is a success only if the recommendation is clicked. The performances of our algorithms in the simulated evaluation are presented in Table <ref type="table" coords="10,134.77,452.88,3.87,8.74">5</ref>. The plots as they progress on a daily basis are presented in Figure <ref type="figure" coords="10,440.82,452.88,3.87,8.74" target="#fig_3">4</ref>. In this evaluation, Recency leads followed by Georec and then RecencyRandom. Using RecencyRandom as a baseline, there was no significant performance difference in both Recency and GeoRec. Comparing the ranking with those of the systems in Task 2, there is no consistency. We conclude that the relative offline performance measurements do not generalize to those online, much less the absolute performance.</p><p>From Table <ref type="table" coords="10,203.94,536.57,3.87,8.74">5</ref>, we observe that only RecencyRandom has invalid responses. We also observed that RecencyRandom has higher error messages and lower performance in Task 1. To understand why, we looked at the response times of the systems under extreme load. The mean, min, max and standard deviations of the response times of the three systems are presented in Table <ref type="table" coords="10,397.28,584.39,3.87,8.74" target="#tab_5">6</ref>. We observe that RecencyRandom has the slowest response time followed by GeoRec. We have also plotted the number of recommendations within 250 milliseconds in Figure <ref type="figure" coords="10,134.77,620.25,3.87,8.74" target="#fig_4">5</ref>. Here too, we observe the lowest reponse times for RecencyRandom (attributed to the randomization before selecting recommendation items). When we look at the publisher-level breakdown of the recommendation response in Table <ref type="table" coords="10,443.76,644.16,3.87,8.74">5</ref>, we see that RecencyRandom has invalid responses for two publishers, but for publisher Table <ref type="table" coords="11,166.20,137.80,4.13,7.89">5</ref>. The performances of our algorithms in simulated evaluation (Task 2). For each system, there are the number of correct clicks (clicks), the number of requests, and the CTR (clicks*1000/requests) and the number of invalid responses (Inv). Results for publishers http://www.cio. <ref type="bibr" coords="11,317.68,170.70,40.78,7.86">de (13554)</ref>  Tagesspiegel (1677), all its recommendations are invalid. In the offline evaluation, invalid response means that the response generates an exception during parsing. We looked into the recommendation responses of RecencyRandom, and compared the response for publisher 694 and 1677. Almost all item responses for publisher 1677 were empty, which we assume to be related with the extreme load. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our systems are very similar to each other, in that they are slight modifications of each other. This means that it is expected that their performances do not vary much. We have analyzed the performance of our systems from the dimensions of online, offline, and time. We have also investigated the the extent of performance difference due to non-algorithmic causes in online evaluation by running two instances of the same algorithms.</p><p>We have observed substantial variation along the four dimensions. The performance measurements in both absolute and relative sense varied significantly in 2015 and in 2016. More surprisingly, the two instances of the same algorithm varied significantly both in the two years and within the same year. This is surprising and indicates how challenging it is to evaluate algorithms online. In the online evaluation, non-algorithmic and non-functional factors impact performance measurements. Non-algorithmic factors include variations in users and items that systems deal with, and the variations in recommendation requests. Non-functional factors include response times and network problems. The performance difference between the two instances of the same algorithms can be considered to reflect the impact of non-algorithmic and non-functional factors on performance. It can then be subtracted from the performances of online algorithms before they are compared with baselines and each other. This can be seen as a way of discounting the randomness in online system evaluation from affecting comparisons.</p><p>The implication of the lack of pattern in the performance of the systems across time and baselines, and more specially the performance differences between the two instances of the same algorithm highlights the challenge of comparing systems online on the basis of statistical significance tests alone. The results call for caution in the comparison of systems online where user-item dynamism, operational decision choices and non-functional factors all play roles in causing performance differences that are not due to the effectiveness of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison With Other Teams</head><p>Let us also compare evaluation results for our systems to those of other teams that participated in 2016 CLEF NewsREEL's Task 1, based on the results over the period between 28 April and 20 May (provided by CLEF NewsREEL). The plot of the team ranking as provided by CLEF NewsREEL is provided in Figure <ref type="figure" coords="14,134.77,233.35,3.87,8.74" target="#fig_5">6</ref>. We examined whether the performance of the best performing systems from the teams that are ranked above us were significantly different from ours. Only the ABC's and Artificial Intelligence's systems were significantly different from Recency2 (our best performing system for 2016). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We set out to investigate the performance of recommender system algorithms online, offline, and in two separate periods. The recommender systems' performances in different dimensions indicate that there is no consistency. The offline performances were not predictive of the online performances in both absolute and relative sense. Also the performance measurements of the systems in 2015 were not predictive of those in 2016, both in relative and absolute sense. Our systems are slight variations of the same algorithm, and yet the performances varied in all dimensions. We conclude that we should be cautious in interpreting the results of performance differences, especially considering the differences observed between the two instances of the same algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,158.24,530.90,298.88,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. CTR performance of the four online recommender systems (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,525.67,345.82,7.89;6,134.77,536.65,133.61,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Cumulative CTR performance measurements of the four online systems, as they progress on a daily basis in 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,134.77,622.68,345.83,7.89;11,134.77,633.66,65.38,7.86"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CTR performance measurements of the three offline systems as they progress on a daily basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,134.77,525.67,345.83,7.89;13,134.77,536.65,104.16,7.86"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Number of recommendation responses against response times in milliseconds, for the systems in Task 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,134.77,455.25,345.83,7.89;14,134.77,466.23,225.27,7.86"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The rankings of the 2016 teams that participated in the CLEF NewsREEL challenge. The plot was provided by CLEF NewsREEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,536.31,345.83,94.27"><head>Table 1 .</head><label>1</label><figDesc>Number of requests, number of clicks and CTR scores of four systems in 2015 and 2016.</figDesc><table coords="4,169.71,566.33,275.95,64.25"><row><cell></cell><cell></cell><cell>2015</cell><cell></cell><cell>2016</cell></row><row><cell>Algorithms</cell><cell cols="4">Requests Clicks CTR(%) Requests Clicks CTR(%)</cell></row><row><cell>Recency</cell><cell>90663</cell><cell>870</cell><cell>0.96</cell><cell>450332 3741 0.83</cell></row><row><cell>Recency2</cell><cell>88063</cell><cell>810</cell><cell>0.92</cell><cell>398162 3589 0.90</cell></row><row><cell cols="2">RecencyRandom 73969</cell><cell>596</cell><cell>0.80</cell><cell>438850 3623 0.83</cell></row><row><cell>GeoRec</cell><cell>88543</cell><cell>847</cell><cell>0.96</cell><cell>448819 3785 0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,155.70,291.95,303.95,62.34"><head>Table 2 .</head><label>2</label><figDesc>Statistical significance when comparing Recency2 to the baseline.</figDesc><table coords="7,203.12,312.75,209.11,41.54"><row><cell>Algorithms</cell><cell cols="3">2015 No Sig Results % No Sig Results % 2016</cell></row><row><cell>Recency</cell><cell>2</cell><cell>2.7 27</cell><cell>47.4</cell></row><row><cell>GeoRec</cell><cell>25</cell><cell>34.3 8</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,141.05,401.54,333.26,62.34"><head>Table 3 .</head><label>3</label><figDesc>Statistical significance when comparing RecencyRandom to the baseline.</figDesc><table coords="7,201.59,422.34,212.18,41.54"><row><cell>Algorithms</cell><cell cols="2">2015 No Sig Results %</cell><cell cols="2">2016 No Sig Results %</cell></row><row><cell>Recency</cell><cell>20</cell><cell cols="2">27.4% 0</cell><cell>0</cell></row><row><cell>GeoRec</cell><cell>41</cell><cell cols="2">56.2 5%</cell><cell>8.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,197.68,345.83,116.18"><head>Table 4 .</head><label>4</label><figDesc>Count of errors messages received by our recommender systems in 2016. Error code 408 is for connection timeout, error code 442 is for invalid format of recommendation response and Error 455 is not described. RecencyRandom has the highest number of errors.</figDesc><table coords="8,163.34,251.51,288.67,62.36"><row><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error Types</cell><cell cols="4">Recency Recency2 RecencyRandom GeoRec</cell></row><row><cell>408 (Connection Timeout)</cell><cell>40</cell><cell>118 40</cell><cell>14</cell><cell>159</cell></row><row><cell>442 (Invalid-Format)</cell><cell>1377</cell><cell>1390</cell><cell>26608</cell><cell>1360</cell></row><row><cell>455</cell><cell>281</cell><cell>217</cell><cell>348</cell><cell>252</cell></row><row><cell>Total</cell><cell>1698</cell><cell>1725</cell><cell>26970</cell><cell>1771</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,134.77,170.70,345.83,428.29"><head></head><label></label><figDesc>, http://www.gulli.com (694), http://www.tagesspiegel.de (1677), sport1 (35774) and All are shown in the table.</figDesc><table coords="11,136.56,202.93,342.79,396.06"><row><cell>Publishers</cell><cell></cell><cell cols="2">Recency</cell><cell></cell><cell cols="3">RecencyRandom</cell><cell></cell><cell cols="2">GeoRec</cell></row><row><cell></cell><cell cols="10">Click Request Inv CTR Click Request Inv CTR Click Request Inv CTR</cell></row><row><cell>13554</cell><cell>0</cell><cell cols="2">21504 0 0</cell><cell>0</cell><cell cols="3">12798 1451 0</cell><cell>0</cell><cell cols="2">21504 0 0</cell></row><row><cell>694</cell><cell>13</cell><cell>4337</cell><cell>0 2</cell><cell>3</cell><cell>4347</cell><cell>0</cell><cell>0</cell><cell>13</cell><cell>4337</cell><cell>0 2</cell></row><row><cell>1677</cell><cell>69</cell><cell cols="2">46101 0 1</cell><cell>0</cell><cell>0</cell><cell cols="2">7695 0</cell><cell>69</cell><cell cols="2">46101 0 1</cell></row><row><cell>35774</cell><cell cols="3">3489 518367 0 6</cell><cell cols="3">2297 519559 0</cell><cell>4</cell><cell cols="3">3445 518411 0 6</cell></row><row><cell>All</cell><cell cols="3">3571 590309 0 6</cell><cell cols="4">2300 536704 9146 3</cell><cell cols="3">3527 590353 0 5</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTR</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Legend</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recency</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RecencyRandom GeoRec</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell>6</cell><cell>7</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Days</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,134.77,206.60,345.82,73.69"><head>Table 6 .</head><label>6</label><figDesc>Response times in milliseconds of the recommender systems. RecencyRandom has the lowest response time.</figDesc><table coords="12,219.00,238.36,177.35,41.94"><row><cell></cell><cell>Mean Min Max stDev</cell></row><row><cell>Recency</cell><cell>9.057 0.0 2530.0 41.619</cell></row><row><cell cols="2">RecencyRandom 83.868 1.0 5380.0 319.463</cell></row><row><cell>GeoRec</cell><cell>11.549 1.0 2320.0 56.570</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="7,144.73,656.80,193.08,7.86"><p>http://orp.plista.com/documentation/download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="9,144.73,623.92,335.87,7.86;9,144.73,634.88,335.86,7.86;9,144.73,645.84,265.01,7.86"><p>We would like to mention a correction here over the reported statistical significance score of GeoRec in 2015. It was reported that Georec did not achieve any significant performance over Recency2<ref type="bibr" coords="9,258.08,645.84,9.22,7.86" target="#b2">[3]</ref>, which was an error in calculation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="9,144.73,656.80,90.43,7.86"><p>http://orp.plista.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was partially supported by <rs type="funder">COMMIT project Infiniti</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.96,197.17,337.64,7.86;15,151.52,208.13,329.07,7.86;15,151.52,219.09,329.07,7.86;15,151.52,230.05,329.07,7.86;15,151.52,241.01,48.38,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,420.66,197.17,59.94,7.86;15,151.52,208.13,329.07,7.86;15,151.52,219.09,117.30,7.86">A comparative analysis of offline and online evaluations and discussion of research paper recommender system evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Genzmehr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,292.10,219.09,188.50,7.86;15,151.52,230.05,275.62,7.86">Proceedings of the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation</title>
		<meeting>the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,251.87,337.63,7.86;15,151.52,262.83,329.07,7.86;15,151.52,273.79,329.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,453.97,251.87,26.62,7.86;15,151.52,262.83,252.20,7.86">Offline and online evaluation of news recommender systems at swissinfo</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Garcin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Donatsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alazzawi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bruttin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,434.71,262.83,45.88,7.86;15,151.52,273.79,212.37,7.86">Proceedings of the 8th ACM Conference on Recommender systems</title>
		<meeting>the 8th ACM Conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,284.66,337.64,7.86;15,151.52,295.61,329.07,7.86;15,151.52,306.57,127.35,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,311.83,284.66,168.76,7.86;15,151.52,295.61,121.93,7.86">The degree of randomness in a live recommender systems evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,298.72,295.61,177.59,7.86">Working Notes for CLEF 2015 Conference</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,317.44,337.64,7.86;15,151.52,328.40,329.07,7.86;15,151.52,339.36,329.07,7.86;15,151.52,350.32,329.07,7.86;15,151.52,361.28,329.07,7.86;15,151.52,372.23,85.17,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,313.03,317.44,167.56,7.86;15,151.52,328.40,160.03,7.86">Random performance differences between online recommender system algorithms</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,454.46,339.36,26.13,7.86;15,151.52,350.32,329.07,7.86;15,151.52,361.28,198.69,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 7th International Conference of the CLEF Association, CLEF 2016</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Goncalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>vora, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">September 5-8, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,383.10,337.63,7.86;15,151.52,394.06,329.07,7.86;15,151.52,405.02,325.01,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,215.53,394.06,261.27,7.86">Benchmarking news recommendations: the clef newsreel use case</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Serény</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,163.04,405.02,53.49,7.86">SIGIR Forum</title>
		<imprint>
			<publisher>ACM Special Interest Group</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,415.88,337.64,7.86;15,151.52,426.84,329.07,7.86;15,151.52,437.80,329.07,7.86;15,151.52,448.76,20.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,151.52,426.84,209.85,7.86">Benchmarking news recommendations in a living lab</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Plumbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,379.93,426.84,100.67,7.86;15,151.52,437.80,220.62,7.86">Information Access Evaluation. Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="250" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,459.63,337.64,7.86;15,151.52,470.59,45.57,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="15,204.37,459.63,43.86,7.86">Abba 0.1.0</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://pypi.python.org/pypi/ABBA/0.1.0" />
		<imprint>
			<date type="published" when="2016-06-18">2016-06-18</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,481.45,16.25,7.86;15,182.67,481.45,33.56,7.86;15,281.11,481.45,24.32,7.86;15,328.89,481.45,42.91,7.86;15,395.27,481.45,22.07,7.86;15,440.81,481.45,39.78,7.86;15,151.52,492.41,259.76,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://www.thumbtack.com/labs/abba/" />
		<title level="m" coord="15,281.11,481.45,24.32,7.86;15,328.89,481.45,42.91,7.86;15,395.27,481.45,22.07,7.86;15,440.81,481.45,35.81,7.86">Abba: Frequently asked questions</title>
		<imprint>
			<date type="published" when="2016-06-18">2016-06-18</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,503.28,337.64,7.86;15,151.52,514.24,329.07,7.86;15,151.52,525.19,329.07,7.86;15,151.52,536.15,329.07,7.86;15,151.52,547.11,329.07,7.86;15,151.52,558.07,329.07,7.86;15,151.52,569.03,264.33,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,367.34,514.24,113.25,7.86;15,151.52,525.19,324.76,7.86">Overview of NewsREEL&apos;16: Multi-dimensional Evaluation of Real-Time Stream-Recommendation Algorithms</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Malagoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sereny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,301.76,547.11,178.83,7.86;15,151.52,558.07,329.07,7.86;15,151.52,569.03,45.21,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction 7th International Conference of the CLEF Association, CLEF 2016</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Goncalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>vora, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">September 5-8, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,579.90,337.97,7.86;15,151.52,590.85,329.07,7.86;15,151.52,601.81,329.07,7.86;15,151.52,612.77,225.58,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,219.11,590.85,261.48,7.86;15,151.52,601.81,34.24,7.86">Stream-based recommendations: Online and offline evaluation as a service</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Serény</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,207.98,601.81,272.61,7.86;15,151.52,612.77,94.64,7.86">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="497" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,623.64,337.98,7.86;15,151.52,634.60,329.07,7.86;15,151.52,645.56,259.08,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,343.02,623.64,137.57,7.86;15,151.52,634.60,211.52,7.86">A live comparison of methods for personalized article recommendation at forbes. com</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kirshenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,388.07,634.60,92.52,7.86;15,151.52,645.56,137.52,7.86">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
