<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.16,146.21,410.66,18.08">XRCE&apos;s Participation to ImageCLEFphoto 2007</title>
				<funder>
					<orgName type="full">Pole CAP DIGITAL (IMVN) de Paris, Ile-de-France</orgName>
				</funder>
				<funder ref="#_GMKSByf">
					<orgName type="full">French Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.83,181.27,83.53,10.46"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.15,181.27,90.57,10.46"><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.42,181.27,70.76,10.46"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 ch. de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.16,146.21,410.66,18.08">XRCE&apos;s Participation to ImageCLEFphoto 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">345A1CB6D54C8EC80C4B040983D16F97</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Cross-media and cross-lingual information retrieval, Trans-media relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our participation to ImageCLEFphoto07, for the first time, was motivated by assessing several transmedia similarity measures that we recently designed and developed. The object of investigation consists here in some "intermediate level" fusion approaches, where we use some principles coming from pseudo-relevance feedback and, more specifically, use transmedia pseudo-relevance feedback for enriching the mono-media representation of an object with features coming from the other media. One issue that arises when adopting such a strategy is to determine how to compute the mono-media similarity between an aggregate of objects coming from a first (pseudo-)feedback step and one single multimodal object. We propose two alternative ways of adressing this issue, that result in what we called the "transmedia document reranking" and "complementary feedback" methods respectively. This year, with a "lightly" annotated corpus of images, it appears that mono-media retrieval performance is more or less equivalent for pure image and pure text content (around 20% MAP). Using our transmedia pseudofeedback-based similarity measures allowed us to dramatically increase the performance by ∼50% (relative). Trying to model the textual "relevance concept" present in the top ranked documents issued from a first (purely visual) retrieval and combining this model with the textual part of the original query turns out to be the best strategy, being slightly superior to our transmedia document reranking method. Enriching the image annotations by extra tags extracted from an external resource (namely the Flickr database) does not offer a significant advantage in the ImageCLEF07 corpus, even if we observed an improvement using other multimedia corpora and query sets. From a cross-lingual perspective, the use of domain-specific, corpus-adapted probabilistic dictionaries seems to offer better results than the use of a broader, more general standard dictionary. With respect to the monolingual baselines, multilingual runs show a slight degradation of retrieval performance ( ∼6 to 10% relative).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Efficient access to multimedia information requires the ability to search and organize the information. While, the technology to search text has been available for some time -and in the form of web search engines is familiar to many people -the technology to search images and videos is much more challenging. Early systems were based mainly on visual similarity with a query image making use of lower-level features like texture, color, and shape. The only visual-based approach to retrieval has several drawbacks. It does not actually bridge the semantic gap but rather forces the user to work on low-level feature space. A gap remains between the user's conceptualization of a query and the query that is actually specified to the system.</p><p>The scientific challenge is to understand the nature of interaction between text and images: How can a text be associated with a piece of image (and reciprocally an illustrative image with text) ? How can we organize and access text and image repositories in a better way than naive late fusion techniques ? A lot of attempts have tried to find correlation between image and text features. The main difficulty is to overcome the semantic gap and, especially, the fact that visual and textual features are expressed at different semantic levels. As far as the "pure" visual mode is concerned, features associated to images have become more and more complex, trying to abstract their representation and to bridge the semantic gap. These trials take at least two directions. The first one is to adopt the strategy of developing more expressive visual vocabularies, potentially hierarchical, relying on latent semantic extraction techniques such as Probabilistic Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA). The second one focus on segmentation approaches, that aim at cutting out an image into several related semantic regions.</p><p>Departing from the classical "late fusion" strategy, recent approaches have considered fusion at the feature level, estimating correspondences or joint distributions between components across the image and text modes from training data. The main idea is to enrich the images with textual data (annotations) -and vice versa -in order to facilitate their retrieval. These methods could be classify into three general families: latent variable models, graph models and cross-lingual models. Latent variable models generally extend PLSA or LDA to explain jointly image and text (e.g. <ref type="bibr" coords="2,90.01,443.19,15.50,10.46" target="#b14">[17,</ref><ref type="bibr" coords="2,109.24,443.19,7.20,10.46" target="#b0">1]</ref>). Graph models consider the structure of an image through a graph, e.g. with a markov network <ref type="bibr" coords="2,128.36,455.15,10.52,10.46" target="#b1">[2]</ref> or a concept graph (e.g. <ref type="bibr" coords="2,254.08,455.15,14.76,10.46" target="#b15">[18]</ref>). Finally, cross-lingual models find their inspiration in machine translation and cross lingual information retrieval (e.g. <ref type="bibr" coords="2,372.30,467.10,14.76,10.46" target="#b9">[12]</ref>).</p><p>Our work is most aligned with the third family, namely Cross-Lingual Models (CLM). The main idea of CLM applied to hybrid text-image retrieval, is to consider the visual feature space as a language constitued of blobs or patches, that we will simply call visual words. Unlike <ref type="bibr" coords="2,456.10,502.96,10.52,10.46" target="#b4">[5]</ref> which was inspired by machine translation, Jeon et al <ref type="bibr" coords="2,278.30,514.93,15.50,10.46" target="#b9">[12]</ref> proposed to extend cross-lingual relevance models to cross-media relevance models. This method estimates the joint probability distribution of blobs that could appear in image and words that could appear in the caption of the image assuming mutual independence between a word and the blobs given an image. These joint probabilities can be used in two ways to annotate/retrieve images. They further showed in <ref type="bibr" coords="2,437.57,562.74,15.50,10.46" target="#b11">[14]</ref> that working directly with continuous features describing the patch instead of working on discrete visual words was even better. Further extensions of this model are: an improved normalization in <ref type="bibr" coords="2,469.02,586.66,15.50,10.46" target="#b10">[13]</ref> and a Bernouilli distribution for text in Feng et al <ref type="bibr" coords="2,284.41,598.61,9.96,10.46" target="#b6">[7]</ref>.</p><p>Relevance Models are a family of pseudo-feedback methods: given a query q and a database of objects o, relevance models first compute the probability P (q|o) and then automatically enhance, enrich the query with textual or visual features. These models, when extended to mixed modalities, can be considered as the ancestors of methods proposed last year by <ref type="bibr" coords="2,381.26,646.43,15.50,10.46" target="#b13">[16]</ref> and <ref type="bibr" coords="2,417.73,646.43,10.52,10.46" target="#b2">[3]</ref> for instance. These last models can be called intermedia feedback, or transmedia feedback techniques. They do not rely on relevance models but act in the same spirit. For example, from a query image, a first visual similarity is computed and an initial set of (assumed) relevant objects is retrieved. As the object are multimodal, each image has also a text, and this text can feed any 'text' feedback method (others than relevance models). In other words, the modality of data is switched , from image to text or text to image, during the (pseudo) feedback process. In that sense, transmedia techniques generalize the pseudo-feedback idea present in cross-media relevance model, but are freed from the particular textual and/or visual models proposed by cross-media relevance model.</p><p>In the remaining of this report, we will first describe our mono-media similarities. The next section will explain the different cross-media similarity models we developed for ImageCLEFphoto 2007. Finally, we will describe our official runs and conclude.</p><p>2 Monomedia Similarities Starting from a traditional bag-of-word representation of pre-processed texts (here, preprocessing includes tokenization, lemmatization, word decompounding and standard stopword removal), we adopt the language modeling approach to information retrieval as the basis of our asymmetric similarity measure:</p><p>• Let p(w|q) be the multinomial language model of a text query q (obtained by maximum likelihood estimates, i.e. by simple counting and normalisation).</p><p>• Let p(w|d), the multinomial language model of a document d. Documents language models are smoothed via a Jelinek-Mercer Method (other schemes are applicable, such as Dirichlet Prior or Absolute Discounting) :</p><formula xml:id="formula_0" coords="3,214.44,359.92,298.56,12.03">p(w|d) = αp M LE (w|d) + (1 -α) p(w|Corpus)<label>(1)</label></formula><p>where p M LE (w|d) (resp. p(w|Corpus)) is simply the ratio of the number of occurrences of w in the textual object d (resp. in the global corpus) to the total document (resp. corpus) length in words.</p><p>The cross-entropy function is used as out textual similarity measure:</p><formula xml:id="formula_1" coords="3,197.97,450.60,315.03,21.32">sim txt (q, d) = CE(q|d) = w p(w|q) log(p(w|d))<label>(2)</label></formula><p>This is obviously an asymmetric similarity measure. It can be trivially generalised to define the similarity between two textual objects d 1 and d 2 :</p><formula xml:id="formula_2" coords="3,173.72,514.67,339.28,22.88">sim txt (d 1 , d 2 ) = CE(d 1 |d 2 ) = w p M LE (w|d 1 ) log(p(w|d 2 ))<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Enriching Text with Flickr</head><p>Motivated by the fact that, this year, the textual content of the documents was very poor (text annotations were limited to the &lt;TITLE&gt; fields of documents), we decided to enrich the corpus thanks to the Flickr database [8], at least for texts in English. Flickr API provide a function to get tags related to a given tag <ref type="bibr" coords="3,223.26,604.80,9.96,10.46">[9]</ref>. According to Flickr documentation, this function returns a list of tags 'related' to the given tag, based on clustered usage analysis. It appears that queries, on the one hand, and photographic annotations on the other hand, adopt a different level of description. Queries are often more abstract and more general than annotations. As a consequence, it is easier and more relevant to enrich the annotations rather than the queries : related tags are often at the same level or at the upper (more general) semantic level. Table <ref type="table" coords="3,403.37,664.58,4.98,10.46" target="#tab_0">1</ref> show some example of enrichment terms, related to the annotation corpus. We can observe the the related terms does encode a kind of semantic similarity, often towards a more abstract direction, but contains also some noise or ambiguity. Below, is an example of an enrich document where each original term has been expanded with its top 20 related terms: Enriching the text corpus partially solved the term mismatch but it also introduced a lot of noise in a document. Hence, most of the probabilitic mass of the language model is devoted to the the original text of a document. In the Language Modelling framework, the enriched terms acts as a smoothing methods: we give more weight to terms from the original text than for those added, by linear interpolation between the original document language model and the word profile derived from Flickr.</p><p>Note that this kind of semantic enrichment was done only for English documents (even if some words in other languages are automatically, and erroneously, added). As we decided to investigate the bilingual case as well (choosing German as the second language), we also built probabilitic translation matrices (ENG -GER) from standard alignment method (Giza++) using the small set of parallel sentences that we were allowed to exploit in the ImageCLEFPhoto 2007 corpus. It appeared that the bilingual lexicons automatically extracted from this parallel corpus provided better results than broader, but more general, standard dictionaries. It is worth to emphasize the fact that such automatically extracted lexicons have often the extra advantage to realize some semantic smoothing by side effect: related -but not strict -translations are often derived as potential candidates. Probabilistic translation dictionaries are applied on the source language models of the query to give the new target language models of the query by matrix product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image</head><p>The image similarity was defined from a continuous vectorial representation of the image, obtained as follows. Image patches are first extracted on regular grids at 5 different scales with a ratio of √ 2 between two consecutive scales. Two types of low-level features are used: grey-level SIFTlike features <ref type="bibr" coords="4,146.78,576.34,15.50,10.46" target="#b12">[15]</ref> and color features. In both cases the image patch is subdivided in 4 × 4 = 16 subregions. SIFT-like features are then computed as gradient orientation histograms (8 bins) collected on each subregion leading to a vector of 128 dimentions. Color features are simple means and standard deviations of the 3 RGB channels in the same subregions, which leads to a 96 dimensional feature vector. The dimensionality of both type of features are subsequently reduced down to 50 using principal component analysis (PCA).</p><p>Then, some kind of Gaussian Mixture Model (GMM) clustering <ref type="bibr" coords="4,389.51,648.07,10.52,10.46" target="#b5">[6,</ref><ref type="bibr" coords="4,403.67,648.07,12.73,10.46" target="#b17">20]</ref> is performed to build a visual vocabulary <ref type="bibr" coords="4,178.58,660.02,15.50,10.46" target="#b18">[21,</ref><ref type="bibr" coords="4,197.65,660.02,7.75,10.46" target="#b3">4]</ref> of low-level image features where each Gaussian component models a visual word (each one is characterized by λ = {w i , µ i , σ i , i = 1...N } where w i , µ i and σ i denote respectively the weight, mean vector and covariance matrix of Gaussian i).</p><p>Two visual vocabularies are built: one is based on texture, the other on color. Both of them have a dictionary size of 64 (meaning that we have 64 Gaussian components fore each).</p><p>Finally, we represent each image with a Fisher Kernel based normalized gradient vector as proposed in <ref type="bibr" coords="4,145.96,731.75,14.61,10.46" target="#b16">[19]</ref>. The main idea is that given a generative model (here the Gaussian Mixture Model) with parameters λ, one can compute the gradient vector of each sample I :</p><formula xml:id="formula_3" coords="5,273.40,132.30,239.60,11.35">∇ λ log p(I|λ) (4)</formula><p>Intuitively, the gradient of the log-likelihood describes the direction in which parameters should be modified to best fit the data. It transforms a variable length sample I into a fixed length vector whose size is only dependent on the number of parameters in the model. Before computing a similarity measure between images, each vector is first normalized using the Fisher Information matrix F λ , as suggested in <ref type="bibr" coords="5,311.29,201.88,14.61,10.46" target="#b8">[11]</ref>:</p><formula xml:id="formula_4" coords="5,210.41,223.65,302.59,11.35">F λ = E X [∇ λ log p(X|λ)∇ λ log p(X|λ) ] .<label>(5)</label></formula><p>The normalized gradient vector, simply called Fisher vector, is thus given by:</p><formula xml:id="formula_5" coords="5,243.99,265.92,269.01,15.50">f = F -1/2 λ ∇ λ log p(X|λ) .<label>(6)</label></formula><p>See <ref type="bibr" coords="5,107.70,292.77,15.50,10.46" target="#b16">[19]</ref> for closed form approximations of F -1/2 λ . The Fischer vectors for color and texture respectively are then simply concatened. To compute the similarity measure between images I and J, we simply use the the L1-norm of the difference between the Fisher vectors:</p><formula xml:id="formula_6" coords="5,160.06,350.97,348.69,23.19">sim Img (I, J) = norm max -||f I -f J || = norm max - i |f i I -f i J | (<label>7</label></formula><formula xml:id="formula_7" coords="5,508.76,352.54,4.24,10.46">)</formula><p>where f i are the elements of the vector f and norm max = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Media Similarities</head><p>We want to define cross-media similarity measures that are more elaborated -and, hopefully, more efficient -than simple late fusion approaches. What we want to investigate here is some "intermediate level" fusion approaches, where we use some principles coming from pseudo-relevance feedback and, more specifically, use transmedia pseudo-relevance feedback for enriching the monomedia representation of an object with features coming from the other media. Our basic material is a multimedia (text + image) database O : in other words, we assume to have at our disposal a collection of images with associated text (for each image) or, in a dual view, a collection of texts illustrated with one image (for each textual element).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transmedia Document Reranking Approach</head><p>The main idea is the following: for a given image i, consider as new features the (textual) terms of the texts associated to the most similar images (from a purely visual viewpoint). We will denote this neighbouring set as N img (i). Its size is fixed a priori: this is typically the topN objects returned from a retrieval system using equation 7 as ranking measure. Then we can compute a new similarity with respect to any multimodal object j of the collection O as the textual similarity of this new representation of the image i with the textual parts of j.</p><p>We still need to be more precise on how we compute the mono-media similarity between an aggregate of objects N img (i) and one single multimodal object. There are three families of approaches:</p><p>1. aggregating N img (i) to form a single object (typically by concatenation) and then compute standard similarity between two objects;</p><p>2. aggregating all similarity measures (assuming we can) between all possible couple of objects 3. use a method of pseudo feedback algorithm (for instance Rochhio's algorithm) to extract relevant, meaningfull features of an aggregate and finally use a mono-media similarity.</p><p>The first approach we propose belongs to the family 2 (using a simple sum or, equivalently, an arithmetic average to aggregate the individual similarity measures). The next section (Complementary Feedback) will propose an alternative approach that belongs to family 3.</p><p>More formally, if we denote by T (u) the text associated to multimodal object u and by T (i) the new textual representation of image i, then the new cross-media similarity measure w.r.t. the multimodal object j is:</p><formula xml:id="formula_8" coords="6,155.29,190.61,357.70,24.76">sim ImgT xt (i, j) = sim txt ( T (i), T (j)) = d∈Nimg(i) sim txt (T (d), T (j)) (8)</formula><p>where sim txt is typically defined by equation 3 (e.g. the one based on Language Modelling, even if it is assymetric). This method can be seen as a reranking method. Suppose that q is some image query; if T (d) is the text of an image belonging to the initial feedback set N img (q), then the rank of the own neighbors of T (d) in the textual sense will be increased, even if they are not so similar from a purely visual viewpoint. In particular, this allows to define a similarity between a purely image query and a simple textual object without visual counterpart. To sum up, the main idea of our method amounts to (i) perform an initial retrieval step to identify N img (q), (ii) to switch mode and to virtually make several queries (one for each element in N img (q) instead of one) and combining them afterwards. Due to renormalization effects and smoothing methods, the resulting ranking function is different from the one obtained by considering the simple concatenation of text in step (ii), since the considered models are not linear. Lastly, the values sim txt (T (u), T (v)) can be pre-computed in a matrix of textual similarities between all pairs of objects in the multimedia database O, if the corpus is of reasonable size By duality, we can define another cross-media similarity measure: for a given text i, we consider as new features the Fisher vectors of the images associated to the most similar texts (from a purely textual viewpoint) in the multimodal database. We will denote this neighbouring set as N txt (i). If we denote by I(u) the image associated to multimodal object u and by Î(i) the new visual representation of text i, then the new cross-media similarity measure is:</p><formula xml:id="formula_9" coords="6,156.96,461.83,356.04,24.76">sim T xtImg (i, j) = sim img ( Î(i), I(j)) = d∈Ntxt(i) sim img (I(d), I(j)) (9)</formula><p>where sim img is typically defined by equation 7 Note that we could even extend these definitions inside one mode. For instance, we have:</p><formula xml:id="formula_10" coords="6,157.72,529.05,204.56,24.76">sim T xtT xt (i, j) = sim txt ( T (i), T (j)) = d∈Ntxt(i)</formula><p>sim txt (T (d), T (j)) <ref type="bibr" coords="6,495.28,531.57,17.71,10.46" target="#b7">(10)</ref> and</p><formula xml:id="formula_11" coords="6,154.53,573.45,354.04,25.35">sim ImgImg (i, j) = sim img ( Î(i), I(j)) = d∈N img (i) sim img (I(d), I(j)) (<label>11</label></formula><formula xml:id="formula_12" coords="6,508.57,575.98,4.43,10.46">)</formula><p>Once again, the process could be fast and efficient, if we can precompute the similarity matrices sim img (I(u), I(v)) and/or sim txt (T (u), T (v)) for all pairs (u, v) of mulmodal objects in O.</p><p>Finally, we can combine all the similarities to define a global similarity measure between two multi-modal objects i and j: for instance, using a linear combination,</p><formula xml:id="formula_13" coords="6,90.00,662.11,423.00,22.41">sim glob (i, j) = λ 1 sim txt (T (i), T (j))+λ 2 sim img (I(i), I(j))+λ 3 sim ImgT xt (i, j)+λ 4 sim T xtImg (i, j) (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complementary Feedback</head><p>Recall that the fundamental problem in transmedia feedback is to define how we compute the mono-media similarity between an aggregate of objects N img (i) (or N txt (i)) and one single multimodal object. Instead of adopting the strategy of the previous section, we would now consider the set N img (i) as the "relevance concept" F and derive its corresponding language model (LM) θ F . Afterwards, we can use the Cross-entropy criterion between θ F and the LM of the textual part of any object j in O as the new transmedia similarity. We illustrate this approach when we use N img (i) (using only the image part of query object i) to derive a textual LM of θ F that can be used in conjunction with the original LM of the textual part of query i.</p><p>To this aim (build the LM of the relevance concept F), we use a pseudo-feedback method issued from the language modelling approach to information retrieval, namely the mixture model method from Zhai and Lafferty <ref type="bibr" coords="7,231.05,194.22,15.50,10.46" target="#b20">[23]</ref> originally designed to enrich textual queries (however, more elaborated techniques of feedback for language models can also be envisaged : e.g. <ref type="bibr" coords="7,453.37,206.18,14.76,10.46" target="#b19">[22]</ref>). Let θ F be a multinomial parameter, standing for the distribution of relevant terms in F: in other words θ F is a probability distribution over words but peaked on relevant terms. A generative model is assumed to estimate θ F from F:</p><formula xml:id="formula_14" coords="7,188.78,263.39,324.22,23.80">P (F|θ) = d∈Nimg(i) w (λθ F,w + (1 -λ)P (w|C)) c(w,d)<label>(13)</label></formula><p>where P (w|C) is word probability built upon the corpus, λ is a fixed parameter, which can be understood as a noise parameter for the distribution of terms. c(w, d) is the number of occurence of term w in document d. Finally θ F is learned by maximum likelihood with an Expectation Maximization algorithm. Once θ F has been estimated, a new query LM can be obtained trough interpolation:</p><formula xml:id="formula_15" coords="7,222.25,358.49,286.32,11.36">θ new query = αθ old query + (1 -α)θ F (<label>14</label></formula><formula xml:id="formula_16" coords="7,508.57,358.49,4.43,10.46">)</formula><p>where θ old query corresponds to the LM of the textual part of the query i. As mentionned, we then use the Cross-Entropy similarity measure to perform a new retrieval on the textual part of objects in O.</p><p>Setting the value of α is done experimentally and adapted to the particular collection. The robustness of the estimation of θ F has a significant impact on the value of α. Lastly, the value of α can be interpreted as a mixing weight between image and text.</p><p>Finally, note that we illustrated the approach using N img (i) to derive a textual LM of θ F that can be used in conjunction with the original LM of the textual part of query i. But we can derive a similar scheme using N txt (i) to derive a new representation (actually some generalized Fisher Vectors) of the "relevance concept", this time relying on Rocchio's method that is more adapted to continuous feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Description of submitted runs</head><p>For a description of the task, we refer to the overview paper <ref type="bibr" coords="7,355.81,550.75,14.61,10.46" target="#b7">[10]</ref>.</p><p>Table <ref type="table" coords="7,132.34,562.70,4.98,10.46" target="#tab_1">2</ref> shows the name of our runs and the corresponding mean average precision measures.</p><p>Below is a detailed description of our official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EN-EN-AUTO-FB-TXT FLR</head><p>This run was a pure text run: documents were basically preprocessed and each document was enriched using Flickr database. For each term of a document, its top 20 related tags from Flickr were added to the document. Then, a unigram language model for each document is estimated, giving more weight to the original document terms. An additional step of pseudo-relevance feedback using the method explained in <ref type="bibr" coords="7,225.65,668.76,15.50,10.46" target="#b20">[23]</ref> is then performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">AUTO-NOFB-IMG COMBFK</head><p>This run is a pure image run: it uses Fisher Kernel metric (cf. equation 7) to define the image similarity. As a query encompasses 3 visual sub-queries, we have to combine the similarity score with respect to these 3 subqueries. To this aim, the result lists from the image sub-queries are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AUTO-FB-TXTIMG PREFFKTXT.off</head><p>This run uses both texts and images: it starts from query images only, to determine N img (i) for each query i (as in the previous run above) and then implements the method described by eq. 8.</p><p>The size of the neighbouring set is 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AUTO-FB-TXTIMG PREFFKTXT FLR</head><p>It is basically the same algorithm as the preceding run, except that the textual part of the data (annotations) is enriched with Flickr tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">EN-EN-AUTO-FB-TXTIMG QTXT COMBPREFFKTXT.off</head><p>This run uses the same algorithm as in AUTO-FB-TXTIMG PREFFKTXT but with one more step at the end, that amounts to merge the result lists from in AUTO-FB-TXTIMG PREFFKTXT and from the purely text queries (EN-EN-AUTO-FB-TXT FLR), by summing the relevance scores after normalisation (by substracting the mean and dividing by the standard deviation for each list).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">EN-EN-AUTO-FB-TXTIMG MPRF.off</head><p>This run uses both texts and images: it starts from query images only, to determine the relevance set N img (i) for each query i (as in the run AUTO-FB-TXTIMG PREFFKTXT.off) and then implements the method described as "the complementary (intermedia) feedback" in section 3.2. The size of the neighbouring set is 15. Refering to the notations of section 3.2, the values of λ and α are respectively 0.5 and 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">DE-EN-AUTO-FB-TXTIMG MPRF FLR.off</head><p>This runs works with the same principle as the previous run EN-EN-AUTO-FB-TXTIMG MPRF.off. The main difference is that (target) english documents have been enriched with Flickr and that the initial query -in German -was translated by multiplying its "Language Model" by the probabilistic translation matrix extracted from the (small) parallel part of the corpus. Otherwise, it uses the same parameters as previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">EN-DE-AUTO-FB-TXTIMG MPRF.off</head><p>This run uses the same process as in EN-EN-AUTO-FB-TXTIMG MPRF.off. The difference is the starting point: english queries to search for german annotations. English queries are translated with the probabilistic translation matrix extracted from the (small) parallel part of the corpus and the translated queries follow the same process as in EN -EN -AU T O -F B -T XT IM G M P RF.of f but with different parameter : the size of the neighbouring set is 10, while the values of λ and α are respectively 0.5 and 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>With a slightly annotated corpus of images, also characterised by an abstraction level in the textual description that is significantly different from the one used in the queries, it appears that mono-media retrieval performance is more or less equivalent for pure image and pure text content (around 20% MAP). Using our transmedia pseudofeedback-based similarity measures allowed us to dramatically increase the performance by ∼50% (relative). Trying to model the textual "relevance concept" present in the top ranked documents issued from a first (purely visual) retrieval and combining this with the textual part of the original query turns out to be the best strategy, being slightly superior to our transmedia document reranking method. Enriching the image annotations by extra tags extracted from the Flickr database does not offer a significant advantage in the ImageCLEF07 corpus, even if we observed an improvement using other multimedia corpora and query sets. From a cross-lingual perspective, the use of domain-specific, corpus-adapted probabilistic dictionaries seems to offer better results than the use of a broader, more general standard dictionary. With respect to the monolingual baseline, multilingual runs show a slight degradation of retrieval performance ( ∼6 to 10% relative).</p><p>In the future, we want to investigate more systematically and more thoroughly the ways to combine the numerous transmedia similarity measures we introduced in this report, by determining in which cases they can provide us with significant advantages with respect to more traditional "late fusion" approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.01,189.52,57.32,12.55;3,90.00,209.59,216.34,10.46"><head>2. 1</head><label>1</label><figDesc>Text 2.1.1 Cross-Entropy measure of similarity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,114.90,117.46,373.21,175.99"><head>Table 1 :</head><label>1</label><figDesc>Corpus Terms and their related terms from Flickr Termas de Papallacta Papallacta Ecuador ADDED TERMS: chillan colina sur caracalla cajon piscina snow roma italy maipo thermal nieve volcan argentina mendoza water italia montaa araucania santiago quito southamerica germany worldcup soccer football bird andes wm church fifa volcano iguana cotopaxi travel mountain mountains cathedral sealion market</figDesc><table coords="4,114.90,129.16,333.49,116.46"><row><cell>Corpus Term</cell><cell>Top 5 related Terms</cell></row><row><cell>Jesus</cell><cell>christ, church, cross, religion, god</cell></row><row><cell>classroom</cell><cell>school, class,students, teacher, children</cell></row><row><cell>hotel</cell><cell>lasvegas, building, architecture, night</cell></row><row><cell>Riviera</cell><cell>france, nice, sea, beach, french</cell></row><row><cell>Ecuador</cell><cell>galapagos, quito, southamerica, germany, worldcup</cell></row><row><cell cols="2">DOCNO: annotations/00/116.eng</cell></row><row><cell>ORIGINAL TEXT:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,117.46,423.03,235.01"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="8,90.00,117.46,423.03,235.01"><row><cell>: Official Runs Run Name</cell><cell>MAP</cell></row><row><cell>TextT Only</cell><cell></cell></row><row><cell>EN-EN-AUTO-FB-TXT FLR</cell><cell>0.2075</cell></row><row><cell>Image Only</cell><cell></cell></row><row><cell>AUTO-NOFB-IMG COMBFK</cell><cell>0.1890</cell></row><row><cell>Image and Text</cell><cell></cell></row><row><cell>Transmedia Reranking</cell><cell></cell></row><row><cell>AUTO-FB-TXTIMG PREFFKTXT</cell><cell>0.2801</cell></row><row><cell>AUTO-FB-TXTIMG PREFFKTXT FLR</cell><cell>0.2761</cell></row><row><cell cols="2">EN-EN-AUTO-FB-TXTIMG QTXT COMBPREFFKTXT 0.3020</cell></row><row><cell>Complementary Feedback</cell><cell></cell></row><row><cell>EN-EN-AUTO-FB-TXTIMG MPRF</cell><cell>0.3168</cell></row><row><cell>DE-EN-AUTO-FB-TXTIMG MPRF FLR</cell><cell>0.2899</cell></row><row><cell>EN-DE-AUTO-FB-TXTIMG MPRF</cell><cell>0.2776</cell></row><row><cell cols="2">renormalized (by substracting the mean and dividing by the standard deviation) and merged by</cell></row><row><cell>simple sum.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Aknowledgments</head><p>This work was partly funded by the <rs type="funder">French Government</rs> under the <rs type="projectName">Infomagic</rs> project, part of the <rs type="funder">Pole CAP DIGITAL (IMVN) de Paris, Ile-de-France</rs>. The authors also want to thank <rs type="person">Florent Perronin</rs> for his greatly appreciated help in applying some of the Generic Visual Categorizer (GVC) components in ImageCLEF07 experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GMKSByf">
					<orgName type="project" subtype="full">Infomagic</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.47,631.95,371.77,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,268.66,631.95,108.48,10.46">Modeling annotated data</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,398.43,631.95,52.63,10.46">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.47,651.88,402.54,10.46;9,110.48,663.83,155.18,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,327.22,651.88,185.80,10.46;9,110.48,663.83,76.86,10.46">A statistical model for general contextual object recognition</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,208.46,663.83,24.96,10.46">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.47,683.76,402.54,10.46;9,110.48,695.71,402.53,10.46;9,110.48,707.66,148.00,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,244.21,683.76,268.80,10.46;9,110.48,695.71,382.53,10.46">Approaches of using a word-image ontology and an annotated image corpus as intermedia for cross-language image retrieval, clef 2006 working notes</title>
		<author>
			<persName coords=""><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,707.66,117.46,10.46">CLEF 2006 Working Notes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.47,727.59,402.53,10.46;9,110.48,739.55,372.29,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,375.65,727.59,137.36,10.46;9,110.48,739.55,51.00,10.46">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,182.75,739.55,269.57,10.46">ECCV Workshop on Statistical Learning for Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,110.53,402.54,10.46;10,110.48,122.49,333.26,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,375.79,110.53,137.23,10.46;10,110.48,122.49,254.70,10.46">Object recognition as machine translation :learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,386.55,122.49,24.95,10.46">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,142.41,402.54,10.46;10,110.48,154.37,291.62,10.46" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m" coord="10,356.48,142.41,156.53,10.46;10,110.48,154.37,58.97,10.46">Improving &quot;bag-of-keypoints&quot; image categorisation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Southampton</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,110.47,174.29,402.54,10.46;10,110.48,186.26,150.31,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,295.08,174.29,217.93,10.46;10,110.48,186.26,71.58,10.46">Multiple bernoulli relevance models for image and video annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.46,186.26,25.06,10.46">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,246.03,402.54,10.46;10,110.48,257.99,402.52,10.46;10,110.48,269.94,91.08,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,360.16,246.03,152.86,10.46;10,110.48,257.99,138.80,10.46">Overview of the ImageCLEFphoto 2007 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,269.87,257.99,190.93,10.46">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,289.86,402.53,10.46;10,110.48,301.82,269.45,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,246.11,289.86,248.11,10.46">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,301.82,226.93,10.46">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,321.74,402.54,10.46;10,110.48,333.70,230.42,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,300.25,321.74,212.77,10.46;10,110.48,333.70,125.01,10.46">Automatic image annotation and retrieval using cross-media relevance models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.08,333.70,52.63,10.46">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,353.62,402.54,10.46;10,110.48,365.58,77.56,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,288.13,353.62,221.03,10.46">Models for automatic video annotation and retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,122.93,365.58,33.51,10.46">ICASSP</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,385.50,402.53,10.46;10,110.48,397.47,52.39,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,294.76,385.50,200.58,10.46">A model for learning the semantics of pictures</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,397.47,21.10,10.46">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,417.39,392.44,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,167.54,417.39,245.39,10.46">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,421.75,417.39,21.10,10.46">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,437.32,402.54,10.46;10,110.48,449.27,402.51,10.46;10,110.48,461.22,93.87,10.46" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,363.85,437.32,149.17,10.46;10,110.48,449.27,331.33,10.46">Ipal inter-media pseudo-relevance feedback approach to imageclef 2006 photo retrieval, clef 2006 working notes</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,462.56,449.27,50.43,10.46;10,110.48,461.22,63.32,10.46">CLEF 2006 Working Notes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,481.15,402.54,10.46;10,110.48,493.10,116.35,10.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,254.28,481.15,258.74,10.46;10,110.48,493.10,21.72,10.46">Plsa-based image auto-annotation: Constraining the latent space</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,153.42,493.10,40.53,10.46">ACM MM</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,513.03,402.53,10.46;10,110.48,524.98,367.05,10.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,327.84,513.03,185.17,10.46;10,110.48,524.98,29.06,10.46">Gcap: Graph-based automatic image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,160.55,524.98,286.43,10.46">CVPR Workshop on Multimedia Data and Document Engineering</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,544.91,402.53,10.46;10,110.48,556.86,69.79,10.46" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,239.35,544.91,269.41,10.46">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,122.93,556.86,25.06,10.46">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,576.79,402.53,10.46;10,110.48,588.75,137.75,10.46" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,342.20,576.79,170.81,10.46;10,110.48,588.75,59.43,10.46">Adapted vocabularies for generic visual categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bressan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.02,588.75,24.96,10.46">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,608.67,402.54,10.46;10,110.48,620.63,146.21,10.46" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,241.76,608.67,271.25,10.46;10,110.48,620.63,25.43,10.46">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,157.01,620.63,22.62,10.46">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,640.55,402.54,10.46;10,110.48,652.51,402.53,10.46;10,110.48,664.46,360.29,10.46" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,205.49,640.55,307.53,10.46;10,110.48,652.51,35.68,10.46">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.89,652.51,347.13,10.46;10,110.48,664.46,233.55,10.46">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,684.39,402.53,10.46;10,110.48,696.34,233.85,10.46" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,240.83,684.39,272.18,10.46;10,110.48,696.34,88.67,10.46">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,219.86,696.34,24.27,10.46">CIKM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
