<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.52,70.53,390.43,10.16">Medical Image Retrieval and Automatic Annotation: OHSU at ImageCLEF 2007</title>
				<funder ref="#_DVr2uX6">
					<orgName type="full">NLM</orgName>
				</funder>
				<funder ref="#_vDsegZB">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,241.74,96.45,128.49,10.16"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,272.52,109.41,66.79,10.16"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health</orgName>
								<orgName type="institution">Science University Portland</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.52,70.53,390.43,10.16">Medical Image Retrieval and Automatic Annotation: OHSU at ImageCLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08B505FFD5642F4505D2F4397FC0B176</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Performance, Image annotation, Experimentation, Algorithms Query parsing, Text retrieval, Image modality classification, Neural networks Medical Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Oregon Health &amp; Science University participated in the medical retrieval and medical annotation tasks of ImageCLEF 2007. In the medical retrieval task, we created a webbased retrieval system for the collection built on a full-text index of both image and case annotations. The text-based search engine was implemented in Ruby using Ferret, a port of Lucene, and a custom query parser. In addition to this textual index of annotations, supervised machine learning techniques using visual features were used to classify the images based on image acquisition modality. All images were annotated with the purported modality. Purely textual runs as well as mixed runs using the purported modality were submitted. Our runs performed moderately well using the MAP metric and better for the early precision (P10) metric. In the automatic annotation task, we used the 'gist' technique to create the feature vectors. Using statistics derived from a set of multi-scale oriented filters, we created a 512 dimensional vector. PCA was then used to create a 100-dimensional vector. This feature vector was fed into a two layer neural network. Our error rate on the 1000 test images was 67.8 using the hierarchical error calculations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Introduction</head><p>Image retrieval systems do not currently perform as well as their text counterparts <ref type="bibr" coords="1,436.38,662.50,9.97,8.48" target="#b0">[1]</ref>. Medical and other image retrieval systems have historically relied on annotations or captions associated with the images for indexing the retrieval system. The labor-intensive task of indexing and cataloging the images in these collections has traditionally been performed manually, a process that can be subjective and prone to errors.</p><p>The last few decades have seen numerous advancements in the area of content-based image retrieval (CBIR) <ref type="bibr" coords="2,142.92,80.80,10.20,8.48" target="#b1">[2,</ref><ref type="bibr" coords="2,153.12,80.80,6.80,8.48" target="#b2">3]</ref>. Although CBIR systems have demonstrated success in fairly constrained medical domains including pathology, dermatology, chest radiology, and mammography, they have demonstrated poor performance when applied to databases with a wide spectrum of imaging modalities, anatomies and pathologies <ref type="bibr" coords="2,256.58,113.26,10.88,8.48" target="#b0">[1,</ref><ref type="bibr" coords="2,267.45,113.26,7.25,8.48" target="#b3">4,</ref><ref type="bibr" coords="2,274.71,113.26,7.25,8.48" target="#b4">5,</ref><ref type="bibr" coords="2,281.96,113.26,7.94,8.48" target="#b5">6 ]</ref>.</p><p>Retrieval performance has shown demonstrable improvement by fusing the results of textual and visual techniques. This has especially been shown to improve early precision <ref type="bibr" coords="2,399.92,140.50,10.21,8.48" target="#b6">[7,</ref><ref type="bibr" coords="2,410.13,140.50,6.80,8.48" target="#b7">8]</ref>. The medical image retrieval task within ImageCLEF (ImageCLEFmed) 2007 campaign is a TREC-style <ref type="bibr" coords="2,432.92,151.36,10.94,8.48" target="#b8">[9]</ref> and provides a forum and set of test collections for the medical image retrieval community to use to benchmark their algorithms on a set of queries. The ImageCLEF campaign has, since 2003, been a part of the Cross Language Evaluation Forum (CLEF) <ref type="bibr" coords="2,251.10,183.82,11.50,8.48" target="#b8">[9,</ref><ref type="bibr" coords="2,262.60,183.82,11.50,8.48" target="#b9">10,</ref><ref type="bibr" coords="2,274.09,183.82,11.50,8.48" target="#b10">11]</ref> which is an offshoot from the Text Retrieval Conference (TREC, trec.nist.gov).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. System description of our adaptive medical image retrieval system</head><p>The ImageCLEF medical retrieval collection consists of about 66,000 medical images and annotations associated with them. We wanted to create a flexible database schema that could incorporate new collections easily while facilitating retrieval using both text and visual techniques. The text annotations in the collection are currently indexed and we continue to add indexable fields for incorporating visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database and web application</head><p>We used the Ruby programming language, with the open source Ruby On Rails web application framework [http://www.ruby-lang.org, http://www.rubyonrails.org ].</p><p>A PostgreSQL relational database was used to store the images and annotations.</p><p>The database has images from the four different collections that were part of the ImageCLEFmed 2006 image retrieval challenge as well as two new collections for 2007. The approximately 66,000 images in these collections resided in cases, with annotations in English, German and/or French. The collections themselves were substantially heterogeneous in their architectures. Some collections had only one image per case while others had many images per case. Annotation fields were also quite different among the collections. Some collections had case-based annotations while others had image-based annotations. This difference is especially significant for text based retrieval as images of different modalities or anatomies or pathologies could be linked to the same case annotation. In this situation, even though only one image from a case containing many images might be relevant to a query (based on the annotation), all images for the case would be retrieved in a purely text based system, reducing the precision of the search. We used the relational database to maintain the mappings between the collections, the cases in the collections, the cased-based annotations, the images associated with a collection, and the image based annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Processing and Analysis</head><p>The image itself has important visual characteristics such as color and texture that can help in the retrieval process. Images that may have had information about the imaging modality or anatomy or view associated with them as part of the DICOM header can lose that information when the image is compressed to become a part of a teaching or on-line collection, as the image format used by these collections is usually compressed JPEG.</p><p>We created additional tables in the database to store image information that was created using a variety of image processing techniques in MATLAB (www.mathworks.com). For instance, the images in the collection typically do not contain explicit details about the imaging modality. In previous work <ref type="bibr" coords="2,474.66,596.80,9.96,8.48" target="#b7">[8]</ref>, we have described our modality classifier that can identify the imaging modality for medical images with a high level of confidence (&gt;95% accuracy on the database used for the validation). Grey scale images are classified into a set of modalities including x-rays, CT, MRI, ultrasound and nuclear medicine. Color image classes include gross pathology, microscopy, and endoscopy.</p><p>Each image was annotated in the database with the purported image modality and a confidence value. This can be extremely useful for queries where the user has specified a desired image modality. An example query from ImageCLEF 2006 was "Show me microscopic images of tissue from the cerebellum".</p><p>The precision of the result of such a query can be improved significantly by restricting the images returned to those of the modality desired <ref type="bibr" coords="3,274.26,80.80,10.05,8.48" target="#b7">[8]</ref>. This is especially useful in eliminating images of the incorrect modality that may be part of a case containing a relevant image from the returned list of images. However, this increase in precision may result in a loss in recall if the classification algorithm incorrectly classifies the image modality.</p><p>We continue to experiment with a variety of image clustering and classification algorithms and adding the numerical data and labels to the database. Clustering images that look visually similar can be again used to improve the precision of the image retrieval process and speed up the system searching of images in the same cluster as the query image (if available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query parser and Search Engine</head><p>The system presents search options to the user including Boolean OR, AND and exact match. There are also options to perform fuzzy searches and custom query parsing.</p><p>The cornerstone of our system is the query parser, written in Ruby. Ferret, a Ruby port of the popular Lucene system, was used in our system as the underlying search engine [http://ferret.davebalmain.com].</p><p>Queries were first analyzed using MedPost, a Parts-of-Speech (POS) A simple Bayesian classifier [classifier.rubyforge.org] was trained to discern the desired image modality from the query, if available. The classifier performed extremely well within the constrained vocabulary of imaging modalities. Stop words were then removed from the query. These include Standard English stop words as well as a small set of stop words determined by analyzing queries from the last three years, including 'finding', 'showing', 'images', 'including' and 'containing'.</p><p>The system is also linked to the UMLS metathesaurus. The user can choose to perform automatic query expansion using synonyms from the metathesarus.</p><p>A sample query "Show me CT images with a brain infarction" is automatically parsed and the following information is extracted from it: CT-&gt; imaging modality, brain -&gt; anatomic location, infarction -&gt; finding. This information can be used to combine the results of the textual and visual systems more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Runs submitted</head><p>We submitted a total of 10 runs. These runs included textual and mixed, automatic and manual options. We also submitted runs using different weighted combinations of the FIRE baseline (published by the organizers) with our baseline textual runs. The run labels and a short description is given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Results and Discussion</head><p>Table <ref type="table" coords="3,134.64,513.52,4.71,8.48" target="#tab_2">2</ref> presents the mean average precision (MAP) and early precision of the OHSU runs as well that of the best overall run. Our baseline textual system performed quite well, with a MAP of 0.3453. There was only a minimal improvement in performance with the use of the image modality. Our system is tuned to improve precision, at the cost of recall as we believe that most real users are interested in early precision. Consequently, our system demonstrated good early precision, with the highest P10 and high P30 results. Our baseline textual system performed quite well, with a MAP of 0.3453. There was only a minimal improvement in performance with the use of the image modality. Our system is tuned to improve precision, at the cost of recall as we believe that most real users are interested in early precision. Consequently, our system demonstrated good early precision, with the highest P10 and high P30 results.  Although there was significant inter-category variation, our textual runs performed well on textual queries and more poorly on visual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>We will continue to improve our image retrieval system by adding more image tags using automatic visual feature extraction. Our next goal is to annotate the images with the their anatomical location and view attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Automatic Image Annotation</head><p>The goal of this task was to correctly classify 1000 radiographic medical images using the hierarchical IRMA code. This code classifies the image along the modality, body orientation, body region, and biological system axes. There were 116 unique classes. The task organizers provided a set of 9,000 training images and 1000 development images. The goal of the task was to classify the images to the most precise level possible, with a greater penalty applied for incorrect classification than for a less specific classification in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Introduction</head><p>A supervised machine learning approach using global gist features and neural network architecture was employed for the task of automatic annotation of medical images with the IRMA code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. System Description</head><p>The automatic image annotation was based on a neural network classifier using Gist features <ref type="bibr" coords="5,463.84,654.94,14.46,8.48" target="#b13">[14]</ref>. The classifiers were created in MATLAB using the Netlab toolbox <ref type="bibr" coords="5,347.48,665.74,14.42,8.48" target="#b14">[15]</ref>.</p><p>All images were convolved with a set of 32 multiscale-oriented Gabor filters. We created a 512dimensional vector using statistics from these filters. Principal component analysis was then used to reduce the dimensionality of the vector to 100.</p><p>A multilayer perceptron with one hidden layer containing 250-500 nodes was used to create and train a multi-class classifier. The training data set of 10,000 images was used to optimize performance of the development set of 1000 images. The final configuration of the classifier used 300 hidden nodes.</p><p>Images originally classified as classes 108 and 111 in the old code were commonly misclassified by the neural network classifier described above. To handle this special case, we created a second layer of classification built around a support vector machine (SVM) using scale-invariant feature transform (SIFT) features <ref type="bibr" coords="6,176.27,140.50,15.75,8.48" target="#b15">[16]</ref> as inputs. This new binary classify was used to determine the final class assignments for images in classes 108 and 111 c. Runs submitted OHSU submitted two runs for the automatic annotation task. The first run used gist feature vectors to train the multi-layer perceptron. A neural network was used to create a multi-class classifier consisting of 116 classes. These were the original classes from 2006 and did not use the hierarchical nature of the IRMA code. These classes were then converted to the IRMA code, as required for the submission in 2007.</p><p>The second run used a hierarchical classifier architecture, with the first layer as described above and the second classifier using SIFT features and an SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Results and Analysis</head><p>The relationship between semantic and visual hierarchy remains an open area of research. Based on our experiments using this collection of images used for automatic annotation, the use of hierarchy of the semantic classes did not improve our automatic annotations.</p><p>The error count for both our runs were quite similar at 67.8 and 67.97 for 1000 images, compared to the best count of 26.84 and worst count of 505.61. There was only a very slight improvement in using the two-layer classifier. There were 227 errors using the 2006 classes, which corresponds to an classification accuracy of 77.3%. However, of these 227 errors, only 15 were wrong along all 4 axes. 76 were misclassified along two axes (primarily view and anatomy) while 12 were misclassified along 3 axes. 77 of our single misclassifications were along the view axis. A significant portion of these occurred where class 111 was misclassified as 108, an error due to confusion between posterioranterior and anterior-posterior views of the chest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e. Future Work</head><p>We would like to further investigate the mapping between the semantic and visual hierarchy of images in the IRMA collection. We also plan to further explore specifying the image classification in the semantic hierarchy only to the level</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,110.52,367.30,223.34,8.48;4,110.58,251.70,406.14,55.26"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screen shot of our system displaying user options</figDesc><graphic coords="4,110.58,251.70,406.14,55.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,110.52,257.62,391.02,30.14"><head></head><label></label><figDesc>//ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/medpost] [14].</figDesc><table coords="3,110.52,257.62,391.02,30.14"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Tagger created using the Medline</cell></row><row><cell>corpus,</cell><cell>and</cell><cell>distributed</cell><cell>by</cell><cell>the</cell><cell>National</cell><cell>Library</cell><cell>of</cell><cell>Medicine</cell></row><row><cell>[ftp:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,110.52,469.30,444.31,176.84"><head>Table 1 :</head><label>1</label><figDesc>OHSU run labels and descriptions</figDesc><table coords="4,126.90,469.30,427.93,154.76"><row><cell>3</cell><cell>OHSU-OHSU_txt_exp2.eval</cell><cell>Textual run using UMLS term expansion (automatic)</cell></row><row><cell></cell><cell></cell><cell>Weighted combination of FIRE and OHSU textual runs</cell></row><row><cell>4</cell><cell>OHSU-oshu_c_e_f_q.eval</cell><cell>(automatic)</cell></row><row><cell>5</cell><cell>OHSU-oshu_man2.eval</cell><cell>Manual query modification, purely textual (manual)</cell></row><row><cell></cell><cell></cell><cell>Weighted combination of FIRE and OHSU textual runs</cell></row><row><cell>6</cell><cell>ohsu_comb3_ef_wt1_rev1_c.txt.eval</cell><cell>(automatic)</cell></row><row><cell></cell><cell></cell><cell>Weighted combination of FIRE and OHSU textual runs</cell></row><row><cell>7</cell><cell>ohsu_fire_ef_wt2_rev1_c.txt.eval</cell><cell>(automatic)</cell></row><row><cell></cell><cell></cell><cell>Mixed run using modality to resort the results of the standard</cell></row><row><cell></cell><cell></cell><cell>textual baseline</cell></row><row><cell>8</cell><cell>ohsu_m2_rev1_c.txt.eval</cell><cell>(automatic)</cell></row><row><cell>9</cell><cell>ohsu_text_e4_out_rev1.txt.eval</cell><cell>Textual run using modified query parser (automatic)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,110.52,267.52,117.89,109.10"><head>Table 2 :</head><label>2</label><figDesc>Results of OHSU runs</figDesc><table coords="5,110.52,317.38,105.25,59.24"><row><cell>Run Type</cell><cell>MAP</cell></row><row><cell>Visual</cell><cell>0.23</cell></row><row><cell>Mixed</cell><cell>0.35</cell></row><row><cell>Textual</cell><cell>0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,110.52,390.76,199.45,8.48"><head>Table 3 :</head><label>3</label><figDesc>Results of OHSU best run by topic category</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge the support of <rs type="funder">NLM</rs> <rs type="grantName">Training</rs> Grant <rs type="grantNumber">1T15 LM009461</rs> and <rs type="funder">NSF</rs> Grant <rs type="grantNumber">ITR-0325160</rs>. We would also like to thank <rs type="person">Steven Bedrick</rs>, <rs type="affiliation">DMICE</rs>, <rs type="affiliation">OHSU</rs> for his help in creating the web-based image retrieval system.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DVr2uX6">
					<idno type="grant-number">1T15 LM009461</idno>
					<orgName type="grant-name">Training</orgName>
				</org>
				<org type="funding" xml:id="_vDsegZB">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,114.07,542.92,387.44,8.48;6,127.50,553.78,242.54,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,232.20,542.92,269.30,8.48;6,127.50,553.78,51.26,8.48">Advancing biomedical image retrieval: development and analysis of a test collection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,184.74,553.78,105.32,8.48">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="488" to="496" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,564.64,387.44,8.48;6,127.50,575.38,345.70,8.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,265.91,564.64,231.56,8.48">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">Awm</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,127.50,575.38,247.05,8.48">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,586.24,387.24,8.48;6,127.50,597.04,195.72,8.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,225.54,586.24,243.85,8.48">Medical Image Databases: A Content-Based Retrieval Approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jaffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,476.34,586.24,24.97,8.48;6,127.50,597.04,113.15,8.48">J. Am. Med. Inform. Assoc. (JAMIA)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,607.84,387.44,8.48;6,127.50,618.70,369.99,8.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,257.28,607.84,244.23,8.48;6,127.50,618.70,241.40,8.48">Automated storage and retrieval of thin-section CT images to assist diagnosis: System description and preliminary assessment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,375.18,618.70,37.44,8.48">Radiology</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,629.50,387.33,8.48;6,127.50,640.30,282.74,8.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,271.44,629.50,229.96,8.48;6,127.50,640.30,42.44,8.48">Towards a computer-aided diagnosis system for pigmented skin lesions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schmid-Saugeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guillod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,175.80,640.30,172.54,8.48">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,651.16,387.35,8.48;6,127.50,661.96,363.41,8.48" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,333.06,651.16,168.36,8.48;6,127.50,661.96,223.88,8.48">A review of content-based image retrieval systems in medicine -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,357.36,661.96,72.45,8.48">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,672.76,387.27,8.48;6,127.50,683.62,277.00,8.49" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,265.62,672.76,235.72,8.48;6,127.50,683.62,46.56,8.48">Medical image retrieval and automated annotation: OHSU at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,200.52,683.62,194.67,8.48">Springer Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,114.07,694.42,387.40,8.48;6,127.50,705.22,334.14,8.48" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,245.28,694.42,256.19,8.48;6,127.50,705.22,209.75,8.48">Automatic Image Modality Based Classification and Annotation to Improve Medical Image Retrieval, accepted to MedInfo</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<pubPlace>Brisbane, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,114.07,69.94,387.41,8.48;7,127.50,80.80,85.75,8.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,216.06,69.94,252.65,8.48">Cross-language evaluation forum: objectives, results, achievements</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,475.35,69.94,26.12,8.48;7,127.50,80.80,27.73,8.48">Inform Retriev</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="7" to="31" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,91.54,383.00,8.48;7,127.50,102.40,373.94,8.48;7,127.50,113.26,373.94,8.48;7,127.50,124.00,120.96,8.48" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,352.38,91.54,149.03,8.48;7,127.50,102.40,373.94,8.48;7,127.50,113.26,33.20,8.48">Overview of the ImageCLEFmed 2006 medical retrieval annotation tasks, Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,169.51,113.26,331.93,8.48;7,127.50,124.00,15.83,8.48">Seventh Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>LNCS 2006. to appear</note>
</biblStruct>

<biblStruct coords="7,118.41,134.86,382.93,8.48;7,127.50,145.72,275.97,8.48" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,227.70,134.86,273.64,8.48;7,127.50,145.72,40.60,8.48">Evaluation Axes for Medical Image Retrieval Systems -The ImageCLEF Experience</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,174.42,145.72,116.27,8.48">ACM Int. Conf. on Multimedia</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,156.46,383.06,8.48;7,127.50,167.32,242.81,8.48" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,366.48,156.46,134.99,8.48;7,127.50,167.32,79.86,8.48">Medical image categorization with MedIC and MedGIFT</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rogozan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbühler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Darmoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIE</publisher>
		</imprint>
		<respStmt>
			<orgName>Medical Informatics Europe</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,178.18,382.96,8.48;7,127.50,188.92,109.79,8.48" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,279.60,178.18,202.45,8.48">MedPost: a part-of-speech tagger for biomedical</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rindflesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,127.50,188.92,56.47,8.48">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,199.78,383.04,8.48;7,127.50,210.58,211.65,8.48" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,218.04,199.78,283.41,8.48;7,127.50,210.58,31.80,8.48">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,165.59,210.58,87.07,8.48">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,221.38,378.02,8.48" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,172.62,221.38,162.59,8.48">Netlab: Algorithms for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Nabney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,118.41,232.24,382.98,8.48;7,127.50,243.04,75.44,8.48" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,171.18,232.24,218.82,8.48">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,396.94,232.24,100.53,8.48">Int. J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
