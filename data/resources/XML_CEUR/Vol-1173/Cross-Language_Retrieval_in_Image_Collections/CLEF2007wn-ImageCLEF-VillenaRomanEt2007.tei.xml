<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.98,80.45,231.26,12.58;1,99.12,96.53,397.02,12.58;1,231.78,112.61,131.79,12.58">MIRACLE at ImageCLEFphoto 2007: Evaluation of Merging Strategies for Multilingual and Multimedia Information Retrieval</title>
				<funder ref="#_eQzdFNU">
					<orgName type="full">Madrid&apos;s R+D Regional Plan</orgName>
				</funder>
				<funder ref="#_6qVZ8nk">
					<orgName type="full">Spanish R+D National Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.64,139.56,83.56,9.02"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.35,139.56,74.75,9.02"><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,164.76,151.02,121.06,9.02"><forename type="first">José</forename><surname>Luis Martínez-Fernández</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.92,151.02,126.16,9.02"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>González-Cristóbal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Decisions and Language</orgName>
								<orgName type="institution">DAEDALUS -Data</orgName>
								<address>
									<country>S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.98,80.45,231.26,12.58;1,99.12,96.53,397.02,12.58;1,231.78,112.61,131.79,12.58">MIRACLE at ImageCLEFphoto 2007: Evaluation of Merging Strategies for Multilingual and Multimedia Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D70618FC1901C05775BDAD89989402F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.2 Information Storage</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital libraries. H.2 [Database Management]: H.2.5 Heterogeneous Databases</term>
					<term>E.2 [Data Storage Representations] Linguistic Engineering, Information Retrieval, Image Retrieval, indexing, domain-specific vocabulary, Semantic Expansion, WordNet, Word Sense Disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of MIRACLE research consortium at the ImageCLEF Photographic Retrieval task of ImageCLEF 2007. For this campaign, the main purpose of our experiments was to thoroughly study different merging strategies, i.e. methods of combination of textual and visual retrieval techniques. While we have applied all the well known techniques which we had already used in previous campaigns, for both textual and visual components of the system, our research has primarily focused on the idea of performing all possible combinations of those techniques in order to evaluate which ones may offer the best results and analyze if the combined results may improve (in terms of MAP) the individual ones.</p><p>The system includes three main modules. On one hand, apart from the search engine (Xapian or Lucene), the textual retrieval module includes parsers, stemming, stopword filtering, proper noun detection and semantic expansion components. On the other hand, the visual retrieval module is based on two well-known content-based engines: GIFT and FIRE. Finally, the merging module allows to use different operators (AND, OR, LEFT, RIGHT) to combine the outputs of the two previous subsystems and to calculate the result relevance based on different metrics (max, min, avg, max-min).</p><p>We finally submitted 110 multilingual textual (text-based) runs, 22 visual (content-based) runs and 21 mixed runs. Results in general show a poor performance for all groups, due to the characteristics of the image collection and the difficulty of the defined topics. The most interesting conclusion is that the defined merging strategies are successful as our best mixed experiment outperforms both the textual and visual experiments in which it is based, using the LEFT operator for the combination along with the max-min metric for computing the relevance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>MIRACLE is a research consortium formed by research groups of three different universities in Madrid (Universidad Politécnica de Madrid, Universidad Autónoma de Madrid and Universidad Carlos III de Madrid) along with DAEDALUS, a small/medium size enterprise (SME) founded in 1998 as a spin-off of two of these groups and a leading company in the field of linguistic technologies in Spain. MIRACLE has taken part in CLEF since 2003 in many different tracks and tasks, including the main bilingual, monolingual and cross lingual tasks as well as ImageCLEF, Question Answering, WebCLEF and GeoCLEF tracks. This paper describes our participation at the ImageCLEF Photographic Retrieval task of ImageCLEF 2007. Briefly, the goal of this task (fully described in <ref type="bibr" coords="2,265.30,84.72,11.26,9.02" target="#b7">[8]</ref>) is: given a multilingual statement describing a user specific information need, find as many relevant images as possible from the given multilingual document collections containing images as well as text. The reference database for this campaign is the IAPR TC-12 Benchmark <ref type="bibr" coords="2,510.49,107.76,10.59,9.02" target="#b8">[9]</ref>, created under Technical Committee 12 (TC-12) of the International Association of Pattern Recognition (IAPR <ref type="bibr" coords="2,70.92,130.74,15.03,9.02">[10]</ref>). This collection contains 20,000 photos (mainly colour photographs) taken from locations around the world and comprises a varying cross-section of still natural images, annotated with semi-structured captions in English and German.</p><p>Participants are provided with a list of 60 topics which include a short textual title representing the research goal in 15 different languages (Danish, Dutch, English, Finnish, French, German, Italian, Japanese, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, and Simplified and Traditional Chinese) and, in addition, three image examples for each topic. The objective is to retrieve as many relevant images as possible from the given visual and multilingual topics.</p><p>For this campaign, the main purpose of our experiments was to thoroughly study the different merging strategies, i.e. methods of combination of visual and textual techniques. While we have applied all the well known techniques which we had already used in previous campaigns <ref type="bibr" coords="2,324.34,257.70,16.70,9.02" target="#b9">[11]</ref>  <ref type="bibr" coords="2,344.00,257.70,15.33,9.02" target="#b10">[12]</ref>, for both textual and visual components of the system, our research has focused on the idea to perform all possible combinations of those techniques in order to evaluate which ones may offer the best results.</p><p>All experiments are fully automatic, thus avoiding any manual intervention. None of the experiments incorporates relevance feedback, due to time constraints. We finally submitted 110 multilingual textual (textbased) runs, 22 visual (content-based) runs and 21 mixed runs (using a combination of both), elaborated on in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>Based on our experience in previous campaigns, to be able to execute a large number of runs that exhaustively cover all the combinations of the different techniques, we developed a flexible system, composed of a set of small components which can be easily added in different configurations and are executed sequentially to build the final result set.  Our approach is based on the multidisciplinary combination of the text-based retrieval techniques using Lucene/Xapian with the GIFT/FIRE content-based retrieval. Thus, the system is logically built up from three different components: the textual (text-based) retrieval module, which indexes the IAPR TC-12 image descriptions to look for those descriptions that are more relevant to the text of the topic; the visual (content-based) retrieval module, that provides the IAPR TC-12 images which are more similar to the given topic images; and, finally, the merging module, which uses different operators to combine the outputs of the two previous subsystems to provide the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Textual Retrieval</head><p>Since MIRACLE has taken part in ImageCLEF (or, in general, in CLEF), different linguistic and statistical techniques have been developed to be used in the text-based part of the different tasks <ref type="bibr" coords="3,426.44,244.02,35.05,9.02">[11] [12]</ref>. This year, the main goal was to make an exhaustive study of these diverse methods, combining them in all possible ways and testing the results achieved. A flexible and configurable system has been developed for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The list of components includes:</head><p>Proper Noun Detection. A heuristic based module to detect the appearance of entities in the text, based on a finite state automaton. Linguistic Analyzer. A component to obtain morphosyntactic analysis (POS tagging) and lemmatization. The target language considered in MIRACLE experiments has been English (although the IAPR TC-12 collection was also available in German and Spanish) so the Charniak parser <ref type="bibr" coords="3,496.23,354.00,11.68,9.02" target="#b1">[2]</ref> has been integrated. The combination of this parser with (Euro) Wordnet <ref type="bibr" coords="3,402.63,365.52,11.72,9.02" target="#b4">[5]</ref> has been used to provide lemmatization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stemming.</head><p>Of course, a component to obtain stems for words has been included as part of the indexing process. The implementation considered for this component has been the Porter's stemmer <ref type="bibr" coords="3,472.22,406.02,15.29,9.02" target="#b13">[14]</ref>.</p><p>Stopword Detection. One of the usual steps to take into consideration in the indexing process is the exclusion of semantic empty words, also called stopwords <ref type="bibr" coords="3,342.11,435.00,11.74,9.02" target="#b2">[3]</ref>  <ref type="bibr" coords="3,356.28,435.00,15.35,9.02" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Expansion.</head><p>Words appearing in the query can be expanded using Wordnet <ref type="bibr" coords="3,479.29,452.52,11.72,9.02" target="#b4">[5]</ref> and an implementation of a Word Sense Disambiguation algorithm <ref type="bibr" coords="3,365.35,464.04,15.37,9.02" target="#b12">[13]</ref>. Two working modes have been considered, one where each word is expanded with all the words considered as synonyms by Wordnet, and another one where a disambiguation process is carried out to try to filter out synonyms for the wrong senses.</p><p>Search Engine. Two different search engines have been used, Lucene <ref type="bibr" coords="3,390.81,515.99,11.62,9.02" target="#b0">[1]</ref> and Xapian <ref type="bibr" coords="3,453.27,515.99,15.32,9.02" target="#b15">[16]</ref>.</p><p>The former methods have been combined in all possible ways and applied to the text extracted from the fields (either individually or combined) in which image descriptions are divided. As described in <ref type="bibr" coords="3,446.04,544.97,10.61,9.02" target="#b8">[9]</ref>, image captions include the following fields: Title, Description, Notes, Location, Date, Image and Thumbnail. However, the Description field is left out in the 2007 collection.</p><p>The final goal of this system configuration was to evaluate which combination of techniques was the best, considering last year relevance assessments. However, there were many possible combinations (although some of them were illogic, like applying semantic expansion to stems). Therefore only those with the best results on 2006 data where selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual Retrieval</head><p>For this part of the system, we resorted to two publicly and freely available Content-Based Information Retrieval systems: GIFT (GNU Image Finding Tool) <ref type="bibr" coords="3,252.56,680.82,11.72,9.02" target="#b6">[7]</ref> and FIRE (Flexible Image Retrieval Engine) <ref type="bibr" coords="3,455.03,680.82,11.68,9.02" target="#b3">[4]</ref>  <ref type="bibr" coords="3,470.27,680.82,10.64,9.02" target="#b5">[6]</ref>. They are both developed under the GNU license and allow to perform query by example on images, using an image as the starting point for the search process and relying entirely on the image contents.</p><p>In the case of GIFT, the complete image database was indexed in a single collection, downscaling each image to 32x32 pixels. For each topic, a visual query is made up of all the images contained in the topic. Then, this visual query was used in GIFT to obtain the list of the most relevant images (i.e., images which are more similar to those included in the visual query), along with the corresponding relevance values.</p><p>On the other hand, we used the results of the FIRE system (kindly provided by the organizers), with no further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Result Merging</head><p>The textual and image result lists are then merged and combined by applying different techniques, which are characterized by an operator and a metric for computing the relevance (score) of the result. Table <ref type="table" coords="4,475.62,174.54,5.01,9.02">1</ref> shows the defined operators: union (OR), intersection (AND) and external join (LEFT/RIGHT JOIN). Each of these operators selects which images are part of the final result set. Then, results are reranked by computing a new relevance measure value based on their corresponding input results, by using different metrics shown in Table <ref type="table" coords="4,515.46,209.04,3.77,9.02">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Combination operators.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operators</head><formula xml:id="formula_0" coords="4,223.26,266.59,137.34,66.99">OR A ∪ B AND A ∩ B LEFT (A ∪ B ) ∪ (A -B) RIGHT (A ∪ B ) ∪ (B -A)</formula><p>Table <ref type="table" coords="4,254.88,355.56,3.77,9.02">2</ref>. Score computing metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>Experiments are defined by the choice of different combinations of the previously described modules, operators and score computation metrics. All experiments are fully automatic, avoiding any manual intervention. None of the experiments incorporates relevance feedback, due to time constraints.</p><p>We finally submitted a wide set of experiments: 110 multilingual textual (text-based) runs, 22 visual (contentbased) runs and 21 mixed runs (using a combination of both). The different name schemas for the run identifiers are described in the following tables: textual retrieval experiments (Table <ref type="table" coords="4,369.41,588.84,3.62,9.02">3</ref>), visual retrieval experiments (Table <ref type="table" coords="4,70.92,600.30,4.19,9.02">4</ref>) and mixed experiments (Table <ref type="table" coords="4,207.84,600.30,3.61,9.02">5</ref>). Table <ref type="table" coords="4,246.00,623.82,3.77,9.02">3</ref>. Textual retrieval experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Identifier Language Engine Method Combination Txt[Engine][Method]</head><p>EN&gt;EN The best experiment uses a combination of all textual and visual results obtained in the previous experiments with the LEFT operator along with the Max-Min (mm) scoring metric. Moreover, the mm metric clearly outperforms the others (max, min, avg) even with the same operator over the same result sets, as seen in Mix2LEFT experiments. The most interesting conclusion is that this merging strategy is successful as the MAP is higher than in both the textual and visual partial experiments with which the experiment is built.</p><p>Finally, although our mixed experiments can really be compared to the others groups' for the reasons already stated before, our MAP is considerably lower than the MAP of the best mixed experiment (Table <ref type="table" coords="7,479.40,702.72,7.95,9.02" target="#tab_1">12</ref>), with a decrease to 70%. However, MIRACLE ranked 5 th (out of over 10 participants), which is indeed considered to be an acceptable position. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Work</head><p>Results in general show a poor performance for all groups, due to the characteristics of the image collection and the difficulty of the defined topics.</p><p>For the textual retrieval, the experiment where the title, description and location fields of the image caption have been indexed using Xapian and applying the proper noun detection module and the stemming component has produced the best results, with a MAP of 0.1995, which shows that no improvement has been reached with regards to last year experiments when our top ranked text-based experiment had a MAP of 0.2005 with a similar combination of techniques. In general, we obtain worse MAP with Lucene rather than Xapian, although this topic has to be further investigated. Regarding the visual retrieval, our results are not good compared to other groups', but it is not strange as our areas of expertise do not specifically include image processing and we resorted to publicly available "black-box" engines. In addition, no specific conclusion can be obtained as we omitted to include the result for FIRE itself, so we could not evaluate if the merging strategy is successful or not.</p><p>However, our main interest was not in experiments where only text or image content is used in the retrieval process. Instead, our challenge was to test whether the text-based image retrieval could improve the analysis of the content of the image, or vice versa. The most interesting conclusion is that merging strategies are successful as our best mixed experiment outperforms both the textual and visual experiments in which it is based, using the LEFT operator for the combination along with the Max-Min (mm) metric for computing the relevance. This result shows that our hypothesis was right. Our combination of a "black-box" search using publicly accessible content-based retrieval engines with a text-based search has turned out to provide better results than other presumably "more complex" techniques. This simplicity may be a good starting point for the implementation of a real system.</p><p>We are sure that there may still be some place for improvement with a more careful study of the parameters for the merging strategy (both the combination operator and the score computing metric). In addition, none of the experiments incorporates relevance feedback, due to time constraints. However, we are already carrying out some experiments that incorporate this technique which are showing promising results, overcoming our submissions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,70.92,428.04,230.69,9.02"><head>Figure 1</head><label>1</label><figDesc>Figure 1 presents an overview of the system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,228.36,744.42,138.53,9.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,159.78,460.44,275.79,145.40"><head>Table 11 .</head><label>11</label><figDesc>Results for mixed textual and visual retrieval experiments.</figDesc><table coords="7,165.12,479.34,265.04,126.50"><row><cell></cell><cell>MAP</cell><cell>P10</cell><cell>P20</cell><cell>P30 RelRet</cell></row><row><cell>Mix2LEFTmm</cell><cell cols="4">0.2244 0.4450 0.3617 0.3067 1888</cell></row><row><cell cols="5">MixLuSG0LEFTmm 0.2027 0.4200 0.3517 0.2872 1812</cell></row><row><cell>Mix2LEFTmax</cell><cell cols="4">0.1985 0.3650 0.3142 0.2739 1888</cell></row><row><cell>Mix1LEFTmm</cell><cell cols="4">0.1975 0.3067 0.2742 0.2500 1763</cell></row><row><cell>Mix1LEFTmax</cell><cell cols="4">0.1915 0.2900 0.2642 0.2433 1763</cell></row><row><cell>Mix2LEFTavg</cell><cell cols="4">0.1901 0.2717 0.2592 0.2489 1888</cell></row><row><cell>Mix2LEFTmin</cell><cell cols="4">0.1901 0.2717 0.2592 0.2489 1888</cell></row><row><cell>Mix1LEFTavg</cell><cell cols="4">0.1888 0.2733 0.2575 0.2411 1763</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,86.52,73.26,422.21,101.84"><head>Table 12 .</head><label>12</label><figDesc>Best mixed experiments at ImageCLEFphoto 2007.</figDesc><table coords="8,343.68,92.10,165.05,9.02"><row><cell>MAP</cell><cell>P10</cell><cell>P20</cell><cell>P30 RelRet</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Spanish R+D National Plan</rs>, by means of the project <rs type="projectName">RIMMEL (Multilingual and Multimedia Information Retrieval, and its Evaluation</rs>), <rs type="grantNumber">TIN2004-07588-C03-01</rs>; and by the <rs type="funder">Madrid's R+D Regional Plan</rs>, by means of the project <rs type="projectName">MAVIR (Enhancing the Access and the Visibility of Networked Multilingual Information for the Community of Madrid</rs>), <rs type="grantNumber">S-0505/TIC/000267</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_6qVZ8nk">
					<idno type="grant-number">TIN2004-07588-C03-01</idno>
					<orgName type="project" subtype="full">RIMMEL (Multilingual and Multimedia Information Retrieval, and its Evaluation</orgName>
				</org>
				<org type="funded-project" xml:id="_eQzdFNU">
					<idno type="grant-number">S-0505/TIC/000267</idno>
					<orgName type="project" subtype="full">MAVIR (Enhancing the Access and the Visibility of Networked Multilingual Information for the Community of Madrid</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NO Txt[Combination][Method]</head><p>EN&gt;EN Both </p><p>NO Mul[Language][Combination] X&gt;EN (4)  Both NPS</p><p>(1) Engine: Xa (Xapian), Lu (Lucene)</p><p>(2) Method: TI (use image title index), DE (use image description index), NO (use image notes index), LO (use image location index), NP (perform proper noun detection), S (apply stemming)</p><p>(3) Combination: OR (images in any result set), AND (images in both result sets), max (use maximum value), min (use minimum value), avg (use average value) (4) Topic language: DE (German), ES (Spanish), FR (French), JA (Japanase), PT (Portuguese), RU (Russian), SV (Swedish) Table <ref type="table" coords="5,248.22,126.66,3.77,9.02">4</ref>. Visual retrieval experiments. 1) Engine: G[1,2,3] (Gift, query with n topic image), G0 (Gift, query with all topic images), F0 (FIRE)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Identifier Engine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Identifier Combination MixLuSG0[Combination]</head><p>TxtLuS (1) VisG0 miracleMix1[Combination] TxtANDminNPS (1) VisG123ANDminF0ANDminG0ANDmin miracleMix2[Combination] TxtORmaxNPS (1) VisG123ORmaxF0ORmaxG0ORmax</p><p>(1) Combination: OR (images in any result set), AND (images in both result sets), LEFT (first OR then AND), max (use maximum value), min (use minimum value), avg (use average value), mm (max-min value)</p><p>After the evaluation by the task organizers, results for each kind of experiment are presented in the following tables. Each table shows the run identifier, the mean average precision (MAP), the precision at 10, 20 and 30 first results and the number of relevant images which have been retrieved (out of 3,416 relevant images). <ref type="table" coords="5,244.68,443.88,3.62,9.02">6</ref>), the best results are achieved with the experiment where the title (TI), description (DE) and location (LO) fields of the image caption have been indexed using Xapian (Xa) and applying the proper noun detection module (NP) along with the Porter stemming algorithm (S). For this experiment a MAP of 0.1995 is obtained. Last year, our top ranked text-based experiment had a MAP of 0.2005 with a similar combination of techniques. The only difference is that, while in 2006 topic narratives where also used to build the query, this year there was no narrative provided for the topics. Implications of this are that no improvement has been reached. In general, our experiments show Lucene turns out to give worse results than Xapian (the first run with Lucene search engine has a MAP of 0.1871), although this conclusion has to be further studied. Moreover, no strategy for merging results from both search engines have led to any improvement in MAP with respect to the baseline experiments, neither with the OR operator (which was supposed to increase the number of results at the expense of a loss of precision) nor the AND operator (which was supposed to increase precision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regarding text-based experiments (Table</head><p>Table <ref type="table" coords="6,97.86,136.74,5.01,9.02">7</ref> shows the precision of the multilingual experiments. The best results correspond to Spanish, our mother tongue in which we have a strong expertise, with a MAP similar to the monolingual experiments. We are negatively surprised by the low precision with French and German languages that may be attributed to a deficient stemming module. However, this issue has to be further researched.</p><p>The 2007 collection did not include the description field and our experiments have been produced using 2006 data collection (we did not notice this fact until the submission was over). Therefore our results are not really comparable to the results provided by the rest of participants, shown in Table <ref type="table" coords="6,416.70,511.80,3.77,9.02">8</ref>. However, our baseline experiment would be among the best results this year and our group would be ranked 4 th out of 16. Our conclusion is that for the IAPR TC-12, textual descriptions of images are very convenient for the retrieval process and should not be dropped out in for the next years. Table <ref type="table" coords="6,98.10,681.78,5.01,9.02">9</ref> in next page shows the results of the visual-based experiments. In this case, MAP is very low due to the complexity of the visual retrieval task in this domain. The best experiment is the combination of results with GIFT and FIRE, and it is better than using GIFT alone. However, we regretfully did not execute the experiment with FIRE alone, which would have been shown whether the combination of both systems improves the final result. Also note that, although the number of relevant images retrieved using AND is lower than when OR is used, the precision is higher.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,97.92,643.86,314.66,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://lucene.apache.org[Visited10/08/2007" />
		<title level="m" coord="8,97.92,643.86,91.47,9.02">Apache Lucene project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,97.92,661.38,390.46,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,174.59,661.38,149.13,9.02">A Maximum-Entropy-Inspired Parser</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,340.97,661.38,116.93,9.02">Proceedings of NAACL-2000</title>
		<meeting>NAACL-2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,97.92,678.84,426.64,9.02;8,97.80,690.36,203.44,9.02" xml:id="b2">
	<monogr>
		<ptr target="http://www.computing.dcu.ie/~gjones/CLEF2005/Multi-8/[Visited10/08/2007" />
		<title level="m" coord="8,97.92,678.84,261.15,9.02">CLEF 2005 Multilingual Information Retrieval resources page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,97.92,707.82,426.60,9.02;8,97.80,719.34,326.20,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,262.85,707.82,261.68,9.02;8,97.80,719.34,41.73,9.02">FIRE -Flexible Image Retrieval Engine: ImageCLEF 2004 Evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,156.96,719.34,45.02,9.02">CLEF 2004</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,73.26,426.58,9.02;9,97.80,84.72,302.66,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,97.92,73.26,392.45,9.02">Eurowordnet: Building a Multilingual Database with Wordnets for several European Languages</title>
		<ptr target="http://www.illc.uva.nl/EuroWordNet/[Visited10/08/2007" />
		<imprint>
			<date type="published" when="1996-03">March (1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,102.24,426.66,9.02;9,97.80,113.76,168.10,9.02" xml:id="b5">
	<monogr>
		<ptr target="http://www-i6.informatik.rwth-aachen.de/~deselaers/fire.html" />
		<title level="m" coord="9,97.92,102.24,192.30,9.02">FIRE: Flexible Image Retrieval System</title>
		<imprint>
			<date type="published" when="2007-08-10">10/08/2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,131.22,411.85,9.02" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,125.43,131.22,121.22,9.02">The GNU Image-Finding Tool</title>
		<author>
			<persName coords=""><surname>Gift</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/software/gift/[Visited10/08/2007" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,148.74,426.66,9.02;9,97.80,160.26,426.75,9.02;9,97.80,171.72,67.76,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,382.97,148.74,141.62,9.02;9,97.80,160.26,120.29,9.02">Overview of the ImageCLEF 2007 Photographic Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">;</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">;</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><forename type="middle">;</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.77,160.26,200.98,9.02">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,189.24,426.59,9.02;9,97.80,200.76,426.70,9.02;9,97.80,212.22,233.33,9.02;9,355.33,212.22,169.19,9.02;9,97.80,223.74,112.79,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,396.66,189.24,127.84,9.02;9,97.80,200.76,233.69,9.02">The IAPR-TC12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">;</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">;</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,353.78,200.76,170.72,9.02;9,97.80,212.22,233.33,9.02;9,355.33,212.22,119.35,9.02">International Workshop OntoImage&apos;2006 Language Resources for Content-Based Image Retrieval, in conjunction with LREC&apos;06</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,258.72,426.58,9.02;9,97.80,270.24,426.73,9.02;9,97.80,281.70,214.44,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,97.80,270.24,210.58,9.02">MIRACLE team report for ImageCLEF IR in CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">M</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paloma</forename><surname>Martínez-Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.23,270.24,188.30,9.02;9,97.80,281.70,26.75,9.02">Proceedings of the Cross Language Evaluation Forum</title>
		<meeting>the Cross Language Evaluation Forum<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">2006. 2006. 20-22 September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,299.22,426.55,9.02;9,97.80,310.74,426.66,9.02;9,97.80,322.19,426.71,9.02;9,97.80,333.71,101.34,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,129.49,310.74,243.19,9.02">Combining Textual and Visual Features for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><forename type="middle">;</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><forename type="middle">M</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>González-Cristóbal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Carlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,379.59,310.74,144.88,9.02;9,97.80,322.19,349.69,9.02">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="9,202.20,333.71,322.23,9.02;9,97.80,345.23,72.81,9.02" xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="s" coord="9,305.34,333.71,143.29,9.02">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<idno type="ISSN">0302-9743</idno>
		<imprint>
			<biblScope unit="volume">4022</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,362.69,426.56,9.02;9,97.80,374.21,316.15,9.02" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,152.67,362.69,214.98,9.02">Método basado en marcas de especificidad para WSD</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,387.96,362.69,136.52,9.02;9,97.80,374.21,216.34,9.02">Proceedings of SEPLN (Sociedad Española para el Procesamiento del Lenguaje Natural)</title>
		<meeting>SEPLN (Sociedad Española para el Procesamiento del Lenguaje Natural)</meeting>
		<imprint>
			<date type="published" when="2000-09-24">nº 24. September 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,391.73,426.47,9.02;9,97.80,403.19,51.41,9.02" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org" />
		<title level="m" coord="9,160.18,391.73,156.76,9.02">Snowball stemmers and resources page</title>
		<imprint>
			<date type="published" when="2007-08-10">10/08/2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,420.71,426.57,9.02;9,97.80,432.23,207.51,9.02" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,201.47,420.71,269.96,9.02">Page of resources for CLEF (Stopwords, transliteration, stemmers</title>
		<ptr target="http://www.unine.ch/info/clef[Visited10/08/2007" />
		<imprint/>
		<respStmt>
			<orgName>University of Neuchatel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.92,449.69,426.55,9.02;9,97.80,461.21,86.10,9.02" xml:id="b15">
	<monogr>
		<ptr target="http://www.xapian.org[Visited10/08/2007" />
		<title level="m" coord="9,97.92,449.69,288.98,9.02">Xapian: an Open Source Probabilistic Information Retrieval library</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
