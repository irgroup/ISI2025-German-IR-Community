<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,87.24,84.01,420.78,12.64">Exploring Image, Text and Geographic Evidences in ImageCLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.16,116.20,62.79,8.96"><forename type="first">João</forename><surname>Magalhães</surname></persName>
							<email>j.magalhaes@imperial.ac.uk</email>
						</author>
						<author>
							<persName coords="1,270.84,116.20,57.37,8.96"><forename type="first">Simon</forename><surname>Overell</surname></persName>
							<email>simon.overell01@imperial.ac.uk</email>
						</author>
						<author>
							<persName coords="1,336.00,116.20,51.32,8.96"><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
							<email>s.rueger@open.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Knowledge Media Institute</orgName>
								<orgName type="institution">The Open University Walton</orgName>
								<address>
									<addrLine>Hall</addrLine>
									<postCode>MK7 6AA</postCode>
									<settlement>Milton, Keynes</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London South Kensington Campus</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,87.24,84.01,420.78,12.64">Exploring Image, Text and Geographic Evidences in ImageCLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F8BC7A47E46C9A1DE316E890223B6C0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Semantic Image Retrieval, Geographic Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year, ImageCLEF2007 data provided multiple evidences that can be explored in many different ways. In this paper we describe an information retrieval framework that combines image, text and geographic data. Text analysis implements the vector space model based on non-geographic terms. Geographic analysis implements a placename disambiguation method and placenames are indexed by their Getty TGN Unique Id. Image analysis implements a query by semantic example model. The paper concludes with an analysis of our results. Finally we identify the weaknesses in our approach and ways in which the system could be optimised and improved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents a system that integrates visual, text and geographic data. Such systems are in great demand as the richness of metadata information increase together with the size of multimedia collections. We evaluated the system on the ImageCLEF Photo IAPR TC-12 photographic collection to assess some of the evidence combination strategies and other design aspects of our system. As the system implemented a set of preliminary algorithms we were able to clearly see the different impacts of each component of our system. This paper is organized as follows: section 2 discusses the dataset characteristics; sections 3 to 6 present our system: the text data processing algorithms, the geographic data processing algorithms, the image data processing algorithms, and the combination strategies. Results are presented in section 7 and finally section 8 presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ImageCLEF Photo Data</head><p>ImageCLEF Photo dataset is ideal to test our system: it includes metadata information (both textual descriptions and geographic information), and visual information. The dataset has 20,000 images with the corresponding metadata. The photos vary in quality, levels of noise, and illustrate several concepts, actions or events. Metadata enriches the images by adding information such as the fact that a street is in some location or the profession of one of the persons in the photos. A more thorough description of the dataset can be found in <ref type="bibr" coords="1,442.80,694.72,10.69,8.96" target="#b3">[4]</ref>.</p><p>The goal of this dataset is to simulate a scenario where collections have heterogeneous sources of data and users submit textual queries together with visual examples. This is similar to TRECVID search task with a slight difference concerning geographic data and actions that are only possible to detect on videos (e.g. walking/running).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System</head><p>The implemented system has two separate indexes for images related data and another one for metadata related data. Next we will describe how information is analysed and stored on both indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metadata Indexes</head><p>The indexing stage of Forostar begins by extracting named entities from text using ANNIE, the Information Extraction engine bundled with GATE. GATE is Sheffield University's General Architecture for Text Engineering. Of the series of tasks ANNIE is able to perform, the only one we use is named entity recognition. We consider ANNIE a "black box" where text goes in, and categorised named entities are returned; because of this, we will not discuss the workings of ANNIE further here but rather refer you to the GATE manual <ref type="bibr" coords="2,484.56,340.12,10.69,8.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Named Entity Fields</head><p>We index all the named entities categorised by GATE in a "Named Entity" field in Lucene (e.g."Police," "City Council," or "President Clinton"). The named entities tagged as Locations by ANNIE we index as "Named Entity -Location" (e.g. "Los Angeles," "Scotland" or "California") and as a Geographic Location (described in Section 3.1.3). The body of the GeoCLEF articles and the article titles are indexed as text fields. This process is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text Fields</head><p>Text fields are pre-processed by a customised analyser similar to Lucene's default analyser <ref type="bibr" coords="2,447.12,466.12,10.69,8.96" target="#b0">[1]</ref>. Text is split at white space into tokens, the tokens are then converted to lower case, stop words discarded and stemmed with the "Snowball Stemmer". The processed tokens are held in Lucene's inverted index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Geographic Fields</head><p>The locations tagged by the named entity recogniser are passed to the disambiguation system. We have implemented a simple disambiguation method based on heuristic rules. For each placename being classified we build a list of candidate locations, if the placename being classified is followed by a referent location this can often cut down the candidate locations enough to make the placename unambiguous. If the placename is not followed by a referent location or is still ambiguous we disambiguate it as the most commonly occurring location with that name.</p><p>Topological relationships between locations are looked up in the Getty Thesaurus of Geographical Names (TGN) <ref type="bibr" coords="2,102.24,619.12,10.60,8.96" target="#b4">[5]</ref>. Statistics on how commonly different placenames refer to different locations and a set of synonyms for each location are harvested from our Geographic Co-occurrence model, which in turn is built by crawling Wikipedia <ref type="bibr" coords="2,116.16,643.12,15.43,8.96" target="#b12">[13]</ref>.</p><p>Once placenames have been mapped to unique locations in the TGN, they need to be converted into Geographic fields to be stored in Lucene. We store locations in two fields:</p><p>• Coordinates. The coordinate field is simply the latitude and longitude as read from the TGN.</p><p>• Unique strings. The unique string is the unique id of this location, preceded with the unique id of all the parent locations, separated with slashes. Thus the unique string for the location "London, UK" is the unique id for London (7011781), preceded by its parent, Greater London (7008136), preceded by its parent, Britain (7002445). . . until the root location, the World (1000000) is reached. Giving the unique string for London as 1000000\1000003\7008591\7002445\7008136\7011781.</p><p>Note the text, named entity and geographic fields are not orthogonal. This has the effect of multiplying the impact of terms occurring in multiple fields. For example if the term "London" appears in text, the token "london" will be indexed in the text field. "London" will be recognised by ANNIE as a Named Entity and tagged as a location (and indexed as Location Entity, "London"). The Location Entity will then be disambiguated as location "7011781" and corresponding geographic fields will be added.</p><p>Previous experiments conducted on the GeoCLEF data set in <ref type="bibr" coords="3,340.20,170.80,16.76,8.96" target="#b10">[11]</ref> showed improved results from having overlapping fields. We concluded from these experiments that the increased weighting given to locations caused these improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Images Indexes</head><p>The image indexing part of our system creates high-level semantic indexing units that allow the user to access the visual content with query-by-keyword or query-by-semantic-example. In our ImageCLEF experiments we only used query by semantic example.</p><p>Following the approach proposed in <ref type="bibr" coords="3,224.88,275.80,10.69,8.96" target="#b8">[9]</ref>, each keyword corresponds to a statistical model that represents that keyword in terms of the visual features of the images. These keyword models are then used to index images with the probability of observing the keyword on each particular image. Next we will describe the different steps of the visual analysis algorithm, see <ref type="bibr" coords="3,240.12,311.80,11.72,8.96" target="#b8">[9]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Visual Features</head><p>Three different low-level features are used in our implementation: marginal HSV distribution moments, a 12 dimensional colour feature that captures the histogram of 4 central moments of each colour component distribution; Gabor texture, a 16 dimensional texture feature that captures the frequency response (mean and variance) of a bank of filters at different scales and orientations; and Tamura texture, a 3 dimensional texture feature composed by measures of image's coarseness, contrast and directionality. We tiled the images in 3 by 3 parts before extracting the low-level features. This has two advantages: it adds some locality information and it greatly increases the amount of data used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature Data Representation</head><p>We create a visual vocabulary where each term corresponds to a set of homogenous visual characteristics (colour and texture features). Since we are going to use a feature space to represent all images, we need a set of visual terms that is able to represent them. Thus, we need to check which visual characteristics are more common in the dataset. For example, if there are a lot of images with a wide range of blue tones we require a larger number of visual terms representing the different blue tones. This draws on the idea that to learn a good high-dimensional visual vocabulary we would benefit from examining the entire dataset to look for the most common set of colour and texture features.</p><p>We build the high-dimensional visual vocabulary by clustering the entire dataset and representing each term as a cluster. We follow the approach presented in <ref type="bibr" coords="3,260.16,563.80,10.69,8.96" target="#b7">[8]</ref>, where the entire dataset is clustered with a hierarchical EM algorithm using a Gaussian mixture model. This approach generates a hierarchy of cluster models that corresponds to a hierarchy of vocabularies with a different number of terms. The ideal number of clusters is selected via the MDL criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Maximum Entropy Model</head><p>Maximum entropy (or logistic regression) is a statistical tool that has been applied to a great variety of fields, e.g. natural language processing, text classification, image annotation. Thus, each keyword i w is represented by a maximum entropy model,</p><formula xml:id="formula_0" coords="3,228.48,674.23,147.21,18.31">( ) ( ) ( ) | MaxEnt F i w i p w V V β = ,</formula><p>where ( ) F V is the feature data representation defined on the previous section of visual feature vector V , and i w β is the vector of the regression coefficients for keyword i w .</p><p>We implemented the binomial model, where one class is always modelled relatively to all other classes, and not a multinomial distribution, which would impose a model that does not reflect the reality of the problem: the multinomial model implies that events are exclusive, whereas in our problem keywords are not exclusive. For this reason, the binomial model is a better choice as documents can have more than one keyword assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Images Indexing by Keyword</head><p>ImageCLEF data is not annotated with keywords, thus we used a different dataset. This dataset was compiled by Duygulu et al. <ref type="bibr" coords="4,132.24,137.80,11.72,8.96" target="#b2">[3]</ref> from a set of COREL Stock Photo CDs. The dataset has some visually similar keywords (jet, plane, Boeing), and some keywords have a limited number of examples (10 or less). Each image is annotated with 1-5 keywords from a vocabulary of 371 keywords of which we modelled 179 keywords to annotate ImageCLEF images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Query Processing</head><p>The previous sections described how the dataset information is processed and stored. This section will describe how the user query is processed and matched to the indexed documents. Similarly to the documents processing the user's query is divided into its text and image elements. Figure <ref type="figure" coords="4,350.16,236.80,4.98,8.96" target="#fig_1">1</ref> illustrates the query processing and how multiple evidences are combined. The textual part is processed in the same way as at indexing time and a combination strategy fuses the results from the different textual part processing (text terms, named entities and nameplaces). Each image is also processed in the same way as previously described and a combination strategy fuses the results from the images. Thus, only the documents-query similarity and the evidence combination is new in the query processing part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Query Processing</head><p>The querying stage is a two step process: (1) manually constructed queries are expanded and converted into Lucene's bespoke querying language; (2) then we query the Lucene index with these expanded queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Manually Constructed Query</head><p>The queries are manually constructed in a similar structure to the Lucene index. Queries have the following parts: a text field, a Named Entity field and a location field. The text field contains the query with no alteration. The named entity field contains a list of named entities referred to in the query (manually extracted). The location field contains a list of location -relationship pairs. These are the locations contained in the query and their to the location being searched for. A location can be specified either with a placename (optionally disambiguated with a referent placename), a bounding box, a bounding circle (centre and radius), or a geographic feature type (such as "lake" or "city"). A relationship can either be "exact match," "contained in (vertical topology)," "contained in (geographic area)," or "same parent (vertical topology)". The negation of relationships can also be expressed i.e. "excluding," "outside," etc.</p><p>We believe such a manually constructed query could be automated with relative ease in a similar fashion to the processing that documents go through when indexed. This was not implemented due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Expanding the Geographic Query</head><p>The geographic queries are expanded in a pipe-line. The location -relation pairs are expanded in turn. The relation governs at which stage the location enters the pipeline. At each stage in the pipeline the geographic query is added to. At the first stage an exact match for this location's unique string is added: for "London" this would be 1000000\1000003\7008591\7002445\7008136\7011781. Then places within the location are added, this is done using Lucene's wild-card character notation: for locations in "London" this becomes 1000000\1000003\7008591\7002445\7008136\7011781\*. Then places sharing the same parent location are added, again using Lucene's wild-card character notation. For "London" this becomes all places within "Greater London," 1000000\1000003 \7008591\7002445\7008136\*. Finally the coordinates of all the locations falling close to this location are added. A closeness value can manually be set in the location field, however default values are based on feature type (default values were chosen by the authors). The feature of "London" is "Administrative Capital," the default value of closeness for this feature is 100km. See <ref type="bibr" coords="5,418.32,209.80,16.76,8.96" target="#b11">[12]</ref> for further discussion on the handling of geographic queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Combining using the VSM</head><p>A Lucene query is built using the text fields, named entity fields and expanded geographic fields. The text field is processed by the same analyzer as at query time and compared to both the notes and title fields in the Lucene index. We define a separate boost factor for each field. We define a separate boost factor for each field. These boost values were set by the authors during initial iterative tests (they are comparable to similar weighting in past GeoCLEF papers <ref type="bibr" coords="5,164.16,308.80,16.76,8.96" target="#b9">[10]</ref> and <ref type="bibr" coords="5,201.24,308.80,15.02,8.96" target="#b13">[14]</ref>). The title had a boost of 10, the notes a boost of 7, named entities a boost of 5, geographic unique string a boost of 5 and geographic co-ordinates a boost of 3. The geographic, text and named entity relevance are then combined using Lucene's Vector Space Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Query Processing</head><p>In automatic retrieval systems, processing time is a pressing feature that directly impacts the usability of the system. We envisage a responsive system that processes a query and retrieves results within 1 second per user, meaning that to support multiple users it must be much less than 1 second. Figure <ref type="figure" coords="5,406.32,395.80,4.98,8.96" target="#fig_2">2</ref> presents the architecture of the system. We implemented a query by semantic example algorithm <ref type="bibr" coords="5,351.00,407.80,11.72,8.96" target="#b6">[7]</ref> that is divided into three parts:</p><p>• Semantic Multimedia Analyser: The semantic multimedia analyser infers the keywords probabilities and is designed to work in less than 100ms. Another important issue is that it should also support a large number of keywords so that the semantic space can accommodate the semantic understanding that the user gives to the query. Section 3.2 presented the semantic multimedia analyser used in this paper, see <ref type="bibr" coords="5,150.60,473.80,11.60,8.96" target="#b8">[9]</ref> for details. • Indexer: Indexer uses a simple storage mechanism capable of storing and providing easy access to each keyword of a given multimedia document. It is not optimised for time complexity. The same indexing mechanisms used for content based image retrieval can be used to index content by semantics.</p><p>• Semantic Multimedia Retrieval: The final part of the system is in charge of retrieving the documents that are semantically close to the given query. First it must run the semantic multimedia analyser on the example to obtain the keyword vector of the query. Then it searches the database for the relevant documents according to a semantic similarity metric on the semantic space of keywords. In this part of the system we are only concerned with studying functions that mirror human understanding of semantic similarity. See <ref type="bibr" coords="6,206.28,134.80,11.72,8.96" target="#b6">[7]</ref> for details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,191.16,439.53,212.85,8.10"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -Query processing and evidence combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,208.44,682.41,178.29,8.10"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Query by semantic example system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,161.76,203.01,271.65,8.10"><head>Table 1 -Example of metadata information available on the collection.</head><label>1</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Semantic Space</head><p>In the semantic space multimedia documents are represented as a feature vector of the probabilities of the T keywords (179 in our case), where each dimension is the probability of keyword i w being present on that document. Note that the vector of keywords is normalised if the similarity metric needs so (normalisation is dependent on the metric). These keywords are extracted by the semantic-multimedia-analyser algorithm described in Section 3.2.</p><p>It is important that the semantic space accommodates as many keywords as possible to be sure that the user idea is represented in that space without losing any concepts. Thus, systems that extract a limited number of keywords are less appropriate. This design requirement pushes us to the research area of metrics on highdimensional spaces.</p><p>We use the tf-idf vector space model. Each document is represented as a vector d , where each dimension corresponds to the frequency of a given term (keyword) i w from a vocabulary of T terms (keywords). The only difference between our formulation and the traditional vector space model is that we use .</p><p>Thus, to implement a vector space model we set each dimension i of a document vector as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Semantic Similarity Metric</head><p>Documents d and queries q are represented by vectors of keyword probabilities that are computed as was explained in the previous section. Several distance metrics exist in the tf-idf representation that compute the similarity between a document j d vector and a query vector q . We rank documents by their similarity to the query image according to the cosine-distance metric. The cosine similarity metric expression is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multiple Images Query Combination</head><p>The semantic similarity metric gives us the distance between a single image query and the documents in the database. There are two major strategies of combining multiple examples of a query: <ref type="bibr" coords="6,419.40,655.84,11.72,8.96" target="#b0">(1)</ref> merging the examples into a single query input and produce a single rank; (2) submit several queries and combine the ranks. Obviously each of these two types of combinations uses different algorithms. For ImageCLEF2007 we implemented a simple and straight forward combination strategy: we submit one query for each example and combine the similarity values from all individual queries:</p><p>q q q D q D q D q D = .</p><p>This is an OR operation while an AND operation would be achieved with a query vector that is the product of all individual query image vectors. We return the top 1000 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rank Combination</head><p>The text query and the image query are processed independently and are later combined by a simple linear combination. Previous work on this area has found a set of good weights to combine text and image ranks.</p><p>Applying these weights we reach the expression that combines ranks:</p><p>0.375 1000 0.675 1000</p><p>The different metric spaces hold different similarity functions, thus producing incompatible numerical measures. Thus, we used the document rank position (e.g.</p><p>i ImageRankPos ) to compute the final rank. The produced metric gives the importance of each document i for the given query. This linear combination only considers the top 1000 documents of each rank. Documents beyond this position are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We ran 5 experiments:</p><p>• Text: The Text part of the query only;</p><p>• TextNoGeo: The Text part of the query with geographic elements removed;</p><p>• ImageOnly: The Image part of the query only;</p><p>• Geo: The geographic part of the image only; and</p><p>• Combination: A combination of the Text, ImageOnly and Geo runs.</p><p>Note the TextNoGeo, ImageOnly and Geo runs are orthogonal. Results are presented on Figure <ref type="figure" coords="7,455.64,369.76,3.76,8.96">3</ref>.</p><p>We can see that TextNoGeo results achieved the best results. Next was the Text results, however, there is not a significant difference (using the Wilcoxon Signed Rank Test <ref type="bibr" coords="7,319.20,399.76,10.57,8.96" target="#b5">[6]</ref>). This was a bit surprising as one would expect to improve results when you add location information to the query and to the documents. We believe that the decrease in performance was due to the fact that some queries use the geographic part as inclusive or exclusive. The Geo results were statistically significantly worse than all the other results with a confidence of 99.98%. This is due to minimal information being used (generally a list of placenames). Only 26 of the 60 queries contained geographic references. Across these geographic queries the Geo method achieved an MAP of 0.062 compared to 0.085 for TextNoGeo and 0.025 for ImageOnly. In fact across these 26 queries there is no significant difference between Text, TextNoGeo, Combination and Geo methods. This shows that (for the geographic queries) the geographic component of the query is extremely important.</p><p>Image results also achieved very low results, which given the scope of the evaluation might seem a bit surprising. This is related to the uses of the images to illustrate keywords that are not obvious. For example from Figure <ref type="figure" coords="7,100.56,706.24,4.98,8.96">4</ref> we can see that in some cases it is very difficult to guess the query or how images should be combined. Summing up all these problems that we faced on single data-type evidences, together with unbalanced combinations of the different types of evidences, we found that the final rank is almost an average of individual ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Most of our work was done on the documents analysis and indexing part of the evaluation. However, it became evident that our single combination strategy does not cover all possible types of queries. In some cases images should be combined with AND, others with OR operations. The same happened with text and geographic, e.g. locations can be inclusive ("in San Francisco") or exclusive ("outside Australia"). Moreover, some images only illustrate part of the query ("people in San Francisco", Figure <ref type="figure" coords="8,325.56,296.44,4.18,8.96">4</ref>) and it is obviously difficult to identify correct results with only the visual part of the query.</p><p>All these lessons show that it is essential to make good use of the different algorithms by combining them properly according to the query text. Moreover, the query analysis must produce an accurate logical combination of the different entities of the query to achieve a good retrieval performance. In our future work we would like to repeat the experiments described in this paper using a combination strategy based on the logical structure of the query.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,108.00,413.44,128.97,8.96" xml:id="b0">
	<monogr>
		<title level="m" coord="8,112.93,413.44,91.21,8.96">Apache Lucene Project</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,431.44,415.18,8.96;8,108.00,443.44,203.25,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,191.07,431.44,231.80,8.96">GATE, a General Architecture for Text Engineering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,438.36,431.44,84.82,8.96;8,108.00,443.44,44.15,8.96">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="223" to="254" />
			<date>May 2004 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,461.44,415.33,8.96;8,108.00,473.44,415.29,8.96;8,108.00,485.44,168.09,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,347.86,461.44,175.46,8.96;8,108.00,473.44,219.75,8.96">Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,357.36,473.44,161.92,8.96">European Conf. on Computer Vision</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,503.44,415.38,8.96;8,108.00,515.44,415.17,8.96;8,108.00,527.44,22.65,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,370.24,503.44,153.14,8.96;8,108.00,515.44,115.59,8.96">Overview of the ImageCLEF 2007 Photographic Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hanburyallan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,248.40,515.44,184.79,8.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,545.44,408.81,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,156.36,545.44,158.77,8.96">User&apos;s Guide to the TGN Data Releases</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Harping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,321.84,545.44,127.07,8.96">The Getty Vocabulary Program</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">200</biblScope>
		</imprint>
	</monogr>
	<note>0 edition</note>
</biblStruct>

<biblStruct coords="8,108.00,563.44,415.17,8.96;8,108.00,575.44,219.33,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,154.91,563.44,287.39,8.96">Using statistical testing in the evaluation of retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,471.00,563.44,52.17,8.96;8,108.00,575.44,44.15,8.96">ACM SIGIR Conference</title>
		<meeting><address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,593.44,415.26,8.96;8,108.00,605.44,415.38,8.96;8,108.00,617.44,230.37,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,282.89,593.44,219.57,8.96">A semantic vector space for query by image example</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.00,605.44,415.38,8.96;8,108.00,617.44,77.40,8.96">ACM SIGIR Conf. on research and development in information retrieval, Multimedia Information Retrieval Workshop</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,635.44,415.50,8.96;8,108.00,647.44,271.65,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,224.89,635.44,280.73,8.96">Logistic regression of generic codebooks for semantic image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.00,647.44,161.54,8.96">Int&apos;l Conf. on Image and Video Retrieval</title>
		<meeting><address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,665.44,415.18,8.96;8,108.00,677.44,268.05,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,224.05,665.44,208.03,8.96">Information-theoretic semantic multimedia indexing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,453.36,665.44,69.82,8.96;8,108.00,677.44,166.66,8.96">ACM Conference on Image and Video Retrieval Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,695.44,415.22,8.96;8,108.00,707.44,292.29,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,368.42,695.44,154.79,8.96;8,108.00,707.44,17.75,8.96">The University of Lisbon at GeoCLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chaves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,147.36,707.44,156.36,8.96">Working Notes for the CLEF Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,725.44,415.23,8.96;8,108.00,737.44,166.17,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,273.43,725.44,108.13,8.96">Forostar: A system for GIR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,404.40,725.44,118.83,8.96;8,108.00,737.44,116.15,8.96">Lecture Notes from the Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.00,74.80,415.50,8.96;9,108.00,86.80,181.65,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,287.95,74.80,214.16,8.96">GIR experiements with Forostar at GeoCLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,108.00,86.80,69.98,8.96">ImageCLEF 2007</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.00,104.80,415.38,8.96;9,108.00,116.80,236.49,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,224.56,104.80,186.45,8.96">Geographic co-occurrence as a tool for GIR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Overell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,438.12,104.80,85.26,8.96;9,108.00,116.80,135.86,8.96">CIKM Workshop on Geographic Information Retrieval</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.00,134.80,415.26,8.96;9,108.00,146.80,186.21,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,398.53,134.80,68.88,8.96">UB at GeoCLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Southwick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,108.00,146.80,156.35,8.96">Working Notes for the CLEF Workshop</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
