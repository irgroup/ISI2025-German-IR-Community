<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.26,148.86,288.50,15.15;1,208.40,170.78,186.20,15.15">Overview of the ImageCLEF 2007 Object Retrieval Task</title>
				<funder ref="#_C3AGgm3">
					<orgName type="full">DFG (German research foundation)</orgName>
				</funder>
				<funder ref="#_SGScRSE">
					<orgName type="full">European Commission MUSCLE NoE</orgName>
				</funder>
				<funder ref="#_RhZGX2m #_qtayygY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.37,204.67,78.64,8.74"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<email>deselaers@cs.rwth-aachen.de</email>
						</author>
						<author>
							<persName coords="1,267.01,204.67,64.79,8.74;1,331.80,204.47,16.26,4.35"><roleName>PRIP</roleName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<email>hanbury@prip.tuwien.ac.at</email>
						</author>
						<author>
							<persName coords="1,354.64,204.67,69.19,8.74"><forename type="first">Ville</forename><surname>Viitaniemi</surname></persName>
						</author>
						<author>
							<persName coords="1,155.27,218.62,69.99,8.74;1,225.26,218.42,23.86,4.35;1,255.70,218.62,31.88,8.74"><roleName>Mátyás</roleName><forename type="first">András</forename><surname>Benczúr Budac</surname></persName>
						</author>
						<author>
							<persName coords="1,290.90,218.62,33.65,8.74;1,324.55,218.42,23.86,4.35;1,354.99,218.62,26.71,8.74"><roleName>Bálint</roleName><forename type="first">Brendel</forename><surname>Budac</surname></persName>
						</author>
						<author>
							<persName coords="1,385.02,218.62,35.59,8.74;1,420.61,218.42,23.86,4.35"><forename type="first">Daróczy</forename><surname>Budac</surname></persName>
							<email>daroczyb@ilab.sztaki.hu</email>
						</author>
						<author>
							<persName coords="1,96.19,232.57,43.07,8.74"><forename type="first">Hugo</forename><surname>Jair</surname></persName>
							<email>hugojair@ccc.inaoep.mx</email>
						</author>
						<author>
							<persName coords="1,142.58,232.57,82.32,8.74;1,224.90,232.37,21.66,4.35"><roleName>INAOE</roleName><forename type="first">Escalante</forename><surname>Balderas</surname></persName>
						</author>
						<author>
							<persName coords="1,253.15,232.57,54.94,8.74"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
							<email>gevers@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,308.10,232.37,15.22,4.35;1,329.91,232.57,27.76,8.74"><forename type="first">Isla</forename><surname>Carlos</surname></persName>
							<email>carloshg@ccc.inaoep.mx</email>
						</author>
						<author>
							<persName coords="1,360.99,232.57,79.22,8.74"><forename type="first">Arturo</forename><surname>Hernández</surname></persName>
						</author>
						<author>
							<persName coords="1,443.54,232.57,38.34,8.74;1,481.88,232.37,21.66,4.35"><forename type="first">Gracidas</forename><surname>Inaoe</surname></persName>
						</author>
						<author>
							<persName coords="1,168.07,246.52,74.17,8.74;1,242.24,246.32,14.55,4.35"><forename type="first">Steven</forename><forename type="middle">C H Hoi</forename><surname>Ntu</surname></persName>
						</author>
						<author>
							<persName coords="1,263.36,246.52,76.47,8.74"><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
						</author>
						<author>
							<persName coords="1,360.96,246.52,51.34,8.74"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
							<email>mjli@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,140.89,260.46,61.24,8.74"><forename type="first">Heidy</forename><surname>Marisol</surname></persName>
						</author>
						<author>
							<persName coords="1,205.46,260.46,58.50,8.74;1,263.96,260.27,21.66,4.35"><roleName>INAOE</roleName><forename type="first">Marin</forename><surname>Castro</surname></persName>
							<email>hmarinc@ccc.inaoep.mx</email>
						</author>
						<author>
							<persName coords="1,292.20,260.46,60.63,8.74"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
						</author>
						<author>
							<persName coords="1,352.84,260.27,20.41,4.35;1,377.07,260.46,46.22,8.74"><forename type="first">Rwth</forename><surname>Xiaoguang</surname></persName>
						</author>
						<author>
							<persName coords="1,426.60,260.46,15.64,8.74;1,442.24,260.27,19.37,4.35"><forename type="first">Rui</forename><surname>Msra</surname></persName>
						</author>
						<author>
							<persName coords="1,194.64,274.41,43.72,8.74"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
						</author>
						<author>
							<persName coords="1,260.17,274.41,73.78,8.74;1,333.95,274.21,16.26,4.35"><roleName>PRIP</roleName><forename type="first">Julian</forename><surname>Stöttinger</surname></persName>
							<email>julian@prip.tuwien.ac.at</email>
						</author>
						<author>
							<persName coords="1,356.80,274.41,31.69,8.74"><forename type="first">Wu</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName coords="1,388.49,274.21,19.37,4.35"><surname>Msra</surname></persName>
						</author>
						<author>
							<persName coords="5,145.90,687.97,67.07,8.74"><forename type="first">Mátyás</forename><surname>Brendel</surname></persName>
							<email>mbrendel@ilab.sztaki.hu</email>
						</author>
						<author>
							<persName coords="5,220.84,687.97,62.87,8.74"><forename type="first">Bálint</forename><surname>Daróczy</surname></persName>
						</author>
						<author>
							<persName coords="5,311.09,687.97,69.99,8.74"><forename type="first">András</forename><surname>Benczúr</surname></persName>
							<email>benczur@ilab.sztaki.hu</email>
						</author>
						<author>
							<persName coords="13,145.90,687.22,74.17,8.74"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">RWTH Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer-Aided Automation</orgName>
								<orgName type="laboratory">PRIP PRIP</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">BUDAC Data Mining and Web search Research Group</orgName>
								<orgName type="laboratory" key="lab2">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Optics and Electronics</orgName>
								<orgName type="laboratory">INAOE TIA Research Group</orgName>
								<orgName type="institution">National Institute of Astrophysics</orgName>
								<address>
									<settlement>Tonantzintla</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">ISLA Intelligent Systems Lab Amsterdam</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">HUT Adaptive Informatics Research Centre</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">MSRA Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">NTU School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="laboratory" key="lab1">Data Mining and Web search Research Group</orgName>
								<orgName type="laboratory" key="lab2">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">Affiliation</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.26,148.86,288.50,15.15;1,208.40,170.78,186.20,15.15">Overview of the ImageCLEF 2007 Object Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">147B35856357F59DCC09FBA62B3264AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image retrieval, image classification, performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the object retrieval task of ImageCLEF 2007, give an overview of the methods of the participating groups, and present and discuss the results.</p><p>The task was based on the widely used PASCAL object recognition data to train object recognition methods and on the IAPR TC-12 benchmark dataset from which images of objects of the ten different classes bicycles, buses, cars, motorbikes, cats, cows, dogs, horses, sheep, and persons had to be retrieved.</p><p>Seven international groups participated using a wide variety of methods. The results of the evaluation show that the task was very challenging and that different methods for relevance assessment can have a strong influence on the results of an evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object class recognition, automatic image annotation, and object retrieval are strongly related tasks. In object recognition, the aim is to identify whether a certain object is contained in an image, in automatic image annotation, the aim is to create a textual description of a given image, and in object retrieval, images containing certain objects or object classes have to be retrieved out of a large set of images. Each of these techniques are important to allow for semantic retrieval from image collections.</p><p>Over the last year, research in these areas has strongly grown, and it is becoming clear that performance evaluation is a very important step to foster progress in research. Several initiatives create benchmark suites and databases to compare different methods tackling the same problem quantitatively.</p><p>In the last years, evaluation campaign for object detection <ref type="bibr" coords="2,372.44,421.19,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,392.28,421.19,7.01,8.74" target="#b8">9]</ref>, content-based image retrieval <ref type="bibr" coords="2,122.20,433.15,10.52,8.74" target="#b4">[5]</ref> and image classification <ref type="bibr" coords="2,248.24,433.15,15.50,8.74" target="#b23">[24]</ref> have developed. There is however, no task aiming at finding images showing particular object from a larger database. Although this task is extremely similar to the PASCAL visual object classes challenge <ref type="bibr" coords="2,327.90,457.06,15.49,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,346.63,457.06,7.01,8.74" target="#b8">9]</ref>, it is not the same. In the PASCAL object recognition challenge, the probability for an object to be contained in an image is relatively high and the images to train and test the methods are from the same data collection. In realistic scenarios, this is not a suitable assumption. Therefore, in the object retrieval task described here, we use the training data that was carefully assembled by the PASCAL NoE with much manual work, and the IAPR TC-12 database which has been created under completely different circumstances as the database from which relevant images are to be retrieved.</p><p>In this paper, we present the results of the object retrieval task that was arranged as part of the CLEF/ImageCLEF 2007 image retrieval evaluation. This task was conceived as a purely visual task, making it inherently cross-lingual. Once one has a model for the visual appearance of a specific object, such as a bicycle, it can be used to find images of bicycles independently of the language or quality of the annotation of an image.</p><p>ImageCLEF<ref type="foot" coords="2,157.94,598.95,3.97,6.12" target="#foot_0">1</ref>  <ref type="bibr" coords="2,166.31,600.52,10.52,8.74" target="#b4">[5]</ref> has started within CLEF<ref type="foot" coords="2,291.81,598.95,3.97,6.12" target="#foot_1">2</ref> (Cross Language Evaluation Forum) in 2003. A medical image retrieval task was added in 2004 to explore domain-specific multilingual information retrieval and also multi-modal retrieval by combining visual and textual features for retrieval. Since 2005, a medical retrieval and a medical image annotation task are both presented as part of ImageCLEF. In 2006, a general object recognition task was presented to see whether interest in this area existed. Although, only a few groups participated, many groups expressed their interest and encouraged us to create an object retrieval task. In 2007, beside the here described object retrieval task, a photographic retrieval task also using the IAPR TC-12 database <ref type="bibr" coords="2,485.59,684.21,14.62,8.74" target="#b13">[14]</ref>, a medical image retrieval task <ref type="bibr" coords="2,216.03,696.16,14.62,8.74" target="#b25">[26]</ref>, and a medical automatic annotation task <ref type="bibr" coords="2,421.11,696.16,15.50,8.74" target="#b25">[26]</ref> were arranged in ImageCLEF 2007. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The task was defined as a visual object retrieval task. Training data was in the form of annotated example images of ten object classes (PASCAL VOC 2006 data). The task was, after learning from the provided annotated images, to find all images in the IAPR-TC12 database containing the learned objects. The particularity of the task is that the training and test images are not from the same set of images. This makes the task more realistic, but also more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>For this task, two datasets were available. As training data, the organisers of the PASCAL Network of Excellence visual object classes challenge kindly agreed that we use the training data they assembled for their 2006 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC 2006 training data</head><p>The PASCAL Visual Object Classes (VOC) challenge 2006 training data is freely available on the PASCAL web-page <ref type="foot" coords="3,372.58,524.24,3.97,6.12" target="#foot_2">3</ref> and consists of approximately 2600 images, where for each image a detailed description is available which of the ten object classes is visible in which area of the image. Example images from this database are shown in Figure <ref type="figure" coords="3,508.02,549.72,4.98,8.74" target="#fig_0">1</ref> with the corresponding annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IAPR TC-12 dataset</head><p>The IAPR TC-12 Benchmark database <ref type="bibr" coords="3,371.38,587.57,15.50,8.74" target="#b14">[15]</ref> consists of 20,000 still images taken from locations around the world and comprising an assorted cross-section of still images which might for example be found in a personal photo collection. It includes pictures of different sports and actions, photographs of people, animals, cities, landscapes and many other aspects of contemporary life. Some example images are shown in Figure <ref type="figure" coords="3,383.91,635.39,3.88,8.74">2</ref>. This data is also strongly annotated using textual descriptions of the images and various meta-data. We use only the image data for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Retrieval Task</head><p>The ten queries correspond to the ten classes of the PASCAL VOC 2006 data: bicycles, buses, cars, motorbikes, cats, cows, dogs, horses, sheep, and persons. For training, only the "train" and Figure <ref type="figure" coords="4,182.58,294.17,3.88,8.74">2</ref>: Example images from the IAPR TC-12 benchmark dataset "val" sections of the PASCAL VOC database were to be used. For each query, participants were asked to submit a list of 1000 images obtained by their method from the IAPR-TC12 database, ranked in the order of best to worst satisfaction of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measure</head><p>To evaluate the retrieval performance we use the same measure used by most retrieval evaluations such as the other tasks in CLEF/ImageCLEF <ref type="bibr" coords="4,299.32,408.14,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="4,319.08,408.14,11.62,8.74" target="#b25">26]</ref>, TREC <ref type="foot" coords="4,367.56,406.57,3.97,6.12" target="#foot_3">4</ref> and TRECVid<ref type="foot" coords="4,440.86,406.57,3.97,6.12" target="#foot_4">5</ref> . The avereage precision (AP) gives an indicate for the retrieval quality for one topic and the mean average precision (MAP) provides a single-figure measure of quality across recall levels averaged over all queries. To calculate these measures, it of course necessary to judge which images are relevant for a given query and which are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Assessments</head><p>To find relevant images, we created pools per topic <ref type="bibr" coords="4,306.03,502.24,10.51,8.74" target="#b0">[1]</ref> keeping the top 100 results from all submitted runs resulting in 1,507 images to be judged per topic on average. This resulted in a total of 15,007 images to be assessed. The normal relevance judgement process in information retrieval tasks envisages that several users judge each document in question for relevance and that for each image relevance for the particular query is judged. In this case, to judge the relevance is easy enough that we can postulate that every two persons among the judges would come to the same conclusion and therefore each image was judged by only one judge. Furthermore, since only 10 queries were to be judged and the concepts to find are simple, the judges complained about the task being to easy and boring. Therefore, after approximately 3000 images were judged, we allowed the judges to additionally specify whether an image is relevant with respect to any of the other queries. The whole judgement process was performed over a web interface which was quickly created and everybody from the RWTH Aachen University Human Language Technology and Pattern Recognition Group and from the Vienna University of Technology PRIP group was invited to judge images. Thus, most of the judges are computer science students and researchers with a Human Language Technology background. Note, that in the pooling process all images that are not judged are automatically considered to be not relevant.</p><p>The web-interface is shown in Figure <ref type="figure" coords="4,272.89,693.52,4.98,8.74" target="#fig_1">3</ref> to give an impression of the process. On each page, 10 images are shown, and the judge has to decide whether a particular object is present in these images or not. To reduce boredom for the judges, they are allowed <ref type="bibr" coords="4,381.66,717.43,86.67,8.74">(and recommended)</ref>  whether other object classes are present in the images. The judges were told to be rather positive about the relevance of an image, e.g. to consider sheep-like animals such as llamas to be sheep and to consider tigers and other non-domestic cats to be cats. Furthermore, Ville Viitaniemi from the HUTCIS group, judged all 20,000 images with respect to relevance for all of the topics with a stricter definition of relevances.</p><p>Results from the Relevance Judgements Table <ref type="table" coords="5,327.04,395.64,4.98,8.74" target="#tab_0">1</ref> gives an overview how many images were found to be relevant for each of the given topics. It can be observed that there are far more relevant images for the person topic than for any other topic. From these numbers it can be seen that the task at hand is really challenging for most of the classes and very similar to the proverbial looking for a needle in a haystack. In particular for the sheep topic, only 0.03% of the images in the database are relevant although some more images were judged to be relevant by the judges in the additional relevance information. If only the data from the conventional pooling process is considered for eight of the ten classes less than a thousandth of the images are relevant. The discrepancy in the results with additional relevance judgements and the results with full relevance judgement are due to different relevance criteria. The judges that created the additional relevance information were instructed to judge images as relevant that show sheep-like animals such as llamas, and to judge tigers as cats, where the full relevance judgement was stricter in this respect. Table <ref type="table" coords="5,117.46,539.10,4.98,8.74" target="#tab_0">1</ref> also shows that the additional relevance judgements found more cats and sheep than are actually in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Seven international groups from academia participated in the task and submitted a total of 38 runs. The group with the highest number of submissions had 13 submissions. In the following sections, the methods of the groups are explained (in alphabetical order) and references to further work are given. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Budapest methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">budapest-acad315</head><p>The task of object retrieval is to classify objects found in images. This means to find an objects in an image, which is similar to sample objects in the pre-classified images. There are two problems with this task: the first is, how do we model objects. The second is, how do we measure similarity of objects. Our first answer to the first question is to model objects with image segments. Segment, region or blob based image similarity is a common method in content based image retrieval, see for example <ref type="bibr" coords="6,128.78,439.65,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="6,142.38,439.65,12.73,8.74" target="#b26">27,</ref><ref type="bibr" coords="6,158.19,439.65,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="6,169.03,439.65,11.62,8.74" target="#b20">21]</ref>. Respectively, the basis of our first method is to find segments on the query image, which are similar to the objects in the pre-classified images. The image is then classified to be in that class, to which we find the most similar segment in the query image.</p><p>Image segmentation in itself is a widely researched and open problem itself. We used an image segmenter developed by our group to extract segments from the query images. Our method is based on a graph-based algorithm developed by Felzenszwalb and Huttenlocher <ref type="bibr" coords="6,444.28,499.42,14.62,8.74" target="#b10">[11]</ref>. We implemented a pre-segmentation method to reduce the computational time and use a different smoothing technique. All images were sized to a fixed resolution. Gaussian-based smoothing helped us cut down high frequency noise. Because of the efficiency of OpenCV<ref type="foot" coords="6,370.64,533.72,3.97,6.12" target="#foot_5">6</ref> implementation we did not implement either a resizing or a Gaussian-based smoothing algorithm. As pre-segmentation we built a three-level Gaussian-Laplacian pyramid to define initial pixel groups. The original pyramidbased method, which considers the connection between pixels on different levels too, was modified to eliminate the so-called blocking problem. We used brightness difference to measure distance between pixels:</p><formula xml:id="formula_0" coords="6,131.23,618.98,381.77,9.65">dif f Y (P 1 , P 2 ) = 0.3 * | R P2 -R P1 | +0.59 * | G P2 -G P1 | +0.11 * | B P2 -B P1 |<label>(1)</label></formula><p>After pre-segmentation, we had segments of 16 pixels maximum. To detect complex segments, we modified the original graph-based method by Felzenszwalb and Huttenlocher <ref type="bibr" coords="6,458.01,648.86,15.50,8.74" target="#b10">[11]</ref> with an adaptive threshold system using Euclidean distance to prefer larger regions instead of small regions of the image. Felzenszwalb and Huttenlocher defined an undirected graph G = (V, E) where ∀v i ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels. This graph-based representation of the image reduces the original proposition into a graph cutting challenge. They made a very efficient and linear algorithm that yields a result near to the optimal normalized cut which is one of the NP-full graph problems <ref type="bibr" coords="6,385.89,720.59,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="6,404.71,720.59,11.63,8.74" target="#b28">29]</ref>.</p><p>Algorithm 1 Algorithm Segmentation (I src , τ 1 , τ 2 ) τ 1 and τ 2 are threshold functions. Let I 2 be the source image, I 1 and I 0 are the down-scaled images. Let P (x, y, i) be the pixel P (x, y) in the image on level i (I i ). Let G = (V, E) be an undirected weighted graph where ∀v i ∈ V corresponds to a pixel P (x, y). Each edge (v i , v j ) has a non-negative weight w(v i , v j ).</p><p>Gaussian-Laplacian Pyramid 1. For every P (x, y, 1) Join(P (x, y, 1), P (x/2, y/2, 0)) if τ 1 &lt; dif f Y (P (x, y, 1), P (x/2, y/2, 0)) 2. For every P (x, y, 2) Join(P (x, y, 2), P (x/2, y/2, 1)) if τ 1 &lt; dif f Y (P (x, y, 2), P (x/2, y/2, 1)) 4. Repeat steps 1,2,3 for every neighboring group (R 1 , R 2 ) until possible to join two groups Algorithm 1 sometimes does not find relevant parts with low initial thresholds. To find the relevant borders which would disappear with the graph-based method using high thresholds we calculated the Sobel-gradient image to separate important edges from other remainders.</p><formula xml:id="formula_1" coords="7,90.00,245.66,233.08,31.87">Graph-based Segmentation 1. Compute M ax weight (R) = max e∈M ST (R,E) w(e)</formula><p>Similarity of complex objects is usually measured on a feature base. This means the the similarity of the objects is defined by the similarity in a certain feature-space.</p><formula xml:id="formula_2" coords="7,198.73,465.74,310.03,9.65">dist(S i , O j ) = d(F (S i ), F (O j )) : S i ∈ S, O j ∈ O (<label>2</label></formula><formula xml:id="formula_3" coords="7,508.76,465.74,4.24,8.74">)</formula><p>where S is the set of segments and O is the set of objects, dist is the distance function of the objects and segments, d is a distance function in the feature space (usually some of the conventional metrics in the n-dimensional real space), F is the function which assigns features to objects and segments. We extracted from the segments features, like mean color, size, shape information, and histogram information. As shape information a 4 × 4 sized low-resolution variant of the segment (framed in a rectangle with background) was used. Our histograms had 5 bins in each channel. Altogether a 35 dimensional, real valued feature-vector was extracted for each of the segments. The same features were extracted for the objects in the pre-classified images taking them as segments. The background and those classes, which were not requested were ignored. The features of the objects were written to a file, with the class-identifiers, which were extracted from the color-coding. This way we obtained a data-base of class samples, containing features of objects belonging to the classes. After this, the comparison of the objects of the pre-classified sample-images and the segments of the query image was possible. We used Euclidean distance to measure similarity. The distance of the query-image Q was computed as:</p><formula xml:id="formula_4" coords="7,208.17,664.99,300.59,14.43">dist(Q) = min i,j dist(S i , O j ) : S i ∈ S, O j ∈ O (<label>3</label></formula><formula xml:id="formula_5" coords="7,508.76,664.99,4.24,8.74">)</formula><p>where S is the set of segments of image Q, O is the set of the pre-classified sample objects. Q is classified to be in the class of that object, which minimizes the distance. The score of an image was computed as:</p><formula xml:id="formula_6" coords="7,246.73,727.36,262.03,8.74">score(Q) = 1000/dist(Q) (<label>4</label></formula><formula xml:id="formula_7" coords="7,508.76,727.36,4.24,8.74">)</formula><p>where Q is the query image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">budapest-acad314</head><p>In our first method (see budapest-acad315) we found that our segments are much smaller than the objects in the pre-segmented images. It would have been possible to get larger segments by adjusting the segmentation algorithm, however this way we would not get objects, which were really similar to the objects. We found that our segmentation algorithm could not generate segments similar to the the objects in the pre-classified images with any settings of the parameters.</p><p>Even, if we tried our algorithm on the sample-images, and the segments were approximately of the same size, the segments did not match the pre-classified objects. The reason for this is that pre-segmentation was made by humans and algorithmic segmentation is far from capable of the same result. For example, it is almost impossible to write an algorithm, which would segment a shape of a human being as one segment, if his clothes are different. However, people were one of the classes defined, and the sample images contained people with the entire body as one object. Therefore we modified our method. Our second method is still segment-based. But we also do a segmentation on the sample-images. We took the segmented sample-images, and if a segment was 80% inside of an area of a pre-defined object, then we took this segment as a proper sample for that object. This way a set of sample segments were created. After this the method is similar to the previous, the difference is only that we have sample-segments instead of sample objects, but we treat them the same way anyway. The features of the segments were extracted and they were written to a file, with the identifier of the class, which was extracted from the color-codes. After this, the comparison of the segments of the pre-classified images and the query image was possible.</p><p>We used Euclidean distance again to measure similarity. The closest segment of the image to a segment in any of the objects was searched.</p><formula xml:id="formula_8" coords="8,209.66,393.42,303.34,14.43">dist(Q) = min i,j dist(S i , S j ) : S i ∈ S, S j ∈ O<label>(5)</label></formula><p>where S is the segments of image Q, O is the set of segments belonging to the pre-classified objects.</p><p>The image was classified according to the object, to which the closest segment belongs. As we expected, this modification made the algorithm better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HUTCIS: Conventional Supervised Learning using Fusion of Image Features</head><p>Authors: Ville Viitaniemi, Jorma Laaksonen Affiliation: Adaptive Informatics Research Centre, Laboratory of Computer and Information Science, Helsinki University of Technology, Finland Email:</p><p>firstname.lastname@tkk.fi</p><p>All our 13 runs identified with prefix HUTCIS implement a similar general system architecture with three system stages:</p><p>1. Extraction of a large number of global and semi-global image features. Here we interpret global histograms of local descriptors as one type of global image feature.</p><p>2. For each individual feature, conventional supervised classification of the test images using the VOC2006 trainval images as the training set.</p><p>3. Fusion of the feature-wise classifier outputs.</p><p>By using this architecture, we knowingly ignored the aspect of qualitatively different training and test data. The motivation was to provide a baseline performance level that could be achieved by just applying a well-working implementation of the conventional supervised learning approach. The following briefly describes the components of the architecture. For more detailed description, see e.g. <ref type="bibr" coords="9,147.61,147.89,14.62,8.74" target="#b33">[34]</ref>.</p><p>Features For different runs, the features are chosen from a set of feature vectors, each with several components. Table <ref type="table" coords="9,207.09,185.74,4.98,8.74" target="#tab_4">3</ref> lists 10 of the features. Additionally, the available feature set includes interest point SIFT feature histograms with different histogram sizes, and concatenations of pairs, triples and quadruples of the tabulated basic feature vectors. The SIFT histogram bins have been selected by clustering part of the images with with self-organising map (SOM) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification and fusion</head><p>The classification is performed either by a C-SVC implementation built around LIBSVM support vector machine (SVM) library <ref type="bibr" coords="9,363.07,259.45,9.96,8.74" target="#b2">[3]</ref>, or a SOM-based classifier <ref type="bibr" coords="9,494.73,259.45,14.62,8.74" target="#b18">[19]</ref>. The SVM classifiers (prefix HUTCIS SVM) are fused together using an additional SVM layer. For the SOM classifiers (prefix HUTCIS PICSOM), the fusion is based on summation of normalized classifier outputs.</p><p>The different runs Our principal run HUTCIS SVM FULLIMG ALL realizes all the three system stages in the best way possible. Other runs use subsets of those image features, inferior algorithms or are otherwise predicted to be suboptimal.</p><p>The run HUTCIS SVM FULLIMG ALL performs SVM-classification with all the tabulated features, SIFT histograms and and twelve previously hand-picked concatenations of the tabulated features, selected on basis of SOM classifier performance in the VOC2006 task. The runs HUT-CIS SVM FULLIMG IP+SC and HUTCIS SVM FULLIMG IP+SC are otherwise similar but use just subsets of the features: SIFT histograms and color histogram, or just SIFT histograms, respectively.</p><p>The runs identified by prefix HUTCIS SVM BB are half-baked attempts to account for the different training and test image distributions. These runs are also based on SIFT histogram and color histogram features. For the training images, the features are calculated from the bounding boxes specified in VOC2006 annotations. For the test images, the features are calculated for whole images. The different runs with this prefix correspond to different ways to select the images as a basis for SIFT codebook formation.</p><p>The run HUTCIS FULLIMG+BB is the rank based fusion of features extracted from full images and bounding boxes. The runs HUTCIS PICSOM1 and HUTCIS PICSOM2 are otherwise identical but use a different setting of SOM classifier parameter. HUTCIS PICSOM2 smooths less  the feature spaces, the detection is based on more local information. Both the runs are based on the full set of features mentioned above. concatenations of pairs, triples and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The tabulated MAP results of HUTCIS runs are corrupted by our own slip in ordering of the queries that prevented us from fully participating the pool selection for query topics 4-10. For the normal pooling, our mistake excluded us completely, for additional relevance judgments only partly. This is expected to degrade our results quite much for the normal pooling and somewhat less for the additional pool, as participating the pool selection is known to be essential for obtaining good performance figures.</p><p>As expected, HUTCIS SVM FULLIMG ALL turned out to be the best of our runs for the uncorrupted query topics 1-3. The idea of extracting features only from bounding boxes of objects in order to reduce the effect of different backgrounds in the training and test images seemed usually not to work as well as features extracted from full images, although results were not very conclusive. Partly this is could be to the asymmetry in feature extraction: the features extracted from bounding boxes of the training objects were compared with the features of whole of the test images. The results of the SOM classifier runs did not provide information that would be of general interest, besides confirming the previously known result of SOM classifiers being inferior to SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INAOE's Annotation-based object retrieval approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors:</head><p>Heidy Marisol Marin Castro, Hugo Jair Escalante Balderas, and Carlos Arturo Hernández Gracidas Affiliation: TIA Research Group, Computer Science Department, National Institute of Astrophysics, Optics and Electronics, Tonantzintla, Mexico Email: {hmarinc,hugojair,carloshg}@ccc.inaoep.mx</p><p>The TIA research group at INAOE, Mexico proposed two methods based on image labeling. Automatic image annotation methods were used for labeling regions within segmented images, and then we performed object retrieval based on the generated annotations. <ref type="bibr" coords="10,413.78,452.78,31.79,8.74">Results</ref> were not what we expected, though it can be due to the fact that annotations were defined subjectively and that not enough images were annotated for creating the training set for the annotation method. Two approaches were proposed: a semi-supervised classifier based on unlabeled data and a supervised one, the last method was enhanced with a recent proposed method based on semantic cohesion <ref type="bibr" coords="10,90.00,512.56,9.97,8.74" target="#b7">[8]</ref>. Both approaches followed the following steps:</p><p>1. Image segmentation 2. Feature extraction 3. Manual labeling of a small subset of the training set 4. Training a classifier 5. Using the classifier for labeling the test-images 6. Using labels assigned to regions images for object retrieval For both approaches the full collection of images was segmented with the normalized cuts algorithm <ref type="bibr" coords="10,90.00,614.14,14.62,8.74" target="#b29">[30]</ref>. A set of 30 features were extracted from each region; we considered color, shape and texture attributes. We used our own tools for image segmentation, feature extraction and manual labeling <ref type="bibr" coords="10,90.00,638.05,14.62,8.74" target="#b21">[22]</ref>. The considered annotations were the labels of the 10 objects defined for this task. The features for each region together with the manual annotations for each region were used as training set with the two approaches proposed. Each classifier was trained with this dataset and then all of the test images were annotated with such a classifier. Finally, the generated annotations were used for retrieving objects with queries. Queries were created using the labels of the objects defined for this task; and selected as relevant those images with the highest number of regions annotated with the object label. Sample segmented images with their corresponding manual annotations are shown in Figure <ref type="figure" coords="10,161.71,721.73,3.88,8.74" target="#fig_2">4</ref>. As we can see the segmentation algorithm works well for some images (isolated cows, close-up of people), however for other objects segmentation is poor (a bicycle, for example).  The next step consisted of improving annotation performance of knn using an approach called (MRFI ) <ref type="bibr" coords="11,128.67,408.68,10.52,8.74" target="#b7">[8]</ref> which we recently proposed for improving annotation systems. This approach consists of modeling each image (region-annotations pairs) with a Markov random field (MRF ), introducing semantic knowledge, see Figure <ref type="figure" coords="11,251.72,432.59,3.88,8.74" target="#fig_3">5</ref>. The top-k more likely annotations for each region are considered. Each of these annotations have a confidence weight related to the relevance of the label to being the correct annotation for that region, according to knn. The MRFI approach uses the relevance weights with semantic information for choosing a unique (the correct) label for each region. Semantic information is considered in the MRF for keeping coherence among annotations assigned to regions within a common image; while the relevance weight is considered for taking into account the confidence of the annotation method (k -nn) on each of the labels, see Figure <ref type="figure" coords="11,90.00,516.27,3.88,8.74" target="#fig_3">5</ref>. The (pseudo) optimal configuration of regions-annotations for each image is obtained by minimizing an energy function defined by potentials. For optimization we used standard simulated annealing.</p><p>The intuitive idea of the MRFI approach is to guarantee that the labels assigned to regions are coherent among them, taking into account semantic knowledge and the confidence of the annotation system. In previous work, semantic information was obtained from cooccurrences of labels on an external corpus. However for this work semantic association between a pair of labels is given by the normalized number of relevant documents returned by Google R to queries generated using the pair of labels. This run is named INAOE-TIA-INAOE-RB-KNN+MRFI, see <ref type="bibr" coords="11,486.11,611.91,10.52,8.74" target="#b7">[8]</ref> for details.</p><p>SSAssemble: Semi-supervised Weighted AdaBoost The semi-supervised approach consist of using a recently proposed ensemble of classifiers, called WSA <ref type="bibr" coords="11,378.59,661.72,14.62,8.74" target="#b21">[22]</ref>. Our WSA ensemble uses naive Bayes as its base classifier. A set of these is combined in a cascade based on the AdaBoost technique <ref type="bibr" coords="11,138.37,685.63,14.61,8.74" target="#b12">[13]</ref>. Ensemble methods work by combining a set of base classifiers in some way, such as a voting scheme, producing a combined classifier which usually outperforms a single classifier. When training the ensemble of Bayesian classifiers, WSA considers the unlabeled images on each stage. These are annotated based on the classifier from the previous stage, and then used to train the next classifier. The unlabeled instances are weighted according to a confidence measure based on their predicted probability value; while the labeled instances are weighted according to Algorithm 2 Semi-supervised Weighted AdaBoost (WSA) algorithm. Require: L: labeled instances, U : unlabeled instances, P : training instances, T : Iterations Ensure: Final Hypothesis and probabilities:</p><formula xml:id="formula_9" coords="12,95.37,136.85,328.55,146.20">H f = argmax T t=1 log 1 B t , P (x i ) 1: W (x i ) 0 = 1 N umInst(L) , ∀x i ∈ L 2: for t from 1 to T do 3: W (x i ) t = W (xi) N i=1 W (x i ) ∀x i ∈ L 4: h t = C(L, W (x i ) t ) 5: e t = N i=1 W (x i ) t if h t (x i ) = y i 6:</formula><p>if e t ≥ 0.5 then </p><formula xml:id="formula_10" coords="12,91.13,344.38,234.73,61.28">B t = et (1-et) 13: W (x i ) (t+1) = W (x i ) t * B t if h t (x i ) = y i ∀x i ∈ L 14: P (x i ) = C(L, U, W (x i ) t ) 15: W (x i ) = P (x i ) * B t ∀x i ∈ U 16:</formula><p>end for the classifier error, as in standard AdaBoost. Our method is based on the supervised multi-class AdaBoost ensemble, which has shown to be an efficient scheme to reduce the rate error of different classifiers.</p><p>Formally WSA algorithm receives a set of labeled data (L) and a set of unlabeled data (U ). An initial classifier N B 1 is build using L. The labels in L are used to evaluate the error of N B 1 . As in AdaBoost the error is used to weight the examples, increasing the weight of the misclassified examples and keeping the same weight of the correctly classified examples. The classifier is used to predict a class for U with certain probability. In the case of U , the weights are multiplied by the predicted probability of the majority class. Unlabeled examples with high probability of their predicted class will have more influence in the construction of the next classifier than examples with lower probabilities. The next classifier N B 2 is build using the weights and predicted class of L ∪ U . N B 2 makes new predictions on U and the error of N B 2 on all the examples is used to re-weight the examples. This process continues, as in AdaBoost, for a predefined number of cycles or when a classifier has a weighted error greater or equal to 0.5. As in AdaBoost, new instances are classified using a weighted sum of the predicted class of all the constructed base classifiers. WSA is described in algorithm 2. We faced several problems when performing the annotation image task. The first one was that the training set and the test set were different, so this caused a classification with high error ratio. The second one was due the segmentation algorithm. The automatic segmentation algorithm did not perform well for all images leading to have incorrect segmentation of the objects in the images. The last one concerns to the different criteria for manual labeling of the training set. Due all these facts we did not get good results. We hope improving the annotation task by changing part of the labeling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MSRA: Object Retrieval</head><p>Authors:</p><p>Mingjing Li, Xiaoguang Rui, and Lei Wu Affiliation: Microsoft Research Asia Email: mjli@microsoft.com Two approaches were adopted by Microsoft Research Asia (MSRA) to perform the object retrieval task in ImageCLEF 2007. One is based on the visual topic model (VTM); the other is the visual language modelling (VLM) method <ref type="bibr" coords="13,249.76,169.80,14.62,8.74" target="#b34">[35]</ref>. VTM represents an image by a vector of probabilities that the image belongs to a set of visual topics, and categorizes images using SVM classifiers. VLM represents an image as a 2-D document consisting of visual words, trains a statistical language model for each image category, and classifies an image to the category that generates the image with the highest probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">VTM: Visual Topic Model</head><p>Probabilistic Latent Semantic Analysis (pLSA) <ref type="bibr" coords="13,302.85,261.91,14.61,8.74" target="#b11">[12]</ref>, which is a generative model from the text literature, is adopted to find out the visual topics from training images. Different from traditional pLSA, all training images of 10 categories are put together in the training process and about 100 visual topics are discovered finally.</p><p>The training process consists of five steps: local feature extraction, visual vocabulary construction, visual topic construction, histogram computation, and classifier training. At first, salient image regions are detected using scale invariant interest point detectors such as the Harris-Laplace and the Laplacian detectors. For each image, about 1,000 to 2,000 salient regions are extracted. Those regions are described by the SIFT descriptor which computes a gradient orientation histogram within the support region. Next, 300 local descriptors are randomly selected from each category and combined together to build a global vocabulary of 3,000 visual words. Based on the vocabulary, images are represented by the frequency of visual words. Then, pLSA is performed to discover the visual topics in the training images. pLSA is also applied to estimate how likely an image belongs to each visual topic. The histogram of the estimated probabilities is taken as the feature representation of that image for classification. For multi-class classification problem, we adopt the one-against-one scheme, and train an SVM classifier with RBF kernel for each possible pair of categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">VLM: Visual Language Modeling</head><p>The approach consists of three steps: image representation, visual language model training and object retrieval. Each image is transformed into a matrix of visual words. First, an image is simply segmented into 8 x 8 patches, and the texture histogram feature is extracted from each path. Then all patches in the training set are grouped into 256 clusters based on their features. Next, each path cluster is represented using an 8-bit hash code, which is defined as the visual word. Finally, an image is represented by a matrix of visual words, which is called visual document.</p><p>Visual words in a visual document are not independent to each other, but correlated with other words. To simply the model training, we assume that visual words are generated in the order from left to right, and top to bottom and each word is only conditionally dependent on its immediate top and left neighbors, and train a trigram language model for each image category. Given a test image, it is transformed into a matrix of visual words in the same way, and the probability that it is generated by each category is estimated respectively. Finally, the image categories are ranked in the descending order of these probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Introduction</head><p>Object retrieval is an interdisciplinary research problem between object recognition and contentbased image retrieval (CBIR). It is commonly expected that object retrieval can be solved more effectively with the joint maximization of CBIR and object recognition techniques. The goal of this paper is to study a typical CBIR solution with application to the object retrieval tasks <ref type="bibr" coords="14,479.48,166.27,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="14,497.50,166.27,11.63,8.74" target="#b17">18]</ref>. We expected that the empirical study in this work will serve as a baseline for future research when applying CBIR techniques for object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Overview of Our Solution</head><p>We study a typical CBIR solution for the object retrieval problem. In our approach, we focus on two key tasks. One is the feature representation, the other is the supervised learning scheme with support vector machines.</p><p>Feature Representation In our approach, three kinds of global features are extracted to represent an image, including color, shape, and texture.</p><p>For color, we study the Grid Color Moment feature (GCM). For each image, we split it into 3 × 3 equal grids and extract color moments to represent each of the 9 grids. Three color moments are then computed: color mean, color variance and color skewness in each color channel (H, S, and V), respectively. Thus, an 81-dimensional color moment is adopted as the color feature for each image.</p><p>For shape, we employ the edge direction histogram. First, an input color image is first converted into a gray image. Then a Canny edge detector is applied to obtain its edge image. Based on the edge images, the edge direction histogram can then be computed. Each edge direction histogram is quantized into 36 bins of 10 degrees each. In addition, we use a bin to count the number of pixels without edge. Hence, a 37-dimensional edge direction histogram is used for shape.</p><p>For texture, we investigate the Gabor feature. Each image is first scaled to the size of 64 × 64. Then, the Gabor wavelet transformation is applied to the scaled image at 5 scale levels and 8 orientations, which results in a total of 40 subimages for each input image. For each subimage, we calculate three statistical moments to represent the texture, including mean, variance, and skewness. Therefore, a 120-dimensional feature vector is used for texture.</p><p>In total, a 238-dimensional feature vector is used to represent each image. The set of visual features has been shown to be effective for content-based image retrieval in our previous experiments <ref type="bibr" coords="14,119.11,511.42,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="14,137.93,511.42,11.63,8.74" target="#b17">18]</ref>.</p><p>Supervised Learning for Object Retrieval The object retrieval task defined in ImageCLEF 2007 is similar to a relevance feedback task in CBIR, in which a number of possible and negative labeled examples are given for learning. This can be treated as a supervised classification task. To solve it, we employ the support vector machines (SVM) technique for training the classifiers on the given examples <ref type="bibr" coords="14,188.61,585.14,14.61,8.74" target="#b16">[17]</ref>. In our experiment, a standard SVM package is used to train the SVM classifier with RBF kernels. The parameters C and γ are best tuned on the VOC2006 training set, in which the training precision is 84.2% for the classification tasks. Finally, we apply the trained classifiers to do the object retrieval by ranking the distances of the objects apart from the classifier's decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Concluding Remarks</head><p>We described our solution for the object retrieval task in the ImageCLEF 2007 by a typical CBIR solution. We found that the current solution, though it was trained with good performance in an object recognition test-bed, did not achieve promising results in the tough object retrieval tasks. In our future work, several directions can be explored to improve the performance, including local feature representation and better machine learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">PRIP: Color Interest Points and SIFT features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors:</head><p>Julian Stöttinger </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction of multi-channel based interest points An extension of the intensity-based</head><p>Harris detector <ref type="bibr" coords="15,160.58,434.81,15.50,8.74" target="#b15">[16]</ref> is proposed in <ref type="bibr" coords="15,246.51,434.81,14.62,8.74" target="#b24">[25]</ref>. Because of common photometric variations in imaging conditions such as shading, shadows, specularities and object reflectance, the components of the RGB color system are correlated and therefore sensitive to illumination changes. However, in natural images, high contrast changes may appear. Therefore, a color Harris detector in RGB space does not dramatically change the position of the corners compared to a luminance based approach. Normalized rgb overcomes the correlation of RGB and favors color changes. The main drawback, however, is its instability in dark regions. We can overcome this by using quasi invariant color spaces. The approach PRIP-PRIP HSI ScIvHarris uses the HSI color space <ref type="bibr" coords="15,421.32,530.45,14.61,8.74" target="#b31">[32]</ref>, which is quasiinvariant to shadowing and specular effects. Therefore, changes in lighting conditions in images should not affect the positions of the interest points, resulting in more stable locations. Additionally, the HSI color space discriminates between luminance and color. Therefore, much information can be discarded, and the locations get more sparse and distinct.</p><p>The PRIP cbOCS ScIvHarris approach follows a different idea. As proposed in <ref type="bibr" coords="15,465.21,590.23,14.62,8.74" target="#b32">[33]</ref>, colors have different occurrence probabilities and therefore different information content. Therefore, rare colors are regarded as more salient than common ones. We wish to find a boosting function so that color vectors having equal information content have equal impact on the saliency function. This transformation can be found analyzing the occurrence probabilities of colors in large image databases. With this change of focus towards rare colors, we aim to discard many repetitive locations and get more stable results on rare features.</p><p>The characteristic scale of an interest point is chosen by applying a principal component analysis (PCA) on the image and thus finding a description for the correlation of the multi-channel information <ref type="bibr" coords="15,144.32,697.82,14.61,8.74" target="#b30">[31]</ref>. The characteristic scale is decided when the Laplacian of Gaussian function of this projection and the Harris energy is a maximum at the same location in the image. The final extraction of these interest points and corresponding scales is done by preferring locations with high Harris energy and huge scales. A maximum number of 300 locations per image has been defined, as over-description diminish the overall recognition ability dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local descriptions of interest points</head><p>The scale invariant feature transform (SIFT) <ref type="bibr" coords="16,497.51,112.02,15.49,8.74" target="#b19">[20]</ref> showed to give best results in a broad variety of applications <ref type="bibr" coords="16,372.18,123.98,14.62,8.74" target="#b22">[23]</ref>. We used the areas of the extracted interest points as a basis for the description phase. SIFT are basically sampled and normalized gradient histograms, which can lead to multiple descriptions per location. This occurs if there is more than one direction of the gradients regarded as predominant.</p><p>Estimating the signature of an image In this bag of visual features approach <ref type="bibr" coords="16,449.04,185.74,14.61,8.74" target="#b35">[36]</ref>, we cluster the descriptions of one image to a fixed number of 40 clusters using k-means. The centroids and the proportional sizes of the clusters build the signature of one image having a fixed dimensionality of 40 by 129.</p><p>Classification The Earth Mover's Distance (EMD) <ref type="bibr" coords="16,335.06,247.50,15.49,8.74" target="#b27">[28]</ref> showed to be a suitable metric for comparing image signatures. It takes the proportional sizes of the clusters into account, which seems to gain a lot of discriminative power. The classification itself is done in the most straight forward way possible: for every object category, the smallest distances to another signature build the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">RWTHi6: Patch-Histograms and Log-Linear Models</head><p>Authors:</p><p>Thomas Deselaers, Hermann Ney Affiliation: Human Language Technology and Pattern Recognition, RWTH Aachen University, Aachen, Germany Email:</p><p>surname@cs.rwth-aachen.de</p><p>The approach used by the Human Language Technology and Pattern Recognition group of the RWTH Aachen University, Aachen, Germany, to participate in the PASCAL Visual Object Classes Challenge consists of four steps:</p><p>1. patch extraction 2. clustering 3. creation of histograms 4. training of a log-linear model where the first three steps are feature extraction steps and the last is the actual classification step. This approach was first published in <ref type="bibr" coords="16,251.89,510.05,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="16,265.73,510.05,7.01,8.74" target="#b6">7]</ref>. The method follows the promising approach of considering objects to be constellations of parts which offers the immediate advantages that occlusions can be handled very well, that the geometrical relationship between parts can be modelled (or neglected), and that one can focus on the discriminative parts of an object. That is, one can focus on the image parts that distinguish a certain object from other objects.</p><p>The steps of the method are briefly outlined in the following paragraphs. To model the difference in the training and test data, the first three steps have been done for the training and test data individually, and then the according histograms have been extracted for the respective other, so that once the vocabulary was learnt for the training data and once for the test data and the histograms are created for each using both vocabularies. Results, however show that this seems not to be a working approach to tackle divergence in training and testing data.</p><p>Patch Extraction Given an image, we extract square image patches at up to 500 image points. Additionally, 300 points from a uniform grid of 15×20 cells that is projected onto the image are used. At each of these points a set of square image patches of varying sizes (in this case 7 × 7, 11 × 11, 21 × 21, and 31 × 31 pixels) are extracted and scaled to a common size (in this case 15 × 15 pixels).</p><p>In contrast to the interest points from the detector, the grid-points can also fall onto very homogeneous areas of the image. This property is on the one hand important for capturing homogeneity in objects which is not found by the interest point detector and on the other hand it captures parts of the background which usually is a good indicator for an object, as in natural image objects are often found in a "natural" environment.</p><p>After the patches are extracted and scaled to a common size, a PCA dimensionality reduction is applied to reduce the large dimensionality of the data, keeping 39 coefficients corresponding to the 40 components of largest variance but discarding the first coefficient corresponding to the largest variance. The first coefficient is discarded to achieve a partial brightness invariance. This approach is suitable because the first PCA coefficient usually accounts for global brightness.</p><p>Clustering The data are then clustered using a k-means style iterative splitting clustering algorithm to obtain a partition of all extracted patches. To do so, first one Gaussian density is estimated which is then iteratively split to obtain more densities. These densities are then reestimated using k-means until convergence is reached and then the next split is done. It has be shown experimentally that results consistently improve up to 4096 clusters but for more than 4096 clusters the improvement is so small that it is not worth the higher computational demands.</p><p>Creation of Histograms Once we have the cluster model, we discard all information for each patch except its closest corresponding cluster center identifier. For the test data, this identifier is determined by evaluating the Euclidean distance to all cluster centers for each patch. Thus, the clustering assigns a cluster c(x) ∈ {1, . . . C} to each image patch x and allows us to create histograms of cluster frequencies by counting how many of the extracted patches belong to each of the clusters. The histogram representation h(X) with C bins is then determined by counting and normalization such that</p><formula xml:id="formula_11" coords="17,218.96,377.34,124.51,15.18">h c (X) = 1 L X L X l=1 δ(c, c(x l ))</formula><p>, where δ denotes the Kronecker delta function, c(x l ) is the closest cluster center to x l , and x l is the l-th image patch extracted from image X, from which a total of L X patches are extracted.</p><p>Training &amp; Classification Having obtained this representation by histograms of image patches, we define a decision rule for the classification of images. The approach based on maximum likelihood of the class-conditional distributions does not take into account the information of competing classes during training. We can use this information by maximizing the class posterior probability K k=1 N k n=1 p(k|X kn ) instead. Assuming a Gaussian density with pooled covariances for the class-conditional distribution, this maximization is equivalent to maximizing the parameters of a log-linear or maximum entropy model</p><formula xml:id="formula_12" coords="17,218.62,522.55,165.77,30.20">p(k|h) = 1 Z(h) exp α k + C c=1 λ kc h c ,</formula><p>where</p><formula xml:id="formula_13" coords="17,119.91,565.04,135.55,14.11">Z(h) = K k=1 exp α k + C c=1</formula><p>λ kc h c is the renormalization factor. We use a modified version of generalized iterative scaling. Bayes' decision rule is used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results for all submissions are given in Table <ref type="table" coords="17,309.77,637.16,4.98,8.74" target="#tab_6">4</ref> using the normal Qrels and in Table <ref type="table" coords="17,481.69,637.16,4.98,8.74" target="#tab_7">5</ref> using the Qrels with additional information. Furthermore in Table <ref type="table" coords="17,365.14,649.12,3.88,8.74" target="#tab_8">6</ref>, the results are presented using the relevance information estimated for each image in the database for all classes. All results are given as AP for the individual topics (first 10 columns) and MAP over all topics (last column).</p><p>The tables are ordered by MAP (last column). When considering the individual topics, it can be observed that some methods work well for some of the topics but completely fail for others. To highlight this, the highest AP in each column in shown in bold. The two runs from Budapest perform well for the bicycles (1) and motorbikes (4) topics respectively, but have bad results for all other topics.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>One issue that should be taken into account when interpreting the results is that 50% of the evaluated runs are from HUTCIS and thus this group had a significant impact on the pools for relevance assessment. This effect is further boosted by the fact that the initial runs from HUTCIS (which were used for the pooling) had the queries 4-10 in wrong order. This implies that for the topics where the runs were in correct order, HUTCIS has a good chance of having many of their images in the pools and that furthermore for the runs where HUTCIS' runs were broken, none of their images were considered in pooling. Therefore, it can be observed that the additional relevance information leads to some significant changes in some of the results. A particular strong change of the results can be seen for the runs from HUTCIS: the APs of the queries which were in correct order for the pooling process are strongly reduced whereas those which were in wrong order are significantly improved, which shows that the large number of runs from HUTCIS had a strong influence on the results of the pooling. The drawbacks of using pooling is visible in the results as the MAPs for some methods increase when the additional relevance judgements are used. This effect is particular strong for topics with only very few relevant images.</p><p>The results clearly show that the task is a very difficult one, and that most recent methods are not yet able to fully cope with tasks where training and test data do not match very well. This means that although the other image-and object retrieval and recognition tasks are very good to foster advancement in research, they do not yet pose a fully realistic challenge as their trainingand testing data match up pretty well. A higher variability between the test-and the training data would make these tasks more realistic.</p><p>The participating methods use a wide variety of different techniques, some methods use global image descriptors, other methods use local descriptors and discriminative models, which are currently the state of the art in object recognition and detection.</p><p>For the classes cat (5), cow <ref type="bibr" coords="20,229.26,112.02,11.63,8.74" target="#b5">(6)</ref>, and sheep <ref type="bibr" coords="20,295.93,112.02,11.63,8.74" target="#b8">(9)</ref>, AP values are so small that it is not obvious which method produces the best results. For the classes cat and sheep this effect is even stronger since only 6 resp. 7 images from the whole database are relevant.</p><p>No single method obtained the highest AP values in all classes. A very strong outlier in this respect are the two methods from the Budapest-group which have clearly the best results for the bicycle and the motorbike class respectively but have very bad performance for all other classes. The fact that the queries are very dissimilar in terms of 1) image statistics (number of correct images) and 2) visual difficulty, makes the AP values vary strongly among the different topics and therefore MAP is not a very reliable measure judge the overall retrieval quality here.</p><p>Finally, although a maximum of 1000 images were to be returned for each query, the number of relevant images exceeded 1000 for two classes (car and person). This implies that the maximum possible AP (using the full annotations) for "car" is 0.789, and for "person" is 0.089, thus the performance of the best runs for the person query is nearly perfect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented the object retrieval task of ImageCLEF 2007, the methods of the participating groups, and the results. The results show that none of the methods really solves the assigned task and therewith that, although large advances in object detection and recognition were achieved over the last years, still many improvements are necessary to solve realistic tasks on a web-scale size with a high variety of the data to be analysed. It can however be observed that some of the methods are able to obtain reasonable results for a limited set of classes.</p><p>Furthermore, the analysis of the results showed that the use of pooling techniques for relevance assessment can be problematic if the pools are biased due to erroneous runs or due to many strongly correlated submissions as it was the case in this evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,140.34,316.31,322.32,8.74;3,215.25,237.75,84.60,63.45"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example images from the PASCAL VOC 2006 training dataset.</figDesc><graphic coords="3,215.25,237.75,84.60,63.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,193.60,329.66,215.80,8.74;6,153.45,108.86,296.09,205.68"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The relevance judgement web-interface.</figDesc><graphic coords="6,153.45,108.86,296.09,205.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,180.92,175.00,241.16,8.74;11,129.77,195.69,170.08,85.04"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample images from the generated training set. .</figDesc><graphic coords="11,129.77,195.69,170.08,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,90.00,294.85,423.00,8.74;11,90.00,306.49,423.00,7.86;11,90.00,317.45,345.13,7.86;11,303.17,195.70,170.07,85.04"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: graphical description of the improvement process of MRFI. Right: interpretation of MRFI for a given configuration of labels and regions; (red) line-arcs consider semantic cohesion between labels, while (blue) dashed-arcs consider relevance weight of each label according to k -nn .</figDesc><graphic coords="11,303.17,195.70,170.07,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,471.41,717.43,41.59,8.74"><head>Table 1 :</head><label>1</label><figDesc>Results from the relevance judgement process. Column 3 shows the number of relevant images when standard pooling is used, column 4 when the additional class information is taken into account. Column 5 shows the results of the relevance judgement of all 20,000 images.</figDesc><table coords="4,471.41,717.43,41.59,8.74"><row><cell>to specify</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,102.18,267.58,410.82,73.41"><head></head><label></label><figDesc>for every coherent group of points R where M ST (R, E) is the minimal spanning tree2. Compute Co(R) = τ 2 (R) + M ax weight (R) as the measure of coherence between points in R 3. Join(R 1 , R 2 ) if e ∈ E exists so w(e) &lt; min(Co(R 1 ), Co(R 2 )) is true, where R 1 R 2 = ∅and w(e) is the weight of the border edge e between R 1 and R 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,721.25,423.00,20.69"><head>Table 2</head><label>2</label><figDesc>The last row of the table indicates what the rank of the HUTCIS SVM FULLIMG ALL run would have been among the 19 VOC 2006 participants.</figDesc><table /><note coords="8,128.54,721.25,384.47,8.74;8,90.00,733.20,423.00,8.74;9,90.00,112.02,157.26,8.74"><p><p><p>with ROC AUC performances in VOC 2006 test set reveals that the performance of our principal run HUTCIS SVM FULLIMG ALL is relatively close to the best performances in the last year's VOC evaluation</p><ref type="bibr" coords="9,233.97,112.02,9.97,8.74" target="#b8">[9]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,103.32,563.20,390.22,66.83"><head>Table 2 :</head><label>2</label><figDesc>ROC AUC performance in VOC2006 test set</figDesc><table coords="9,103.32,574.51,390.22,55.51"><row><cell>Run id.</cell><cell>bic.</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>cow</cell><cell>dog</cell><cell cols="3">horse mbike person</cell><cell>sheep</cell></row><row><cell>FULLIMG ALL</cell><cell>0.921</cell><cell>0.978</cell><cell>0.974</cell><cell>0.930</cell><cell>0.937</cell><cell>0.866</cell><cell>0.932</cell><cell>0.958</cell><cell>0.874</cell><cell>0.941</cell></row><row><cell>FULLIMG IP+SC</cell><cell>0.922</cell><cell>0.977</cell><cell>0.974</cell><cell>0.924</cell><cell>0.934</cell><cell>0.851</cell><cell>0.928</cell><cell>0.953</cell><cell>0.865</cell><cell>0.941</cell></row><row><cell>FULLIMG IP</cell><cell>0.919</cell><cell>0.952</cell><cell>0.970</cell><cell>0.917</cell><cell>0.926</cell><cell>0.840</cell><cell>0.903</cell><cell>0.943</cell><cell>0.834</cell><cell>0.936</cell></row><row><cell>Best in VOC2006</cell><cell>0.948</cell><cell>0.984</cell><cell>0.977</cell><cell>0.937</cell><cell>0.940</cell><cell>0.876</cell><cell>0.927</cell><cell>0.969</cell><cell>0.863</cell><cell>0.956</cell></row><row><cell>Rank</cell><cell>7th</cell><cell>4th</cell><cell>3rd</cell><cell>4th</cell><cell>4th</cell><cell>3rd</cell><cell>1st</cell><cell>5th</cell><cell>1st</cell><cell>6th</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,131.04,663.71,334.76,56.17"><head>Table 3 :</head><label>3</label><figDesc>Some of the image features used in the HUTCIS runs</figDesc><table coords="9,131.04,675.03,334.76,44.85"><row><cell>Colour layout</cell><cell>Dominant colour</cell></row><row><cell>Sobel edge histogram (4 × 4 tiling of the image)</cell><cell>HSV colour histogram</cell></row><row><cell>Average colour (5-part tiling)</cell><cell>Colour moments (5-part tiling)</cell></row><row><cell>16 × 16 FFT of edge image</cell><cell>Sobel edge histogram (5-part tiling)</cell></row><row><cell>Sobel edge co-occurrence matrix (5-part tiling)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,90.00,128.84,423.00,277.64"><head></head><label></label><figDesc>1 , Allan Hanbury 1 , Nicu Sebe 2 , Theo Gevers 2 Affiliation: 1 PRIP, Institute of Computer-Aided Automation, Vienna University of Technology, Vienna, Austria 2 Intelligent Systems Lab Amsterdam, University of Amsterdam, The Netherlands Email: {julian,hanbury}@prip.tuwien.ac.at, {nicu,gevers}@science.uva.nl In the field of retrieval, detection, recognition and classification of objects, many state of the art methods use interest point detection at an early stage. This initial step typically aims to find meaningful regions in which descriptors are calculated. Finding salient locations in image data is crucial for these tasks. Most current methods use only the luminance information of the images. This approach focuses on the use of color information in interest point detection and its gain in performance. Based on the Harris corner detector, multi-channel visual information transformed into different color spaces is the basis to extract the most salient interest points. To determine the characteristic scale of an interest point, a global method of investigating the color information on a global scope is used. The two different PRIP-PRIP ScIvHarris approaches differ in the properties of these interest points only.</figDesc><table coords="15,90.00,327.12,238.63,79.36"><row><cell>The method consists of the following stages:</cell></row><row><cell>1. Extraction of multi-channel based interest points</cell></row><row><cell>2. Local descriptions of interest points</cell></row><row><cell>3. Estimating the signature of an image</cell></row><row><cell>4. Classification</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="18,90.00,120.64,450.16,302.87"><head>Table 4 :</head><label>4</label><figDesc>Results from the ImageCLEF 2007 object retrieval task with the normal relevance information. All values have been multiplied by 100 to make the table more readable. The numbers in the top row refer to the class id's (see Table1). The MAP over all classes is in the last column. The highest AP per class is shown in bold.</figDesc><table coords="18,95.98,167.82,444.19,255.68"><row><cell cols="2">rank run id</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>avg</cell></row><row><cell>1</cell><cell>HUTCIS_SVM_FULLIMG_ALL</cell><cell>21.3</cell><cell>1.5</cell><cell>28.1</cell><cell>0.3</cell><cell>0.0</cell><cell>0.2</cell><cell>0.2</cell><cell>0.8</cell><cell>21.0</cell><cell>0.8</cell><cell>7.4</cell></row><row><cell>2</cell><cell>HUTCIS_SVM_FULLIMG_IP+SC</cell><cell>10.2</cell><cell>1.2</cell><cell>25.8</cell><cell>0.2</cell><cell>0.0</cell><cell>0.2</cell><cell>0.1</cell><cell>1.4</cell><cell>20.3</cell><cell>1.6</cell><cell>6.1</cell></row><row><cell>3</cell><cell>HUTCIS_SVM_FULLIMG+BB</cell><cell>13.0</cell><cell>1.5</cell><cell>11.4</cell><cell>0.1</cell><cell>0.0</cell><cell>0.4</cell><cell>0.1</cell><cell>0.6</cell><cell>22.4</cell><cell>1.1</cell><cell>5.1</cell></row><row><cell>4</cell><cell>HUTCIS_SVM_FULLIMG_IP</cell><cell>9.3</cell><cell>1.3</cell><cell>23.6</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>2.6</cell><cell>3.0</cell><cell>1.2</cell><cell>4.1</cell></row><row><cell>5</cell><cell>MSRA-MSRA_RuiSp</cell><cell>2.5</cell><cell>1.9</cell><cell>7.9</cell><cell>3.5</cell><cell>0.9</cell><cell>0.0</cell><cell>0.3</cell><cell>0.7</cell><cell>2.1</cell><cell>13.7</cell><cell>3.4</cell></row><row><cell>6</cell><cell>HUTCIS_SVM_BB_BAL_IP+SC</cell><cell>4.9</cell><cell>1.3</cell><cell>2.4</cell><cell>0.0</cell><cell>0.0</cell><cell>1.5</cell><cell>0.0</cell><cell>0.1</cell><cell>10.4</cell><cell>0.4</cell><cell>2.1</cell></row><row><cell>7</cell><cell>HUTCIS_PICSOM1</cell><cell>3.2</cell><cell>1.3</cell><cell>13.5</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.0</cell><cell>0.4</cell><cell>0.5</cell><cell>1.9</cell></row><row><cell>8</cell><cell>HUTCIS_PICSOM2</cell><cell>1.7</cell><cell>1.4</cell><cell>13.2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.4</cell><cell>0.0</cell><cell>0.6</cell><cell>0.5</cell><cell>1.8</cell></row><row><cell>9</cell><cell>HUTCIS_SVM_BB_ALL</cell><cell>5.8</cell><cell>1.6</cell><cell>2.4</cell><cell>0.0</cell><cell>0.0</cell><cell>1.4</cell><cell>0.0</cell><cell>0.1</cell><cell>4.4</cell><cell>0.3</cell><cell>1.6</cell></row><row><cell>10</cell><cell>HUTCIS_SVM_BB_BB_IP+SC</cell><cell>5.2</cell><cell>1.3</cell><cell>2.8</cell><cell>0.1</cell><cell>0.0</cell><cell>0.8</cell><cell>0.0</cell><cell>0.1</cell><cell>4.4</cell><cell>0.4</cell><cell>1.5</cell></row><row><cell>11</cell><cell>HUTCIS_SVM_BB_FULL_IP+SC</cell><cell>8.1</cell><cell>1.7</cell><cell>1.5</cell><cell>0.1</cell><cell>0.0</cell><cell>1.0</cell><cell>0.0</cell><cell>0.1</cell><cell>2.4</cell><cell>0.3</cell><cell>1.5</cell></row><row><cell>12</cell><cell>RWTHi6-HISTO-PASCAL</cell><cell>0.3</cell><cell>2.7</cell><cell>1.7</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.1</cell><cell>9.4</cell><cell>1.5</cell></row><row><cell>13</cell><cell>HUTCIS_SVM_BB_BB_IP</cell><cell>4.0</cell><cell>0.7</cell><cell>1.2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.2</cell><cell>2.1</cell><cell>0.3</cell><cell>0.9</cell></row><row><cell>14</cell><cell>MSRA-MSRA-VLM_8_8_640_ful</cell><cell>0.6</cell><cell>0.2</cell><cell>1.2</cell><cell>0.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>2.0</cell><cell>0.1</cell><cell>3.6</cell><cell>0.8</cell></row><row><cell>15</cell><cell>HUTCIS_SVM_BB_BAL_IP</cell><cell>3.9</cell><cell>1.6</cell><cell>1.2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.1</cell><cell>0.5</cell><cell>0.3</cell><cell>0.8</cell></row><row><cell>16</cell><cell>MSRA-MSRA-VLM-8-8-800-HT</cell><cell>0.9</cell><cell>0.1</cell><cell>1.1</cell><cell>0.4</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.6</cell><cell>0.3</cell><cell>3.2</cell><cell>0.7</cell></row><row><cell>17</cell><cell>PRIP-PRIP_HSI_ScIvHarris</cell><cell>0.2</cell><cell>0.0</cell><cell>0.6</cell><cell>0.2</cell><cell>2.6</cell><cell>0.2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>2.2</cell><cell>0.6</cell></row><row><cell>18</cell><cell>budapest-acad-budapest-acad314</cell><cell>2.3</cell><cell>0.1</cell><cell>0.0</cell><cell>0.5</cell><cell>1.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell>19</cell><cell>HUTCIS_SVM_BB_FULL_IP</cell><cell>0.2</cell><cell>1.2</cell><cell>1.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>1.2</cell><cell>0.2</cell><cell>0.4</cell></row><row><cell>20</cell><cell>NTU_SCE_HOI-NTU_SCE_HOI_1</cell><cell>2.0</cell><cell>0.1</cell><cell>1.8</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell>21</cell><cell>PRIP-PRIP_cbOCS_ScIvHarr2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell><cell>0.8</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>1.3</cell><cell>1.0</cell><cell>0.4</cell></row><row><cell>22</cell><cell>budapest-acad-budapest-acad315</cell><cell>0.3</cell><cell>0.0</cell><cell>0.0</cell><cell>1.5</cell><cell>0.0</cell><cell>0.1</cell><cell>0.3</cell><cell>0.0</cell><cell>0.1</cell><cell>0.6</cell><cell>0.3</cell></row><row><cell>23</cell><cell>INAOE-TIA-INAOE_SSAssemble</cell><cell>0.5</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.6</cell><cell>0.2</cell></row><row><cell>24</cell><cell>INAOE-TIA-INAOE-RB-KNN+MRFI_ok</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>25</cell><cell>INAOE-TIA-INAOE-RB-KNN+MRFI</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>26</cell><cell>INAOE-TIA-INAOE-RB-KNN</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="18,90.00,446.42,435.52,246.08"><head>Table 5 :</head><label>5</label><figDesc>Results from the ImageCLEF 2007 object retrieval task with the additional relevance information. All values have been multiplied by 100 to make the table more readable. The numbers in the top row refer to the class id's (see Table1). The MAP over all classes is in the last column. The highest AP per topic is shown in bold.</figDesc><table coords="18,95.98,493.60,429.54,198.89"><row><cell cols="2">rank run id</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>avg</cell></row><row><cell>1</cell><cell>HUTCIS_SVM_FULLIMG_ALL</cell><cell>7.2</cell><cell>1.3</cell><cell>14.9</cell><cell>0.4</cell><cell>0.0</cell><cell>0.4</cell><cell>0.1</cell><cell>4.1</cell><cell>2.7</cell><cell>5.8</cell><cell>3.7</cell></row><row><cell>2</cell><cell>HUTCIS_SVM_FULLIMG_IP+SC</cell><cell>3.7</cell><cell>0.7</cell><cell>14.1</cell><cell>0.5</cell><cell>0.0</cell><cell>1.1</cell><cell>0.0</cell><cell>3.5</cell><cell>3.0</cell><cell>6.3</cell><cell>3.3</cell></row><row><cell>3</cell><cell>HUTCIS_SVM_FULLIMG_IP</cell><cell>3.3</cell><cell>0.7</cell><cell>13.0</cell><cell>0.8</cell><cell>0.0</cell><cell>1.4</cell><cell>0.0</cell><cell>4.0</cell><cell>0.8</cell><cell>5.0</cell><cell>2.9</cell></row><row><cell>4</cell><cell>HUTCIS_SVM_FULLIMG+BB</cell><cell>4.7</cell><cell>1.6</cell><cell>6.5</cell><cell>0.7</cell><cell>0.0</cell><cell>0.5</cell><cell>0.0</cell><cell>2.0</cell><cell>3.1</cell><cell>4.2</cell><cell>2.4</cell></row><row><cell>5</cell><cell>HUTCIS_PICSOM1</cell><cell>1.5</cell><cell>1.2</cell><cell>7.5</cell><cell>0.2</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>0.4</cell><cell>0.6</cell><cell>5.1</cell><cell>1.7</cell></row><row><cell>6</cell><cell>HUTCIS_PICSOM2</cell><cell>1.0</cell><cell>1.1</cell><cell>7.4</cell><cell>0.3</cell><cell>0.0</cell><cell>0.2</cell><cell>0.2</cell><cell>0.3</cell><cell>0.6</cell><cell>4.6</cell><cell>1.6</cell></row><row><cell>7</cell><cell>MSRA-MSRA_RuiSp</cell><cell>1.4</cell><cell>0.7</cell><cell>5.4</cell><cell>1.0</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.3</cell><cell>5.1</cell><cell>1.5</cell></row><row><cell>8</cell><cell>HUTCIS_SVM_BB_BAL_IP+SC</cell><cell>1.9</cell><cell>2.3</cell><cell>1.1</cell><cell>0.3</cell><cell>0.0</cell><cell>0.7</cell><cell>0.0</cell><cell>0.5</cell><cell>1.8</cell><cell>3.2</cell><cell>1.2</cell></row><row><cell>9</cell><cell>HUTCIS_SVM_BB_BB_IP+SC</cell><cell>1.9</cell><cell>2.2</cell><cell>1.3</cell><cell>0.5</cell><cell>0.1</cell><cell>0.5</cell><cell>0.0</cell><cell>0.6</cell><cell>1.0</cell><cell>3.2</cell><cell>1.1</cell></row><row><cell>10</cell><cell>HUTCIS_SVM_BB_BB_IP</cell><cell>1.5</cell><cell>0.5</cell><cell>0.9</cell><cell>3.8</cell><cell>0.2</cell><cell>0.2</cell><cell>0.0</cell><cell>0.7</cell><cell>0.4</cell><cell>2.9</cell><cell>1.1</cell></row><row><cell>11</cell><cell>HUTCIS_SVM_BB_ALL</cell><cell>2.2</cell><cell>1.3</cell><cell>1.1</cell><cell>0.5</cell><cell>0.0</cell><cell>0.7</cell><cell>0.0</cell><cell>0.7</cell><cell>1.1</cell><cell>3.1</cell><cell>1.1</cell></row><row><cell>12</cell><cell>budapest-acad-budapest-acad314</cell><cell>9.1</cell><cell>0.2</cell><cell>0.0</cell><cell>0.7</cell><cell>0.3</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>1.1</cell></row><row><cell>13</cell><cell>HUTCIS_SVM_BB_FULL_IP+SC</cell><cell>3.0</cell><cell>1.4</cell><cell>0.8</cell><cell>0.2</cell><cell>0.1</cell><cell>0.6</cell><cell>0.0</cell><cell>0.6</cell><cell>1.0</cell><cell>3.0</cell><cell>1.1</cell></row><row><cell>14</cell><cell>RWTHi6-HISTO-PASCAL</cell><cell>0.2</cell><cell>1.0</cell><cell>1.5</cell><cell>0.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>5.0</cell><cell>0.8</cell></row><row><cell>15</cell><cell>HUTCIS_SVM_BB_BAL_IP</cell><cell>1.5</cell><cell>0.8</cell><cell>0.7</cell><cell>0.6</cell><cell>0.3</cell><cell>0.2</cell><cell>0.0</cell><cell>0.6</cell><cell>0.2</cell><cell>2.9</cell><cell>0.8</cell></row><row><cell>16</cell><cell>budapest-acad-budapest-acad315</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>6.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.2</cell><cell>0.1</cell><cell>0.7</cell></row><row><cell>17</cell><cell>HUTCIS_SVM_BB_FULL_IP</cell><cell>0.3</cell><cell>0.8</cell><cell>0.6</cell><cell>0.4</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.8</cell><cell>0.4</cell><cell>2.7</cell><cell>0.6</cell></row><row><cell>18</cell><cell>MSRA-MSRA-VLM_8_8_640_ful</cell><cell>0.3</cell><cell>0.3</cell><cell>1.3</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>3.2</cell><cell>0.6</cell></row><row><cell>19</cell><cell>MSRA-MSRA-VLM-8-8-800-HT</cell><cell>0.4</cell><cell>0.1</cell><cell>1.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>2.5</cell><cell>0.5</cell></row><row><cell>20</cell><cell>PRIP-PRIP_HSI_ScIvHarris</cell><cell>0.1</cell><cell>0.0</cell><cell>0.4</cell><cell>0.1</cell><cell>1.5</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>1.8</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="19,90.00,118.94,445.28,302.86"><head>Table 6 :</head><label>6</label><figDesc>Results from the ImageCLEF 2007 object retrieval task with complete relevance information for the whole database. All values have been multiplied by 100 to make the table more readable. The numbers in the top row refer to the class id's (see Table1). The MAP over all classes is in the last column. The highest AP per class is shown in bold.</figDesc><table coords="19,95.98,166.12,439.30,255.68"><row><cell cols="2">rank run id</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>avg</cell></row><row><cell>1</cell><cell>budapest-acad-budapest-acad314</cell><cell>28.3</cell><cell>0.3</cell><cell>0.1</cell><cell>1.8</cell><cell>0.2</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>3.1</cell></row><row><cell>2</cell><cell>HUTCIS_SVM_FULLIMG_ALL</cell><cell>4.1</cell><cell>1.2</cell><cell>10.6</cell><cell>0.4</cell><cell>0.0</cell><cell>0.6</cell><cell>0.1</cell><cell>3.8</cell><cell>0.0</cell><cell>8.3</cell><cell>2.9</cell></row><row><cell>3</cell><cell>HUTCIS_SVM_FULLIMG_IP+SC</cell><cell>2.6</cell><cell>1.0</cell><cell>11.1</cell><cell>1.0</cell><cell>0.0</cell><cell>1.0</cell><cell>0.1</cell><cell>3.2</cell><cell>0.0</cell><cell>8.2</cell><cell>2.8</cell></row><row><cell>4</cell><cell>HUTCIS_SVM_FULLIMG_IP</cell><cell>2.4</cell><cell>1.1</cell><cell>10.3</cell><cell>1.8</cell><cell>0.0</cell><cell>1.1</cell><cell>0.1</cell><cell>3.0</cell><cell>0.0</cell><cell>8.1</cell><cell>2.8</cell></row><row><cell>5</cell><cell>HUTCIS_SVM_FULLIMG+BB</cell><cell>3.0</cell><cell>1.1</cell><cell>4.2</cell><cell>0.6</cell><cell>0.0</cell><cell>0.7</cell><cell>0.1</cell><cell>2.5</cell><cell>0.0</cell><cell>8.6</cell><cell>2.1</cell></row><row><cell>6</cell><cell>budapest-acad-budapest-acad315</cell><cell>0.6</cell><cell>0.0</cell><cell>0.1</cell><cell>18.5</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.6</cell><cell>0.1</cell><cell>2.0</cell></row><row><cell>7</cell><cell>HUTCIS_SVM_BB_ALL</cell><cell>1.6</cell><cell>0.9</cell><cell>0.5</cell><cell>0.3</cell><cell>0.0</cell><cell>0.6</cell><cell>0.1</cell><cell>1.5</cell><cell>0.0</cell><cell>8.3</cell><cell>1.4</cell></row><row><cell>8</cell><cell>HUTCIS_SVM_BB_BB_IP+SC</cell><cell>1.4</cell><cell>1.0</cell><cell>0.7</cell><cell>0.3</cell><cell>0.0</cell><cell>0.5</cell><cell>0.1</cell><cell>1.1</cell><cell>0.0</cell><cell>8.4</cell><cell>1.4</cell></row><row><cell>9</cell><cell>HUTCIS_SVM_BB_FULL_IP+SC</cell><cell>2.0</cell><cell>0.8</cell><cell>0.4</cell><cell>0.2</cell><cell>0.0</cell><cell>0.8</cell><cell>0.1</cell><cell>1.1</cell><cell>0.0</cell><cell>8.2</cell><cell>1.3</cell></row><row><cell>10</cell><cell>HUTCIS_PICSOM1</cell><cell>0.9</cell><cell>0.7</cell><cell>4.5</cell><cell>0.6</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>0.7</cell><cell>0.0</cell><cell>5.6</cell><cell>1.3</cell></row><row><cell>11</cell><cell>MSRA-MSRA_RuiSp</cell><cell>0.9</cell><cell>0.5</cell><cell>3.6</cell><cell>0.6</cell><cell>0.7</cell><cell>0.1</cell><cell>0.1</cell><cell>0.4</cell><cell>0.0</cell><cell>6.0</cell><cell>1.3</cell></row><row><cell>12</cell><cell>HUTCIS_SVM_BB_BAL_IP+SC</cell><cell>1.3</cell><cell>0.8</cell><cell>0.5</cell><cell>0.2</cell><cell>0.0</cell><cell>0.5</cell><cell>0.1</cell><cell>0.8</cell><cell>0.0</cell><cell>8.4</cell><cell>1.3</cell></row><row><cell>13</cell><cell>HUTCIS_PICSOM2</cell><cell>0.8</cell><cell>0.6</cell><cell>4.2</cell><cell>0.5</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>0.4</cell><cell>0.0</cell><cell>5.4</cell><cell>1.2</cell></row><row><cell>14</cell><cell>HUTCIS_SVM_BB_BB_IP</cell><cell>1.1</cell><cell>0.7</cell><cell>0.4</cell><cell>1.4</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell><cell>1.0</cell><cell>0.0</cell><cell>7.2</cell><cell>1.2</cell></row><row><cell>15</cell><cell>HUTCIS_SVM_BB_BAL_IP</cell><cell>1.1</cell><cell>0.8</cell><cell>0.3</cell><cell>0.3</cell><cell>0.0</cell><cell>0.4</cell><cell>0.1</cell><cell>0.9</cell><cell>0.0</cell><cell>6.9</cell><cell>1.1</cell></row><row><cell>16</cell><cell>HUTCIS_SVM_BB_FULL_IP</cell><cell>0.3</cell><cell>0.9</cell><cell>0.3</cell><cell>0.3</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell><cell>1.1</cell><cell>0.0</cell><cell>6.6</cell><cell>1.0</cell></row><row><cell>17</cell><cell>RWTHi6-HISTO-PASCAL</cell><cell>0.4</cell><cell>0.2</cell><cell>1.4</cell><cell>0.2</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>5.5</cell><cell>0.8</cell></row><row><cell>18</cell><cell>NTU_SCE_HOI-NTU_SCE_HOI_1</cell><cell>1.2</cell><cell>0.7</cell><cell>2.4</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.8</cell><cell>0.5</cell></row><row><cell>19</cell><cell>MSRA-MSRA-VLM_8_8_640_ful</cell><cell>0.4</cell><cell>0.3</cell><cell>0.7</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell><cell>2.5</cell><cell>0.4</cell></row><row><cell>20</cell><cell>MSRA-MSRA-VLM-8-8-800-HT</cell><cell>0.3</cell><cell>0.2</cell><cell>0.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.2</cell><cell>0.1</cell><cell>2.5</cell><cell>0.4</cell></row><row><cell>21</cell><cell>INAOE-TIA-INAOE_SSAssemble</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>3.2</cell><cell>0.4</cell></row><row><cell>22</cell><cell>PRIP-PRIP_HSI_ScIvHarris</cell><cell>0.1</cell><cell>0.0</cell><cell>0.3</cell><cell>0.1</cell><cell>1.4</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>1.5</cell><cell>0.4</cell></row><row><cell>23</cell><cell>INAOE-TIA-INAOE-RB-KNN+MRFI_ok</cell><cell>0.5</cell><cell>0.1</cell><cell>0.6</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>2.2</cell><cell>0.4</cell></row><row><cell>24</cell><cell>INAOE-TIA-INAOE-RB-KNN+MRFI</cell><cell>0.5</cell><cell>0.1</cell><cell>0.6</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>2.2</cell><cell>0.4</cell></row><row><cell>25</cell><cell>INAOE-TIA-INAOE-RB-KNN</cell><cell>0.3</cell><cell>0.0</cell><cell>0.5</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>2.2</cell><cell>0.3</cell></row><row><cell>26</cell><cell>PRIP-PRIP_cbOCS_ScIvHarr2</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.5</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell><cell>0.8</cell><cell>0.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,727.62,131.26,6.64"><p>http://ir.shef.ac.uk/imageclef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,737.13,122.79,6.64"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,737.04,191.04,6.64"><p>http://www.pascal-network.org/challenges/VOC/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,105.24,737.49,88.91,6.64"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,105.24,746.99,169.87,6.64"><p>http://www-nlpir.nist.gov/projects/t01v/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,105.24,740.65,207.47,6.64"><p>http://www.intel.com/technology/computing/opencv/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. This work was partially funded by the <rs type="funder">European Commission MUSCLE NoE</rs> (<rs type="grantNumber">FP6-507752</rs>) and the <rs type="funder">DFG (German research foundation)</rs> under contract <rs type="grantNumber">Ne-572/6</rs>.</p><p>We would like to thank the <rs type="institution">PASCAL NoE</rs> for allowing us to use the training data of the PASCAL 2006 Visual object classes challenge as training data for this task, in particular, we would like to thank <rs type="person">Mark Everingham</rs> for his cooperation.</p><p>We would like to thank <rs type="person">Paul Clough</rs> from <rs type="affiliation">Sheffield University</rs> for support in creating the pools and <rs type="person">Jan Hosang</rs>, <rs type="person">Jens Forster</rs>, <rs type="person">Pascal Steingrube</rs>, <rs type="person">Christian Plahl</rs>, <rs type="person">Tobias Gass</rs>, <rs type="person">Daniel Stein</rs>, <rs type="person">Morteza Zahedi</rs>, <rs type="person">Richard Zens</rs>, <rs type="person">Yuqi Zhang</rs>, <rs type="person">Markus Nusbaum</rs>, <rs type="person">Gregor Leusch</rs>, <rs type="person">Michael Arens</rs>, <rs type="person">Lech Szumilas</rs>, <rs type="person">Jan Bungeroth</rs>, <rs type="person">David Rybach</rs>, <rs type="person">Peter Fritz</rs>, <rs type="person">Arne Mauser</rs>, <rs type="person">Saša Hasan</rs>, and <rs type="person">Stefan Hahn</rs> for helping to create relevance assessments for the images.</p><p>The work of the Budapest group was supported by a <rs type="grantName">Yahoo! Faculty Research Grant</rs> and by grants <rs type="grantNumber">MOLINGV NKFP-2/0024/2005</rs>, <rs type="grantNumber">NKFP-2004</rs> project <rs type="projectName">Language Miner</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SGScRSE">
					<idno type="grant-number">FP6-507752</idno>
				</org>
				<org type="funding" xml:id="_C3AGgm3">
					<idno type="grant-number">Ne-572/6</idno>
				</org>
				<org type="funding" xml:id="_RhZGX2m">
					<idno type="grant-number">MOLINGV NKFP-2/0024/2005</idno>
					<orgName type="grant-name">Yahoo! Faculty Research Grant</orgName>
				</org>
				<org type="funded-project" xml:id="_qtayygY">
					<idno type="grant-number">NKFP-2004</idno>
					<orgName type="project" subtype="full">Language Miner</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="20,110.48,658.89,402.52,8.74;20,110.48,670.84,402.52,8.74;20,110.48,682.80,334.88,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="20,241.83,658.89,142.53,8.74">CLEF Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,485.38,658.89,27.63,8.74;20,110.48,670.84,376.17,8.74">Crosslanguage information retrieval and evaluation: Proceedings of the CLEF2001 Workshop</title>
		<title level="s" coord="20,495.04,670.84,17.96,8.74;20,110.48,682.80,139.29,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="394" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,702.72,402.52,8.74;20,110.48,714.68,402.52,8.74;20,110.48,726.63,188.49,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="20,344.65,702.72,168.35,8.74;20,110.48,714.68,295.02,8.74">Blobworld: Image Segmentation Using Expectation-Maximization and Its Application to Image Querying</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,418.61,714.68,94.40,8.74;20,110.48,726.63,82.49,8.74">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1026" to="1038" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,112.02,402.51,8.74;21,110.48,123.44,266.11,9.55" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="21,234.82,112.02,205.20,8.74">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,143.90,402.52,8.74;21,110.48,155.86,157.60,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="21,223.46,143.90,272.60,8.74">Image Categorization by Learning and Reasoning with Regions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,504.71,143.90,8.29,8.74;21,110.48,155.86,78.64,8.74">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="913" to="939" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,175.78,402.51,8.74;21,110.48,187.74,402.52,8.74;21,110.48,199.69,402.52,8.74;21,110.48,211.65,402.53,8.74;21,110.48,223.60,167.79,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="21,304.61,175.78,208.38,8.74;21,110.48,187.74,153.03,8.74">Overview of the CLEF Cross-Language Image Retrieval Track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,274.98,199.69,238.03,8.74;21,110.48,211.65,235.62,8.74">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="21,354.58,211.65,154.14,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,243.53,402.52,8.74;21,110.48,255.48,402.53,8.74;21,110.48,267.44,185.75,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="21,280.45,243.53,232.56,8.74;21,110.48,255.48,60.78,8.74">Discriminative Training for Object Recognition using Image Patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,191.41,255.48,272.69,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,287.36,402.51,8.74;21,110.48,299.32,402.52,8.74;21,110.48,311.27,402.52,8.74;21,110.48,323.23,22.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="21,274.83,287.36,238.17,8.74;21,110.48,299.32,116.61,8.74">Improving a Discriminative Approach to Object Recognition using Image Patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,249.15,299.32,258.69,8.74">DAGM 2005, Pattern Recognition, 26th DAGM Symposium</title>
		<title level="s" coord="21,180.58,311.27,151.48,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="volume">3663</biblScope>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,343.15,402.52,8.74;21,110.48,355.11,402.52,8.74;21,110.48,367.06,291.61,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="21,351.32,343.15,161.68,8.74;21,110.48,355.11,179.24,8.74">Word Co-occurrence and MRF&apos;s for Improving Automatic Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,331.95,355.11,181.05,8.74;21,110.48,367.06,143.94,8.74">Proceedings of the 18th British Machine Vision Conference (BMVC 2007)</title>
		<meeting>the 18th British Machine Vision Conference (BMVC 2007)<address><addrLine>Warwick, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,386.99,402.52,8.74;21,110.48,398.94,402.52,8.74;21,110.48,411.62,159.68,8.30" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/" />
		<title level="m" coord="21,396.69,386.99,116.31,8.74;21,110.48,398.94,191.87,8.74">The Pascal Visual Object Classes Challenge 2006 (VOC2006) Results</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="21,110.48,430.82,402.52,8.74;21,110.48,442.78,402.52,8.74;21,110.48,454.73,402.52,8.74;21,110.48,466.69,402.52,8.74;21,110.48,478.65,402.52,8.74;21,110.48,490.60,402.53,8.74;21,110.48,502.56,402.52,8.74;21,110.48,514.51,402.52,8.74;21,110.48,526.47,22.69,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,381.12,478.65,131.88,8.74;21,110.48,490.60,94.75,8.74">The 2005 PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dorko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D R</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,227.28,490.60,285.73,8.74;21,110.48,502.56,394.01,8.74">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment (PASCAL Workshop 05</title>
		<title level="s" coord="21,183.60,514.51,168.24,8.74">Lecture Notes in Artificial Intelligence</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="117" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,546.39,402.52,8.74;21,110.48,558.35,228.45,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,317.45,546.39,190.81,8.74">Efficient Graph-Based Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,110.48,558.35,181.95,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,578.27,402.51,8.74;21,110.48,590.23,393.32,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="21,335.79,578.27,177.20,8.74;21,110.48,590.23,54.24,8.74">Learning object categories from Google&apos;s image search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,185.82,590.23,199.38,8.74">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">Oct 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,610.15,402.52,8.74;21,110.48,622.11,241.18,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="21,236.48,610.15,197.02,8.74">Experiments with a New Boosting Algorithm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,456.07,610.15,56.93,8.74;21,110.48,622.11,142.51,8.74">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,642.03,402.52,8.74;21,110.48,653.99,402.52,8.74;21,110.48,665.94,114.91,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="21,360.79,642.03,152.21,8.74;21,110.48,653.99,124.95,8.74">Overview of the ImageCLEF 2007 Photographic Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,261.33,653.99,197.96,8.74">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,110.48,685.87,402.52,8.74;21,110.48,697.82,402.53,8.74;21,110.48,709.78,370.82,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="21,370.60,685.87,142.41,8.74;21,110.48,697.82,221.79,8.74">The IAPR Benchmark: A New Evaluation Resource for Visual Information Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,351.33,697.82,161.67,8.74;21,110.48,709.78,195.99,8.74">LREC 06 OntoImage 2006: Language Resources for Content-Based Image Retrieval</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
	<note>page in press</note>
</biblStruct>

<biblStruct coords="21,110.48,729.70,402.51,8.74;21,110.48,741.66,144.91,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="21,237.91,729.70,178.38,8.74">A Combined Corner and Edge Detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,439.52,729.70,73.47,8.74;21,110.48,741.66,46.15,8.74">4th Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,112.02,402.52,8.74;22,110.48,123.98,402.52,8.74;22,110.48,135.93,172.21,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="22,235.89,112.02,277.11,8.74;22,110.48,123.98,64.70,8.74">A novel log-based relevance feedback technique in content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,200.85,123.98,283.64,8.74">12th ACM International Conference on Multimedia (MM 2004)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">oct 2004</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,154.92,402.51,8.74;22,110.48,166.87,384.82,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="22,265.27,154.92,247.72,8.74;22,110.48,166.87,34.67,8.74">A unified log-based relevance feedback scheme for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,153.42,166.87,244.91,8.74">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="524" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,185.86,402.52,8.74;22,110.48,197.82,402.52,8.74;22,110.48,209.77,262.12,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="22,300.03,185.86,212.97,8.74;22,110.48,197.82,134.54,8.74">PicSOM-Self-Organizing Image Retrieval with MPEG-7 Content Descriptions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,256.89,197.82,256.11,8.74;22,110.48,209.77,143.34,8.74">IEEE Transactions on Neural Networks, Special Issue on Intelligent Multimedia Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="853" />
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,228.76,402.52,8.74;22,110.48,240.71,22.69,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="22,156.31,228.76,257.11,8.74">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,424.52,228.76,21.10,8.74">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,259.70,402.52,8.74;22,110.48,271.66,402.52,8.74;22,110.48,283.61,351.29,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="22,257.40,259.70,235.71,8.74">Image similarity search with compact data structures</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,110.48,271.66,402.52,8.74;22,110.48,283.61,99.13,8.74">CIKM &apos;04: Proceedings of the thirteenth ACM international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="208" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,302.60,402.52,8.74;22,110.48,314.55,402.52,8.74;22,110.48,326.51,402.52,8.74;22,110.48,338.46,127.13,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="22,358.26,302.60,154.74,8.74;22,110.48,314.55,179.31,8.74">Automatic image annotation using a semi-supervised ensemble of classifiers</title>
		<author>
			<persName coords=""><forename type="first">H.-M</forename><surname>Marin-Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,369.50,314.55,143.49,8.74;22,110.48,326.51,146.73,8.74">12th Iberoamerican Congress on Pattern Recognition CIARP 2007</title>
		<title level="s" coord="22,266.28,326.51,156.48,8.74">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>del Mar, Valparaiso, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="22,110.48,357.45,402.52,8.74;22,110.48,369.41,336.31,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="22,249.89,357.45,197.57,8.74">A performance evaluation of local descriptors</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolaczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,456.72,357.45,56.28,8.74;22,110.48,369.41,224.92,8.74">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,388.40,402.51,8.74;22,110.48,400.35,65.59,8.74" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P.-A</forename><surname>Moellic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fluhr</surname></persName>
		</author>
		<title level="m" coord="22,244.21,388.40,159.78,8.74">ImageEVAL 2006 Official Campaign</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Im-agEVAL</note>
</biblStruct>

<biblStruct coords="22,110.48,419.34,402.52,8.74;22,110.48,431.29,66.97,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="22,295.46,419.34,168.58,8.74">Differential Invariants for Color Images</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,485.24,419.34,22.21,8.74">ICPR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">838</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,450.28,402.52,8.74;22,110.48,462.24,402.52,8.74;22,110.48,474.19,374.91,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="22,160.50,462.24,347.96,8.74">Overview of the ImageCLEFmed 2007 Medical Retrieval and Annotation Tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,122.93,474.19,191.90,8.74">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,493.18,402.52,8.74;22,110.48,505.13,378.44,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="22,314.78,493.18,198.23,8.74;22,110.48,505.13,137.12,8.74">Region-based image retrieval using integrated color, shape, and location index</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,256.39,505.13,127.33,8.74">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="193" to="233" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,524.12,402.51,8.74;22,110.48,536.08,160.20,8.74" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="22,292.94,524.12,220.05,8.74;22,110.48,536.08,37.76,8.74">The Earth Mover&apos;s Distance as a Metric for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,156.85,536.08,21.10,8.74">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,555.06,402.52,8.74;22,110.48,567.02,228.16,8.74" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="22,211.26,555.06,188.28,8.74">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,412.86,555.06,100.13,8.74;22,110.48,567.02,144.44,8.74">IEEE Transactions on Pattern and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,586.01,402.52,8.74;22,110.48,597.96,151.51,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="22,201.78,586.01,182.85,8.74">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,393.91,586.01,119.09,8.74;22,110.48,597.96,55.49,8.74">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,616.95,402.51,8.74;22,110.48,628.91,148.76,8.74" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="22,334.00,616.95,178.99,8.74;22,110.48,628.91,68.89,8.74">Do Colour Interest Points Improve Image Retrieval? ICIP</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Stöttinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="22,110.48,647.89,402.52,8.74;22,110.48,659.85,121.38,8.74" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="22,248.88,647.89,259.75,8.74">Edge and Corner Detection by Photometric Quasi-Invariants</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,110.48,659.85,23.15,8.74">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,678.84,402.51,8.74;22,110.48,690.79,170.49,8.74" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="22,327.59,678.84,185.40,8.74;22,110.48,690.79,40.22,8.74">Boosting Color Saliency in Image Feature Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,159.59,690.79,23.15,8.74">PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,110.48,709.78,402.51,8.74;22,110.48,721.73,402.52,8.74;22,110.48,733.69,402.53,8.74;22,110.48,745.64,142.13,8.74" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="22,253.97,709.78,259.02,8.74;22,110.48,721.73,58.97,8.74">Improving the accuracy of global feature fusion based image categorisation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,191.60,721.73,321.40,8.74;22,110.48,733.69,149.50,8.74">Proceedings of the 2nd International Conference on Semantic and Digital Media Technologies (SAMT 2007)</title>
		<title level="s" coord="22,268.98,733.69,156.52,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 2nd International Conference on Semantic and Digital Media Technologies (SAMT 2007)<address><addrLine>Genova, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-12">December 2007</date>
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct coords="23,110.48,112.02,402.51,8.74;23,110.48,123.98,402.52,8.74;23,110.48,135.93,225.92,8.74" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="23,350.69,112.02,162.30,8.74;23,110.48,123.98,56.41,8.74">Visual Language Modeling for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,194.29,123.98,318.71,8.74;23,110.48,135.93,84.62,8.74">9th ACM SIGMM International Workshop on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">sep 2007</date>
		</imprint>
	</monogr>
	<note>MIR&apos;07)</note>
</biblStruct>

<biblStruct coords="23,110.48,155.86,402.51,8.74;23,110.48,167.81,402.52,8.74;23,110.48,179.77,22.69,8.74" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="23,347.14,155.86,165.85,8.74;23,110.48,167.81,292.82,8.74">Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,412.61,167.81,27.10,8.74">CWPR</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
