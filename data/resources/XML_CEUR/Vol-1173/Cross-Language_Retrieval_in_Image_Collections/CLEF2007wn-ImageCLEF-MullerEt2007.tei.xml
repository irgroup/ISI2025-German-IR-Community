<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.26,148.51,288.56,15.61;1,129.85,170.43,343.39,15.61">Overview of the ImageCLEF 2007 Medical Retrieval and Annotation Tasks</title>
				<funder ref="#_FGVurdA">
					<orgName type="full">MUSCLE NoE</orgName>
				</funder>
				<funder ref="#_KMNpGyP">
					<orgName type="full">Swiss National Science Foundation (FNS)</orgName>
				</funder>
				<funder ref="#_bX6XR9F">
					<orgName type="full">DFG (Deutsche Forschungsgemeinschaft)</orgName>
				</funder>
				<funder ref="#_ym5tDmQ">
					<orgName type="full">American National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_7kQRfRM">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,120.04,204.00,68.12,9.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Informatics</orgName>
								<orgName type="institution">University and Hospitals of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.70,204.00,78.63,9.96"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Dep</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.91,204.00,53.83,9.96"><forename type="first">Eugene</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.30,204.00,123.40,9.96"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.26,217.95,88.28,9.96"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Medical Informatics</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.10,217.95,63.19,9.96"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oregon Health and Science University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.26,148.51,288.56,15.61;1,129.85,170.43,343.39,15.61">Overview of the ImageCLEF 2007 Medical Retrieval and Annotation Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">476B6719ABAE61234E8441D0B2FFD413</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image Retrieval, Performance Evaluation, Image Classification, Medical Imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the medical image retrieval and medical image annotation tasks of ImageCLEF 2007. Separate sections describe each of the two tasks, with the participation and an evaluation of major findings from the results of each given. A total of 13 groups participated in the medical retrieval task and 10 in the medical annotation task.</p><p>The medical retrieval task added two news data sets for a total of over 66'000 images. Tasks were derived from a log file of the Pubmed biomedical literature search system, creating realistic information needs with a clear user model in mind.</p><p>The medical annotation task was in 2007 organised in a new format as a hierarchical classification had to be performed and classification could be stopped at any confidence level. This required algorithms to change significantly and to integrate a confidence level into their decisions to be able to judge where to stop classification to avoid making mistakes in the hierarchy. Scoring took into account errors and unclassified parts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF 1 <ref type="bibr" coords="1,151.02,706.76,10.52,9.96" target="#b2">[3,</ref><ref type="bibr" coords="1,165.09,706.76,7.75,9.96" target="#b1">2]</ref> started within CLEF 2 (Cross Language Evaluation Forum <ref type="bibr" coords="1,435.40,706.76,15.50,9.96" target="#b14">[15]</ref>) in 2003 with the goal to benchmark image retrieval in multilingual document collections. A medical image retrieval task was added in 2004 to explore domain-specific multilingual information retrieval and also multi-modal retrieval by combining visual and textual features for retrieval. Since 2005, a medical retrieval and a medical image annotation task were both part of ImageCLEF <ref type="bibr" coords="2,466.12,135.26,14.61,9.96" target="#b11">[12]</ref>.</p><p>The enthusiastic participation in CLEF and particularly for ImageCLEF has shown the need for benchmarks and their usefulness to the research community. Again in 2007, a total of 48 groups registered for ImageCLEF to get access to the data sets and tasks. Among these, 13 participated in the medical retrieval task and 10 in the medical automatic annotation task.</p><p>Other important benchmarks in the field of visual information retrieval include TRECVID 3 on the evaluation of video retrieval systems <ref type="bibr" coords="2,283.57,207.00,14.61,9.96" target="#b17">[18]</ref>, ImagEval 4 , mainly on visual retrieval of images and image classification, and INEX 5 (INiative for the Evaluation of XML retrieval) concentrating on retrieval of multimedia based on structured data. Close contact exists with these initiatives to develop complementary evaluation strategies.</p><p>This article focuses on the two medical tasks of ImageCLEF 2007, whereas two other papers <ref type="bibr" coords="2,90.00,266.77,10.52,9.96" target="#b6">[7,</ref><ref type="bibr" coords="2,104.34,266.77,7.76,9.96" target="#b3">4]</ref> describe the new object classification task and the new photographic retrieval task. More detailed information can also be found on the task web pages for ImageCLEFmed 6 and the medical annotation task 7 . A detailed analysis of the 2005 medical image retrieval task and its outcomes is also available in <ref type="bibr" coords="2,172.59,302.63,9.96,9.96" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Medical Image Retrieval Task</head><p>The medical image retrieval task has been run for four consecutive years. In 2007, two new databases were added for a total of more than 66'000 images in the collection. For the generation of realistic topics or information needs, log files of the medical literature search system Pubmed were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Overview</head><p>Again and as in previous years, the medical retrieval task showed to be popular among many research groups registering for CLEF. In total 31 groups from all continents and 25 countries registered. A total of 13 groups submitted 149 runs that were used for the pooling required for the relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Databases</head><p>In 2007, the same four datasets were used as in 2005 and 2006 and two new datasets were added. The Casimage 8 dataset was made available to participants <ref type="bibr" coords="2,353.45,533.64,14.61,9.96" target="#b12">[13]</ref>, containing almost 9'000 images of 2'000 cases <ref type="bibr" coords="2,155.78,545.60,14.62,9.96" target="#b13">[14]</ref>. Images present in Casimage include mostly radiology modalities, but also photographs, PowerPoint slides and illustrations. Cases are mainly in French, with around 20% being in English and 5% without annotation. We also used the PEIR 9 (Pathology Education Instructional Resource) database with annotation based on the HEAL 10 project (Health Education Assets Library, mainly Pathology images <ref type="bibr" coords="2,278.45,593.42,10.29,9.96" target="#b0">[1]</ref>). This dataset contains over 33'000 images with English annotations, with the annotation being on a per image and not a per case basis as in Casimage. The nuclear medicine database of MIR, the Mallinkrodt Institute of Radiology 11 <ref type="bibr" coords="2,494.74,617.33,14.61,9.96" target="#b21">[22]</ref>, was also made available to us for ImageCLEFmed. This dataset contains over 2'000 images mainly from nuclear medicine with annotations provided per case and in English. Finally, the PathoPic 12 In 2007, we added two new datasets. The first was the myPACS<ref type="foot" coords="3,379.05,352.16,7.93,6.97" target="#foot_0">13</ref> dataset of 15'140 images and 3'577 cases, all in English and containing mainly radiology images. The second was the Clinical Outcomes Research Initiative (CORI <ref type="foot" coords="3,254.22,376.07,7.93,6.97" target="#foot_1">14</ref> ) Endoscopic image database contains 1'496 images with an English annotation per image and not per case. This database extends the spectrum of the total dataset as so far there were only few endoscopic images in the dataset. An overview of the datasets can be seen in Table <ref type="table" coords="3,221.33,413.31,4.98,9.96" target="#tab_0">1</ref> As such, we were able to use more than 66'000 images, with annotations in three different languages. Through an agreement with the copyright holders, we were able to distribute these images to the participating research groups. The myPACS database required an additional copyright agreement making the process slightly more complex than in previous years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Registration and Participation</head><p>In 2007, 31 groups from all 6 continents and 25 countries registered for the ImageCLEFmed retrieval task, underlining the strong interest in this evaluation campaign. As in previous years, only about half of the registered groups finally submitted results, often blaming a lack of time for this. The feedback of these groups remains positive as they say to use the data for their research as a very useful resource.</p><p>The following groups finally also submitted results for the medical image retrieval task:</p><p>• CINDI group, Concordia University, Montreal, Canada;</p><p>• Dokuz Eylul University, Izmir, Turkey;</p><p>• IPAL/CNRS joint lab, Singapore, Singapore;</p><p>• IRIT-Toulouse, Toulouse, France;</p><p>• MedGIFT group, University and Hospitals of Geneva, Switzerland;</p><p>• Microsoft Research Asia, Beijing, China;</p><p>• MIRACLE, Spanish University Consortium, Madrid, Spain;</p><p>Ultrasound with rectangular sensor. Ultraschallbild mit rechteckigem Sensor.</p><p>Ultrason avec capteur rectangulaire.</p><p>Figure <ref type="figure" coords="4,252.22,244.21,3.87,9.96">1</ref>: Example for a visual topic.</p><p>• MRIM-LIG, Grenoble, France;</p><p>• OHSU, Oregon Health &amp; Science University, Portland, OR, USA;</p><p>• RWTH Aachen Pattern Recognition group. Aachen, Germany;</p><p>• SINAI group, University of Jaen Intelligent Systems, Jaen, Spain;</p><p>• State University New York (SUNY) at Buffalo, NY, USA;</p><p>• UNAL group, Universidad Nacional Colombia, Bogotà, Colombia;</p><p>In total, 149 runs were submitted, with the maximum being 36 of a single group and the minimum a single run per group. Several runs had incorrect formats. These runs were corrected by the organisers whenever possible but a few runs were finally omitted from the pooling process and the final evaluation because trec eval could not parse the results even after our modifications. All groups have the possibility to describe further runs in their working notes papers after the format corrections as the qrels files were made available to all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Topics</head><p>Query topics for 2007 were generated based on a log file of Pubmed <ref type="foot" coords="4,391.91,488.21,7.93,6.97" target="#foot_2">15</ref> . The log file of 24 hours contained a total of 77'895 queries. In general, the search terms were fairly vague and did not contain many image-related topics, so we filtered out words such as image, video, and terms relating to modalities such as x-ray, CT, MRI, endoscopy etc. We also aimed for the resulting terms to cover at least two or more of the axes: modality, anatomic region, pathology, and visual observation (e.g., enlarged heart). A total of 50 candidate topics were taken from these and sometimes an additional axis such as modality was added. From these topics we checked whether at least a few relevant images are in the database and once this was finished, 30 topics were selected.</p><p>All topics were categorised with respect to the retrieval approach expected to perform best: visual topics, textual (semantic) topics and mixed topics. This was performed by an experienced image retrieval system developer. For each of the three retrieval approach groups, ten topics were selected for a total of 30 query topics that were distributed among the participants. Each topic consisted of the query itself in three languages (English, German, French) and 2-3 example images for the visual part of the topic. Topic images were searched for on the Internet and were not part of the database. This made visual retrieval significantly harder as most images were taken with different collections compared to those in the database and had changes in the grey level or colour values.</p><p>Figure <ref type="figure" coords="4,137.34,704.78,4.98,9.96">1</ref> shows a visual topic, Figure <ref type="figure" coords="4,276.19,704.78,4.98,9.96" target="#fig_1">2</ref> a topic that should be retrieved well with a mixed approach and Figure <ref type="figure" coords="4,188.23,716.73,4.98,9.96" target="#fig_2">3</ref> a topics with very different images in the results sets that should be well-suited for textual retrieval, only.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Relevance Judgements</head><p>Relevance judgments in ImageCLEFmed were performed by physicians and other studennts in the OHSU biomedical informatics graduate program. All were paid an hourly rate for their work. The pools for relevance judging were created by selecting the top ranking images from all submitted runs. The actual number selected from each run has varied by year. In 2007, it was 35 images per run, with the goal of having pools of about 800-1200 images in size for judging. The average pool size in 2007 was 890 images. Judges were instructed to rate images in the pools are definitely relevant (DR), partially relevant (PR), or not relevant (NR). Judges were instructed to use the partially relevant desingation only in case they could not determine whether the image in question was relevant.</p><p>One of the problems was that all judges were English speakers but that the collection had a fairly large number of French and German documents. If the judgment required reading the text, judges had more difficulty ascertaining relevance. This could create a bias towards relevance for documents with English annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Submissions and Techniques</head><p>This section quickly summarises the main techniques used by the participants for retrieval and the sort of runs that they submitted. We had for the first time several problems with the submissions although we sent out a script to check runs for correctness before submission. In 2006, this script was part of the submission web site, but performance problems had us change this setup. The unit for retrieval and relevance was the image and not the case but several groups submitted case IDs that we had to replace with the first image of the case. Other problems include the change of upper/lower case for the image IDs and the change of the database names that also changed the image IDs. Some groups reused the 2006 datasets that were corrected before 2007 and also ended up with invalid IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">CINDI</head><p>The CINDI group submitted a total of 4 valid runs, two feedback runs and two automatic runs, each time one with mixed media and a purely visual run. Text retrieval uses a simple tf/idf weighting model and uses English, only. For visual retrieval a fusion model of a variety of features and image representations is used. The mixed media run simply combine the two outcomes in a linear fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">DEU</head><p>Dokuz Eylul University submitted 5 runs, 4 visual and one textual run. The text runs is a simple bag of words approach and for visual retrieval several strategies were used containing color layout, color structure, dominant color and an edge histogram. Each run contained only one single technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">IPAL</head><p>IPAL submitted 6 runs, all of them text retrieval runs. After having had the best performance for two years, the results are now only in the middle of the performance scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">IRIT</head><p>The IRIT group submitted a single valid run, which was a text retrieval run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.5">MedGIFT</head><p>The MedGIFT group submitted a total of 13 runs. For visual retrieval the GIFT (GNU Image Finding Tool) was used to create a sort of baseline run, as this system had been used in the same configuration since the beginning of ImageCLEF. Multilingual text retrieval was performed with EasyIR and a mapping of the text in the three languages towards MeSH (Medical Subject Headings) to search in semantic terms and avoid language problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.6">MIRACLE</head><p>MIRACLE submitted 36 runs in total and thus most runs of all groups. The text retrieval runs were among the best, whereas visual retrieval was in the midfield. The combined runs were worse than text alone and also only in the midfield.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.7">LIG</head><p>MRIM-LIG submitted 6 runs, all of them textual runs. Besides the best textual results, this was also the best overall result in 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.8">OHSU</head><p>The OHSU group submitted 10 textual and mixed runs, using Fire as a visual system. Their mixed runs had good performance as well as the best early precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.9">RWTH</head><p>The Human language technology and pattern recognition group from the RWTH Aachen University in Aachen, Germany submitted 10 runs using the FIRE image retrieval system. The runs are based on a wide variety of 8 visual descriptors including image thumbnails, patch histograms, and different texture features. For the runs using textual information, a text retrieval system is used in the same way as in the last years. The weights for the features are trained with the maximum entropy training method using the qrels of the 2005 and 2006 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.10">SINAI</head><p>The SINAI group submitted 30 runs in total, all of them textual or mixed. For text retrieval, the terms of the query are mapped onto MeSH, and then, the query is expanded with these MeSH terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.11">SUNY</head><p>SUNY submitted 7 runs, all of which are mixed runs using Fire as visual system. One of the runs is among the best mixed runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.12">UNAL</head><p>The UNAL group submitted 8 runs, all of which are visual. The runs use a single visual feature, only and range towards the lower end of the performance spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.13">MIXED</head><p>The combination of runs from RWTH, OHSU, MedGIFT resulted in 13 submissions, all of which were automatic and all used visual and textual information. The combinations were linear and surprisingly the results are significantly worse than the results of single techniques of the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Results</head><p>For the first time in 2007, the best overall system used only text for the retrieval. Up until now the best systems always used a mix of visual and textual information. Nothing can really be said on the outcome of manual and relevance feedback submissions as there were too few submitted runs.</p><p>It became clear that most research groups participating had a single specialty, usually either visual or textual retrieval. By supplying visual and textual results as example, we gave groups the possibility to work on multi-modal retrieval as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1">Automatic Retrieval</head><p>As always, the vast majority of results were automatic and without any interaction. There were 146 runs in this category, with 27 visual runs, 80 mixed runs and 39 textual submissions, making automatic mixed media runs the most popular category. The results shown in the following tables are averaged over all 30 topics, thus hiding much information about which technique performed well for what kind of tasks.</p><p>Visual Retrieval Purely visual retrieval was performed in 27 runs and by six groups. Results from GIFT and FIRE (Flexible Image Retrieval Engine) were made available for research groups not having access to a visual retrieval engine themselves.</p><p>To make the tables shorter and to not bias results shown towards groups with many submissions, only the best two and the worst two runs of every group are shown in the results tables of each category. Table <ref type="table" coords="7,195.65,642.07,4.98,9.96" target="#tab_1">2</ref> shows the results for the visual runs. Most runs had an extremely low MAP (&lt;3% MAP), which had been the case during the previous years as well. The overall results were lower than in preceding years, indiacting that tasks might have become harder. On the other hand, two runs had good results and rivaled, at least for early precision, the best textual results. These two runs actually used data from 2005 and 2006 that was somewhat similar to the tasks in 2007 to train the system for optimal feature selection. This showed that an optimised feature weighting may result in a large improvement! Table <ref type="table" coords="8,132.47,437.93,4.98,9.96" target="#tab_2">3</ref> shows the best and worst two results of every group for purely textual retrieval. The best overall runs were from LIG and were purely textual, which happened for the first time in ImageCLEF. (LIG participated in ImageCLEF this year for the first time. Early precision (P10) was only slightly better than the best purely visual runs and the best mixed runs had a very high early precision whereas the highest P10 was actually a purely textual system where the MAP was situated significantly lower. (Despite its name, MAP is more of a recall-oriented measure.) mixed retrieval Mixed automatic retrieval had the highest number of submissions of all categories. There were 80 runs submitted by 8 participating groups.</p><p>Table <ref type="table" coords="8,133.35,547.51,4.98,9.96" target="#tab_3">4</ref> summarises the best two and the worst two mixed runs of every group. For some groups the results for mixed runs were better than the best text runs but for others this was not the case. This underlines the fact that combinations between visual and textual features have to be done with care. Another interesting fact is that some systems with only a mediocre MAP performed extremely well with respect to early precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Manual and Interactive retrieval</head><p>Only three runs in 2007 were in the manual or interactive sections, making any real comparison impossible. Table <ref type="table" coords="8,169.81,653.56,4.98,9.96" target="#tab_4">5</ref> lists these runs and their performance Although information retrieval with relevance feedback or manual query modifications are seen as a very important area to improve retrieval performance, research groups in ImageCLEF 2007 did not make use of these categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Conclusions</head><p>Visual retrieval without learning had very low results for MAP and even for early precision (although with a smaller difference from text retrieval). Visual topics still perform well using visual techniques. Extensive learning of feature selection and weighting can have enormous gain in performance as shown by the FIRE runs. Purely textual runs had the best overall results for the first time and text retrieval was shown to work well for most topics. Mixed-media runs were the most popular category and are often better in performance than text or visual features alone. Still, in many cases the mixed media runs did not perform as well as text alone, showing that care needs to be taken to combine media.</p><p>Interactive and manual queries were almost absent from the evaluation and this remains an important problem. ImageCLEFmed has to put these domains more into the focus of the researchers although this requires more resources to perform the evaluation. System-oriented evaluation is an important part but only interactive retrieval can show how well a system can really help the users.</p><p>With respect to performance measures, there was less correlation between the measures than in previous years. The runs with the beast early precision (P10) were not close in MAP to the best overall systems. This needs to be investigated as MAP is indeed a good indicator for overall system performance but early precision might be much more what real users are looking for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Medical Automatic Annotation Task</head><p>Over the last two years, automatic medical image annotation has been evolved from a simple classification task with about 60 classes to a task with about 120 classes. From the very start however, it was clear that the number of classes cannot be scaled indefinitely, and that the number of classes that are desirable to be recognised in medical applications is far to big to assemble sufficient training data to create suitable classifiers. To address this issue, a hierarchical class structure such as the IRMA code <ref type="bibr" coords="11,240.88,435.57,10.52,9.96" target="#b8">[9]</ref> can be a solution which allows to create a set of classifiers for subproblems.</p><p>The classes in the last years were based on the IRMA code where created by grouping similar codes in one class. This year, the task has changed and the objective is to predict complete IRMA codes instead of simple classes. This year's medical automatic annotation task builds on top of last year: 1,000 new images were collected and are used as test data, the training and the test data of last year was used as training and development data respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database &amp; Task Description</head><p>The complete database consists of 12'000 fully classified medical radiographs taken randomly from medical routine at the RWTH Aachen University Hospital. 10'000 of these were release together with their classification as training data, another 1'000 were also published with their classification as validation data to allow for tuning classifiers in a standardised manner. One thousand additional images were released at a later date without classification as test data. These 1'000 images had to be classified using the 11'000 images (10'000 training + 1'000 validation) as training data.</p><p>Each of the 12'000 images is annotated with its complete IRMA code (see Sec. 3.1.1). In total, 116 different IRMA codes occur in the database, the codes are not uniformly distributed, but some codes have a significant larger share among the data than others. The least frequent codes however, are represented at least 10 times in the training data to allow for learning suitable models.</p><p>Example images from the database together with textual labels and their complete code are given in Figure <ref type="figure" coords="11,159.09,708.99,3.87,9.96" target="#fig_4">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">IRMA Code</head><p>Existing medical terminologies such as the MeSH thesaurus are poly-hierarchical, i.e., a code entity can be reached over several paths. However, in the field of content-based image retrieval, we frequently find class-subclass relations. The mono-hierarchical multi-axial IRMA code strictly relies on such part-of hierarchies and, therefore, avoids ambiguities in textual classification <ref type="bibr" coords="12,499.72,501.06,9.96,9.96" target="#b8">[9]</ref>. In particular, the IRMA code is composed from four axes having three to four positions, each in {0, . . . 9, a, . . . z}, where "'0"' denotes "'not further specified"'. More precisely,</p><p>• the technical code (T) describes the imaging modality;</p><p>• the directional code (D) models body orientations;</p><p>• the anatomical code (A) refers to the body region examined; and</p><p>• the biological code (B) describes the biological system examined. This results in a string of 13 characters (IRMA: TTTT -DDD -AAA -BBB). For instance, the body region (anatomy, three code positions) is defined as follows: The IRMA code can be easily extended by introducing characters in a certain code position, e.g., if new imaging modalities are introduced. Based on the hierarchy, the more code position differ from "'0"', the more detailed is the description.</p><formula xml:id="formula_0" coords="12,90.00,657.03,15.69,9.20">AAA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Hierarchical Classification</head><p>To define a evaluation scheme for hierarchical classification, we can consider the 4 axes to be independent, such that we can consider the axes independently and just sum up the errors for each axis independently.</p><p>Hierarchical classification is a well-known topic in different field. For example the classification of documents often is done using a ontology based class hierarchy <ref type="bibr" coords="13,369.03,307.07,15.50,9.96" target="#b19">[20]</ref> and in information extraction similar techniques are applied <ref type="bibr" coords="13,223.38,319.02,14.62,9.96" target="#b10">[11]</ref>. In our case, however we developed a novel evaluation scheme to account for the particularities of the IRMA code which considers errors that are made early in a hierarchy to be worse than errors that are made at a very fine level, and it is explicitly possible to predict a code partially, i.e. to predict a code up to a certain position and put wild-cards for the remaining positions, which is penalised but only with half the penalty a misclassification is penalised.</p><p>Our evaluation scheme is described in the following, where we only consider one axis. The same scheme is applied to each axis individually.</p><p>Let l I 1 = l 1 , l 2 , . . . , l i , . . . , l I be the correct code (for one axis) of an image, i.e. if a classifier predicts this code for an image, the classification is perfect. Further, let lI 1 = l1 , l2 , . . . , li , . . . , lI be the predicted code (for one axis) of an image.</p><p>The correct code is specified completely: l i is specified for each position. The classifiers however, are allowed to specify codes only up to a certain level, and predict "don't know " (encoded by *) for the remaining levels of this axis.</p><p>Given an incorrect classification at position li we consider all succeeding decisions to be wrong and given a not specified position, we consider all succeeding decisions to be not specified.</p><p>We want to penalise wrong decisions that are easy (fewer possible choices at that node) over wrong decisions that are difficult (many possible choices at that node), we can say, a decision at position l i is correct by chance with a probability of 1  bi if b i is the number of possible labels for position i. This assumes equal priors for each class at each position.</p><p>Furthermore, we want to penalise wrong decisions at an early stage in the code (higher up in the hierarchy) over wrong decisions at a later stage in the code (lower down on the hierarchy) (i.e. l i is more important than l i+1 ).</p><p>Assembling the ideas from above in a straight forward way leads to the following equation:</p><formula xml:id="formula_1" coords="13,104.94,617.65,269.98,124.76">I i=1 1 b i (a) 1 i (b) δ(l i , li ) (c) with δ(l i , li ) =      0 if l j = lj ∀j ≤ i 0.5 if l j = * ∃j ≤ i 1 if l j = lj ∃j ≤ i</formula><p>where the parts of the equation account for  (c) correct/not specified/wrong, respectively.</p><p>In addition, for every code, the maximal possible error is calculated and the errors are normed such that a completely wrong decision (i.e. all positions wrong) gets an error count of 1.0 and a completely correctly classified image has an error of 0.0.</p><p>Table <ref type="table" coords="14,133.83,379.44,4.98,9.96" target="#tab_7">7</ref> shows examples for a correct code with different predicted codes. Predicting the completely correct code leads to an error measure of 0.0, predicting all positions incorrectly leads to an error measure of 1.0. The examples demonstrate that a classification error in a position at the back of the code results in a lower error measure than a position in one of the first positions. The last column of the table show the effect of the branching factor. In this column we assumed the branching factor of the code is 2 in each node of the hierarchy. It can be observed that the errors for the later positions have more weight compared to the real errors in the real hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Participating Groups &amp; Methods</head><p>In the medical automatic annotation task, 29 groups registered of which 10 groups participated, submitting a total of 68 runs. The group with the highest number of submissions had 30 runs in total.</p><p>In the following, groups are listed alphabetically and their methods are described shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">BIOMOD: University of Liege, Belgium</head><p>The Bioinformatics and Modelling group from the University Liege 16 in Belgium submitted four runs. The approach is based on an object recognition framework using extremely randomised trees and randomly extracted sub-windows <ref type="bibr" coords="14,259.15,601.50,14.61,9.96" target="#b9">[10]</ref>. The runs all use the same technique and differ how the code is assembled. One run predicts the full code, one run predicts each axis independently and the other two runs are combinations of the first ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">BLOOM: IDIAP, Switzerland</head><p>The Blanceflor-om2-toMed group from IDIAP in Martigny, Switzerland submitted 7 runs. All runs use support vector machines (either in one-against-one or one-against-the-rest manner). Features used are downscaled versions of the images, SIFT features extracted from sub-images, and combinations of these <ref type="bibr" coords="14,187.53,705.57,14.61,9.96" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Geneva: medGIFT Group, Switzerland</head><p>The medGIFT group <ref type="foot" coords="15,184.15,128.37,7.93,6.97" target="#foot_3">17</ref> from Geneva, Switzerland submitted 3 runs, each of the runs uses the GIFT image retrieval system. The runs differ in the way, the IRMA-codes of the top-ranked images are combined <ref type="bibr" coords="15,183.89,153.66,14.62,9.96" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">CYU: Information Management AI lab, Taiwan</head><p>The Information Management AI lab from the Ching Yun University of Jung-Li, Taiwan submitted one run using a nearest neighbour classifier using different global and local image features which are particularly robust with respect to lighting changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">MIRACLE: Madrid, Spain</head><p>The Miracle group from Madrid Spain<ref type="foot" coords="15,258.94,264.76,7.93,6.97" target="#foot_4">18</ref> submitted 30 runs. The classification was done using a 10-nearest neighbour classifier and the features used are gray-value histograms, Tamura texture features, global texture features, and Gabor features, which were extracted using FIRE. The runs differ which features were used, how the prediction was done (predicting the full code, axis-wise prediction, different subsets of axes jointly), and whether the features were normalised or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Oregon Health State University, Portland, OR, USA</head><p>The Department of Medical Informatics and Clinical Epidemiology<ref type="foot" coords="15,386.43,356.87,7.93,6.97" target="#foot_5">19</ref> of the Oregon Health and Science University in Portland, Oregon submitted two runs using neural networks and GIST descriptors. One of the runs uses a support vector machine as a second level classifier to help discriminating the two most difficult classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7">RWTHi6: RWTH Aachen University, Aachen, Germany</head><p>The Human Language Technology and Pattern Recognition group <ref type="foot" coords="15,381.59,437.01,7.93,6.97" target="#foot_6">20</ref> of the RWTH Aachen University in Aachen, Germany submitted 6 runs, all are based on sparse histograms of image patches which were obtained by extracting patches at each position in the image. The histograms have 65536 or 4096 bins <ref type="bibr" coords="15,174.98,474.25,9.96,9.96" target="#b4">[5]</ref>. The runs differ in the resolution of the images. One run is a combination of 4 normal runs, and one run does the classification axis-wise, the other runs, directly predict the full code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.8">IRMA: RWTH Aachen University, Medical Informatics, Aachen, Germany</head><p>The IRMA group from the RWTH Aachen University Hospital<ref type="foot" coords="15,364.37,541.08,7.93,6.97" target="#foot_7">21</ref> , in Aachen Germany submitted three baseline runs using weighted combinations of nearest neighbour classifiers using texture histograms, image cross correlations, and the image deformation model. The parameters used are exactly the same as used in previous years. The runs differ in the way in which the codes of the five nearest neighbours are used to assemble the final predicted code.</p><p>3.2.9 UFR: University of Freiburg, Computer Science Dep., Freiburg, Germany</p><p>The Pattern Recognition and Image Processing group from the University Freiburg<ref type="foot" coords="15,456.27,633.19,7.93,6.97" target="#foot_8">22</ref> , Germany, submitted four runs using relational features calculated around interest points which are later combined to form cluster cooccurrence matrices <ref type="bibr" coords="15,302.77,658.47,14.62,9.96" target="#b16">[17]</ref>. Three different classification methods were used: a flat classification scheme using all of the 116 classes , an axiswise-flat classification scheme (i.e. 4 multi-class classifiers), and a binary classification tree (BCT) based scheme. The BCT based approach is much faster to train and classify, but this comes at a slight performance penalty. The tree was generated as described in <ref type="bibr" coords="16,242.45,135.26,14.61,9.96" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.10">UNIBAS: University of Basel, Switzerland</head><p>The Databases and Information Systems group from the University Basel 23 , Switzerland submitted 14 runs using a pseudo two-dimensional hidden Markov model to model image deformation in the images which were scaled down keeping the aspect ratio such that the longer side has a length of 32 pixels <ref type="bibr" coords="16,131.13,215.41,14.62,9.96" target="#b18">[19]</ref>. The runs differ in the features (pixels, Sobel features) that were used to determine the deformation and in the k-parameter for the k-nearest neighbour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results of the evaluation are given in Table <ref type="table" coords="16,297.19,273.64,3.87,9.96" target="#tab_7">7</ref>. For each run, the run-id, the score as described above and additionally, the error rate, which was used in the last years to evaluate the submissions to this task are given. The method which had the best result last year is now at rank 8, which gives an impression how much improvement in this field was achieved over the last year.</p><p>Looking at the results for individual images, we noted, that only one image was classified correctly by all submitted runs (top left image in Fig. <ref type="figure" coords="16,327.62,345.37,3.87,9.96" target="#fig_4">4</ref>). No image was misclassified by all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Analysing the results, it can be observed that the top-performing runs do not consider the hierarchical structure of the given task, but rather use each individual code as one class and train a 116 classes classifier. This approach seems to work better given the currently limited amount of codes, but obviously would not scale up infinitely and would probably lead to a very high demand for appropriate training data if a much larger amount of classes is to be distinguished. The best run using the code is on rank 6, builds on top of the other runs from the same group and uses the hierarchy only in a second stage to combine the four runs.</p><p>Furthermore, it can be seen that a method that is applied once accounting for the hierarchy/axis structure of the code and once using the straight forward classification into 116 classes approach, the one which does not know about the hierarchy clearly outperforms the other one (runs on ranks 11 and 13/7 and 14,16).</p><p>Another clear observation is that methods using local image descriptors outperform methods using global image descriptors. In particular, the top 16 runs are all using either local image features alone or local image features in combination with a global descriptor.</p><p>It is also observed that images where a large amount of training data is available are more far more likely to be classified correctly.</p><p>Considering the ranking wrt. to the applied hierarchical measure and the ranking wrt. to the error rate it can clearly be seen that there are hardly any differences. Most of the differences are clearly due to use of the code (mostly inserting of wildcard characters) which can lead to an improvement for the hierarchical evaluation scheme, but will always lead to a deterioration wrt. to the error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conclusion</head><p>The success of the medical automatic annotation task could be continued, the number of participants is pretty constant, but a clear performance improvement of the best method could be observed. Although only few groups actively tried to exploit the hierarchical class structure many of the participants told us that they consider this an important research topic and that a further investigation is desired.  Our goal for future tasks is to motivate more groups to participate and to increase the database size such that it is necessary to use the hierarchical class structure actively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Overall Conclusions</head><p>The two medical tasks of ImageCLEF again attracted a very large number of registrations and participation. This underlines the importance of such evaluation campaigns giving researchers the opportunity to evaluate their systems without the tedious task of creating databases and topics. In domains such as medical retrieval this is particularly important as data access if often difficult.</p><p>In the medical retrieval task, visual retrieval without any learning only obtained good results for a small subset of topics. With learning this can change strongly and deliver even for purely visual retrieval fairly good results. Mixed-media retrieval was the most popular category and results were often better for mixed-media than textual runs of the same groups. This shows that mixed-media retrieval requires much work and more needs to be learned on such combinations. Interactive retrieval and manual query modification were only used in 3 out of the 149 submitted runs. This shows that research groups prefer submitting automatic runs , although interactive retrieval is important and still must be addressed by researchers.</p><p>For the annotation task, it was observed that techniques that rely heavily on recent developments in machine learning and build on modern image descriptors clearly outperform other methods. The class hierarchy that was provided could only lead to improvements for a few groups. Overall, the runs that use the class hierarchy perform worse than those which consider every unique code as a unique class which gives the impression that for the current number of 116 unique codes the training data is sufficient to train a joint classifier. As opposed to the retrieval task, none of the groups used any interaction although this might allow for a big performance gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,250.16,198.39,102.72,9.96;5,230.62,210.34,141.80,9.96;5,232.34,222.30,138.33,9.96"><head></head><label></label><figDesc>Lung xray tuberculosis. Röntgenbild Lunge Tuberkulose. Radio pulmonal de tuberculose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,220.25,244.21,162.52,9.96;5,244.49,265.61,111.48,85.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example for a mixed topic.</figDesc><graphic coords="5,244.49,265.61,111.48,85.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,214.41,400.93,174.19,9.96;5,162.30,265.61,78.84,85.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example for a semantic topic.</figDesc><graphic coords="5,162.30,265.61,78.84,85.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,176.22,197.94,74.66,9.96;12,128.98,217.62,154.08,4.98;12,128.98,223.59,160.92,4.98;12,128.98,229.57,108.40,4.98;12,128.98,235.55,152.60,4.98;12,352.06,197.94,74.66,9.96;12,304.82,217.62,154.08,4.98;12,304.82,223.59,160.91,4.98;12,304.82,229.57,106.80,4.98;12,304.82,235.55,152.60,4.98;12,176.22,335.34,74.66,9.96;12,128.98,355.01,154.08,4.98;12,128.98,360.99,146.74,4.98;12,128.98,366.97,110.99,4.98;12,128.98,372.94,136.61,4.98;12,352.06,331.70,74.66,9.96;12,304.82,351.38,161.55,4.98;12,304.82,357.35,122.82,4.98;12,304.82,363.33,99.41,4.98;12,304.82,369.31,117.26,4.98"><head></head><label></label><figDesc>1121-120-200-700 T: x-ray, plain radiography, analog, overview image D: coronal, anteroposterior (AP, coronal), unspecified A: cranium, unspecified, unspecified B: musculosceletal system, unspecified, unspecified 1121-120-310-700 T: x-ray, plain radiography, analog, overview image D: coronal, anteroposterior (AP, coronal), unspecified A: spine, cervical spine, unspecified B: musculosceletal system, unspecified, unspecified 1121-127-700-500 T: x-ray, plain radiography, analog, overview image D: coronal, anteroposterior (AP, coronal), supine A: abdomen, unspecified, unspecified B: uropoietic system, unspecified, unspecified 1123-211-500-000 T: x-ray, plain radiography, analog, high beam energy D: sagittal, lateral, right-left, inspiration A: chest, unspecified, unspecified B: unspecified, unspecified, unspecified</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,90.00,403.03,422.99,9.96;12,90.00,414.98,64.59,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example images from the medical annotation task with full IRMA-code and its textual representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,90.00,283.80,319.27,9.96;14,90.00,303.72,287.91,9.96"><head></head><label></label><figDesc>(a) accounts for difficulty of the decision at position i (branching factor) (b) accounts for the level in the hierarchy (position in the string)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="18,90.00,330.58,423.01,9.96"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Code-wise relative error as a function of the frequency of this code in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,118.27,423.02,233.27"><head>Table 1 :</head><label>1</label><figDesc>The databases used in ImageCLEFmed 2007.</figDesc><table coords="3,138.61,129.97,325.77,153.83"><row><cell>Collection Name</cell><cell cols="4">Cases Images Annotations Annotations by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Language</cell></row><row><cell>Casimage</cell><cell>2076</cell><cell>8725</cell><cell>2076</cell><cell>French -1899,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>English -177</cell></row><row><cell>MIR</cell><cell>407</cell><cell>1177</cell><cell>407</cell><cell>English -407</cell></row><row><cell>PEIR</cell><cell cols="2">32319 32319</cell><cell>32319</cell><cell>English -32319</cell></row><row><cell>PathoPIC</cell><cell>7805</cell><cell>7805</cell><cell>15610</cell><cell>German -7805,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>English -7805</cell></row><row><cell>myPACS</cell><cell>3577</cell><cell>15140</cell><cell>3577</cell><cell>English -3577</cell></row><row><cell>Endoscopic</cell><cell>1496</cell><cell>1496</cell><cell>1496</cell><cell>English -1496</cell></row><row><cell>Total</cell><cell cols="2">47680 66662</cell><cell>55485</cell><cell>French -1899,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>English -45781,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>German -7805</cell></row></table><note coords="3,90.00,317.67,422.99,9.96;3,90.00,329.62,423.02,9.96;3,90.00,341.58,102.41,9.96"><p><p><p>collection (Pathology images</p><ref type="bibr" coords="3,220.76,317.67,10.79,9.96" target="#b5">[6]</ref></p>) was included into our dataset. It contains 9'000 images with extensive annotation on a per image basis in German. A short part of the German annotation is translated into English.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,118.83,423.02,317.11"><head>Table 2 :</head><label>2</label><figDesc>Automatic runs using only visual information (best and worst two runs of every group).</figDesc><table coords="8,101.38,131.07,400.24,249.07"><row><cell>Run</cell><cell cols="2">Relevant MAP R-prec</cell><cell>P10</cell><cell>P30</cell><cell>P100</cell></row><row><cell>RWTH-FIRE-ME-NT-tr0506</cell><cell>1613</cell><cell cols="4">0.2328 0.2701 0.4867 0.4333 0.2823</cell></row><row><cell>RWTH-FIRE-ME-NT-tr06</cell><cell>1601</cell><cell cols="4">0.2227 0.2630 0.4867 0.4256 0.2763</cell></row><row><cell>CINDI IMG FUSION</cell><cell>630</cell><cell cols="4">0.0333 0.0532 0.1267 0.1222 0.0777</cell></row><row><cell>RWTH-FIRE-NT-emp</cell><cell>584</cell><cell cols="4">0.0284 0.0511 0.1067 0.0856 0.0590</cell></row><row><cell>RWTH-FIRE-NT-emp2</cell><cell>562</cell><cell cols="4">0.0280 0.0493 0.1067 0.0811 0.0587</cell></row><row><cell>miracleVisG</cell><cell>532</cell><cell cols="4">0.0186 0.0396 0.0833 0.0833 0.0470</cell></row><row><cell>miracleVisGFANDmm</cell><cell>165</cell><cell cols="4">0.0102 0.0255 0.0667 0.0500 0.0347</cell></row><row><cell>miracleVisGFANDavg</cell><cell>165</cell><cell cols="4">0.0087 0.0214 0.0467 0.0556 0.0343</cell></row><row><cell>UNALCO-nni FeatComb</cell><cell>644</cell><cell cols="4">0.0082 0.0149 0.0200 0.0144 0.0143</cell></row><row><cell>miracleVisGFANDmin</cell><cell>165</cell><cell cols="4">0.0081 0.0225 0.0367 0.0478 0.0333</cell></row><row><cell>UNALCO-nni RGBHisto</cell><cell>530</cell><cell cols="4">0.0080 0.0186 0.0267 0.0156 0.0153</cell></row><row><cell>UNALCO-svmRBF RGBHisto</cell><cell>368</cell><cell cols="4">0.0050 0.0103 0.0133 0.0100 0.0093</cell></row><row><cell>UNALCO-svmRBF Tamura</cell><cell>375</cell><cell cols="4">0.0048 0.0109 0.0067 0.0100 0.0100</cell></row><row><cell>GE 4 8.treceval</cell><cell>292</cell><cell cols="4">0.0041 0.0192 0.0400 0.0322 0.0203</cell></row><row><cell>GE-GE GIFT8</cell><cell>292</cell><cell cols="4">0.0041 0.0194 0.0400 0.0322 0.0203</cell></row><row><cell>GE-GE GIFT4</cell><cell>290</cell><cell cols="4">0.0040 0.0192 0.0400 0.0322 0.0203</cell></row><row><cell>DEU CS-DEU R2</cell><cell>277</cell><cell cols="4">0.0028 0.0052 0.0067 0.0022 0.0033</cell></row><row><cell>DEU CS-DEU R3</cell><cell>260</cell><cell cols="4">0.0018 0.0053 0.0100 0.0056 0.0057</cell></row><row><cell>DEU CS-DEU R4</cell><cell>238</cell><cell cols="4">0.0018 0.0074 0.0033 0.0056 0.0057</cell></row><row><cell>DEU CS-DEU R5</cell><cell>249</cell><cell cols="4">0.0014 0.0062 0.0000 0.0078 0.0077</cell></row></table><note coords="8,90.00,414.02,423.02,9.96;8,90.00,425.98,31.64,9.96"><p>textual retrieval A total of 39 submissions were purely textual and came from nine research groups.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,90.00,245.45,423.02,368.92"><head>Table 3 :</head><label>3</label><figDesc>Automatic runs using only textual information (best and worst two runs of every group).</figDesc><table coords="9,101.38,257.70,400.25,356.66"><row><cell>Run</cell><cell cols="2">Relevant MAP R-prec</cell><cell>P10</cell><cell>P30</cell><cell>P100</cell></row><row><cell>LIG-MRIM-LIG MU A</cell><cell>2347</cell><cell cols="4">0.3962 0.4146 0.5067 0.4600 0.3593</cell></row><row><cell>LIG-MRIM-LIG GM A</cell><cell>2341</cell><cell cols="4">0.3947 0.4134 0.5000 0.4678 0.3617</cell></row><row><cell>LIG-MRIM-LIG GM L</cell><cell>2360</cell><cell cols="4">0.3733 0.3904 0.5200 0.4667 0.3330</cell></row><row><cell>SinaiC100T100</cell><cell>2449</cell><cell cols="4">0.3668 0.3942 0.5467 0.5044 0.3457</cell></row><row><cell>LIG-MRIM-LIG MU L</cell><cell>2363</cell><cell cols="4">0.3643 0.3784 0.5033 0.4422 0.3183</cell></row><row><cell>miracleTxtENN</cell><cell>2294</cell><cell cols="4">0.3518 0.3890 0.5800 0.4556 0.3600</cell></row><row><cell>SinaiC040T100</cell><cell>2401</cell><cell cols="4">0.3507 0.3737 0.5533 0.5122 0.3490</cell></row><row><cell>OHSU as out 1000rev1 c</cell><cell>2306</cell><cell cols="4">0.3453 0.3842 0.5300 0.4433 0.3033</cell></row><row><cell>OHSU-oshu as is 1000</cell><cell>2304</cell><cell cols="4">0.3453 0.3842 0.5300 0.4433 0.3033</cell></row><row><cell>SinaiC030T100</cell><cell>2345</cell><cell cols="4">0.3340 0.3433 0.5100 0.4889 0.3363</cell></row><row><cell>ohsu text e4 out rev1</cell><cell>1850</cell><cell cols="4">0.3321 0.3814 0.5867 0.4878 0.2893</cell></row><row><cell>UB-NLM-UBTextBL1</cell><cell>2244</cell><cell cols="4">0.3182 0.3306 0.5300 0.4756 0.3190</cell></row><row><cell>OHSU-OHSU txt exp2</cell><cell>1433</cell><cell cols="4">0.3135 0.3775 0.5867 0.4878 0.2893</cell></row><row><cell>IPAL-IPAL1 TXT BAY ISA0</cell><cell>1895</cell><cell cols="4">0.3057 0.3320 0.4767 0.4044 0.3163</cell></row><row><cell>IPAL-IPAL TXT BAY ALLREL2</cell><cell>1896</cell><cell cols="4">0.3042 0.3330 0.4633 0.4067 0.3127</cell></row><row><cell>IPAL-IPAL3 TXT BAY ISA0</cell><cell>1852</cell><cell cols="4">0.2996 0.3212 0.4733 0.3989 0.3140</cell></row><row><cell>miracleTxtXN</cell><cell>2252</cell><cell cols="4">0.2990 0.3540 0.4067 0.3756 0.2943</cell></row><row><cell>SinaiC020T100</cell><cell>2028</cell><cell cols="4">0.2950 0.3138 0.4400 0.4389 0.2980</cell></row><row><cell>IPAL-IPAL4 TXT BAY ISA0</cell><cell>1831</cell><cell cols="4">0.2935 0.3177 0.4733 0.3978 0.3073</cell></row><row><cell>GE EN</cell><cell>2170</cell><cell cols="4">0.2714 0.2989 0.3900 0.3356 0.2467</cell></row><row><cell>UB-NLM-UBTextBL2</cell><cell>2084</cell><cell cols="4">0.2629 0.2873 0.4033 0.3644 0.2543</cell></row><row><cell>GE MIX</cell><cell>2123</cell><cell cols="4">0.2416 0.2583 0.3500 0.3133 0.2243</cell></row><row><cell>DEU CS-DEU R1</cell><cell>891</cell><cell cols="4">0.1694 0.2191 0.3967 0.3622 0.2533</cell></row><row><cell>GE DE</cell><cell>1364</cell><cell cols="4">0.1631 0.1770 0.2200 0.1789 0.1333</cell></row><row><cell>GE FR</cell><cell>1306</cell><cell cols="4">0.1557 0.1781 0.1933 0.2067 0.1520</cell></row><row><cell>UB-NLM-UBTextFR</cell><cell>1503</cell><cell cols="4">0.1184 0.1336 0.2033 0.1767 0.1320</cell></row><row><cell>miracleTxtDET</cell><cell>694</cell><cell cols="4">0.0991 0.0991 0.2300 0.1222 0.0837</cell></row><row><cell>miracleTxtDEN</cell><cell>724</cell><cell cols="4">0.0932 0.1096 0.1800 0.1356 0.0970</cell></row><row><cell>IRIT RunMed1</cell><cell>1418</cell><cell cols="4">0.0660 0.0996 0.0833 0.1100 0.1023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,90.00,146.09,423.01,416.73"><head>Table 4 :</head><label>4</label><figDesc>Automatic runs using visual and textual information (best and worst two runs of every group).</figDesc><table coords="10,101.38,170.30,400.25,392.52"><row><cell>Run</cell><cell cols="2">Relevant MAP R-prec</cell><cell>P10</cell><cell>P30</cell><cell>P100</cell></row><row><cell>SinaiC100T80</cell><cell>2433</cell><cell cols="4">0.3719 0.4050 0.5667 0.5122 0.3517</cell></row><row><cell>SinaiC100T70</cell><cell>2405</cell><cell cols="4">0.3598 0.3925 0.5500 0.4878 0.3453</cell></row><row><cell>ohsu m2 rev1 c</cell><cell>2164</cell><cell cols="4">0.3461 0.3892 0.5567 0.4622 0.3287</cell></row><row><cell>UB-NLM-UBTI 1</cell><cell>2237</cell><cell cols="4">0.3230 0.3443 0.5167 0.4911 0.3317</cell></row><row><cell>UB-NLM-UBTI 3</cell><cell>2253</cell><cell cols="4">0.3228 0.3388 0.5367 0.4767 0.3270</cell></row><row><cell>RWTH-FIRE-ME-tr0506</cell><cell>1920</cell><cell cols="4">0.3044 0.3409 0.5267 0.4644 0.3410</cell></row><row><cell>RWTH-FIRE-ME-tr06</cell><cell>1916</cell><cell cols="4">0.3022 0.3370 0.5300 0.4611 0.3363</cell></row><row><cell>miracleMixGENTRIGHTmin</cell><cell>2002</cell><cell cols="4">0.2740 0.2876 0.4500 0.3822 0.2697</cell></row><row><cell>UB-NLM-UBmixedMulti2</cell><cell>2076</cell><cell cols="4">0.2734 0.2995 0.4167 0.3767 0.2693</cell></row><row><cell>RWTH-FIRE-emp2</cell><cell>1813</cell><cell cols="4">0.2537 0.3085 0.4533 0.4467 0.3017</cell></row><row><cell>miracleMixGENTRIGHTmax</cell><cell>2045</cell><cell cols="4">0.2502 0.2821 0.3767 0.3500 0.2900</cell></row><row><cell>miracleMixGENTRIGHTmm</cell><cell>2045</cell><cell cols="4">0.2486 0.2817 0.3733 0.3578 0.2890</cell></row><row><cell>RWTH-FIRE-emp</cell><cell>1809</cell><cell cols="4">0.2457 0.3123 0.4567 0.4467 0.3020</cell></row><row><cell>GE VT1 4</cell><cell>2123</cell><cell cols="4">0.2425 0.2596 0.3533 0.3133 0.2253</cell></row><row><cell>GE VT1 8</cell><cell>2123</cell><cell cols="4">0.2425 0.2596 0.3533 0.3133 0.2253</cell></row><row><cell>SinaiC030T50</cell><cell>2313</cell><cell cols="4">0.2371 0.2594 0.4600 0.3756 0.2700</cell></row><row><cell>SinaiC020T50</cell><cell>1973</cell><cell cols="4">0.2148 0.2500 0.4033 0.3422 0.2403</cell></row><row><cell>OHSU-ohsu m1</cell><cell>652</cell><cell cols="4">0.2117 0.2618 0.5200 0.4578 0.2173</cell></row><row><cell>GE VT10 4</cell><cell>1402</cell><cell cols="4">0.1938 0.2249 0.3600 0.3133 0.2160</cell></row><row><cell>GE VT10 8</cell><cell>1407</cell><cell cols="4">0.1937 0.2247 0.3600 0.3133 0.2157</cell></row><row><cell>CINDI TXT IMAGE LINEAR</cell><cell>1053</cell><cell cols="4">0.1659 0.2196 0.3867 0.3300 0.2270</cell></row><row><cell>miracleMixGFANDminENTORmm</cell><cell>1972</cell><cell cols="4">0.1427 0.1439 0.2200 0.2000 0.1793</cell></row><row><cell cols="2">miracleMixGFANDminENTORmax 1972</cell><cell cols="4">0.1419 0.1424 0.2067 0.1911 0.1770</cell></row><row><cell>UB-NLM-UBmixedFR</cell><cell>1308</cell><cell cols="4">0.1201 0.1607 0.2100 0.2022 0.1567</cell></row><row><cell>OHSU-oshu c e f q</cell><cell>598</cell><cell cols="4">0.1129 0.1307 0.2000 0.1544 0.0837</cell></row><row><cell>ohsu fire ef wt2 rev1 c</cell><cell>542</cell><cell cols="4">0.0586 0.0914 0.2000 0.1211 0.0760</cell></row><row><cell>3fire-7ohsu</cell><cell>2222</cell><cell cols="4">0.0344 0.0164 0.0100 0.0078 0.0113</cell></row><row><cell>3gift-3fire-4ohsu</cell><cell>2070</cell><cell cols="4">0.0334 0.0235 0.0067 0.0111 0.0137</cell></row><row><cell>5gift-5ohsu</cell><cell>1627</cell><cell cols="4">0.0188 0.0075 0.0033 0.0044 0.0070</cell></row><row><cell>7gift-3ohsu</cell><cell>1629</cell><cell cols="4">0.0181 0.0060 0.0033 0.0044 0.0073</cell></row><row><cell cols="2">miracleMixGFANDminENTLEFTmm 165</cell><cell cols="4">0.0099 0.0240 0.0533 0.0544 0.0363</cell></row><row><cell cols="2">miracleMixGFANDminENTLEFTmax 165</cell><cell cols="4">0.0081 0.0225 0.0367 0.0478 0.0333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,95.98,646.18,440.79,67.55"><head>Table 5 :</head><label>5</label><figDesc>The only three runs not using automatic retrieval.</figDesc><table coords="10,95.98,667.90,440.79,45.84"><row><cell>Run</cell><cell cols="6">Relevant MAP R-prec P10 P30 P100 media</cell><cell>interaction</cell></row><row><cell>CINDI TXT IMG RF LIN</cell><cell>860</cell><cell>0.08</cell><cell>0.12</cell><cell cols="3">0.38 0.27 0.14 mixed</cell><cell>feedback</cell></row><row><cell>CINDI IMG FUSION RF</cell><cell>690</cell><cell>0.04</cell><cell>0.05</cell><cell cols="3">0.14 0.13 0.08 visual</cell><cell>feedback</cell></row><row><cell>OHSU-oshu man2</cell><cell>2245</cell><cell>0.34</cell><cell>0.37</cell><cell>0.54 0.44</cell><cell>0.3</cell><cell cols="2">textual manual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,90.00,118.27,422.99,141.61"><head>Table 6 :</head><label>6</label><figDesc>Example scores for hierarchical classification, based on the correct code IRMA TTTT = 318a and assuming the branching factor would be 2 in each node of the hie</figDesc><table coords="14,195.79,141.93,211.43,117.95"><row><cell cols="3">classified error measure error measure (b=2)</cell></row><row><cell>318a</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>318*</cell><cell>0.024</cell><cell>0.060</cell></row><row><cell>3187</cell><cell>0.049</cell><cell>0.120</cell></row><row><cell>31*a</cell><cell>0.082</cell><cell>0.140</cell></row><row><cell>31**</cell><cell>0.082</cell><cell>0.140</cell></row><row><cell>3177</cell><cell>0.165</cell><cell>0.280</cell></row><row><cell>3***</cell><cell>0.343</cell><cell>0.260</cell></row><row><cell>32**</cell><cell>0.687</cell><cell>0.520</cell></row><row><cell>1000</cell><cell>1.000</cell><cell>1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,90.00,166.05,423.01,536.98"><head>Table 7 :</head><label>7</label><figDesc>Results of the medical image annotation task. Score is the hierarchical evaluation score, and ER is the error rate in % that was used last year to evaluate the annotation results.</figDesc><table coords="17,169.40,190.16,264.20,512.87"><row><cell>rank run id</cell><cell>score</cell><cell>ER</cell></row><row><cell>1 BLOOM-BLOOM_MCK_oa</cell><cell cols="2">26.8 10.3</cell></row><row><cell>2 BLOOM-BLOOM_MCK_oo</cell><cell cols="2">27.5 11.0</cell></row><row><cell>3 BLOOM-BLOOM_SIFT_oo</cell><cell cols="2">28.7 11.6</cell></row><row><cell>4 BLOOM-BLOOM_SIFT_oa</cell><cell cols="2">29.5 11.5</cell></row><row><cell>5 BLOOM-BLOOM_DAS</cell><cell cols="2">29.9 11.1</cell></row><row><cell>6 RWTHi6-4RUN-MV3</cell><cell cols="2">30.9 13.2</cell></row><row><cell>7 UFR-UFR_cooc_flat</cell><cell cols="2">31.4 12.1</cell></row><row><cell>8 RWTHi6-SH65536-SC025-ME</cell><cell cols="2">33.0 11.9</cell></row><row><cell>9 UFR-UFR_cooc_flat2</cell><cell cols="2">33.2 13.1</cell></row><row><cell>10 RWTHi6-SH65536-SC05-ME</cell><cell cols="2">33.2 12.3</cell></row><row><cell>11 RWTHi6-SH4096-SC025-ME</cell><cell cols="2">34.6 12.7</cell></row><row><cell>12 RWTHi6-SH4096-SC05-ME</cell><cell cols="2">34.7 12.4</cell></row><row><cell>13 RWTHi6-SH4096-SC025-AXISWISE</cell><cell cols="2">44.6 17.8</cell></row><row><cell>14 UFR-UFR_cooc_codewise</cell><cell cols="2">45.5 17.9</cell></row><row><cell>15 UFR-UFR_cooc_tree2</cell><cell cols="2">47.9 16.9</cell></row><row><cell>16 UFR-UFR_cooc_tree</cell><cell cols="2">48.4 16.8</cell></row><row><cell>17 rwth_mi_k1_tn9.187879e-05_common.run</cell><cell cols="2">51.3 20.0</cell></row><row><cell>18 rwth_mi_k5_majority.run</cell><cell cols="2">52.5 18.0</cell></row><row><cell>19 UNIBAS-DBIS-IDM_HMM_W3_H3_C</cell><cell cols="2">58.1 22.4</cell></row><row><cell>20 UNIBAS-DBIS-IDM_HMM2_4812_K3</cell><cell cols="2">59.8 20.2</cell></row><row><cell>21 UNIBAS-DBIS-IDM_HMM2_4812_K3_C</cell><cell cols="2">60.7 23.2</cell></row><row><cell>22 UNIBAS-DBIS-IDM_HMM2_4812_K5_C</cell><cell cols="2">61.4 23.1</cell></row><row><cell>23 UNIBAS-DBIS-IDM_HMM2_369_K3_C</cell><cell cols="2">62.8 22.5</cell></row><row><cell>24 UNIBAS-DBIS-IDM_HMM2_369_K3</cell><cell cols="2">63.4 21.5</cell></row><row><cell>25 UNIBAS-DBIS-IDM_HMM2_369_K5_C</cell><cell cols="2">65.1 22.9</cell></row><row><cell>26 OHSU-OHSU_2</cell><cell cols="2">67.8 22.7</cell></row><row><cell>27 OHSU-gist_pca</cell><cell cols="2">68.0 22.7</cell></row><row><cell>28 BLOOM-BLOOM_PIXEL_oa</cell><cell cols="2">68.2 20.1</cell></row><row><cell>29 BLOOM-BLOOM_PIXEL_oo</cell><cell cols="2">72.4 20.8</cell></row><row><cell>30 BIOMOD-full</cell><cell cols="2">73.8 22.9</cell></row><row><cell>31 BIOMOD-correction</cell><cell cols="2">75.8 25.3</cell></row><row><cell>32 BIOMOD-safe</cell><cell cols="2">78.7 36.0</cell></row><row><cell>33 im.cyu.tw-cyu_w1i6t8</cell><cell cols="2">79.3 25.3</cell></row><row><cell>34 rwth_mi_k5_common.run</cell><cell cols="2">80.5 45.9</cell></row><row><cell>35 BIOMOD-independant</cell><cell cols="2">95.3 32.9</cell></row><row><cell>36 miracle-miracleAAn</cell><cell cols="2">158.8 50.3</cell></row><row><cell>37 miracle-miracleVAn</cell><cell cols="2">159.5 49.6</cell></row><row><cell>38 miracle-miracleAATDABn</cell><cell cols="2">160.2 49.9</cell></row><row><cell>39 miracle-miracleAATABDn</cell><cell cols="2">162.2 50.1</cell></row><row><cell>40-62 runs from miracle group</cell><cell>-</cell><cell></cell></row><row><cell>63 GE-GE_GIFT10_0.5ve</cell><cell cols="2">375.7 99.7</cell></row><row><cell>64 GE-GE_GIFT10_0.15vs</cell><cell cols="2">390.3 99.3</cell></row><row><cell>65 GE-GE_GIFT10_0.66vd</cell><cell cols="2">391.0 99.0</cell></row><row><cell>66 miracle-miracleVATDAB</cell><cell cols="2">419.7 84.4</cell></row><row><cell>67 miracle-miracleVn</cell><cell cols="2">490.7 82.6</cell></row><row><cell>68 miracle-miracleV</cell><cell cols="2">505.6 86.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_0" coords="3,105.25,726.17,93.02,7.35"><p>http://www.mypacs.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_1" coords="3,105.25,735.67,80.34,7.35"><p>http://www.cori.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_2" coords="4,105.25,746.91,93.02,7.35"><p>http://www.pubmed.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_3" coords="15,105.25,689.93,135.30,7.35"><p>http://www.sim.hcuge.ch/medgift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_4" coords="15,105.25,699.44,198.72,7.35"><p>http://www.mat.upm.es/miracle/introduction.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_5" coords="15,105.25,708.94,109.93,7.35"><p>http://www.ohsu.edu/dmice/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_6" coords="15,105.25,718.45,165.89,7.35"><p>http://www-i6.informatik.rwth-aachen.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_7" coords="15,105.25,727.95,114.65,7.35"><p>http://www.irma-project.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_8" coords="15,105.25,737.45,161.16,7.35"><p>http://lmb.informatik.uni-freiburg.de/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. We would like to thank all the organizations who provided images and annotations for this year's task, including myPACS.net (<rs type="person">Rex Jakobovits</rs>) and the <rs type="institution">OHSU CORI</rs> project (<rs type="person">Judith Logan</rs>).</p><p>This work was partially funded by the <rs type="funder">DFG (Deutsche Forschungsgemeinschaft)</rs> under contracts <rs type="grantNumber">Ne-572/6</rs> and <rs type="grantNumber">Le-1108/4</rs>, the <rs type="funder">Swiss National Science Foundation (FNS)</rs> under contract <rs type="grantNumber">205321-109304/1</rs>, the <rs type="funder">American National Science Foundation (NSF)</rs> with grant <rs type="grantNumber">ITR-0325160</rs>, and the <rs type="programName">EU Sixth Framework Program</rs> with the <rs type="projectName">SemanticMining</rs> project (<rs type="grantNumber">IST NoE 507505</rs>) and the <rs type="funder">MUSCLE NoE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bX6XR9F">
					<idno type="grant-number">Ne-572/6</idno>
				</org>
				<org type="funding" xml:id="_KMNpGyP">
					<idno type="grant-number">Le-1108/4</idno>
				</org>
				<org type="funding" xml:id="_ym5tDmQ">
					<idno type="grant-number">205321-109304/1</idno>
				</org>
				<org type="funded-project" xml:id="_7kQRfRM">
					<idno type="grant-number">ITR-0325160</idno>
					<orgName type="project" subtype="full">SemanticMining</orgName>
					<orgName type="program" subtype="full">EU Sixth Framework Program</orgName>
				</org>
				<org type="funding" xml:id="_FGVurdA">
					<idno type="grant-number">IST NoE 507505</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="19,110.48,213.94,402.52,9.96;19,110.47,225.90,339.42,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,427.54,213.94,85.46,9.96;19,110.47,225.90,151.88,9.96">Introducing HEAL: The health education assets library</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><forename type="middle">S</forename><surname>Candler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><forename type="middle">H</forename><surname>Uijtdehaage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharon</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,270.65,225.90,82.18,9.96">Academic Medicine</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="253" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,245.82,402.50,9.96;19,110.48,257.78,402.53,9.96;19,110.48,269.73,402.54,9.96;19,110.48,281.69,402.51,9.96;19,110.48,293.64,127.37,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,346.00,245.82,166.98,9.96;19,110.48,257.78,60.50,9.96">The CLEF 2004 cross language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,147.05,269.73,365.96,9.96;19,110.48,281.69,123.01,9.96">Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<title level="s" coord="19,313.33,281.69,189.10,9.96">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,313.57,402.51,9.96;19,110.48,325.52,402.55,9.96;19,110.48,337.48,402.54,9.96;19,110.48,349.43,402.52,9.96;19,110.48,361.38,252.54,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,345.08,313.57,167.91,9.96;19,110.48,325.52,179.95,9.96">Overview of the CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,372.40,337.48,140.62,9.96;19,110.48,349.43,322.80,9.96">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="19,441.15,349.43,71.85,9.96;19,110.48,361.38,77.10,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,381.31,402.51,9.96;19,110.48,393.27,402.54,9.96;19,110.48,405.22,22.68,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,326.18,381.31,186.80,9.96;19,110.48,393.27,54.80,9.96">Overview of the ImageCLEF 2007 object retrieval task</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,183.88,393.27,186.80,9.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,425.15,402.52,9.96;19,110.48,437.10,402.49,9.96;19,110.48,449.06,402.49,9.96;19,110.48,461.01,149.73,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,450.45,425.15,62.54,9.96;19,110.48,437.10,232.62,9.96">Sparse patchhistograms for object classification in cluttered images</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andre</forename><surname>Hegerath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,363.33,437.10,149.63,9.96;19,110.48,449.06,102.81,9.96">DAGM 2006, Pattern Recognition, 26th DAGM Symposium</title>
		<title level="s" coord="19,289.76,449.06,151.18,9.96">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
			<biblScope unit="volume">4174</biblScope>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,480.94,402.52,9.96;19,110.48,492.89,402.49,9.96;19,110.48,504.85,22.68,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,432.44,480.94,80.56,9.96;19,110.48,492.89,297.44,9.96">Webbasierte Lernwerkzeuge für die Pathologie -web-based learning tools for pathology</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Glatz-Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glatz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mihatsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,415.53,492.89,39.97,9.96">Pathologe</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="394" to="399" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,524.77,402.52,9.96;19,110.48,536.73,402.54,9.96;19,110.48,548.68,162.15,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="19,440.86,524.77,72.14,9.96;19,110.48,536.73,191.99,9.96">Overview of the ImageCLEF 2007 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,321.07,536.73,186.84,9.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,568.61,402.53,9.96;19,110.48,580.56,402.54,9.96;19,110.48,592.52,282.34,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,110.48,580.56,303.11,9.96">Imageclefmed: A text collection to advance biomedical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffery</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,421.34,580.56,91.68,9.96;19,110.48,592.52,159.74,9.96">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2006-10">September/October, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,612.45,402.58,9.96;19,110.48,624.40,402.51,9.96;19,110.48,636.35,116.83,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,141.93,624.40,261.27,9.96">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertold</forename><forename type="middle">B</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,427.48,624.40,45.70,9.96">SPIE 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5033</biblScope>
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,656.28,402.50,9.96;19,110.48,668.24,402.51,9.96;19,110.48,680.19,402.53,9.96;19,110.48,692.14,300.30,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,420.28,656.28,92.70,9.96;19,110.48,668.24,130.52,9.96">Random subwindows for robust image classification</title>
		<author>
			<persName coords=""><forename type="first">Raphaël</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justus</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,137.34,680.19,375.66,9.96;19,110.48,692.14,49.75,9.96">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</editor>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005-06">2005. June 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,110.48,712.07,402.50,9.96;19,110.48,724.03,402.55,9.96;19,110.48,735.98,43.99,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,330.98,712.07,181.99,9.96;19,110.48,724.03,97.40,9.96">Metrics for evaluation of ontology-based information extraction</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wim</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaoyong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,232.37,724.03,223.13,9.96">Evaluation of Ontologies for the Web (EON 2006)</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,111.36,402.52,9.96;20,110.48,123.31,402.53,9.96;20,110.48,135.26,145.55,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,110.48,123.31,319.27,9.96">Overview of the imageclefmed 2006 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,449.50,123.31,63.51,9.96;20,110.48,135.26,21.19,9.96">CLEF working notes</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,155.19,402.54,9.96;20,110.48,167.15,402.54,9.96;20,110.48,179.10,215.09,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,110.48,167.15,331.03,9.96">A reference data set for the evaluation of medical image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Paul</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francois</forename><surname>Terrier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,453.81,167.15,59.20,9.96;20,110.48,179.10,130.83,9.96">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,199.02,402.53,9.96;20,110.48,210.98,402.53,9.96;20,110.48,222.94,163.77,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,177.64,210.98,291.89,9.96">Casimage project -a digital teaching files authoring environment</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Dfouni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Paul</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Osman</forename><surname>Ratib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,479.89,210.98,33.12,9.96;20,110.48,222.94,86.55,9.96">Journal of Thoracic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,242.86,402.50,9.96;20,110.47,254.82,402.55,9.96;20,110.47,266.77,52.43,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="20,184.04,242.86,156.76,9.96">Report on CLEF-2001 experiments</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,367.27,242.86,145.71,9.96;20,110.47,254.82,178.08,9.96">Report on the CLEF Conference 2001 (Cross Language Evaluation Forum)</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2406</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,286.69,402.53,9.96;20,110.47,298.65,365.12,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,267.51,286.69,200.16,9.96">Learning taxonomies in large image databases</title>
		<author>
			<persName coords=""><forename type="first">Lokesh</forename><surname>Setia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,489.79,286.69,23.22,9.96;20,110.47,298.65,238.02,9.96">ACM SIGIR Workshop on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>Amsterdam, Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,318.57,402.49,9.96;20,110.47,330.53,402.53,9.96;20,110.47,342.48,402.54,9.96;20,110.47,354.45,49.25,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="20,428.15,318.57,84.82,9.96;20,110.47,330.53,277.50,9.96">Image classification using cluster-cooccurrence matrices of local relational features</title>
		<author>
			<persName coords=""><forename type="first">Lokesh</forename><surname>Setia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Teynor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alaa</forename><surname>Halawani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,414.30,330.53,98.70,9.96;20,110.47,342.48,302.50,9.96">Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting>the 8th ACM International Workshop on Multimedia Information Retrieval<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,374.36,402.52,9.96;20,110.47,386.32,402.51,9.96;20,110.47,398.28,402.54,9.96;20,110.47,410.24,22.68,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,312.61,374.36,200.38,9.96;20,110.47,386.32,173.15,9.96">TRECVID: Evaluating the effectiveness of information retrieval tasks on digital video</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,303.07,386.32,209.92,9.96;20,110.47,398.28,168.07,9.96">Proceedings of the international ACM conference on Multimedia 2004 (ACM MM 2004)</title>
		<meeting>the international ACM conference on Multimedia 2004 (ACM MM 2004)<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
			<biblScope unit="page" from="652" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,430.15,402.52,9.96;20,110.47,442.11,265.56,9.96" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Springmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Dander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heiko</forename><forename type="middle">T B</forename><surname>Schuldt</surname></persName>
		</author>
		<title level="m" coord="20,418.60,430.15,94.40,9.96;20,110.47,442.11,94.99,9.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,462.04,402.52,9.96;20,110.47,473.99,402.53,9.96;20,110.47,485.94,69.75,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,246.73,462.04,200.23,9.96">Hierarchical text classification and evaluation</title>
		<author>
			<persName coords=""><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,471.52,462.04,41.48,9.96;20,110.47,473.99,235.85,9.96">IEEE International Conference on Data Mining (ICDM 2001)</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,505.87,402.50,9.96;20,110.47,517.83,402.53,9.96;20,110.47,529.78,187.59,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,383.24,505.87,129.74,9.96;20,110.47,517.83,206.14,9.96">CLEF2007 Image Annotation Task: an SVM-based Cue Integration Approach</title>
		<author>
			<persName coords=""><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,337.19,517.83,175.81,9.96;20,110.47,529.78,17.70,9.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,549.71,402.53,9.96;20,110.47,561.66,392.68,9.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="20,458.65,549.71,54.36,9.96;20,110.47,561.66,155.84,9.96">An internetbased nuclear medicine teaching file</title>
		<author>
			<persName coords=""><forename type="first">Jerold</forename><forename type="middle">W</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michelle</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Vreeland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,273.82,561.66,122.30,9.96">Journal of Nuclear Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1520" to="1527" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,110.48,581.59,402.53,9.96;20,110.47,593.55,402.53,9.96;20,110.47,605.50,114.91,9.96" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,391.87,581.59,121.14,9.96;20,110.47,593.55,125.12,9.96">University and Hospitals of Geneva at ImageCLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,261.37,593.55,197.94,9.96">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
