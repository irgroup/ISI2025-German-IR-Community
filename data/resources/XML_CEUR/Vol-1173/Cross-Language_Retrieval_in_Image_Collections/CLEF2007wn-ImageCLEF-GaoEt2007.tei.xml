<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.72,98.98,205.68,15.20;1,125.16,120.94,353.04,15.20">IPAL at ImageClef 2007 Mixing Features, Models and Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,106.68,154.68,44.76,8.85"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.83,154.68,92.03,8.85"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.56,154.68,46.84,8.85"><forename type="first">Thi</forename><surname>Hoang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.58,154.68,69.08,8.85"><roleName>Trong</roleName><forename type="first">Diem</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,381.96,154.68,42.89,8.85"><forename type="first">Ton</forename><surname>Pham</surname></persName>
							<email>ttpham@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.77,154.68,62.60,8.85"><forename type="first">Joo</forename><forename type="middle">Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IPAL French-Singaporean Joint Lab Institute for Infocomm Research (I2R</orgName>
								<orgName type="institution">Centre National de la Recherche Scientifique (CNRS)</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace -Singapore</addrLine>
									<postCode>119613</postCode>
									<settlement>gaosheng, viscjp</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.72,98.98,205.68,15.20;1,125.16,120.94,353.04,15.20">IPAL at ImageClef 2007 Mixing Features, Models and Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEF57285D06EEB49B1FAED2F5F1817FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval-Query Formulation Algorithms</term>
					<term>Measurement</term>
					<term>Performance</term>
					<term>Experimentation Language model</term>
					<term>latent semantic indexing</term>
					<term>information retrieval</term>
					<term>multimodality fusion</term>
					<term>contentbased image retrieval</term>
					<term>text based image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents IPAL ad-hoc photographic retrieval and medical image retrieval results in the ImageClef 2007 campaign. For the photo task, IPAL group is ranked at the 3rd place among 20 participants. The MAP of our best run is 0.2833, which is ranked at the 6th place among the 476 runs. The IPAL system is based on the mixed modality search, i.e. textual and visual modalities. Compare with our results in 2006, our results are significantly enhanced by extracting multiple low-level visual content descriptors and fusing multiple CBIR. Several text based image search (TBIR) engines are also developed such as the language model (LM) approach, the latent semantic indexing (LSI) approach. We also have used external knowledge like Wordnet, and Wikipedia for document expansion. Then the cross-modality pseudo-relevance feedback is applied to boost each individual modality. Linear fusion is used to combine different ranking lists. Combining the CBIR and TBIR outperforms the individual modality search. On medical side, our run ranks 19 among 111. We continue to use a conceptual indexing with vector space weighting, but we add this year a Bayesian network on concepts extracted from UMLS meta-thesaurus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year IPAL group continues to participate both in the ad-hoc photographic retrieval task and the medical task. The photographic retrieval task is different from the task of last year in that this year 1) the notes in the image document are not used while last year they can be used, 2) only text keywords in the title field are used for the query while last year all keywords in all fields could be used, 3) query image samples are excluded from the image database, and 4) 244 new images are added. Thus there is much less text information available, which obviously decreases discrimination, especially for the query. That is the reason why we have tried this year to expend documents using external information source (Wikipedia). After removing stop words, the average query length is about 3.3 text keywords with the maximal 6 keywords and the minimal 2 keywords. The less text keywords also make it impossible to discriminate some image documents which have same text keywords; however, their real visual contents are much different. Some image documents in the database have very similar text decryptions. Thus, only text information cannot distinguish the two images. However, when we measure their visual content, it is easy to discriminate one from another. For example in the figure <ref type="figure" coords="2,482.14,328.32,3.90,8.85" target="#fig_0">1</ref>, both images have similar textual content: "Accommodation Huanchaco -Exterior View,Trujillo, Peru" except one word "interior/exterior". The left image ("exterior", image id=01/1311), is relevant with the query 1, i.e. "accommodation with swimming pool", while the image at right ("interior", image id=01/1310) is irrelevant. With some external knowledge one could deduce that an "interior view" has less chance to show a swimming pool. Unfortunately, this remains difficult to do in practice. Without being able to use in a efficient way, extra knowledge for the matching, this example clearly shows that the visual content plays the same important role and can complement text matching. Therefore, this year we pay much attention to improve our Content based Image Retrieval (CBIR) system. We also try to use external knowledge from Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Photographic Image Indexing</head><p>We build various CBIR and Text Based Information Retrieval (TBIR) systems using different indexing methods and similarity functions. Totally we submit 27 runs. In the following, the details are given. To enrich the visual content representation, 4 types of low-level visual feature are extracted from the local regions or global images. They are detailed in the following: COR: Auto color correlogram with the depth 2 in the HSV space. It is extracted from the whole image and is represented by one 324-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HSV:</head><p>Histogram in HSV and gray-level space with 162-dimension for HSV plus 4-dimension for gray-level. An image is represented by a 166-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDGE:</head><p>We used canny operator to detect the contours object. A vector of 80-dimensional is used to capture magnitudes and gradient of the contours.</p><p>GABOR: texture feature using Gabor filter in the uniformly segmented 5x5 grids at the 2-scale and 12-orientations. Thus, the mean and variance are calculated at each grid to generate 48-dimensional feature vector.</p><p>SIFT: the SIFT feature is extracted using David Lowe' tool and the region around the keypoint is described by a 128-dimensional appearance feature <ref type="bibr" coords="2,350.20,695.64,9.91,8.85" target="#b7">[8]</ref>.</p><p>As the above low-level features are obtained, we now begin to discus how to index the image and calculate the similarity score. For the first two visual features extracted from the image, two methods are developed to index the image. tf-idf: the tf-idf method is a popular way to describe the text document <ref type="bibr" coords="3,417.49,104.88,9.95,8.85" target="#b1">[2]</ref>. Here we use it to process the histogram. We treat each bin in the histogram as a distinctive word. Then we can calculate the occurrence number of the word occurred in the image, which is the frequency in the bin, and the number of the images contained the word in the database. Now the histogram feature is further processed using the tf-idf method to index the image. The ranking function is the cosine function.</p><p>SVD: use Single Value Decomposition (SVD) to remove the correlation among the feature components. Assuming the full rank is N, we empirically select the top N*0.8 eigenvectors and index the image in the eigen-space. The cosine function is used to calculate similarity score.</p><p>Bag-of-visterm: images are divided into 16 equal patches. Then, image features are extracted for each patch. Patches are clustered into 1000 cluster using their feature vectors. Finnally, we quantise each image by a discret vector. Image retrieval will be performed by normalizing and matching using their vector signatures.</p><p>ISM: the ranking function is learned from the training sample using the supervised learning algorithms. Here it is the Integrated Statistic Model (ISM) based binary classifier <ref type="bibr" coords="3,477.56,292.68,10.00,8.85" target="#b3">[4]</ref>. The training samples include 3 query images as the positive class and another 3 negative samples randomly selected from the image database. Then the likelihood ratio between the positive model and the negative model is calculated for each image documents in the database and all images are ranked from the highest value to the lowest one.</p><p>Therefore, 3 individual runs are generated for each type of visual features. For the other two grid/patch based visual features, i.e. Gabor and SIFT, we use the following approaches to calculate the ranking score: HME: Use the supervised learning approach discussed above to train the hidden maximum entropy (HME) based model using the Gabor or SIFT feature and then use likelihood ratio for ranking images.</p><p>WORD SVD: the method is to index images at in word space rather than the image signal level (Gabor or SIFT space). To do so, we first find 200 most frequent keywords by analyzing the attached text documents in the image database. For each keyword, a HME classifier is trained using the corresponding training set. The training set for a word is constructed as follows: if the attached text for an image contains the word, then it is labelled as a positive sample. Otherwise, it is labelled as the negative. After 200 HME classifiers are trained, we will get all likelihood ratio scores for an image to index the image at the 200-dimensional keyword space. To further reduce the correlation among the feature components, SVD is used as discussed above. The similarity function is still the cosine distance.</p><p>WORD ISM: we can also train the ISM model for each query topic using the 200-dimensional feature in the keyword space. Then the likelihood ratio is used as the ranking score.</p><p>So there are also 3 runs for the Gabor or SIFT feature to be generated. Totally we have 12 visual runs. Table <ref type="table" coords="3,175.87,602.16,4.98,8.85" target="#tab_1">2</ref> summarizes the individual visual runs. Row are feature type, column are indexing method. A "+" means the indexing method is available for the feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pseudo-relevance Feedback</head><p>Learned from the ImageClef 2006 <ref type="bibr" coords="3,243.74,659.88,10.00,8.85" target="#b8">[9]</ref>, the cross-modality pseudo-relevance feedback (PRF) can improve the system performance, i.e. the TBIR can be boosted by the top-N (here 10 documents are selected) documents from the CBIR as the feedback and vice versa. This year PRF is also adopted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Indexing</head><p>We think the use of knowledge to assist the retrieval task is important, because, retrieval is clearly a human task linked to understanding. Knowledge can be defined as: information used to complete a task. In our case it is every piece of information that can be used to help the derivation of the document to the query. For example, the language used, the part of speech of words, and the grammar, are simple examples of knowledge that can be useful to use for text retrieval. The document itself can be viewed as a source of knowledge in different steps of the retrieval process:</p><p>• At indexing time: in the matching model. For example, the use of inverse document frequency (idf) is a simple way to use statistical global knowledge from a corpus. In a language model <ref type="bibr" coords="4,144.48,309.36,14.60,8.85" target="#b11">[12]</ref>, the collection is also used to smooth the language model of the document.</p><p>• At querying time: pseudo relevance feed back is a technique that uses content of top documents that are supposed to be relevant, to enlarge the query. In a sense, part of information of the corpus is used to help to interpret the query. Co-occurrence thesaurus is obtained from text-mining from the corpus. It can be used also to enlarge the query. The effect is similar to pseudo-feedback.</p><p>In this experiment, we have both used knowledge from document corpus and external knowledge (i.e. information that are not part of documents). The external source we use is Wikipedia. This choice is appropriate because Wikipedia includes a large amount of proper nouns, and specially named entities. In IAPR collection, named entities play a major role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using Language Model and Document Knowledge</head><p>We have built two individual runs using the language model (LM) based information retrieval <ref type="bibr" coords="4,497.53,479.16,15.54,8.85" target="#b11">[12]</ref> and latent semantic indexing (LSI) method <ref type="bibr" coords="4,285.17,491.04,24.38,8.85">[3] [5]</ref>. The attached text document for each image is provided in XML format. It is pre-processed as follows: 1) extract pure text, 2) remove the stop words and 3) do the stemming. For the LM-based TBIR, we first build a lexicon dictionary (6,644 words) from all text documents and then train a unigram language model only based on the attached text document for each image in the database. This is done using the CMU-Cambridge Statistical Language Modelling Toolkit <ref type="bibr" coords="4,322.09,550.92,15.61,8.85" target="#b12">[13]</ref> with the Witten-Bell smoothing. We have also produced some run using the Lemur Toolkit<ref type="foot" coords="4,331.20,560.75,3.97,6.97" target="#foot_0">1</ref> . With Lemur we use absolute discount smoothing.</p><p>Each image is indexed by the word probabilities in the lexicon. Given a query topic and any image in the database, the probability of query words generated by the corresponding image LM can be calculated. The image documents in the database are ranked by the probability from the highest to the lowest. For the LSI-based TBIR, we first use latent semantic indexing <ref type="bibr" coords="4,459.89,622.56,10.51,8.85" target="#b2">[3,</ref><ref type="bibr" coords="4,473.51,622.56,7.81,8.85" target="#b4">5]</ref> to find the 1,635-dimension eigen space. Then the image text document is indexed by a 1,635-dimensional vector. The cosine distance is applied to get the similarity scores between the query words and the image documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using Language Model and External Knowledge (Wikipedia)</head><p>There is an increasing production of information in a collaborative form in the "Wiki" format. It starts from Wikipedia<ref type="foot" coords="5,187.20,90.35,3.97,6.97" target="#foot_1">2</ref> , which is a free Encyclopedia, but there exists also now Wiktionary <ref type="foot" coords="5,496.80,90.35,3.97,6.97" target="#foot_2">3</ref> , a free open dictionnary, Wikiquote<ref type="foot" coords="5,234.60,102.35,3.97,6.97" target="#foot_3">4</ref> a compendium of quotations from notable people, Wikibooks and Wikisource, free collections of open-content text books, Wikinews, etc. All these information can be valuable source to help document indexing because of their large coverage. The drawback is the lake of explicit structure: these documents are produces for human reading only.</p><p>The full content of Wikipedia can be freely downloaded<ref type="foot" coords="5,356.04,150.11,3.97,6.97" target="#foot_4">5</ref> in XML format. XML is a more convenient form to extract some information than HTML because tags refer to content and not only layout. This format is also more adapted for automatic mining than HTML because of special tags (i.e. non XML) that are used to produce the HTML page. They can be use to localize typed content. For example information boxes on the right of a Wikipedia page, comes from different types of structures: Infobox, Geobox, Taxobox. These are structured in explicit fields : name, type, etc. They are templates which provide structured standardized information, mainly for named entities.</p><p>We have used the dump of 2 April 2007 of Wikipedia in English, which is a 9.8Gb file. This file is organized in 4881983 pages. We have used the Taxobox structure to extract a list of animal and plants. This structure is used to abstract common fact in a box about taxonomy of living things (animal and plants). We found 36188 of this box from which we extract 34321 terms, using the name and regnum field. For CLEF, only the 23399 animal terms are useful for queries 5, 20 and 35. Due to lake of disambiguation, terms like wall (a butterfly) and plane (a tree) can cause noise.</p><p>We have also used the Geobox for geographical terms. There is only 1805 box of this type, from which we can extract only 709 terms because we put a strong typographical constraint on terms, for example, we exclude abbreviations. This list contains river, mountain, region, city. For CLEF only mountain should be useful for queries 4, 44. The general box type Infobox is the most common in this version of Wikipedia, with 320311 objects. The keyword Infobox is followed by 2057 different types. These types are a useful indication of the category of the object described in the page. Table <ref type="table" coords="5,173.32,646.08,4.98,8.85" target="#tab_1">2</ref> shows the top 24 Infobox categories with frequencies.  For all these extractions, we use the title of the page to complete the name on the box. This helps to capture variations. More variations can be found using the directive #REDIRECT that link a page directly to another page. By using the title tag, we build a file that contains terms variations. This variation is then used to enlarge the previous lists of terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document Expansion with Extracted Knowledge</head><p>We have first proposed a basic indexing, using language model implemented into Lemur (see below run 09) with a MAP of 0.1342. We use corpus knowledge in the form of pseudo-feedback with 7 documents and 6 best terms. This gives an important improvement (run 21). We have test the different smoothing methods implemented in Lemur. The simple linear interpolation between document and corpus probability (Jelinek-Mercer method) gives good results with a weight of 0.8 between document and corpus statistics. The absolute discount seems to give similar results, a little better than the Bayesien dirichet one. With this setting we obtain a MAP of 0.1588. These results are similar with the CMU Toolkit (run 08). We have then expanded the documents with knowledge extracted from Wikipedia (run 15) with MAP of 0.1593. Using Wikipedia and pseudo feed back give similar results. Usually, it is the query which is expanded. We have chosen to expand document, first because, in this collection, document are very short, but also because we wanted to generalize named entities to a more general category in order to answer queries with generic terms. Also, because we are using language model, hence statistics on document are important. When we expand all documents with the Taxobox information, and all Geobox and Infobox on the type Mountain, the increase is strong but similar to pseudo feedback.</p><p>We use little information from the images: the fact that they are black an white or colors using a tool based on maximum saturation (HVS) of the image. This information ("black and white photo") is automatically added to the description of all images (run 17). With this information from image we obtain 0.1707 of MAP, only by strongly enhancing the query 11. Finally the adding of more information with a manual built thesaurus (run 19), is a more efficient way to increase the MAP to 0.1806 from run 17. Combining this run with the visual one 04 give good results in run 20 (MAP 0.2295 but not as good are our other results. We can say that adding knowledge is effective, but not as good as one may expect. In fact run 11 with MAP of 0.2442 is better because is uses feed back information from the visual side. Hence it seems that inter-media feed back is more effective that adding external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Description of Submitted Runs</head><p>Totally 27 runs where submitted including the CBIR runs, TBIR runs and mixed runs. To combine the ranking scores from different runs, the linear fusion method is utilized. The weight coefficients are empirically set. In the following we describe 6 the condition of each type of run. After the run number, the V means only visual, T only textual, VrT means pseudo relevance feedback on visual rank list using textual modality, TrV is also preudo feedback but on text using visual ranking, TiV means a textual indexing but with some input from visual analysis, finally TfV means a late fusion between the two modalities. We give also the official MAP. 24 FMLF HSVEDGENW 0.0526 same as run 06 but using eual weight for fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>01V HSV</head><p>25V FMLF HSVEDGEGABORWW 0.0554 same as run 07. Linear fusion of two ranking lists is performed with weighting scores 0.6 for HSV and 0.3 for edge and 0.1 for gabor.</p><p>26V COOC PATCHES HSVEDGE 1000CL 0.0369 index using Bag-of-visterm model with 1000 clusters.</p><p>27V COOC 16PATCHES HSVEDGE 1000CL KEYWORDFILTER 0.0311 same as run 26 but with filter of visual cluster by using top 100 most frequent keywords from text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results for Photographic Task</head><p>The best run is 14 with the MAP value 0.2833. It is ranked at the 6th place among 476 runs and at the 2nd place among automatic runs. This run is the mixed modality retrieval plus the CBIR based pseudo-relevance feedback (see section 2.4 (item 11) for the detailed description to the run). The best CBIR run reaches the MAP value 0.1204 for run 04 without the PRF. It is ranked at the 4th place for the visual based retrieval. Compared with the 1st run (INAOE/ INAOE-TIA-INAOE-VISUAL-EN-AN E, MAP: 0.1925) and the 2nd run (XRCE/ XRCE-IMG COMBFK, MAP: 0.1890), there are still room for us to improve our CBIR system. Now we see how PRF effects on the performance. First, for the CBIR system, the feedback from the TBIR seems have few effect on the performance. For example, with the PRF, the MAP of the HSV-based CBIR (run 02) is only increased to 0.0693 from 0.0684 (run 02). When combining 12 CBIR runs, the MAP is increased to 0.1358 (run 05)) from 0.1204 (run 04). This should due to the diversity of visual content and divergence of visual features. However, the PRF has a significant improvement for the TBIR. For example, the MAP value of the LM-based TBIR is 0.1377 (run 08). When the CBIR based pseudo-relevance feedback (run 04) is applied, significant improvement is observed and its MAP value reaches 0.2442 (run 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Medical Task: Bayesian Network Conceptual Indexing</head><p>We propose a Bayesian network based model, inferred from <ref type="bibr" coords="9,355.82,305.64,14.59,8.85" target="#b13">[14]</ref>, for capturing the concepts and semantic relationship between document concepts and query concepts. The conceptualisation is based on an external knowledge, named UMLS (Unified Medical Language System). The method of concept extraction is the same as our previous work in CLEF2006 <ref type="bibr" coords="9,392.40,341.52,10.00,8.85" target="#b5">[6]</ref>, i.e. we use Metamap <ref type="bibr" coords="9,502.45,341.52,10.56,8.85" target="#b0">[1]</ref> for mapping concepts in English reports, and our tool for French and German reports after tagging by TreeTager. These concepts extracted are then used to build up the network for our Bayesian network based retrieval framework. Bayesian network is a Direct Acyclic Graph(DAG) <ref type="bibr" coords="9,466.91,377.40,14.61,8.85" target="#b10">[11]</ref>, called also belief network, causal network. Our Bayesian network based model includes document nodes, query nodes and concept nodes and direct links between nodes.</p><p>There are two types of links: links which connect concepts nodes to documents or queries in which these concepts appear and links between document concept nodes and query concept nodes if there're semantic relations between them found in UMLS. These semantic relations are derived from UMLS relationship database files (MRREL.RRF for direct relation and MRHIER.RRF for hierarchical context of each instant of concept in different knowledge sources of UMLS). We decide the direction of links is on the path from documents to queries so that we can calculate the probabilistic inference from documents to queries in the network. Nodes which are pointed by a link are considered as child nodes.</p><p>The retrieval process is a probabilistic inference from document node to query node, considered as a 3-steps process:</p><p>• A document is observed which permits an initiation of prior probabilities of document concept nodes: in this model we chose tf.idf normalized as prior probability for document concept nodes.</p><formula xml:id="formula_0" coords="9,114.96,582.91,246.01,65.33">P (c|D) = w c c ∈D w 2 c with w c = tf * idf</formula><p>• These prior probabilities of document concept nodes are then inferred to query concept nodes via semantic links between them by this formulas:</p><formula xml:id="formula_1" coords="9,228.84,694.95,170.08,10.65">P (c|pa(c), D) = max i (α * P (pa i (c)|D))</formula><p>in which pa(c) is the set of parent concepts of c, α is parameter used to estimate the uncertainty of the relatedness or "aboutness" between c and its parent according to the type of semantic link. We suppose α may depend also on the quality of knowledge source who supply the relationship information. The bigger α is, the more relevant the two concepts are.</p><p>We predefine α empirically in the range [0, 1] according to the semantic relation corresponding or estimate it by the semantic similarity formulas of Leacock-Chorodo <ref type="bibr" coords="10,454.74,137.52,10.45,8.85" target="#b6">[7]</ref> in case of indirect hierarchical relations:</p><formula xml:id="formula_2" coords="10,259.80,169.23,107.08,24.21">sim(c 1 , c 2 ) = log 2 * L l(c 1 , c 2 )</formula><p>in which L is the maximum length of the taxonomy, l(c 1 , c 2 ) is the minimum length of path from c 1 to c 2 .</p><p>• The relevance status value (RSV) of a query Q corresponding to a document D is the conditional probability of query node Q with the condition that document node D is observed.</p><p>In this case we use the inference mechanism as <ref type="bibr" coords="10,320.71,262.44,15.50,8.85" target="#b10">[11]</ref> in which the probability of a node given their parents is calculated by link matrix. We chose weighted sum link matrix formulas in this case in other to take into account at the same time the inferred probability and weight of query concept nodes:</p><formula xml:id="formula_3" coords="10,224.40,321.07,177.42,25.38">RSV (Q, D) = P (Q|pa(Q), D) = i p i w i i w i</formula><p>p i is the probability of query concept node (in the set pa(Q)) inferred from document concept nodes of D; w i is the weight of query concept node correspond to query Q. This weight is measured by tf.idf normalized similarly to document concept nodes.</p><p>The relevant status value in the ranking list then can be re-weighted by semantic groups of concepts: RSV is multiplied with the number of concepts in the matched concepts set that correspond to the three important semantic groups: Anatomy, Pathology, and Modality. The purpose is to emphasize the important of these three semantic groups considering the structure of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results of Medical Runs</head><p>For the CLEF image medical 2007 collection <ref type="bibr" coords="10,294.61,497.64,14.59,8.85" target="#b9">[10]</ref>, we analyse the new image sets(MyPacs and Endoscopic) the same way as the collection last year.</p><p>In the submitted runs, based on predefined set of semantic relations in UMLS, we use isaindirect relations in four runs and different types of relations in the two other runs: isa, parent-child other than isa(PAR-CHD), broader-narrower(BR-RN), similar or alike(RL), related or possibly synonym(RQ). The value of α in all of these submitted runs is predefined in the range (0, 1), corresponding: 0.1, 0.2, 0.3, 0.4. In addition to these runs, we experiment another run which replaces predefined α by semantic similarity measurement of Leacock-Chorodo for isa-indirect relation. This method yeilds similar result. Results of our submitted runs are reported in table <ref type="table" coords="10,505.24,593.28,3.90,8.85" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The Bayesian model proposed here, subsume the VSM model with cosine similarity, and in addition take into account the semantic relationship between documents concepts and query concepts in an unified framework. Experimentation shows that this model can enhance the VSM by adding the semantic relatedness between concepts. Improvements on relationship weighting issue as well as performance of model are our further study. For the photographic task, we have improved our CBIR system the use of multiple visual features and different indexing techniques. Combined with the cross-modality pseudo-relevance feedback and text based search, our system obtains a good performance. From our results, we find 1) CBIR is an important component to search multi-modality image database, especially when the text description has weak discrimination; 2) cross-modality pseudo-relevance feedback can enhance the performance. But it has much more effect on the text based search than on the visual based search. The use of external knowledge like Wikipedia gives an extra bonus to the results but is less effective than expected. The large size of this set tends also to produce confusion because of lake of disambiguation. This disambiguation problem occurs mostly on small terms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,124.68,235.56,353.73,8.85;2,159.88,119.66,283.17,100.68"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Images with very similar text content but very different visual content.</figDesc><graphic coords="2,159.88,119.66,283.17,100.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,203.16,69.00,196.79,8.85;6,90.00,81.53,63.31,8.42;6,90.00,93.53,79.15,8.42;6,90.00,105.53,84.43,8.42;6,90.00,117.41,131.47,8.42;6,90.00,129.41,63.31,8.42;6,90.00,141.41,68.59,8.42;6,90.00,153.29,94.87,8.42;6,90.00,165.29,89.47,8.42;6,90.00,177.17,79.15,8.42;6,90.00,189.17,126.19,8.42;6,90.00,201.17,68.59,8.42"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Exemple extractions from Taxobox stevia|plant squirrel|animal springbok|animal springbok antelope|animal smelt|animal smelts|animal nitidulidae|animal sap beetle|animal scorpion|animal streptococcus|eubacteria snakes|animal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,206.40,446.04,190.34,8.85;6,90.00,458.57,193.75,8.42;6,90.00,470.57,178.03,8.42;6,90.00,482.45,157.15,8.42;6,90.00,494.45,235.64,8.42;6,90.00,506.45,183.07,8.42;6,90.00,518.33,266.96,8.42"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of redirection extraction torres del paine|cordillera del paine whitpaines creek|wissahickon creek cocker spainell|cocker spaniel paine art center|paine art center and gardens emma paine|the x factor uk series 1 torres del paine national park|cordillera del paine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,149.28,69.00,304.46,79.89"><head>Table 1 :</head><label>1</label><figDesc>Summary of individual visual runs</figDesc><table coords="4,149.28,90.60,304.46,58.29"><row><cell></cell><cell cols="4">tf-idf SVD ISM HME WORD SVD WORD ISM</cell></row><row><cell>COR</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>HSV</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>GABOR</cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>SIFT</cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,124.56,240.12,353.97,104.25"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="6,124.56,240.12,353.97,104.25"><row><cell></cell><cell>Top 24 of Infobox types</cell><cell></cell></row><row><cell>47765 Album</cell><cell>4857 City</cell><cell>3379 Biography</cell></row><row><cell>20042 Film</cell><cell>4839 actor</cell><cell>3163 band</cell></row><row><cell>10056 Single</cell><cell cols="2">4670 Television episode 2972 Radio Station</cell></row><row><cell>9793 musical artist</cell><cell>4353 University</cell><cell>2936 Software</cell></row><row><cell>9026 CVG</cell><cell>3976 Military Conflict</cell><cell>2789 Australian Place</cell></row><row><cell>7961 Company</cell><cell>3925 road</cell><cell>2681 Swiss town</cell></row><row><cell>6307 CityIT</cell><cell>3805 Book</cell><cell>2540 Broadcast</cell></row><row><cell cols="2">5070 Indian urban area 3449 Mountain</cell><cell>2475 Military Person</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,377.16,432.81,310.05"><head>21T LMKL S0M2D0.7C9T6 0.1588 This</head><label></label><figDesc>0.0684 index at the HSV SVD space and cosine distance. It is the CBIR baseline with only one type of feature. LM-based TBIR, i.e., run 08, plus the PRF. The feedback documents are from the combined CBIR without PRF, i.e. run 04. 12TiV LM 12RUNSVISUAL FB 0.2516 LM-based TBIR, i.e., run 08T, plus the PRF. The feedback documents are from the combined CBIR with PRF, i.e. run 05. This run is an extension of run 15 using a black and white image detector based on HSV value of image. This information is injected into all corresponding texts. The matching is still done on text domain. This just boosts only one query, but boost globally the result. The run 19 is fused with purely visual run 04. This late fusion is linear without weight, but with a linear transformation of all RSV output to the interval [1.0;0.00001] We do not choose to go to zero as minimum, to differentiate with document that are not retrieved, in either of the two modalities. We consider that a document not retrieved has a Relevance Status Value of zero. run is a variation of run 09. Relevance feed back is used on text only, with 9 top documents used as feedback and 6 best terms added to the query. The improvement is important.</figDesc><table coords="7,90.00,503.88,432.81,183.33"><row><cell>10T LSI 0.1322 only LSI based TBIR system.</cell></row><row><cell>11TrV LM 12RUNSVISUAL 0.2442 13TfV LM 12RUNSVISUAL 0.2226 7 combine runs 08 and 04.</cell></row><row><cell>14TfV LM FB 12RUNSVISUAL 0.2833 combine runs 11 and 04.</cell></row><row><cell>15T WTm S0M2D0.8C6T6 0,1593 LM-based TBIR with document expansion using infor-</cell></row><row><cell>mation automatically extracted from Wikipedia. It uses extracted plant, animals, geographic</cell></row><row><cell>terms, mountains, and also information of the redirection. New terms are added to docu-</cell></row><row><cell>ment to enlarge the matching. LM discount is adapted to 0.8 and top document in pseudo</cell></row><row><cell>relevance feedback to 6 and keep 6 terms.</cell></row><row><cell>16TfV WTm 12RUNSVISUAL 0.2180 combine runs 15 and 04.</cell></row><row><cell>17TiV WTm S0M2D0.8C6T6 0.1707 18TiVfV T1.0V1.0 0.2165 19TiV WTmM S0M2D0.8C6T6 0.1806 This run is an extension of run 17, with a very small</cell></row><row><cell>adds thesaurus manually extracted from Wikipedia. It is terms that are not from info boxes</cell></row><row><cell>but are relevant to this collection.</cell></row><row><cell>20TiVfV T1.0V1.0 0.2295 The same simple fusion is performed than run 18 but with the</cell></row><row><cell>textual run 19.</cell></row><row><cell>05VrT 12RUNS 0.1358 combine 12 PRF-based CBIR runs using the empirically tuned weights.</cell></row><row><cell>Refer to run 02 for more details. 22V 13RUNSVISUAL 0.1195 combine 12 CBIR runs and one run with the edge histogram</cell></row><row><cell>06V FMLF HSVEDGEWW 0.0547 global feature matching (HSV histogram and EDGE his-using the equal weight, i.e. not tuning the weights.</cell></row><row><cell>togram) with late fusion and weighting score for each feature. Normalize feature matrices 23TfV LM FB 13RUNSVISUAL 0.2732 combine runs 11 and 22.</cell></row><row><cell>(vectors) by Mean and Var. Linear fusion of rankink lists with weighting scores 0.7 for HSV</cell></row><row><cell>and 0.3 for edge.</cell></row><row><cell>07V FMLF HSVEDGEGABORNW 0.0541 global feature matching (HSV histogram and</cell></row><row><cell>EDGE histogram and Gabor features) with late fusion and equal weight for each feature.</cell></row><row><cell>08T LM SWITTEN 0.1377 only LM-based TBIR system with CMU and Wittenbaum smooth-</cell></row><row><cell>ing.</cell></row><row><cell>09T LMKL S0M2D0.7 0.1342 only LM-based TBIR system with Lemur, no stemming. Smooth-</cell></row><row><cell>ing used with interpolate smoothing strategy. The smoothing method is the "Absolute</cell></row><row><cell>discount" with the default parameter 0.7 for the discount delta.</cell></row></table><note coords="7,90.00,408.84,423.11,8.85;7,114.96,420.84,398.05,8.85;7,114.96,432.84,373.42,8.85;7,90.00,452.52,423.03,8.85;7,114.96,464.40,83.67,8.85;7,90.00,484.20,423.17,8.85"><p><p><p>02VrT HSV 0.0693 run 01 plus the PRF. The feedback documents are from the LM-based TBIR, i.e. run 08. It is provided to demonstrate the PRF effect. For the other 11 CBIR runs, the same method is carried out. Thus, 12 PRF-based CBIR runs are generated.</p>03V 12RUNS EQWEIGHT 0.1122 combine 12 CBIR runs using the equal weights, i.e. not tuning the weights.</p>04V 12RUNS WEIGHT 0.1204 combine 12 CBIR runs using the empirically tuned weights.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,102.12,69.00,398.87,100.05"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table coords="11,102.12,69.00,398.87,100.05"><row><cell></cell><cell cols="3">MAP results of CLEFMed2007</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="3">Isa PAR-CHD BR-RN RL</cell><cell>RQ</cell><cell>Map</cell><cell>R-prec</cell></row><row><cell>IPAL-TXT-BAY-ISA0.1</cell><cell>0.1 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.3057 0.332</cell></row><row><cell cols="2">IPAL-TXT-BAY-ALLREL2 0.2 0.01</cell><cell>0.01</cell><cell>0</cell><cell>0</cell><cell cols="2">0.3042 0.333</cell></row><row><cell cols="2">IPAL-TXT-BAY-ALLREL1 0.2 0.01</cell><cell>0.01</cell><cell cols="3">0.001 0.001 0.304</cell><cell>0.3338</cell></row><row><cell>IPAL-TXT-BAY-ISA0.2</cell><cell>0.2 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.3039 0.3255</cell></row><row><cell>IPAL-TXT-BAY-ISA0.3</cell><cell>0.3 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.2996 0.3212</cell></row><row><cell>IPAL-TXT-BAY-ISA0.4</cell><cell>0.4 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.2935 0.3177</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,677.30,108.36,7.21"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,664.94,100.78,7.21"><p>http://www.wikipedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,674.42,104.29,7.21"><p>http://www.wiktionary.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,105.24,683.90,87.03,7.21"><p>http://en.wikiquote.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,105.24,693.38,115.34,7.21"><p>http://download.wikimedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="8,105.24,682.58,225.85,7.21"><p>Due to some error this run has not being officially evaluated.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,110.53,356.52,91.92,8.85;11,229.44,356.52,44.06,8.85;11,293.17,356.52,220.03,8.85;11,110.52,368.40,212.46,8.85" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<ptr target="http://mmtx.nlm.nih.gov/docs.shtml" />
		<title level="m" coord="11,229.44,356.52,44.06,8.85;11,293.17,356.52,215.31,8.85">Metamap: Mapping text to the umls metathesaurus</title>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,388.32,402.49,8.85;11,110.52,400.32,131.98,8.85" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,351.96,388.32,129.18,8.84">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berthier</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM Press / Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,420.24,402.54,8.85;11,110.52,432.24,245.66,8.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,207.37,420.24,300.99,8.85">Exploiting latent semantic information in statistical language modeling</title>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,110.52,432.24,102.90,8.84">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1279" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,452.16,402.50,8.85;11,110.52,464.04,230.77,8.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,307.74,452.16,205.28,8.85;11,110.52,464.04,91.67,8.85">An integrated statistical model for multimedia evidence combination</title>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qibin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.68,464.04,86.49,8.84">ACM Multimedia&apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,483.96,402.53,8.85;11,110.52,495.96,402.49,8.85;11,110.52,507.96,89.30,8.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,361.16,483.96,151.89,8.85;11,110.52,495.96,292.07,8.85">A maximal figure-of-merit (mfom)learning approach to robust classifier design for text categorization</title>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,411.48,495.96,97.60,8.84">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="218" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,527.88,402.35,8.85;11,110.52,539.76,402.46,8.85;11,110.52,551.76,402.42,8.85;11,110.52,563.64,246.49,8.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,383.47,539.76,129.51,8.85;11,110.52,551.76,136.25,8.85">Ipal knowledge-based medical image retrieval in imageclefmed</title>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Raccoceanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diem</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thi</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Vuillenemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,287.52,551.76,193.06,8.84">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">2006. 20-22 September. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Medical Image Track</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.53,583.68,402.65,8.85;11,110.52,595.56,402.64,8.85;11,110.52,607.56,143.27,8.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,242.41,583.68,270.78,8.85;11,110.52,595.56,55.58,8.85">Combining local context and WordNet similarity for word sense identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,277.08,595.56,231.79,8.85">WordNet: An Electronic Lexical Database, chapter 11</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.53,627.48,402.41,8.85;11,110.52,639.36,118.70,8.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,185.54,627.48,247.93,8.85">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,443.64,627.48,69.30,8.84;11,110.52,639.36,26.55,8.84">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,659.40,402.53,8.85;11,110.52,671.28,402.38,8.85;11,110.52,683.28,297.25,8.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,441.92,659.40,71.14,8.85;11,110.52,671.28,301.17,8.85">Ipal inter-media pseudo-relevance feedback approach to imageclef 2006 photo retrieval</title>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlad</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joo Hwee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,432.12,671.28,80.77,8.84;11,110.52,683.28,111.56,8.84">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">20-22 September. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.52,62.04,402.57,8.85;12,110.52,74.04,402.49,8.85;12,110.52,85.92,402.49,8.85;12,110.52,97.92,86.91,8.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,306.39,74.04,206.62,8.85;12,110.52,85.92,131.26,8.85">Overview of the ImageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,264.84,85.92,194.97,8.84">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">sept 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.51,117.84,402.38,8.85;12,110.52,129.84,292.99,8.85" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,171.60,117.84,337.14,8.84">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<author>
			<persName coords=""><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.51,149.76,402.60,8.85;12,110.52,161.64,402.57,8.84;12,110.52,173.64,402.49,8.85;12,110.52,185.64,25.92,8.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,261.90,149.76,234.98,8.85">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,110.52,161.64,402.57,8.84;12,110.52,173.64,178.58,8.84">SIGIR &apos;98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,205.56,402.81,8.85;12,110.52,217.44,148.69,8.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,266.45,205.56,246.86,8.85;12,110.52,217.44,27.65,8.85">Statistical language modeling using the cmu-cambridge toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,158.88,217.44,69.34,8.84">Eurospeech 1997</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.50,237.36,402.61,8.85;12,110.52,249.36,188.66,8.85" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,266.73,237.36,241.52,8.85">Evaluation of an inference network-based retrieval model</title>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,110.52,249.36,97.00,8.84">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="222" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
