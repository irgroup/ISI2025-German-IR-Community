<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.88,110.86,412.25,15.15">TIA-INAOE&apos;s Participation at ImageCLEF 2007</title>
				<funder ref="#_ypKvHVH">
					<orgName type="full">CONACyT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.82,144.76,73.43,8.74"><forename type="first">H</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.99,144.76,88.75,8.74"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
							<email>carloshg@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.96,144.76,59.05,8.74"><forename type="first">Aurelio</forename><surname>López</surname></persName>
							<email>allopez@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.10,144.76,68.24,8.74"><forename type="first">Heidy</forename><forename type="middle">M</forename><surname>Marín</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.84,158.71,64.89,8.74"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
							<email>mmontesg@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.96,158.71,72.85,8.74"><forename type="first">Eduardo</forename><surname>Morales</surname></persName>
							<email>emorales@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.74,158.71,57.27,8.74"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
							<email>esucar@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.85,158.71,65.31,8.74"><forename type="first">Luis</forename><surname>Villaseñor</surname></persName>
							<email>villasen@ccc.inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Coordinación de Ciencias Computacionales Instituto Nacional de Astrofísica</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<address>
									<addrLine>Luis Enrique Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.88,110.86,412.25,15.15">TIA-INAOE&apos;s Participation at ImageCLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">19FD9699BD905D49609EE379BC641294</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Image retrieval, automatic image annotation, web-based query expansion, intermedia feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of INAOEs research group on machine learning, image processing and information retrieval (TIA) in the photographic retrieval task at ImageCLEF2007. Many experiments were performed comprising most of the query and target languages proposed for this years competition. A web-based query expansion technique was proposed for introducing context terms in the query topics. An intermedia blind relevance feedback technique was adopted in some runs for query refinement. Furthermore, experiments were performed with a novel technique for query/document expansion based on automatic image annotation methods. Initial experimental results give evidence that this idea could help to improve effectiveness of retrieval methods, though several issues should be addressed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Technology innovations have provided humans with more information than we are able to manually analyze. Currently, many huge repositories with textual, visual and auditive information are available. In addition there are collections of mixed data, for example images accompanied with textual descriptions. These last type of collections are referred to as annotated collections (one data type, text, is the annotation of another data type, images). Since documents in annotated collections are described in two modalities, document retrieval from these collections is a more challenging task than that of traditional information retrieval. This due to the problem of the lack of correspondence between modalities (the semantic gap) <ref type="bibr" coords="1,465.78,628.43,14.61,8.74" target="#b10">[11]</ref>. Though the consideration of two modalities could help improving accuracy of information retrieval systems. This paper is concerned with the task of retrieval of images in (manually) weakly annotated collections. Weakly annotated image collections are image databases in which each image is accompanied by a very small textual description, regarding the visual or semantic content of the image.</p><p>Current solutions for image retrieval from annotated collections are based on a single modality (text or image). Though retrieval accuracy of such systems is limited, mainly, because of the omitted modality and the querying complexity. In consequence there exist an increasing interest on the development of methods that can take advantage of all of the available information, that is text + image. In this research line, an effort is being carried out by the organizers of ImageCLEF<ref type="foot" coords="2,297.17,96.36,3.97,6.12" target="#foot_0">1</ref> ; the cross-language image retrieval track of the CLEF<ref type="foot" coords="2,536.66,96.36,3.97,6.12" target="#foot_1">2</ref> forum, whose goal is to investigate the effectiveness of combining text and image for retrieval, to collect and provide resources for benchmarking image retrieval systems and to promote the exchange of ideas which will lead to improvements in the performance of retrieval systems. Organizers provide participants with image collections, query topics and relevance assessments of retrieval systems' runs <ref type="bibr" coords="2,416.62,145.76,9.97,8.74" target="#b8">[9]</ref>. This sort of forums are very useful because they motivate research advances on several problems related to information retrieval, furthermore they contribute to the creation of new multidisciplinary research groups and collaborations between participants.</p><p>This paper describes the participation of the INAOE's research group on machine learning, image processing and information retrieval (TIA) in the photographic retrieval competition at ImageCLEF2007. This is the first time TIA participates on ImageCLEF. A total of 95 runs were submitted comprising all of the target languages and the following query ones: English, Spanish, German, Italian, French, Swedish, Japanese, Portuguese, Russian and Visual. Experiments were carried out with different techniques, like intermedia feedback and fusion of independent retrieval models. Other experiments were carried out with an approach based on web query expansion and with a new method based on automatic image annotation. We propose a web-based technique for expanding ImageCELF2007 queries, with which promising results were obtained. Furthermore, we propose a new research direction not explored yet for image retrieval from annotated collections. The approach consist of using automatic image annotation methods for obtaining text from the visual content of the images. This text is then combined with the original annotations of the images (and/or with the queries) and then standard strategies are adopted for retrieving documents. Experimental results show that runs based on text and image improve those based on text only. The best results were obtained with a combination of intermedia feedback and our Web-based query expansion technique. Relatively good results were obtained with the annotation based expansion. However, several issues should be addressed in order to obtain better results with this technique.</p><p>The rest of the document is organized as follows. In the next section we briefly introduce the ad-hoc photographic retrieval task, for further information we refer the reader to the overview paper by Grubinger et al <ref type="bibr" coords="2,93.12,408.77,9.96,8.74" target="#b8">[9]</ref>. Next, in Section 3 the text-only methods are described. In Section 4 two techniques that were used for combining textual and image information are presented. Then, in Section 5 the proposed annotationbased expansion technique is introduced. In Section 6 the results of our runs are presented and analyzed. Finally, in Section 7, we highlight some conclusions and discuss future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad-hoc photographic retrieval</head><p>The ImageCLEF2007 is a track running as part of the CLEF campaign. It comprises four tasks related to image retrieval, namely: ad-hoc photographic retrieval, object retrieval, medical image retrieval and automatic annotation for medical images. This paper presents developments and contributions for the first task, that of retrieving images from a collection of annotated photographs. The goal of this task is the following: given a multilingual statement describing an user information need, find as many relevant images as possible from the given document collection <ref type="bibr" coords="2,280.46,559.18,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,295.10,559.18,7.01,8.74" target="#b8">9]</ref>. Organizers provide participants with a collection of annotated images, together with some topics describing information needs. Participants use these resources with their retrieval systems and submit to the organizers the identifiers of the relevant documents (according to the retrieval system used) for each topic. Organizers evaluate the results set of each submission for every participant and rank submissions according to a standard evaluation measure. The collection of documents used for ImageCLEF2007 is the recently created IAPR TC-12 Benchmark <ref type="bibr" coords="2,400.35,618.95,14.61,8.74" target="#b9">[10]</ref>. Each query topic consist of a fragment of text describing a single information need, together with three sample images visually similar to the desired relevant images <ref type="bibr" coords="2,192.15,642.86,9.96,8.74" target="#b8">[9]</ref>. Participants use topic's content for creating queries that are used with their retrieval systems. System's runs are then evaluated by the organizers using standard evaluation measures like MAP <ref type="bibr" coords="2,116.57,666.77,9.97,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual methods for image retrieval</head><p>The predominant approach for image retrieval from annotated collections consist of using text-only retrieval methods. This mainly because in some collections annotations describe effectively the visual and semantic content of images, for example the collection used for ImageCLEF2006. However in realistic collections images are described by only a few words indicating the semantic content of each image, for example the collection used for ImageCLEF2007. In these scenario information extracted from the images could be helpful for improving retrieval performance. For this reason we performed experiments with methods based on both text and images, text-only runs were also submitted for measuring the gain we have by using visual information. Independently of the type of run all of our submissions used a textual retrieval system (for the mixed runs text was extracted from images and used with the textual retrieval system), which is described in Section 3.1. Furthermore, in Section 3.2 the proposed technique for expansion of textual queries is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline retrieval system</head><p>Since text was used in all of the TIA's runs a textual retrieval system was implemented. For this purposed the TMG Matlab R toolbox was used, kindly provided by Zeimpekis et al <ref type="bibr" coords="3,385.32,261.67,14.61,8.74" target="#b16">[17]</ref>. This toolbox includes standard methods for indexing and retrieval of middle size text collections. We decided to use this method because working with Matlab R allows the easy processing of images (through predefined functions), necessary for the development of the annotation based expansion technique.</p><p>After removing meta-data and useless information, the text of the captions was indexed on the four target languages (English, Spanish, German and Random), resulting on four indexed collections. For indexing we used a tf-idf weighting, English stop words were removed and standard stemming was applied <ref type="bibr" coords="3,475.54,333.41,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,488.70,333.41,11.63,8.74" target="#b16">17]</ref>. Queries for the baseline runs were created by using the text in topics as provided by the organizers, after removing meta-data. For multilingual experiments queries were translated using the online Systran<ref type="foot" coords="3,485.59,355.74,3.97,6.12" target="#foot_2">3</ref> translation software. For retrieval we considered the cosine similarity function <ref type="bibr" coords="3,364.19,369.27,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Web based query expansion</head><p>The Web is the largest repository of information ever existed. There are millions of documents available in the Web that can be used to extract semantic knowledge. In this work we propose a web-based query expansion technique in order to incorporate context terms into the original queries, with the hope that expanded queries are able to reach (through a retrieval model) relevant documents containing other terms than the ones contained in the original queries.</p><p>The proposed approach is a very intuitive method that uses the Google R search engine. For each topic, we take the textual description and submitted a search to Google R ; the top-k snippets returned by the search engine are considered for expanding a query. We tried two approaches that we called naive and repetition. The naive approach consist of taking the snippets as they are returned by Google R with no preprocessing. On the other hand, the repetition approach consist of retaining the terms that most co-occurred among the snippets. For the experiments reported here the top-20 snippets were considered. Queries expanded with the naive method resulted on larger queries, while the ones expanded with the repetition method included only a few extra terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mixed methods for image retrieval</head><p>In order to consider both textual and visual information in the retrieval process, two strategies were adopted: intermedia feedback and late fusion of independent retrieval systems. We decided to use these techniques due to their simplicity, and because both methods have obtained relatively good results on the retrieval task <ref type="bibr" coords="3,70.87,649.64,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,84.70,649.64,7.01,8.74" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intermedia pseudo-relevance feedback</head><p>Relevance feedback is a way of allowing user interaction in the retrieval process. It was first proposed for retrieval from text collections and then used on the content-based image retrieval (CBIR) task <ref type="bibr" coords="4,477.98,104.37,14.62,8.74" target="#b14">[15]</ref>. Although user interaction is allowed on the retrieval process, always is preferable to develop fully automatic systems. Pseudo-relevance feedback (PRF ) is a variant that does not need user interaction, as it considers as relevant the set of the top-k documents returned by the retrieval system. Queries are refined considering such a set of documents as relevant for the user's information needs. This technique has been widely used in several text-based image retrieval systems within the ImageCLEF <ref type="bibr" coords="4,329.86,164.14,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="4,343.70,164.14,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="4,354.77,164.14,7.01,8.74" target="#b8">9]</ref>.</p><p>A novel technique based on PRF has been proposed for image retrieval from annotated collections <ref type="bibr" coords="4,530.62,176.10,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="4,70.87,188.06,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="4,83.41,188.06,12.73,8.74" target="#b13">14,</ref><ref type="bibr" coords="4,100.93,188.06,11.63,8.74" target="#b11">12]</ref>. This technique called intermedia feedback (IMFB ) consist of using a CBIR<ref type="foot" coords="4,468.20,186.48,3.97,6.12" target="#foot_3">4</ref> system with a query image for retrieving documents. The top-k documents returned are assumed to be relevant and the captions of such documents are combined to create a textual query. The textual query is then used with a pure textual retrieval system, and the documents returned by such a system are returned to the user. This technique was first used by the NTU group at ImageCLEF2005 and since then several groups have been adopted it <ref type="bibr" coords="4,119.35,247.83,10.51,8.74" target="#b1">[2,</ref><ref type="bibr" coords="4,133.34,247.83,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="4,149.55,247.83,11.63,8.74" target="#b13">14]</ref>. We decided to perform experiments with this technique in order to improve retrieval effectiveness of the query expansion method proposed, by taking into account information extracted from images. Combined runs of query expansion and IMFB consist of applying the query expansion technique to the textual topics. Next, the expanded queries are combined with the captions of the top-k relevant documents according to a CBIR system, and then used with our text-only retrieval system. FIRE was used as CBIR system; using the baseline run provided by the organizers <ref type="bibr" coords="4,366.21,307.61,9.97,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Late fusion of independent systems</head><p>For some runs we adopted a late fusion strategy for merging the output of independent visual and textual systems. This merging method consists of running two independent retrieval systems using a single (different) modality each. Then the relevant documents returned by both systems are combined.</p><p>For this work we proposed the following fusion strategy. Assuming that T R is the list of relevant documents (ranked in descending order of relevance) according to a textual retrieval system applied to the images' captions; similarly, V R is the list of ranked relevant documents according the a CBIR system that only uses the images. We combined and re-ranked the documents returned by both retrieval systems, generating a new list of relevant documents LF R = {T R ∪ V R }; where each document d i ∈ LF R is ranked according to the score formula given by Equation ( <ref type="formula" coords="4,219.54,449.53,4.24,8.74" target="#formula_0">1</ref>)</p><formula xml:id="formula_0" coords="4,213.48,468.89,327.66,21.89">score(di) = α × RT R (di) + (1 -α) × RV R (di) 1 T R (d i ) + 1 V R (d i )<label>(1)</label></formula><p>where R T R (d i ) and R V R (d i ) is the position in the ranked list of document d i according to the textual and visual retrieval systems, respectively. 1 T R (di) and 1 T R (di) are indicator functions that take the value 1 if document d i is in the list of relevant documents according to the textual and visual retrieval systems respectively, and zero otherwise. The denominator accounts for documents appearing in both lists of relevant documents (T R and V R ). Documents are sorted in ascending order of their score. Intuitively with this score documents appearing in both sets (visual and textual) will appear at the top of the ranking, regarding their position on the independent lists of relevant documents. We tried several values for α and the best results were obtained with α = 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Annotation-based document and query expansion</head><p>The task of automatic image annotation (AIA) consists of assigning textual descriptors (words, labels) to images, starting from visual attributes extracted from them. There are two ways of facing this problem, at image level and at region level. In the first case, labels are assigned to the entire image as an unit, not specifying which words are related to which objects within the image. In the second approach, which can  be conceived as an object recognition task, the assignment of labels is at region level; providing a one-to-one correspondence between words and regions<ref type="foot" coords="5,258.44,343.84,3.97,6.12" target="#foot_4">5</ref> . The last approach can provide more semantic information for the retrieval task, although it is more challenging than the former.</p><p>In this work we decided to use region-level AIA methods for obtaining text from images and then using these textual labels for expanding topics and images' annotations. We believe that by taking both modalities to a common representation and then using standard retrieval strategies can be helpful for improving singlemodality approaches. A graphical description of the proposed approach is shown in Figure <ref type="figure" coords="5,468.83,405.19,3.88,8.74" target="#fig_0">1</ref>. As we can see the process involves several steps that can be summarized in the following tasks: segmentation, annotation, and expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Segmentation</head><p>The first step for obtaining words from regions within images is obtaining regions from images. This task is known as segmentation and consists of discovering partitions within a given image with the restriction that each of the partitions contains a single object. Many segmentation methods have been proposed, although this is still an open problem in vision. For segmenting the IAPR-TC 12 Benchmark collection we decided to use a state of the art algorithm named normalized cuts <ref type="bibr" coords="5,332.02,523.20,14.61,8.74" target="#b15">[16]</ref>. This is the algorithm used by most of the region-level annotation approaches <ref type="bibr" coords="5,226.54,535.15,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="5,240.84,535.15,11.63,8.74" target="#b10">11]</ref>. In Figure <ref type="figure" coords="5,307.03,535.15,4.98,8.74" target="#fig_1">2</ref> sample images segmented with normalized cuts are shown. As we can see the algorithm works well for some images, isolating single objects; however for other similar images segmentation is not as good, partitioning single objects into several regions.</p><p>Using a recently developed segmentation-annotation interface <ref type="bibr" coords="5,367.30,571.02,14.62,8.74" target="#b12">[13]</ref>, the full IAPR-TC 12 Benchmark collection was segmented with the normalized cuts algorithm. Then visual attributes were extracted from each region. Attributes include color, texture and shape information of the region, for a total of 30 attributes. Each region is then described by its vector of attributes. In order to facilitate reading hereafter we refer to the attribute's vector describing a region simply by the term region.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Annotation</head><p>AIA methods start from visual attributes extracted from the image or regions and assign labels based on some training examples. Each training example is a pair composed of a region and its corresponding label.</p><p>In order to create a training set of region-label pairs an annotation interface was used <ref type="bibr" coords="6,455.47,341.70,14.61,8.74" target="#b12">[13]</ref>. Such interface allows the manual annotation, at region-level, of segmented images, as well as options for re-segmentation and for joining adjacent regions containing the same object. The IAPR-TC 12 Benchmark consist of 20,000 images, taking the larger 5 regions for each image we have 100,000 regions to annotate. In consequence we would need a large training set of annotated images-regions. However time constrains allowed us to create only a small training set. For creating the training set we randomly selected around 2% of the total number of regions and manually annotated them using the developed interface.</p><p>The set of labels that can be assigned to regions (that is, the annotation vocabulary) was defined subjectively by the authors, by looking at the ImageCLEF2007 textual topic descriptions. The vocabulary of allowed words is shown in Table <ref type="table" coords="6,213.13,449.30,3.88,8.74" target="#tab_0">1</ref>, and the number of regions, in our training set, annotated with each label is shown in Figure <ref type="figure" coords="6,156.27,461.25,3.88,8.74" target="#fig_2">3</ref>. Some labels are supposed to represent several concepts, for example the label water was used to label regions of rivers, ocean, sea, and streams. While other labels represent specific objects, such as swimming-pool and tower. From Figure <ref type="figure" coords="6,285.80,485.16,4.98,8.74" target="#fig_2">3</ref> we can see that there are several labels that have many training examples (for example, Sky, Person), though several other labels have only a few. This fact together with poor segmentation made difficult the process of annotation. Sample images with their corresponding annotations are shown in Figure <ref type="figure" coords="6,215.08,521.03,3.88,8.74" target="#fig_1">2</ref>.</p><p>The training set of region-label pairs is used with a knn classifier for annotating the rest of un-annotated images-regions on the IAPR-TC 12 Benchmark collection for document expansion, or the topic's images for query expansion. Note that the training set size is very small<ref type="foot" coords="6,372.87,555.32,3.97,6.12" target="#foot_5">6</ref> for achieving good results with the knn algorithm; even when knn have obtained good results on this task, outperforming other state of the art annotation methods <ref type="bibr" coords="6,159.75,580.81,10.51,8.74" target="#b6">[7,</ref><ref type="bibr" coords="6,172.74,580.81,7.01,8.74" target="#b7">8]</ref>. In order to overcome in part the issues of poor segmentation and imbalanced/small training set we decided to apply a postprocessing to knn for improving annotation accuracy. Recently a method, called MRFI, for improving accuracy on AIA has been proposed <ref type="bibr" coords="6,408.33,604.72,9.97,8.74" target="#b7">[8]</ref>. MRFI considers a set of candidate labels for each region and selects an unique label for each region based on sematic information between labels and the confidence of the AIA method on each label. The process of improvement is graphically described in Figure <ref type="figure" coords="6,158.01,640.58,3.88,8.74" target="#fig_3">4</ref>, left.</p><p>Candidate labels and confidence values are obtained from knn, the annotation method. Semantic association between labels is obtained by measuring co-occurrences of labels at document level in an external  corpus of documents. For each region knn ranks labels in decreasing order of the relevance of the labels to being the correct annotation for such region. We consider the set of the top-k more relevant labels for each region. In this way we have k-candidate labels for each region in each image, each of these candidate labels is accompanied by a relevance weight, according to the knn ranking. Taking into account these relevance weights for the region-label assignments MRFI assigns to each region within a given image the label that is more likely, given the labels assigned to neighboring regions in the same image. Intuitively MRFI selects the best configuration of regions-labels assignments for each image, given the semantic cohesion between labels assigned to spatially connected regions and given the relevance weight of each region-label assignment. For doing this the set of regions-labels assignments for a given image is modeled with a Markov random field (MRF ), see Figure <ref type="figure" coords="7,156.61,490.13,3.88,8.74" target="#fig_3">4</ref>, right. Each possible assignment of regions-labels for the image is said a configuration of the MRF. The goal of MRFI is to select the (pseudo) optimal configuration by considering the relevance of each assignment and the semantic association between labels assigned to neighboring regions. The optimal configuration for each image is the configuration that minimizes an energy function defined by potentials. Each potential is a function that considers one aspect of the problem at hand. A first potential function considers the relevance weight of each region-label assignment, while another potential attempts to keep semantic cohesion between labels assigned to connected regions. The minimization of the energy function is carried out by simulated annealing. The best configuration is then considered the correct annotation of each image. For further details about this method we suggest the reader to follow the references <ref type="bibr" coords="7,471.92,585.77,9.97,8.74" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query/document expansion</head><p>For document expansion the annotations assigned to each region in each image in the IAPR-TC 12 Benchmark are added to the original annotation. For query expansion the sample topic images were segmented and annotated, then the set of annotations was used for expanding (or creating) textual queries, in Figure <ref type="figure" coords="7,536.16,655.96,4.98,8.74" target="#fig_4">5</ref> an expanded topic is shown. As we can see some labels are repeated on the resulting expanded topic (sky, people and tree); we decided to consider repeated labels because in this way repeated terms are considered representative terms of the expanded query. If we would not considered label's repetitions the word people,</p><p>We have described the first participation of INAOE's TIA research group at the ImageCLEF2007 photographic retrieval task. A total of 95 runs were submitted comprising most of the query languages and all of the target ones. Experiments were performed with two widely used approaches on image retrieval from annotated collections: intermedia feedback and late fusion of independent retrievers. A Web-based query expansion technique was proposed for introducing context into the topics and a novel annotation based expansion technique was also developed. The best results were obtained with the intermedia feedback combined with a version of the web-based query expansion technique. With the naive expansion technique several relevant related terms are added to the original query, as well as many noisy terms. However noisy terms do not affected the performance of the retrieval system. The intermedia feedback technique outperformed late fusion by a large margin.</p><p>Relatively good results were obtained with the annotation based runs, however this can be due to the techniques combined with the ABE methods. Results with this technique give evidence that automatic image annotation methods could be helpful on image retrieval from annotated collections. This because promising results were obtained even when segmentation was poor, the training set was extremely small and imbalanced, and annotations did not covered the objects present within the image collection. In consequence for future work we will address all of these issues by creating a larger training set of annotated images, using other segmentation algorithms, defining labels objectively trying to cover most of the objects present in images within the collection and keeping balanced the training set. Also we intent to use other annotation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,140.77,156.85,326.38,8.74;5,130.94,177.54,85.04,99.21"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical schema of the proposed approach for AIA-based expansion.</figDesc><graphic coords="5,130.94,177.54,85.04,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,70.87,290.87,470.27,8.74;5,70.87,302.51,188.83,7.86;5,219.30,177.54,85.04,99.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample images from the IAPR TC-12 2007 collection, segmented with the normalized cuts algorithm. Manual annotations are shown for each region.</figDesc><graphic coords="5,219.30,177.54,85.04,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,93.32,267.58,417.17,8.74;6,228.05,153.25,155.90,99.21"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of regions in the training set annotated with the words defined in the vocabulary.</figDesc><graphic coords="6,228.05,153.25,155.90,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,70.87,170.02,470.26,8.74;7,70.87,181.66,470.27,7.86;7,70.87,192.62,123.41,7.86;7,164.27,212.44,283.46,113.39"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: graphical description of the improvement process of MRFI. Right: graphical representation of the MRF ; (red) line-arcs consider semantic cohesion between labels, while (blue) dashed-arcs consider relevance weight of each label according to knn.</figDesc><graphic coords="7,164.27,212.44,283.46,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,70.87,339.94,470.27,8.74;7,70.87,351.58,212.71,7.86"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Expansion of the topic 36 using annotations. Annotations are showed below each segmented image. The expanded query is shown below image's annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,70.87,72.45,470.27,68.85"><head>Table 1 :</head><label>1</label><figDesc>Vocabulary of labels considered for the annotation process. The number above each label is the identifier for such label used in the histogram of Figure3.</figDesc><table coords="6,84.03,72.45,443.94,36.58"><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell></row><row><cell>Sky</cell><cell>Person</cell><cell>Building</cell><cell>Trees</cell><cell>Clouds</cell><cell>Grass</cell><cell>Water</cell><cell>Mountain</cell><cell>Sand</cell><cell>Other</cell><cell>Furniture</cell><cell>Road</cell></row><row><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell></row><row><cell>Animal</cell><cell>Snow</cell><cell>Rock</cell><cell>Sun</cell><cell>Vehicle</cell><cell>Boat</cell><cell>Church</cell><cell>Tower</cell><cell>Plate</cell><cell>Flag</cell><cell>Statue</cell><cell>Prize</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,86.11,686.26,134.61,6.99"><p>http://ir.shef.ac.uk/imageclef/2007/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,86.11,695.76,95.51,6.99"><p>http://clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,86.11,669.13,108.77,6.99"><p>http://www.systranbox.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,86.11,693.34,349.95,6.99"><p>Note that the process can start from text, obtaining query images for a CBIR system, as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,86.11,637.77,455.02,6.99;5,70.87,647.23,93.67,6.99"><p>Note that with region-level annotation we have also an annotation at image level, which is the set of labels assigned to the regions within the image.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,86.11,683.42,455.02,6.99;6,70.87,692.89,271.94,6.99"><p>The training set was created for applying a recently proposed semi-supervised learning algorithm based on unlabeled data<ref type="bibr" coords="6,70.87,692.89,12.42,6.99" target="#b12">[13]</ref>, however time constrains do not allowed us to use such an algorithm.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank <rs type="person">M. Grubinger</rs>, <rs type="person">A. Harbury</rs>, <rs type="person">T. Desealers</rs>, <rs type="person">P. Clough</rs> and all other organizers of ImageCLEF2007 because of their important effort. This work was partially supported by <rs type="funder">CONACyT</rs> under grant <rs type="grantNumber">205834</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ypKvHVH">
					<idno type="grant-number">205834</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for example, would be considered equally important (for representing the query) if it appeared as annotation of an unique region in a single image or as 4 annotations of 4 different regions within the three topic's images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>A total of 95 runs were submitted to the ImageCLEF2007, comprising all of the above described methods: intermedia feedback (IMFB ), naive web-based query expansion (NQE), repetition web-based query expansion (WBQE ), late fusion of independent retrievers (LF ) and annotation based expansion (ABE ). Note that some runs are combination of these methods. The top ranked entries for each language configuration together with a description of the methods used are shown in Table <ref type="table" coords="8,307.48,466.32,3.88,8.74">2</ref>.</p><p>As we can see, most of the entries are ranked near the top one. The best performance overall runs were obtained by using IMFB together with NQE s. We can observe that the runs with IMFB+NQE for target language English have exactly the same MAP value, independently of the query language. This is due to the fact that all of the queries were translated to English, then the snippets returned by Google R were used for expanding the translated query. Snippets were similar because the English search engine was used, and translations were very similar for all the query languages. The expanded queries were then combined with the captions of the top-10 relevant documents according to the baseline FIRE run, which were the same for all the query languages.</p><p>Other entries ranked high were those using NQE. Actually the NQE is present in all of the top ranked runs. NQE outperformed WBQE in all of the language configurations. This can be graphically appreciated in Figure <ref type="figure" coords="8,113.96,597.83,4.98,8.74">6</ref> (left). This is a surprising result because with the naive approach several noisy terms are added to the queries. While with the repetition approach only the terms that most appear among all the snippets are added. Therefore, the good results of NQE are due to the inclusion of many highly related terms, while the insertion of some noisy terms does not affect the performance of the retrieval model. From Figure <ref type="figure" coords="8,511.42,633.69,4.98,8.74">6</ref> (left) we can also clearly appreciate that IMFB outperformed the LF approach.</p><p>Six runs based on annotation expansion were submitted to the ImageCLEF2007. In these runs document and query expansion was combined with the other techniques proposed in previous sections. The descriptions of the annotation based expansion (ABE ) runs submitted to ImageCLEF2007 are shown in Table <ref type="table" coords="8,494.52,681.51,3.88,8.74">3</ref>, together with their overall ranking position. Results with ABE are mixed. The two top ranked runs with</p><p>Table <ref type="table" coords="9,98.15,151.38,3.88,8.74">3</ref>: Settings of the annotation expansion based runs. An X indicates that the corresponding technique is used.  correspond to entries that used ABE +IMFB. This the result can be due to the IMFB performance instead of the ABE technique. The third ABE ranked run used ABE of documents and queries with NQE+LF which obtained a slightly lower MAP than NQE+LF without ABE. Therefore no gain can be attributed to the ABE technique. The other ABE runs were ranked low. Summarizing, relatively good results were obtained with ABE, though this results can be due to the other techniques employed (IMFB and NQE ). However, we should emphasize that this was our very first effort towards developing annotation based methods for improving image retrieval. In Figure <ref type="figure" coords="9,234.05,482.68,4.98,8.74">6</ref> (right) we compare mixed and text-only runs. From this Figure it is clear that the mixed approaches were always superior to the text-only runs. This result clearly illustrates the fact that performance of independent retrievers can be improved by considering both modalities for image retrieval.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,90.18,230.45,362.36,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,243.33,230.45,119.69,7.86">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Pearson E. L</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,245.02,450.95,7.86;10,90.18,255.98,324.13,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,186.20,245.02,354.93,7.86;10,90.18,255.98,133.85,7.86">Approaches of using a word-image ontology and an annotated image corpus as intermedia for cross-language image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.19,255.98,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,270.55,450.96,7.86;10,90.18,281.50,107.68,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,251.52,270.55,189.90,7.86">Combining text and image queries at imageclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,481.01,270.55,60.13,7.86;10,90.18,281.50,47.85,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,296.07,450.95,7.86;10,90.18,307.03,338.16,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,362.14,296.07,178.99,7.86;10,90.18,307.03,147.63,7.86">Overview of the imageclef 2006 photographic retrieval and object annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.23,307.03,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,321.60,450.95,7.86;10,90.18,332.56,355.71,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,484.63,321.60,56.50,7.86;10,90.18,332.56,143.04,7.86">The clef 2005 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müeller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.66,332.56,134.55,7.86">Working Notes of the CLEF 2005</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,347.13,450.95,7.86;10,90.18,358.09,450.95,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,224.42,347.13,279.75,7.86">Content-based image retrieval -approaches and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,523.48,347.13,17.65,7.86;10,90.18,358.09,302.90,7.86">Proceedings ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting>ACM International Workshop on Multimedia Information Retrieval<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Multimedia</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,372.65,450.96,7.86;10,90.18,383.61,426.12,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,268.05,372.65,269.01,7.86">Improving automatic image annotation based on word co-occurrence</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,101.70,383.61,307.36,7.86">Proccedings of the 5th International Adaptive Multimedia Retrieval workshop</title>
		<meeting>cedings of the 5th International Adaptive Multimedia Retrieval workshop<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,398.18,450.95,7.86;10,90.18,409.14,450.96,7.86;10,90.18,420.10,88.86,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,285.90,398.18,255.23,7.86;10,90.18,409.14,41.90,7.86">Word co-occurrence and mrf&apos;s for improving automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.03,409.14,334.98,7.86">Proceedings of the 18th British Machine Vision Conference (BMVC 2007) To appear</title>
		<meeting>the 18th British Machine Vision Conference (BMVC 2007) To appear<address><addrLine>Warwick, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,434.67,450.96,7.86;10,90.18,445.63,369.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,311.88,434.67,229.26,7.86;10,90.18,445.63,15.40,7.86">Overview of the ImageCLEF 2007 photographic retrieval task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,125.04,445.63,177.30,7.86">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,460.19,450.95,7.86;10,90.18,471.15,149.22,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,319.91,460.19,221.22,7.86;10,90.18,471.15,119.94,7.86">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,485.72,450.95,7.86;10,90.18,496.68,450.96,7.86;10,90.18,507.64,450.95,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,344.58,485.72,196.55,7.86;10,90.18,496.68,145.29,7.86">Mind the gap: Another look at the problem of the semantic gap in image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G B</forename><surname>Enser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Sandom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,483.63,496.68,57.51,7.86;10,90.18,507.64,230.20,7.86">Proceedings of Multimedia Content Analysis, Management and Retrieval</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Eds</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<meeting>Multimedia Content Analysis, Management and Retrieval<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6073</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,522.21,450.95,7.86;10,90.18,533.17,310.16,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,311.05,522.21,230.08,7.86;10,90.18,533.17,119.88,7.86">Ipal inter-media pseudo-relevance feedback approach to imageclef 2006 photo retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Maillot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Valea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,229.23,533.17,111.28,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,547.74,450.95,7.86;10,90.18,558.69,450.95,7.86;10,90.18,569.65,52.62,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,290.17,547.74,250.97,7.86;10,90.18,558.69,47.65,7.86">Automatic image annotation using a semi-supervised ensemble of classifiers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Marin-Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.64,558.69,351.95,7.86">Proceedings of the 12th Iberoamerican Congress on Pattern Recognition (CIARP 2007)</title>
		<meeting>the 12th Iberoamerican Congress on Pattern Recognition (CIARP 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,90.18,584.22,450.95,7.86;10,90.18,595.18,450.96,7.86;10,90.18,606.14,51.33,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,353.37,584.22,187.77,7.86;10,90.18,595.18,310.99,7.86">Cindi at imageclef 2006: Image retrieval and annotation tasks for the general photographic and medical image collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.51,595.18,113.19,7.86">Working Notes of the CLEF</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,620.71,450.95,7.86;10,90.18,631.67,415.91,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,287.00,620.71,254.13,7.86;10,90.18,631.67,58.70,7.86">Relevance feedback: A power tool for interactive content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,156.52,631.67,264.47,7.86">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,646.23,450.95,7.86;10,90.18,657.19,171.14,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,180.21,646.23,167.64,7.86">Normalized cuts and image segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.69,646.23,182.45,7.86;10,90.18,657.19,81.93,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.18,671.76,450.95,7.86;10,90.18,682.72,450.95,7.86;10,90.18,693.68,181.46,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,235.35,671.76,305.78,7.86;10,90.18,682.72,40.41,7.86">Tmg: A matlab toolbox for generating term-document matrices from text collections</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeimpekis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,337.06,682.72,204.07,7.86;10,90.18,693.68,50.94,7.86">Grouping Multidimensional Data: Recent Advances in Clustering</title>
		<editor>
			<persName><forename type="first">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Kogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="187" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
