<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.62,146.21,317.74,18.08;1,112.78,168.13,377.41,18.08;1,190.57,190.05,221.85,18.08">Multi-Modal Interactive Approach to ImageCLEF 2007 Photographic and Medical Retrieval Tasks by CINDI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.85,225.11,64.24,10.46"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.05,225.11,62.33,10.46"><forename type="first">Bipin</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.12,225.11,90.04,10.46"><forename type="first">Prabir</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.62,146.21,317.74,18.08;1,112.78,168.13,377.41,18.08;1,190.57,190.05,221.85,18.08">Multi-Modal Interactive Approach to ImageCLEF 2007 Photographic and Medical Retrieval Tasks by CINDI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01C2AD440C213D362E2CEFB023E63F1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Object Recognition Algorithms, Performance, Experimentation Content-based image retrieval, Vector space model, Feature extraction, Query expansion, Relevance feedback, Data fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the contribution of CINDI group to the ImageCLEF 2007 ad-hoc retrieval tasks. We experiment with multi-modal (e.g., image and text) interaction and fusion approaches based on relevance feedback information for image retrieval tasks of photographic and medical image collections. For a text-based image search, keywords from the annotated files are extracted and indexed by employing the vector space model of information retrieval. For a content-based image search, various global, semi-global, region-specific and visual concept-based features are extracted at different levels of image abstraction. Based on relevance feedback information, multiple textual and visual query refinements are performed and user's perceived semantics are propagated from one modality to another with query expansion. The feedback information also dynamically adjusts intra and inter-modality weights in linear combination of similarity matching functions. Finally, the top ranked images are obtained by performing both sequential and simultaneous retrieval approaches. The analysis of results of different runs are reported in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the 2007 ImageCLEF competition, CINDI research group has participated in two different tasks of ImageCLEF track: an ad-hoc retrieval from a photographic collection (e.g., IAPR data set) and ad-hoc retrieval from a medical collection (e.g., CASImage, MIR, PathoPic, Peir, endoscopic and myPACS data sets) <ref type="bibr" coords="2,202.27,110.53,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="2,217.20,110.53,7.01,10.46" target="#b1">2]</ref>. The goal of the ad-hoc task is given a multilingual statement describing a user information need along with example images, find as many relevant images as possible from the given collection. Our work exploits advantages of both text and image modalities by involving users in the retrieval loop for cross-modal interaction and integration. This paper presents our multi-modal retrieval methodologies, description of submitted runs, and analysis of retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text-Based Image Retrieval Approach</head><p>This section describes the text-based image retrieval approach where a user submits a query topic using keywords to retrieve images which are associated with retrieved annotation files. For a textbased search, it is necessary to prepare the document collection consisting of annotated XML and SGML files into an easily accessible representation. Each annotation file in the collection is linked to image(s) either in a one-to-one or one-to-many relationships. To incorporate a keyword-based search on these annotation files, we rely on the vector space model of information retrieval <ref type="bibr" coords="2,486.21,284.85,9.96,10.46" target="#b2">[3]</ref>. In this model, a document is represented as a vector of words where each word is a dimension in an Euclidean space. The indexing is performed by extracting keywords from selected elements of the XML and SGML documents depending on the image collection. Let, T = {t 1 , t 2 , • • • , t N } denotes the set of keywords (terms) in the collection. A document D j is represented as a vector in a N -dimensional space as</p><formula xml:id="formula_0" coords="2,90.00,343.56,423.00,24.39">f D j = [w j1 , • • • , w jk , • • • , w jN ] T . The element w jk = L jk * G k denotes the tf-idf weight [3] of term t k , k ∈ {1, • • • , N }, in a document D j .</formula><p>Here, the local weight is denoted as L jk = log(f jk ) + 1, where f jk is the frequency of occurrence of keyword t k in a document D j . The global weight G k is denoted as inverse document frequency as G k = log(M/M k ), where M k is the number of documents in which t k is found and M is the total number of documents in the collection. A query D q is also represented as an N -dimensional vector f D q = [w q1 , • • • , w qk , • • • , w qN ] T . To compare D q and D j , the cosine similarity measure is applied as follows Sim text (D q , D j ) = where w qk and w jk are the weights of the term t k in D q and D j respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Query Refinement by Relevance Feedback</head><p>Query reformulation is a standard technique for reducing ambiguity due to word mismatch problem in information retrieval <ref type="bibr" coords="2,191.94,540.17,9.96,10.46" target="#b3">[4]</ref>. In the present work, we investigate interactive way to generate multiple query representations and their integration in a similarity matching function by applying various relevance feedback methods. The relevance feedback technique prompts the user for feedback on retrieval results and then use that information on subsequent retrievals with the goal of increasing retrieval performance <ref type="bibr" coords="2,187.88,587.99,10.52,10.46" target="#b3">[4,</ref><ref type="bibr" coords="2,202.59,587.99,7.01,10.46" target="#b4">5]</ref>. We generate multiple query vectors by applying various relevance feedback methods. For the first method, we use the well known Rocchio algorithm <ref type="bibr" coords="2,453.34,599.94,10.52,10.46" target="#b5">[6]</ref> as follows</p><formula xml:id="formula_1" coords="2,178.93,619.82,334.07,32.63">f m Dq (Rocchio) = α f o Dq + β 1 |R| f D j ∈R f Dj -γ 1 | R| fD j ∈ R fDj<label>(2)</label></formula><p>where f m Dq and f o Dq are the modified and the original query vectors, R and R are the set of relevant and irrelevant document vectors and α, β, and γ are weights. This algorithm generally moves a new query point toward relevant documents and away from irrelevant documents in feature space <ref type="bibr" coords="2,90.01,701.21,9.96,10.46" target="#b5">[6]</ref>. For our second feedback method, we use the Ide-dec-hi formula as</p><formula xml:id="formula_2" coords="2,204.21,723.55,308.79,25.63">f m D q (Ide) = α f o D q + β f D j ∈R f Dj -γ max R (f Dj )<label>(3)</label></formula><p>where max R(f Dj ) is a vector of the highest ranked non-relevant document. This is a modified version of the Rocchio's formula which eliminates the normalization for the number of relevant and non-relevant documents and allows limited negative feedback from only the top-ranked nonrelevant document. For the experimental purpose, we consider the weights as α = 1, β = 1, and γ = 1. We also perform two different query reformulation based on local analysis. Generally, local analysis considers the top k most highly ranked documents for query expansion without any assistance from the user <ref type="bibr" coords="3,196.61,194.22,15.50,10.46" target="#b11">[12,</ref><ref type="bibr" coords="3,215.09,194.22,7.01,10.46" target="#b2">3]</ref>. However, in this work, we consider only the user selected relevant images for further analysis. At first, a simpler approach of query expansion is considered based on identifying most frequently occurring five keywords from user selected relevant documents. After selecting the additional keywords, the query vector is reformulated as f m D q (Local1) by reweighting its keywords based on the tf-idf weighting scheme and is re-submitted to the system as a new query. The other query reformulation approach is based on expanding the query with terms correlated to the query terms. Such correlated terms are those present in local clusters built from the relevant documents as indicated by the user. There are many ways to build a local cluster before performing any query expansion <ref type="bibr" coords="3,322.90,289.86,15.50,10.46" target="#b11">[12,</ref><ref type="bibr" coords="3,342.21,289.86,7.01,10.46" target="#b2">3]</ref>. For this work, a correlation matrix <ref type="bibr" coords="3,237.16,301.82,10.52,10.46" target="#b7">[8]</ref> in which the rows and columns are associated with terms in a local vocabulary T l . The element of this matrix c u,v is defined as</p><formula xml:id="formula_3" coords="3,90.00,301.82,143.78,12.33">C (|T l |×|T l |) = [c u,v ] is constructed</formula><formula xml:id="formula_4" coords="3,253.58,331.88,259.42,24.93">c u,v = n u,v n u + n v -n u,v<label>(4)</label></formula><p>where, n u is the number of local documents which contain term t u , n v is the number of local documents which contain term t v , and n u,v is the number of local documents which contain both terms t u and t v . Here, c u,v measures the ratio between the number of local documents where both t u and t v appear and the total number of local documents where either t u or t v appear. If t u and t v have many co-occurrences in documents, then the value of c u,v increases, and the documents are considered to be more correlated. Now, given the correlation matrix C, we use it to build the local correlation cluster. For a query term t u ∈ D q , we consider the u-th row in C (i.e., the row with all the correlations for the keyword t u ). From that row, we return three largest correlation values c u,l , u = l, and add corresponding terms t l for query expansion. The process is continued for each query term and finally the query vector is reformulated as f m Dq (Local2) by re-weighting its keywords based on the tf-idf weighting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content-based Image Retrieval Approach</head><p>In content-based image retrieval (CBIR), access to information is performed at a perceptual level based on automatically extracted low-level features (e.g., color, texture, shape, etc.) <ref type="bibr" coords="3,470.93,552.45,14.61,10.46" target="#b12">[13]</ref>. The performance of a content-based image retrieval (CBIR) system depends on the underlying image representation, usually in the form of a feature vector. To generate feature vectors, various global, semi-global, region-specific, and visual concept-based image features are extracted at different levels of abstraction. The MPEG-7 based Edge Histogram Descriptor (EHD) and Color Layout Descriptor (CLD) are extracted for image representation at global level <ref type="bibr" coords="3,408.79,612.23,14.61,10.46" target="#b13">[14]</ref>. To represent EHD as vector f ehd , a histogram with 16 × 5 = 80 bins is obtained. The CLD represents spatial layout of images in a very compact form in YCbCr color space where Y is the luma component and Cb and Cr are the blue and red chroma components <ref type="bibr" coords="3,311.37,648.10,14.61,10.46" target="#b13">[14]</ref>. In this work, CLD with 10 Y , 3 Cb and 3 Cr coefficients is extracted to form a 16-dimensional feature vector f cld . The global distance measure between feature vectors of query image I q and database image I j is a weighted Euclidean distance measure and is defined as</p><formula xml:id="formula_5" coords="3,157.59,704.30,355.41,15.52">Dis global (I q , I j ) = ω cld Dis cld (f cld Iq , f cld Ij ) + ω ehd Dis ehd (f ehd Iq , f ehd Ij ),<label>(5)</label></formula><p>where, Dis cld (f cld I q , f cld I j ) and Dis ehd (f ehd I q , f ehd I j ) are the Euclidean distance measures for CLD and EHD respectively and ω cld and ω ehd are weights for each feature distance measure subject to 0 ≤ ω cld , ω ehd ≤ 1 and ω cld + ω ehd = 1 and initially adjusted with equal weights as ω cld = 0.5 and ω ehd = 0.5. For semi-global feature vector, a simple grid-based approach is used to divide the images into five overlapping sub-images <ref type="bibr" coords="4,263.62,134.45,14.61,10.46" target="#b15">[16]</ref>. Several moment based color and texture features are extracted from each of the sub-images and later they are combined to form a semi-global feature vector. The mean and standard deviation of each color channel in HSV color space are extracted form each overlapping sub-region of an image I j . Various texture moment-based features (such as energy, maximum probability, entropy, contrast and inverse difference moment) are also extracted from the grey level co-occurrence matrix (GLCM) <ref type="bibr" coords="4,319.41,194.22,14.61,10.46" target="#b14">[15]</ref>. Color and texture feature vectors are normalized and combined to form a joint feature vector f sg rj of each sub-image r and finally they are combined as the semi-global feature vector for an entire image as f sg . The semi-global distance measure between I q and I j is defined as</p><formula xml:id="formula_6" coords="4,176.88,251.93,336.12,31.42">Dis s-global (I q , I j ) = D sg (f sg I q , f sg I j ) = 1 r 5 r=1 ω r Dis r (f sg rq , f sg rj )<label>(6)</label></formula><p>where, Dis r (f sg r q , f sg r j ) is the Euclidean distance measure of the feature vector of region r and ω r are the weights for the regions, which are set as equal initially.</p><p>Region-based image retrieval (RBIR) aims to overcome the limitations of global and semiglobal retrieval approaches by fragmenting an image automatically into a set of homogeneous regions based on color and/or texture properties. Hence, we consider a local region specific feature extraction approach by fragmenting an image automatically into a set of homogeneous regions made up of (2 × 2) pixel blocks based on a fast k-means clustering technique. The image level distance between I q and I j is measured by integrating properties of all regions in the images. Suppose, there are M regions in image I q and N regions in image I j . Now, the image-level distance is defined as Dis local (I q , I j ) = M i=1 w ri q Dis ri q (q, j)</p><formula xml:id="formula_7" coords="4,335.14,418.76,173.62,28.77">+ N k=1 w r k j Dis r k j (j, q) 2 (<label>7</label></formula><formula xml:id="formula_8" coords="4,508.76,430.24,4.24,10.46">)</formula><p>where w r iq and w r k j are the weights (e.g., number of image block as unit) for region r i q and region r kj of image I q and I j respectively. For each region r iq ∈ I q , Dis ri q (q, j) is defined as the minimum Bhattacharyya distance <ref type="bibr" coords="4,238.38,481.06,15.50,10.46" target="#b17">[18]</ref> between this region and any region r k j ∈ I j as Dis r iq (q, j) = min(Dis(r i q , r 1 j ), • • • , Dis(r i q , r N j )). The Bhattacharyya distance is computed based on mean color vector and covariance matrix of color channels in HSV color space of each region. The details of the segmentation, local feature extraction and similarity matching schemes were described in our previous work in <ref type="bibr" coords="4,165.66,528.87,14.61,10.46" target="#b15">[16]</ref>.</p><p>We also extract visual concept-based image features that is analogous to a keyword-based representation in text retrieval domain. The visual concepts depict perceptually distinguishable color or texture patches in local image regions. For example, a predominant yellow color patch can be presented either in an image of the sun or in a sunflower image. To generate a set of visual concepts analogous to a dictionary of keywords, we consider a fixed decomposition approach to generate a 16 × 16 grid based partition of images. Therefore, sample images from a training set are equally partitioned into 256 non-overlapping smaller blocks. To represent each block as a feature vector, color and texture moment-based features are extracted as described for the semiglobal feature. To generate a coodbook of prototype concept vectors from the block features, we use a SOM-based clustering technique <ref type="bibr" coords="4,260.20,648.43,14.61,10.46" target="#b16">[17]</ref>. The basic structure of a SOM consists of two layers: an input layer and a competitive output layer. The input layer consists of a set of input node vector</p><formula xml:id="formula_9" coords="4,90.00,671.26,423.01,24.39">X = {x 1 , • • • x i , • • • x n }, x i ∈ d , while the output layer consists of a set of N neurons C = {c 1 , • • • c j , • • • c N },</formula><p>where each neuron c j is associated with a weight vector c j ∈ d . After the weight vectors are determined through the learning process, each output neuron c j resembles as a visual concept with the associated weight vector c j as code vector of a codebook. To encode an image, it is also decomposed into an even gird-based partition, where the color and texture moment-based features are extracted from each block. Now, for joint color and texture momentbased feature vector of each block, the nearest output node c k , 1 ≤ k ≤ N is identified by applying the Euclidean distance measure and the corresponding index k of the output node c k is stored for that particular block of the image. Based on this encoding scheme, an image I j can be represented as a vector f</p><formula xml:id="formula_10" coords="5,145.33,133.64,151.06,15.04">V-concept Ij = [f 1j , • • • , f ij , • • • f N j ] T</formula><p>, where each dimension corresponds to a concept index in the codebook. The element f ij represents the frequency of occurrences of c i appearing in I j . For this work, codebooks of size of 400 (e.g.,20 × 20 units) are constructed for the photographic and medical collection by manually selecting 2% images from each collection as training set. Since, the concept-based feature space is closely related to the keyword-based feature space of documents, we apply the cosine measure to compare image I q and I j as described in equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Query Refinement by Relevance Feedback</head><p>This section presents the visual query refinement approach at different levels of image representation. The query refinement is closely related to the approach in <ref type="bibr" coords="5,365.37,253.90,9.96,10.46" target="#b8">[9]</ref>. It is assumed that, all positive feedback images at some particular iteration belong to user perceived visual and/or semantic category and obey the Gaussian distribution to form a cluster in the feature space. We consider the rest of the images as irrelevant and they may belong to different semantic categories. However, we do not consider the irrelevant images for query refinement. The modified query vector at a particular iteration is represented as the mean of the relevant image vectors</p><formula xml:id="formula_11" coords="5,261.74,333.00,251.26,30.93">f x m Iq = 1 |R| f I l ∈R f x I l (8)</formula><p>where, R is the set of relevant image vectors and x ∈ {global, sg, V -concept}. Next, the covariance matrix of the positive feature vectors is estimated as</p><formula xml:id="formula_12" coords="5,213.06,407.51,299.94,32.62">C x = 1 |R| -1 |R| l=1 (f x m I l -f x Iq )(f x m I l -f x Iq ) T<label>(9)</label></formula><p>However, singularity issue will arise in covariance matrix estimation if fewer training samples or positive images are available compared to the feature dimension (as will be the case in user feedback images). So, we add regularization to avoid singularity in matrices as follows <ref type="bibr" coords="5,464.34,473.05,15.89,10.46" target="#b18">[19]</ref>:</p><formula xml:id="formula_13" coords="5,258.60,493.41,249.97,13.01">Ĉx = αC x + (1 -α)I (<label>10</label></formula><formula xml:id="formula_14" coords="5,508.57,495.96,4.43,10.46">)</formula><p>for some 0 ≤ α ≤ 1 and I is the identity matrix. After generating the mean vector and covariance matrix for a feature x ∈ {global, sg, V -concept}, we adaptively adjust the distance measure functions in equations ( <ref type="formula" coords="5,197.97,541.79,4.24,10.46" target="#formula_5">5</ref>) and ( <ref type="formula" coords="5,236.42,541.79,4.24,10.46" target="#formula_6">6</ref>) with the following Mahalanobis distance measures <ref type="bibr" coords="5,480.75,541.79,15.50,10.46" target="#b17">[18]</ref> for query image I q and database image I j as Dis x (I q , I j ) = (f</p><formula xml:id="formula_15" coords="5,277.26,574.59,235.74,17.39">x m I q -f x I j ) T Ĉx -1 (f x m I q -f x I j )<label>(11)</label></formula><p>The Mahalanobis distance differs from the Euclidean distance in that it takes into account the correlations of the data set and is scale-invariant, i.e. not dependent on the scale of measurements <ref type="bibr" coords="5,90.01,624.88,14.61,10.46" target="#b17">[18]</ref>. We did not perform any query refinement for region-specific feature due to its variable feature dimension for variable number of regions in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combination of Evidence by Dynamic Weight Update</head><p>In recent years, the category of work known as data fusion or multiple-evidence described a range of techniques in information retrieval whereby multiple pieces of information are combined to achieve improvements in retrieval effectiveness <ref type="bibr" coords="5,302.24,715.50,15.50,10.46" target="#b9">[10,</ref><ref type="bibr" coords="5,322.44,715.50,11.62,10.46" target="#b10">11]</ref>. These pieces of information can take many forms including different query representations, different document (image) representations, and different retrieval strategies used to obtain a measure of relationship between a query and a document (image). Motivated by this paradigm, in Sections 2 and 3, we described multiple textual query and image representation schemes. This section presents an adaptive linear combination approach based on relevance feedback information. One of the most commonly used approaches in data fusion is the linear combination of similarity scores. For our multi-modal retrieval purpose, let us consider q as a multi-modal query which has an image part as I q and a document part as annotation file as D q . In a linear combination scheme, the similarity between q and a multi-modal item j, which also has two parts (e.g., image I j and text D j ), is defined as Sim(q, j) = ω I Sim I (I q , I j ) + ω D Sim D (D q , D j ) <ref type="bibr" coords="6,495.29,203.09,17.71,10.46" target="#b11">(12)</ref> where ω I and ω D are inter-modality weights within the text or image feature space, which subject to 0 ≤ ω I , ω D ≤ 1 and ω I + ω D = 1. Now, the image based similarity is again defined as the linear combination of similarity measures in different level of image representation as</p><formula xml:id="formula_16" coords="6,223.01,268.28,285.56,23.57">Sim I (I q , I j ) = IF ω IF I Sim IF I (I q , I j ) (<label>13</label></formula><formula xml:id="formula_17" coords="6,508.57,270.08,4.43,10.46">)</formula><p>where IF ∈ {global, semi -global, region, V -concept} and ω IF are the weights within the different image representation schemes (e.g., intra-modality weights). On the other hand, the text based similarity is defined as the linear combination of similarity matching based on different query representation schemes.</p><formula xml:id="formula_18" coords="6,211.07,357.12,301.93,24.00">Sim D (D q , D j ) = QF ω QF D Sim QF D (D q , D j )<label>(14)</label></formula><p>where QF ∈ {Rocchio, Ide, Local1, Local2} and ω QF are the weights within the different query representation schemes. The effectiveness of the linear combination depends mainly on the choice of the different inter and intra-modality weights. We use a dynamic weight updating method in linear combination schemes by considering both precision and rank order information of top retrieved K images. Before any fusion, the distance scores of each representation are normalized and converted to the similarity scores with a range of [0, 1] as Sim(q, j) = 1 -Dis(q,j)-min(Dis(q,j)) max(Dis(q,j))-min(Dis(q,j)) , where min(•) and max(•) are the minimum and maximum distance scores. In this approach, an equal emphasis is given based on their weights to all the features along with their similarity matching functions initially. However, the weights are updated dynamically during the subsequent iterations by incorporating the feedback information from the previous round. To update the inter-modality weights (e.g., ω I and ω D ), we at first need to perform the multi-modal similarity matching based on equation <ref type="bibr" coords="6,143.77,541.11,16.38,10.46" target="#b11">(12)</ref>. After the initial retrieval result with a linear combination of equal weights (e.g., ω I = 0.5 and ω D = 0.5), a user needs to provide a feedback about the relevant images from the top K returned images. For each ranked list based on individual similarity matching, we also consider top K images and measure the effectiveness of a query/image feature as</p><formula xml:id="formula_19" coords="6,227.41,596.63,281.16,26.64">E(D or I) = K i=1 Rank(i) K/2 * P(K) (<label>15</label></formula><formula xml:id="formula_20" coords="6,508.57,605.97,4.43,10.46">)</formula><p>where Rank(i) = 0 if image in the rank position i is not relevant based on user's feedback and Rank(i) = (K -i)/(K -1) for the relevant images. Here, P (K) = R K /K is the precision at top K, where R k be the number of relevant images in the top K retrieved result. Hence, the equation ( <ref type="formula" coords="6,94.42,668.50,8.86,10.46" target="#formula_19">15</ref>) is basically the product of two factors, rank order and precision. The raw performance scores obtained by the above procedure are then normalized by the total score as Ê(D) = ωD =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(D) E(D)+E(I)</head><p>and Ê(I) = ωI = E(I) E(D)+E(I) to generate the updated text and image feature weights respectively. For the next iteration of retrieval with the same query, these modified weights are utilized for the multi-modal similarity matching function as Sim(q, j) = ωI Sim I (I q , I j ) + ωD Sim D (D q , D j ) <ref type="bibr" coords="6,495.29,744.16,17.71,10.46" target="#b15">(16)</ref> This weight updating process might be continued as long as users provide relevant feedback information or until no changes are noticed due to the system convergence.</p><p>In a similar fashion, to update the intra-modality weights (e.g., ω QF D and ω IF I ), we need to consider the top K images in individual result list. So, for image-based similarity in equation ( <ref type="formula" coords="7,482.37,146.40,8.19,10.46" target="#formula_16">13</ref>), we consider the result lists of different image features of IF ∈ {global, semi -global, region, V -concept} and measure their weights by using equation <ref type="bibr" coords="7,291.88,170.31,17.71,10.46" target="#b14">(15)</ref> for the next retrieval iteration. On the other hand, for text-based similarity in equation ( <ref type="formula" coords="7,280.69,182.26,8.19,10.46" target="#formula_18">14</ref>), the top K images in result lists of different query features of QF ∈ {Rocchio, Ide, Local1, Local2} are considered and text-level weights are determined in a similar way by applying equation <ref type="bibr" coords="7,287.95,206.18,16.38,10.46" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sequential approach with pre-filtering and re-ordering</head><p>This section describes the process about how to interact with both the modalities in a user's perceived semantical and sequential way. Since a query can be represented with both keywords and visual features, it can be initiated either by the keyword-based search or by the visual example image search. However, we consider a pre-filtering and re-ranking approach based on the image search in the filtered image set which is obtained previously by the textual search. It would be more appropriate to perform a text-based search at first due to the higher level information content and latter use visual only search to refine or re-rank the top returned images by the textual search. In this method, combining the results of the text and image based retrieval is a matter of re-ranking or re-ordering of the images in a text-based pre-filtered result set. The steps involved in this approach are as follows:</p><p>Step 1: Initially, for a multi-modal query q with a document part as D q , perform a textual search with vector f D q and rank the images based on the ranking of the associated annotation files by applying equation (1).</p><p>Step 2: Obtain user's feedback from top retrieved K = 30 images about relevant and irrelevant images for the textual query refinement.</p><p>Step 3: Calculate the optimal textual query vectors as f m D q (Rocchio), f m D q (Ide), f m D q (Local1) and f m D q (Local2).</p><p>Step 4: Re-submit the modified query vectors in the text engine and merge the results with an equal weighting in similarity matching in equation <ref type="bibr" coords="7,326.70,489.31,16.38,10.46" target="#b13">(14)</ref>.</p><p>Step 5: Continue steps 2 to 4 with dynamically updated weights based on equation ( <ref type="formula" coords="7,476.19,501.27,8.86,10.46" target="#formula_19">15</ref>) until the user switch to visual only search.</p><p>Step 6: Extract different features as</p><formula xml:id="formula_21" coords="7,262.09,522.93,67.87,15.46">f global Iq , f sg Iq , f local I q</formula><p>, and f V-concept</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iq</head><p>for the multi-modal query q with an image part as I q .</p><p>Step 7: Perform visual only search in top L = 1000 images retrieved by text-based search and rank them based on the similarity values by applying equation <ref type="bibr" coords="7,365.54,561.05,17.71,10.46" target="#b12">(13)</ref> with equal feature weighting.</p><p>Step 8: Obtain user's feedback from top retrieved K = 30 images about the relevant images and perform visual query refinement as f x m I q , where x ∈ {global, sg, V -concept} at a particular iteration.</p><p>Step 9: At next iteration, calculate the feature weights based on equation <ref type="bibr" coords="7,427.26,608.86,17.71,10.46" target="#b14">(15)</ref> and apply it to equation (13) for ranked based retrieval result.</p><p>Step 10: Continue steps 8 and 9, until the user is satisfied or the system converges.</p><p>The process flow diagram of the sequential search approach is shown in Figure <ref type="figure" coords="7,463.17,644.73,3.87,10.46" target="#fig_1">1</ref>. For this approach, the text-based search with query reformulation is performed first as shown in the (1) left portion of the figure and image-based search is performed in the filtered image set as shown in the (1) right portion of the figure <ref type="figure" coords="7,250.32,680.59,3.87,10.46" target="#fig_1">1</ref>. This section describes our approach of simultaneous multi-modal search. Here, textual and content-based search are performed simultaneously from the beginning and the results are combined with an adaptive linear combination scheme as described in Section4. The steps involved in this approach are as follows:</p><p>Step 1: Initially, for a multi-modal query q with a document part as D q and an image part as I q , extract textual query vector as f D q and different image feature vectors as</p><formula xml:id="formula_22" coords="8,422.79,405.60,67.87,15.46">f global Iq , f sg Iq , f local I q</formula><p>, and</p><formula xml:id="formula_23" coords="8,90.00,421.04,46.58,15.04">f V-concept Iq .</formula><p>Step 2: Perform a multi-modal search to rank the images based on equation <ref type="bibr" coords="8,461.82,436.66,16.38,10.46" target="#b11">(12)</ref>, where Sim D (D q , D j ) is initially performed through Sim text (D q , D j ) equation ( <ref type="formula" coords="8,417.35,448.61,4.24,10.46">1</ref>) and Sim I (I q , I j ) is performed through equation ( <ref type="formula" coords="8,221.61,460.56,8.86,10.46" target="#formula_16">13</ref>) with initially equal weighting in both inter and intra-modality weights.</p><p>Step 3: Obtain user's feedback from top retrieved K = 30 images about relevant and irrelevant images for both textual and visual query refinement and for dynamically update the weights.</p><p>Step 4: Based on the feedback information, calculate the optimal textual query vectors as f m D q (Rocchio), f m D q (Ide), f m D q (Local1) and f m D q (Local2) and image query vectors as f x m I q , where x ∈ {global, sg, V -concept} and update the inter and intra-modality weights based on equation <ref type="bibr" coords="8,492.53,533.51,16.38,10.46" target="#b14">(15)</ref>.</p><p>Step 5: Re-submit the modified textual and image query vectors to the system and apply multimodal similarity matching based on equation <ref type="bibr" coords="8,297.07,557.43,16.38,10.46" target="#b15">(16)</ref>, where Sim D (D q , D j ) is performed through equation ( <ref type="formula" coords="8,135.10,569.38,8.86,10.46" target="#formula_18">14</ref>) and Sim I (I q , I j ) is performed through equation <ref type="bibr" coords="8,360.65,569.38,16.38,10.46" target="#b12">(13)</ref>.</p><p>Step 6: Continue steps 3 to 5, until the user is satisfied or the system converges.</p><p>The process flow diagram of the above multi-modal simultaneous search approach is shown in Figure <ref type="figure" coords="8,121.77,605.24,3.87,10.46" target="#fig_2">2</ref>. For this approach, both text and image-based search are performed simultaneously as shown in left and right portions of Figure <ref type="figure" coords="8,274.65,617.20,3.87,10.46" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.0.1">Analysis of the submitted runs</head><p>The types and performances of the different runs are shown in Table <ref type="table" coords="8,389.82,661.49,4.98,10.46" target="#tab_0">1</ref> and Table <ref type="table" coords="8,444.06,661.49,4.98,10.46" target="#tab_1">2</ref> for the ad-hoc retrieval of the photographic and medical collections respectively. In all these runs, only English is used as the source and target language without any translation for the text-based retrieval approach. We submitted five different runs for the ad-hoc retrieval of the photographic collection, where first two runs are based on text only search and last three runs are based on mixed modality search as shown in Table <ref type="table" coords="8,201.40,721.27,3.87,10.46" target="#tab_0">1</ref>. For the first run "CINDI-TXT-ENG-PHOTO", we performed only a manual text-based search without any query expansion as our base run. This run achieved a MAP  files instead of using the link XML file as provided. We are currently fixing this problem and later analyze and report the results of these runs. Table <ref type="table" coords="10,347.80,249.94,4.98,10.46" target="#tab_1">2</ref> shows the official result of the four runs out of our seven submitted runs. In the first run "INDI-IMG-FUSION", we performed only a visual only search based on various image feature representation schemes as described in Section 3 without any feedback information and with a linear combination of equal feature weights. For the second run "CINDI-IMG-FUSION-RF", we performed only one iteration of manual feedback for visual query refinement and combined the similarity matching functions based on the dynamic weight updating scheme. For this run we achieved a MAP score of 0.0372, which is slightly better then the score (0.0333) achieved by the first run without any relevance feedback information. However, compared to the the text-based approaches the performances are very low as it happened in previous years of ImageCLEFmed. For the third run "CINDI-TXT-IMAGE-LINEAR", we performed a simultaneous retrieval approach without any feedback information with a linear combination of weights as ω D = 0.7 and ω I = 0.3 and for the fourth run "CINDI-TXT-IMG-RF-LINEAR", two iterations of manual relevance feedback are performed similar to the last two runs of photographic retrieval task. From Table <ref type="table" coords="10,394.84,405.36,3.87,10.46" target="#tab_1">2</ref>, it is clear that combining both modalities for the medical retrieval task is far better then using only a single modality (e.g., only image) and we achieved the best MAP score as 0.1483 among all our submissions for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presents the ad-hoc image retrieval approaches of CINDI research group for Image-CLEF 2007. We submitted several runs with different combination of methods, features and parameters. We investigated with cross-modal interaction and fusion approaches for the retrieval of the photographic and medical image collections. The description of the runs and analysis of the results are discussed in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,320.44,437.48,6.31,7.32;2,320.44,445.48,14.49,7.32;2,337.09,439.95,7.13,10.46;2,344.23,443.98,8.13,7.32;2,355.24,439.95,14.32,10.46;2,369.56,443.98,7.93,7.32;2,296.30,456.46,6.31,7.32;2,296.30,464.47,14.49,7.32;2,311.29,458.93,11.01,10.46;2,322.30,462.96,8.13,7.32;2,331.10,458.59,15.53,10.80;2,369.33,456.46,6.31,7.32;2,369.33,464.47,14.49,7.32;2,384.32,458.93,11.01,10.46;2,395.33,462.96,7.93,7.32;2,403.93,458.59,7.84,10.80;2,500.27,446.82,12.73,10.46"><head>N</head><label></label><figDesc>k=1 w qk * w jk N k=1 (w qk ) 2 * N k=1 (w jk ) 2 (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,174.62,280.95,253.78,10.46;8,164.70,108.53,274.07,158.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Process flow diagram of the sequential approach</figDesc><graphic coords="8,164.70,108.53,274.07,158.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,168.09,294.86,266.84,10.46;9,164.70,109.28,273.94,172.12"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Process flow diagram of the simultaneous approach</figDesc><graphic coords="9,164.70,109.28,273.94,172.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,96.68,325.62,409.65,118.00"><head>Table 1 :</head><label>1</label><figDesc>Results of the ImageCLEFphoto Retrieval task</figDesc><table coords="9,96.68,341.51,409.65,102.12"><row><cell>Run ID</cell><cell cols="3">Modality Run Type QE/RF</cell><cell>MAP</cell><cell>BPREF</cell></row><row><cell>CINDI-TXT-ENG-PHOTO</cell><cell>TXT</cell><cell>Manual</cell><cell>NOFB</cell><cell>0.1529</cell><cell>0.1426</cell></row><row><cell>CINDI-TXT-QE-PHOTO</cell><cell>TXT</cell><cell>Manual</cell><cell>FBQE</cell><cell>0.2637</cell><cell>0.2515</cell></row><row><cell>CINDI-TXT-QE-IMG-RF-RERANK</cell><cell>MIXED</cell><cell>Manual</cell><cell>FBQE</cell><cell>0.2336</cell><cell>0.2398</cell></row><row><cell>CINDI-TXTIMG-FUSION-PHOTO</cell><cell>MIXED</cell><cell>Manual</cell><cell>NOFB</cell><cell>0.1483</cell><cell>0.1620</cell></row><row><cell>CINDI-TXTIMG-RF-PHOTO</cell><cell>MIXED</cell><cell>Manual</cell><cell>FBQE</cell><cell>0.1363</cell><cell>0.1576</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,110.98,117.46,381.04,97.74"><head>Table 2 :</head><label>2</label><figDesc>Results of the Medical Retrieval task</figDesc><table coords="10,110.98,131.40,381.04,83.79"><row><cell>Run ID</cell><cell cols="3">Modality Run Type QE/RF</cell><cell>MAP</cell><cell>R-prec</cell></row><row><cell>CINDI-IMG-FUSION</cell><cell>IMAGE</cell><cell>Manual</cell><cell>NOFB</cell><cell cols="2">0.0333 0.0532</cell></row><row><cell>CINDI-IMG-FUSION-RF</cell><cell>IMAGE</cell><cell>Manual</cell><cell>FBQE</cell><cell cols="2">0.0372 0.0549</cell></row><row><cell>CINDI-TXT-IMAGE-LINEAR</cell><cell>MIXED</cell><cell>Manual</cell><cell>NOFB</cell><cell cols="2">0.1659 0.2196</cell></row><row><cell>CINDI-TXT-IMG-RF-LINEAR</cell><cell>MIXED</cell><cell>Manual</cell><cell>FBQE</cell><cell cols="2">0.0823 0.1168</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.48,597.54,402.53,10.46;10,110.48,609.49,402.53,10.46;10,110.48,621.46,44.28,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,343.83,597.54,169.17,10.46;10,110.48,609.49,105.80,10.46">Overview of the ImageCLEF 2007 Photographic Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.47,609.49,192.84,10.46">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,640.17,402.52,10.46;10,110.48,652.13,402.52,10.46;10,110.48,664.09,294.13,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,110.48,652.13,356.08,10.46">Overview of the ImageCLEFmed 2007 Medical Retrieval and Annotation Tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kalpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,475.93,652.13,37.07,10.46;10,110.48,664.09,151.27,10.46">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,682.80,398.44,10.46" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,274.67,682.80,129.55,10.46">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribiero-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,701.53,402.52,10.46;10,110.48,713.48,334.27,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,226.15,701.53,234.83,10.46">Improving retrieval performance by relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,468.48,701.53,44.52,10.46;10,110.48,713.48,198.19,10.46">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,732.21,402.52,10.46;10,110.48,744.16,316.68,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,202.26,732.21,310.74,10.46;10,110.48,744.16,37.76,10.46">Relevance Feedback: A Power Tool for Interactive Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,155.75,744.16,153.04,10.46">IEEE Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,110.53,402.52,10.46;11,110.48,122.49,402.53,10.46;11,110.48,134.45,66.41,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,169.57,110.53,186.10,10.46">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,374.67,110.53,138.32,10.46;11,110.48,122.49,206.32,10.46">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, Prentice Hall, Inc</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,154.37,402.53,10.46;11,110.48,166.33,237.32,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,141.44,154.37,164.90,10.46">New experiments in relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,325.02,154.37,187.99,10.46;11,110.48,166.33,151.60,10.46">The SMART retrieval system -Experiments in Automatic Document Processing</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="337" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,186.26,402.54,10.46;11,110.48,198.21,402.52,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,286.24,186.26,226.78,10.46;11,110.48,198.21,176.30,10.46">A fuzzy document retrieval system using the keyword connection matrix and a learning method</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,294.81,198.21,99.50,10.46">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="163" to="179" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,218.13,402.53,10.46;11,110.48,230.09,402.51,10.46;11,110.48,242.04,22.69,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,321.39,218.13,191.62,10.46;11,110.48,230.09,81.05,10.46">MindReader: Querying Databases Through Multiple Examples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.26,230.09,204.58,10.46">24th Internat. Conf. on Very Large Databases</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="24" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,261.97,402.53,10.46;11,110.48,273.92,335.61,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,223.03,261.97,146.58,10.46">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,377.48,261.97,135.53,10.46;11,110.48,273.92,94.09,10.46">Proc. of the 2nd Text Retrieval Conference (TREC-2)</title>
		<title level="s" coord="11,213.20,273.92,111.12,10.46">NIST Special Publication</title>
		<meeting>of the 2nd Text Retrieval Conference (TREC-2)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,293.85,402.53,10.46;11,110.48,305.81,227.24,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,151.37,293.85,331.53,10.46">Combining Multiple Evidence from Different Properties of Weighting Schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,490.33,293.85,22.68,10.46;11,110.48,305.81,137.34,10.46">Proc. of the 18th Annual ACM-SIGIR</title>
		<meeting>of the 18th Annual ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,325.73,402.52,10.46;11,110.48,337.69,131.72,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,237.93,325.73,191.15,10.46">Local feedback in full-text retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Attar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Fraenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,437.63,325.73,68.87,10.46">Journal of ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="417" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,357.61,402.53,10.46;11,110.48,369.57,402.51,10.46;11,110.48,381.52,73.61,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,363.85,357.61,149.15,10.46;11,110.48,369.57,115.32,10.46">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,236.89,369.57,220.87,10.46">IEEE Trans. on Pattern Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,401.45,402.52,10.46;11,110.48,413.40,297.63,10.46" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,324.46,401.45,188.54,10.46;11,110.48,413.40,111.10,10.46">Introduction to MPEG-7-Multimedia Content Description Interface</title>
		<editor>B.S. Manjunath, P. Salembier, T. Sikora</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>John Wiley Sons Ltd</publisher>
			<biblScope unit="page" from="187" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,433.33,402.52,10.46;11,110.48,445.28,258.62,10.46" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,307.53,433.33,174.45,10.46">Textural features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,488.89,433.33,24.11,10.46;11,110.48,445.28,140.20,10.46">IEEE Trans System, Man, Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,465.20,402.53,10.46;11,110.48,477.16,391.28,10.46" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,324.44,465.20,188.58,10.46;11,110.48,477.16,163.60,10.46">A Feature Level Fusion in Similarity Matching to Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,281.59,477.16,189.64,10.46">Proc. 9th Internat Conf Information Fusion</title>
		<meeting>9th Internat Conf Information Fusion</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,497.08,341.71,10.46" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="11,168.60,497.08,90.65,10.46">Self-Organizing Maps</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct coords="11,110.48,517.01,402.24,10.46" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m" coord="11,172.89,517.01,199.30,10.46">Introduction to Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct coords="11,110.48,536.93,402.52,10.46;11,110.48,548.90,115.68,10.46" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,167.78,536.93,147.65,10.46">Regularized Discriminant Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,322.64,536.93,185.97,10.46">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
