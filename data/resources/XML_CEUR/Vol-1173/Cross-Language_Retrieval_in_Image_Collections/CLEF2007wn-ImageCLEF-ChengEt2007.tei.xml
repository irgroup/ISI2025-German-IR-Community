<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.30,75.59,366.74,12.58">CYU_IM</title>
				<funder ref="#_neQjFCk">
					<orgName type="full">National Science Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.84,112.68,70.84,9.02"><forename type="first">Pei-Cheng</forename><surname>Cheng</surname></persName>
							<email>c.pccheng@cyu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">Ching Yun University</orgName>
								<address>
									<addrLine>229, Chien-Hsin Rd</addrLine>
									<settlement>Jung-Li</settlement>
									<country>Taiwan 320. R.O</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.39,112.68,64.18,9.02"><forename type="first">Wei-Pang</forename><surname>Yang</surname></persName>
							<email>wpyang@mail.ndhu.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Dong Hwa University</orgName>
								<address>
									<addrLine>1, Sec. 2, Da Hsueh Rd., Shou-Feng</addrLine>
									<settlement>Hualien</settlement>
									<country>Taiwan, R.O</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.30,75.59,366.74,12.58">CYU_IM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">375FF7D9B5CB0CABCDB9F06FBFA8EA1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Content-based image retrieval</term>
					<term>Pattern recognition</term>
					<term>Medical Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2007 Medical Automatic Annotation Task, base on the IRMA project a database of 11,000 fully classified radiographs was used to train a classification system and 1,000 radiographs have to be classified. Radiography medical image always contain particular anatomic regions (lung, liver, head, and so on). Thus, similar images have similar spatial structures. We proposed a relative vector representation that represents the local spatial relationship between pixels. In this experiment, we transform the gray value to relative vector which is an illumination invariant feature. We calculate the occurrence frequency and standard deviation of relative vector as the image feature. Based on the image feature we can find the similar image with less distance. Finally, we use the nearest neighbor method to classify the 1000 test images. In this task, we have submitted one run to medical annotation task. 1,000 radiographs for participants have to be classified. The score of our result is 79.303 which is the error count. The rank of our result is 33 of all 68 runs. The best score around all runs is 26.847 and the worst score of rank 68 is 505.618. The image feature we proposed is easy to implement and the performance of our method is good.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic image annotation is an important step when searching for images from a database. In this task a database of 11,000 fully classified radiographs is made available and can be used to train a classification system. 1,000 test radiographs for which classification labels are not available have to be classified. The automatic image annotation task does not contain any text as input for this task and is aimed at image analysis technologies. In this task, we proposed a relative vector representation that represents the local spatial relationship between pixels. In this experiment, we transform the gray value to relative vector which is an illumination invariant feature. We calculate the occurrence frequency and standard deviation of relative vector as the image feature. Based on the image feature we can find the similar image with less distance. Finally, we use the nearest neighbor method to classify the 1000 test images. The classification result consider the complete IRMA code to evaluate what level of detail the images will be annotated. Therefore, errors in the annotation will be counted depending on the depth in the tree, and the difficulty of the choice. The rest of this paper is organized as follows. In Section 2, the employed image features are described. Section 3 illustrates the submissions to the ImageCLEF 2007 Evaluation. Finally, Section 4 provides concluding remarks and future directions for medical image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image features</head><p>This section describes the features used in this paper for the ImageCLEF 2007 evaluation. In an image retrieval system, image features are extracted from pixels. The extracted features are then used for similarity comparison. For fast response, image features must be concise, and for precision, image features must contain meaningful information to represent the image itself. Image features will directly affect the retrieval result. In this paper we propose an illuminative invariant image features to emphasize the contrast of an image and handle images with little illuminative influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relative local image feature.</head><p>Color histogram <ref type="bibr" coords="2,138.84,209.46,40.81,9.02" target="#b2">[Swain91]</ref> is a basic method and has good performance with regards to representing image content. The color histogram method gathers statistics about proportion of each color as the signature of an image. However, in medical image data, the images are gray. Histogram method quantize the gray value into 256(0~255) levels, and calculate the ratio of each level as a histogram. The histogram method lacks spatial information between neighbors and illumination influence.</p><p>In this paper, we propose an illumination invariant relative feature to represent the content of medical image. The brightness will affect the gray value in digital computer. However, human's perception emphasizes the contrast rather than the illumination of an image. The representation we designed considers human's perception concomitant with low level image features that have illumination invariant character.</p><p>We consider the relationship between neighbor pixels as relative features and defined three relative relationships: equal, bigger, and less. Previous techniques using a single pixel as the basic unit of measure are dominated by the absolute gray value. We propose a relative local feature in which each pixel refers to two neighbor pixels as a basic unit of measure to generate relative vectors. Thus, the basic unit of measure is a relative vector and thus illumination invariant. The detailed definition of relative vector is defined as following:</p><p>Definion 1: Relation R, any two neighbor pixels p i and p j have a relationship; the relation R is defined as follows:</p><p>R</p><formula xml:id="formula_0" coords="2,85.99,404.94,184.95,43.52">(p i , p j )={ 0 if gray_value(p i )=gray_value(p j ) 1 if gray_value(p i )&gt;gray_value(p j ) 2 if gray_value(p i )&lt;gray_value(p j ) }</formula><p>As pixel(x,y) of image, the relative vector(RV) is defined as Definition 2.</p><p>Definition 2: Each pixel refers to two neighbor pixels (down, right) to generate a relative identify feature. RV(pixel(x,y))= &lt; R(pixel(x,y),pixel(x+1,y)), R(pixel(x,y),pixel(x,y+1)),R(pixel(x+1,y), pixel(x,y+1))&gt; All pixels of images will be translated into a relative vector, RV∈{[0-2],[0-2],[0-2]}. The relative vector total can be categorized into 27 classes, we can translate the relative vector RV(pixel(x,y)) into an integer for fast computations. We hash the relative vector into one of 27 classes by equation (1).</p><p>Let RV(pixel(x,y))=&lt; i, j, k&gt;, Class(RV(pixel(x,y)))=i*3^2+j*3^1+k (1)</p><p>The representation based on the relative relation between pixels to denote an image will have an illumination invariant character. Second, as in above definition each pixel refers to two neighbors (down, right) to generate the RV. The relative vector we proposed will have local spatial information between neighbor pixels. Up to now, an image will be translated into an illumination invariant representation. Each pixel refers to two neighboring pixels will fall into a class. Next, we partition an image into four areas and calculate the occurrence frequency, average distribution distance and spatial standard deviation of each area separate as the image feature. As an image, the final signature representation totally has 324 features. We will now describe the calculation of average distribution distance and spatial standard deviation.</p><p>There are n pixels in a class. (x i , y i ) are the coordinates of pixel p i, and (u x , u y ) is the centroid of the pixels. ud is the average distance of pixels from (u x , u y ). std is the standard deviation of pixels from (u x , u y ). The calculation equation is described as following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ ∑</head><formula xml:id="formula_1" coords="3,72.79,74.41,144.74,151.06">= = = = n i i y n i i x y n x n 1 1 1 , 1 μ μ ∑ = - + - = n i y i x i u y u x n d 1 2 2 ) ( ) ( 1 μ 2 2 ) ( ) ( y i x i i u y u x d - + - = ∑ = - = n i i ud d std 1 2 ) (</formula><p>For each class, we get the u d and std as the moment of spatial distribution to describe the global spatial relationship. An image will first be translated into an illumination invariant vector space. We calculate the occurrence frequency, average distance and standard deviation of each class as the feature of an image. As an image, signature representation totally has 324 vectors in total.</p><p>The similarity between two features is evaluated by a distance metric. Two images I a and I b that have smaller distance are more similar. The equation ( <ref type="formula" coords="3,266.96,296.85,3.90,9.05">2</ref>) is the distance metric formula.</p><formula xml:id="formula_2" coords="3,70.92,316.95,289.84,37.31">SIM_RF(I a, I b )= (2) ∑ - × + - × + × + - = 26 0 |) ) ( ) ( | | ) ( ) ( (| | ) ( ) ( | | ) ( ) ( | i b i a i b i a i b i a i b i a i I std I std I ud I ud I f I f I f I f λ α</formula><p>Where f i is the occurrence frequency of class i , ud i is the average distance of pixels from centroid of class i and std i is the standard deviation of distance with respect to the average distance. We set α=0.4 and λ=2 in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Correlogram feature.</head><p>Based on the relative vector, the extracted image feature is more stable than directly operate on gray value in different illumination. After the relative feature transform, we analyze the image pixels by modified correlogram algorithm. The definition of the correlogram <ref type="bibr" coords="3,254.65,474.95,30.07,9.02" target="#b6">[Ma97]</ref> is in Eq. (3). Let D denote a set of fixed distances {d 1 , d 2 , d 3 ,…, d n }. The correlogram of an image I is defined as the probability of a color pair (ci, cj) at a distance d.</p><formula xml:id="formula_3" coords="3,196.02,502.88,262.84,20.06">} | | { Pr ) ( 2 1 2 , , 2 1 d p p c p I j I p c p d c c i j i = - ∈ = ∈ ∈ γ .<label>(3)</label></formula><p>For computational efficiency, the autocorrelogram is defined in Eq. ( <ref type="formula" coords="3,346.47,538.19,3.89,9.02" target="#formula_4">4</ref>)</p><formula xml:id="formula_4" coords="3,199.32,560.42,255.65,20.64">} | | { Pr ) ( 2 1 2 , 2 1 d p p c p I i I p c p d c i i = - ∈ = ∈ ∈ λ . (<label>4</label></formula><formula xml:id="formula_5" coords="3,454.97,567.23,3.89,9.02">)</formula><p>Our modified correlogram algorithm works as follows. Any two pixels have a distance, and we estimate the probability that the distance falls within an interval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Submissions to the ImageCLEF 2007 Evaluation</head><p>In ImageCLEF 2007, the medical automatic annotation task considers the complete IRMA code and penalizes misclassifications at different levels of the code differently. For every code, the maximal possible error is calculated and the errors are normed such that a completely wrong decision (i.e. all positions wrong) gets an error count of 1 and a completely correctly classified image has an error of 0. A detailed description of the error counting scheme is in <ref type="bibr" coords="4,161.22,96.23,48.93,9.02" target="#b5">[Thomas07]</ref> Examples: correct: 318a classified error count 318a 0.0 318* 0.0244653860094 3187 0.0489307720188 31*a 0.0824574121058 31** 0.0824574121058 3177 0.164914824212 3*** 0.34342152954 32** 0.686843059079 1000 1.0</p><p>In this task, we have submitted one run to medical annotation task. 1,000 radiographs for participants have to be classified. The score of our result is 79.303 which is the error count. The rank of our result is 33 of all 68 runs. The best score around all runs is 26.847 and the score of rank 68 is 505.618. The image feature we proposed is easy to implement and the performance of our method is good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and future work</head><p>The medical image application is unlike general-propose images. In general propose images, the image representation always consider the invariance in image rotation, zooming and shift. Medical images have more stable camera settings than general propose images; therefore, the spatial information becomes very important in medical images, and we must improve the representation regarding spatial relation in this kind of images. On the other hand, radiography medical image is gray image that the similarity counting wills be influence by illumination. In this paper, an illumination invariant image feature is proposed for medical image data. The proposed image feature immunizes against illumination and has local spatial information. In the result also have good performance. In the future, we try to design a pyramid relative vector for fast medical image retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.92,669.58,88.06,12.16;3,179.23,668.86,5.44,12.16;3,142.08,680.43,15.44,8.51;3,141.24,665.75,6.95,6.26;3,146.10,682.67,3.48,6.26;3,151.92,665.75,6.95,6.26;3,157.73,682.67,3.48,6.26;3,199.68,672.06,3.31,8.94;3,160.44,672.06,3.31,8.94;3,139.68,682.67,13.39,6.26;3,194.64,676.55,3.48,6.26;3,172.02,676.55,3.48,6.26;3,186.66,672.06,7.17,8.94;3,163.86,672.06,7.17,8.94"><head>SIM_CF</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.92,607.85,453.60,54.98"><head></head><label></label><figDesc>We calculate the probability of each interval to form the correlogram vector. For 27 classes, we totally have 324 feature bins. The similarity between two features is evaluated by a distance metric. Two images I a and I b that have smaller distance are more similar.</figDesc><table coords="3,70.92,607.85,453.37,20.54"><row><cell>The distance intervals we set are {(0,2), (2,4), (4,6), (6,8),</cell></row><row><cell>(8,12), (12,16), (16,26), (26,36), (36,46), (46,56), (56,76), (76,100)}.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Science Council</rs> (grant number: <rs type="grantNumber">NSC-96-2221-E-259-017-MY3</rs>). Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors only and do not necessarily reflect the views of the <rs type="institution">National Science Council</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_neQjFCk">
					<idno type="grant-number">NSC-96-2221-E-259-017-MY3</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,123.49,531.17,403.47,9.02;4,70.92,542.69,456.87,9.02;4,70.92,554.21,54.98,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,210.44,542.69,219.31,9.02">Query by Image and Video Content: The QBIC system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,436.62,542.69,64.32,9.02">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,123.61,565.67,400.82,9.02;4,70.92,577.19,453.54,9.02;4,70.92,588.71,61.11,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,340.53,565.67,183.91,9.02;4,70.92,577.19,93.54,9.02">Adaptation in Statistical Pattern Recognition using Tangent Vectors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dahmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,174.18,577.19,280.49,9.02">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,115.85,600.17,408.53,9.02;4,70.92,611.69,63.34,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,248.52,600.17,59.94,9.02">Color Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,320.22,600.17,168.60,9.02">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,122.17,623.21,402.32,9.02;4,70.92,634.67,453.50,9.02;4,70.92,646.19,94.25,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,437.10,623.21,87.39,9.02;4,70.92,634.67,203.48,9.02">KIDS&apos;s evaluation in medical image retrieval task at ImageCLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">Pei-Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hao-Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Pang</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,287.34,634.67,188.72,9.02">Working Notes for the CLEF 2004 Workshop</title>
		<meeting><address><addrLine>Bath,UK</addrLine></address></meeting>
		<imprint>
			<date>September</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,120.75,657.72,403.68,9.02;4,70.92,669.18,453.52,9.02;4,70.92,680.70,399.54,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,429.34,657.72,95.09,9.02;4,70.92,669.18,255.45,9.02">Combining Textual and Visual Features for Cross-Language Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Pei-Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hao-Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Pang</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,350.63,669.18,173.81,9.02;4,70.92,680.70,75.26,9.02">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="4,153.15,680.70,142.68,9.02">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="712" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,122.33,692.22,368.77,9.02;4,70.92,703.68,306.16,9.02;4,70.92,715.20,314.75,9.02" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,70.92,703.68,301.84,9.02">Hierarchical classification for ImageCLEF 2007 Medical Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>M¨uller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<ptr target="http://www-i6.informatik.rwth-aachen.de/~deselaers/imageclef07/medaat.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,103.19,726.72,413.79,9.02;4,70.92,738.18,318.71,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,253.28,726.72,200.16,9.02">Tools for texture and color-based search of images</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,460.38,726.72,56.60,9.02;4,70.92,738.18,103.25,9.02">Human Vision and Electronic Imaging II</title>
		<imprint>
			<biblScope unit="volume">3016</biblScope>
			<date type="published" when="1997">1997</date>
			<pubPlace>San Jose, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
