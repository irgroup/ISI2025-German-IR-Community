<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.84,146.21,329.31,18.08;1,138.14,168.13,326.72,18.08">CLEF2007 Image Annotation Task: an SVM-based Cue Integration Approach</title>
				<funder>
					<orgName type="full">Swiss National Center of Competence in Research</orgName>
					<orgName type="abbreviated">NCCR</orgName>
				</funder>
				<funder>
					<orgName type="full">ToMed.IM2</orgName>
				</funder>
				<funder>
					<orgName type="full">Blanceflor Boncompagni Ludovisi foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.36,203.19,73.91,10.46"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
							<email>ttommasi@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="department">IDIAP Research Institute</orgName>
								<address>
									<addrLine>Centre Du Parc, Av. des Pres-Beudin 20</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.87,203.19,81.23,10.46"><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
							<email>forabona@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="department">IDIAP Research Institute</orgName>
								<address>
									<addrLine>Centre Du Parc, Av. des Pres-Beudin 20</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.88,203.19,70.77,10.46"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>bcaputo@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="department">IDIAP Research Institute</orgName>
								<address>
									<addrLine>Centre Du Parc, Av. des Pres-Beudin 20</addrLine>
									<postBox>P. O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.84,146.21,329.31,18.08;1,138.14,168.13,326.72,18.08">CLEF2007 Image Annotation Task: an SVM-based Cue Integration Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">996732C025C51FE822453C33DD88031E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Database Applications-Image databases</term>
					<term>I.5 [Pattern Recognition]: I.5.2 Design Methodology Measurement, Performance, Experimentation Automatic Image Annotation, Cue Integration, Support Vector Machines, Kernel Methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the algorithms and results of our participation to the medical image annotation task of ImageCLEFmed 2007. We proposed, as a general strategy, a multi-cue approach where images are represented both by global and local descriptors, so to capture different types of information. These cues are combined during the classification step following two alternative SVM-based strategies. The first algorithm, called Discriminative Accumulation Scheme (DAS), trains an SVM for each feature type, and considers as output of each classifier the distance from the separating hyperplane. The final decision is taken on a linear combination of these distances: in this way cues are accumulated, thus even when they both are misleaded the final result can be correct. The second algorithm uses a new Mercer kernel that can accept as input different feature types while keeping them separated. In this way, cues are selected and weighted, for each class, in a statistically optimal fashion. We call this approach Multi Cue Kernel (MCK). We submitted several runs, testing the performance of the single-cue SVM and of the two cue integration methods. Our team was called BLOOM (BLanceflOr-tOMed.im2) from the name of our sponsors. The DAS algorithm obtained a score of 29.9, which ranked fifth among all submissions. We submitted two versions of the MCK algorithm, one using the one-vs-all multiclass extension of SVMs and the other using the one-vs-one extension. They scored respectively 26.85 and 27.54, ranking first and second among all submissions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The amount of medical image data produced nowadays is constantly growing, with average-sized radiology departments producing several tera-bites of data annually. The cost of manually annotating these images is very high; furthermore, manual classification induces errors in the tag assignment, which means that a part of the available knowledge is not accessible anymore to physicians <ref type="bibr" coords="2,138.16,180.17,9.96,10.46" target="#b4">[5]</ref>. This calls for automatic annotation algorithms able to perform the task reliably, and benchmark evaluations are thus extremely useful for boosting advances in the field. The ImageCLEFmed annotation task has been established in 2005, and in 2007 it has provided participants with 11000 training and development data, spread across 116 classes. The task consisted in assigning the correct label to 1000 test images. For further informations on the annotation task of ImageCLEF 2007 we refer the reader to <ref type="bibr" coords="2,278.43,239.95,9.96,10.46" target="#b5">[6]</ref>.</p><p>This paper describes the algorithms submitted by the BLOOM (BLanceflOr-tOMed.im2) team, at its first participation to the CLEF benchmark competition. In order to achieve robustness, a crucial property for a reliable automatic system, we opted for a multi-cue approach, using raw pixels as global descriptors and SIFT features as local descriptors. The two feature types were combined together using two different SVM-based integration schemes. The first is the Discriminative Accumulation Scheme (DAS), proposed first in <ref type="bibr" coords="2,374.95,311.68,9.96,10.46" target="#b6">[7]</ref>. For each feature type, an SVM is trained and its output consists of the distance from the separating hyperplane. Then, the decision function is built as a linear combination of the distances, with weighting coefficients determined via cross validation. We submitted a run using this method (BLOOM-BLOOM DAS) that obtained a score of 29.9, ranking fifth among all submissions.</p><p>The second integration scheme consists in designing a new Mercer kernel, able to take as input different feature types for each image data. We call it Multi Cue Kernel (MCK); the main advantage of this approach is that features are selected and weighted during the SVM training, thus the final solution is optimal as it minimizes the structural risk. We submitted two runs using this algorithm, the first (BLOOM-BLOOM MCK oa) using the one-vs-all multiclass extension of SVM; the second (BLOOM-BLOOM MCK oo) using instead the one-vs-one extension. These two runs ranked first and second among all submissions, with a score of respectively 26.85 and 27.54. These results overall confirm the effectiveness of using multiple cues for automatic image annotation.</p><p>The rest of the paper is organized as follows: section 2 describes the two types of feature descriptors we used at the single cue stage. Section 3 gives details on the two alternative SVMbased cue integration approaches. Section 3 reports the experimental procedure adopted and the results obtained, with a detailed discussion on the performance of each algorithm. The paper concludes with a summary discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Single Cue Image Annotation</head><p>The aim of the automatic image annotation task is to classify images into a set of classes. In particular classes were defined in relation to the four independent axis of modality, body orientation, body region, and biological system examined, according to the IRMA code <ref type="bibr" coords="2,429.38,605.55,9.96,10.46" target="#b2">[3]</ref>. The labels are hierarchical therefore, errors in the annotation are counted depending on the level at which the error is done and on the number of possible choices. For each image the error ranges from 0 to 1, respectively if the image is correctly classified or if the predicted label is completely wrong. It is also possible to assign a "don't know" label, in this case the score is 0.5.</p><p>The strategy we propose is to extract a set of features from each image and to use then a Support Vector Machine (SVM) to classify the images. We have explored a local approach, using SIFT descriptors, and a global approach, using the raw pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>We explored the idea of "bag of words" for classification, a common concept in many state of the art approaches in images classification. This is based on the idea that it is possible to transform the images into a set of prespecified visual words, and to classify the images using the statistics of appearance of each word as feature vectors.</p><p>Most of these systems are based on the use of the SIFT descriptor <ref type="bibr" coords="3,395.44,176.74,9.96,10.46" target="#b3">[4]</ref>. The basic idea of SIFT is to describe an area of an image in a way that is robust to noise, illumination, scale, translation and rotation changes. The SIFT points are selected in the image as local maxima of the scalespace, in this sense the SIFT points are intrinsically easy to be tracked. Despite the usefulness of SIFT, there is no reason to believe that these points are the most informative for a classification task. This has been pointed out by different works and systematically verified by <ref type="bibr" coords="3,441.34,236.52,9.96,10.46" target="#b7">[8]</ref>. In that work it is shown that a dense random sampling of the SIFT points is always superior to any strategy based on interest points detectors. Moreover due to the low contrast of the radiographs it would be difficult to use any interest point detector. So in our approach we densely sampled each input image, extracting in each point a SIFT descriptor.</p><p>Another modification we made is based on the fact that the rotation invariance could be useless for the ImageCLEF classification task, as the various structures present in the radiographs are likely to appear always with the same orientation. Moreover the scale is not likely to change too much between images of the same class, so we extracted the SIFT at only one octave, the one that gave us the best classification performances. In this sense we have decoupled the extraction of a SIFT keypoint from the description of the point itself. To keep the complexity of the description of each image low and at the same to retain as much information as possible, we matched each extracted SIFT with a number of template SIFTs. These template SIFTs form our vocabulary of visual words. It is built using a standard K-means algorithm, with K equal to 500, on a random collection of SIFTs extracted from the training images. Various sizes of vocabulary were tested with no significant differences, so we have chosen the smaller one with good recognition performances. Note that in this phase also testing images can be used, because the process is not using the labels and it is unsupervised. At this point each image could be described with the raw counts of each visual word.</p><p>To add some kind of spatial information to our features we divided the images in four subimages, collecting the histograms separately for each subimage. In this way the dimension of the input space is multiplied by four, but in our tests we gained about 3% of classification performances. We have extracted 1500 SIFT in each subimage: such dense sampling adds robustness to the histograms. See Figures <ref type="figure" coords="3,197.45,511.50,4.98,10.46" target="#fig_0">1</ref> and<ref type="figure" coords="3,225.13,511.50,4.98,10.46" target="#fig_1">2</ref> for an example.</p><p>Another approach that we explored was the simplest possible global description method: the raw pixels. The images were resized to 32x32 pixels, regardless of the original dimension, and normalized to have sum equal to one, then the 1024 raw pixels values were used as input features. This approach is at the same time a baseline for the classification system and a useful "companion" method to boost the performance of the SIFT based classifier (see section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification</head><p>For the classification step we used an SVM with an exponential χ 2 as kernel, for both the local and global approaches:</p><formula xml:id="formula_0" coords="3,218.09,640.73,290.67,31.54">K(X, Y ) = exp -γ N i=1 (X i -Y i ) 2 X i + Y i . (<label>1</label></formula><formula xml:id="formula_1" coords="3,508.76,650.65,4.24,10.46">)</formula><p>The parameter γ was tuned through cross-validation (see section 4). This kernel has been successfully applied for histogram comparison and it has been demonstrated to be positive definite <ref type="bibr" coords="3,90.01,701.76,9.96,10.46" target="#b1">[2]</ref>, thus it is a valid kernel.</p><p>Even if the labels are hierarchical, we have chosen to use the standard multi-class approaches. This choice is motivated by the finding that, with our features, the recognition rate was lower using an axis-wise classification. This could be due to the fact that each super-class has a variability so high that the chosen features are not able to model it, while they can very well model the small sub-classes. In particular we have tested both one-vs-one and one-vs-all multi-class extension for SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi Cue Annotation</head><p>Due to the fundamental difference in how local and global features are computed it is reasonable to suppose that the two representations provide different kinds of information. Thus, we expect that by combining them through an integration scheme, we should achieve a better performance, namely higher classification performance and higher robustness.</p><p>In the computer vision and pattern recognition literature some authors have suggested different methods to combine information derived from different cues (for a review on the topic we refer the reader to <ref type="bibr" coords="4,151.52,493.65,10.30,10.46" target="#b8">[9]</ref>). Some of them are based on building new representations, but this technique does not solve the robustness problem because if one of the cues gives misleading information it is quite probable that the new feature vector will be adversely affected. Moreover, the dimension of such a feature vector would increase as the number of of cues grows, implying longer learning and recognition times, greater memory requirements and possibly curse of dimensionality effects. The strategy we follow in this paper is to use integration schemes, thus keeping the feature descriptors separated and fusing them at a mid-or high-level. In the rest of the section we describe the two alternative integration schemes we used in the ImageCLEF competition. The first, the Discriminative Accumulation Scheme (DAS, <ref type="bibr" coords="4,336.65,589.30,10.30,10.46" target="#b6">[7]</ref>), is a high-level integration scheme, meaning that each single cue first generate a set of hypotheses on the correct label of the test image, and then those hypotheses are combined together so to obtain a final output. This method is described in section 3.1. The second, the Multi Cue Kernel (MCK), is a mid-level integration scheme, meaning that the different features descriptors are kept separated but they are combined in a single classifier generating the final hypothesis. This algorithm is described in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discriminative Accumulation Scheme</head><p>The Discriminative Accumulation Scheme is an integration scheme for multiple cues that does not neglect any cue contribution. It is based on a weak coupling method called accumulation. The main idea of this method is that information from different cues can be summed together.</p><p>Suppose we are given M object classes and for each class, a set of N j training images {I j i } N j i=1 , j = 1, . . . M . For each image, we extract a set of P different cues: </p><formula xml:id="formula_2" coords="5,245.68,372.26,267.32,14.89">T p = T p (I j i ), p = 1 . . . P<label>(2)</label></formula><p>so that for an object j we have P new training sets {T p (I j i )}</p><p>N j i=1 , j = 1, . . . M, p = 1 . . . P . For each we train an SVM. Kernel functions may differ from cue to cue and model parameters can be estimated during the training step via cross validation. Given a test image Î and assuming M ≥ 2, for each single-cue SVM we compute the distance from the separating hyperplane:</p><formula xml:id="formula_3" coords="5,208.71,463.35,300.05,35.56">D j (p) = m p j i=1 α p ij y ij K p T p (I j i ), T p ( Î) + b p j . (<label>3</label></formula><formula xml:id="formula_4" coords="5,508.76,477.29,4.24,10.46">)</formula><p>After collecting all the distances {D j (p)} P p=1 for all the j objects j = 1, . . . , M and the p cues p = 1, . . . , P , we classify the image Î using the linear combination:</p><formula xml:id="formula_5" coords="5,218.19,553.57,290.57,31.41">j * = M argmax j=1 { P p=1 a p D j(p) }, a p ∈ + . (<label>4</label></formula><formula xml:id="formula_6" coords="5,508.76,563.48,4.24,10.46">)</formula><p>The coefficients {a p } P p=1 are evaluated via cross validation during the training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi Cue Kernel</head><p>DAS can be defined a high-level integration scheme, as fusion is performed as a post-processing step after the single-cue classification stage. As an alternative, we developed a mid-level integrating scheme based on multi-class SVM with a Multi Cue Kernel K M C . This new kernel combines different features extracted form images; it is a Mercer kernel, as positively weighted linear combination of Mercer kernels are Mercer kernels themselves <ref type="bibr" coords="5,340.86,687.03,9.96,10.46" target="#b0">[1]</ref>:</p><formula xml:id="formula_7" coords="5,186.27,720.27,326.73,31.41">K M C ({T p (I i )} p , {T p (I)} p ) = P p=1 a p K p (T p (I i ), T p (I)).<label>(5)</label></formula><p>In this way it is possible to perform only one classification step, identifying the best weighting factors a p while optimizing the other kernel parameters. Another advantage of this approach is that it makes it possible to work both with one-vs-all and one-vs-one SVM extensions to the multiclass problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments started evaluating the performance of local and global features separately before testing our integration methods. Two sets of experiments using single-cue SVM were ran to select the best kernel parameters through cross validation. The original dataset was divided in three parts: training, validation and testing. We merged them together and extracted 5 random and disjoint train/test splits of 10000/1000 images. We considered as the best parameters the one giving the best average score on the 5 splits. Note that, according to the method used for the score evaluation, the best average score is not necessary the best recognition rate. Besides obtaining the optimal parameters, these experiments showed that the SIFT features outperform the raw pixel ones. It could be predictable since the last year ImageCLEF competition results showed that local features are generally more informative than global features for the annotation task.</p><p>Then we adopted the same experimental setup for DAS and MCK. In particular for DAS we used the distances from the separating hyperplanes associated with the best results of the previous step, so the cross validation was used only to search the best weights for cue integration. On the other hand, for MCK the cross validation was applied to look for the best kernel parameters and the best feature's weights at the same time. In both cases weights could vary from 0 to 1.</p><p>Finally we used the results of the previous phases to run our submission experiment on the 1000 unlabeled images of the challenge test set using all the 11000 images of the original dataset as training.</p><p>The ranking, name and score of our submitted runs together with the score gain respect to the best run of other participants are listed in Table <ref type="table" coords="6,307.60,428.31,3.87,10.46" target="#tab_0">1</ref>. Our two runs based on the MCK algorithm ranked first and second among all submissions stating the effectiveness of using multiple cues for automatic image annotation. It is interesting to note that even if DAS has a higher recognition rate, its score is worse than that obtained using the feature SIFT alone. This could be due to the fact that when the label predicted by the global approach, the raw pixels, is wrong, the true label is far from the top of the decision ranking.</p><p>In Table <ref type="table" coords="6,144.44,500.04,4.98,10.46" target="#tab_1">2</ref> there is a summary of the parameters used for our runs and the number of support vectors obtained. As we could expect, the best feature weight (see ( <ref type="formula" coords="6,390.66,512.01,4.24,10.46" target="#formula_5">4</ref>) and ( <ref type="formula" coords="6,426.57,512.01,4.15,10.46" target="#formula_7">5</ref>)) for SIFT results higher than that for raw pixels for all the integration methods. The number of support vectors for the MCK run using one-vs-one multiclass SVM extension (MCK oa) is slightly higher than that used by the single cue SIFT oa but lower than that used by PIXEL oa. For the MCK run using one-vs-one multiclass SVM extension (MCK oo) the number of support vectors is even lower than that of both the single cues SIFT oo and PIXEL oo. These results show that combining two features with the MCK algorithm can simplify the classification problem. For DAS we counted the support vectors summing the ones from SIFT oa and PIXEL oa but considering only once the support vectors associated with the training images that resulted in common between the single cues. The number of support vector for DAS exceed that obtained for both MCK oa and MCK oo showing a higher complexity of the classification problem.</p><p>Table <ref type="table" coords="6,132.98,643.51,4.98,10.46" target="#tab_2">3</ref> shows in details some examples of classification results. The first, second and third column contain examples of images misclassified by one of the two cues but correctly classified by DAS and MCK oa. The fourth column shows an example of an image misclassified by both cues and by DAS but correctly classified by MCK oa. It is interesting to note that combining local and global features can be useful to recognize images even if they are compromised by the presence of artifacts that for medical images can be prosthesis or reference labels put on the acquisition screen.</p><p>A deeper analysis of our results can be done considering the performance of the single-cue, discriminative accumulation and multicue kernel approach for each class. In Table <ref type="table" coords="6,454.31,739.15,4.98,10.46">4</ref>  of images correctly recognized for each class are listed and it is possible to note that in few cases PIXEL oa outperforms SIFT oa, and to observe where MCK oa outperforms both SIFT oa and DAS. The difference between our approaches can be better evaluated considering the confusion matrices. They are shown as images in Figure <ref type="figure" coords="7,288.84,455.42,3.87,10.46" target="#fig_2">3</ref>. We ordered the classes following the way in which they are listed in table <ref type="table" coords="7,196.28,467.37,4.98,10.46">4</ref> and used a colormap corresponding to the number of images varying from zero to five to let the misclassified images stand out. It is clear that our methods differ principally for how the wrong images are labeled. The more matrices present sparse values out of the diagonal and far away from it, the worse the method is. Table <ref type="table" coords="8,117.08,335.08,3.91,10.56">4</ref>: Performance of the single-cue, discriminative accumulation and multicue kernel approach for each class.</p><formula xml:id="formula_8" coords="7,191.77,600.23,206.79,48.59">PIXEL oa 11 • 1 • 12 • 5 • SIFT oa 1 • 2 • 2 • 5 • DAS 1 • 1 • 1 • 2 • MCK oa 1 • 1 • 1 • 1 •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presented a discriminative multi-cue approach to medical image annotation. We combined global and local information using two alternative fusion strategies, the discriminative accumulation scheme <ref type="bibr" coords="8,178.78,431.88,10.62,10.56" target="#b6">[7]</ref> and the multi cue kernel. This last method gave the best performance obtaining a score of 26.85, which ranked first among all submissions. This work can be extended in many ways. First, we would like to use various types of local and global descriptors, so to select the best features for the task. Second, we would like to add shape descriptors in our fusion scheme, which should result in a better performance. Finally, our algorithm does not exploit at the moment the natural hierarchical structure of the data, but we believe that this information is crucial for achieving significant improvements in performance. Future work will explore these directions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.01,292.30,422.99,10.46;4,90.00,304.25,56.61,10.46;4,114.68,113.86,175.94,152.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Radiographic image divided in 4 subimages and (b) corresponding counts of the visual words.</figDesc><graphic coords="4,114.68,113.86,175.94,152.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.01,292.30,422.99,10.46;5,90.00,304.25,423.01,10.46;5,90.00,316.20,249.99,10.46;5,114.68,113.86,175.94,152.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference between random sampling and interest point detector. In (a) the four most present visual words in the image are drawn, each with a different color. In (b) the result of standard SIFT extraction in the same octave used in (a).</figDesc><graphic coords="5,114.68,113.86,175.94,152.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,90.01,437.11,422.99,10.46;9,90.00,449.06,423.01,10.46;9,90.00,461.01,423.01,10.46;9,90.01,472.97,423.00,10.46;9,90.00,484.93,105.30,10.46;9,121.87,275.03,141.93,130.88"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: These images represent the confusion matrices respectively for (a) SIFT oa, (b) Pixel oa, (c) DAS and (d) MCK oa. We ordered the classes following the way in which they are listed in table 4 and used a colormap corresponding to the number of images varying from zero to five to let the misclassified images stand out. All the position in the matrices containing five or more images appear dark red.</figDesc><graphic coords="9,121.87,275.03,141.93,130.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,462.75,739.15,50.25,10.46"><head>Table 1 :</head><label>1</label><figDesc>Ranking of our submitted runs, name, score and gain respect to the best run of the other participants.</figDesc><table coords="6,462.75,739.15,50.25,10.46"><row><cell>the number</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,365.82,423.01,22.41"><head>Table 2 :</head><label>2</label><figDesc>Here are shown the best parameters obtained by cross validation and used for the classification, together with the number of Support Vectors for each of our submitted runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,660.23,423.01,22.41"><head>Table 3 :</head><label>3</label><figDesc>Example of images misclassified by one or both cues and correctly classified by DAS or MCK.The values correspond to the decision rank.</figDesc><table coords="8,98.98,108.43,411.66,215.88"><row><cell>MCK oa</cell><cell>SIFT oa</cell><cell>DAS</cell><cell>PIXEL oa</cell><cell>TOT</cell><cell>MCK oa</cell><cell>SIFT oa</cell><cell>DAS</cell><cell>PIXEL oa</cell><cell>TOT</cell><cell>MCK oa</cell><cell>SIFT oa</cell><cell>DAS</cell><cell>PIXEL oa</cell><cell>TOT</cell><cell>MCK oa</cell><cell>SIFT oa</cell><cell>DAS</cell><cell>PIXEL oa</cell><cell>TOT</cell></row><row><cell>CLASS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLASS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLASS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLASS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1121-110-213-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>1121-120-516-700 2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>1121-210-331-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-240-441-700 4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>5</cell></row><row><cell cols="4">1121-110-411-700 13 11 12 8</cell><cell cols="2">14 1121-120-517-700 2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>1121-220-213-700 2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>1121-240-442-700 4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell cols="20">1121-110-414-700 38 38 38 35 38 1121-120-800-700 22 22 22 22 22 1121-220-230-700 17 17 17 17 17 1121-320-941-700 10 10 10 10 10</cell></row><row><cell>1121-110-415-700 9</cell><cell>9</cell><cell>8</cell><cell>6</cell><cell>9</cell><cell>1121-120-911-700 5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>1121-220-310-700 7</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>7</cell><cell>1121-420-212-700 4</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>4</cell></row><row><cell cols="6">1121-115-700-400 13 13 13 12 13 1121-120-914-700 6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>1121-220-330-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-420-213-700 4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>1121-115-710-400 2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>1121-120-915-700 5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>1121-228-310-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1121-430-213-700 8</cell><cell>8</cell><cell>8</cell><cell>6</cell><cell>9</cell></row><row><cell>1121-116-917-700 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1121-120-918-700 2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>1121-229-310-700 1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1121-430-215-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell cols="6">1121-120-200-700 33 33 33 33 34 1121-120-919-700 2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>1121-230-462-700 2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1121-460-216-700 1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell></row><row><cell cols="6">1121-120-310-700 20 20 20 20 20 1121-120-921-700 9</cell><cell>9</cell><cell>9</cell><cell>5</cell><cell>9</cell><cell>1121-230-463-700 5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>1121-490-310-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>1121-120-311-700 3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell cols="4">1121-120-922-700 11 11 11 7</cell><cell cols="2">11 1121-230-911-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1121-490-415-700 6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell cols="4">1121-120-320-700 11 11 11 9</cell><cell cols="2">11 1121-120-930-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1121-230-914-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1121-490-915-700 4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell cols="6">1121-120-330-700 22 23 22 20 23 1121-120-933-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-230-915-700 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1122-220-333-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>1121-120-331-700 1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1121-120-934-700 2</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>1121-230-921-700 7</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>7</cell><cell cols="5">1123-110-500-000 84 78 80 69 91</cell></row><row><cell>1121-120-413-700 3</cell><cell>3</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>1121-120-942-700 9</cell><cell cols="2">10 9</cell><cell>7</cell><cell cols="2">10 1121-230-922-700 6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>8</cell><cell>1123-112-500-000 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>5</cell></row><row><cell>1121-120-421-700 4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>1121-120-943-700 9</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell cols="2">10 1121-230-930-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1123-121-500-000 5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>8</cell></row><row><cell>1121-120-422-700 4</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>1121-120-950-700 0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-230-934-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell cols="5">1123-127-500-000 182 184 184 172 196</cell></row><row><cell>1121-120-433-700 1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1121-120-951-700 1</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>3</cell><cell>1121-230-942-700 9</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell cols="5">1123-211-500-000 89 89 89 88 89</cell></row><row><cell>1121-120-434-700 2</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>1121-120-956-700 1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1121-230-943-700 7</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>7</cell><cell>1124-310-610-625 6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell>1121-120-437-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-120-961-700 3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>1121-230-950-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1124-310-620-625 7</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>7</cell></row><row><cell>1121-120-438-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-120-962-700 5</cell><cell>4</cell><cell>5</cell><cell>3</cell><cell>5</cell><cell>1121-230-953-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1124-410-610-625 7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>1121-120-441-700 4</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>1121-127-700-400 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>1121-230-961-700 4</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>1124-410-620-625 7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>1121-120-442-700 3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>1121-127-700-500 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-230-962-700 2</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>3</cell><cell>1121-120-91a-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>1121-120-451-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1121-129-700-400 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1121-240-413-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1121-12f-466-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>1121-120-452-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-200-411-700 9</cell><cell>7</cell><cell>7</cell><cell>4</cell><cell cols="2">13 1121-240-421-700 4</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>5</cell><cell>1121-12f-467-700 2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>1121-120-454-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-210-213-700 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1121-240-422-700 2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>3</cell><cell>1121-4a0-310-700 2</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>2</cell></row><row><cell>1121-120-462-700 4</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>5</cell><cell cols="6">1121-210-230-700 13 13 13 11 13 1121-240-433-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>3</cell><cell>1121-4a0-414-700 8</cell><cell>7</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>1121-120-463-700 7</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>7</cell><cell cols="4">1121-210-310-700 10 10 10 9</cell><cell cols="2">10 1121-240-434-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>3</cell><cell>1121-4a0-914-700 3</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>5</cell></row><row><cell>1121-120-514-700 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell cols="6">1121-210-320-700 11 11 11 10 12 1121-240-437-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1121-4a0-918-700 0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>1121-120-515-700 3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell cols="6">1121-210-330-700 20 20 20 18 21 1121-240-438-700 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1121-4b0-233-700 4</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">ToMed.IM2</rs> project (B. C. and F. O), under the umbrella of the <rs type="funder">Swiss National Center of Competence in Research (NCCR)</rs> on Interactive Multimodal Information Management (IM2, www.im2.ch), and by the <rs type="funder">Blanceflor Boncompagni Ludovisi foundation</rs> (T. T., www.blanceflor.se). The support is gratefully acknowledged.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.61,662.48,411.49,10.56;8,105.61,674.55,201.43,10.56" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,267.51,662.48,249.60,10.56;8,105.61,674.55,137.56,10.56">An Introduction to Support Vector Machines (and Other Kernel-Based Learning Methods</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CUP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.61,693.82,411.50,10.56;8,105.61,705.89,385.75,10.56" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,319.85,693.82,192.12,10.56">Spectral grouping using the nyström method</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,105.61,705.89,288.36,10.56">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="225" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.61,725.18,411.50,10.56;8,105.61,737.25,411.50,10.56;8,105.61,749.32,216.66,10.56" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,105.61,737.25,257.95,10.56">The irma code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">Schubert</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keysers</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kohnen</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wein</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,388.98,737.25,128.13,10.56;8,105.61,749.32,33.28,10.56">Proceedings of SPIE Medical Imaging</title>
		<meeting>SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="volume">5033</biblScope>
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,523.73,407.50,10.46;9,105.50,535.68,407.51,10.46;9,105.50,547.64,181.03,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,164.06,523.73,231.47,10.46">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.69,523.73,95.31,10.46;9,105.50,535.68,223.80,10.46">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,566.07,407.51,10.46;9,105.50,578.03,407.51,10.46;9,105.50,589.98,192.14,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,105.50,578.03,263.88,10.46">Quality of dicom header information for image categorization</title>
		<author>
			<persName coords=""><forename type="first">M-O-Gueld</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bredno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,389.61,578.03,123.40,10.46;9,105.50,589.98,32.96,10.46">Proceedings of SPIE Medical Imaging</title>
		<meeting>SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4685</biblScope>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,608.41,407.51,10.46;9,105.50,620.37,407.51,10.46;9,105.50,632.32,407.50,10.46;9,105.50,644.27,114.90,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,304.57,620.37,208.44,10.46;9,105.50,632.32,132.23,10.46">Overview of the ImageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,262.44,632.32,197.05,10.46">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,662.71,407.51,10.46;9,105.50,674.66,402.34,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,241.28,662.71,230.09,10.46">Cue integration through discriminative accumulation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,493.90,662.71,19.11,10.46;9,105.50,674.66,371.95,10.46">Proceedings of the International conference on Computer Vision and Pattern Recognition</title>
		<meeting>the International conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,693.10,407.50,10.46;9,105.50,705.05,307.08,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,257.71,693.10,251.44,10.46">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,117.95,705.05,264.18,10.46">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,723.49,407.50,10.46;9,105.50,735.44,74.17,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,155.02,723.49,185.22,10.46">Ensemble based systems in decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,348.56,723.49,159.58,10.46">IEEE Circuits and Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
