<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.14,83.51,409.76,12.58;1,239.16,100.97,116.99,12.58">Model Fusion Experiments for the Cross Language Speech Retrieval Task at CLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,233.64,139.32,65.78,9.02"><forename type="first">Muath</forename><surname>Alzghool</surname></persName>
							<email>alzghool@site.uottawa.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.81,139.32,54.20,9.02"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Engineering</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.14,83.51,409.76,12.58;1,239.16,100.97,116.99,12.58">Model Fusion Experiments for the Cross Language Speech Retrieval Task at CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E63243A094BFFE1838894B0603A03F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Data Fusion</term>
					<term>Retrieval Models</term>
					<term>Query Expansion .7% -1.5%</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the University of Ottawa group in the Cross-Language Speech Retrieval (CL-SR) task at CLEF 2007. We present the results of the submitted runs for the English collection. We have used two Information Retrieval systems in our experiments: SMART and Terrier, with two query expansion techniques: one based on a thesaurus and the second one based on blind relevant feedback. We proposed two novel data fusion methods for merging the results of several models (retrieval schemes available in SMART and Terrier). Our experiments showed that the combination of query expansion methods and data fusion methods helps to improve the retrieval performance. We also present cross-language experiments, where the queries are automatically translated by combining the results of several online machine translation tools. Experiments on indexing the manual summaries and keywords gave the best retrieval results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the third participation of the University of Ottawa group in the Cross-Language Speech Retrieval (CL-SR) track, at CLEF 2007. We present our systems, followed by results for the submitted runs for the English collection. We present results for many additional runs for the English collection. We experimented with many possible weighting schemes for indexing the documents and the queries, and with several query expansion techniques. Several researchers in the literature have explored the idea of combining the results of different retrieval strategies, different document representations and different query representations; the motivation is that each technique will retrieve different sets of relevant documents; therefore combining the results could produce a better result than any of the individual techniques. We propose new data fusion techniques for combining the results of different Information Retrieval (IR) schemes. We applied our data fusion techniques to monolingual settings and to cross-language settings where the queries are automatically translated from French and Spanish into English by combining the results of several online machine translation (MT) tools. At the end we present the best results, when manual summaries and manual keywords were indexed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The University of Ottawa Cross-Language Information Retrieval systems were built with off-the-shelf components. For the retrieval part, the SMART <ref type="bibr" coords="1,267.05,703.02,10.88,9.02" target="#b2">[3,</ref><ref type="bibr" coords="1,280.95,703.02,13.35,9.02" target="#b10">11]</ref> IR system and the Terrier <ref type="bibr" coords="1,404.83,703.02,10.83,9.02" target="#b1">[2,</ref><ref type="bibr" coords="1,418.74,703.02,13.35,9.02" target="#b9">10]</ref> IR system were tested with many different weighting schemes for indexing the collection and the queries.</p><p>SMART was originally developed at Cornell University in the 1960s. SMART is based on the vector space model of information retrieval. We used nnn.ntn, ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn, nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc weighting schemes <ref type="bibr" coords="2,341.54,81.54,25.92,9.02">[3 ,11]</ref>; lnn.ntn performs very well in CLEF-CLSR 2005 and 2006 <ref type="bibr" coords="2,160.00,93.00,11.51,9.02" target="#b5">[6,</ref><ref type="bibr" coords="2,171.51,93.00,7.68,9.02" target="#b0">1]</ref> .</p><p>Terrier was originally developed at University of Glasgow. It is based on Divergence from Randomness models (DFR) where IR is seen as a probabilistic process <ref type="bibr" coords="2,328.19,115.98,10.87,9.02" target="#b1">[2,</ref><ref type="bibr" coords="2,344.08,115.98,11.91,9.02" target="#b9">10]</ref>. We experimented with the In(exp)C2 weighting model, one of Terrier's DFR-based document weighting models.</p><p>For translating the queries from French and Spanish into English, several free online machine translation tools were used. The idea behind using multiple translations is that they might provide more variety of words and phrases, therefore improving the retrieval performance. Seven online MT systems <ref type="bibr" coords="2,408.34,161.94,11.67,9.02" target="#b5">[6]</ref> were used for translating from Spanish and from French into English. We combined the outputs of the MT systems by simply concatenating all the translations. All seven translations of a title made the title of the translated query; the same was done for the description and narrative fields. We used the combined topics for all the cross-language experiments reported in this paper.</p><p>We have used two query expansion methods. The first one is based on the Shoah Visual History Foundation thesaurus provided with the Mallach collection; our method adds two items and their alternatives (synonyms) from the thesaurus, based on the similarity between the thesaurus terms and the title field for each topic. More specifically, to select two items from the thesaurus, we used SMART with the title of each topic as query and the thesaurus terms as documents, using the weighting scheme lnn.ntn. After computing the similarity, the top two thesaurus terms were added to the topic; for these terms all the alternative terms was also added to the topic. For example, in topic 3005, the title is "Death marches", and the most similar terms from the thesaurus are "death marches" and "deaths during forced marches"; the alternative terms for theses terms are "death march" and "Todesmärsche". Table <ref type="table" coords="2,168.95,312.00,5.01,9.02">1</ref> shows two entries from the thesaurus; each entry contains six types of fields: name ̶ contains a unique numeric code for each entry, label ̶ a phrase or word which represents the entry, alt-label ̶ contains the alternative phrase or the synonym for the entry, usage ̶ contains the usage or the definition of the entry. There are two more relations in the thesaurus: is-a and of-type, which contain the numeric code of the entry involved in the relation. The second query expansion method extracts the most informative terms from the top-returned documents as the expanded query terms. In this expansion process, 12 terms from the returned documents (the top 15 documents) were added to the topic, based on Bose-Einstein 1 model (Bo1) <ref type="bibr" coords="2,482.73,382.32,11.54,9.02" target="#b3">[4,</ref><ref type="bibr" coords="2,494.27,382.32,11.54,9.02" target="#b9">10]</ref>; we have put a restriction on the new terms: their document frequency must be less than the maximum document frequency in the title of the topic. The aim of this restriction is avoid more-general terms being added to the topic. Any term that satisfies this restriction will be a part of the new topic. We have also up weighted the title terms five times higher than the other terms in the topic.</p><p>Table <ref type="table" coords="2,94.85,462.93,3.38,8.10">1</ref>. The top two entries from the thesaurus that are similar to the topic title "Death marches". &lt;keyword&gt; &lt;name&gt;9125&lt;/name&gt; &lt;alt-label&gt;death march&lt;/alt-label&gt; &lt;alt-label&gt; Todesmärsche&lt;/alt-label&gt; &lt;broader-term&gt;15445&lt;/broader-term&gt; &lt;label&gt;death marches&lt;/label&gt; &lt;of-type&gt;5289&lt;/of-type&gt; &lt;usage&gt;Forced marches of prisoners over long distances, under heavy guard and extremely harsh conditions. (The term was probably coined by concentration camp prisoners.)&lt;/usage&gt; &lt;/keyword&gt; &lt;keyword&gt; &lt;name&gt;15460&lt;/name&gt; &lt;broader-term&gt;15445&lt;/broader-term&gt; &lt;label&gt;deaths during forced marches&lt;/label&gt; &lt;of-type&gt;4109&lt;/of-type&gt; &lt;usage&gt;The daily experience of individuals with death during forced marches that was not the result of executions, punishments, arbitrary killings or suicides.&lt;/usage&gt; &lt;/keyword&gt; For the data fusion part, we proposed two methods that use the sum of normalized weighted similarity scores of 15 different IR schemes as shown in the following formulas :</p><formula xml:id="formula_0" coords="3,116.58,111.34,271.30,57.85">∑ ∈ * + = schems IR i i MAP r NormSim i W i W Fusion )] ( ) ( [ 1 3 4 (1) ∑ ∈ * = schems IR i i MAP r NormSim i W i W Fusion ) ( * ) ( 2 3 4 (2)</formula><p>where W r (i) and W MAP (i) are experimentally determined weights based on the recall (the number of relevant documents retrieved) and precision (MAP score) values for each IR scheme computed on the training data. For example, suppose that two retrieval runs r1 and r2 give 0.3 and 0.2 (respectively) as MAP scores on training data; we normalize these scores by dividing them by the maximum MAP value: then W MAP (r1) is 1 and W MAP (r2) is 0.66 (then we compute the power 3 of these weights, so that one weight stays 1 and the other one decreases; we chose power 3 for MAP score and power 4 for recall, because the MAP is more important than the recall). We hope that when we multiply the similarity values with the weights and take the summation over all the runs, the performance of the combined run will improve. NormSim i is the normalized similarity for each IR scheme. We did the normalization by dividing the similarity by the maximum similarity in the run. The normalization is necessary because different weighting schemes will generate different range of similarity values, so a normalization method should applied to each run. Our method is differed than the work done by Fox and Shaw in 1994 <ref type="bibr" coords="3,106.02,301.02,11.68,9.02" target="#b4">[5]</ref> and Lee in 1995 <ref type="bibr" coords="3,193.92,301.02,10.87,9.02" target="#b6">[7]</ref>; they combined the results by taking the summation of the similarity scores without giving any weight to each run. In our work we weight each run according to the precision and recall on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>Table <ref type="table" coords="3,95.97,417.78,5.01,9.02" target="#tab_0">2</ref> shows the results of the submitted results on the test data (33 queries). The evaluation measure we report is the standard measure computed with the trec_eval script (version 8): MAP (Mean Average Precision) and Recall. The information about what fields of the topic were indexed is given in the column named Fields: T for title only, TD for title + description, TDN for title + description + narrative. For each run we include an additional description of the experimental settings and which document fields were indexed; <ref type="bibr" coords="3,460.00,463.74,11.50,9.02" target="#b7">[8,</ref><ref type="bibr" coords="3,471.50,463.74,7.67,9.02" target="#b8">9]</ref> give more information about the training and test data . For the uoEnTDtManF1 and uoEnTDtQExF1 runs we used the Fusion1 formula for data fusion; and for uoEnTDtQExF2, uoFrTDtF2, and uoEsTDtF2 we used the Fuison2 formula for data fusion. We used blind relevance feedback and query expansion from the thesaurus for the uoEnTDtManF1, uoEnTDtQExF1, and uoEnTDtQExF2 runs; we didn't use any query expansion techniques for uoFrTDtF2 and uoEsTDtF2.</p><p>Our required run, English TD, obtained a MAP score of 0.0855. Comparing this result to the median and average of all runs submitted by all the teams that participated in the track (0.0673, 0.0785) <ref type="bibr" coords="3,449.94,544.20,10.63,9.02" target="#b8">[9]</ref>, our result was significantly better (based on a two-tailed Wilcoxon Signed-Rank Test for paired samples at p &lt; 0.05 across the 33 evaluation topics) with a relative improvement of 21% and 8%; there is a small improvement using Fusion1 (uoEnTDtQExF1) over Fusion2 (uoEnTDtQExF2), but this improvement is not significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison of Systems and Query Expansion Methods</head><p>In order to compare between different methods of query expansion and a base run without query expansion, we selected the base run with the weighting scheme lnn.ntn, topic fields title and description, and document fields ASRTEXT2004A, AUTOKEYWORD2004A1, and AUTOKEYWORD2004A2. We used the two techniques for query expansion, one based on the thesaurus and the other one on blind relevance feedback (denoted Bo1 in Table <ref type="table" coords="4,514.13,138.48,3.61,9.02" target="#tab_1">3</ref>). We present the results (MAP scores) with and without query expansion, and with the combination of both query expansion methods, on the test and training topics. According to Table <ref type="table" coords="4,370.15,161.46,3.77,9.02" target="#tab_1">3</ref>, we note that both methods help to improve the retrieval results, but the improvement is not significant on the training and test data; also the combination of the two methods helps to improve the MAP score on the training data (not significantly), but not on the test data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments using Data Fusion</head><p>We applied the data fusion methods described in section 2 to 14 runs produced by SMART and one run produced by Terrier; all runs was produced using a combination of the two methods of query expansion as described in section 2. Performance results for each single run and fused runs are presented in Table <ref type="table" coords="4,479.83,380.94,3.76,9.02" target="#tab_2">4</ref>, in which % change is given with respect to the run providing better effectiveness in each combination on the training data. The Manual English column represents the results when only the manual keywords and the manual summaries were used for indexing the documents using English topics, the Auto-English column represents the results when automatic fields are indexed from the documents (ASRTEXT2004A, and AUTOKEYWORD2004A1, A2) using English topics. For cross-languages experiments the results are represented in the columns Auto-French, and Auto-Spanish.</p><p>Data fusion helps to improve the performance (MAP score) on the test data The best improvement using data fusion (Fusion1) was on the French cross-language experiments with 21.7%, which is statistically significant while on monolingual the improvement was only 6.5% which is not significant. Also, there is an improvement in the number of relevant documents retrieved (recall) for all the experiments, except Auto-French on the test data, as shown in Table <ref type="table" coords="4,149.22,507.36,3.76,9.02">5</ref>. We computed these improvements relative to the results of the best single-model run, as measured on the training data. This supports our claim that data fusion improves the recall by bringing some new documents that were not retrieved by all the runs. On the training data, the Fusion2 method gives better results than Fusion1 for all cases except on Manual English, but on the test data Fusion1 is better than Fusion2. In general, the data fusion seems to help, because the performance on the test data in not always good for weighting schemes that obtain good results on the training data, but combining models allows the best-performing weighting schemes to be taken into consideration.</p><p>The retrieval results for the translations from French were very close to the monolingual English results, especially on the training data, but on the test data the difference was significantly worse. For Spanish, the difference was significantly worse on the training data, but not on the test data.</p><p>Experiments on manual keywords and manual summaries showed high improvements, the MAP score jumped from 0.0855 to 0.2761 on the test data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We experimented with two different systems: Terrier and SMART, with combining the various weighting schemes for indexing the document and query terms. We proposed two approaches for query expansion, one based on the thesaurus and another one based on blind relevance feedback. The combination of the query expansion methods obtained a small improvement on the training and test data (not statistically significant according to a Wilcoxon signed test).</p><p>Our focus this year was on data fusion: we proposed two methods to combine different weighting scheme from different systems, based on weighted summation of normalized similarity measures; the weight for each scheme was based on the relative precision and recall on the training data. Data fusion helps to improve the retrieval significantly for some experiments (Auto-French) and for other not significantly (Manual English).</p><p>The idea of using multiple translations proved to be good. More variety in the translations would be beneficial. The online MT systems that we used are rule-based systems. Adding translations by statistical MT tools might help, since they could produce radically different translations.</p><p>Combining query expansion methods and data fusion helped to improve the retrieval significantly comparing to the median and average of all required runs submitted by all the teams that participated in the track.</p><p>In future work we plan to investigate more methods of data fusion, removing or correcting some of the speech recognition errors in the ASR content words, and to use speech lattices for indexing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.56,603.63,454.33,134.36"><head>Table 2 .</head><label>2</label><figDesc>Results of the five submitted runs, for topics in English, French, and Spanish. The required run (English, title + description) is in bold.</figDesc><table coords="3,100.02,625.26,377.44,112.74"><row><cell>Runs</cell><cell>MAP</cell><cell>Recall</cell><cell cols="2">Fields Description</cell></row><row><cell cols="2">uoEnTDtManF1 0.2761</cell><cell>1832</cell><cell>TD</cell><cell>English: Fusion 1, query expansion methods, fields: MANUALKEYWORD + SUMMARY</cell></row><row><cell cols="2">uoEnTDtQExF1 0.0855</cell><cell>1333</cell><cell>TD</cell><cell>English: Fusion 1, query expansion methods, fields: ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell cols="2">uoEnTDtQExF2 0.0841</cell><cell>1336</cell><cell>TD</cell><cell>English: Fusion 2, query expansion methods, fields: ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell>uoFrTDtF2</cell><cell>0.0603</cell><cell>1098</cell><cell>TD</cell><cell>French : Fusion 2, fields:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row><row><cell>uoEsTDtF2</cell><cell>0.0619</cell><cell>1171</cell><cell>TD</cell><cell>Spanish : Fusion 2, fields:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASRTEXT2004A + AUTOKEYWORD2004A1, A2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,70.56,219.09,454.25,91.42"><head>Table 3 .</head><label>3</label><figDesc>Results (MAP scores) for Terrier and SMART, with or without relevance feedback, for English topics (using the TD query fields).</figDesc><table coords="4,128.04,246.60,303.98,63.92"><row><cell>System</cell><cell>Training</cell><cell>Test</cell></row><row><cell>1 lnn.ntn</cell><cell>0.0906</cell><cell>0.0725</cell></row><row><cell>2 lnn.ntn +thesaurus</cell><cell>0.0941</cell><cell>0.0730</cell></row><row><cell>3 lnn.ntn +Bo1</cell><cell>0.0954</cell><cell>0.0811</cell></row><row><cell>4 lnn.ntn+ thesaurus+ Bo1</cell><cell>0.0969</cell><cell>0.0799</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.56,93.21,454.33,287.44"><head>Table 4 .</head><label>4</label><figDesc>Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In(exp)C2 model), and the results for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding results on the test data. Underlined are the results of the submitted runs.</figDesc><table coords="5,70.56,131.04,452.24,249.62"><row><cell>Weighting</cell><cell cols="2">Manual English</cell><cell cols="2">Auto-English</cell><cell cols="2">Auto-French</cell><cell cols="2">Auto-Spanish</cell></row><row><cell>scheme</cell><cell>Training</cell><cell>Test</cell><cell>Training</cell><cell>Test</cell><cell>Training</cell><cell>Test</cell><cell>Training</cell><cell>Test</cell></row><row><cell>nnc.ntc</cell><cell>0.2546</cell><cell>0.2293</cell><cell>0.0888</cell><cell>0.0819</cell><cell>0.0792</cell><cell>0.055</cell><cell>0.0593</cell><cell>0.0614</cell></row><row><cell>ntc.ntc</cell><cell>0.2592</cell><cell>0.2332</cell><cell>0.0892</cell><cell>0.0794</cell><cell>0.0841</cell><cell>0.0519</cell><cell>0.0663</cell><cell>0.0545</cell></row><row><cell>lnc.ntc</cell><cell>0.2710</cell><cell>0.2363</cell><cell>0.0898</cell><cell>0.0791</cell><cell>0.0858</cell><cell>0.0576</cell><cell>0.0652</cell><cell>0.0604</cell></row><row><cell>ntc.nnc</cell><cell>0.2344</cell><cell>0.2172</cell><cell>0.0858</cell><cell>0.0769</cell><cell>0.0745</cell><cell>0.0466</cell><cell>0.0585</cell><cell>0.062</cell></row><row><cell>anc.ntc</cell><cell>0.2759</cell><cell>0.2343</cell><cell>0.0723</cell><cell>0.0623</cell><cell>0.0664</cell><cell>0.0376</cell><cell>0.0518</cell><cell>0.0398</cell></row><row><cell>ltc.ntc</cell><cell>0.2639</cell><cell>0.2273</cell><cell>0.0794</cell><cell>0.0623</cell><cell>0.0754</cell><cell>0.0449</cell><cell>0.0596</cell><cell>0.0428</cell></row><row><cell>atc.ntc</cell><cell>0.2606</cell><cell>0.2184</cell><cell>0.0592</cell><cell>0.0477</cell><cell>0.0525</cell><cell>0.0287</cell><cell>0.0437</cell><cell>0.0304</cell></row><row><cell>nnn.ntn</cell><cell>0.2476</cell><cell>0.2228</cell><cell>0.0900</cell><cell>0.0852</cell><cell>0.0799</cell><cell>0.0503</cell><cell>0.0599</cell><cell>0.061</cell></row><row><cell>ntn.ntn</cell><cell>0.2738</cell><cell>0.2369</cell><cell>0.0933</cell><cell>0.0795</cell><cell>0.0843</cell><cell>0.0507</cell><cell>0.0691</cell><cell>0.0578</cell></row><row><cell>lnn.ntn</cell><cell>0.2858</cell><cell>0.245</cell><cell>0.0969</cell><cell>0.0799</cell><cell>0.0905</cell><cell>0.0566</cell><cell>0.0701</cell><cell>0.0589</cell></row><row><cell>ntn.nnn</cell><cell>0.2476</cell><cell>0.2228</cell><cell>0.0900</cell><cell>0.0852</cell><cell>0.0799</cell><cell>0.0503</cell><cell>0.0599</cell><cell>0.061</cell></row><row><cell>ann.ntn</cell><cell>0.2903</cell><cell>0.2441</cell><cell>0.0750</cell><cell>0.0670</cell><cell>0.0743</cell><cell>0.038</cell><cell>0.057</cell><cell>0.0383</cell></row><row><cell>ltn.ntn</cell><cell>0.2870</cell><cell>0.2435</cell><cell>0.0799</cell><cell>0.0655</cell><cell>0.0871</cell><cell>0.0522</cell><cell>0.0701</cell><cell>0.0501</cell></row><row><cell>atn.ntn</cell><cell>0.2843</cell><cell>0.2364</cell><cell>0.0620</cell><cell>0.0546</cell><cell>0.0722</cell><cell>0.0347</cell><cell>0.0586</cell><cell>0.0355</cell></row><row><cell>In(exp)C2</cell><cell>0.3177</cell><cell>0.2737</cell><cell>0.0885</cell><cell>0.0744</cell><cell>0.0908</cell><cell>0.0487</cell><cell>0.0747</cell><cell>0.0614</cell></row><row><cell>Fusion 1</cell><cell>0.3208</cell><cell>0.2761</cell><cell>0.0969</cell><cell>0.0855</cell><cell>0.0912</cell><cell>0.0622</cell><cell>0.0731</cell><cell>0.0682</cell></row><row><cell cols="2">% change 1.0%</cell><cell>0.9%</cell><cell>0.0%</cell><cell>6.5%</cell><cell>0.4%</cell><cell>21.7%</cell><cell>-2.2%</cell><cell>10.0%</cell></row><row><cell>Fusion 2</cell><cell>0.3182</cell><cell>0.2741</cell><cell cols="2">0.0975 0.0842</cell><cell>0.0942</cell><cell>0.0602</cell><cell>0.0752</cell><cell>0.0619</cell></row><row><cell cols="2">% change 0.2%</cell><cell>0.1%</cell><cell>0.6%</cell><cell>5.1%</cell><cell>3.6%</cell><cell>19.1%</cell><cell>0.7%</cell><cell>0.8%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,73.93,421.95,450.90,8.10;6,88.56,432.27,336.62,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,196.04,421.95,268.36,8.10">Experiments for the Cross Language Speech Retrieval Task at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alzghool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,480.77,421.95,44.06,8.10;6,88.56,432.27,50.27,8.10">Proceedings of CLEF 2006</title>
		<title level="s" coord="6,145.09,432.27,126.99,8.10">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2006</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="778" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,442.59,450.92,8.10;6,88.56,452.97,376.52,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,229.38,442.59,295.47,8.10;6,88.56,452.97,60.87,8.10">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,155.82,452.97,155.72,8.10">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002-10">October (2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,463.29,450.97,8.10;6,88.56,473.61,161.52,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,228.83,463.29,221.58,8.10">Automatic retrieval with locality information using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,470.45,463.29,54.45,8.10;6,88.56,473.61,78.20,8.10">Text REtrieval Conference (TREC-1)</title>
		<imprint>
			<date type="published" when="1993-03">March (1993</date>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,483.99,450.97,8.10;6,88.56,494.31,321.67,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,283.50,483.99,237.54,8.10">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,88.56,494.31,183.95,8.10">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001-01">January (2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,504.63,450.90,8.10;6,88.56,515.01,361.94,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,228.12,504.63,128.91,8.10">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,365.78,504.63,159.06,8.10;6,88.56,515.01,78.24,8.10">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<publisher>National Institute of Standards and Technology Special Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,525.33,450.85,8.10;6,88.56,535.71,436.37,8.10;6,88.56,546.03,229.28,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,234.60,525.33,290.19,8.10;6,88.56,535.71,42.32,8.10">Using various indexing schemes and multiple translations in the CL-SR task at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alzghool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,150.54,535.71,374.39,8.10;6,88.56,546.03,69.01,8.10">Accessing Multilingual Information Repositories, 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">21-23 September. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,556.35,446.75,8.10;6,88.56,566.73,417.13,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,151.85,556.35,277.19,8.10">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.42,556.35,85.27,8.10;6,88.56,566.73,367.87,8.10">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,577.05,450.82,8.10;6,88.56,587.37,436.23,8.10;6,88.56,597.75,26.29,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,88.56,587.37,334.51,8.10">Building an Information Retrieval Test Collection for Spontaneous Conversational Speech</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gustman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.78,587.37,79.71,8.10">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,73.93,608.07,450.92,8.10;6,88.56,618.45,271.69,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,265.13,608.07,245.60,8.10">Overview of the CLEF 2007 cross-language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.56,618.45,165.49,8.10">Working Notes of the CLEF-2007 Evaluation</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,78.06,628.77,446.81,8.10;6,88.56,639.09,385.42,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,369.08,628.77,141.17,8.10">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://ir.dcs.gla.ac.uk/wiki/Terrier" />
	</analytic>
	<monogr>
		<title level="m" coord="6,88.56,639.09,227.01,8.10">27th European Conference on Information Retrieval (ECIR 05)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,78.06,649.47,446.74,8.10;6,88.56,659.79,114.52,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,186.71,649.47,180.60,8.10">Term-weighting approaches in automatic retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,373.08,649.47,147.24,8.10">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
