<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.90,148.86,237.21,15.15;1,133.61,170.78,335.79,15.15">Overview of the CLEF-2007 Cross-Language Speech Retrieval Track</title>
				<funder ref="#_S2sPRz7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_ATfMpnj">
					<orgName type="full">Ministry of Education of the Czech Republic</orgName>
				</funder>
				<funder ref="#_aJYn7z5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.19,204.67,55.34,8.74"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
							<email>pecina@ufal.mff.cuni.czhoffmannova@knih.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<address>
									<addrLine>Charles University Malostranske namesti 25</addrLine>
									<postCode>118 00</postCode>
									<settlement>Praha 1</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.23,204.67,84.58,8.74"><forename type="first">Petra</forename><surname>Hoffmannov√°</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<address>
									<addrLine>Charles University Malostranske namesti 25</addrLine>
									<postCode>118 00</postCode>
									<settlement>Praha 1</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.39,288.36,78.33,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ieyzhang@computing.dcu.ie</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.41,288.36,51.20,8.74"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.20,372.04,76.60,8.74"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.90,148.86,237.21,15.15;1,133.61,170.78,335.79,15.15">Overview of the CLEF-2007 Cross-Language Speech Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D0E28A516B83A09A26542E518F87EF4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Speech Retrieval</term>
					<term>Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CLEF-2007 Cross-Language Speech Retrieval (CL-SR) track included two tasks: to identify topically coherent segments of English interviews in a known-boundary condition, and to identify time stamps marking the beginning of topically relevant passages in Czech interviews in an unknown-boundary condition. Six teams participated in the English evaluation, performing both monolingual and cross-language searches of ASR transcripts, automatically generated metadata, and manually generated metadata. Four teams participated in the Czech evaluation, performing monolingual searches of automatic speech recognition transcripts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 2007 Cross-Language Evaluation Forum (CLEF) Cross-Language Speech Retrieval (CL-SR) track is the third and final year for evaluation of ranked retrieval from spontaneous conversational speech from an oral history collection at CLEF. As in the CLEF 2006 CL-SR task [2], automatically transcribed interviews conducted in English could be searched using queries in one of six languages, and automatically transcribed interviews conducted in Czech could be searched using queries in one of two languages. New relevance judgments for additional topics were created to expand the Czech collection in 2007. The English collection used in 2007 was the same as that used in 2006. As in CLEF 2005 and CLEF 2006, the English task was based on a known-boundary condition for topically coherent segments. The Czech task was based on a unknown-boundary condition in which participants were required to identify a time stamp for the beginning of each distinct topically relevant passage.</p><p>The remainder of this paper is organized as follows. Section 2 describes the English task and summarizes the results for the submitted runs. Section 3 does the same for the Czech task. The paper concludes in Section 4 with a brief recap of what has been learned across all three years of the CLEF CL-SR track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">English Task</head><p>The structure of the CLEF 2007 CL-SR English task was identical to that used in 2006, which we review here briefly (see [2] for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segments</head><p>The "documents" searched in the English task are 8,104 segments that were designated by professional indexers as topically coherent. A detailed description of the structure and fields of the English segment collection is given in the 2005 track overview paper [3]. Automatically generated transcripts from two Automatic Speech Recognition (ASR) systems are available. The ASR-TEXT2006B field contains a transcript generated using the best presently available ASR system, which has a mean word error rate of 25% on held-out data. Only 7,378 segments have text in this field. For the remaining 726 segments, no ASR output was available from that system, so in those cases the ASRTEXT2006B field includes content identical to the ASRTEXT2004A field (which has a 35% mean word error rate) which was generated using an earlier less accurate transcription system. An extensive set of manually and automatically generated metadata is also available for each segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>The same 63 training topics and 33 evaluation topics were used for the English task this year as had been used in 2006. Participating teams were asked not to use the evaluation topics for system tuning. Translations into Czech, Dutch, French, German, and Spanish had been created by native speakers of those languages. Participating teams were asked to submit runs for 105 topics (the 63 training topics, the 33 evaluation topics, and 9 further topics for which relevance data is not currently available, to support possible future construction of new relevance assessment pools), but results are reported only for the 33 evaluation topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measure</head><p>As in the CLEF-2006 CL-SR track, we report uninterpolated Mean Average Precision (MAP) as the principal measure of retrieval effectiveness. Version 8.0 of the trec eval program was used to compute this measure. <ref type="bibr" coords="3,190.08,110.45,3.97,6.12" target="#b0">1</ref> The Wilcoxon signed-rank signed test was employed for evaluation of significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Judgments</head><p>Subject matter experts created multi-scale and multi-level relevance assessments in the same manner as was done for the CLEF-2005 CL-SR track [3]. These were then conflated into binary judgments using the same procedure as was used for CLEF-2005: the union of direct and indirect relevance judgments with scores of 2, 3, or 4 (on a 0-4 scale) were treated as topically relevant, and any other case as non-relevant. This resulted in a total of 20, 560 binary judgments across the 33 topics, among which 2, 449 (12%) are relevant. Results from 2007 may not be strictly comparable with results from 2006; both were generated from the same initial set of relevance judgments, but those judgments were filtered at different sites in 2006 and 2007, in both cases to remove judgments for segments that are not contained in the distributed collection, and we have not yet done a detailed comparison of the results of those filtering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Techniques</head><p>This section gives a brief description of the methods used by each team participating in the English task. Additional details are available in each team's paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Brown University (BLLIP)</head><p>The Brown Laboratory for Linguistic Information Processing (BLLIP) team extended the basic Dirichlet-smoothed unigram IR model to incorporate bigram mixing and collection smoothing.</p><p>In their enhanced language model, the bigram and unigram models were mixed using a tunable mixture weight over all documents. They attempted linearly mixing the test collection with two larger text corpora, 40,000 sentences from the Wall Street Journal and 450,000 sentences from the North American News Corpus, in order to alleviate the sparse data problems in the case of small collections. They observed that bigram statistics appeared to have greater impact with pseudo-relevance feedback than without. The collection smoothing approach clearly provided a substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Dublin City University (DCU)</head><p>Dublin City University concentrated on the issues of topic translation, combining this with search field combination and pseudo-relevance feedback methods used for their CLEF 2006 submissions. Non-English topics were translated into English using the Yahoo! BabelFish free online translation service combined with domain-specific translation lexicons gathered automatically from Wikipedia. The combination of multiple fields using the BM25F variant of Okapi weights was explored. Additionally, they integrated their information retrieval methods based on the Okapi model with summary-based pseudo-relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">University of Amsterdam (UVA)</head><p>The University of Amsterdam explored the use of character n-gram tokenization to improve the retrieval of documents using automatically generated text, as well as the combination of manually generated with automatically generated text. They reported that n = 4 provided the best retrieval effectiveness when the cross-word overlapping n-gram tokenization strategy is used. The field combination was done using the Indri query language, in which varying weights were assigned to different fields. Cross-language experiments were conducted using manually created Dutch topics donated by the University of Twente. Dutch topics were automatically translated into English using two different online tools, SYSTRAN and FreeTranslation. The translations generated from each MT system were then combined as a 'bag-of-words' English query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">University of Chicago (UC)</head><p>The University of Chicago team focused on the contribution of automatically assigned thesaurus terms to retrieval effectiveness and the utility of different query translation strategies. For French-English cross-language retrieval, they adopted two query translation strategies: MT-based translation using the publicly available translation tool provided by Google, and dictionary-based translation. Their dictionary-based translation procedure applied a backoff stemming strategy in order to support matching with highest precision between the query terms and the bilingual word list. They noted that 27% of the French query terms remained untranslated and were thus retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">University of Ja√©n (SINAI)</head><p>The SINAI group at the University of Ja√©n investigated the effect of selection of different fields (referred to as "labels" in their paper) on retrieval effectiveness. The Information Gain measure was employed to select the best XML tags in the document collection. The tags with higher values of Information Gain were selected to compose the final collection. Their experiments were conducted with the Lemur retrieval information system by applying the KL-divergence weighing scheme. The French, German and Spanish topics were translated to English using a translation module, SINTRAM, which works with different online machine translators and combines the different translations based on heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.6">University of Ottawa (UO)</head><p>The University of Ottawa used weighted summation of normalized similarity measures to combine 15 different weighting schemes from two IR systems (Terrier and SMART). Two query expansion techniques, one based on the thesaurus and the other one on blind relevance feedback, were examined. In their cross-language experiments, the queries were automatically translated from French and Spanish into English by combining the results of multiple online machine translation tools. Results for an extensive set of locally scored runs were also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Results</head><p>Table <ref type="table" coords="4,117.60,480.45,4.98,8.74">1</ref> summarizes the evaluation results for all 29 official runs averaged over the 33 evaluation topics, listed in descending order of MAP. These 29 runs were further categorized into four groups based on the query language used (English or non-English) and the document fields (automaticonly or at least one manual assigned) indexed: 9 automatic-only monolingual runs, 6 automaticonly cross-language runs, 9 monolingual runs with manually assigned metadata, and 5 crosslanguage runs with manually assigned metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Automatic-Only Monolingual Runs</head><p>Teams were required to run at least one monolingual condition using the title (T) and description (D) fields of the topics and indexing only automatically generated fields; the best "required runs" are shown in bold in Table <ref type="table" coords="4,211.46,608.42,4.98,8.74" target="#tab_0">2</ref> as a basis for cross-system comparisons. The University of Ottawa (0.0855), Dublin City University (0.0787), and the BLLIP team (0.0785) reported comparable results (no significant difference at the 95% confidence level). These results are statistically significant better than those reported by the next two teams, the University of Chicago (0.0571) and the University of Amsterdam (0.0444), which were statistically indistinguishable from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Automatic-Only Cross-Language Runs</head><p>As shown in Table <ref type="table" coords="4,174.17,700.53,3.88,8.74">3</ref>, the best result (0.0636) for cross-language runs on automatically generated indexing data (a French-English run from Dublin City University) achieved 81% of the monolingual retrieval effectiveness with comparable conditions (0.0787 as shown in  <ref type="table" coords="5,117.94,496.28,3.88,8.74">1</ref>: Evaluation results for all English official runs. MK = MANUALKEYWORD (Manual metadata), SUM = SUMMARY (Manual metadata), AK1 = AUTOKEYWORD2004A1 (Automatic), AK2 = AUTOKEYWORD2004A2, ASR03 = ASRTEXT2003A (Automatic), ASR04 = ASRTEXT2004A (Automatic), ASR06A = ASRTEXT2006A (Automatic), ASR06B = ASR-TEXT2006B (Automatic), and ALL = all fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Monolingual Runs With Manual Metadata</head><p>For monolingual TD runs on manually generated indexing data, the University of Ottawa achieved the best result (0.2761), which is statistically significantly better than all other runs under comparable conditions, as shown in Table <ref type="table" coords="5,246.21,618.78,3.88,8.74">4</ref>. For TDN runs, the DCU result (0.2847) it not statistically significantly better than that obtained by BLLIP (0.2577).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">Cross-Language Runs With Manual Metadata</head><p>The evaluation results for cross-language runs on manually generated indexing data are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SINAI</head><p>Table <ref type="table" coords="7,119.12,217.17,3.88,8.74" target="#tab_1">5</ref>: Evaluation results for cross-language runs with manual metadata. MK = MANU-ALKEYWORD, SUM = SUMMARY, AK1 = AUTOKEYWORD2004A1, AK2 = AUTOKEY-WORD2004A2, ASR04 = ASRTEXT2004A, ASR06A = ASRTEXT2006A, ASR06B = ASR-TEXT2006B, and ALL = all fields.</p><p>overview paper [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interviews</head><p>The default "quickstart" collection was generated from the same set of 357 Czech interviews as in 2006. It contained 11,377 overlapping passages with the following fields:</p><p>DOCNO containing a unique document number in the same format as the start times that systems were required to produce in a ranked list.</p><p>INTERVIEWDATA containing the first name and last initial for the person being interviewed. This field is identical for every passage that was generated from the same interview.</p><p>ASRSYSTEM specifying the type of the ASR transcript, where "2004" and "2006" denote colloquial and formal Czech transcripts respectively.</p><p>CHANNEL specifying which recorded channel (left or right) was used to produce the transcript.</p><p>ASRTEXT containing words in order from the transcript selected by ASRSYSTEM and CHAN-NEL for a passage beginning at the start time indicated in DOCNO.</p><p>The average passage duration in the default 2007 quickstart collection is 3.75 minutes, and each passage has a 33% overlap with the subsequent passage (i.e., passages begin about every 2.5 minutes).</p><p>No thesaurus terms (neither manual nor automatic, neither English nor Czech) were distributed with the collection this year. This step was taken as an expedient because it was not practical to correct the time misalignment that was present in the 2006 quickstart collection for the manually assigned thesaurus terms (and because automatically assigned thesaurus terms had not proven to be useful in 2006, perhaps because of poorly matched training data having been used to train the classifier).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>This year we released a total of 118 topics: 105 original English topics from 2006, 10 broadened Czech topics from 2006, and 3 new broadened topics that were constructed this year. All topics were originally created in English and then translated into Czech by native speakers. Some minor errors in the Czech translations from last year were corrected. <ref type="bibr" coords="7,359.25,690.66,3.97,6.12" target="#b1">2</ref> Translations into other languages were not distributed with the collection. No teams used the English topics this year; all official runs with the Czech collection were monolingual. Participating teams were asked to run all 118 available topics. Two of the 118 topics were used as assessment training topics and excluded from the evaluation, 29 topics were available for training systems (with relevance judgments from 2006), 50 of the remaining 87 topics were selected as possible evaluation topics (with at least 6 relevant passages identified during the search-guided assessment phase), highly-ranked (i.e., "pooled") assessment was completed for 42 of those 50 topics, and those 42 were used as the evaluation topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Measure</head><p>The evaluation measure used in the Czech task is the same as in 2006. It's based on the mean Generalized Average Precision (mGAP) measure, which was originally introduced to deal with human assessments of partial relevance [1]. In our case, the human assessments are binary but the degree of match to those assessments can be partial. The Wilcoxon signed-rank signed test was employed for evaluation of significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relevance Judgments</head><p>Relevance judgments were completed at Charles University in Prague for 42 topics this year under the same conditions as in 2006 by six relevance assessors. Evaluation topics had been selected to have at least six relevant start times in the Czech collection in order to minimize the effect of quantization noise on the computation of mGAP. A total of 2,389 start times for relevant passages were identified, thus yielding an average of 56 relevant passages per topic (minimum 6, maximum 199). Table <ref type="table" coords="8,145.61,570.29,4.24,8.74">3</ref>.4 shows the number of relevant start times for each of the 42 topics. To support future experiments on searching a bilingual speech collection, 34 of the 2007 CLEF CL-SR Czech task evaluation topics also present in the 2007 CLEF CL-SR English task collection (as training, evaluation, or unused topics). <ref type="bibr" coords="8,218.55,604.58,3.97,6.12" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Techniques</head><p>All participating teams employed existing information retrieval systems to perform monolingual retrieval and submitted total of 15 runs for official scoring. Each team submitted at least one run in the required title+description condition. The narrative field was used only in two runs by University of West Bohemia. Most of the teams used only automatically generated queries. Manual query construction was performed only by Charles University. All teams used the provided quickstart collection for at least some runs. The University of West Bohemia also used the quickstart scripts with different parameters to generate another collection for some experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Brown University (BLLIP)</head><p>The system of Brown University was based on the language model paradigm for retrieval and implemented in the Indri system. A unigram language model, Czech-specific stemming, and pseudo-relevance feedback were applied in three officially submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Charles University (CUNI)</head><p>The Charles University team performed experiments with Indri retrieval model from the Lemur project with pseudo-relevance feedback, stopwords removal, and morphological lemmatization obtained by in-house morphological analysis and a part-of-speech tagger. The team submitted four official runs; two of them employed manual query construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">University of Chicago (UC)</head><p>The University of Chicago employed the InQuery information retrieval system with stop-word removal and three different stemming approaches: no stemming, light stemming, and aggressive stemming. Three runs were submitted for official scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">University of West Bohemia (UWB)</head><p>The University of West Bohemia employed a TF*IDF model with blind relevance feedback implemented in Lemur. Five runs submitted for official scoring differed in methods used for word normalization (none, lemmatization, stemming), in formulas used for term weighting (Raw TF, BM25), and in topic fields used (TDN, TD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">Results</head><p>The results of all official runs evaluated on 42 topics are reported in  average passage duration (from 3.75 minutes to 2.5 minutes) and to increase the overlap between subsequent passages (from 33% to 50%). This had the effect of substantially decreasing the average start-time spacing between passages (from 2.5 for 1.25 minutes). This resulted in an apparent improvement in mGAP (compare UWB 2-1 tdn l: mGAP=0.0264 and UWB 3-1 tdn l: mGAP=0.0237) that turned out not to be statistically significant. The two-sided width of the scoring window is set at 5 minutes in our evaluation script, so this range of start time spacings is well within the scorable range, but more closely spaced passages offer some potential for reducing quantization noise in the evaluation script. Although we compute evaluation results only from start times, out assessors marked both start and end times. Their average duration of a marked relevant passage is 2.83 minutes, which seems to be somewhat better matched to the 2.5 minutes passages used in the University of West Bohemia's alternate condition (2.5 minutes for UWB 2-1 tdn l, 3.75 minutes for UWB 3-1 tdn l and all runs from other sites).</p><p>The Charles University team reported on the first experiments with interactive use of the Czech collection. One of their runs based on manual query construction turned out to be statistically indistinguishable from a run under comparable conditions from the same team with queries that were generated automatically, and a second run with manually formed queries did not do well at all (probably because lemmatization was not used in that second run).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Plans</head><p>Like all CLEF tracks, the CL-SR track had three key goals: (1) to develop evaluation methods and reusable evaluation resources for an important information access problem in which crosslanguage information access is a natural part of the task, (2) to generate results that can provide a strong baseline against which future research results with the same evaluation resources can be compared, and (3) to foster the development of a research community with the experience and expertise to make those future advances. In the case of the CL-SR track, those goals have now been achieved. Over 3 years, research teams from 14 universities in 6 countries submitted 123 runs for official scoring, and additional locally scored runs have been reported in papers published by those research teams. The resulting English and Czech collections are the first information retrieval test collections of substantial size for spontaneous conversational speech, unique characteristics of the English collection have fostered new research comparing searches based on automatic speech recognition and manually assigned metadata, and unique characteristics of the Czech collection have inspired new research on evaluation of information retrieval from unsegmented speech. Now that the track has been completed, these new CLEF test collections will be made available to nonparticipants through the Evaluations and Language Resources Distribution Agency (ELDA). The training data for the automatic speech retrieval systems that were used to generate the transcripts in those collections is also expected to become available soon, most likely through the Linguistic Data Consortium (LDC). It is our hope that these resources will be used together to investigate more closely coupled techniques than have been possible to date with just the present CLEF CL-SR test collections. Looking further forward, we believe that it is now time for the information retrieval research community to look beyond oral history to other instances of spontaneous conversational speech such as that found in recordings of meetings, historically significant telephone conversations, and broadcast conversations (e.g., call-in radio "talk shows"). We also believe that it would be productive to begin to explore the application of some of the technology developed for this track to improve access to a broad range oral history collections and similar cultural heritage materials (e.g., interviews contained in broadcast archives). Together, these directions for future work will likely continue to extend the legacy and impact of this initial investment in exploring the retrieval of information from spontaneous conversational speech.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,397.82,724.44,39.03,8.74"><head>Table 2 )</head><label>2</label><figDesc>.</figDesc><table coords="5,90.00,112.82,421.77,392.20"><row><cell>Run ID</cell><cell>MAP</cell><cell cols="3">Lang Query Document Fields</cell><cell>Site</cell></row><row><cell>dcuEnTDNmanual</cell><cell>0.2847</cell><cell>EN</cell><cell>TDN</cell><cell>MK,SUM</cell><cell>DCU</cell></row><row><cell>uoEnTDtManF1</cell><cell>0.2761</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM</cell><cell>UO</cell></row><row><cell>brown.TDN.man</cell><cell>0.2577</cell><cell>EN</cell><cell>TDN</cell><cell>MK,SUM</cell><cell>BLLIP</cell></row><row><cell cols="2">dcuEnTDmanualauto 0.2459</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM,ASR06B</cell><cell>DCU</cell></row><row><cell>brown.TD.man</cell><cell>0.2366</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM</cell><cell>BLLIP</cell></row><row><cell>brown.T.man</cell><cell>0.2348</cell><cell>EN</cell><cell>T</cell><cell>MK,SUM</cell><cell>BLLIP</cell></row><row><cell>UvA 4 enopt</cell><cell>0.2088</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM,ASR06B</cell><cell>UVA</cell></row><row><cell cols="2">dcuFrTDmanualauto 0.1980</cell><cell>FR</cell><cell>TD</cell><cell>MK,SUM,ASR06B</cell><cell>DCU</cell></row><row><cell>UvA 5 nlopt</cell><cell>0.1408</cell><cell>NL</cell><cell>TD</cell><cell>MK,SUM,AK2,ASR06B</cell><cell>UVA</cell></row><row><cell>uoEnTDtQExF1</cell><cell>0.0855</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>uoEnTDtQExF2</cell><cell>0.0841</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>brown.TDN.auto</cell><cell>0.0831</cell><cell>EN</cell><cell>TDN</cell><cell>AK1,AK2,ASR06B</cell><cell>BLLIP</cell></row><row><cell>dcuEnTDauto</cell><cell>0.0787</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>DCU</cell></row><row><cell>brown.TD.auto</cell><cell>0.0785</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>BLLIP</cell></row><row><cell>SinaiSp100</cell><cell>0.0737</cell><cell>ES</cell><cell>TD</cell><cell>ALL</cell><cell>SINAI</cell></row><row><cell>dcuFrTDauto</cell><cell>0.0636</cell><cell>FR</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>DCU</cell></row><row><cell>uoEsTDtF2</cell><cell>0.0619</cell><cell>ES</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>uoFrTDtF2</cell><cell>0.0603</cell><cell>FR</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>SinaiFr100</cell><cell>0.0597</cell><cell>FR</cell><cell>TD</cell><cell>ALL</cell><cell>SINAI</cell></row><row><cell>SinaiEn100</cell><cell>0.0597</cell><cell>EN</cell><cell>TD</cell><cell>ALL</cell><cell>SINAI</cell></row><row><cell>SinaiSp050</cell><cell>0.0579</cell><cell>ES</cell><cell>TD</cell><cell>SUM,AK1,AK2,ASR04,ASR06A,ASR06B</cell><cell>SINAI</cell></row><row><cell>UCkwENTD</cell><cell>0.0571</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>UC</cell></row><row><cell>SinaiEn050</cell><cell>0.0515</cell><cell>EN</cell><cell>TD</cell><cell>SUM,AK1,AK2,ASR04,ASR06A,ASR06B</cell><cell>SINAI</cell></row><row><cell>UCbaseENTD1</cell><cell>0.0512</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UC</cell></row><row><cell>UvA 2 en4g</cell><cell>0.0444</cell><cell>EN</cell><cell>TD</cell><cell>AK2,ASR06B</cell><cell>UVA</cell></row><row><cell>UvA 1 base</cell><cell>0.0430</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UVA</cell></row><row><cell>UCkwFRTD1</cell><cell>0.0406</cell><cell>FR</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>UC</cell></row><row><cell>UvA 3 nl4g</cell><cell>0.0400</cell><cell>NL</cell><cell>TD</cell><cell>AK2,ASR06B</cell><cell>UVA</cell></row><row><cell>UCbaseFRTD1</cell><cell>0.0322</cell><cell>FR</cell><cell>TD</cell><cell>ASR06B</cell><cell>UC</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,686.97,423.00,32.65"><head>Table 5</head><label>5</label><figDesc></figDesc><table coords="6,103.76,113.65,395.48,127.49"><row><cell>Run ID</cell><cell>MAP</cell><cell>Lang</cell><cell>Query</cell><cell>Document Fields</cell><cell>Site</cell></row><row><cell>uoEnTDtQExF1</cell><cell>0.0855</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>uoEnTDtQExF2</cell><cell>0.0841</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>brown.TDN.auto</cell><cell>0.0831</cell><cell>EN</cell><cell>TDN</cell><cell>AK1,AK2,ASR06B</cell><cell>BLLIP</cell></row><row><cell>dcuEnTDauto</cell><cell>0.0787</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>DCU</cell></row><row><cell>brown.TD.auto</cell><cell>0.0785</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>BLLIP</cell></row><row><cell>UCkwENTD</cell><cell>0.0571</cell><cell>EN</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>UC</cell></row><row><cell>UCbaseENTD1</cell><cell>0.0512</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UC</cell></row><row><cell>UvA 2 en4g</cell><cell>0.0444</cell><cell>EN</cell><cell>TD</cell><cell>AK2,ASR06B</cell><cell>UVA</cell></row><row><cell>UvA 1 base</cell><cell>0.0430</cell><cell>EN</cell><cell>TD</cell><cell>ASR06B</cell><cell>UVA</cell></row></table><note coords="5,134.75,686.97,378.25,8.74;5,90.00,698.93,423.00,8.74;5,90.00,710.88,85.90,8.74"><p><p><p>. The best cross-language result (0.1980), representing 81% of monolingual retrieval effectiveness under comparable conditions (0.2459 shown in Table</p>4</p>), was achieved by DCU's French-English run.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,269.77,423.01,479.85"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for automatic English monolingual runs. Bold runs are the required</figDesc><table coords="6,90.00,281.73,423.00,434.97"><row><cell cols="7">condition. AK1 = AUTOKEYWORD2004A1, AK2 = AUTOKEYWORD2004A2, ASR03 = AS-</cell></row><row><cell cols="7">RTEXT2003A, ASR04 = ASRTEXT2004A, ASR06A = ASRTEXT2006A, and ASR06B = ASR-</cell></row><row><cell>TEXT2006B.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run ID</cell><cell>MAP</cell><cell cols="2">Lang</cell><cell>Query</cell><cell>Document Fields</cell><cell>Site</cell></row><row><cell>dcuFrTDauto</cell><cell>0.0636</cell><cell cols="2">FR</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>DCU</cell></row><row><cell>uoEsTDtF2</cell><cell>0.0619</cell><cell>ES</cell><cell></cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>uoFrTDtF2</cell><cell>0.0603</cell><cell cols="2">FR</cell><cell>TD</cell><cell>AK1,AK2,ASR04</cell><cell>UO</cell></row><row><cell>UCkwFRTD1</cell><cell>0.0406</cell><cell cols="2">FR</cell><cell>TD</cell><cell>AK1,AK2,ASR06B</cell><cell>UC</cell></row><row><cell>UvA 3 nl4g</cell><cell>0.0400</cell><cell cols="2">NL</cell><cell>TD</cell><cell>AK2,ASR06B</cell><cell>UVA</cell></row><row><cell>UCbaseFRTD1</cell><cell>0.0322</cell><cell cols="2">FR</cell><cell>TD</cell><cell>ASR06B</cell><cell>UC</cell></row><row><cell cols="7">Table 3: Evaluation results for automatic cross-language runs. AK1 = AUTOKEYWORD2004A1,</cell></row><row><cell cols="7">AK2 = AUTOKEYWORD2004A2, ASR04 = ASRTEXT2004A, and ASR06B = ASRTEXT2006B.</cell></row><row><cell>Run ID</cell><cell>MAP</cell><cell cols="4">Lang Query Document Fields</cell><cell>Site</cell></row><row><cell>dcuEnTDNmanual</cell><cell>0.2847</cell><cell>EN</cell><cell>TDN</cell><cell>MK,SUM</cell><cell></cell><cell>DCU</cell></row><row><cell>uoEnTDtManF1</cell><cell>0.2761</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM</cell><cell></cell><cell>UO</cell></row><row><cell>brown.TDN.man</cell><cell>0.2577</cell><cell>EN</cell><cell>TDN</cell><cell>MK,SUM</cell><cell></cell><cell>BLLIP</cell></row><row><cell cols="2">dcuEnTDmanualauto 0.2459</cell><cell>EN</cell><cell>TD</cell><cell cols="2">MK,SUM,ASR06B</cell><cell>DCU</cell></row><row><cell>brown.TD.man</cell><cell>0.2366</cell><cell>EN</cell><cell>TD</cell><cell>MK,SUM</cell><cell></cell><cell>BLLIP</cell></row><row><cell>brown.T.man</cell><cell>0.2348</cell><cell>EN</cell><cell>T</cell><cell>MK,SUM</cell><cell></cell><cell>BLLIP</cell></row><row><cell>UvA 4 enopt</cell><cell>0.2088</cell><cell>EN</cell><cell>TD</cell><cell cols="2">MK,SUM,ASR06B</cell><cell>UVA</cell></row><row><cell>SinaiEn100</cell><cell>0.0597</cell><cell>EN</cell><cell>TD</cell><cell>ALL</cell><cell></cell><cell>SINAI</cell></row><row><cell>SinaiEn050</cell><cell>0.0515</cell><cell>EN</cell><cell>TD</cell><cell cols="2">SUM,AK1,AK2,ASR04,ASR06A,ASR06B</cell><cell>SINAI</cell></row><row><cell cols="7">Table 4: Evaluation results for monolingual English runs with manual metadata. MK = MAN-</cell></row><row><cell cols="7">UALKEYWORD, SUM = SUMMARY, AK1 = AUTOKEYWORD2004A1, AK2 = AUTOKEY-</cell></row><row><cell cols="7">WORD2004A2, ASR04 = ASRTEXT2004A, ASR06A = ASRTEXT2006A, ASR06B = ASR-</cell></row><row><cell cols="3">TEXT2006B, and ALL = all fields.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 Czech Task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="6,90.00,728.93,423.00,8.74;6,90.00,740.89,423.00,8.74"><p>The structure of the Czech task was quite similar to the one used in the 2006 with differences which we describe in the following subsections. Further details can be found in the 2006 track</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,125.06,112.82,352.88,174.46"><head>Table 6 :</head><label>6</label><figDesc>Number of relevant passages identified for each of the evaluation topics.</figDesc><table coords="8,165.65,112.82,271.70,140.64"><row><cell cols="8">Topic # rel Topic # rel Topic # rel Topic # rel</cell></row><row><cell>1192</cell><cell>18</cell><cell>2265</cell><cell>113</cell><cell>3019</cell><cell>14</cell><cell>4005</cell><cell>68</cell></row><row><cell>1345</cell><cell>12</cell><cell>2358</cell><cell>126</cell><cell>3021</cell><cell>16</cell><cell>4006</cell><cell>135</cell></row><row><cell>1554</cell><cell>46</cell><cell>2384</cell><cell>37</cell><cell>3022</cell><cell>29</cell><cell>4007</cell><cell>51</cell></row><row><cell>1829</cell><cell>6</cell><cell>2404</cell><cell>8</cell><cell>3023</cell><cell>78</cell><cell>4009</cell><cell>10</cell></row><row><cell>1897</cell><cell>31</cell><cell>3000</cell><cell>41</cell><cell>3024</cell><cell>105</cell><cell>4011</cell><cell>132</cell></row><row><cell>1979</cell><cell>17</cell><cell>3001</cell><cell>102</cell><cell>3026</cell><cell>33</cell><cell>4012</cell><cell>61</cell></row><row><cell>2000</cell><cell>114</cell><cell>3002</cell><cell>95</cell><cell>3027</cell><cell cols="2">86 14313</cell><cell>17</cell></row><row><cell>2006</cell><cell>63</cell><cell>3007</cell><cell>107</cell><cell>3028</cell><cell cols="2">199 15601</cell><cell>108</cell></row><row><cell>2012</cell><cell>90</cell><cell>3008</cell><cell>53</cell><cell>3032</cell><cell cols="2">9 15602</cell><cell>25</cell></row><row><cell>2185</cell><cell>25</cell><cell>3010</cell><cell>18</cell><cell>4001</cell><cell>35</cell><cell></cell><cell></cell></row><row><cell>2224</cell><cell>63</cell><cell>3016</cell><cell>40</cell><cell>4004</cell><cell>13</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,240.58,311.42,121.85,8.74"><head>Table 7 :</head><label>7</label><figDesc>Czech official runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,90.00,666.35,423.01,80.47"><head>Table 7 .</head><label>7</label><figDesc>The effect of term normalization handling the rich Czech morphology is quite significant. The runs employing any type of term normalization (stemming or lemmatization) outperform systems indexing only original word forms with no normalization by 61-119%. The scores of directly comparable runs are given in Table8, all the differences are statistically significant at a 95% confidence level.The second collection generated by the University of West Bohemia generated some interesting insights. They used the quickstart scripts distributed with the test collection to decrease the</figDesc><table coords="10,114.62,112.80,373.75,108.09"><row><cell>Run</cell><cell>mGAP</cell><cell>mGAP</cell><cell>Query</cell><cell>Topic</cell><cell>Term</cell><cell>Site</cell></row><row><cell>name</cell><cell>score</cell><cell cols="3">improvement construction fields</cell><cell>normalization</cell><cell>name</cell></row><row><cell>UWB 2-1 td s</cell><cell>0.0228</cell><cell>+76.7%</cell><cell>Auto</cell><cell>TD</cell><cell>stem</cell><cell>UWB</cell></row><row><cell cols="2">UWB 2-1 td w 0.0129</cell><cell></cell><cell>Auto</cell><cell>TD</cell><cell>none</cell><cell>UWB</cell></row><row><cell>UCcsaTD2</cell><cell>0.0203</cell><cell>+61.1%</cell><cell>Auto</cell><cell>TD</cell><cell>aggressive stem</cell><cell>UC</cell></row><row><cell>UCunstTD3</cell><cell>0.0126</cell><cell></cell><cell>Auto</cell><cell>TD</cell><cell>none</cell><cell>UC</cell></row><row><cell>prague02</cell><cell>0.0181</cell><cell>+77.5%</cell><cell>Manual</cell><cell>TD</cell><cell>lemma</cell><cell>CUNI</cell></row><row><cell>prague03</cell><cell>0.0102</cell><cell></cell><cell>Manual</cell><cell>TD</cell><cell>none</cell><cell>CUNI</cell></row><row><cell>brown.s.f</cell><cell>0.0114</cell><cell>+119.2%</cell><cell>Auto</cell><cell>TD</cell><cell>light stem</cell><cell>BLLIP</cell></row><row><cell>brown.f</cell><cell>0.0052</cell><cell></cell><cell>Auto</cell><cell>TD</cell><cell>none</cell><cell>BLLIP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,115.50,235.90,372.00,8.74"><head>Table 8 :</head><label>8</label><figDesc>Comparison of systems with term normalization and without normalization.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,747.00,263.11,6.99"><p>The trec eval program is available from http://trec.nist.gov/trec eval/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,735.08,407.75,6.99;7,90.00,744.54,383.85,6.99"><p>The corrected topics were 1259, 1282, 1551, 14313, and 24313. Of these, only topic 14313 was used in the 2006 Czech task evaluation, and none have been used for reported official results in the English task to date.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,105.24,743.10,213.93,6.99"><p>The exceptions being the broadened topics, which are the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="8,322.00,743.10,41.97,6.99"><p>4000-series.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This year's track would not have been possible without the efforts of a great many people. Our heartfelt thanks go to the dedicated group of relevance assessors in Prague without whom the Czech collection simply would not exist, to <rs type="person">Scott Olsson</rs> for helping to prepare the English collection this year, to <rs type="person">Ayelet Goldin</rs> and <rs type="person">Jianqiang Wang</rs> for their timely help with critical details of the Czech relevance assessment and scoring process, to <rs type="person">Jan Hajic</rs> for his support and advice throughout, and to <rs type="person">Carol Peters</rs> for her seemingly endless patience. This work has been supported in part by <rs type="funder">NSF</rs> <rs type="grantName">IIS</rs> award <rs type="grantNumber">0122466</rs> (MALACH) and by the <rs type="funder">Ministry of Education of the Czech Republic</rs>, projects <rs type="grantNumber">MSM 0021620838</rs> and #<rs type="grantNumber">1P05ME786</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_S2sPRz7">
					<idno type="grant-number">0122466</idno>
					<orgName type="grant-name">IIS</orgName>
				</org>
				<org type="funding" xml:id="_ATfMpnj">
					<idno type="grant-number">MSM 0021620838</idno>
				</org>
				<org type="funding" xml:id="_aJYn7z5">
					<idno type="grant-number">1P05ME786</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,105.50,400.88,407.50,8.74;11,105.50,412.84,359.67,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,282.88,400.88,225.81,8.74">Using graded relevance assessments in IR evaluation</title>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekalainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Jarvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,117.95,412.84,316.64,8.74">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.50,432.76,407.50,8.74;11,105.50,444.72,407.50,8.74;11,105.50,456.67,407.51,8.74;11,105.50,468.63,170.74,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,319.98,444.72,193.01,8.74;11,105.50,456.67,91.65,8.74">Overview of the CLEF-2006 cross-language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,218.54,456.67,294.47,8.74;11,105.50,468.63,140.25,8.74">Proceedings of the CLEF 2006 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2006 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,105.50,488.55,407.50,8.74;11,105.50,500.51,407.51,8.74;11,105.50,512.46,349.27,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,105.50,500.51,280.50,8.74">Overview of the CLEF-2005 cross-language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,405.97,500.51,107.03,8.74;11,105.50,512.46,318.78,8.74">Proceedings of the CLEF 2005 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2005 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
