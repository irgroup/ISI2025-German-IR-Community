<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.91,148.86,403.17,15.15;1,99.91,170.78,403.17,15.15;1,196.98,192.69,209.05,15.15">Attempts to Search Czech Spontaneous Spoken Interviews -the University of West Bohemia at CLEF 2007 CL-SR track</title>
				<funder ref="#_KaZwSJt">
					<orgName type="full">Grant Agency of the Czech Academy of Sciences</orgName>
				</funder>
				<funder ref="#_ZN6cb6h">
					<orgName type="full">Ministry of Education of the Czech Republic</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,234.78,226.59,51.92,8.74"><forename type="first">Pavel</forename><surname>Ircing</surname></persName>
							<email>ircing@kky.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">University of West Bohemia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.39,226.59,58.83,8.74"><forename type="first">Luděk</forename><surname>Müller</surname></persName>
							<email>muller@kky.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">University of West Bohemia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.91,148.86,403.17,15.15;1,99.91,170.78,403.17,15.15;1,196.98,192.69,209.05,15.15">Attempts to Search Czech Spontaneous Spoken Interviews -the University of West Bohemia at CLEF 2007 CL-SR track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">231ECB4E9DA0A7014E0FDB0CE7548E5F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>3 [Information Storage and Retrieval]: H</term>
					<term>3</term>
					<term>1 Content Analysis and Indexing; H</term>
					<term>3</term>
					<term>3 Information Search and Retrieval Speech Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents an overview of the system build and experiments performed for the CLEF 2007 CL-SR track by the University of West Bohemia. We have concentrated on the monolingual experiments using the Czech collection only. The approach that was successfully employed by our team in the last year's campaign (simple tf.idf model with blind relevance feedback, accompanied with solid linguistic preprocessing) was used again but the set of performed experiments was broadened.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Czech subtask of the CL-SR track, which was first introduced at CLEF 2006 campaign, is enormously challenging -let us repeat once again that the goal is to identify appropriate replay points (that is, the moments where the discussion about the queried topics starts) in a continuous stream of text generated by automatic transcription of spontaneous speech. Therefore, it is neither the standard document retrieval task (as there are no true documents defined) nor the fully-fledged speech retrieval (since the participants do not have the speech data nor the lattices, so they can't explore alternative hypotheses and must rely on one-best transcription). However, in order to lower the barrier of entry for teams proficient at classic document retrieval (or, for that matter, even total IR beginners), the last year's organisers prepared a so called Quickstart collection with artificially defined "documents" that were created by sliding 3-minute window over the stream of transcriptions with a 2-minute step (i.e., the consecutive documents have a one minute overlap). <ref type="foot" coords="1,161.43,702.68,3.97,6.12" target="#foot_0">1</ref> The last year's Quickstart collection was further equipped with both manually and automatically generated keywords (see <ref type="bibr" coords="2,281.47,112.02,10.52,8.74" target="#b4">[5]</ref> for details) but they have shown itself to be of no benefit for IR performance <ref type="bibr" coords="2,207.30,123.98,12.10,8.74" target="#b2">[3]</ref>(the former for the timing problems, the latter for the problems with their assignment that yet remain to be identified) and thus have been dropped from this year's data. The scripts for generating such Quickstart collection with variable window and overlap times were also included in the data release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description</head><p>Our current system largely builds upon the one that was successful in the last year's campaign <ref type="bibr" coords="2,499.72,214.61,9.96,8.74" target="#b2">[3]</ref>, with only minor modifications and larger set of tested settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linguistic preprocessing</head><p>Stemming (or lemmatization) is considered to be vital for good IR performance even in the case of weakly inflected languages such as English; thus it is probably even more crucial for Czech as the representative of the richly inflectional language family. This assumption was experimentally proven by our group in the last year's CLEF CL-SR track <ref type="bibr" coords="2,339.01,308.71,9.96,8.74" target="#b2">[3]</ref>. Thus we have used the same method of linguistic preprocessing, that is, the serial combination of Czech morphological analyser and tagger <ref type="bibr" coords="2,121.06,332.62,9.96,8.74" target="#b1">[2]</ref>, which provides both the lemma and stem for each input word form, together with a detailed morphological tag. This tag (namely it's first position) is used for stop-word removalwe removed from indexing all the words that were tagged as prepositions, conjunctions, particles and interjections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval</head><p>All our retrieval experiments were performed using the Lemur toolkit <ref type="bibr" coords="2,393.54,414.76,9.96,8.74" target="#b0">[1]</ref>, which offers a variety of retrieval models. We have decided to stick to the tf.idf model where both documents and queries are represented as weighted term vectors</p><formula xml:id="formula_0" coords="2,268.09,438.67,244.92,9.65">d i = (w i,1 , w i,2 , • • • , w i,n ) and q k = (w k,1 , w k,2 , • • • , w k,n ),</formula><p>respectively (n denotes the total number of distinct terms in the collection). The inner-product of such weighted term vectors then determines the similarity between individual documents and queries. There are many different formulas for computation of the weights w i,j , we have tested two of them, varying in the tf component:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw term frequency</head><formula xml:id="formula_1" coords="2,259.31,528.67,253.69,23.22">w i,j = tf i,j • log d df j<label>(1)</label></formula><p>where tf i,j denotes the number of occurrences of the term t j in the document d i (term frequency), d is the total number of documents in the collection and finally df j denotes the number of documents that contain t j .</p><p>BM25 term frequency</p><formula xml:id="formula_2" coords="2,219.03,625.63,293.97,26.84">w i,j = k 1 • tf i,j tf i,j + k 1 (1 -b + b l d l C ) • log d df j<label>(2)</label></formula><p>where tf i,j , d and df j have the same meaning as in (1), l d denotes the length of the document, l C the average length of a document in the collection and finally k 1 and b are the parameters to be set.</p><p>The tf components for queries are defined analogously, except for the average length of a query, which obviously cannot be determined as the system is not aware of the full query set and processes one query at a time. The Lemur documentation is however not clear about the exact way of handling the l C value for queries.</p><p>The values of k 1 and b were set according to the suggestions made by <ref type="bibr" coords="3,399.95,112.02,10.52,8.74" target="#b6">[7]</ref> and <ref type="bibr" coords="3,431.48,112.02,9.96,8.74" target="#b5">[6]</ref>, that is k 1 = 1.2 and b = 0.75 for computing document weights and k 1 = 1 and b = 0<ref type="foot" coords="3,386.41,122.40,3.97,6.12" target="#foot_1">2</ref> for query weights.</p><p>We have also tested the influence of the blind relevance feedback. The simplified version of the Rocchio's relevance feedback implemented in Lemur <ref type="bibr" coords="3,321.72,147.89,10.52,8.74" target="#b6">[7]</ref> was used for this purposes. The original Rocchio's algorithm is defined by the formula</p><formula xml:id="formula_3" coords="3,239.88,183.70,122.70,9.65">q new = q old + α • d R -β • d R</formula><p>where R and R denote the set of relevant and non-relevant documents, respectively, and d R and d R denote the corresponding centroid vectors of those sets. In other words, the basic idea behind this algorithm is to move the query vector closer to the relevant documents and away from the non-relevant ones. In the case of blind feedback, the top M documents from the first-pass run are simply considered to be relevant. The Lemur modification of this algorithm sets the β = 0 and keeps only the K top-weighted terms in d R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We have created 3 different indices from the collection -using original data and their lemmatized and stemmed version. There were 29 training topics and 42 evaluation topics defined by the organisers. We have first run the set of experiments for the training topics (see Table <ref type="table" coords="3,450.70,345.19,3.87,8.74" target="#tab_0">1</ref>), comparing:</p><p>• Results obtained for the queries constructed by concatenating the tokens (either words, lemmas or stems) from the &lt;title&gt; and &lt;desc&gt; fields of the topics (TD -upper section of the table) with results for queries made from all three topic fields, i.e. &lt;title&gt;, &lt;desc&gt; and &lt;narr&gt; (TDN -lower section).</p><p>• Results achieved on the "original" Quickstart collection (i.e. 3-minute window with 1-minute overlap -Segments 3-1) with results computed using the collection created by using 2-minute window with 1-minute overlap (Segments 2.1).</p><p>In all cases the performance of raw term frequency (Raw TF) and BM25 term frequency (BM25 TF) is tested, both with (BRF) and without (no FB) application of the blind relevance feedback. The mean Generalized Average Precision (mGAP) is used as the evaluation metric -the details about this measure can be found in <ref type="bibr" coords="3,248.16,500.61,9.96,8.74" target="#b3">[4]</ref>. Then we identified the 5 most promising/illustrative runs from the Table <ref type="table" coords="3,437.00,678.23,3.87,8.74" target="#tab_0">1</ref>, repeated them for the evaluation topics and send to the organisers for judgment. After receiving the relevance judgments for evaluation topics, we have replicated all the runs for those topics too (Table <ref type="table" coords="3,488.62,702.14,3.87,8.74" target="#tab_1">2</ref>).</p><p>It turns out that the structure of the results for different experimental settings is similar for both the training and evaluation topics -thus we could observe the following trends: • Two minute "documents" seem to perform better than the three minute ones -probably the three minute segmentation is too coarse.</p><p>• The simplest raw term frequency weighting scheme generally outperforms the more sophisticated BM25 -one possible explanation is that in a standard document retrieval setup the BM25 scheme profits mostly from its length normalization component that is completely unnecessary in our case (remember that our documents all have approximately identical length by design).</p><p>The fact that both stemming and lemmatization boost the performance by about the same margin was already observed in the last year's experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In the CLEF 2007 CL-SR task, we have made just a little step further towards successful searching of Czech spontaneous speech. In order to make a bigger progress, we would need to really take the speech part of the task into account -that is, to use the speech recognizer lattices when searching for the desired information, or even to modify the ASR components so that it will be more likely to produce output useful for IR (for example, enrich the language model with rare named entities that are currently often being misrecognized).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,108.52,523.82,385.97,139.79"><head>Table 1 :</head><label>1</label><figDesc>Mean GAP of the individual runs -training topics.</figDesc><table coords="3,108.52,523.82,385.97,107.97"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Segments 3-1</cell><cell></cell><cell></cell><cell cols="2">Segments 2-1</cell></row><row><cell></cell><cell></cell><cell cols="2">Raw TF</cell><cell cols="2">BM25 TF</cell><cell cols="2">Raw TF</cell><cell>BM25 TF</cell></row><row><cell></cell><cell></cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell></row><row><cell>TD</cell><cell>words</cell><cell cols="7">0.0184 0.0183 0.0152 0.0183 0.0212 0.0246 0.0147 0.0174</cell></row><row><cell></cell><cell cols="8">lemmas 0.0277 0.0303 0.0279 0.0324 0.0293 0.0383 0.0276 0.0346</cell></row><row><cell></cell><cell>stems</cell><cell cols="7">0.0281 0.0315 0.0258 0.0322 0.0323 0.0389 0.0281 0.0335</cell></row><row><cell cols="2">TDN words</cell><cell cols="7">0.0194 0.0209 0.0132 0.0169 0.0211 0.0234 0.0161 0.0202</cell></row><row><cell></cell><cell cols="8">lemmas 0.0330 0.0374 0.0231 0.0325 0.0389 0.0453 0.0286 0.0376</cell></row><row><cell></cell><cell>stems</cell><cell cols="7">0.0332 0.0356 0.0235 0.0341 0.0390 0.0443 0.0288 0.0374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,110.82,423.00,151.75"><head>Table 2 :</head><label>2</label><figDesc>Mean GAP of the individual runs -evaluation topics. Bold runs were submitted for official scoring.</figDesc><table coords="4,102.29,110.82,398.42,107.97"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Segments 3-1</cell><cell></cell><cell></cell><cell cols="2">Segments 2-1</cell></row><row><cell></cell><cell></cell><cell cols="2">Raw TF</cell><cell cols="2">BM25 TF</cell><cell cols="2">Raw TF</cell><cell>BM25 TF</cell></row><row><cell></cell><cell></cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell><cell>no FB</cell><cell>BRF</cell></row><row><cell>TD</cell><cell>words</cell><cell cols="4">0.0105 0.0121 0.0088 0.0121</cell><cell cols="3">0.0123 0.0126 0.0097 0.0108</cell></row><row><cell></cell><cell cols="8">lemmas 0.0168 0.0189 0.0126 0.0126 0.0183 0.0206 0.0144 0.0133</cell></row><row><cell></cell><cell>stems</cell><cell cols="4">0.0188 0.0205 0.0132 0.0161</cell><cell cols="3">0.0196 0.0217 0.0157 0.0187</cell></row><row><cell cols="2">TDN words</cell><cell cols="4">0.0113 0.0142 0.0089 0.0108</cell><cell cols="3">0.0141 0.0162 0.0099 0.0125</cell></row><row><cell></cell><cell cols="5">lemmas 0.0205 0.0226 0.0114 0.0150</cell><cell cols="3">0.0206 0.0254 0.0164 0.0150</cell></row><row><cell></cell><cell>stems</cell><cell cols="4">0.0215 0.0215 0.0092 0.0107</cell><cell cols="3">0.0218 0.0246 0.0120 0.0125</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,723.73,407.75,6.99;1,90.00,733.20,423.00,6.99;1,90.00,742.66,259.71,6.99"><p>It turned out later that the actual timing was different due to some faulty assumptions during the Quickstart collection design, but since the principle of the document creation remains the same, we will still use the "intended" time figures instead of the actual ones, just for the sake of readability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,744.98,309.14,6.99"><p>This is actually not a choice, as the value of b is hard-set to 0 for queries in Lemur.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Grant Agency of the Czech Academy of Sciences</rs> project No. <rs type="grantNumber">1ET101470416</rs> and the <rs type="funder">Ministry of Education of the Czech Republic</rs> project No. <rs type="grantNumber">LC536</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KaZwSJt">
					<idno type="grant-number">1ET101470416</idno>
				</org>
				<org type="funding" xml:id="_ZN6cb6h">
					<idno type="grant-number">LC536</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.50,634.34,407.50,8.74;4,105.50,646.29,375.49,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,391.72,634.34,121.28,8.74;4,105.50,646.29,183.90,8.74">The Lemur Toolkit for Language Modeling and Information Retrieval</title>
		<ptr target="http://www.lemurproject.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University and the University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,666.08,407.50,8.74;4,105.50,678.03,111.72,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,168.50,666.08,155.69,8.74">Disambiguation of Rich Inflection</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,339.75,666.08,168.71,8.74">Computational Morphology of Czech)</title>
		<meeting><address><addrLine>Karolinum, Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,697.82,407.51,8.74;4,105.50,709.78,407.50,8.74;4,105.50,721.73,407.50,8.74;4,105.50,733.69,407.50,8.74;4,105.50,745.64,312.34,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,256.01,697.82,257.00,8.74;4,105.50,709.78,193.86,8.74">Benefit of Proper Language Processing for Czech Speech Retrieval in the CL-SR Task at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Ircing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luděk</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,382.41,721.73,130.59,8.74;4,105.50,733.69,407.50,8.74;4,105.50,745.64,48.71,8.74">Evaluation of Multilingual and Multi-modal Information Retrieval -7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<title level="s" coord="4,162.15,745.64,152.88,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,112.02,407.50,8.74;5,105.50,123.98,407.50,8.74;5,105.50,135.93,183.81,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,256.98,112.02,256.02,8.74;5,105.50,123.98,230.20,8.74">One-Sided Measures for Evaluating Ranked Retrieval Effectiveness with Spontaneous Conversational Speech</title>
		<author>
			<persName coords=""><forename type="first">Baolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,361.56,123.98,118.27,8.74">Proceedings of SIGIR 2006</title>
		<meeting>SIGIR 2006<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="673" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,155.86,407.50,8.74;5,105.50,167.81,407.50,8.74;5,105.50,179.77,407.50,8.74;5,105.50,191.72,407.50,8.74;5,105.50,203.68,407.50,8.74;5,105.50,215.63,191.53,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,261.98,167.81,251.01,8.74;5,105.50,179.77,53.31,8.74">Overview of the CLEF-2006 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,243.04,191.72,269.96,8.74;5,105.50,203.68,338.17,8.74">Evaluation of Multilingual and Multi-modal Information Retrieval -7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<title level="s" coord="5,451.93,203.68,61.07,8.74;5,105.50,215.63,88.73,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,235.56,407.51,8.74;5,105.50,247.51,156.35,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,279.21,235.56,120.80,8.74">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,424.72,235.56,88.28,8.74;5,105.50,247.51,125.03,8.74">The Eight Text REtrieval Conference (TREC-8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,267.44,407.50,8.74;5,105.50,279.39,114.29,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,188.07,267.44,151.26,8.74">Notes on the Lemur TFIDF model</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>School of CS, CMU</orgName>
		</respStmt>
	</monogr>
	<note>Note with Lemur 1.9 documentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
