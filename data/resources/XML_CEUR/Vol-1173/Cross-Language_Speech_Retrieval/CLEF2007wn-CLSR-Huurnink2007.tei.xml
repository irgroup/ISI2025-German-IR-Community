<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.33,170.78,327.67,15.15">Language Speech Retrieval Track 2007</title>
				<funder ref="#_Ks82Sp6">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,265.58,204.67,71.84,8.74"><forename type="first">Bouke</forename><surname>Huurnink</surname></persName>
							<email>bhuurnin@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">CLEF Cross</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.33,170.78,327.67,15.15">Language Speech Retrieval Track 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">253933F7A0BDECCB6AF88810E32A4DCC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech Retrieval</term>
					<term>Cross-Language Information Retrieval</term>
					<term>Text Transformations</term>
					<term>Field Combination</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the contents of the University of Amsterdam submission in the CLEF Cross Language Speech Retrieval 2007 English task. We describe the effects of using character n-grams and field combinations on both monolingual English retrieval, and crosslingual Dutch to English retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Even in a well-funded archive, it is often infeasible to manually annotate all documents in the collection. The digitisation of multimedia collections opens the door to automatic techniques for discovering interesting documents, provided that we can leverage automatically generated annotations to their best advantage. The University of Amsterdam participated in the CLEF CL-SR 2007 English task in the hope of applying lessons learned there to the retrieval of documents from large Dutch audio-visual archives, in particular the Netherlands Institute for Sound and Vision<ref type="foot" coords="1,117.45,500.53,3.97,6.12" target="#foot_0">1</ref> which stores the nation's public television broadcasts. These archives contain a lot of spoken material, some of which has been manually annotated by a team of archivists. A significant portion, however, has not been annotated at all. Therefore we investigated strategies both for search using only automatically generated text, as well as combining this text with manually generated annotations.</p><p>Our focus was on simple techniques that can easily be transferred to other domains. In our experiments we explored the use of character n-grams to improve the retrieval of documents using automatically generated text. We also explored the combination of manually generated with automatically generated text. In both cases we contrasted monolingual retrieval of English documents using English queries with cross-lingual retrieval of English results using Dutch queries.</p><p>The remainder of this paper is structured as follows. We first describe the setup of the retrieval system and experiments in Section 2. This is followed by the runs and results in Section 3. Finally we present our conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head><p>We work with the CLEF CL-SR experimental English spoken document collection, which consists of a series of English language interviews that have been manually split into short segments. Each segment has been associated with manually and automatically assigned metadata, including manual summaries, manually assigned keywords, automatic speech transcriptions, and a number of fields containing automatically assigned keywords. For experiments using only automatically assigned information we use the ASRTEXT2006B (speech transcription) and ASRKEY-WORD2004A2 (automatic keyword) fields. Other automatic transcripts and keywords were available, but we chose to use only one of each, which may have had a negative impact on our results. For experiments including manual annotations we also added the MANUALKEYWORD (manual keyword) and SUMMARY (manual summary) fields.</p><p>The CLEF CL-SR benchmark provided 63 training topics with a ground truth, as well as 33 test topics. The original topic descriptions are in English, and have the traditional TREC titledescription -narrative structure. Also available were manually created Dutch topic translations, donated by the University of Twente. We used these Dutch topics for the cross-lingual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Infrastructure</head><p>All documents were indexed and retrieved using the Indri engine from the Lemur retrieval toolkit<ref type="foot" coords="2,505.76,275.85,3.97,6.12" target="#foot_1">2</ref> . This engine allows for fielded search in a language modeling framework. As is standard in English text retrieval, commonly occurring stop words were removed. Terms were stemmed to their morphological roots using the Porter <ref type="bibr" coords="2,249.57,313.29,10.52,8.74" target="#b2">[3]</ref> stemming algorithm. Retrieval parameters were optimised for automatic monolingual retrieval on the ASRTEXT2006B field, using the training topics to find the best combination.</p><p>As for the topics, the title and description fields of each topic were combined to make a text query. The Dutch topics were automatically translated to English using online resources, in order to be able to retrieve the English documents. As different translation systems perform better for different topics <ref type="bibr" coords="2,175.58,385.02,9.96,8.74" target="#b3">[4]</ref>, we used two different online tools to translate the topics from Dutch to English. We used the SYSTRAN<ref type="foot" coords="2,232.65,395.40,3.97,6.12" target="#foot_2">3</ref> and FreeTranslation.com<ref type="foot" coords="2,347.19,395.40,3.97,6.12" target="#foot_3">4</ref> systems, and combined the results to form a large 'bag-of-words' cross-lingual query. Some of the differences between translations can be seen in the example given in Table <ref type="table" coords="2,255.64,420.89,3.88,8.74" target="#tab_0">1</ref>. For instance, the word 'acts' is translated into 'deeds' by FreeTranslation.com and 'prowesses' by SYSTRAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Character n-Gram Experiments</head><p>Character n-gram tokenisation has been shown to boost retrieval in certain situations <ref type="bibr" coords="2,465.35,626.37,9.96,8.74" target="#b1">[2]</ref>, such as retrieval from English newspapers <ref type="bibr" coords="2,243.65,638.32,9.96,8.74" target="#b0">[1]</ref>. We were interested to see whether this would also prove useful for the specific situation of (cross-lingual) retrieval of automatically generated text. To test this, we followed the tokenisation strategy in <ref type="bibr" coords="2,293.15,662.23,9.97,8.74" target="#b1">[2]</ref>, and created overlapping, cross-word character n-grams of the text before it was indexed. An example is shown in Table <ref type="table" coords="2,426.36,674.19,3.88,8.74" target="#tab_1">2</ref>. In designing the experiment, we used only the (weighted) ASRTEXT2006B and AUTOKEYWORD2004A2 fields.</p><p>We evaluated MAP for retrieval at different n-gram sizes on the training topics prior to submission, and found that 4-grams provided the best performance. Likewise, we evaluated different weightings for the ASRTEXT2006B and AUTOKEYWORD2004A2 fields. Here we found the best setting to be ASRTEXT2006B = 0.75 and AUTOKEYWORD2004A2= 0.25. These, then, are the settings that we used in our officially submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Field Combination Experiments</head><p>We evaluated field combination, as we may later wish to apply this technique to retrieving the annotated portion of multimedia documents in a audio-visual archive. Combination was done using the Indri query language, giving different fields different weights. The fields that we used were MANUALKEYWORD, SUMMARY, ASRTEXT2006B, and AUTOKEYWORD2004A2.</p><p>As with the n-gram experiments, we determined the optimal combination setting on the set of 63 training topics that were provided, using MAP as our evaluation measure. We found that the best weighting for monolingual retrieval was MANUALKEYWORD = 0.375, SUMMARY = 0.375, ASRTEXT2006B = 0.125, and AUTOKEYWORD2004A2 = 0.125. For the cross-lingual task, the automatic keywords gave no contribution to retrieval performance and the best weighting was MANUALKEYWORD = 0.375, SUMMARY = 0.375 and ASRTEXT2006B = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs and Results</head><p>Table <ref type="table" coords="3,118.50,463.92,4.98,8.74" target="#tab_2">3</ref> shows the results of the official runs submitted to CLEF CL-SR. Also shown are two runs that were generated post-hoc to allow fair comparison of the n-gram techniques to a baseline. The post-hoc runs were both generated using stopped and stemmed text from both the ASRTEXT2006B and AUTOKEYWORD2004A2 fields, weighted as described in Section 2.2.</p><p>Examining the n-gram runs, we found that monolingual retrieval of the automatic fields using character 4-grams decreased MAP by 9.6% . Cross-lingual retrieval, on the other hand, benefited from the use of 4-grams with an increase in MAP of 4%.</p><p>The combination runs, which included both manual and automatic information, performed much better than runs containing only automatically derived text. This is not surprising, it has been demonstrated in previous CLEF CL-SR tracks that manual annotation allows much better retrieval than automatic information alone. The weightings derived in the training phase indicate that automatically generated keywords are helpful for monolingual retrieval, but do not help for this specific case of cross-lingual retrieval. Automatically recognised speech, however, was useful for both monolingual and cross-lingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper has described the setup and performance of the University of Amsterdam's entry in the CLEF CL-SR 2007 English retrieval task. We investigated the effect of using n-grams to retrieve automatically generated text, finding that they decreased monolingual performance but improved cross-lingual performance. Furthermore, we examined the effects of combining manual and automatically generated text, and saw that both can be useful. We hope that the lessons learned here will aid us in practice, and help us enhance search through Dutch audio-visual archives. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,119.47,463.18,364.07,103.08"><head>Table 1 :</head><label>1</label><figDesc>Sample Topic Translations (Topic 15602)</figDesc><table coords="2,119.47,485.37,364.07,80.90"><row><cell cols="2">Original English Topic FreeTranslation.com</cell><cell>SYSTRAN</cell></row><row><cell>Heroic survival stories.</cell><cell>Heroic survivals story.</cell><cell>Herosche overlevingsver-</cell></row><row><cell>Stories of heroic acts or ac-</cell><cell>Tell of heroic deeds or</cell><cell>halen. Tales of prowesses</cell></row><row><cell>tivities that led to the sur-</cell><cell>heroic actions that led</cell><cell>or herosche action which</cell></row><row><cell>vival of one or more indi-</cell><cell>till the (save) [survive] of</cell><cell>led to (save) [survive] of</cell></row><row><cell>viduals are desired.</cell><cell>an or several individuals</cell><cell>one or more individuals</cell></row><row><cell></cell><cell>have been wished.</cell><cell>have been needed.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,182.14,118.94,238.73,78.62"><head>Table 2 :</head><label>2</label><figDesc>n-Gram Tokenisation Example</figDesc><table coords="3,182.14,140.57,238.73,56.99"><row><cell>Original Text</cell><cell>4-Grams</cell></row><row><cell>heroic survivals story</cell><cell>hero eroi roic oic* ic*s</cell></row><row><cell></cell><cell>c*su *sur surv urvi rviv</cell></row><row><cell></cell><cell>viva ival vals als* ls*s s*st</cell></row><row><cell></cell><cell>*sto stor tory</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,121.19,118.94,360.63,232.10"><head>Table 3 :</head><label>3</label><figDesc>Results of CLEF CL-SR runs</figDesc><table coords="4,121.19,138.63,360.63,212.40"><row><cell>Run ID</cell><cell>Type</cell><cell>Fields</cell><cell>MAP</cell></row><row><cell>UvA 1 base</cell><cell>monolingual baseline</cell><cell>ASRTEXT2006B</cell><cell>0.0430</cell></row><row><cell>UvA 2 en4g</cell><cell>monolingual 4-grams</cell><cell>ASRTEXT2006B,</cell><cell>0.0444</cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row><row><cell>UvA 3 nl4g</cell><cell>cross-lingual 4-grams</cell><cell>ASRTEXT2006B,</cell><cell>0.0400</cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row><row><cell>UvA 4 enopt</cell><cell cols="2">monolingual combination MANUALKEYWORD,</cell><cell>0.2088</cell></row><row><cell></cell><cell></cell><cell>SUMMARY,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ASRTEXT2006B,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row><row><cell>UvA 5 nlopt</cell><cell cols="2">cross-lingual combination MANUALKEYWORD,</cell><cell>0.1408</cell></row><row><cell></cell><cell></cell><cell>SUMMARY,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ASRTEXT2006B,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row><row><cell cols="2">unsubmitted run monolingual</cell><cell>ASRTEXT2006B,</cell><cell>0.0491</cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row><row><cell cols="2">unsubmitted run cross-lingual</cell><cell>ASRTEXT2006B,</cell><cell>0.0385</cell></row><row><cell></cell><cell></cell><cell>AUTOKEYWORD2004A2</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,743.17,111.03,6.99"><p>http://www.beeldengeluid.nl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,728.00,112.70,6.99"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,737.50,100.07,6.99"><p>http://www.systran.co.uk/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,747.00,122.38,6.99"><p>http://www.freetranslation.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> <rs type="projectName">MUNCH</rs> project under project number <rs type="grantNumber">640.002.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Ks82Sp6">
					<idno type="grant-number">640.002.501</idno>
					<orgName type="project" subtype="full">MUNCH</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.50,474.68,407.51,8.74;4,105.50,486.63,178.89,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,316.93,474.68,196.07,8.74;4,105.50,486.63,40.40,8.74">Monolingual document retrieval for European languages</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,154.80,486.63,39.68,8.74">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,506.56,407.51,8.74;4,105.50,518.51,172.52,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,248.31,506.56,264.70,8.74;4,105.50,518.51,34.67,8.74">Character n-gram tokenization for European language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,148.44,518.51,39.68,8.74">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,538.44,407.50,8.74;4,105.50,550.40,37.64,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,170.60,538.44,142.78,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,336.41,538.44,144.94,8.74">Readings in information retrieval</title>
		<imprint>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,570.32,407.50,8.74;4,105.50,582.28,407.51,8.74;4,105.50,594.23,161.71,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,171.81,570.32,177.75,8.74">Report on CLEF-2003 multilingual tracks</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="4,394.54,582.28,118.47,8.74;4,105.50,594.23,30.49,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
