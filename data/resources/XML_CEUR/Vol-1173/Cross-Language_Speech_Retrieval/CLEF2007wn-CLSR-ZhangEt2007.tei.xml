<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,420.46,146.47,44.16,18.08;1,121.17,168.83,360.71,18.08;1,248.14,191.19,106.75,18.08">2007: Cross-Language Speech Retrieval (CL-SR) Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.99,226.52,49.00,10.46"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<email>yzhang@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,255.29,226.52,79.98,10.46"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
						</author>
						<author>
							<persName coords="1,362.43,226.52,42.62,10.46"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>kzhang@computing.dcu.ie</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University at CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Digital Video Processing</orgName>
								<orgName type="department" key="dep2">School of Computing Dublin</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,420.46,146.47,44.16,18.08;1,121.17,168.83,360.71,18.08;1,248.14,191.19,106.75,18.08">2007: Cross-Language Speech Retrieval (CL-SR) Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">967405F0148F20F487FA481B7E89AE26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Speech Retrieval</term>
					<term>Domain specific translation</term>
					<term>Evaluation</term>
					<term>Generalized Average Precision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Dublin City University participated in the CLEF 2007 CL-SR English task. For CLEF 2007 we concentrated primarily on the issues of topic translation, combining this with search field combination and pseudo relevance feedback methods used for our CLEF 2006 submissions. Topics were translated into English using the Yahoo! BabelFish free online translation service combined with domain-specific translation lexicons gathered automatically from Wikipedia. We explored alternative translations methods with document retrieval based the combination of the multiple document fields using the BM25F field combination model. Our results indicate that extending machine translation tools using automatically generated domain-specific translation dictionaries can provide improved CLIR effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Dublin City University participation in the CLEF 2007 CL-SR task focussed on extending our CLEF 2006 system to investigate combinations of general and domain-specific topic translation resources. Our 2006 participation in the CL-SR task concentrated on the combination of the multiple fields associated with the speech documents. Our study was based on using the document field combination extended version of BM25 termed BM25F introduced in <ref type="bibr" coords="1,395.73,670.81,14.62,10.46" target="#b8">[11]</ref>. In addition, we incorporate our existing information retrieval methods based on the Okapi model with summary-based pseudo-relevance feedback (PRF) <ref type="bibr" coords="1,237.95,695.20,9.97,10.46" target="#b6">[9]</ref>. Our official submissions included both English monolingual and French-English bilingual tasks using automatic only and combined automatic and manual fields. Topics were translated into English using a baseline of the online Yahoo! BabelFish machine translation system <ref type="bibr" coords="1,202.16,731.79,9.97,10.46">[1]</ref>. For our CLEF 2007 experiments these translations are combined with domain-specific translation lexicons gathered automatically from Wikipedia.</p><p>The remainder of this paper is structured as follows: Section 2 summarises the motivation and implementation of the BM25F retrieval model, Section 3 overviews our basic retrieval system and describes our sentence boundary creation technique, Section 4 describes our topic translation methods, Section 5 presents the results of our experimental investigations, and Section 6 concludes the paper with a discussion of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Field Combination</head><p>The English collection comprises 8104 "documents" that are manually-determined topically-coherent segments taken from 272 interviews with Holocaust survivors, witnesses and rescuers, totaling 589 hours of speech. The spoken documents are provided with a rich set of data fields, full details of these are given in <ref type="bibr" coords="2,167.69,251.08,14.40,10.46" target="#b10">[13]</ref> <ref type="bibr" coords="2,182.09,251.08,10.80,10.46" target="#b4">[7]</ref>. In this work, we explored field combination based on the following fields:</p><p>• a transcription of the spoken content of the document generated using an automatic speech recognition (ASR) system, (several transcriptions are available, for experiments we use the ASR2006B field,</p><p>• two assigned sets of keywords generated automatically (AKW1,AKW2),</p><p>• one assigned set of manually generated keywords (MK),</p><p>• a short three sentence manually written summary of each segment (SUM),</p><p>• a list of the names of all individuals appearing in the segment.</p><p>Two standard methods of combining multiple document fields in retrieval are:</p><p>• to simply merge all the fields into a single document representation and apply standard single document field information retrieval methods,</p><p>• to index the fields separately, perform individual retrieval runs for each field and then merge the resulting ranked lists by summing in a process of data fusion.</p><p>The topic of field combination for this type of task with ranked information retrieval schemes is explored in <ref type="bibr" coords="2,151.84,501.50,14.62,10.46" target="#b8">[11]</ref>. That paper demonstrated the weaknesses of the simple standard combination methods and proposed an extended version of the standard BM25 term weighting scheme referred to as BM25F, which combines multiple fields in a more well-founded way.</p><p>The BM25F combination approach uses a simple weighted summation of the multiple fields of the documents to form a single field for each document in the usual way. The importance of each document field for retrieval can be determined empirically in separate runs, the count of each term appearing in each field is multiplied by a scalar constant representing the importance of this field, and the components of all fields are then summed to form the overall single field document representation for indexing. Once the fields have been combined in a weighted sum, standard single field IR methods can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Okapi Retrieval System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Term Weighting</head><p>The basis of our experimental system is the City University research distribution version of the Okapi system <ref type="bibr" coords="2,154.09,699.47,14.62,10.46" target="#b9">[12]</ref>. The documents and search topics are processed to remove stopwords from a standard list of about 260 words, suffix stripped using the Okapi implementation of Porter stemming <ref type="bibr" coords="2,136.63,723.86,15.50,10.46" target="#b7">[10]</ref> and terms are indexed using a small standard set of synonyms. None of these procedures were adapted for the CLEF 2007 CL-SR test collection. Document terms were weighted using the Okapi BM25 weighting scheme shown as follows,</p><formula xml:id="formula_0" coords="3,120.25,130.72,361.09,58.77">cw(i, j) = cf w(i) × tf (i, j) × (k 1 + 1) k 1 × ((1 -b) + (b × ndl(j))) + tf (i, j) cf w(i) = log (rload + 0.5)(N -n(i) -bigrload + rload + 0.5) (n(i) -rload + 0.5)(bigrload -rload + 0.5 , ndl(j) = dl(j) agvdl</formula><p>where cw(i, j) represents the weight of term i in document j; n(i)</p><p>is the total number of documents containing term i; N is the total number of documents in the collection; tf (i, j)</p><p>is the within document term frequency; ndl(j)</p><p>is the normalized document length; dl <ref type="bibr" coords="3,143.14,266.83,12.47,10.46">(j)</ref> is the length of j; avgdl is the average document length in the collection; k 1 and b are empirically selected tuning constants for a particular collection.</p><p>The matching score for each document is computed by summing the weights of terms appearing in the query and the document. The BM25 k 1 and b values used for our submitted runs were tuned using the 63 CLEF 2007 CL-SR English training topics. rload and bigrload take the default parameters of 4 and 5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-Relevance Feedback</head><p>Query expansion by pseudo relevance feedback (PRF) is a well-established procedure in both monolingual and cross-lingual IR, potentially providing some improvement in retrieval effectiveness. The method used here is based on our work originally described in <ref type="bibr" coords="3,421.81,414.43,9.97,10.46" target="#b5">[8]</ref>, and modified for the CLEF 2005 CL-SR task <ref type="bibr" coords="3,215.43,426.63,9.97,10.46" target="#b6">[9]</ref>. A summary is made of the automatic speech recognition (ASR) transcription of each of the top ranked documents, which are assumed to be relevant to a given query. The document summary is then expanded to include all terms in the other metadata fields used in this document index. All non-stopwords in these augmented summaries are ranked using a slightly modified version of the Robertson Selection Value (RSV) <ref type="bibr" coords="3,385.55,475.40,14.62,10.46" target="#b9">[12]</ref>.</p><p>In our modified version of RSV, the top t potential expansion terms are selected from the augmented summaries of the top d 1 ranked documents, but ranked using statistics from a larger number d 2 of assumed relevant ranked documents from the initial run.</p><p>The summary-based PRF method operates by selecting topical-related expansion terms from document summaries. However, since the ASR transcriptions of the conversational speech documents do not contain punctuation, we developed a method of selecting significant document segments to identify documents "summaries". Our approach is derived from Luhn's word cluster hypothesis. Luhns hypothesis states that significant words separated by up to five non-significant words maximum are likely to be strongly related. Clusters of these strongly related word were identified in the running document transcription by searching for word groups separated by not more than five insignificant words. Words appearing between clusters are not included in clusters, and thus can be ignored for the purposes of query expansion since they are by definition stop words. The clusters were then awarded a significance score based on the following two measures:</p><p>Luhn's Keyword cluster method Luhns method assigns a sentence score LS for the highest scoring cluster within a sentence <ref type="bibr" coords="3,234.24,672.26,9.97,10.46" target="#b5">[8]</ref>. We adapted this method to assign a cluster score as follows:</p><formula xml:id="formula_1" coords="3,275.96,692.74,49.39,25.11">LS = SW 2 T W</formula><p>where SW is the number of bracketed significant words, and T W is the total number of bracketed words.</p><p>Query-biasd method This method assigns a score QS to each sentence based on the number of query terms in the sentence as follows:</p><formula xml:id="formula_2" coords="4,276.61,138.12,48.09,25.11">QS = T Q 2 N Q</formula><p>where T Q is the number of query terms occurring in the sentence, and N Q is the total number of terms in a query.</p><p>For each sentence (cluster), the overall sentence score SS is calculated using SS = LS + QS. The top s sentences (clusters) with the highest SS are then selected as the document summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MT-based Query Translation</head><p>Machine Translation (MT) based query translation uses an existing MT system to provide automatic translation. This approach has been widely used in cross-language information retrieval with good average performance when such an MT system is available for the language pair of the topic and document. In our experiments, topics were translated into English using the Yahoo! BabelFish powered by SYSTRAN <ref type="bibr" coords="4,244.11,308.20,9.97,10.46">[1]</ref>. While BabelFish can provide reasonable translations for general language expressions, it is not sufficient for domain-specific terms such as personal names, organization names, place names, etc. To reduce the errors introduced by such terms during query translation, we augmented the standard BabelFish with domain-specific lexicon resources gathered from Wikipedia [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domain-specific lexicon construction</head><p>As a multilingual hypertext medium, Wikipedia<ref type="foot" coords="4,303.70,401.95,3.97,7.32" target="#foot_0">1</ref> has been proved to be a valuable new source of translation information <ref type="bibr" coords="4,207.00,415.22,10.52,10.46" target="#b0">[3,</ref><ref type="bibr" coords="4,221.22,415.22,7.75,10.46" target="#b1">4,</ref><ref type="bibr" coords="4,232.69,415.22,7.75,10.46" target="#b2">5,</ref><ref type="bibr" coords="4,244.15,415.22,7.01,10.46" target="#b3">6]</ref>. Unlike the web, the hyperlinks in Wikipedia have a more consistent pattern and meaningful interpretation. A Wikipedia page written in one language can contain hyperlinks to its counterparts in other languages, where the hyperlink basenames are translation pairs. For example, the English wikipedia page en.wikipedia.org/wiki/World_War_II contains hyperlinks to German de.wikipedia.org/wiki/Zweiter_Weltkrieg , French fr.wikipedia. org/wiki/Seconde_Guerre_mondial, and Spanish es.wikipedia.org/wiki/Segunda_Guerra_Mundial. The English term "World War II" is the translation of the German term "Zweiter Weltkrieg", the French term "Seconde Guerre mondial", and the Spanish term "Segunda Guerra Mundial".</p><p>Additionally, we observed that multiple English wikipedia URLs en.wikipedia.org/wiki/World_ War_II, en.wikipedia.org/wiki/World_War_2, en.wikipedia.org/wiki/WW2, and en.wikipedia.org/ wiki/Second_world_war are redirected to the same wikipedia page and the URL basenames "World War II", "World War 2", "WW2", and "Second world war" are synonyms. Using all these English terms during query translation is a straightforward approach to the automatic post-translation query expansion.</p><p>To utilize the multilingual linkage and the link redirection features, we implement a threestage automatic process to extract German, French, and Spanish to English translations from Wikipedia:</p><p>1. An English vocabulary for the domain of the test collection was constructed by performing a limited crawl of the English wikipedia<ref type="foot" coords="4,294.14,638.13,3.97,7.32" target="#foot_1">2</ref> , Category:World War II. This category is more likely to contain links to pages and subcategories concerning events, persons, places, and organizations pertaining to war crimes or crimes against humanity especially during the second world war. In total, we collected 7431 English web pages.</p><p>2. For each English page obtained, we extracted the hyperlinks to each of the query languages. This provided a total of 4446, 3338, and 4062 hyperlinks to German, Spanish, and French, respectively. 3. We then selected the basenames of each pair of hyperlinks (German-English, French-English, and Spanish-English) as translations and added into our domain-specific lexicons. The non-English multi-word terms were added into the phrase dictionary for each query language. These phrase dictionaries are later used for phrase identification during query pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query translation process</head><p>As shown in Figure <ref type="figure" coords="5,178.01,385.75,3.88,10.46" target="#fig_0">1</ref>, our query translation process is performed in the following manner:</p><p>1. Query pre-processing: We used the phrase dictionary with the maximum forward matching algorithm to segment each query Q into a list of terms {q 1 , q 2 , q 3 , ..., q n }.</p><p>2. Domain-specific lexicon lookup: For each query term q i (where i ∈ (1, n)), we obtained all its English translations {e i1 , e i2 , e i3 , ..., e im } via a domain-specific lexicon look-up.</p><p>3. BabelFish translation: we then translated the original query Q into the English query E using the Yahoo! BabelFish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Translation results merging:</head><p>For each English term e ij (where i ∈ (1, n) and j ∈ (1, m)) obtained in Step 2, we appended it to the end of the translated English query E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section we report results for our experimental runs for the CLEF 2007 English CL-SR task.</p><p>Results are shown for combinations of manual only fields, automatic only fields and combining both manual and automatic fields. For monolingual retrieval results show precision at cutoff ranks of 5, 10 and 30, standard TREC mean average precision (MAP) and recall in terms of the total number of relevant documents retrieved for the test topic set. For CLIR results compare alternative topic translations resources showing MAP and precision at rank 10. Our submitted runs for the CLEF 2007 are indicated by a * in the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Parameters</head><p>Our retrieval system requires a number of parameters to be set for the term weighting, field combination, and PRF components. All parameter values were set empirically using the 63 CLEF 2007 training topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUN Description</head><p>Query Fields Recall MAP P@5 P@10 P@30 Manual field combination (MK×1+SUM×1, k1 = 1.0, b = 0. Term Weighting and Field Combination Based on these training runs the term weighting and field combination parameters were set as follows:</p><p>• For the manual data field combination, Okapi parameters k 1 = 1.0 and b = 0.5 give the best results when the document fields are weighted as MK×1, and SUM×1;</p><p>• For the automatic data field combination, k 1 = 8.0 and b = 0.5 perform the best when the document fields are weighted as A1K×1, AK2×1, and ASR06B×2; and</p><p>• For the manual and automatic data field combination, k 1 = 3.0 and b = 0.6 produce the best results when the document fields are weighted as MK×4, SUM×4, and ASR06B×1.</p><p>PRF For all our PRF runs, the top d 1 ranked documents were assumed relevant for term selection and document summaries comprised the best scoring s clusters. The RSV values to rank the potential expansion terms were estimated based on the top d 2 ranked assumed relevant documents. The top t ranked expansion terms taken from the clusters were added to the original query in each case. The original topic terms are up-weighted by a factor α relative to the expansion terms. Our PRF query expansion thus involves five parameters as follows:</p><p>t is the number of the expansion terms selected from the summary; s is the number of sentences (clusters) selected as the document summary; d 1 is the number of documents used for sentence (cluster) selection; d 2 is the number of documents used for expansion terms ranking; α is the up-weighting factor.</p><p>This set of parameters were again tuned using the CLEF 2007 CL-SR English training data. We note that PRF involves selection of parameter values that are not necessarily consistent from one collection (indexed using different field combination methods) to another. Our experiments showed that t = 60, s = 6, d 1 = 3, d 2 = 20, and α = 3.0 give the best results for the manual data field combination and manual and automatic data field combination; t = 40, s = 6, d 1 = 3, d 2 = 20, and α = 3.0 produce the best results for the automatic data field combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Field Combination and summary-based PRF</head><p>This section presents results for our field combination experiments for monolingual English retrieval. Table <ref type="table" coords="6,151.32,695.20,4.98,10.46" target="#tab_0">1</ref> shows results for both the baseline condition without application of PRF and with our summary-based PRF.</p><p>For the combination of the MK and SUM fields we can field than application of PRF generally produces a small improvement in performance. Note that the topics here use all three topics fields Title, Description and Narrative (TDN), and thus these results cannot be compared directly (TD runs on automatic field combination, A1K×1+AK2×1+ASR2006B×2, k 1 = 8.0, b = 0.5. Our submitted run is denoted by the *.) to any other results shown here which use only Title and Description fields (TD). Similarly for both the automatic only fields runs combining AK1, AK2 and ASR2006B, and the combination of manual and automatic fields using MK, SUM and ASR2006B, application of PRF produces a small improvement in average and high rank precision, although there appear to be some problems at lower ranks which we intend to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Yahoo! BabelFish combined with domain-specific lexicons</head><p>We then explore the combinations of the query translation and post-translation query expansion, and investigate the improvement contributed by each component in German, French, and Spanish to English CL-SR. The results of these experiments are shown in Table <ref type="table" coords="7,404.11,365.04,3.88,10.46" target="#tab_1">2</ref>.</p><p>As shown in Table <ref type="table" coords="7,185.25,377.23,3.88,10.46" target="#tab_1">2</ref>, in comparison to the standard BabelFish translation (BabelFish baseline), augmented translations from the domain-specific lexicons (BabelFish+LEX) led to a significant improvement (27%) in French-English retrieval task, but only 3% and 4% in Spanish-English and German-English, respectively. This can be explained by the fact that the MAP values for the baseline runs of German and Spanish are much higher than the MAP for the French baseline. We noticed that the description field of German topics sometimes contains additional explanation enclosed by square brackets. The effect of this was often that more correct documents should be retrieved in the German-English task. We therefore believe that the BabelFish system gives a better translation from Spanish, rather French and German, to English.</p><p>At the individual query level (shown in Table <ref type="table" coords="7,320.08,486.98,3.88,10.46">3</ref>), we observed that retrieval effectiveness sometimes slightly degraded when the query was augmented to contain translations from our domain-specific lexicons, despite the fact that they are correct translations of the original query terms. This occurred mainly due to the fact that additional terms result in a decrease of relevant documents at ranks, because they are too general in the collection. For example, "war", "Europe", "Poland", "holocaust", "country", "Jewish", "people", "history", "concentration camp", etc. This problem may be solved if we down-weight the general-term translations during the retrieval process, so that when term frequency is used in calculating similarity, documents with many general terms will not be over-emphasized. We intend to explore this issue in further experiments.</p><p>We used the summary-based PRF to provide post-translation query expansion in all crosslingual retrieval runs (see BabelFish+PRF and BabelFish+LEX+PRF shown in Table <ref type="table" coords="7,467.68,608.92,3.88,10.46" target="#tab_1">2</ref>). It gave improvements of 7% for the mono-lingual run, but only provided improvements of 5%, 1%, and 5% in French-English, Spanish-English, and German-English CL-SR effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper has described results for our participation in the CLEF 2007 CL-SR track. In 2007 our experiments focussed on the combination of standard machine translation with domain-specific translation resources. Our results indicate that combining domain-specific translation derived from Wikipedia with the output of standard machine translation can produce substantial improvements in MAP. Further improvements can also be observed when combined with PRF. How-BabelFish BabelFish+Lex Table <ref type="table" coords="8,505.79,716.10,5.23,7.75">3</ref>: Examples of using extra translations from the domain-specific lexicons leds to a deterioration in retrieval effectiveness. (TD runs on automatic field combination, A1K×1+AK2×1+ASR06B×2, k ever, these trends are not observed consistently in all cases, and further investigations will focus on understanding differences in behaviour more clearly and refining our procedures for training domain-specific translation resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,120.91,269.99,361.21,10.46"><head>French Query:Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of French-English query translation. (Topic numbered 3005)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,92.43,678.93,9.41,61.50;8,103.61,733.52,9.41,4.61;8,103.61,700.83,9.41,18.43;8,103.61,629.88,9.41,25.60;8,103.61,566.49,9.41,25.60;8,103.61,487.87,9.41,48.26;8,103.61,430.25,9.41,54.57;8,103.61,403.14,9.41,24.06;8,103.61,349.25,9.41,50.81;8,103.61,317.21,9.41,28.98;8,103.61,304.92,9.41,9.22;8,114.78,733.52,9.41,4.61;8,114.78,700.83,9.41,18.43;8,114.78,629.88,9.41,25.60;8,114.78,566.49,9.41,25.60;8,114.78,493.94,9.41,42.20;8,114.78,447.35,9.41,43.51;8,114.78,416.25,9.41,28.03;8,125.96,733.52,9.41,4.61;8,125.96,700.83,9.41,18.43;8,125.96,629.88,9.41,25.60;8,125.96,566.49,9.41,25.60;8,125.96,511.69,9.41,24.45;8,125.96,473.47,9.41,35.14;8,125.96,445.96,9.41,24.45;8,125.96,410.30,9.41,32.58;8,125.96,364.70,9.41,42.53;8,125.96,311.13,9.41,50.50;8,125.96,297.83,9.41,10.24;8,137.14,733.52,9.41,4.61;8,137.14,700.83,9.41,18.43;8,137.14,629.88,9.41,25.60;8,137.14,566.49,9.41,25.60;8,137.14,504.51,9.41,31.63;8,137.14,484.51,9.41,16.93;8,148.32,733.52,9.41,4.61;8,148.32,700.83,9.41,18.43;8,148.32,629.88,9.41,25.60;8,148.32,566.49,9.41,25.60;8,148.32,519.21,9.41,16.93;8,159.50,733.52,9.41,4.61;8,159.50,700.83,9.41,18.43;8,159.50,629.88,9.41,25.60;8,159.50,566.49,9.41,25.60;8,159.50,516.65,9.41,19.48;8,159.50,478.50,9.41,35.09;8,159.50,444.70,9.41,30.74;8,170.67,733.52,9.41,4.61;8,170.67,700.83,9.41,18.43;8,170.67,629.88,9.41,25.60;8,170.67,566.49,9.41,25.60;8,170.67,519.21,9.41,16.93;8,181.85,733.52,9.41,4.61;8,181.85,700.83,9.41,18.43;8,181.85,629.88,9.41,25.60;8,181.85,566.49,9.41,25.60;8,181.85,516.65,9.41,19.48;8,181.85,484.52,9.41,29.08;8,193.03,733.52,9.41,4.61;8,193.03,700.83,9.41,18.43;8,193.03,629.88,9.41,25.60;8,193.03,566.49,9.41,25.60;8,193.03,497.31,9.41,38.83;8,193.03,457.97,9.41,36.28;8,193.03,430.32,9.41,24.59;8,204.61,673.63,9.41,66.80;8,215.79,733.52,9.41,4.61;8,215.79,700.83,9.41,18.43;8,215.79,629.88,9.41,25.60;8,215.79,566.49,9.41,25.60;8,215.79,509.48,9.41,26.66;8,215.79,490.91,9.41,15.51;8,215.79,456.30,9.41,31.55;8,215.79,414.53,9.41,38.71;8,215.79,371.67,9.41,39.78;8,226.96,733.52,9.41,4.61;8,226.96,700.83,9.41,18.43;8,226.96,629.88,9.41,25.60;8,226.96,566.49,9.41,25.60;8,226.96,522.03,9.41,14.10;8,238.14,733.52,9.41,4.61;8,238.14,700.83,9.41,18.43;8,238.14,629.88,9.41,25.60;8,238.14,566.49,9.41,25.60;8,238.14,485.64,9.41,50.50;8,238.14,469.78,9.41,12.79;8,238.14,409.59,9.41,57.12;8,238.14,382.48,9.41,24.06;8,238.14,322.29,9.41,57.11;8,238.14,291.54,9.41,27.69;8,238.14,243.63,9.41,44.83;8,249.32,733.52,9.41,4.61;8,249.32,700.83,9.41,18.43;8,249.32,629.88,9.41,25.60;8,249.32,566.49,9.41,25.60;8,249.32,507.06,9.41,29.08;8,260.50,733.52,9.41,4.61;8,260.50,700.83,9.41,18.43;8,260.50,629.88,9.41,25.60;8,260.50,566.49,9.41,25.60;8,260.50,519.21,9.41,16.93;8,271.68,733.52,9.41,4.61;8,271.68,700.83,9.41,18.43;8,271.68,629.88,9.41,25.60;8,271.68,566.49,9.41,25.60;8,271.68,497.40,9.41,38.73;8,271.68,456.80,9.41,37.54;8,271.68,426.95,9.41,26.78;8,271.68,400.58,9.41,23.30;8,271.68,370.72,9.41,26.78;8,271.68,341.79,9.41,25.87;8,282.85,733.52,9.41,4.61;8,282.85,700.83,9.41,18.43;8,282.85,629.88,9.41,25.60;8,282.85,566.49,9.41,25.60;8,282.85,497.40,9.41,38.73;8,282.85,476.30,9.41,18.04;8,282.85,443.89,9.41,29.35;8,282.85,414.02,9.41,26.79;8,282.85,381.90,9.41,29.06;8,282.85,359.71,9.41,19.12;8,294.03,733.52,9.41,4.61;8,294.03,700.83,9.41,18.43;8,294.03,629.88,9.41,25.60;8,294.03,566.49,9.41,25.60;8,294.03,518.10,9.41,18.04;8,294.03,485.68,9.41,29.35;8,294.03,455.82,9.41,26.79;8,294.03,423.69,9.41,29.06;8,294.03,401.51,9.41,19.12;8,305.21,733.52,9.41,4.61;8,305.21,700.83,9.41,18.43;8,305.21,629.88,9.41,25.60;8,305.21,566.49,9.41,25.60;8,305.21,516.65,9.41,19.48;8,305.21,471.05,9.41,42.54;8,305.21,452.12,9.41,15.87;8,305.21,406.50,9.41,42.54;8,305.21,372.70,9.41,30.74;8,316.39,731.21,9.41,9.22;8,316.39,700.83,9.41,18.43;8,316.39,629.88,9.41,25.60;8,316.39,566.48,9.41,25.60;8,316.39,493.59,9.41,42.54;8,316.39,474.65,9.41,15.87;8,316.39,429.04,9.41,42.54;8,316.39,389.02,9.41,36.95;8,327.57,731.21,9.41,9.22;8,327.57,700.83,9.41,18.43;8,327.57,629.88,9.41,25.60;8,327.57,566.48,9.41,25.60;8,327.57,516.65,9.41,19.48;8,327.57,471.05,9.41,42.54;8,327.57,452.12,9.41,15.87;8,327.57,409.05,9.41,39.99;8,338.74,731.21,9.41,9.22;8,338.74,700.83,9.41,18.43;8,338.74,629.88,9.41,25.60;8,338.74,566.48,9.41,25.60;8,338.74,516.65,9.41,19.48;8,338.74,495.56,9.41,18.04;8,338.74,463.14,9.41,29.35;8,338.74,433.27,9.41,26.79;8,338.74,401.15,9.41,29.06;8,338.74,376.42,9.41,21.67;8,338.74,344.27,9.41,29.07;8,349.92,731.21,9.41,9.22;8,349.92,700.83,9.41,18.43;8,349.92,629.88,9.41,25.60;8,349.92,566.48,9.41,25.60;8,349.92,479.02,9.41,57.12;8,349.92,451.90,9.41,24.06;8,349.92,391.71,9.41,57.11;8,349.92,360.96,9.41,27.69;8,349.92,313.05,9.41,44.83;8,361.50,674.69,9.41,65.74;8,372.68,733.52,9.41,4.61;8,372.68,700.83,9.41,18.43;8,372.68,629.88,9.41,25.60;8,372.68,566.49,9.41,25.60;8,372.68,519.48,9.41,16.66;8,372.68,475.79,9.41,40.62;8,383.86,733.52,9.41,4.61;8,383.86,700.83,9.41,18.43;8,383.86,629.88,9.41,25.60;8,383.86,566.49,9.41,25.60;8,383.86,487.87,9.41,48.26;8,383.86,430.25,9.41,54.57;8,383.86,403.14,9.41,24.06;8,383.86,349.25,9.41,50.81;8,383.86,322.39,9.41,23.80;8,383.86,289.30,9.41,30.03;8,383.86,261.37,9.41,24.86;8,383.86,235.99,9.41,22.31;8,383.86,225.49,9.41,7.42;8,383.86,197.82,9.41,24.60;8,383.86,177.84,9.41,16.93;8,383.86,165.31,9.41,9.47;8,383.86,133.27,9.41,28.98;8,383.86,120.98,9.41,9.22;8,395.03,733.52,9.41,4.61;8,395.03,700.83,9.41,18.43;8,395.03,629.88,9.41,25.60;8,395.03,566.49,9.41,25.60;8,395.03,503.50,9.41,32.64;8,395.03,469.85,9.41,30.59;8,395.03,424.25,9.41,42.54;8,395.03,405.31,9.41,15.87;8,395.03,362.25,9.41,39.99;8,406.21,733.52,9.41,4.61;8,406.21,700.83,9.41,18.43;8,406.21,629.88,9.41,25.60;8,406.21,566.49,9.41,25.60;8,406.21,478.07,9.41,58.07;8,406.21,424.51,9.41,50.50;8,406.21,408.65,9.41,12.79;8,406.21,381.79,9.41,23.80;8,406.21,348.69,9.41,30.02;8,406.21,320.77,9.41,24.86;8,406.21,295.39,9.41,22.31;8,406.21,284.89,9.41,7.42;8,406.21,257.22,9.41,24.59;8,406.21,237.24,9.41,16.92;8,406.21,227.27,9.41,6.90;8,417.39,733.52,9.41,4.61;8,417.39,700.83,9.41,18.43;8,417.39,629.88,9.41,25.60;8,417.39,566.49,9.41,25.60;8,417.39,504.51,9.41,31.63;8,417.39,484.51,9.41,16.93;8,428.57,733.52,9.41,4.61;8,428.57,700.83,9.41,18.43;8,428.57,629.88,9.41,25.60;8,428.57,566.49,9.41,25.60;8,428.57,519.21,9.41,16.93;8,439.75,733.52,9.41,4.61;8,439.75,700.83,9.41,18.43;8,439.75,629.88,9.41,25.60;8,439.75,566.49,9.41,25.60;8,439.75,493.59,9.41,42.54;8,439.75,474.66,9.41,15.87;8,439.75,431.59,9.41,39.99;8,450.92,733.52,9.41,4.61;8,450.92,700.83,9.41,18.43;8,450.92,629.88,9.41,25.60;8,450.92,566.49,9.41,25.60;8,450.92,493.59,9.41,42.54;8,450.92,474.66,9.41,15.87;8,450.92,429.04,9.41,42.54;8,450.92,390.88,9.41,35.09;8,450.92,357.08,9.41,30.74;8,462.10,733.52,9.41,4.61;8,462.10,700.83,9.41,18.43;8,462.10,629.88,9.41,25.60;8,462.10,566.49,9.41,25.60;8,462.10,499.54,9.41,36.60;8,462.10,476.99,9.41,19.47;8,462.10,431.39,9.41,42.53;8,462.10,412.44,9.41,15.87;8,462.10,369.39,9.41,39.98;8,473.28,731.21,9.41,9.22;8,473.28,700.83,9.41,18.43;8,473.28,629.88,9.41,25.60;8,473.28,566.48,9.41,25.60;8,473.28,519.21,9.41,16.93;8,484.46,731.21,9.41,9.22;8,484.46,700.83,9.41,18.43;8,484.46,629.88,9.41,25.60;8,484.46,566.48,9.41,25.60;8,484.46,496.21,9.41,39.93;8,484.46,476.23,9.41,16.93"><head></head><label></label><figDesc>concentration camp, Buchenwald, Allied powers, Allies, Allies of World War II, SS, Allied powers, Allies, Allies of World War</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,93.72,124.84,415.65,140.57"><head>Table 1 :</head><label>1</label><figDesc>Results for English monolingual retrieval. (Our submitted runs are denoted by the *.)</figDesc><table coords="6,357.96,124.84,7.69,9.41"><row><cell>5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,111.03,384.84,99.93"><head>Table 2 :</head><label>2</label><figDesc>Results for cross-lingual retrieval.</figDesc><table coords="7,128.16,111.03,346.68,76.88"><row><cell>RUN Description</cell><cell>* French MAP P@10</cell><cell>Spanish MAP P@10</cell><cell>German MAP P@10</cell></row><row><cell>BabelFish baseline</cell><cell>0.0476 0.1242</cell><cell>0.0566 0.1364</cell><cell>0.0563 0.1394</cell></row><row><cell>BabelFish+PRF</cell><cell>0.0501 0.1242</cell><cell>0.0541 0.1303</cell><cell>0.0655 0.1303</cell></row><row><cell>BabelFish+LEX</cell><cell>0.0606 0.1394</cell><cell>0.0581 0.1394</cell><cell>0.0586 0.1424</cell></row><row><cell cols="2">* BabelFish+LEX+PRF 0.0636 0.1394</cell><cell>0.0588 0.1273</cell><cell>0.0617 0.1364</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,735.27,117.68,9.41"><p>http://www.wikipedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,744.83,108.26,9.41"><p>http://en.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2" coords="8,517.99,466.60,10.46,7.75;8,517.99,448.34,10.46,15.50;8,517.99,440.74,10.46,4.27;8,517.99,430.22,10.46,7.75;8,517.99,408.08,10.46,19.38"><p>= 8.0, b = 0.5.)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,109.30,217.78,403.70,9.41;9,109.32,228.96,403.62,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,292.11,217.78,154.49,9.41">Discovering missing links in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adafre</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.12,217.78,45.88,9.41;9,109.32,228.96,203.40,9.41">Proceedings of the 3rd international workshop on Link discovery</title>
		<meeting>the 3rd international workshop on Link discovery<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,244.12,403.58,9.41;9,109.32,255.30,403.72,9.41;9,109.32,266.48,242.24,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,293.55,244.12,219.33,9.41;9,109.32,255.30,39.37,9.41">Finding similar sentences across multiple languages in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adafre</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.94,255.30,341.09,9.41;9,109.32,266.48,104.47,9.41">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,281.64,403.58,9.41;9,109.32,292.82,403.60,9.41;9,109.32,304.00,391.32,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,142.50,292.82,348.32,9.41">The University of Groningen at QA@CLEF 2006 using syntactic knowledge for QA</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,109.32,304.00,295.50,9.41">Working Notes for the Cross Language Evaluation Forum 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,319.16,403.61,9.41;9,109.32,330.34,403.69,9.41;9,109.32,341.51,286.50,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,109.32,330.34,246.59,9.41">Multilingual lexical semantic resources for ontology translation</title>
		<author>
			<persName coords=""><forename type="first">Asunciòn</forename><surname>Thierry Declerck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ovidiu</forename><surname>Gòmez Pèrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeno</forename><surname>Vela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manzano-Macho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,372.90,330.34,140.10,9.41;9,109.32,341.51,203.22,9.41">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,356.68,403.55,9.41;9,109.32,367.86,403.68,9.41;9,109.32,379.03,403.66,9.41;9,109.32,390.21,36.38,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,109.32,367.86,274.92,9.41">Overview of the CLEF-2006 cross-Language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,409.27,367.86,103.73,9.41;9,109.32,379.03,305.00,9.41">Proceedings of the CLEF 2006: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2006: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,405.38,403.59,9.41;9,109.32,416.55,403.69,9.41;9,109.32,427.73,403.63,9.41;9,109.32,438.91,102.93,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,310.45,405.38,202.44,9.41;9,109.32,416.55,102.19,9.41">Applying summarization techniques for term selection in relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adenike</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,231.10,416.55,281.91,9.41;9,109.32,427.73,222.24,9.41">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, Louisiana, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,454.07,403.60,9.41;9,109.32,465.25,403.57,9.41;9,109.32,476.43,403.60,9.41;9,109.32,487.61,397.91,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,324.36,454.07,188.54,9.41;9,109.32,465.25,192.86,9.41">Dublin City University at CLEF 2005: crosslanguage speech retrieval (CL-SR) experiments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adenike</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,236.14,487.61,140.52,9.41">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Rijke</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="792" to="799" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,502.77,403.69,9.41;9,109.32,513.95,82.42,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,186.63,502.77,132.20,9.41">An algorithm for su#x stripping</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,328.33,502.77,180.35,9.41">Automated Library and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,529.11,403.58,9.41;9,109.32,540.29,403.68,9.41;9,109.32,551.47,74.79,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,295.58,529.11,201.95,9.41">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,109.32,540.29,398.78,9.41">Proceedings of the 13th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 13th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,566.63,403.59,9.41;9,109.32,577.81,377.97,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,109.32,577.81,69.50,9.41">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,199.67,577.81,196.28,9.41">Proceedings of the 3rd Text REtrieval Conference</title>
		<meeting>the 3rd Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.30,592.97,403.57,9.41;9,109.32,604.15,403.72,9.41;9,109.32,615.33,350.07,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,398.87,592.97,114.00,9.41;9,109.32,604.15,143.07,9.41">Overview of the CLEF-2005 cross-language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,269.62,604.15,243.42,9.41;9,109.32,615.33,149.71,9.41">Proceedings of the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2005: Workshop on Cross-Language Information Retrieval and Evaluation<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
