<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,434.20,148.86,38.75,15.15;1,136.74,170.78,329.54,15.15">2007 Cross-language Speech Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,262.38,204.67,78.25,8.74"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
							<email>levow@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,434.20,148.86,38.75,15.15;1,136.74,170.78,329.54,15.15">2007 Cross-language Speech Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7F346DE3B9E3C581AF0496BDE9901FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Multilingual retrieval, Cross-language Retrieval, Stemming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Chicago participated in the CLEF 2007 CL-SR track, performing monolingual retrieval for both English and Czech and cross-language French-English retrieval. English experiments considered the impact of automatically generated keywords on retrieval. Czech experiments explored the effect of different stemming approaches on retrieval for this morphologically rich language. The best results for English employed automatically generated keywords, and the best results for Czech employed stemming strategies which significantly outperformed unstemmed techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Chicago participated in the CLEF 2007 cross-language speech retrieval (CL-SR) track employing both English and Czech document collections. For the English documents, we submitted four official runs, two in the required English monolingual title+description condition and two in the French-English cross-language condition. For English documents, contrastive experiments focused on the contribution of automatically generated keywords to retrieval effectiveness and the utility of different query translation strategies. For Czech documents, we submitted three official runs in the required monolingual Czech title+description condition. These experiments explored the utility of a range of different stemming procedures for this highly inflected language. We found that the use of automatically generated keywords and high quality free translation software in English and aggressive stemming techniques for Czech yielded the best results for each of the conditions evaluated.</p><p>In the remainder of the paper, we describe our processing and retrieval system in detail. We introduce the baseline monolingual English system and describe the impact of keyword inclusion. We then describe the query processing for the French-English cross-language system and comparative results. We present the Czech retrieval experiments, stemming strategies and impact. Finally, we conclude with some overall observations as well as plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR</head><p>ASR &amp; Keyword Mono 0.0512 0.0571 Table <ref type="table" coords="2,117.98,145.04,3.88,8.74">1</ref>: Monolingual retrieval on English spoken documents, with and without automatic keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">English Baseline System</head><p>We describe the query formulation, document creation, and retrieval processing for the monolingual English retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Formulation</head><p>All retrieval experiments employed the title and description fields of the original topic specifications. The components were simply concatenated together, employing the weighted sum operator (#sum) from the InQuery system, with default stemming and stopword removal. No additional removal of stop structure was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Creation</head><p>Two document formulations formed a primary contrast for monolingual and cross-language retrieval for English documents, one configuration included automatically generated keywords and the other did not. In both cases, we employed the manual document segmentation provided by the track organizers and the ASRTEXT2006B automatic speech recognition output field as the core document representation, based on its prior assessed effectiveness. <ref type="bibr" coords="2,382.26,406.86,11.69,8.74" target="#b3">[4]</ref> In the automatic keyword condition, all automatically generated keywords (AK1 and AK2) were added to the core document representation by concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval Engine</head><p>For the experiments reported below, we used the InQuery information retrieval system (version 3.1p1) <ref type="bibr" coords="2,121.44,489.01,10.52,8.74" target="#b0">[1]</ref> developed at the University of Massachusetts, Amherst, with a design motivated by inference networks, which normalizes the individual term weights when they are computed and then uses an unnormalized inner product to produce retrieval status values. The documents are then sorted in order of decreasing retrieval status value to form a list in an order that approximates a decreasing degree of relevance to the searcher's query. For English, stopwords were removed based on the default stopword list, and stemming was performed with the default kstem algorithm. We adopt the convention that values of p &lt; 0.05 for a Wilcoxon signed ranks test on a pair of retrieval results is considered significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Monolingual English Results</head><p>The baseline ASR transcript only yields a mean average precision of 0.0512. When augmented with automatically generated keywords, the results rise to 0.0571. The difference does not quite reached significance (p &lt;= 0.5068) by Wilcoxon Signed-Ranks test. The automatically generated keywords enrich the document representation with topical terms which appear to enhance retrieval and reduce the effects of the variance of the noisy output of the recognition of the automatic recognition of spontaneous speech.</p><p>3 French-English Cross-language Retrieval</p><p>For the cross-language retrieval conditions, the official runs employed a publicly available translation tool, while contrastive runs employed a dictionary-based word-for-word translation strategy, with a freely available dictionary and statistically derived stemming. We discuss the query translation procedure below; all document processing and retrieval components were identical to the monolingual English configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Translation</head><p>For the official cross-language French-English retrieval runs, we employed the publicly available translation tool provided by Google (http://translate.google.com) to translate the queries. Queries were created by concatenating the title and description fields of the French topics, analogous to the procedure used for English query formulation.</p><p>For contrastive runs, we employed a dictionary-based word-for-word translation procedure consistent with <ref type="bibr" coords="3,141.87,353.45,9.96,8.74" target="#b1">[2]</ref>. We obtained a freely available French-English bilingual term list from http://www.freedict.com. For the word-for-word translation process, all terms are first converted to lowercase and all accent diacritics are removed for consistency with the translation resource. Next, the translation procedure applies a backoff stemming strategy <ref type="bibr" coords="3,301.93,389.32,9.97,8.74" target="#b4">[5]</ref>, to support matching with highest precision between the query terms and the dictionary, but backing off to stemmed forms to enhance recall. We attempt initially to match the unstemmed forms in the query with unstemmed forms in the bilingual term list. Only if no match is found, do we perform stemming, attempting to match the stemmed query term in the unstemmed term list, the surface form of the query in the stemmed term list, and finally the stemmed query term in the stemmed term list. The stemming procedure employed stemming rules derived by a statistical stemming process as in <ref type="bibr" coords="3,416.96,461.05,9.96,8.74" target="#b2">[3]</ref>. 27% of the query terms remained untranslated, and all untranslatable terms were retained.</p><p>In both cases, the resulting English translations are stemmed and stopwords are removed, consistent with earlier document processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-language Retrieval Results &amp; Discussion</head><p>We find a substantial drop in mean average precision from the monolingual to the cross-language conditions, and from system-based to dictionary-based translation. Results appear in Table <ref type="table" coords="3,505.25,555.15,3.88,8.74" target="#tab_0">2</ref>. With comparable document representations, effectiveness for the cross-language condition drops 29-37% from monolingual levels for comparable document representations. A larger drop is observed for the baseline condition without automatically generated keywords than for that with keywords. Furthermore, the drop in retrieval effectiveness is even more pronounced in the wordfor-word case, and is low enough that the ASR-based retrieval is very similar to keyword-based retrieval.</p><p>These results indicate overall good effectiveness for the online translation tool for the French-English language pair and the limitations of the small dictionary-based translation strategy. However, the degradation from monolingual retrieval is quite large, and alternate strategies will be needed to overcome it. The less formal and highly variable character of the spontaneous speech materials limits the effectiveness of retrieval on ASR transcripts alone, while enrichment with automatically generated keywords appears to provide more useful representations of topical information, though differences to not reach significance. Additional strategies for enrichment and denoising of the query and document content will be necessary to overcome these challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Monolingual Czech Retrieval: Stemming Strategies</head><p>The Czech language poses special challenges for information retrieval. In contrast with English which has a relatively impoverished morphology, Czech employs a very rich morphology. As a result, to support effective matching between the document content and the specification of the user's information need, some means of overcoming the surface variance due to morphology is required. Here we explore the impact on retrieval effectiveness of three different stemming strategies: no stemming, light stemming, and aggressive stemming. We employ two freely available java-based Czech stemmers from the University of Neuchatel (http://members.unine.ch/jacques.savoy/clef/index.html) to perform light and aggressive stemming for Czech. "Light" stemming in these cases refers to removing affixes only for nouns and adjectives. The basic query formulation, indexing, and retrieval processes are consistent with those described above for monolingual English with three contrasts, in stemming, stopword removal, and document formation. First, we apply one of the three stemming strategies -none, light, or aggressive -to both queries and documents to enhance matching and retrieval in this morphologically rich language. We also incorporate a freely available Czech stopword list from the same source as the stemmers, to support stopword removal. <ref type="foot" coords="4,287.64,374.42,3.97,6.12" target="#foot_0">1</ref> Finally, we employ the playback point based document segmentation provided by the track organizers. Results thus employ the mGAP measure for Czech retrieval <ref type="bibr" coords="4,173.38,399.91,9.97,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and Discussion</head><p>We find that the best results are obtained with the aggressive stemming strategy, followed by light stemming, and lastly no stemming. All results appear in Table <ref type="table" coords="4,390.98,458.13,3.88,8.74" target="#tab_1">3</ref>. The results for aggressive stemming were the second best title+description based results for Czech in this evaluation. Clearly, the stemming approaches more effectively overcome the high degree of surface form variation of Czech terms. The unstemmed case is significantly outperformed by both the aggressive stemmer (p &lt;= 0.002) and the light stemmer (p &lt;= 0.01), although the two stemmers are not significantly different from each other.</p><p>However, the overall effectiveness of spontaneous speech retrieval in Czech is still quite limited. We speculate that not only must term matching be improved, for example through improved stemming and enhanced retrieval through pseudo-relevance feedback query or document expansion, but also through improvements in basic transcription accuracy and handling of spontaneous speech phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Experiments in monolingual English and cross-language French-English speech retrieval obtained the best results by augmenting ASR transcripts with automatically generated keywords. Experiments in monolingual Czech speech retrieval demonstrated the importance of stemming to overcome surface form variation for this morphologically rich language.</p><p>However, retrieval from spontaneous speech in both languages remains a very challenging task due both to the difficulties of speech recognition and to the challenging structure of spontaneous speech. In future work we plan to explore approaches to minimize the impact of speech recognition errors and variations in lexical choice through denoising strategies such as Generalized Latent Semantic Analysis to enhance document and query similarity even without lexical overlap. Also, while the current work has only employed the document segmentations provided, we hope to explore novel approaches to automatic segmentation of spontaneous speech which is important for broadening access to lengthy recorded materials and also poses many interesting challenges to integrating lexical and acoustic evidence of structure in spontaneous speech.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,110.82,422.99,79.62"><head>Table 2 :</head><label>2</label><figDesc>Mono-and cross-language retrieval on English documents, with and without automatic keywords.</figDesc><table coords="3,218.48,110.82,166.05,45.80"><row><cell>Query</cell><cell>ASR</cell><cell>ASR &amp; Keyword</cell></row><row><cell>Mono</cell><cell>0.0512</cell><cell>0.0571</cell></row><row><cell>FR + G</cell><cell>0.0322</cell><cell>0.0406</cell></row><row><cell cols="2">FR + wd 0.0256</cell><cell>0.0241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,112.11,110.82,378.78,42.95"><head>Table 3 :</head><label>3</label><figDesc>Czech monolingual speech retrieval results with different stemming strategies.</figDesc><table coords="4,182.76,110.82,237.46,21.09"><row><cell>Query</cell><cell cols="3">No Stem Light Stem Aggressive Stem</cell></row><row><cell>Czech TD</cell><cell>0.0126</cell><cell>0.0189</cell><cell>0.0203</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,747.00,357.71,6.99"><p>For compatibility with the retrieval system, we also removal all accent diacritics after stemming.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,202.65,407.49,8.74;5,105.50,214.61,407.50,8.74;5,105.50,226.56,214.13,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,383.64,202.65,129.35,8.74;5,105.50,214.61,14.53,8.74">The INQUERY retrieval system</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,144.11,214.61,368.89,8.74;5,105.50,226.56,51.93,8.74">Proceedings of the Third International Conference on Database and Expert Systems Applications</title>
		<meeting>the Third International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,246.49,407.50,8.74;5,105.50,258.44,407.50,8.74;5,105.50,270.40,234.80,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,372.37,246.49,140.63,8.74;5,105.50,258.44,155.19,8.74">Dictionary-based techniques for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,269.74,258.44,243.27,8.74;5,105.50,270.40,175.86,8.74">Information Processing and Management: Special Issue on Cross-language Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,290.32,407.50,8.74;5,105.50,302.28,224.04,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,318.75,290.32,194.25,8.74;5,105.50,302.28,149.91,8.74">CLEF Experiments at Maryland: Statistical Stemming and Backoff Translation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G.-A</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Cabezas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,322.20,407.50,8.74;5,105.50,334.16,407.51,8.74;5,105.50,346.11,160.23,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,300.78,334.16,212.22,8.74;5,105.50,346.11,59.85,8.74">Overview of the clef-2006 cross-language speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,186.38,346.11,48.01,8.74">CLEF-2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,366.04,407.50,8.74;5,105.50,377.99,407.51,8.74;5,105.50,389.95,117.28,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,365.48,366.04,147.53,8.74;5,105.50,377.99,106.62,8.74">Improved cross-language retrieval using backoff translation</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,235.21,377.99,277.80,8.74">Proceedings of Human Language Technology Conference (HLT)</title>
		<meeting>Human Language Technology Conference (HLT)</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="153" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
