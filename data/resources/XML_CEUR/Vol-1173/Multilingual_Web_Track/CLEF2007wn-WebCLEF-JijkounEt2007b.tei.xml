<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,148.86,423.01,15.15;1,131.74,170.78,339.51,15.15">The University of Amsterdam at WebCLEF 2007: Using Centrality to Rank Web Snippets</title>
				<funder ref="#_Vfk49Rj">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_RgqtvC4 #_rzqRzZM #_RNAUd7y #_hD3Aza5 #_tRZjcPx #_RrRWDvD #_3mMTweT #_N3VqHwN #_Anv5kGN #_aENCU4D">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,216.39,204.67,71.54,8.74"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
							<email>jijkoun@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.62,204.67,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,148.86,423.01,15.15;1,131.74,170.78,339.51,15.15">The University of Amsterdam at WebCLEF 2007: Using Centrality to Rank Web Snippets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B6A796B167DBF5780706A584C8B5B922</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the WebCLEF 2007 task, targeted at snippet retrieval from web data. Our system ranks snippets based on a simple similarity-based centrality, inspired by the web page ranking algorithms. We experimented with retrieval units (sentences and paragraphs) and with the similarity functions used for centrality computations (word overlap and cosine similarity). We found that using paragraphs with the cosine similarity function shows the best performance with precision around 20% and recall around 25% according to human assessments of the first 7,000 bytes of responses for individual topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The WebCLEF 2007 task<ref type="foot" coords="1,202.66,422.37,3.97,6.12" target="#foot_0">1</ref> differed substantially from the previous editions <ref type="bibr" coords="1,425.94,423.95,24.35,8.74">(2005)</ref><ref type="bibr" coords="1,455.16,423.95,19.48,8.74">(2006)</ref> of Web-CLEF). Rather than retriving a ranked list of web documents relevant to a topic, in the 2007 setup, systems were asked to return a ranked list of snippets (character spans) extracted from the top 1,000 web documents identified using the Google web search engine. The definition of the retrieval unit (snippet) was left up to a system, and thus the task is targeting information retrieval rather than document retrieval.</p><p>The remainder of this paper is organized as follows. We describe the WebCLEF 2007 task and topics in Section 2, present the architecture of our system in Section 3, describe our three runs, evaluation measures and evaluation results in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Topics</head><p>In WebCLEF 2007 for each topic systems are provided with the following information:</p><p>• topic title (e.g., Big Bang Theory);</p><p>• description of the information need (e.g., I have to make a presentation about Big Bang Theory for undergraduate students. I assume that the students have some basic knowledge of physics.);</p><p>• languages in which information can be returned;</p><p>• known sources: URLs and content of pages already "known" to the topic author;</p><p>• a list of web pages (original and text format) retrieved using Google with queries provided by the topic author (e.g., Big Bang); for each query, at most 1,000 pages are included in the list.</p><p>The task of a system is to return a ranked list of text spans from the provided web pages that, together, would satisfy the user's information need. Task organizers provided two development topics and 30 test topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Architecture</head><p>For each topic, our system used only text versions of the web documents. One the one hand, the decision not to use the original versions of the documens (HTML, PDF, Postscript, etc.) led to some noise in the output of the system. In the text versions, the text encoding was often broken, which was especially problematic for non-English documents (the task included Spanish and Dutch topics and pages). Moreover, in cases where an original document was a double-column PDF, in the corresponding text version, the lines of the columns were often intervened, making the text version hardly readable for humans. For some of the original documents (e.g., for Word files) text versions were missing, and therefore our system did not use these documents at all. On the other hand, using only text version simplified the data processing considerably:</p><p>• no sophisticated content extraction had to be developed, and</p><p>• the text versions often preserved some text layout of the original pages (e.g., paragraph starts), which we used to detect suitable snippet boundaries.</p><p>Given a topic, our system first identifies candidate snippets in the source documents by simply splitting the text of the documents into sentences (using punctuation marks as separators) or into paragraphs (using empty lines as separators). The same snippet extraction method is applied to the text of the "known" pages for the topic, resulting in a list of known snippets. We ignored candidate and known snippets shorter that 30 bytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ranking snippets</head><p>We rank the candidate snippets based on similiarity-based centrality, which is a simplified version of the graph-based snippet ranking of <ref type="bibr" coords="2,255.99,464.12,9.97,8.74" target="#b1">[2]</ref>, inspired by the methods for computing authority of the Web pages, such as PageRank and HITS <ref type="bibr" coords="2,271.05,476.08,9.97,8.74" target="#b3">[4]</ref>. For each candidate snippet we compute a centrality score by summing similarities of the snippet with all other candidate snippets. Then, to avoid assiging high scores to snippets containing information that is already known to the user, we subtract from the resulting centrality score similarities of the candidate snippet with all known snippets. As a final step, we remove from consideration candidate snippets whose similarity to one of the known snippets is higher than a threshold. The pseudocode for this calculation is shown below: Finally, the candidate snippets are ranked according to score(•) and top snippets are returned so that the total size of the response is not larger 10,000 bytes.</p><formula xml:id="formula_0" coords="2,109.93,567.62,35.79,9.76">let c 1 . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity between snippets</head><p>A key component of our snippet ranking method is the snippet similarity function sim(x, y). Similarly to <ref type="bibr" coords="3,127.79,142.36,9.97,8.74" target="#b2">[3]</ref>, we conducted experiments with two versions of the similarity function: one based on word overlap and one based on the cosine similarity in the vector space retrieval model. Specifically, for two text snippets, word overlap similarity is defined using the standard Jaccard coefficient on snippets considered as sets of terms:</p><formula xml:id="formula_1" coords="3,250.95,198.53,101.11,22.31">sim wo (x, y) = |x ∩ y | |x ∪ y | ,</formula><p>where x and y are sets of non-stopwords of snippets x and y respectively. The vector space similarity between two snippets is defined as the cosine of the angle between the vector representations of the snippets computed using the standard TF.IDF weighting scheme:</p><formula xml:id="formula_2" coords="3,233.03,273.32,136.94,26.98">sim vs (x, y) = -→ x • -→ y √ -→ x • -→ x √ -→ y • -→ y .</formula><p>Here, -→ a • -→ b denotes the scalar product of vectors -→ a and -→ b . Components of the vectors correspond to distinct non-stopword terms occuring in the set of candidate snippets. For a term t, the value of the component -→ a is defined according to the TF.IDF weighting scheme:</p><formula xml:id="formula_3" coords="3,217.19,360.54,168.63,23.23">-→ a (t) = TF (a, t) • log n |{c i : t ∈ c i }| .</formula><p>Here, TF (a, t) is the frequency of term t in snippet a and c 1 , . . . , c n are all candidate snippets. Both versions of the similarity function produce values between 0 and 1. The similarity threshold sim max for detecting near-duplicates is selected based on manual assessment of duplicates among candidate snippets for the development topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs and Evaluation Results</head><p>Our goal was to experiment with the units of snippet retrieval and with similarity functions. We submitted three runs:</p><p>• UvA sent wo -snippets defined as sentences, word overlap used for ranking;</p><p>• UvA par wo -snippets defined as paragraphs, word overlap for ranking;</p><p>• UvA par vs -paragraphs, vector space similarity.</p><p>The evaluation measures used for the task were character-based precision and recall, based on human assessments of the first 7,000 bytes of system's response. Precision is defined as the length of the character spans in the response identified by humans as relevant, divided by the total of the response (limited to 7,000 characters). Recall is defined as the length of spans in the reponse identified as relevant, divided by the total length of all distinct spans identified as relevant for the responses submitted by all systems.</p><p>The evaluation results for the three runs are shown below: The results indicate that paragraphs provide better retrieval units and using a more sophisticated similarity function based on the vector space model has a slight positive effect on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Unfortunately, we did not have enough time to analyse performance of the versions of the system per topic or to check whether the improvement with the vector space similarity function is significant.</p><p>Overall, we believe that the paragraph-based runs may serve as a reasonable baseline for the WebCLEF task: around 1/5 of the returned character content is considered relevant by human assessors. At the same time, such performance is probably not sufficient for a real-life information retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have described our participation in the WebCLEF 2007 snippet retrieval task. In our submission we experimented with retrieval units (sentences vs. paragraphs) and with similarity functions used for semantic centrality computations (word overlap vs. cosine similarity). We found what using paragraphs with the cosine similarity function shows the best performance with precision around 20% and recall around 25% according to human assessments of first 7,000 bytes of per-topic responses.</p><p>Detailed analysis of the performance of the runs is part of our immediate agenda for future work. Another interesting direction for further study is the similarity model suitable for short snippets. The vector space model that we use in this paper is not necessarily the best option. However, it has been shown (see, e.g., <ref type="bibr" coords="4,262.21,346.11,10.51,8.74" target="#b0">[1,</ref><ref type="bibr" coords="4,276.57,346.11,7.74,8.74" target="#b1">2]</ref>) that more sophisticated models do not necessarily lead to improvements when working with short text fragments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,147.37,567.62,103.16,9.76;2,109.93,579.58,131.34,9.76;2,109.93,591.54,117.34,8.85;2,129.85,603.49,66.31,8.85;2,129.85,615.45,117.33,8.85;2,149.78,627.40,147.47,8.85;2,129.85,639.36,105.04,8.85;2,149.78,651.31,145.86,8.85;2,129.85,663.27,105.04,8.85;2,149.78,675.22,90.76,9.76;2,169.70,687.18,66.31,8.85"><head>c n be candidate snippets let k 1</head><label>1</label><figDesc>. . . k m be known snippets for each candidate snippet c let score(c) = 0 for each candidate snippet c let score(c) = score(c) + sim(c, c ) for each known snippet k let score(c) = score(c) -sim(c, k) for each known snippet k if sim(c, k) &gt; sim max let score(c) = 0</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,738.80,217.81,7.21"><p>URL: http://ilps.science.uva.nl/WebCLEF/WebCLEF2007</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research presented in this paper was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220.80.001</rs>, <rs type="grantNumber">264.70.050</rs>, <rs type="grantNumber">354.20.005</rs>, <rs type="grantNumber">600.065.120</rs>, <rs type="grantNumber">612.13.001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, and <rs type="grantNumber">640.002.501</rs>. We are grateful to all participants and assessors of the WebCLEF 2007 task.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Vfk49Rj">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_RgqtvC4">
					<idno type="grant-number">220.80.001</idno>
				</org>
				<org type="funding" xml:id="_rzqRzZM">
					<idno type="grant-number">264.70.050</idno>
				</org>
				<org type="funding" xml:id="_RNAUd7y">
					<idno type="grant-number">354.20.005</idno>
				</org>
				<org type="funding" xml:id="_hD3Aza5">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_tRZjcPx">
					<idno type="grant-number">612.13.001</idno>
				</org>
				<org type="funding" xml:id="_RrRWDvD">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_3mMTweT">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_N3VqHwN">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_Anv5kGN">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_aENCU4D">
					<idno type="grant-number">640.002.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.50,503.47,407.51,8.74;4,105.50,515.42,141.98,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,264.92,503.47,229.71,8.74">Retrieval and novelty detection at the sentence level</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bolivar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,105.50,515.42,43.83,8.74">SIGIR &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,535.35,407.50,8.74;4,105.50,547.30,348.62,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,387.86,535.35,120.40,8.74">Fact discovery in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Sisay Fissaha Adafre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Jijkouni</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,117.95,547.30,279.42,8.74">IEEE/WIC/ACM International Conference on Web Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,567.23,407.50,8.74;4,105.50,579.18,407.51,8.74;4,105.50,591.14,407.51,8.74;4,105.50,603.10,164.49,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,285.66,567.23,227.34,8.74;4,105.50,579.18,35.70,8.74">Recognizing textual entailment: Is lexical similarity enough?</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,467.03,579.18,45.97,8.74;4,105.50,591.14,331.74,8.74">Evaluating Predictive Uncertainty, Textual Entailment and Object Recognition Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Dalche</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Quinonero</forename><surname>Candela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="449" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,623.02,407.50,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<title level="m" coord="4,149.96,623.02,289.35,8.74">Web Data Mining. Exploring Hyperlinks, Contents and Usage Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
