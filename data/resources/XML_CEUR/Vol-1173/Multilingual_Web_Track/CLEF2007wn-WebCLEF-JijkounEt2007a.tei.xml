<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.70,148.86,241.61,15.15">Overview of WebCLEF 2007</title>
				<funder ref="#_7t2sR3M">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_vFb2X6q #_tTyCDFW">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_ZjtR2VA #_r8X9JDb #_QvWX6HH #_tKgJhRk #_MT8fabY #_4CRwTtk #_kYUQmAy #_wmXuqVd #_z6m6nHC #_4uWEGVp #_gJkFwDZ #_CAw7NX9">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.77,182.75,71.54,8.74;1,309.23,182.75,75.99,8.74"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Maarten De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.70,148.86,241.61,15.15">Overview of WebCLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A84003D94F7312A448394A58503BCC08</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the WebCLEF 2007 task. The task definition-which goes beyond traditional navigational queries and is concerned with undirected information search goals-combines insights gained at previous editions of WebCLEF and of the WiQA pilot that was run at CLEF 2006. We detail the task, the assessment procedure and the results achieved by the participants.</p><p>The WebCLEF 2007 task combines insights gained from previous editions of WebCLEF 2005-2006 [6, 1]  and the WiQA 2006 pilot [4, 3], and goes beyond the navigational queries considered at WebCLEF 2005 and 2006. At WebCLEF 2007 we consider so-called undirected informational search goals [5] in a web setting: "I want to learn anything/everything about my topic." A query for topic X might be interpreted as "Tell me about X."</p><p>In the remainder of the paper we detail the task, the assessment procedure and the results achieved by the participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Task description</head><p>As key starting points for defining the WebCLEF task we took several issues into account. First, the task should correspond as close as possible to some real-world information need with a clear definition of a user. Second, multi-and cross-linguality should be natural (or even essential) for the task in the CLEF setting. Next, the collection(s) used in the task should be a natural source of choice for the user's information need. Then, collections, topics and assessors' judgements, resulting from the task should be re-usable in future. Finally, the task should be challenging for the state-of-the-art IR and NLP technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task model</head><p>Our hypothetical user is a knowledgable person, perhaps even an an expert, writing a survey article on a specific topic with a clear goal and audience, for example, a Wikipedia article, or a state of the art survey, or an article in a scientific journal. She needs to locate items of information to be included in the article and wants to use an automatic system to help with this. The user does not have immediate access to offline libraries and only uses online sources.</p><p>The user formulates her information need (the topic) by specifying:</p><p>• a short topic title (e.g., the title of the survey article),</p><p>• a free text description of the goals and the intended audience of the article,</p><p>• a list of languages in which the user is willing to accept the found information,</p><p>• an optional list of known sources: online resources (URLs of web pages) that the user considers to be relevant to the topic and information from which might already have been included in the article, and</p><p>• an optional list of Google retrieval queries that can be used to locate the relevant information; the queries may use site restrictions (see examples below) to express the user's preferences.</p><p>Here's an example of an information need:</p><p>• topic title: Significance testing</p><p>• description: I want to write a survey (about 10 screen pages) for undergraduate students on statistical significance testing, with an overview of the ideas, common methods and critiques. I will assume some basic knowledge of statistics.</p><p>• language(s): English</p><p>• known source(s): http://en.wikipedia.org/wiki/Statistical_hypothesis_testing ; http://en.wikipedia.org/wiki/Statistical_significance</p><p>• retrieval queries: significance testing; site:mathworld.wolfram.com significance testing; significance testing pdf; significance testing site:en.wikipedia.org Defined in this way, the task model corresponds to addressing undirected informational search goals, that are reported to account for over 23% of web queries <ref type="bibr" coords="2,368.41,323.23,9.96,8.74" target="#b4">[5]</ref>.</p><p>Each participating team was asked to develop several topics and subsequently assess responses of all participating systems for the created topics and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Data collection</head><p>In order to keep the idealized task as close as possible to the real-world scenario (i.e., there are many relevant documents) but still tractable (i.e., the size of the collection is manageable), our collection is defined per topic. Specifically, for each topic, the subcollection for the topic contains the following set of documents along with their URLs:</p><p>• all "known" sources specified for the topic;</p><p>• the top 1000 (or less, depending at the actual availability) hits from Google for each of the retrieval queries specified in the topic, or for the topic title if the queries are not specified;</p><p>• for each online document included in the collection, its URL, the original content retrieved from the URL and the plain text conversion of the content are provided. The plain text conversion is only available for HTML, PDF and Postscript documents. For each document, the subcollection also provides its origin: which query or queries were used to locate it and at which rank(s) in the Google result list it was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">System response</head><p>For each topic description, a response of an automatic system consists of a ranked list of plain text snippets extracted from the sub-collection of the topic. Each snippet should indicate what document of the sub-collection it comes from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Assessment</head><p>In order to comply with the task model, the manual assessment of the responses of the systems was done by the topic creators. The assessment procedure was somewhat similar to assessing answers to OTHER questions at TREC 2006 Question Answering task <ref type="bibr" coords="2,402.05,697.70,9.97,8.74" target="#b7">[8]</ref>.</p><p>The assessment was to be blind. For a given topic, all responses of all system were pooled into anonymized sequence of text segments. To limit the amount of required assessments, for each topic only first 7,000 characters of each response were included (according to the ranking of the snippets in the response). The cut-off point 7,000 was chosen so that for at least half of the submitted runs the length of the responses was at least 7,000 for all topics. For the pool created in this way for each topic, the assessor was asked to make a list of nuggets, atomic facts, that, according to the assessor, should be included in the article for the topic. A nugget may be to character spans in the responses, so that all spans linked to one nugget express this atomic fact. Different character spans in one snippet in the response may be linked to more than one nugget. The assessors used a GUI to mark character spans in the responses and link each span to the nugget it expresses (if any). Assessors could also mark character spans as "known" if they expressed fact relevant for the topic but alredy present in one of the known sources.</p><p>Figure <ref type="figure" coords="3,135.79,556.37,4.98,8.74" target="#fig_0">1</ref> shows the assessment interface for the topic "iPhone opinions". Snippets (left bottom of the figure) are separated by grey horisontal lines. Note that only part of the first snippet is marked as relevant and the second snippet contain two marked spans that are linked to distinct nuggets. This example illustrates the many-to-many relation between nuggets and character spans in the response.</p><p>Similar to INEX <ref type="bibr" coords="3,184.55,616.15,10.52,8.74" target="#b1">[2]</ref> and to some tasks at TREC (i.e., the 2006 Expert Finding task <ref type="bibr" coords="3,498.61,616.15,10.79,8.74" target="#b6">[7]</ref>) assessment was carried out by the topic developer, i.e., by the participants themselves.</p><p>Table <ref type="table" coords="3,132.34,640.06,4.98,8.74" target="#tab_0">1</ref> gives the statistics for the 30 test topics and for the assessments of the topics. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation measures</head><p>The evaluation measures for the task are based on standard precision and recall. For a given response R (a ranked list of text snippets) of a system S for a topic T we define:</p><p>1 Full definition of the test topics is available from http://ilps.science.uva.nl/WebCLEF/WebCLEF2007/Topics.  • recall as the sum of character lenghts of all spans in R linked to nuggets, divided by the total sum of span lengths in the responses for T in all submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id</head><p>• precision as the number of characters that belong to at least one span linked to a nugget, divided by the total character length of the system's response.</p><p>Note that the evaluation measures described above differ slightly from the measures originally proposed in the task description. <ref type="foot" coords="5,237.74,441.01,3.97,6.12" target="#foot_0">2</ref> The original measures were based on the fact that spans are linked to nuggets by assessors: as described in section 2, different spans linked to one nugget are assumed to bear approximately the same factual content. Then, in addition to character-based measures above, a nugget-based recall can be defined based on the number of nuggets (rather than lengths of character spans) found by a system. However, an analysis of the assessments showed that some assessors used nuggets in a way not intended by the assessment guidelines: namely, to group related rather than synonymous character spans. We believe that this misinterpretation of the assessment guidelines indicates that the guidelines are overly complicated and need to be simplified in future edition of the task. As a consequence, we did not use nugget-based measures for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>In total, 12 runs were submitted from 4 research groups. To provide a baseline for the task, we created an artificial run: for each topic, a response of the baseline was created as the ranked list of at most 1000 snippets provided by Google in response to retrieval queries from the topic definition. Note that the Google web search engine was designed for a task very different from WebCLEF 2007 (namely, for the task of web page finding), and therefore the evaluation results of our baseline can in no way be interpreted as an indication of Google's performance.</p><p>Table <ref type="table" coords="5,132.22,676.68,4.98,8.74" target="#tab_1">2</ref> shows the submitted runs with the basic statistics: the average length (the number of bytes) of the snippets in the run, the averate number of snippets in the response for one topic, and the average total length of response per topic.</p><p>Run @ 1,500 bytes @ 3,500 bytes @ 7,000 bytes  <ref type="table" coords="6,116.52,310.18,3.88,8.74">3</ref>: Evaluation results for the baseline (Google snippets) and the 12 submitted runs calculated at cut-off points 1,500, 3,500 and 7,000 bytes for a response for one topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" coords="6,116.47,375.78,4.98,8.74">3</ref> shows the evaluation results for the baseline and the submitted runs: precision and recall at three different cut-off points. Since the sizes of the submitted runs varied substantially (Table <ref type="table" coords="6,501.38,387.73,3.87,8.74" target="#tab_1">2</ref>), the cut-off points were chosen to enable comparison across runs. Table <ref type="table" coords="6,131.95,411.64,4.98,8.74">3</ref> indicates that most runs outperform the baseline, or show a similar performance. Two of the runs (UVA par vs and UVA par wo) show the best performance for all cut-off points. Two other runs (USAL reina0.25 and USAL reina1 ) show a comparable performance. We will provide a more detailed per-topic analysis and significance testing results by the time of the workshop.</p><p>One unexpected phenomenon is that for all runs (except the baseline) the precision grows as the cut-off point increases. This might indicate that although systems manage to find relevant information snippets in the collection, the ranking of the found snippets is far from optimal. This issue, however, also needs a detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We described WebCLEF 2007. This was the first year in which a new task was being assessed, one aimed at undirected information search goals. While the number of participants was limited, we believe the track was a success, as most submitted runs outperformed the Google-based baseline. For the best runs, in top 7,000 bytes per topic about 1/5 of the text was found relevant and important by the assessors.</p><p>The WebCLEF 2007 evaluation also raised several important issues. The task definition did not specify the exact size of a system's response for a topic, which has make a comparison across systems problematic. Furthermore, assessor's guidelines appeared to be overly complicated: not all assessors used nuggets as was intented by the organizers.</p><p>Our plans for future work include a detailed per-topic analysis of runs and a study of re-usability of the results of the assessments for improving the systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,404.44,423.00,8.74;3,90.00,416.39,423.01,8.74;3,90.00,428.35,68.63,8.74;3,90.00,108.86,423.00,280.47"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A screenshot of the WebCLEF 2007 assessment interface: the topic definition (top), responses of the systems with annotated character spans (left), list of nuggets created by an assessor (right).</figDesc><graphic coords="3,90.00,108.86,423.00,280.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,703.20,423.01,56.56"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="5,107.38,108.86,389.74,200.39"><row><cell>Participant</cell><cell>Run</cell><cell>Average snippet length</cell><cell>Average snippets per topic</cell><cell>Average response length per topic</cell></row><row><cell>Baseline</cell><cell>Google snippets</cell><cell>145</cell><cell>898</cell><cell>131041</cell></row><row><cell>School of Computing,</cell><cell>DCU run1 simple</cell><cell>118</cell><cell>30</cell><cell>3552</cell></row><row><cell>Dublin City University</cell><cell>DCU run2 parsed</cell><cell>137</cell><cell>27</cell><cell>3770</cell></row><row><cell></cell><cell>DCU run2 topfilter</cell><cell>112</cell><cell>29</cell><cell>3346</cell></row><row><cell cols="2">Faculty of Computer Science, UIWC07odwgstr</cell><cell>151</cell><cell>10</cell><cell>1522</cell></row><row><cell>University of Indonesia</cell><cell>UIWC07uw10</cell><cell>155</cell><cell>10</cell><cell>1559</cell></row><row><cell></cell><cell>UIWC07wstr</cell><cell>152</cell><cell>10</cell><cell>1530</cell></row><row><cell>REINA Research Group,</cell><cell>USAL reina0.25</cell><cell>833</cell><cell>50</cell><cell>41680</cell></row><row><cell>University of Salamanca</cell><cell>USAL reina0</cell><cell>832</cell><cell>50</cell><cell>41658</cell></row><row><cell></cell><cell>USAL reina1</cell><cell>833</cell><cell>50</cell><cell>41708</cell></row><row><cell>ISLA,</cell><cell>UVA par vs</cell><cell>254</cell><cell>29</cell><cell>7420</cell></row><row><cell>University of Amsterdam</cell><cell>UVA par wo</cell><cell>277</cell><cell>25</cell><cell>7158</cell></row><row><cell></cell><cell>UVA sent wo</cell><cell>214</cell><cell>33</cell><cell>7225</cell></row></table><note coords="4,132.22,703.20,380.78,8.74;4,90.00,715.15,423.00,8.74;4,90.00,727.11,423.00,8.74;4,90.00,739.06,423.01,8.74;4,90.00,751.02,183.79,8.74"><p>Statistics for WebCLEF 2007 topics and assessments. For each topic we show: topic id and title, author/assessor (anonymized), accepted languages, the number of "known" sources (web pages), the total number of snippets assessed from all submissions, the total number of spans marked as "known", the total number of spans attached to one of the nuggets, and the total length (the number of characters) in these spans.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,109.99,332.50,383.03,8.74"><head>Table 2 :</head><label>2</label><figDesc>Simple statistics for the baseline (Google snippets) and the 12 submitted runs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,105.24,719.52,330.74,7.21"><p>See http://ilps.science.uva.nl/WebCLEF/WebCLEF2007/Tasks/\#Evaluation_measures.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>7 Acknowledgments <rs type="person">Valentin Jijkoun</rs> was supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.065.120</rs> and <rs type="grantNumber">612.000.106</rs>. Maarten de Rijke was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">600.065.120</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, <rs type="grantNumber">640.002.501</rs>, and and by the <rs type="programName">E.U. IST programme</rs> of the <rs type="programName">6th FP for RTD</rs> under project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7t2sR3M">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_ZjtR2VA">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_vFb2X6q">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_tTyCDFW">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_r8X9JDb">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_QvWX6HH">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_tKgJhRk">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_MT8fabY">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_4CRwTtk">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_kYUQmAy">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_wmXuqVd">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_z6m6nHC">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_4uWEGVp">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_gJkFwDZ">
					<idno type="grant-number">640.002.501</idno>
					<orgName type="program" subtype="full">E.U. IST programme</orgName>
				</org>
				<org type="funded-project" xml:id="_CAw7NX9">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">6th FP for RTD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,190.70,407.51,8.74;7,105.50,202.65,49.15,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,340.61,190.70,101.86,8.74">Overview of WebCLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,486.36,190.70,26.64,8.74;7,105.50,202.65,18.51,8.74">CLEF 2006</title>
		<imprint>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,222.58,407.50,8.74;7,105.50,234.53,407.50,8.74;7,105.50,246.49,166.13,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,314.26,222.58,198.73,8.74;7,105.50,234.53,407.50,8.74;7,105.50,246.49,37.59,8.74">Comparative Evaluation of XML Information Retrieval Systems: 5th International Workshop of the Initiative for the Evaluation of XML Retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,150.83,246.49,47.48,8.74">INEX 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,266.41,407.50,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,229.92,266.41,183.65,8.74">Overview of the WiQA task at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,434.18,266.41,48.43,8.74">CLEF 2006</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,286.34,407.51,8.74;7,105.50,298.29,407.50,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,228.87,286.34,266.61,8.74">WiQA: Evaluating Multi-lingual Focused Access to Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,323.92,298.29,100.53,8.74">Proceedings EVIA 2007</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Evans</surname></persName>
		</editor>
		<meeting>EVIA 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,318.22,407.51,8.74;7,105.50,330.17,407.51,8.74;7,105.50,342.13,25.74,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,226.11,318.22,165.78,8.74">Understanding user goals in web search</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,410.41,318.22,102.60,8.74;7,105.50,330.17,194.65,8.74">WWW &apos;04: Proceedings of the 13th intern. conf. on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,362.05,407.51,8.74;7,105.50,374.01,407.51,8.74;7,105.50,385.96,407.51,8.74;7,105.50,397.92,220.96,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,308.59,362.05,99.95,8.74">Overview of WebCLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,385.96,208.50,8.74">Accessing Multilingual Information Repositories</title>
		<title level="s" coord="7,393.19,385.96,119.81,8.74;7,105.50,397.92,30.49,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005-09">2005. September 2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="810" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,417.85,407.51,8.74;7,105.50,429.80,270.62,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,296.93,417.85,198.09,8.74">Overview of the TREC 2006 Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,429.80,240.07,8.74">The Fifteenth Text REtrieval Conference (TREC 2006)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,449.73,407.51,8.74;7,105.50,461.68,258.02,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,244.02,449.73,232.02,8.74">Overview of the TREC 2005 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,496.20,449.73,16.81,8.74;7,105.50,461.68,227.47,8.74">The Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
