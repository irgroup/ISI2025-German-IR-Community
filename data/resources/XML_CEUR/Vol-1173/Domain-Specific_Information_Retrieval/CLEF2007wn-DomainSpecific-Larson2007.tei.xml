<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.96,98.63,379.15,15.51;1,124.32,120.59,354.49,15.51;1,193.08,142.43,216.87,15.51">Experiments in Classification Clustering and Thesaurus Expansion for Domain Specific Cross-Language Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.76,175.93,63.74,9.96"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<email>ray@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.96,98.63,379.15,15.51;1,124.32,120.59,354.49,15.51;1,193.08,142.43,216.87,15.51">Experiments in Classification Clustering and Thesaurus Expansion for Domain Specific Cross-Language Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59CCD6617C81FE83A896882CDA5D04F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Performance</term>
					<term>Measurement Cheshire II</term>
					<term>Logistic Regression</term>
					<term>Entry Vocabulary Indexes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will describe Berkeley's approach to the Domain Specific (DS) track for CLEF 2007. This year we are using forms of the Entry Vocabulary Indexes and Thesaurus expansion approaches used by Berkeley in 2005[10]. Despite the basic similarity of approach, we are using quite different implementations with different characteristics. We are not, however, using the tools for de-compounding for German that were developed over the past many years and used very successfully in earlier Berkeley entries in this track. All of the runs submitted were performed using the Cheshire II system. This year Berkeley submit a total of submitted 24 runs, including one for each subtask of the DS track. These include 6 Monolingual runs for English, German, and Russian, 12 Bilingual runs (4 X2EN, 4 X2DE, and 4 X2RU), and 6 Multilingual runs (2 EN, 2 DE, and 2 RU). Since the overall results were not available at the time this paper was due, we do not know how these results fared compared to other participants, so the discussion in this paper focuses on comparisions between our own runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper discusses the retrieval methods and evaluation results for Berkeley's participation in the CLEF 2007 Domain Specific track. Last year for this track we used a baseline approach using text retrieval methods only <ref type="bibr" coords="1,204.65,643.57,12.45,9.96" target="#b6">[7]</ref> without query expansion or use of the Thesaurus. This year we have focused instead on query expansion using Entry Vocabulary Indexes(EVIs) <ref type="bibr" coords="1,415.50,655.57,13.63,9.96" target="#b3">[4,</ref><ref type="bibr" coords="1,432.72,655.57,11.62,9.96" target="#b9">10]</ref>, and thesaurus lookup of topic terms. We continue to use probabilistic IR methods based on logistic regression.</p><p>All of the submitted runs for this year's Domain Specific track used the Cheshire II system for indexing and retrieval. The "Classification Clustering" feature of the system was used to generate the EVIs used in query expansion. The original approach for Classification Clustering was in searching was described in <ref type="bibr" coords="2,239.28,473.77,10.45,9.96" target="#b4">[5]</ref> and <ref type="bibr" coords="2,272.53,473.77,9.91,9.96" target="#b5">[6]</ref>. Although the method has experienced considerable changes in implementation, the basic approach is still the same: topic-rich elements extracted from individual records in the database (such as titles, classification codes, or subject headings) are merged based on a normalized version of a particular organizing element (usually the classification or subject headings), and each such classification cluster is treated as a single "document" containing the combined topic-rich elements of all the individual documents that have the same values of the organizing element. The EVI creation and search approach taken for this research is described below in Section 3.3.</p><p>This paper first very briefly describes the probabilistic retrieval methods used, including our blind feedback method for text, which are discussed in greater detail in our ImageCLEF notebook paper <ref type="bibr" coords="2,111.08,593.29,12.65,9.96" target="#b7">[8]</ref>. We then describe our submissions for the various DS sub-tasks and the results obtained. Finally we present conclusions and discussion of future approaches to this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Retrieval Algorithms</head><p>As we have discussed in our other papers for the ImageCLEF and GeoCLEF tracks in this volume, basic form and variables of the Logistic Regression (LR) algorithm used for all of our submissions were originally developed by Cooper, et al. <ref type="bibr" coords="2,277.22,683.89,9.91,9.96" target="#b2">[3]</ref>. To formally the LR method, the goal of the logistic regression method is to define a regression model that will estimate (given a set of training data), for a particular query Q and a particular document D in a collection the value P (R | Q, D), that is, the probability of relevance for that Q and D. This value is then used to rank the documents in the collection which are presented to the user in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, giving a regression formula for estimating the log odds from those statistics:</p><formula xml:id="formula_0" coords="3,235.08,548.34,277.96,30.57">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of a sample set of queries, a collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="3,232.44,626.01,280.60,25.12">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For all of our Domain Specific submissions this year we used a version of the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="4,179.50,61.33,12.29,9.96" target="#b0">[1]</ref> and which is also used in our GeoCLEF and Domain Specific submissions.</p><p>For the Domain Specific track we used the Cheshire II information retrieval system implementation of this algorithm. One of the current limitations of this implementation is the lack of decompounding for German documents and query terms in the current system. As noted in our other CLEF notebook papers, the Logistic Regression algorithm used was originally developed by Cooper et al. <ref type="bibr" coords="4,151.44,121.09,10.57,9.96" target="#b1">[2]</ref> for text retrieval from the TREC collections for TREC2. The basic formula is:</p><formula xml:id="formula_2" coords="4,184.08,139.30,233.67,147.36">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable,</p><formula xml:id="formula_3" coords="4,90.00,324.13,422.95,59.53">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 - p(R|C, Q) |Q c</formula><p>| is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term, tf i is the within-document frequency of the ith matching term, ctf i is the occurrence frequency in a collection of the ith matching term, ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations), cl is component length (i.e., number of terms in a component), and N t is collection length (i.e., number of terms in a test collection).</p><p>c k are the k coefficients obtained though the regression analysis.</p><p>More details of this algorithm and the coefficients used with it may be found in our ImageCLEF notebook paper where the same algorithm and coefficients were used. In addition to this primary algorithm we used a version that performs "blind feedback" during the retrieval process. The method used is described in detail in our ImageCLEF notebook paper <ref type="bibr" coords="4,403.15,557.05,12.65,9.96" target="#b7">[8]</ref>. Our blind feedback approach uses the 10 top-ranked documents from an initial retrieval using the LR algorithm above, and selects the top 10 terms from the content of those documents, using a version of the Robertson and Sparck Jones probabilistic term relevance weights <ref type="bibr" coords="4,328.34,592.93,14.58,9.96" target="#b10">[11]</ref>. Those ten terms are merged with the original query and new term frequency weights are calculated, and the revised query submitted to obtain the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for Domain Specific Retrieval</head><p>In this section we describe the specific approaches taken for our submitted runs for the Domain Specific track. First we describe the database creation and the indexing and term extraction methods used, and then the search features we used for the submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database creation</head><p>For the purposes of this research we combined the GIRT German/English thesaurus along with the English and Russian mappings for the CSASA and ISISS databases to produce a multilingual thesaurus where elements from each of the original sources, as well as transliterations and capitalizations and the conversion of all data to UTF-8 encoding (this was also performed on the databases themselves before indexing). An example entry from this thesaurus is shown below: &lt;entry&gt; &lt;german&gt;Absatz&lt;/german&gt; &lt;german-caps&gt;ABSATZ&lt;/german-caps&gt; &lt;scope-note-de&gt;nicht im Sinne von Vertrieb&lt;/scope-note-de&gt; &lt;english-translation&gt;sale&lt;/english-translation&gt; &lt;german_utf8&gt;Absatz&lt;/german_utf8&gt; &lt;russian&gt; sbyt &lt;/russian&gt; &lt;translit&gt;sbyt &lt;/translit&gt; &lt;mapping&gt; &lt;original-term&gt;Absatz&lt;/original-term&gt; &lt;mapped-term&gt;Sales&lt;/mapped-term&gt; &lt;/mapping&gt; &lt;mapping&gt; &lt;original-term&gt;sale&lt;/original-term&gt; &lt;mapped-term&gt;Sales&lt;/mapped-term&gt; &lt;/mapping&gt; &lt;/entry&gt; Note that the spacing around the Russian cyrillic term was inserted in the paper formatting process and was not in the original data.</p><p>Because not all of the terms had mappings, or equivalent Russian terms those parts are not present for all of the thesaurus entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing and Term Extraction</head><p>Although the Cheshire II system uses the XML structure of documents and extracts selected portions of the record for indexing and retrieval, for the submitted runs this year we used only a single one of these indexes that contains the entire content of the document.  <ref type="table" coords="6,132.12,497.65,4.98,9.96" target="#tab_0">1</ref> lists the indexes created for the Domain Specific database and the document elements from which the contents of those indexes were extracted. The "Used" column in Table <ref type="table" coords="6,466.80,509.65,4.98,9.96" target="#tab_0">1</ref> indicates whether or not a particular index was used in the submitted Domain Specific runs. This year we used the Entry Vocabulary Indexes (search term recommenders) that were used in somewhat different form by Berkeley in previous years (see <ref type="bibr" coords="6,289.08,545.53,14.73,9.96" target="#b9">[10]</ref>), without overall data on the track performance this year it is difficult to say whether this approach improved upon, or degraded, the text-retrieval baseline we established last year. Given the changes in the collections used (the addition of the CSASA English collection and elimination of the Russian SocioNet data), it is not possible to directly compare MAP or other evaluation measures across years. The implementation of the Classification Cluster -based EVIs will be discussed in the next section.</p><p>For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs, however, did not use decompounding in the indexing and querying processes to generate simple word forms from compounds (actually we tried, but there was a bug that failed to match any compounds in our runs). This is another aspect of our indexing for this year's Domain Specific task that reduced our results relative to last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entry Vocabulary Indexes</head><p>As noted above earliest versions of Entry Vocabulary Indexes were developed to facilitate automatic classification of library catalog records, and first used in searching in <ref type="bibr" coords="7,435.61,91.69,10.00,9.96" target="#b5">[6]</ref>. Those used a simple frequency-based probabilistic model in searching, but a primary feature was that the "Classification clusters", were treated as documents and the terms associated with top-ranked clusters were combined with the original query, in a method similar to "blind feedback", to provide an enhanced second stage of search.</p><p>Our later work with EVIs used a maximum likelihood weighting for each term (word or phrase) in each classification. This was the approach described in <ref type="bibr" coords="7,338.65,163.45,10.55,9.96" target="#b3">[4]</ref> and used for Cross-language Domain-Specific retrieval for CLEF 2005. One limitation of that approach is that the EVI can produce maximum likelihood estimates for only a single term at a time, and alternative approaches needed to be explored for combining terms (see <ref type="bibr" coords="7,266.76,199.33,15.49,9.96" target="#b9">[10]</ref> for the various approaches).</p><p>Although the method has experienced considerable changes in implementation, the basic approach for "Classification Clustering" in Cheshire II is still the same. Various topic-rich elements are extracted from individual records in the database (such as titles, classification codes, or subject headings) and are merged into single records based on a normalized version of a particular organizing element (usually the classification or subject headings, e.g., one record is created for each unique classification or subject heading). Each of these classification clusters is treated as a single "document" containing the combined topic-rich elements of all the individual documents that have the same values of the organizing element. In place of the simpler probabilistic model used in the early research, we use the same logistic regression based algorithm that is used for text retrieval. In effect, we just search the "Classification Clusters" as if they were documents using the TREC2 algorithm with blind feedback described above, then take some number of the top-ranked terms and use those to expand the query for submission to the normal document collection. Testing with the 2006 data showed that just taking the single top-ranked term performed better than using multiple terms for this approach, so only the single top-ranked recommended term was used in the experiments reported here.</p><p>Two separate EVIs were built for the databases in each target language. The first used the contents of the "CONTROLLED-TERM-??" (or "KEYWORD" for Russian) fields as the organizing element. The second EVI used the contents of the "CLASSIFICATION-??" fields. Both of these EVIs were used in query expansion. One problem was that some records included multiple controlled terms in a single field instead of as separate fields. This was particularly common for the Russian "KEYWORD" terms. For this year we just ignored this problem rather than attempting to fix it, but we will be examining the effects in our analysis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search Processing</head><p>Searching the Domain Specific collection used Cheshire II scripts to parse the topics and submit the title and description elements from the topics to the "topic" index containing all terms from the documents. For the monolingual search tasks we used the topics in the appropriate language (English, German, or Russian), and for bilingual tasks the topics were translated from the source language to the target language using the LEC Power Translator PC-based program. Our original testing of LEC Power Translator seemed to show a good translations between any of the languages needed for the track, but we intend to do some further testing to compare to previous approaches (which used web-based translation tools like Babelfish and PROMT). We suspect that, as always, different tools provide a more accurate representation of different topics for some languages, but the LEC Power Translator seemed to do pretty good (and often better) translations for all of the needed languages.</p><p>Because all of our submitted runs this year used some form of query expansion, each required a 2-phase search process. The first phase involved a search in the EVI or the merged thesaurus, and the second phase combined some of the results of first phase search with the original query and used the expanded query to search the collections in the target language. Multiling. from Russian Thes 0.0400</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">EVI Searches</head><p>For the monolingual and bilingual EVI searches (all those indicated in Table <ref type="table" coords="8,426.00,446.65,4.98,9.96" target="#tab_2">2</ref> with "EVI" in the "Exp." or expansion column) the first search phase used all terms included in the "title" and "desc" fields of the topics (or the tranlated version of these fields). These terms were searched using the TREC2 algorithm with blind feedback to obtain a ranked result of classification clusters from the EVIs. The main or "organizing term" phrases for the top-ranked two clusters from the results for the "CONTROLLED-TERM" EVI, and the single top-ranked result phrase for the "CLASSIFICATION" EVI were extracted for use in the second phase. For example, Topic #190 was searched using "mortality rate : find information on mortality rates in individual european countries" and the two EVIs yielded the following terms: "child mortality : infant mortality : demography and human biology; demography (population studies)".</p><p>For the second phase search the original query was searched using the initial title+desc from the topic using the "topic" index and the expansion terms were searched in the "subject" index, these searches were merged using a weighted sum for items in both lists that is based on the "Pivot" method described by Mass and Mandelbrod <ref type="bibr" coords="8,308.40,602.05,14.57,9.96" target="#b8">[9]</ref> to combine the results of different document components. In our case the probability of relevance for a component is a weighted combination of the initial estimate probability of relevance for the subject search and the probability of relevance for the entire document. Formally this is:</p><formula xml:id="formula_4" coords="8,152.04,661.81,356.75,10.33">P (R | Q, C new ) = (X * P (R | Q, C subj )) + ((1 -X) * P (R | Q, C doc )) (<label>3</label></formula><formula xml:id="formula_5" coords="8,508.79,661.81,4.25,9.96">)</formula><p>Where X is a "pivot value" between 0 and 1, and P (R | Q, C new ), P (R | Q, C subj ) and P (R | Q, C doc ) are the new weight, the original subject search weight, and document weight for a given query. We found that a pivot value of 0.15 was most effective for CLEF2006 data when combining EVI and search queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Thesaurus-based searches</head><p>The basic steps for the searched doing thesaurus lookup is the same for EVIs, but the search structure is different. For the first phase search the topic title is searched among the languageappropriate main terms of the thesaurus, and the description is searched among all terms in the thesaurus entry. These intermediate results are combined using the pivot merger method described about with a pivot weight of 0.55. The top two results are used, and both the language-appropriate main term, and the appropriate mapping terms are used for the query expansion. In the second phase the full topic title and desc fields are searched as topics, and the thesaurus terms are also searched as topics. These searches are combined using the pivot merge with a pivot weight of 0.07.</p><p>For topic #190 the first part of the query (i.e., the topic title and desc terms) is the same as for the EVI searches, but the second part of the search uses the terms yielded by the thesaurus search: "mortality : Infant mortality" (only a single thesaurus entry was retrieved in the search).</p><p>For multilingual searches, we combined the various translations of the topic title and desc fields produced by the LEC Power Translator for each source language and searched those combined translations in each target language. The results for each language were merged based on the MINMAX normalized score for each resultset. Within each language the same approaches were used as for EVI and Thesaurus-based expansion of bilingual and monolingual searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Submitted Runs</head><p>The summary results (as Mean Average Precision) for all of our submitted runs for English, German and Russian are shown in Table <ref type="table" coords="9,277.46,363.61,3.90,9.96" target="#tab_2">2</ref>, the Recall-Precision curves for these runs are also shown in Figure <ref type="figure" coords="9,164.17,375.61,4.98,9.96" target="#fig_0">1</ref> (for monolingual), Figure <ref type="figure" coords="9,287.33,375.61,4.98,9.96" target="#fig_1">2</ref> (for bilingual) and Figure <ref type="figure" coords="9,411.73,375.61,4.98,9.96" target="#fig_2">3</ref> (for multilingual). In Figures <ref type="figure" coords="9,124.68,387.61,7.81,9.96" target="#fig_0">1,</ref><ref type="figure" coords="9,135.24,387.61,3.90,9.96" target="#fig_1">2</ref>, and 3 the names are abbrevated to the letters and numbers of the full name in Table <ref type="table" coords="9,507.98,387.61,4.98,9.96" target="#tab_2">2</ref> describing the languages and query expansion approach used. For example, in Figure <ref type="figure" coords="9,458.90,399.49,4.98,9.96" target="#fig_1">2</ref> DEEN-CC corresponds to run Berk B DEEN CC p15 in Table <ref type="table" coords="9,318.12,411.49,3.90,9.96" target="#tab_2">2</ref>.</p><p>Since summary information on the scores for all submissions were not available at the time this paper was written, we have no idea of how our result stack up against other approaches for the same data. We can, however, compare the results for EVIs versus Thesaurus lookup.</p><p>Since our experiments were conducted using the same topics, database, translation tools, and basic combination approaches for both EVIs and Thesaurus-based expansion, we were hoping to find a clear benefit for one approach versus the other. Unfortunately, the results are not at all clear. While EVIs seem to best results when English is the target language, the opposite is true for German and Russian targets. As always our multilingual results are significantly lower than monolingual or bilingual results for a given source language, with the exception of German⇒Russian, which is the lowest MAP of any of the runs.</p><p>As the precision/recall graphs show, (Figures <ref type="figure" coords="9,313.33,543.01,7.81,9.96" target="#fig_0">1,</ref><ref type="figure" coords="9,326.05,543.01,3.90,9.96" target="#fig_1">2</ref>, and 3) there is very little difference in the curves for the EVI and Thesaurus-based expansion in a given source/target language set. Although we did not run significance testing of the results, we suspect that there is no statistically significant different between these runs.</p><p>It is worth noting that the approaches used in our submitted runs provided the best results when testing with 2006 data and topics. However, as we discovered after the 2007 qrels were made available, some simpler approaches worked as well or better than the more complex methods described above. For example a simplified version of English monolingual search using only the topic title and desc fields, and searching each of those in the topic and subject indexes, and merging the results using a pivot value of 0.15 obtained a MAP result of 0.2848, compared to the 0.2814 obtained in our best submitted monolingual run. Further simplification to individual index searches does not, however provide results approaching those of the pivot-merged results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,68.77,423.04,9.96;2,90.00,80.77,87.40,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Berkeley Domain Specific Monolingual Runs for English (top left), German (top right), and Russian (lower)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,90.00,68.77,423.19,9.96;3,90.00,80.77,99.52,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Berkeley Domain Specific Bilingual Runs -To English (top left), to German (top right) and to Russian (lower)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,90.00,68.77,423.16,9.96;5,90.00,80.77,163.27,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Berkeley Domain Specific Multilingual Runs -From English (top left), from German (top right), and from Russian (lower)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,136.92,289.81,329.12,181.28"><head>Table 1 :</head><label>1</label><figDesc>Cheshire II Indexes for Domain Specific 2007</figDesc><table coords="6,136.92,302.62,329.12,168.47"><row><cell>Name</cell><cell>Description</cell><cell>Content Tags</cell><cell>Used</cell></row><row><cell>docno</cell><cell>Document ID</cell><cell>DOCNO, DOCID</cell><cell>no</cell></row><row><cell>author</cell><cell>Author name</cell><cell>AUTHOR</cell><cell>no</cell></row><row><cell>title</cell><cell>Article Title</cell><cell>TITLE-DE, TITLE-EN,</cell><cell>no</cell></row><row><cell></cell><cell></cell><cell>TITLE-RU, TITLE</cell><cell></cell></row><row><cell>topic</cell><cell>All Content Words</cell><cell>DOC</cell><cell>yes</cell></row><row><cell>date</cell><cell>Date</cell><cell>DATE, PUBLICATION-YEAR</cell><cell>no</cell></row><row><cell>subject</cell><cell cols="2">Controlled Vocabulary CONTROLLED-TERM-EN</cell><cell>yes</cell></row><row><cell></cell><cell></cell><cell>CONTROLLED-TERM-DE,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CLASSIFICATION-TEXT-EN,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CLASSIFICATION-TEXT-DE,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CLASSIFICATION,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>KEYWORDS, KEYWORDS-RU,</cell><cell></cell></row><row><cell cols="2">geoname Geographic names</cell><cell>GEOGR-AREA, COUNTRY-CODE</cell><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,105.00,497.65,24.11,9.96"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,165.48,68.29,271.85,323.48"><head>Table 2 :</head><label>2</label><figDesc>Submitted Domain Specific Runs</figDesc><table coords="8,165.48,80.98,271.85,310.79"><row><cell>Run Name</cell><cell>Description</cell><cell>Exp.</cell><cell>MAP</cell></row><row><cell>Berk M DE CC p15</cell><cell>Monolingual German</cell><cell>EVI</cell><cell>0.3150</cell></row><row><cell>Berk M DE TH p7</cell><cell>Monolingual German</cell><cell cols="2">Thes 0.3199</cell></row><row><cell>Berk M EN CC p15</cell><cell>Monolingual English</cell><cell>EVI</cell><cell>0.2814</cell></row><row><cell>Berk M EN TH p7</cell><cell>Monolingual English</cell><cell cols="2">Thes 0.2733</cell></row><row><cell>Berk M RU CC p15</cell><cell>Monolingual Russian</cell><cell>EVI</cell><cell>0.1390</cell></row><row><cell>Berk M RU TH p7</cell><cell>Monolingual Russian</cell><cell cols="2">Thes 0.1401</cell></row><row><cell cols="2">Berk B DEEN CC p15 German⇒English</cell><cell>EVI</cell><cell>0.1096</cell></row><row><cell>Berk B DEEN TH p7</cell><cell>German⇒English</cell><cell cols="2">Thes 0.1043</cell></row><row><cell cols="2">Berk B DERU CC p15 German⇒Russian</cell><cell>EVI</cell><cell>0.0269</cell></row><row><cell>Berk B DERU TH p7</cell><cell>German⇒Russian</cell><cell cols="2">Thes 0.0285</cell></row><row><cell cols="2">Berk B ENDE CC p15 English⇒German</cell><cell>EVI</cell><cell>0.2412</cell></row><row><cell>Berk B ENDE TH p7</cell><cell>English⇒German</cell><cell cols="2">Thes 0.2514</cell></row><row><cell cols="2">Berk B ENRU CC p15 English⇒Russian</cell><cell>EVI</cell><cell>0.1348</cell></row><row><cell>Berk B ENRU TH p7</cell><cell>English⇒Russian</cell><cell cols="2">Thes 0.1341</cell></row><row><cell cols="2">Berk B RUDE CC p15 Russian⇒German</cell><cell>EVI</cell><cell>0.1520</cell></row><row><cell>Berk B RUDE TH p7</cell><cell>Russian⇒German</cell><cell cols="2">Thes 0.1501</cell></row><row><cell cols="2">Berk B RUEN CC p15 Russian⇒English</cell><cell>EVI</cell><cell>0.1757</cell></row><row><cell>Berk B RUEN TH p7</cell><cell>Russian⇒English</cell><cell cols="2">Thes 0.1701</cell></row><row><cell>BerkMUDEp15</cell><cell cols="2">Multiling. from German EVI</cell><cell>0.0468</cell></row><row><cell>BerkMUDETHp7</cell><cell cols="3">Multiling. from German Thes 0.0486</cell></row><row><cell>BerkMUENp15</cell><cell>Multiling. from English</cell><cell>EVI</cell><cell>0.0884</cell></row><row><cell>BerkMUENTHp7</cell><cell>Multiling. from English</cell><cell cols="2">Thes 0.0839</cell></row><row><cell>BerkMURUp15</cell><cell cols="2">Multiling. from Russian EVI</cell><cell>0.0414</cell></row><row><cell>BerkMURUTHp7</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Conclusions</head><p>We cannot say, overall, how effective query expansion by EVI or Thesaurus are relative to other approaches for this task. We can assume that there is very little difference in the effectiveness of the two methods, and that both seem to perform better than simple single-index "bag of words" searches of the collection contents.</p><p>We plan to conduct further runs to test whether modifications and simplifications, as well as combinations, of the EVI and Thesaurus-based approaches will provide can provide improved performance for the Domain Specific tasks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,110.52,253.93,402.63,9.96;10,110.52,265.93,350.53,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,250.83,253.93,262.32,9.96;10,110.52,265.93,169.07,9.96">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,288.96,265.93,92.95,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,285.13,402.59,9.96;10,110.52,297.01,402.36,9.96;10,110.52,309.01,81.01,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,285.16,285.13,227.96,9.96;10,110.52,297.01,205.17,9.96">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,342.36,297.01,165.22,9.96">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,328.21,402.51,9.96;10,110.52,340.09,402.54,9.96;10,110.52,352.09,402.65,9.96;10,110.52,364.09,122.53,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,376.02,328.21,137.01,9.96;10,110.52,340.09,105.63,9.96">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.80,340.09,277.26,9.96;10,110.52,352.09,185.49,9.96">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,383.29,402.73,9.96;10,110.52,395.17,402.45,9.96;10,110.52,407.17,295.69,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,374.76,383.29,138.49,9.96;10,110.52,395.17,110.59,9.96">Entry vocabulary -a technology to enhance digital search</title>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Buckland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,246.72,395.17,266.25,9.96;10,110.52,407.17,124.97,9.96">Proceedings of HLT2001, First International Conference on Human Language Technology</title>
		<meeting>HLT2001, First International Conference on Human Language Technology<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-03">March 2001</date>
			<biblScope unit="page" from="91" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,426.37,402.50,9.96;10,110.52,438.25,210.62,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,184.09,426.37,328.93,9.96;10,110.52,438.25,29.72,9.96">Classification clustering, probabilistic information retrieval, and the online catalog</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,148.68,438.25,75.36,9.96">Library Quarterly</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="173" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,457.45,402.74,9.96;10,110.52,469.45,333.61,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,178.57,457.45,330.45,9.96">Evaluation of advanced retrieval techniques in an experimental online catalog</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.52,469.45,246.46,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="53" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,488.65,402.42,9.96;10,110.52,500.53,402.32,9.96;10,110.52,512.53,340.24,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,181.69,488.65,176.90,9.96">Domain specific retrieval: Back to basics</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,379.68,488.65,133.27,9.96;10,110.52,500.53,402.32,9.96;10,110.52,512.53,115.14,9.96">Evaluation of Multilingual and Multi-modal Information Retrieval -Seventh Workshop of the Cross-Language Evaluation Forum, CLEF 2006, LNCS</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct coords="10,110.53,531.73,402.43,9.96;10,110.52,543.73,311.20,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,185.77,531.73,243.38,9.96">Linked relevance feedback for the imageclef photo task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,453.60,531.73,59.36,9.96;10,110.52,543.73,69.61,9.96">CLEF 2007 -Notebook Papers</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct coords="10,110.53,562.81,402.55,9.96;10,110.52,574.81,402.42,9.96;10,110.52,586.81,402.39,9.96;10,110.52,598.69,61.09,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,268.24,562.81,244.83,9.96;10,110.52,574.81,54.47,9.96">Component ranking and automatic query refinement for xml retrieval</title>
		<author>
			<persName coords=""><forename type="first">Yosi</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matan</forename><surname>Mandelbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,185.76,574.81,327.18,9.96;10,110.52,586.81,266.31,9.96">Advances in XML Information Retrieval: Third International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX2004</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3493</biblScope>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,617.89,402.50,9.96;10,110.52,629.89,402.33,9.96;10,110.52,641.77,402.49,9.96;10,110.52,653.77,52.81,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,312.52,617.89,200.51,9.96;10,110.52,629.89,265.55,9.96">Domain-specific CLIR of english, german and russian using fusion and subject metadata for query expansion</title>
		<author>
			<persName coords=""><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,395.40,629.89,117.45,9.96;10,110.52,641.77,87.82,9.96">Cross-Language Evaluation Forum: CLEF 2005</title>
		<title level="s" coord="10,323.93,641.77,189.08,9.96">Lecture Notes in Computer Science LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="226" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.53,672.97,402.50,9.96;10,110.52,684.97,328.34,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,283.72,672.97,157.06,9.96">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.84,672.97,62.19,9.96;10,110.52,684.97,181.54,9.96">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
