<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.68,95.86,280.56,12.64">Query Parsing Task for GeoCLEF2007 Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.72,116.99,48.49,9.02"><forename type="first">Zhisheng</forename><surname>Li</surname></persName>
							<email>zsli@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sci. &amp; Tech. of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.48,116.99,53.10,9.02"><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<email>chwang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
								<orgName type="institution">Sigma</orgName>
								<address>
									<addrLine>4F, Center, No.49, Zhichun Road</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.92,116.99,36.82,9.02"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
								<orgName type="institution">Sigma</orgName>
								<address>
									<addrLine>4F, Center, No.49, Zhichun Road</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.04,116.99,55.78,9.02"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
							<email>wyma@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
								<orgName type="institution">Sigma</orgName>
								<address>
									<addrLine>4F, Center, No.49, Zhichun Road</addrLine>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.68,95.86,280.56,12.64">Query Parsing Task for GeoCLEF2007 Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2BB4397E4E3F013272F478304267FAB0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Geographical Information Retrieval</term>
					<term>Query Parsing</term>
					<term>Task Design</term>
					<term>Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Geo-query parsing task is a sub-task in GeoCLEF2007 and it is run by Microsoft Research Asia. We have provided a query set of 800,000 real queries (in English) from MSN search. The proposed task requires that, based on the provided query set, the participants first identify the local queries and then analyze the different components for the local queries. We have provided a sample labeled query set of 100 queries for training. There are six valid submissions from six teams. We selected 500 queries to form our evaluation set. Under a rather strict evaluation criterion and metric, the Miracle team achieves the highest F1-score, 0.488, and the highest Recall too, 0.566, while the Ask team achieves the highest Precision, 0.625.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Geographical Information Retrieval (GIR) is becoming more and more important nowadays. GIR is more related to people's daily life than general search because people need to deal with locations all the time, such as dining, traveling and shopping. Many large Internet companies are paying more attention to GIR, such as Microsoft, Google, Yahoo. Among many problems in GIR, query location detection is one of the most important problems. This is the first step when GIR system tries to understand the intentions of the queries. Accurately and effectively detecting and analyzing the locations where search queries are truly about has huge potential impact on increasing search relevance.</p><p>A local query often contains some non-location words, too. In general, a local query is usually composed of three components, "what", "relation-type" and "where". The keywords in the "what" component indicate what the user wants to search. The "where" indicates the geographical area that the user wants to know. The "relation-type" indicates the relationship between "what" and "where". For example, for such a query "Restaurant in Beijing, China", "what" is "Restaurant", "where" is "Beijing, China", and "Relation-type" is "IN"; while for another query "Mountains in the south of United States", "what" is "Mountains", "where" is "United States", and "Relation-type" is "SOUTH-OF". How to extract these components from queries is one of the key problems for GIR. If the problem is well solved, the GIR system can handle the different components more efficiently and effectively. Therefore, we conducted such a task for GeoCLEF2007 to test the performance of the query tagging algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task design</head><p>Our goal in this query tagging task is to identify the local query and extract the corresponding three components described above. Moreover, we define three types according to the "what" terms, which are "Yellow page", "Map" and "Information". Here we restrict the local query as the query containing EXPLICIT locations.</p><p>In our query set, a common local query structure will be "what" + "geo-relation" + "where". The keywords in the "what" component indicate what users want to search; "where" indicates the geographic area users are interested in; "geo-relation" stands for the relationship between "what" and "where". There also exist non-local queries in our query set which also need to be recognized.</p><p>For example, for a local query "Restaurant in Beijing, China", "what" = "Restaurant", "where" = "Beijing, China", and "geo-relation" = "IN". For another query, "Mountains in the south of United States", "what" = "Mountains", "where" = "United States", and "geo-relation" = "SOUTH_OF".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks Description</head><p>1) Detect whether the query is a local query or not. A query is defined to be "local" if a query contains at least a "where" component. For example, "pizza in Seattle, WA" is a local query, while "Microsoft software" is a non-local query. For non-local queries, further processing is not needed. 2) If the query is local, extract the "where" component and output the corresponding latitude/longitude. For example, in the query "pizza in Seattle, WA", "Seattle, WA" will be extracted and lat/long value (47.59, -122.33) will be output. Sometimes terms in the "where" component are ambiguous. In this case, the participant should output the lat/long value with the highest confidence. A few queries contain multiple locations, for example, "bus lines from US to Canada". We try our best to avoid this kind of queries appearing in our query set. 3) Extract the "geo-relation" component from the local query and transform it into a pre-defined relation type.</p><p>A suggested relation type list is shown in Table <ref type="table" coords="2,300.08,201.47,3.77,9.02" target="#tab_0">1</ref>. If the relation type you find is not defined in Table <ref type="table" coords="2,516.08,201.47,3.77,9.02" target="#tab_0">1</ref>, you should categorize it into "UNDEFINED". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Set</head><p>We provided a query data set of 800,000 queries. The queries were selected from MSN search logs collected over fifteen days in Aug. 2006. The queries can be classified as four types according to whether they contain locations or geo-relations or not. Table <ref type="table" coords="3,181.52,325.19,4.98,9.02">2</ref> shows the number of four type queries in the data set. Here "relation" means the types listed in Table <ref type="table" coords="3,130.64,336.83,3.77,9.02" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Composition of the Data Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Has location terms</head><p>No location terms</p><p>Has geo-relation terms 50,000 20,000</p><p>No geo-relation terms 350,000 380,000</p><p>And we provided a sample labeled query set of 100 queries for participants. The format is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Format</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Data Set Format</head><p>The query set is provided in XML format. If a submission from a team does not contain all queries or all the fields, those absent queries or fields will be treated as errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>The contest is open to any party planning to attend GeoCLEF 2007. A person can participate in only one group. Multiple submissions are allowed before the deadline, but we only evaluated the last submissions.</p><p>The participants take the responsibility of obtaining any permission to use any algorithms/tools/data that are intellectual property of third party.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Set</head><p>To evaluate the performance of the submission, we choose a set of queries from the query set to form an evaluation set. However, if all the queries are chosen randomly, there will be several problems as follows. 1) There are some typos in the queries, e.g. "beuty"; 2) The query is ambiguous and difficult to understand. For example, "Cambridge", "daa files"; 3) Many geo-relations don't appear very often, e.g. "NORTH_EAST_TO", "NORTH_OF", so it is difficult to include this kind of cases in the evaluation set if the queries are chosen randomly. So we choose the following steps to construct the final evaluation set to cover as many different types as possible.</p><p>1) Choose 800 queries randomly from the query set.</p><p>2) Remove the typos and the ambiguous queries from the 800 ones manually.</p><p>3) Select the queries with special geo-relations from the remainder queries in the query set manually and add them to the evaluation set. 4) Select 500 queries for the final evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distribution</head><p>Figure <ref type="figure" coords="4,100.76,515.51,4.98,9.02">1</ref> shows the distribution of the evaluation set. The three types of queries, including map, information and yellow page, consist of the local queries which occupy 61.4%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Distribution of the evaluation set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Labeling approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Labeling tool</head><p>To accelerate the labeling efficiency, we design a labeling tool. With its help we can easily identify each part of the query. Figure <ref type="figure" coords="5,127.16,112.79,4.98,9.02" target="#fig_0">2</ref> shows the interface of the tool. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Label process</head><p>Two experts identify the six fields of these queries according to the task description including the &lt;LOCAL&gt;, &lt;WHAT&gt;, &lt;WHERE&gt;, &lt;GEO-RELATION&gt;, &lt;WHAT-TYPE&gt;, &lt;LAT-LONG&gt; of the location in the query. For the &lt;LOCAL&gt; field, we label it as "local" only if the query contains explicit locations. For the &lt;WHAT&gt; field, we keep all the terms in the query after extracting the &lt;GEO-RELATION&gt; and the &lt;WHERE&gt; fields. For example, the &lt;WHAT&gt; field of the query "ambassador suite hotel in Atlanta" is "ambassador suite hotel". For &lt;WHAT-TYPE&gt; field, we define three types: Map type, Yellow page type, and Information type, which have been described above.</p><p>For the &lt;WHERE&gt; field, if the locations are ambiguous, we choose the location with the highest confidence score. The format is "location name + its upper location name", e.g. "Atlanta, United States". Meanwhile, we label the latitude and longitude of the location point. This value is only for reference here, because the lat-long values from different participants may vary greatly especially for the "big" locations, e.g. "Asia", "Canada".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Method</head><p>To evaluate the performance of the query tagging task for the participants, we do the following three steps. First we pre-process the submissions of the participants to solve problems such as the format errors or data absence. Then we choose the subset from submissions with the same query number as in the evaluation set. Here we don't use automatic checking since the format of the &lt;WHERE&gt; field is not unique. Three experts checked all the submissions independently and reach a final decision through discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Criterion</head><p>We consider the following criterions in the evaluation process:</p><p>1) the &lt;LOCAL&gt; field should be the same as the answer;</p><p>2) the terms in the &lt;WHAT&gt; field should be the same as the answer;</p><p>3) the &lt;WHAT-TYPE&gt; and &lt;GEO-RELATION&gt; should be the same as the answer;</p><p>4) the &lt;WHERE&gt; field should contain the locations in the original query, no matter its upper location is correct or not; 5) we ignore the &lt;LAT-LONG&gt; field in this evaluation;</p><p>If one record in the submission meets the entire above criterions, it is correct, otherwise wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Metric</head><p>We evaluate the submissions based on several evaluation metrics, including Precision, Recall, and F1-score. The participants do not know which queries will be used for evaluation. Here are the set of measures we use to evaluate results submitted by the participants: </p><formula xml:id="formula_0" coords="6,223.28,178.61,164.39,81.99">Correct_tagged_query_num Precision = all_tagged_query_num Correct_tagged_query_num Recall = all_local_query_num 2* Precision* Recall F1-score = Precision+ Recall</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We only give the results for the local query. We can see that the Miracle team achieves the highest F1-score, 0.488, and the highest Recall too, 0.566, while the Ask team achieves the highest Precision, 0.625. Table <ref type="table" coords="6,465.32,307.91,4.98,9.02" target="#tab_3">3</ref> shows the results of all participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>In total, six teams participated in this query tagging task. We find several problems (technical) during the evaluations.</p><p>1) Fail to classify the local queries. Some local queries are classified as non-local by a few teams, so the recall for the local queries drops significantly. 2) The &lt;WHAT&gt; field is not complete. Some terms in the query are missing. For example, "apartments to rent in Cyprus", the &lt;WHAT&gt; field should be "apartments to rent", but some participants just output "apartments". And "homer Alaska real estate", &lt;WHAT&gt; field should be "homer real estate", not "homer" or "real estate". 3) Fail to classify the &lt;WHAT-TYPE&gt;. Especially for the "Yellow Page" and "Information", a few of teams classify the "Yellow Page" queries as "Information". Frankly speaking, sometimes it's really hard to differentiate "Yellow Page" from "Information", because of the ambiguity. For example, "Kansas state government", if you want to know about the information about state government, it can be classified as "Information", if you want to find the location, it can be classified as "Yellow Page". Moreover, some teams don't output the &lt;WHAT-TYPE&gt; for the local queries. Though their extraction precision for other fields is quite high, we have no choice but to label them as wrong cases. 4) Fail to identify &lt;GEO-RELATION&gt; field correctly. Most of the teams can recognize the geo-relation "IN", but for the others, like "SOUTH_OF", "SOUTH_WEST_OF", "NORTH_WEST_OF", few teams can identify them correctly. For example, "bank west of nevada", the &lt;GEO-RELATION&gt; should be "WEST-OF".</p><p>5) Fail to find the correct &lt;WHERE&gt;. A few of teams fail to extract the locations from the queries and label them as non-local queries. We guess the reason is that their gazetteer is not big enough. Moreover, some teams fail to disambiguate the locations and don't output the locations with the highest confidence scores. 6) Although we don't consider the &lt;LAT-LONG&gt; field this time, we find some participants don't output &lt;LAT-LONG&gt; at all. Maybe they don't have such information.</p><p>Most teams have employed a sophisticated gazetteer for location extraction, containing millions of geographical references. Their approaches for analyzing and classifying queries were mainly based on pre-defined rules. The system from the Miracle team, which achieved the best F1, was composed of three modules, namely geo-entity identifier, query analyzer and a two-level multi-classifier. Other systems followed similar designs. Generally speaking, the performance for most teams is not high. We list several possible reasons as follows:</p><p>1) New task. This query tagging task is totally new for the participants. And the time left is a bit short from the very beginning, just 2 months. 2) Critical standard. Our criterions to judge the results are quite critical. Some teams are good at the extraction but fail to identify the &lt;WHAT-TYPE&gt;. The overall performance is thus affected. 3) Queries are ambiguous. We find three kinds of ambiguity here.</p><p>a. Local/Non-Local Ambiguity: Some queries, like "airport", "space needle", are defined as non-local here because they don't contain explicit locations. b. Yellow Page/Information Ambiguity: Due to the lack of background knowledge, it's hard to say some queries, like "Atlanta medical", are Yellow Page or Information. But we defined it as Yellow Page in this task. c. Location Ambiguity: Some locations are ambiguous, like "Washington", "Columbus". We just choose the locations with the highest confidence based on our algorithm. 4) Culture understanding problem. When we were labeling the queries, we found that we lacked the necessary background of culture of western countries. In such situation, the labeling process may still contain some errors, even if we tried our best to avoid them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this report, we summarized the configuration and the results of the new query parsing task in GeoCLEF2007. The main purpose for organizing this task is to gather researchers who have similar interests. We first discussed the motivation of this task. Then we described the task design and evaluation methods. We also reported the evaluation results for all the participants. This is the first time for us to organize such a task and we have learned a lot of lessons from it. In general, the task was conducted smoothly but it can be improved in many aspects, such as evaluation measure and data set preparation. We also plan to include more challenging parsing tasks and multilingual queries in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,208.52,412.97,195.01,8.96;5,114.80,126.00,383.64,280.56"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Interface of the Query Labeling tool</figDesc><graphic coords="5,114.80,126.00,383.64,280.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,233.84,267.41,144.06,8.96"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Precision, Recall and F1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,154.52,228.77,252.86,471.80"><head>Table 1 . Relation-Type</head><label>1</label><figDesc>Extract the "what" component from the local query and categorize it into one of three predefined types, which are listed below: a. Map type, users are looking for natural points of interests, like river, beach, mountain, monuments, etc. b. Yellow page type, users are looking for businesses or organizations, like hotels, restaurants, hospitals, etc. c. Information type, users are looking for text information, like news, articles, blogs, etc.</figDesc><table coords="2,154.52,245.15,252.86,455.42"><row><cell>Example query</cell><cell>Geo-relation</cell></row><row><cell>Beijing</cell><cell>NONE</cell></row><row><cell>in Beijing</cell><cell>IN</cell></row><row><cell>on the Long Island</cell><cell>ON</cell></row><row><cell>of Beijing</cell><cell>OF</cell></row><row><cell>near Beijing</cell><cell>NEAR</cell></row><row><cell>next to Beijing</cell><cell></cell></row><row><cell>in or around Beijing</cell><cell>IN_NEAR</cell></row><row><cell>in and around Beijing</cell><cell></cell></row><row><cell>along the Rhine</cell><cell>ALONG</cell></row><row><cell>at Beijing</cell><cell>AT</cell></row><row><cell>from Beijing</cell><cell>FROM</cell></row><row><cell>to Beijing</cell><cell>TO</cell></row><row><cell>within d miles of Beijing</cell><cell>DISTANCE</cell></row><row><cell>north of Beijing</cell><cell>NORTH_OF</cell></row><row><cell>in the north of Beijing</cell><cell></cell></row><row><cell>south of Beijing</cell><cell>SOUTH_OF</cell></row><row><cell>in the south of Beijing</cell><cell></cell></row><row><cell>east of Beijing</cell><cell>EAST_OF</cell></row><row><cell>in the east of Beijing</cell><cell></cell></row><row><cell>west of Beijing</cell><cell>WEST_OF</cell></row><row><cell>in the west of Beijing</cell><cell></cell></row><row><cell>northeast of Beijing</cell><cell>NORTH_EAST_OF</cell></row><row><cell>in the northeast of Beijing</cell><cell></cell></row><row><cell>northwest of Beijing</cell><cell>NORTH_WEST_OF</cell></row><row><cell>in the northwest of Beijing</cell><cell></cell></row><row><cell>southeast of Beijing</cell><cell>SOUTH_EAST_OF</cell></row><row><cell>in the southeast of Beijing</cell><cell></cell></row><row><cell>southwest of Beijing</cell><cell>SOUTH_WEST_OF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,178.28,335.21,255.32,118.76"><head>Table 3 . Results of all Participants (only for the local query)</head><label>3</label><figDesc></figDesc><table coords="6,197.96,351.47,194.22,102.50"><row><cell>Team</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Ask</cell><cell>0.625</cell><cell>0.258</cell><cell>0.365</cell></row><row><cell>Csusm</cell><cell>0.201</cell><cell>0.197</cell><cell>0.199</cell></row><row><cell>Linguit</cell><cell>0.112</cell><cell>0.038</cell><cell>0.057</cell></row><row><cell>Miracle</cell><cell>0.428</cell><cell>0.566</cell><cell>0.488</cell></row><row><cell>Talp</cell><cell>0.222</cell><cell>0.249</cell><cell>0.235</cell></row><row><cell>Xldb</cell><cell>0.096</cell><cell>0.08</cell><cell>0.088</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
