<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.68,86.56,418.10,14.42;1,165.36,104.92,264.72,14.42;20,260.47,108.52,180.38,7.85;20,105.24,128.34,50.62,9.02;21,148.44,372.15,298.74,8.10;21,84.30,392.16,50.62,9.02">GeoCLEF 2007: the CLEF 2007 Cross-Language Geographic Information Retrieval Track Overview</title>
				<funder>
					<orgName type="full">Portuguese Government</orgName>
				</funder>
				<funder ref="#_CXFeECc">
					<orgName type="full">European Union (FEDER and FSE)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.52,141.09,60.88,8.74"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
							<email>mandl@uni-hildesheim.de</email>
							<affiliation key="aff2">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">GERMANY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.63,141.09,48.10,8.74"><forename type="first">Fredric</forename><surname>Gey</surname></persName>
							<email>gey@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.93,141.09,75.51,8.74"><forename type="first">Giorgio</forename><forename type="middle">Di</forename><surname>Nunzio</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.67,141.09,50.89,8.74"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.64,152.61,46.39,8.74"><forename type="first">Ray</forename><surname>Larson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.25,152.61,65.80,8.74"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>m.sanderson@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.34,152.61,53.11,8.74"><forename type="first">Diana</forename><surname>Santos</surname></persName>
							<email>diana.santos@sintef.no</email>
							<affiliation key="aff3">
								<orgName type="department">Linguateca</orgName>
								<orgName type="institution">SINTEF ICT</orgName>
								<address>
									<country key="NO">NORWAY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.82,164.13,97.47,8.74"><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">GERMANY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.54,164.13,36.98,8.74"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="21,86.63,413.66,38.39,6.27"><surname>Cheshire</surname></persName>
						</author>
						<author>
							<persName coords="22,406.01,192.99,51.31,8.74"><forename type="first">Julia</forename><surname>Jürgens</surname></persName>
						</author>
						<author>
							<persName coords="22,464.94,192.99,55.73,8.74"><forename type="first">Theresa</forename><surname>Märtl</surname></persName>
						</author>
						<author>
							<persName coords="22,70.62,204.51,55.84,8.74"><forename type="first">Ben</forename><surname>Heuwing</surname></persName>
						</author>
						<author>
							<persName coords="22,136.77,204.51,55.21,8.74"><forename type="first">Frauke</forename><surname>Zurek</surname></persName>
						</author>
						<author>
							<persName coords="22,201.85,204.51,55.23,8.74"><forename type="first">Robert</forename><surname>Stoldt</surname></persName>
						</author>
						<author>
							<persName coords="22,338.05,204.51,55.46,8.74"><forename type="first">Jens</forename><surname>Plattfaut</surname></persName>
						</author>
						<author>
							<persName coords="22,422.49,204.51,71.01,8.74"><forename type="first">Rafael</forename><surname>Hellmann</surname></persName>
						</author>
						<author>
							<persName coords="22,142.40,239.01,48.95,8.74"><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
						</author>
						<author>
							<persName coords="22,198.86,239.01,98.21,8.74"><roleName>Luís</roleName><forename type="first">Miguel</forename><surname>Luís Costa</surname></persName>
						</author>
						<author>
							<persName coords="22,299.82,239.01,60.24,8.74"><forename type="first">Susana</forename><surname>Cabral</surname></persName>
						</author>
						<author>
							<persName coords="22,362.82,239.01,88.13,8.74"><forename type="first">Maria</forename><forename type="middle">Cláudia</forename><surname>Inácio</surname></persName>
						</author>
						<author>
							<persName coords="22,453.76,239.01,27.26,8.74;22,501.05,239.01,23.88,8.74"><forename type="first">Diana</forename><surname>Freitas</surname></persName>
						</author>
						<author>
							<persName coords="22,70.62,250.47,25.03,8.74"><surname>Santos</surname></persName>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">CLEF2007.HILDESHEIM.HIMODENE2NA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory" key="lab1">CLEF2007.HAGEN.FUHTD6DE X</orgName>
								<orgName type="laboratory" key="lab2">HILDESHEIM.HIMODEBASE</orgName>
								<address>
									<postBox>X 10</postBox>
									<postCode>2415, GC-MONO-DE-CLEF2007</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="laboratory">CLEF2007.HAGEN.FUHTD3DE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="laboratory">CLEF2007.HILDESHEIM.HIMODENE2</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="laboratory">CLEF2007.HAGEN.FUHTD2DE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="laboratory">CLEF2007.HILDESHEIM.HIMODENE3</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="laboratory">CLEF2007.CHESHIRE.BERKBIESENBASE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="department">.DEPOK.UIBITDGPGEOFB</orgName>
								<address>
									<postCode>X2EN-CLEF2007</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="department">.DEPOK.UIBITDGP</orgName>
								<address>
									<postCode>X2EN-CLEF2007</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="laboratory">CLEF2007.HAGEN.FUHTD2EN</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff16">
								<orgName type="laboratory">CLEF2007.HAGEN.FUHTD3EN</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff17">
								<orgName type="department">CHESHIRE</orgName>
								<orgName type="institution">BILI-X2PT-CLEF2007.CHESHIRE</orgName>
								<address>
									<addrLine>BERKBIESPTBASE, 10.2415</addrLine>
									<postCode>GC-BILI-X2PT</postCode>
									<country>CLEF2007</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff18">
								<orgName type="institution">BERKBIDEPTBASE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff19">
								<orgName type="institution">University of Hildesheim. The</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.68,86.56,418.10,14.42;1,165.36,104.92,264.72,14.42;20,260.47,108.52,180.38,7.85;20,105.24,128.34,50.62,9.02;21,148.44,372.15,298.74,8.10;21,84.30,392.16,50.62,9.02">GeoCLEF 2007: the CLEF 2007 Cross-Language Geographic Information Retrieval Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE4B4D1A573038D95C074755283689B5</idno>
					<idno type="DOI">Groups10.2415/GC-</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Performance, Experimentation Multilingual Information Retrieval, Geographic Information Retrieval, Evaluation Standards 10.2415</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GeoCLEF ran as a regular track for the second time within the Cross Language Evaluation Forum (CLEF) 2007. The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR): retrieval for topics with a geographic specification. GeoCLEF 2007 consisted of two sub tasks. A search task ran for the third time and a query classification task was organized for the first. For the GeoCLEF 2007 search task, twenty-five search topics were defined by the organizing groups for searching English, German, Portuguese and Spanish document collections. Topics were translated into English, German and Spanish. Several topics in 2007 were geographically challenging. Thirteen groups submitted 108 runs. The groups used a variety of approaches. For the classification task, a query log from a search engine was provided and the groups needed to identify the queries with a geographic scope and the geographic components within the local queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>GeoCLEF <ref type="foot" coords="1,111.72,679.69,3.24,5.65" target="#foot_0">1</ref> is the first track in an evaluation campaign dedicated to evaluating geographic information retrieval systems. The aim of GeoCLEF is to provide the necessary framework in which to evaluate GIR systems for search tasks involving both spatial and multilingual aspects. Participants are offered a TREC style ad hoc retrieval task based on existing CLEF newspaper collections. GeoCLEF 2005 was run as a pilot track and in 2006, GeoCELF was a regular CLEF track. GeoCLEF has continued to evaluate retrieval of documents with an emphasis on geographic information retrieval from text. Geographic search requires the combination of spatial and content based relevance into one result. Many research and evaluation issues surrounding geographic monoand bilingual search have been addressed in GeoCLEF.</p><p>GeoCLEF was a collaborative effort by research groups at the University of California, Berkeley (USA) , the University of Sheffield (UK), the University of Hildesheim (Germany) and Linguateca (Norway and Portugal). Thirteen research groups (17 in 2006) from a variety of backgrounds and nationalities submitted 108 runs (149 in 2005) to GeoCLEF.</p><p>For 2007, Portuguese, German and English were available as document and topic languages. There were two Geographic Information Retrieval tasks: monolingual (English to English, German to German and Portuguese to Portuguese) and bilingual (language X to language Y, where X or Y was one of English, German or Portuguese). In the three editions of GeoCLEF so far, 75 topics with relevance assessments have been developed. Thus, GeoCLEF developed a standard evaluation collection which supports long-term research. Geographical Information Retrieval (GIR) concerns the retrieval of information involving some kind of spatial awareness. Many documents contain some kind of spatial reference which may be important for IR. For example, to retrieve, rank and visualize search results based on a spatial dimension (e.g. "find me news stories about bush fires near Sidney").</p><p>Many challenges of geographic IR involve geographical references (geo-references). Documents contain georeferences expressed in multiple languages which may or may not be the same as the query language. For example, the city Cape Town (English) is also Kapstadt (German), Cidade do Cabo in Portuguese and Ciudad del Cabo (Spanish). Queries with names may require an additional translation step to enable successful retrieval. Depending on the language and the culture, translation may not helpful in some cases. For example, the word new within New York is often translated in Spanish (Nueva York) and Portuguese (Nova Iorque), but never in German. On some occasions, names may be changed and a recent modification may not be well reflected within a foreign collection. E.g. there were still references to the German city Karl-Marx-Stadt in Spain after it had been renamed to Chemnitz in 1990. Geographical references are often ambiguous (e.g. there is a St. Petersburg also in Florida and Pennsylvania in the United States).</p><p>The query parsing (and classification) task was offered for the first time at GeoCLEF 2007. It is dedicated to identify geographic queries within a log from the msn search engine. This task has been organized by Xie Xing from Microsoft Research Asia. A log of 800,000 real queries was provided. Out of these, 100 were labeled as training data and 500 were assessed as test data. The task required participants to find the geographic entity, the relation type and the non geographic content restrictions. The task attracted six participating groups. The task design, the data, participation and evaluation results are discussed in a separate overview paper <ref type="bibr" coords="2,460.77,591.93,55.55,8.74" target="#b5">[Li et al. 2007</ref>].</p><p>The task is of high practical relevance to GeoCLEF and the real log data is of great value for research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GeoCLEF 2007 Search Task</head><p>Search is the main task of GeoCLEF. The following sections describe the test design adopted by GeoCLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Collections used in GeoCLEF 2007</head><p>The document collections for this year's GeoCLEF experiments consists of newspaper and newswire stories from the years 1994 and 1995 used in previous CLEF ad-hoc evaluations <ref type="bibr" coords="3,348.20,195.51,99.80,8.74">[Braschler &amp; Peters 2004</ref>]. The Portuguese, English and German collections contain stories covering international and national news events, therefore representing a wide variety of geographical regions and places. In all collections, the documents have a common structure: newspaper-specific information like date, page, issue, special filing numbers and usually one or more titles, a byline and the actual text. The document collections were not geographically tagged and contained no semantic location-specific information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating Search Topics</head><p>A total of 25 topics were generated for this year's GeoCLEF (GC 51 -GC75). Topic creation was shared among the three organizing groups, who all utilized the DIRECT System provided by the University of Padua. A search utility for the collections was provided within DIRECT to facilitate the interactive exploration of potential topics. Each group created initial versions of nine proposed topics in their language, with subsequent translation into English. Topics are meant to express a natural information need which a user of the collection might have. These candidates were subsequently checked for relevant documents in the other collections. In many cases, topics needed to be refined. For example, the topic candidate honorary doctorate degrees at Scottish universities was expanded to topic GC53 scientific research at Scottish universities due to an initial lack of relevant documents in the German and Portuguese collections. Relevant documents were marked within the DIRECT system. After intensive discussion, a decision was made about the final set of 25 topics. Finally, all missing topics were translated into Portuguese and German and all translations were checked. The following section will discuss the creation of topics with spatial parameters for the track.</p><p>The organizers continued the efforts of GeoCELF 2006 aimed at creating a geographically challenging topic set. This means that explicit geographic knowledge should be necessary in order for the participants to successfully retrieve relevant documents. Keyword-based approaches should not be favored by the topics. While many geographic searches may be well served by keyword approaches, others require a profound geographic reasoning. We speculate that for a realistic topic set where these difficulties might be less common, most systems could perform better.</p><p>In order to achieve that, several difficulties were explicitly included into the topics of GeoCLEF 2006 and 2007:</p><p>• ambiguity (St. Pauls Cathedral, exists in London and São Paulo)</p><p>• vague geographic regions (Near East)</p><p>• geographical relations beyond IN (near Russian cities, along Mediterranean Coast)</p><p>• cross-lingual issues (Greater Lisbon , Portuguese: Grande Lisboa , German: Großraum Lissabon)</p><p>• granularity below the country level (French speaking part of Switzerland, Northern Italy)</p><p>• complex region shapes (along the rivers Danube and Rhine)</p><p>However, it was difficult to develop topics which fulfilled all criteria. For example, local events which allow queries on a level of granularity below the country often do not lead to newspaper articles outside the national press. This makes the development of cross-lingual topics difficult.</p><p>For English topic generation, topics were initially generated by Mark Sanderson and tested on the DIRECT system. Additional consultation was conducted with other members of the GeoCLEF team to determine if the topics had at least some relevant documents in the German and Portuguese collections. Those found to have few such documents were altered in order ensure that at least some relevant documents existed for each topic.</p><p>The German group at Hildesheim started with brain storming on interesting geographical notions. Challenging geographic notions below the country granularity were procured. We came up with German speaking part of Switzerland, which is a vaguely defined region. A check in the collection showed that there were sport events, but not enough to specify a sport discipline. Another challenge was introduced with Nagorno-Karabakh which has many spelling variants.</p><p>The Portuguese topics were chosen in a way similar to the one suggested for the choice of adhoc topics in previous years [cf. <ref type="bibr" coords="4,150.32,353.49,92.16,8.74">Santos &amp; Rocha 2005]</ref>. The tripartite division among international, European and national, however, was reduced to national vs. international because we did not consider European as a relevant category (given that neither Portuguese nor English language newspaper collections used in CLEF are totally based in Europe): so, we chose some culturally-bound topics (Senna, crime in Grande Lisboa), some purely international or global (sharks and floods) and some related to specific regions (because of the geographic relevance to GeoCLEF).</p><p>In all cases, but especially for those focusing on a particular region (inside or outside the national borders covered by any newspaper collection), we tried to come up with a sensible user model: either a prospective tourist (St. Paul's or Northern Italy) or a cub reporter (Myanmar human rights violation or casualties on the Hymalaia). In some cases, we managed to create topics whose general relevance could be either, although naturally the choices would be different for the different kind of users --consider the case of navigation in the Portuguese islands, both relevant for a tourist and for a journalist discussing the subject.</p><p>We were also intent on trying some specifically known geographically ambiguous topics, such as St. Paul's or topics where the geographical names were ambiguous with non geographic concepts, such as Madeira (means wood in Portuguese and can also mean a kind of wine).</p><p>All the topics were then tried out in the CHAVE collection, encoded in CQP <ref type="bibr" coords="4,382.04,560.49,46.33,8.74" target="#b10">[Evert 2005</ref>] and available for Web search through the AC/DC project <ref type="bibr" coords="4,211.10,571.95,83.21,8.74" target="#b11">[Santos &amp; Bick 2000</ref>] at http://www.linguateca.pt/ACDC/ in order to estimate the number of possible hits. In general, there were very few hits for all topics, as can be appreciated by the number of relevant documents per topic found in the Portuguese pool (see Table/figure X??).</p><p>The translation of the topics leads to new challenges. One of the English topics about the Scottish town, St. Andrews, was judged to be challenging as it was more ambiguous than in English, because Santo André also denotes a village in Portugal and a city in Brazil. So this is a case where depending on the language the kind of results expected is different. While we are not defending a user model where this particular case would be relevant, we are showing that a mere topic translation (as might be effected by a crosslingual system) would not be enough if one were interested in the Scottish St. Andrews alone. Another interesting remark is the use of the word "continent", which is very much context dependent and again therefore cannot be translated simply from "continent" to "continente", because depending on your spatial basis the continent is different. Again this requires some clever processing and/or processing for the translation.</p><p>Finally, it appears that perto de X (near X, or close to X) carries in Portuguese the presupposition that X is not included, and this made us consider that we would have translated better "airports near to London" by "que servem Londres" (i.e., that are used to reach London). (Although we also used the phrase aeroprtos londrinos which may also include airports inside London). On the other hand, airplanes clashes close to Russian cities seemed more naturally translated by "na proximidade" and not included. We used perto for both, but this might have been a translation weakness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Format of Topic Description</head><p>The format of GeoCLEF 2007 differed slightly from that of 2006 as no markup of geographic entities in the topics was provided. Systems were expected to reveal that information to themselves from the topic. Two examples of full topics are shown in Figure <ref type="figure" coords="5,246.71,186.69,3.76,8.74" target="#fig_0">1</ref>  As can be seen, after the brief descriptions within the title and description tags, the narrative tag contains detailed description of the geographic detail sought and the relevance criteria. In some topics, lists of relevant regions are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Several kinds of geographical topics</head><p>A tentative classification for geographical topics was suggested at GIR 2006 and applied at GeoCLEF2006 <ref type="bibr" coords="5,501.35,441.51,20.08,8.74;5,70.62,453.03,39.41,8.74" target="#b4">[Gey et al. 2007</ref> relations between events which require their precise localization (was it the same river that flooded last year and in which killings occurred in the XVth century?)</p><p>This year we kept topics of both kinds 1 and 2 as last year. The major innovation and diversity introduced in GeoCLEF 2007 were more complicated geographic restriction than at previous GeoCLEF editions. The following three difficulties were introduced: 1 by specifying complex (multiply defined) geographic relations: East Coast of Scotland; Europe excluding the Alps, main roads north of Perth, Mediterranean coast, Portuguese islands, and "the region between the UK and the Continent";</p><p>2 by insisting on as politically defined regions, both smaller than countries, such as French speaking part of Switzerland, the Bosphorus, Northern Italy, Grande Lisboa, or larger than countries: East European countries, Africa and north western Europe; 3 by having finer geographic subjects, such as lakes, airports, F1 circuits, and even one cathedral as place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Approaches to Geographic Information Retrieval</head><p>The participants used a wide variety of approaches to the GeoCLEF tasks, ranging from basic IR approaches (with no attempts at spatial or geographic reasoning or indexing) to deep natural language processing (NLP) processing to extract place and topological clues from the texts and queries. Specific techniques used included:</p><p>• Ad-hoc techniques (weighting, probabilistic retrieval, language model, blind relevance feedback )</p><p>• Semantic analysis (annotation and inference)</p><p>• Geographic knowledge bases (Gazetteers, thesauri, ontologies) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Relevance assessment</head><p>English assessment was shared by Berkeley and Sheffield Universities. German assessment was done by the University of Hildesheim and Portuguese assessment by Linguateca. The DIRECT System was utilized for assessment. The system provided by the University of Padua allowed the automatic submission of runs by participating groups and supported assembling the GeoCLEF assessment pools by language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">English relevance assessment</head><p>English relevance assessment was conducted primarily by a group of ten paid volunteers from the University of Sheffield, who were paid a small sum of money for each topic assessed. The English document pool extracted from 53 monolingual and 13 bilingual (language X to) English runs consisted of 15,637 documents to be reviewed and judged by our 13 assessors or about 1,200 documents per assessor. The box plot of figure <ref type="figure" coords="6,164.70,690.45,5.01,8.74" target="#fig_1">2</ref> shows the distribution of different types of documents across the topics of the English pool. In particular, the upper box shows the distribution of the number of pooled documents across the topics; as it can be noted, the distribution is a little bit asymmetric towards topics with a higher number of pooled documents and does not present outliers. The middle box shows the distribution of the number of not relevant documents across the topics; as it can be noted, the distribution is a little bit asymmetric towards topics with a lower number of not relevant documents and does not present outliers. Finally, the lower box shows the distribution of the number of relevant documents across the topics; as it can be noted, the distribution is almost symmetric; with a median number of relevant documents around 20 per topic, but it present some outliers, which are topics with a large number of relevant documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">German relevance assessment</head><p>While judging relevance was generally easier for the short news agency articles of SDA with their headlines, keywords and restriction to one issue, Spiegel articles took rather long to judge, because of their length and essay-like stories often covering multiple events etc. without a specific narrow focus. Many borderline cases for relevance resulted from uncertainties about how broad/narrow a concept term should be interpreted and how explicit the concept must be stated in the document. One topic required systems to find documents which report shark attacks. Documents telling the reader that a certain area is "full of sharks" were not judged as relevant.</p><p>For other topics, implicit information in the document was used for the decision. For example, the topic sport events in German speaking Switzerland led to documents where the place of a soccer game was not mentioned, but the result was included in a standardized form which indicates that the game was played in the first city mentioned (e.g. Lausanne -Genf 0:2, has most usually been played in Lausanne). It was also assumed that documents which report that hikers are missing in the Himalayas are relevant for the topic casualties in the Himalayas. Many documents are at first identified as borderline cases and need to be discussed further. One topic requested topics on travel delays at London airports. One document mentioned that air travel had been delayed and some flight had to be directed to Gatwick. Because a delay at Gatwick is not explicitly mentioned, the document was regarded as not relevant.</p><p>The box plot of figure <ref type="figure" coords="8,164.28,130.71,5.01,8.74" target="#fig_2">3</ref> shows the distribution of different types of documents across the topics of the German pool. As it can be noted the distribution of the pooled documents is almost symmetrical with no outlier; on the other hand, the distribution of not relevant documents is asymmetrical with a tail towards topics with a lower number of not relevant documents and does not present outliers; finally, also the distribution of the relevant documents is asymmetrical but towards topics with a greater number of relevant documents and presents outliers, which are topics with a great number of relevant documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Portuguese Relevance Assessment</head><p>In addition to the problem (already reported before) that some if the news articles included in the CHAVE collection are in fact a list of "last news" which concern several different subjects (and have therefore to be read in their entirety, making it especially tiresome), we had some general problems assessing topics, which we illustrate here in detail for the "free elections in Africa" subject:</p><p>What is part of an election (or presupposed by it)? In other words, which parts are necessary or sufficient to consider that a text talks about elections: campaign, direct results, who were the winners, "tomada de posse", speeches when receiving the power, cabinet constitution, balance after one month, after more time...</p><p>In fact, how far in time is information relevant? For example, does mention to the murder of the first democratically elected president in Ruanda qualify as text about free elections in Africa? And if elections took place and were subsequently annulated as in Argelia, do they count as elections or not? Also, how much indirectly conveyed information can be considered relevant? A text about the return of Portuguese citizens to Portugal after the (free) South African elections is about free elections in South Africa?</p><p>And what to do if in the text no mention is made to whether the elections were free or not? Are we to assume anything? As in the case of a text about Uganda mentioning "voltou à Presidência no fim de 1980, pela via eleitoral" (X came back to presidency through the electoral path). Are either our knowledge or our opinions going to play a role on the relevance assessment, or we are supposed to just look at the document and not bring our own bias?</p><p>Finally, how much difference of opinions is relevant to a topic? Consider the following piece of news "Savimbi considera ilegais as eleições consideradas livres e justas pela ONU..." (Savimbi considers illegal the elections considered free and just by UN). Are we to stand with UN or with Savimbi, as far as the elections in Angola are concerned? (In our opinion, this text is very relevant to the subject, anyway, since it mentions, and discusses, precisely the issue of "free elections in an African country".)</p><p>Due to this (acknowledged) difficulty of assessing relevance for some topics, it would have been beneficial to have a pool of judges assessing the same documents and produce a relevance cline. Although this is currently not possible with the DIRECT system, it might make sense in the future, especially for more evaluative topics that involve complex issues. The box plot of figure <ref type="figure" coords="9,174.07,398.31,5.01,8.74" target="#fig_3">4</ref> shows the distribution of different types of documents across the topics of the Portuguese pool. As it can be noted the distribution of the pooled documents is a little bit asymmetrical towards topics with a lower number of pooled document and presents both upper and lower outliers, i.e. topics with many or few pooled documents; on the other hand, the distribution of not relevant documents is almost symmetrical with an outlier, which is a topic with few not relevant documents; finally, also the distribution of the relevant documents is asymmetrical towards topics with a greater number of relevant documents and presents outliers, which are topics with a great number of relevant documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GeoCLEF 2007 Results</head><p>The results of the participating groups are reported in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="10,148.44,156.99,3.74,8.74" target="#tab_9">6</ref>, a total of 13 groups from 9 different countries submitted results for one or more of the GeoCLEF tasks. A total of 108 experiments were submitted.   Five different topic languages were used for GeoCLEF bilingual experiments: German, English, Indonesian, Portuguese, and Spanish. Differently from usual, the most popular language for queries was Spanish (11 experiments out of 28 bilingual experiments); English (7 experiments) and Indonesian (6 experiments) almost tied for the second place; German (2 experiments) and Portuguese (2 experiments) tied for the third place. The number of bilingual runs by topic language is shown in Table <ref type="table" coords="11,319.22,364.17,3.77,8.74" target="#tab_14">9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monolingual Experiments</head><p>Monolingual retrieval was offered for the following target collections: English, German, and Portuguese.</p><p>Table <ref type="table" coords="11,95.88,568.41,10.02,8.74" target="#tab_15">10</ref> shows the top five groups for each target collection, ordered by mean average precision. Note that only the best run is selected for each group, even if the group may have more than one top run. The table reports: the short name of the participating group; the experiment Digital Object Identifier (DOI); the mean average precision achieved by the experiment; and the performance difference between the first and the last participant.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bilingual Experiments</head><p>The bilingual task was structured in four subtasks (X → DE, EN, or PT target collection). Table <ref type="table" coords="14,457.17,95.73,10.03,8.74" target="#tab_17">11</ref> shows the best results for this task with the same logic of Table <ref type="table" coords="14,284.11,107.19,3.77,8.74" target="#tab_10">7</ref>. Note that the top five participants contain both "newcomer" groups and "veteran" groups.</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines:     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Result Analysis</head><p>The test collection of GeoCLEF grew of 25 topics each year. This is usually considered the minimal test collection size to produce reliable results. Therefore, statistical testing and further reliability analysis are performed to assess the validity of the results obtained. The range of difficulties in the topics might have led to topics more difficult and more diverse than in traditional ad-hoc evaluations. To gain some insight on this issue, a topic performance analysis was also conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical Testing</head><p>Statistical testing for retrieval tests is intended to determine whether the order of the systems which results from the evaluation reliably measures the quality of the systems <ref type="bibr" coords="16,322.80,529.41,117.18,8.74" target="#b1">[Buckley &amp; Voorhees 2005]</ref>. In most cases, the statistical analysis gives an conservative estimate of the upper level of significance <ref type="bibr" coords="16,404.46,540.87,105.07,8.74" target="#b7">[Sanderson &amp; Zobel 2005]</ref>. We used the MATLAB Statistics Toolbox, which provides the necessary functionality plus some additional functions and utilities. We use the ANalysis Of VAriance (ANOVA) test. Table <ref type="table" coords="16,95.87,718.11,10.05,8.74" target="#tab_18">12</ref> shows the results of the Lilliefors test before and after applying the Tague-Sutcliffe transformation.</p><p>The results of the statistical analysis is shown in tables 13-18.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,70.62,340.11,123.00,8.10"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Topics GC058 and GC075</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,150.00,396.03,295.49,8.10;7,116.64,128.10,362.25,265.92"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. GeoCLEF English 2007 Pool: distribution of the different document types.</figDesc><graphic coords="7,116.64,128.10,362.25,265.92" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,149.28,475.35,296.99,8.10;8,119.70,210.90,355.92,262.56"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. GeoCLEF German 2007 Pool: distribution of the different document types.</figDesc><graphic coords="8,119.70,210.90,355.92,262.56" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,143.82,728.61,307.88,8.10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. GeoCLEF Portuguese 2007 Pool: distribution of the different document types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,139.38,733.71,316.78,8.10;12,81.24,392.40,433.04,339.30"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Monolingual English top participants. Interpolated Recall vs. Average Precision.</figDesc><graphic coords="12,81.24,392.40,433.04,339.30" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,138.66,391.17,318.20,8.10"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Monolingual German top participants. Interpolated Recall vs. Average Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,147.06,381.87,301.49,8.10;15,110.40,70.62,374.70,302.58"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Bilingual English top participants. Interpolated Recall vs Average Precision.</figDesc><graphic coords="15,110.40,70.62,374.70,302.58" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,146.34,696.21,302.93,8.10;15,117.84,413.64,360.09,280.56"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Bilingual German top participants. Interpolated Recall vs Average Precision.</figDesc><graphic coords="15,117.84,413.64,360.09,280.56" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="16,138.60,351.15,318.40,8.10;16,118.44,70.62,358.74,271.92"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Bilingual Portuguese top participants. Interpolated Recall vs Average Precision.</figDesc><graphic coords="16,118.44,70.62,358.74,271.92" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,79.02,257.31,437.53,65.14"><head>Table 1 .</head><label>1</label><figDesc>GeoCLEF test collection -collection and topic languages</figDesc><table coords="2,79.02,276.78,437.53,45.67"><row><cell>GeoCLEF Year</cell><cell>Collection Languages</cell><cell>Topic Languages</cell></row><row><cell>2005 (pilot)</cell><cell>English, German</cell><cell>English, German</cell></row><row><cell>2006</cell><cell cols="2">English, German, Portuguese, Spanish English, German, Portuguese, Spanish, Japanese</cell></row><row><cell>2007</cell><cell>English, German, Portuguese</cell><cell>English, German, Portuguese</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,70.62,218.49,454.37,123.76"><head></head><label></label><figDesc>The English document collection consists of 169,477 documents and was composed of stories from the British newspaper The Glasgow Herald (1995) and the American newspaper The Los Angeles Times(1994). The German document collection consists of 294,809 documents from the German news magazine Der Spiegel (1994/95), the German newspaper Frankfurter Rundschau (1994) and the Swiss newswire agency Schweizer Depeschen Agentur (SDA, 1994/95). For Portuguese, GeoCLEF 2007 utilized two newspaper collections, spanning over 1994-1995, for respectively the Portuguese and Brazilian newspapers Público (106,821 documents) and Folha de São Paulo (103,913 documents). Both are major daily newspapers in their countries. Not all material published by the two newspapers is included in the collections (mainly for copyright reasons), but every day is represented with documents. The Portuguese collections are also distributed for IR and NLP research by Linguateca as the CHAVE collection 2 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,178.86,403.53,237.70,40.17"><head>Table 2 .</head><label>2</label><figDesc>GeoCLEF 2007 test collection size</figDesc><table coords="3,178.86,422.58,237.70,21.13"><row><cell>Language</cell><cell>English German Portuguese</cell></row><row><cell cols="2">Number of documents 169,477 294,809 210,734</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,81.96,453.03,442.99,181.24"><head></head><label></label><figDesc>subject associated to a place (independence, concern, economic handlings to favour/harm that region, etc.) Examples: independence of Quebec, love for Peru (as often remarked, this is frequently, but not necessarily, associated to the metonymical use of place names) 5 non-geographic subject that is a complex function of place (for example, place is a function of topic) (European football cup matches, winners of Eurovision Song Contest) 6 geographical relations among places (how are the Himalayas related to Nepal? Are they inside? Do the Himalaya mountains cross Nepal's borders? etc.) 7 geographical relations among (places associated to) events (Did Waterloo occur more north than the battle of X? Were the findings of Lucy more to the south than those of the Cromagnon in Spain?) 8</figDesc><table coords="5,81.96,453.03,442.95,77.74"><row><cell></cell><cell>]:</cell></row><row><cell>1</cell><cell>non-geographic subject restricted to a place (music festivals in Germany) [only kind of topic in</cell></row><row><cell></cell><cell>GeoCLEF 2005]</cell></row><row><cell>2</cell><cell>geographic subject with non-geographic restriction (rivers with vineyards) [new kind of topic added in</cell></row><row><cell></cell><cell>GeoCLEF 2006]</cell></row><row><cell>3</cell><cell>geographic subject restricted to a place (cities in Germany)</cell></row><row><cell>4</cell><cell>non-geographic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,123.48,540.75,330.51,135.00"><head>Table 3 .</head><label>3</label><figDesc>GeoCLEF English 2007 Pool</figDesc><table coords="6,123.48,558.00,330.51,117.76"><row><cell>Pool Size</cell><cell>15,637 documents • 14,987 not relevant • 650 relevant</cell></row><row><cell></cell><cell>25 topics</cell></row><row><cell></cell><cell>• about 625 documents per topic</cell></row><row><cell cols="2">Pooled Experiments 27 out of 66 submitted experiments • monolingual: 21 out of 53 submitted experiments • bilingual: 6 out of 13 submitted experiments</cell></row><row><cell>Assessors</cell><cell>13 assessors • about 1,200 documents per assessor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,150.60,601.53,276.27,135.00"><head>Table 4 .</head><label>4</label><figDesc>GeoCLEF German 2007 Pool</figDesc><table coords="7,150.60,618.78,276.27,117.76"><row><cell>Pool Size</cell><cell>15,488 documents • 14,584 not relevant • 904 relevant</cell></row><row><cell></cell><cell>25 topics</cell></row><row><cell></cell><cell>• about 620 documents per topic</cell></row><row><cell cols="2">Pooled Experiments 24 out of 24 submitted experiments • monolingual: 16 experiments • bilingual: 8 experiments</cell></row><row><cell>Assessors</cell><cell>8 assessors • about 1,900 documents per assessor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,150.60,234.75,276.27,135.00"><head>Table 5 .</head><label>5</label><figDesc>GeoCLEF Portuguese 2007 Pool</figDesc><table coords="9,150.60,252.06,276.27,117.70"><row><cell>Pool Size</cell><cell>15,572 documents • 14,810 not relevant • 762 relevant</cell></row><row><cell></cell><cell>25 topics • about 623 documents per topic</cell></row><row><cell cols="2">Pooled Experiments 18 out of 18 submitted experiments • monolingual: 11 experiments • bilingual: 7 experiments</cell></row><row><cell>Assessors</cell><cell>6 assessors • about 2,600 documents per assessor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,151.74,192.09,292.09,187.11"><head>Table 6 .</head><label>6</label><figDesc>GeoCLEF 2007 participants -new groups are indicated by *</figDesc><table coords="10,151.74,211.08,292.09,168.13"><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>catalunya</cell><cell>U.Politecnica Catalunya</cell><cell>Spain</cell></row><row><cell>cheshire</cell><cell>U.C.Berkeley</cell><cell>United States</cell></row><row><cell>csusm</cell><cell>Cal State U.-San marcos</cell><cell>United States</cell></row><row><cell>depok*</cell><cell>U. Indonesia</cell><cell>Indonesia</cell></row><row><cell>groningen</cell><cell>U. Groningen</cell><cell>The Netherlands</cell></row><row><cell>hagen</cell><cell>U. Hagen-Comp.Sci</cell><cell>Germany</cell></row><row><cell>hildesheim</cell><cell>U. Hildesheim</cell><cell>Germany</cell></row><row><cell>icl</cell><cell cols="2">Imperial College London -Computing United Kingdom</cell></row><row><cell>linguit*</cell><cell>Linguit Ltd</cell><cell>United Kingdom</cell></row><row><cell>moscow*</cell><cell>Moscow State U.</cell><cell>Russia</cell></row><row><cell>msasia</cell><cell>Microsoft Asia</cell><cell>China</cell></row><row><cell>valencia</cell><cell>U.Politecnica Valencia</cell><cell>Spain</cell></row><row><cell>xldb</cell><cell>U.Lisbon</cell><cell>Portugal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="10,70.62,407.25,274.46,8.74"><head>Table 7</head><label>7</label><figDesc>reports the number of participants by their country of origin.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="10,207.78,430.77,179.87,151.30"><head>Table 7 .</head><label>7</label><figDesc>GeoCLEF 2007 participants by country</figDesc><table coords="10,207.78,449.82,179.87,132.26"><row><cell>Country</cell><cell># Participants</cell></row><row><cell>China</cell><cell>1</cell></row><row><cell>Germany</cell><cell>2</cell></row><row><cell>Indonesia</cell><cell>1</cell></row><row><cell>Portugal</cell><cell>1</cell></row><row><cell>Russia</cell><cell>1</cell></row><row><cell>Spain</cell><cell>2</cell></row><row><cell>The Netherlands</cell><cell>1</cell></row><row><cell>United Kingdom</cell><cell>2</cell></row><row><cell>United States</cell><cell>2</cell></row><row><cell>TOTAL</cell><cell>13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="10,70.62,609.99,435.38,8.74"><head>Table 8</head><label>8</label><figDesc>provides a breakdown of the experiments submitted by each participant for each of the offered tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="11,149.34,73.29,296.73,217.06"><head>Table 8 .</head><label>8</label><figDesc>GeoCLEF 2007 experiments by task</figDesc><table coords="11,149.34,93.36,296.73,197.00"><row><cell>Participant</cell><cell cols="6">Monolingual Tasks DE EN PT X2DE X2EN X2PT Bilingual Tasks</cell><cell>TOTAL</cell></row><row><cell>catalunya</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>cheshire</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>12</cell></row><row><cell>csusm</cell><cell>6</cell><cell>6</cell><cell>5</cell><cell></cell><cell>4</cell><cell>4</cell><cell>25</cell></row><row><cell>depok*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>6</cell></row><row><cell>groningen</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>hagen</cell><cell>5</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>hildesheim</cell><cell>4</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>icl</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>linguit*</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>moscow*</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>msasia</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>valencia</cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell></row><row><cell>xldb</cell><cell></cell><cell>5</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>TOTAL</cell><cell>16</cell><cell>53</cell><cell>11</cell><cell>8</cell><cell>13</cell><cell>7</cell><cell>108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="11,183.18,387.75,229.21,96.82"><head>Table 9 .</head><label>9</label><figDesc>Bilingual experiments by topic language</figDesc><table coords="11,183.18,407.82,229.21,76.76"><row><cell>Track</cell><cell cols="5">Source Language DE EN ES ID PT</cell><cell>TOTAL</cell></row><row><cell>Bilingual X2DE</cell><cell></cell><cell>6</cell><cell>1</cell><cell></cell><cell>1</cell><cell>8</cell></row><row><cell>Bilingual X2EN</cell><cell>1</cell><cell></cell><cell>5</cell><cell>6</cell><cell>1</cell><cell>13</cell></row><row><cell>Bilingual X2PT</cell><cell>1</cell><cell>1</cell><cell>5</cell><cell></cell><cell></cell><cell>7</cell></row><row><cell>TOTAL</cell><cell>2</cell><cell cols="2">7 11</cell><cell>6</cell><cell>2</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="12,70.62,73.29,454.38,307.41"><head>Table 10 .</head><label>10</label><figDesc>Best entries for the monolingual track. Additionally, the performance difference between the best and the last (up to 5) placed group is given (in terms of mean average precision) -new groups are indicated by *</figDesc><table coords="12,70.62,103.32,454.38,227.00"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1 st</cell><cell>catalunya</cell><cell>10.2415/GC-MONO-EN-CLEF2007.CATALUNYA.TALPGEOIRTD2</cell><cell>28.50%</cell></row><row><cell></cell><cell>2 nd</cell><cell>cheshire</cell><cell>10.2415/GC-MONO-EN-CLEF2007.CHESHIRE.BERKMOENBASE</cell><cell>26.42%</cell></row><row><cell>Monolingual</cell><cell>3 rd</cell><cell>valencia</cell><cell>10.2415/GC-MONO-EN-CLEF2007.VALENCIA.RFIAUPV06</cell><cell>26.36%</cell></row><row><cell>English</cell><cell>4 th</cell><cell>groningen</cell><cell>10.2415/GC-MONO-EN-CLEF2007.GRONINGEN.CLCGGEOEETD00</cell><cell>25.15%</cell></row><row><cell></cell><cell>5 th</cell><cell>csusm</cell><cell>10.2415/GC-MONO-EN-CLEF2007.CSUSM.GEOMOEN5</cell><cell>21.32%</cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>33,68%</cell></row><row><cell></cell><cell>1 st</cell><cell>hagen</cell><cell>10.2415/GC-MONO-DE-CLEF2007.HAGEN.FUHTDN5DE</cell><cell>25.76%</cell></row><row><cell></cell><cell>2 nd</cell><cell>csusm</cell><cell>10.2415/GC-MONO-DE-CLEF2007.CSUSM.GEOMODE4</cell><cell>21.41%</cell></row><row><cell>Monolingual</cell><cell>3 rd</cell><cell>hildesheim</cell><cell>10.2415/GC-MONO-DE-CLEF2007.HILDESHEIM.HIMODENE2NA</cell><cell>20.67%</cell></row><row><cell>German</cell><cell>4 th</cell><cell>cheshire</cell><cell>10.2415/GC-MONO-DE-CLEF2007.CHESHIRE.BERKMODEBASE</cell><cell>13.92%</cell></row><row><cell></cell><cell>5 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>85.06%</cell></row><row><cell></cell><cell>1 st</cell><cell>csusm</cell><cell>10.2415/GC-MONO-PT-CLEF2007.CSUSM.GEOMOPT3</cell><cell>17.83%</cell></row><row><cell></cell><cell>2 nd</cell><cell>cheshire</cell><cell>10.2415/GC-MONO-PT-CLEF2007.CHESHIRE.BERKMOPTBASE</cell><cell>17.39%</cell></row><row><cell>Monolingual</cell><cell>3 rd</cell><cell>xldb</cell><cell>10.2415/GC-MONO-PT-CLEF2007.XLDB.XLDBPT_1</cell><cell>3.29%</cell></row><row><cell>Portuguese</cell><cell>4 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>441.95%</cell></row></table><note coords="12,70.62,371.97,452.18,8.74"><p>Figures 5 to 7 show the interpolated recall vs. average precision for the top participants of the monolingual tasks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="14,70.62,150.43,454.29,94.13"><head></head><label></label><figDesc>• X DE: 81.1% of best monolingual German IR system • X EN: 77.4% of best monolingual English IR system • X PT: 112.9% of best monolingual Portuguese IR system Note that there is a significant improvement for Bilingual German since CLEF 2006, when it was 70% of the best monolingual system; Bilingual English shows a small improvement, with respect to the 74% of the best monolingual system in CLEF 2006; finally, Bilingual Portuguese is quite surprising since it outperforms the monolingual and it represents a complete overturn with respect to the 47% of CLEF 2006.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="14,70.62,259.41,454.28,307.47"><head>Table 11 .</head><label>11</label><figDesc>Best entries for the bilingual task. The performance difference between the best and the last (up to 5) placed group is given (in terms of mean average precision) -new groups are indicated by * Figure8to 10 show the interpolated recall vs. average precision graph for the top participants of the different bilingual tasks.</figDesc><table coords="14,70.62,289.44,450.72,227.00"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1 st</cell><cell cols="2">cheshire 10.2415/GC-BILI-X2EN-CLEF2007.CHESHIRE.BERKBIDEENBASE</cell><cell>22.08%</cell></row><row><cell></cell><cell>2 nd</cell><cell>depok*</cell><cell>10.2415/GC-BILI-X2EN-CLEF2007.DEPOK.UIBITDGP</cell><cell>20.96%</cell></row><row><cell>Bilingual</cell><cell>3 rd</cell><cell>csusm</cell><cell>10.2415/GC-BILI-X2EN-CLEF2007.CSUSM.GEOBIESEN2</cell><cell>19.62%</cell></row><row><cell>English</cell><cell>4 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>12.54%</cell></row><row><cell></cell><cell>1 st</cell><cell>hagen</cell><cell>10.2415/GC-BILI-X2DE-CLEF2007.HAGEN.FUHTDN4EN</cell><cell>20.92%</cell></row><row><cell></cell><cell>2 nd</cell><cell cols="2">cheshire 10.2415/GC-BILI-X2DE-CLEF2007.CHESHIRE.BERKBIPTDEBASE</cell><cell>11.09%</cell></row><row><cell>Bilingual</cell><cell>3 rd</cell><cell></cell><cell></cell><cell></cell></row><row><cell>German</cell><cell>4 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>88,64%</cell></row><row><cell></cell><cell>1 st</cell><cell cols="2">cheshire 10.2415/GC-BILI-X2PT-CLEF2007.CHESHIRE.BERKBIENPTBASE</cell><cell>20.12%</cell></row><row><cell></cell><cell>2 nd</cell><cell>csusm</cell><cell>10.2415/GC-BILI-X2PT-CLEF2007.CSUSM.GEOBIESPT4</cell><cell>5.33%</cell></row><row><cell>Bilingual</cell><cell>3 rd</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Portuguese</cell><cell>4 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5 th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>277.49%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="16,76.80,587.01,441.84,114.58"><head>Table 12 .</head><label>12</label><figDesc>Lilliefors test for each track with (LL) and without Tague-Sutcliffe arcsin transformation (LL &amp; TS). Jarque-Bera test for each track with (JB) and without Tague-Sutcliffe arcsin transformation (JB &amp; TS).</figDesc><table coords="16,170.58,618.48,254.79,83.11"><row><cell>Track</cell><cell cols="4">LL LL &amp; TS JB JB &amp; TS</cell></row><row><cell>Monolingual English</cell><cell>10</cell><cell cols="2">39 27</cell><cell>45</cell></row><row><cell>Monolingual German</cell><cell>0</cell><cell>13</cell><cell>8</cell><cell>14</cell></row><row><cell>Monolingual Portuguese</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>8</cell></row><row><cell>Bilingual English</cell><cell>1</cell><cell cols="2">7 10</cell><cell>13</cell></row><row><cell>Bilingual German</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>7</cell></row><row><cell>Bilingual Portuguese</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.38,747.75,108.15,8.74"><p>http://ir.shef.ac.uk/geoclef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,76.38,747.75,139.57,8.74"><p>http://www.linguateca.pt/CHAVE/</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>in the scope of the Linguateca project, jointly funded by the <rs type="funder">Portuguese Government</rs> and the <rs type="funder">European Union (FEDER and FSE)</rs> under contract ref. <rs type="grantNumber">POSC/339/1.3/C/NAC</rs>. The topics were thoroughly checked by <rs type="person">Sven Hartrumpf</rs> from the <rs type="affiliation">University of Hagen (Germany</rs>). The future direction and scope of GeoCLEF will be heavily influenced by funding and the amount of volunteer effort available.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CXFeECc">
					<idno type="grant-number">POSC/339/1.3/C/NAC</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment DOI Groups</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="22,86.71,349.96,438.27,7.85;22,81.96,360.03,137.49,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="22,207.80,349.96,299.90,7.85">Carol: Cross-Language Evaluation Forum: Objectives, Results, Achievements</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,81.96,360.03,78.25,8.10">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,83.78,373.66,441.15,7.85;22,81.96,383.98,223.24,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,220.41,373.66,102.30,7.85">Retrieval System Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,341.92,373.66,183.00,7.85;22,81.96,383.98,31.75,7.85">TREC: Experiment and Evaluation in Information Retrieval</title>
		<meeting><address><addrLine>Cambridge &amp; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="53" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,83.40,397.36,441.49,7.85;22,81.96,407.43,442.95,8.10;22,81.96,418.06,107.73,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,311.86,397.36,209.30,7.85">Challenges and Resources for Evaluating Geographical IR</title>
		<author>
			<persName coords=""><forename type="first">Marcirio</forename><surname>Chaves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mário</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,93.96,407.43,327.71,8.10">Proceedings of the 2nd International Workshop on Geographic Information Retrieval</title>
		<meeting>the 2nd International Workshop on Geographic Information Retrieval<address><addrLine>CKIM; Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">2005. Nov. 2005</date>
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,85.50,431.38,439.44,7.85;22,81.96,441.51,193.74,8.10;22,275.70,439.54,4.68,5.40;22,282.96,441.51,241.89,8.10;22,81.96,452.08,237.95,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="22,362.67,431.38,162.27,7.85;22,81.96,441.76,172.68,7.85">GeoCLEF: the CLEF 2005 cross-language geographic information retrieval track overview</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,271.20,441.51,4.50,8.10;22,275.70,439.54,4.68,5.40;22,282.96,441.51,237.85,8.10">6 th Workshop of the Cross-Language Evaluation Forum: CLEF 2005</title>
		<title level="s" coord="22,119.05,452.08,152.86,7.85">Lecture Notes in Computer Science LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,83.82,465.46,441.07,7.85;22,81.96,475.78,442.89,7.85;22,81.96,485.91,159.24,8.10;22,241.20,483.94,4.68,5.40;22,248.40,485.91,276.53,8.10;22,81.96,496.48,442.96,7.85;22,81.96,506.80,153.29,7.85;22,81.96,517.18,331.24,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="22,290.51,475.78,234.34,7.85;22,81.96,486.16,138.08,7.85">GeoCLEF 2006: the CLEF 2006 Cross-Language Geographic Information Retrieval Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><forename type="middle">;</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">;</forename><surname>Bishoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kerstin</forename><forename type="middle">;</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">;</forename><surname>Womser-Hacker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><forename type="middle">;</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><forename type="middle">;</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2006/working_notes/workingnotes2006/geyOCLEF2006.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,236.70,485.91,4.50,8.10;22,241.20,483.94,4.68,5.40;22,248.40,485.91,237.24,8.10">7 th Workshop of the Cross-Language Evaluation Forum: CLEF 2006</title>
		<title level="s" coord="22,288.41,496.48,156.05,7.85">Lecture Notes in Computer Science LNCS</title>
		<editor>et al.</editor>
		<meeting><address><addrLine>Alicante, Spain; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="852" to="876" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers. preprint: CLEF 2006 Working Notes</note>
</biblStruct>

<biblStruct coords="22,83.71,530.25,441.20,8.10;22,81.96,540.63,267.02,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="22,316.26,530.50,171.08,7.85">Query Parsing Task for GeoCLEF 2007 Report</title>
		<author>
			<persName coords=""><forename type="first">Zhisheng</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chong</forename><forename type="middle">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wie-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,503.88,530.25,21.02,8.10;22,81.96,540.63,134.72,8.10">Cross Language Evaluation Forum (CLEF)</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>Working Notes. in this volume</note>
</biblStruct>

<biblStruct coords="22,85.50,554.20,439.42,7.85;22,81.96,564.33,443.01,8.10;22,81.96,574.65,440.17,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="22,260.01,554.20,264.92,7.85;22,81.96,564.58,326.04,7.85">Inside the evaluation process of the cross-language evaluation forum (CLEF): Issues of multilingual topic creation and multilingual relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,429.42,564.33,95.55,8.10;22,81.96,574.65,261.44,8.10">Proceedings of the third International Conference on Language Resources and Evaluation, LREC</title>
		<meeting>the third International Conference on Language Resources and Evaluation, LREC<address><addrLine>Las Palmas, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,84.07,588.28,440.80,7.85;22,81.96,598.35,9.00,8.10;22,90.96,596.38,4.68,5.40;22,97.92,598.35,374.67,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="22,231.65,588.28,279.39,7.85">Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">;</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,81.96,598.35,9.00,8.10;22,90.96,596.38,4.68,5.40;22,97.92,598.35,374.67,8.10">28 th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,83.95,611.73,440.97,8.10;22,81.96,622.05,395.23,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="22,209.05,611.98,196.80,7.85">Portuguese at CLEF 2005: Reflections and challenges</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org" />
	</analytic>
	<monogr>
		<title level="m" coord="22,422.70,611.73,102.21,8.10;22,81.96,622.05,133.29,8.10">Cross Language Evaluation Forum (CLEF) 2005 Working Notes</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09-23">21-23 September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,88.23,635.68,436.54,7.85;22,81.96,645.75,415.50,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="22,198.97,635.68,202.70,7.85">The place of place in geographical information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chaves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,111.00,645.75,239.68,8.10">Workshop on Geographic Information Retrieval (GIR06), SIGIR06</title>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Purves</surname></persName>
		</editor>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08-10">10 August 2006. 2006</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,90.01,659.38,434.91,7.85;22,81.96,669.70,286.19,7.85" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
		<ptr target="http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/CQPTutorial/html/" />
		<title level="m" coord="22,146.04,659.38,225.54,7.85">The CQP Query Language Tutorial (CWB version 2.2.b90</title>
		<imprint>
			<date type="published" when="2005-07-10">10 July 2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Stuttgart</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="22,89.60,683.08,435.20,7.85;22,81.96,693.16,442.92,8.10;22,81.96,703.53,362.72,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="22,223.39,683.08,257.58,7.85">Providing Internet access to Portuguese corpora: the AC/DC project</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eckhard</forename><surname>Bick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,457.08,693.16,67.80,8.10;22,81.96,703.53,292.41,8.10">Proceedings of the Second International Conference on Language Resources and Evaluation, LREC</title>
		<editor>
			<persName><forename type="first">Maria</forename><surname>Gavriladou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Carayannis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stella</forename><surname>Markantonatou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gregory</forename><surname>Stainhaouer</surname></persName>
		</editor>
		<meeting>the Second International Conference on Language Resources and Evaluation, LREC</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,88.03,717.10,436.91,7.85;22,81.96,727.23,442.85,8.10;22,81.96,737.55,189.39,8.10;22,271.38,735.58,4.68,5.40;22,279.12,737.55,245.71,8.10;22,81.96,748.18,345.66,7.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,209.22,717.10,300.15,7.85">The key to the first CLEF in Portuguese: Topics, questions and answers in CHAVE</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,479.82,727.23,44.99,8.10;22,81.96,737.55,189.39,8.10;22,271.38,735.58,4.68,5.40;22,279.12,737.55,245.71,8.10">Multilingual Information Access for Text, Speech and Images: 5 th Workshop of the Cross-Language Evaluation Forum (CLEF 2004)</title>
		<title level="s" coord="22,249.05,748.18,126.85,7.85">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-09-17">15-17 September 2004</date>
			<biblScope unit="page" from="821" to="832" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
