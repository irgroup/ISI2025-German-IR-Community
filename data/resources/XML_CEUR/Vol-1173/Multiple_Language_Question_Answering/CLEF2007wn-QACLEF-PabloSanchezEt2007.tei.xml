<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.95,146.21,367.07,18.08;1,205.99,168.13,191.01,18.08;1,397.02,166.36,5.98,12.55">MIRACLE Question Answering System for Spanish at CLEF 2007 *</title>
				<funder ref="#_VFHpcjy">
					<orgName type="full">Spanish Ministry of Education and Science</orgName>
				</funder>
				<funder>
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_ur6wa5E">
					<orgName type="full">Research Network MAVIR</orgName>
				</funder>
				<funder ref="#_4tpswWJ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.91,203.19,101.82,10.46"><forename type="first">César</forename><surname>De Pablo-Sánchez</surname></persName>
						</author>
						<author>
							<persName coords="1,313.71,203.19,80.74,10.46"><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Martínez</surname></persName>
						</author>
						<author>
							<persName coords="1,186.49,217.13,89.26,10.46"><forename type="first">Ana</forename><surname>García-Ledesma</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,283.91,217.13,46.48,10.46"><forename type="first">Doaa</forename><surname>Samy</surname></persName>
							<email>dsamy@inf.uc3m.es</email>
						</author>
						<author>
							<persName coords="1,338.85,217.13,73.01,10.46"><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
						</author>
						<author>
							<persName coords="1,201.78,231.09,111.13,10.46"><forename type="first">Antonio</forename><surname>Moreno-Sandoval</surname></persName>
						</author>
						<author>
							<persName coords="1,321.08,231.09,80.14,10.46"><forename type="first">Harith</forename><surname>Al-Jumaily</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Autónoma de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.95,146.21,367.07,18.08;1,205.99,168.13,191.01,18.08;1,397.02,166.36,5.98,12.55">MIRACLE Question Answering System for Spanish at CLEF 2007 *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9103EFA1C6281F425911DC9D44D7E624</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Spanish</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system developed by MIRACLE group to participate in the Spanish monolingual question answering task at QA@CLEF 2007. A basic subsystem, similar to our last year participation, was used separately for EFE and Wikipedia collection. Answers from the two subsystems are combined using temporal information from the questions and the collections. The system is also enhanced with a correference module that processes question series based on a few simple heuristics that constraint the structure of the dialogue. The analysis of the results show that the reuse of strategies for factoids is feasible but definitions would benefit from adaptation. Regarding questions series, our heuristics have good coverage but we should find alternatives to avoid error chaining from previous questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QA@CLEF 2007 has introduced two innovations over last year evaluation. The first change consists on topic-related questions, a series of ordered questions about a common topic that simulates the dialogue between a user and the system to obtain information related to the topic. In this dialogue the user could introduce anaphoric expressions to refer to mentioned entities or events that appear in previous answers or questions. The second innovation is related to the inclusion of the November 2006 dump of the Wikipedia [14] as a source of answers in addition to the classic newspaper collections.</p><p>MIRACLE submitted a run for the Spanish monolingual task using a system that was enhanced to answer questions from Wikipedia and EFE collections. Each subsystem results were combined in a unified ranked list. The system also included a module that handles topic related questions. It identifies the topic and use it to enhance the representation of the following questions. The basic QA system was based on the architecture of our last year submission <ref type="bibr" coords="2,427.59,218.13,10.52,10.46" target="#b5">[6]</ref> although almost all components have evolved since then. This system was based on the use of filters for semantic information and therefore is tailored for factual questions. Last year we also tried to improve it for temporally restricted questions. An additional requirement has been to develop a fast system that could be competitive in real time with a classic information retrieval system. This paper is structured as follow, the next section decribes the system architecture with special attention to the new modules. Section 3 introduces the results and a preliminary analysis of the kind of errors that the system made. Conclusions and directions of future work to solve the main problems follow in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The architecture of the system used this year is presented in figure <ref type="figure" coords="2,390.17,367.76,4.98,10.46" target="#fig_0">1</ref> and it is composed of two streams for each of the sources similar to <ref type="bibr" coords="2,268.72,379.72,10.52,10.46" target="#b3">[4,</ref><ref type="bibr" coords="2,282.12,379.72,7.01,10.46" target="#b2">3]</ref>. The first stream uses the EFE newswire collection as a source of answers while the second uses Wikipedia. Each stream produces a ranked list of answers that are merged and combined by the Answer Source Mixer component described below. The two QA streams share a similar basic pipeline architecture and work as an independent QA system, with different configuration parameters, collections, etc. The way we perform question analysis is common to the two streams and therefore, when the two streams are composed, this module is shared. Another new common module complements question analysis for managing context and anaphora resolution in topic-related question series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic system architecture</head><p>The basic system follows the classic pipeline architecture used by many other QA systems <ref type="bibr" coords="2,483.87,508.90,10.52,10.46" target="#b7">[8,</ref><ref type="bibr" coords="2,497.50,508.90,11.62,10.46" target="#b10">11]</ref>. The different operations are split between those that are performed online and offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Offline operations</head><p>These operations are performed during the preparation of the collection to speed up online document retrieval and answer extraction.</p><p>• Collection indexing. Collections are indexed at the word level using Lucene <ref type="bibr" coords="2,470.46,592.61,9.96,10.46" target="#b0">[1]</ref>. They are processed to extract the text that will be indexed. In the case of Wikipedia we remove format and links and therefore we do not use this information for relevance. Documents are indexed after tokenization, stopword removal and stemming based on Snowball <ref type="bibr" coords="2,463.02,628.47,14.61,10.46" target="#b9">[10]</ref>.</p><p>• Collection processing. In order to speed the process of answer extraction we can enable the use of preprocessed collections. In this case, collections are analyzed using the output of language tools or services. For Spanish we use DAEDALUS STILUS <ref type="bibr" coords="2,430.87,670.49,15.36,10.46" target="#b4">[5]</ref> analyzers that provide tokenization, sentence detection and token analysis. Token analysis include detailed part of speech analysis, lemmatization and semantic information. STILUS has been improved from last year to support part of speech tagging. Regarding the semantic information, we have use STILUS Named Entities tagging, that is based on linguistic resources organized in a classification inspired by Sekine's typology <ref type="bibr" coords="2,304.56,730.27,17.20,10.46" target="#b12">[13]</ref>. The processed collection is stored on disk. Without compression the collection is about 10 times larger than the corresponding original. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Online operations</head><p>Online operations has been described in detail in previous participations <ref type="bibr" coords="3,423.98,562.38,9.96,10.46" target="#b5">[6]</ref>. We outline the description of the three main modules here and remark only the main changes.</p><p>• Question analysis. Questions are analyzed using STILUS online and the module produce a custom representation of the question that includes focus, theme, relevant and query terms, question type and expected answer type (EAT).</p><p>• Passage retrieval. We have change this module to use Lucene as information retrieval engine. The question reresentation is used to build a query using Lucene syntax and relevant documents are retrieved. Lucene uses a vector model for document representation and cosine similarity for ranking. Documents are analyzed and only sentences that contain a number of relevant terms are selected for the next step.</p><p>• Answer selection. This module uses the question class and the expected answer type to select an appropiate filter to locate candidate answers. Extracted candidates are grouped if they are similar and ranked using a score that combines redundancy, sentence score and document score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">New modules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Group topic identification in question series</head><p>With the inclusion of topic related questions, the system needed a method to solve referential expressions that appear between questions and answers in the same series. The system processes the first question and generates a set of candidates with the topic, the focus and the future answer.</p><p>We have implemented a few rules that we believed covered the most common cases to select the best topic for the whole group. The rules use the information available after question analysis and simplified assumptions about the syntactic structure of the questions. The rules to locate the topic for the question series are only applied to the first question:</p><p>• Answers of subtypes NUMEX (numbers and quantities) are ignored as topics for questions series. We believe it is improbable for a number, if not representing any other type of entity, to be a natural topic. We use the subject, the topic of this question, as the topic of the question series.</p><p>• Answers of subtypes TIMEX (dates, years, etc...) are also ignored as topics, but in this case it is because we believed that they fall outside the guidelines for this year. We use the same rule than for NUMEX so far. Using a temporal expresion as a topic is very natural. In fact, most temporally restricted questions and specially those that have an event as restrictions are naturally split into two questions. This strategy has already been used by <ref type="bibr" coords="4,453.04,342.58,15.50,10.46" target="#b11">[12]</ref> to answer this kind of questions. We would need to recognize referential expressions like ese año (that year), durante ese período (during that period), etc. and solve the reference correctly in a second step.</p><p>• The question ask for a definition like Quién es George Bush? (Who is George Bush?) will add the topic (Named Entity) and the answer (presidente de los Estados Unidos) to the group topic. We have a similar case when we have questions like Quién es el presidente de los Estados Unidos? ( Who is the president of the United States?).</p><p>• The question follows the pattern Qué NP * ? (What NP * ?) " like Qué organización se fundó en 1995? (Which organization was created in 1995?). In this cases the noun group that follow the interrogative article is the focus of the question. Both the answer and the focus would be added to the group topic. We should remark that this case is different from a question beginning with a preposition like En qué lugar... ? (In which place...?).</p><p>• For the rest of the classes we use the answer as the topic for the rest of the group.</p><p>Once the topic for the group is identified, the rest of the questions use it as an additional relevant term in order to locate documents and filter relevant sentences. This is obviously a problem when the topic was the answer but the system was not able to find the right one.</p><p>These rules are based on the structure of the information seeking dialogue and how we introduce new topics in conversation. Our rules select the most promising candidate using the firt question and ignoring the rest. Nevertheless, it is posible to shift the topic by introducing a longer referential expresion like the examples mentioned for TIMEX. We plan to investigate how to modify our procedure to work in two steps, generating a ranking list of cantidates and selecting the best candidate depending on constraints expressed in the referential expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Combining EFE and Wikipedia answers</head><p>As already mentioned, this year there are two possible collections to find the right answer to a question, one is Wikipedia and the other is the EFE newswire collection for years 1994 and 1995. This means that there should be some automatic method to decide which of these sources should be more relevant to extract candidate answers to a given question. This is the role of the Source Mixer component, based on very simple heuristics:</p><p>• If the verb of the question appears in present tense preference is given to answers appearing in the Wikipedia collection.</p><p>• If the verb is in past tense and the question makes reference to the period covered by the EFE news collection, i.e., 1994 or 1995 years appearing in the question, then preference is given to answers from the EFE news collection.</p><p>The preference given to each answer is measured by two parameters, one referring to the verb tense factor and the other referring to the time period factor. In this way, no answer is really dropped from the candidates list but the list is reordered according to this clues. At the output of the Source Mixer component there is a list of candidates ordered according to their source and the information present in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Run description</head><p>Using the system described above we have submitted one monolingual run for the Spanish subtask of this edition. Evaluation results and combined measures are outlined in tables 1 and 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Acc@1(F) Acc@1(T) Acc@1(D) Acc@1(All) MIRA071ESES 18. <ref type="bibr" coords="5,232.03,469.45,9.08,10.46">35</ref> 13.95 3.13 15.00</p><p>Results are disappointing in general, as they are lower than previous years results for almost all types, despite the inclusion of new sources like Wikipedia. Even though almost all modules have been improved, the overall accuracy is not improving. We believe that whether the question set is more difficult or the system was not tuned as much as needed. We do not show results for the ten list questions because we did not implement an specific strategy. The case of definitions is analyzed with more detail below. We believe that question series introduce additional complexity in the task and this is reflected in the results. If we ignore question series we obtain an accuracy around 18% for the rest of the questions, which supports this idea but reflect that even base results are not good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of errors</head><p>We are analysing our results in order to estimate which parts of the system need further improvement. For the moment, we are only able to present a preliminary analysis of the contents of our submission that uses one answer per question with their supporting sentence and document. The document returned is correct in 44% of the results, which is a lower bound to measure the performance of the document retrieval system. It also includes errors caused by an incorrect selection of a document collection. Given a correct document, 31% of the sentences selected with the first answer do not really contain a correct answer. Even if the sentence is correct, the error selecting a correct string is about 47%. These kind of error accumulates incorrect selection of a candidate, incorrect extraction and incorrect identification of the expected answer type. Finally, at least 8 questions have found an unsupported answer, which signals that in those cases where the same answer string has been extracted from several sentences, the score to select the most representative answer is not working as well as expected. A more detailed analysis isolating the contribution of the main modules will be presented in the final version of this article.</p><p>Analyzing the behaviour of the system for different types we have detected that the accuracy of definition questions has dropped dramatically. We believe that the heuristics that we have defined to extract definitions in EFE do not work for Wikipedia. The system used to signal appositions, nominal phrases before Named Entities and expansion of acronyms as valid definitions of persons and organizations. In contrast, definitional sentences in Wikipedia usually are copulative, they have a longer distance between the defined object and a valid definition and they usually are placed in the beginning of the document. Unfortunately, we did not implement any special strategy for these questions. If we consider the rest of the types, the behaviour is similar to previous years, most of the factual questions with well defined Named Entities achieve a reasonable accuracy. Questions with EAT OTHER are in contrast much harder.</p><p>There were 20 topic related groups of questions that made a total of 50 questions. The system correctly answered 5 questions of this kind, from three different groups. This behaviour was expected as errors usually chain, specially in the case when the answer to the first question is not correct and this happens to be the topic. We have analyze the main source of errors in order to evaluate the correference component. Rules for topic detection are the source of errors only for three of the cases, and therefore seems to work reasonably well although there is room for some simple improvements. In one of these cases, the error is due to a question not correctly identified as a NUMEX type. The rest of the errors are due to incorrect identification of the first answer and the chaining of errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have presented the architecture of the MIRACLE QA system and the main modifications introduced to cope with new challenges in the CLEF@QA task. Using this system we have submitted one run for the monolingual Spanish subtask where results were lower than expected.</p><p>The analysis of the performance across types have signalled that for some type of questions the style of the text is an important issue. This is specially accute in the case of definitions. We have employed the same subsystem and strategies for the EFE and the Wikipedia collections with dissapointing results. The analysis of the errors have shown that the methods for document retrieval and candidate extraction could be adapted to improve the accuracy. We plan to use type speficic approaches like the ones used by <ref type="bibr" coords="6,265.42,523.96,10.52,10.46" target="#b6">[7,</ref><ref type="bibr" coords="6,278.57,523.96,7.75,10.46" target="#b8">9]</ref> and collection specific approaches to improve results for these types.</p><p>We have found that the module for correference resolution is effective even if it uses a limited amount of knowledge. In contrast, the greater contribution of errors is due to the low accuracy at the first answer. This is even more accute as a great deal of the questions that set the initial topic are definitional. Besides type specific approaches, we need to improve the correference method to consider more than one candidate answer and cope with uncertainty.</p><p>Another question that we have not evaluated thoroughly yet is the way we combine results. We use a semantic kind of combination that exploits the different time spans of the two collections. It is still unclear if these method is appropriate or whether techniques adapted from information retrieval from heterogeneus collections as in <ref type="bibr" coords="6,283.99,643.51,10.52,10.46" target="#b1">[2]</ref> would work better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,199.46,502.20,204.10,10.46;3,123.00,118.12,339.49,379.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MIRACLE 2007 System architecture</figDesc><graphic coords="3,123.00,118.12,339.49,379.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,172.97,349.59,257.07,44.47"><head>Table 1 :</head><label>1</label><figDesc>Judged answers for submitted run</figDesc><table coords="5,172.97,371.25,257.07,22.81"><row><cell>Name</cell><cell cols="4">Right Wrong Inexact Unsupp.</cell></row><row><cell cols="2">MIRA071ESES 30</cell><cell>158</cell><cell>4</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,196.56,437.37,209.87,10.46"><head>Table 2 :</head><label>2</label><figDesc>Evaluation measures for submitted run</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* This work has been partially supported by the <rs type="funder">Regional Government of Madrid</rs> under the <rs type="funder">Research Network MAVIR</rs> (<rs type="grantNumber">S-0505/TIC-0267</rs>) and two projects by the <rs type="funder">Spanish Ministry of Education and Science</rs> (<rs type="grantNumber">TIN2004/07083</rs> and <rs type="grantNumber">TIN2004-07588-C03-02</rs>.)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ur6wa5E">
					<idno type="grant-number">S-0505/TIC-0267</idno>
				</org>
				<org type="funding" xml:id="_VFHpcjy">
					<idno type="grant-number">TIN2004/07083</idno>
				</org>
				<org type="funding" xml:id="_4tpswWJ">
					<idno type="grant-number">TIN2004-07588-C03-02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,110.47,698.28,253.71,10.46" xml:id="b0">
	<monogr>
		<ptr target="http://lucene.apache.org/" />
		<title level="m" coord="6,110.47,698.28,68.54,10.46">Lucene webpage</title>
		<imprint>
			<date type="published" when="2007-08">August 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.47,718.20,402.53,10.46;6,110.48,730.16,402.53,10.46;6,110.48,742.11,205.44,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,455.58,718.20,57.42,10.46;6,110.48,730.16,216.62,10.46">Fusion de respuestas en la busqueda de respuestas multilingue</title>
		<author>
			<persName coords=""><forename type="first">Rita</forename><forename type="middle">M</forename><surname>Aceves-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes Y Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villasenor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,335.93,730.16,177.08,10.46;6,110.48,742.11,131.30,10.46">SEPLN, Sociedad Espaola para el Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,110.53,402.54,10.46;7,110.48,122.49,402.54,10.46;7,110.48,134.45,105.76,10.46" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,148.45,122.49,364.56,10.46;7,110.48,134.45,73.88,10.46">Making Stone Soup: Evaluating a Recall-Oriented Multi-stream Question Answering System for Dutch</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,154.37,402.54,10.46;7,110.48,166.33,402.52,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,151.93,166.33,284.33,10.46">A multi-strategy and multi-source approach to question answering</title>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,456.43,166.33,24.84,10.46">TREC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,186.26,320.26,10.46" xml:id="b4">
	<monogr>
		<author>
			<orgName type="collaboration" coords="7,110.47,186.26,51.91,10.46">DAEDALUS</orgName>
		</author>
		<ptr target="http://www.daedalus.es" />
		<title level="m" coord="7,173.29,186.26,58.05,10.46">Stilus website</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,206.18,402.53,10.46;7,110.48,218.13,402.52,10.46;7,110.48,230.09,288.50,10.46" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Cesar</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><forename type="middle">-</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Gonzalez-Ledesma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Moreno-Sandoval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria-Teresa</forename><surname>Vicente-Diez</surname></persName>
		</author>
		<title level="m" coord="7,209.64,218.13,303.36,10.46;7,110.48,230.09,219.32,10.46">MIRACLE experiments in QA@CLEF 2006 in spanish: main task, real-time QA and exploratory QA using wikipedia</title>
		<imprint>
			<publisher>WiQA</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,250.01,402.54,10.46;7,110.48,261.97,115.25,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,393.71,250.01,119.30,10.46;7,110.48,261.97,79.33,10.46">Offline strategies for online question answering</title>
		<author>
			<persName coords=""><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Fleischman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,281.89,402.55,10.46;7,110.48,293.85,402.53,10.46;7,110.48,305.81,144.96,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,240.73,293.85,272.28,10.46;7,110.48,305.81,74.14,10.46">The structure and performance of an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,206.14,305.81,17.47,10.46">ACL</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,325.73,402.54,10.46;7,110.48,337.69,402.53,10.46;7,110.48,349.64,402.51,10.46;7,110.48,361.60,63.65,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,362.30,337.69,150.72,10.46;7,110.48,349.64,145.70,10.46">A full data-driven system for multiple language question answering</title>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes-Y Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villaseñor Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilio</forename><surname>Sanchís-Arnal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,273.91,349.64,207.37,10.46">Accessing Multilingual Information Repositories</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,381.52,402.55,10.46;7,110.48,393.47,44.69,10.46" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org" />
		<title level="m" coord="7,177.51,381.52,177.34,10.46">Snowball stemmers and resources website</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,413.40,402.54,10.46;7,110.48,425.35,402.54,10.46;7,110.48,437.32,402.52,10.46;7,110.48,449.27,22.69,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,392.06,413.40,120.96,10.46;7,110.48,425.35,77.18,10.46">Question-answering by predictive annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,208.52,425.35,304.50,10.46;7,110.48,437.32,330.15,10.46">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Question Answering</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Question Answering</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,469.20,402.54,10.46;7,110.48,481.15,402.52,10.46;7,110.48,493.10,90.82,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,416.35,469.20,96.66,10.46;7,110.48,481.15,123.08,10.46">Evaluation of complex temporal questions in clef-qa</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez-Barco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,248.38,481.15,259.90,10.46">Multilingual Information Access for Text, Speech and Images</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,513.03,402.55,10.46;7,110.48,524.98,57.34,10.46" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<ptr target="http://nlp.cs.nyu.edu/ene/" />
		<title level="m" coord="7,177.75,513.03,174.61,10.46">Sekine&apos;s extended named entity hierarchy</title>
		<imprint>
			<date type="published" when="2007-08">August 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
