<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,318.57,149.46,173.17,14.93;1,271.99,171.38,59.01,14.93">2007: Answer Validation Exercise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,266.81,203.69,69.39,10.37"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>iglockner@web.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Hagen at CLEF</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Intelligent Information and Communication Systems (IICS)</orgName>
								<orgName type="institution">FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,318.57,149.46,173.17,14.93;1,271.99,171.38,59.01,14.93">2007: Answer Validation Exercise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EED3762BACC472DAE234DA95E2708557</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information filtering, Selection process</term>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation (efficiency and effectiveness)</term>
					<term>I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Predicate Logic, Semantic networks</term>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing Experimentation, Measurement, Verification Logical Answer Validation, Answer Selection, Question Answering, Recognising Textual Entailment (RTE), Information Fusion, Robust Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAVE (Multinet-based Answer VErification</head><p>) is an answer validation system based on deep linguistic processing and logical inference originally developed for AVE 2006. Robustness of the entailment check is obtained by embedding the theorem prover in a constraint relaxation loop. The system can also be used for answer selection, which is then guided by the joint evidence of all available text passages supporting a considered answer. Recent improvements of the basic MAVE system target at boosting answer selection. In order to profit from redundancy, the validation set is actively extended by additional supporting text passages. Three question-answering (QA) systems developed in our group were used to generate such candidates which were then filtered by methods for spotting answer-answer relationships. A novel technique called evidence reassignment (ERA) restructures the validation set by assigning each piece of evidence to all answers potentially supported by it. Further changes to MAVE comprise the integration of large lexical-semantic resources; backing the main logic-based features with overlap-based methods which are robust against parsing failure; implementation of two additional sanity checks (assessing the usefulness of an answer to a definition question, and compatibility of expected and actual answer types for factual questions); and finally the use of separate thresholds for selecting/rejecting the best answer and validating/rejecting the remaining choices, in order to capture the fact that many factual questions have a unique answer. Resources used include HaGenLex (a semantic-based lexicon for German developed in our group), GermaNet, and OpenThesaurus. Moreover MAVE has a very limited domain model. The results obtained confirm the proposed approach to answer selection, with f-measure in the 70 percent range, and 93 percent selection performance compared to an optimal selection strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 System description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Introductory remarks</head><p>The first prototype of the MAVE answer validator <ref type="bibr" coords="2,294.77,155.28,11.62,8.64" target="#b1">[2]</ref> was developed for the Answer Validation Exercise 2006 and subsequently extended to support answer selection <ref type="bibr" coords="2,331.46,167.23,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="2,344.51,167.23,7.19,8.64" target="#b3">4]</ref>. This paper will focus on changes to the basic system and details omitted here will likely be found in one of these earlier publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Active enhancement of validation sets</head><p>The most important improvement to the MAVE system is the possibility to extract supporting text passages from the QA@CLEF news collection and the German Wikipedia. These passages are then added to the validation set as sources of additional evidence. This process of active enhancement can be described as follows. The AVE07 development set and the AVE07 test set comprise all 'original' validation items. (Depending on the context, both collections can be considered the 'validation set'). Each validation item can be represented by a quadruple (q, a, w, o) which relates to a certain question q and specifies an exact answer a and a witness text w extracted from the document corpus which is intended to justify the answer. The fourth component indicates if the validation item belongs to the original validation set (official validation item with o = 1), or if the item has later been added by active enhancement of the validation set (auxiliary item with o = 0). It is important to make this distinction between 'official' and 'auxiliary' validation items since only official validation items are admissible choices for answer selection. The auxiliary items need not support an answer which directly occurs in the test set. Generally speaking, such an item will also be useful if it refers only to a paraphrase or spelling variant of one of the answers of interest. Moreover a more detailed answer can be useful which includes one of the answers of interest as a special case. Two methods for leveraging such answer-answer relationships have been developed for MAVE.</p><p>The first method, already explained in <ref type="bibr" coords="2,258.95,405.71,10.58,8.64" target="#b3">[4]</ref>, uses a simplification function σ in order to map the original answers to simplified answer keys by translating into lowercase, removing accents, omitting stopwords, omitting whitespace, etc. The resulting simplified answers are then used for grouping validation items which share the same cluster key, and evidence is aggregated for all validation items which belong to a given answer cluster. This method can be used for extending the original validation set. For that purpose, one needs QA systems which generate additional answer candidates and supporting text passages. Those generated answers which correspond to a cluster-key variant of one of the answers in the original validation set are then added as auxiliary validation items.</p><p>The second method, called evidence reassignment (ERA), makes it possible to combine answer validation and information fusion in a way consistent with non-local and non-monotonic phenomena in natural language (see <ref type="bibr" coords="2,146.99,525.27,11.62,8.64" target="#b0">[1]</ref> for an exposition of such phenomena).</p><p>To explain how ERA works, let us assume a simple method for hypothesizing answer-answer relationships which just checks the two answers for ordered character containment. Further suppose that v = ('Who is Di Mambro?', 'a prophet', w, 1) is the original validation item in the test set (actually, an item which should be rejected) and further suppose that v = ('Who is Di Mambro?', 'a false prophet', w , 0) was found by one of the QA systems used for enhancing the validation set, where w is a supporting text passage which mentions that Di Mambro is a false prophet. Then 'a false prophet' would be wrongly considered as a specialization of 'a prophet' by the simple inclusion-spotting method. A subsequent aggregation of evidence, treating the text passage mentioning that Di Mambro is a false prophet as evidence for Di Mambro being a prophet, would thus mean rewarding a false conclusion.</p><p>The easiest way to ensure sound results despite such non-monotonic and non-local phenomena is to simply reassign evidence by building new validation items. To this end, the ERA method builds a new validation item v = ('Who is Di Mambro?', 'a prophet', w , 0), while the extracted item v (which does not refer to the exact answer of interest) will be discarded. The basic entailment check of the validation system will therefore check if the hypothesis 'Di Mambro is a prophet' (rather than 'Di Mambro is a false prophet') is entailed by the witness text w . Since this text only mentions that Di Mambro is a false prophet, this test will fail (at least in our system which does not treat 'false' as an intersective adjective). Because the new supporting text is reassigned to the actual answer of interest (i.e. 'a prophet') and validated against this answer rather than the presumably including answer extracted by the inclusion spotting method (i.e. 'a false prophet'), errors in the inclusion hypotheses will no longer spoil the results of logical validation and aggregation of evidence.</p><p>Four methods for recognizing inclusions and answer variants have been implemented for supporting the basic ERA approach: a) recognition of variants by clustering simplified answer strings; b) unsorted lexical overlap test; c) sequential lexical overlap, i.e. also considering the order of occurrence. This method is usually a strengthening of b., but might find additional matches in certain circumstances; d) logical inclusion as checked by an exact proof of the representation of the more general answer from the more specific answer given the background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Processing of validation items 1.3.1 Syntactic-semantic analysis and coreference resolution</head><p>The WOCADI parser <ref type="bibr" coords="3,179.91,261.90,11.62,8.64" target="#b4">[5]</ref> is used for a deep linguistic analysis of the questions, answers, and supporting text passages. This process results in information about the tokens, lemmata, and word senses in the analyzed natural language expression. Moreover the parser constructs a semantic representation of the natural language input, which is expressed in the MultiNet formalism <ref type="bibr" coords="3,342.01,297.77,11.62,8.64" target="#b5">[6]</ref> (a variant of semantic networks specifically suited for natural-language processing). WOCADI handles intrasentential coreference resolution of pronouns and nominal anaphora and also gathers the necessary data for MAVE to perform a subsequent intersentential coreference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Postprocessing and synonym normalization</head><p>The post-processing of parsing results includes correction of a few known parsing problems and refinement of information on the facticity and genericity of objects and events mentioned in the text, which is also encoded in the MultiNet representation. The most important postprocessing step consists in the socalled synonym normalization, i.e. a list of synonyms and near-synonyms is used for replacing all lexical concept constants in the semantic representation with a canonical synset representative. This normalization eliminates the need of handling synonyms on the level of knowledge processing. To ensure this, the method must also be applied to the logical axioms, i.e. all lexical constants which occur in the axioms must be syno-normalized as well. The synonyms known to MAVE now cover 111,436 lexical constants and spelling variants which form 48,991 synonym classes (synsets). The core synonyms were taken from GermaNet and OpenThesaurus<ref type="foot" coords="3,215.92,484.77,3.49,6.05" target="#foot_0">1</ref> and filtered by several quality criteria; they cover 24,619 word senses of non-compound words (simplicia). In German, nouns do not form multi-word units as in English; such groups rather combine into a single compound noun. Synsets for compounds were automatically computed from 920,429 compounds found in the QA@CLEF news corpus and the German Wikipedia by identifying compounds with synonymous parts. A total of 86,817 word senses for compound nouns were covered in this way. Some examples of discovered synonyms are 'Anrainerstaat' vs. 'Nachbarland' (neighboring country), 'Backsteinplastik' vs. 'Ziegelsteinskulptur' (brick sculpture) etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.3">Hypothesis construction</head><p>The following statistics on parse qualities of our parser for the AVE06 test set demonstrates the difficulties of constructing textual hypotheses for a highly inflecting language like German. As remarked in <ref type="bibr" coords="3,498.90,615.33,10.58,8.64" target="#b3">[4]</ref>, 93.3% of the questions in the AVE06 test set can be parsed with optimal quality, compared to only 64.7% of the (automatically constructed) textual hypotheses. Moreover parsing failed completely for 9.4% of the hypotheses, but only for 1.1% of the questions. These observations motivated the use of a different approach in MAVE, which generally avoids the construction of a textual hypothesis in favor of the direct construction of a logical hypothesis from the logical representations of the question and of the answer (see <ref type="bibr" coords="3,90.00,687.06,11.62,8.64" target="#b3">[4]</ref> for an example). Apart from avoiding parsing problems, the method has the additional advantage of being more precise. Consider a question like 'In which city is the Walk of Fame?' and the answer 'USA'. Constructing a textual hypothesis which expresses the full information given by question and answer can be difficult in such a case (e.g., 'The Walk of Fame is in the city (of) USA'.) A simpler hypothesis like 'The Walk of Fame is in USA', however, would drop the important information that the question asks for a city while the answer wrongly specifies a country. This kind of constraints is easily preserved if the representations of question and answer are combined into a hypothesis representation on the logical level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.4">Logic-based entailment test</head><p>In order to achieve a robust entailment test, the theorem prover of MAVE is embedded in a relaxation loop which drops part of the hypothesis literals when a proof of the current hypothesis fragment fails or can not been found within the time limit. By subsequently removing 'critical' literals from the query, this process always finds a (possibly empty) query fragment provable from the assumed knowledge. The number of skipped query literals not contained in the provable fragment is used as a numeric indicator synth-failed-literals of (non-)entailment strength which is robust against slight errors in the logical representation of hypothesis and supporting text, but also against a few gaps in knowledge modeling.</p><p>Resources used by the prover include: 8,464 relations covering nominalizations of verbs; 947 subordination relationships (most of them relating female forms of occupation terms (like 'Pilotin' -female pilot) to the male form of the occupation term (like 'Pilot'), which for purposes of answer-validation can be considered as the gender-neutral form rather than refering e.g. to male pilots. Finally there are 76 entailment relationships between adjectives kept from the first version of MAVE. Apart from these lexical-semantic relations, the background knowledge of MAVE comprises 109 implicative rules. These rules correspond to those already used in AVE06, with some restructuring due to changes in the lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.5">Fallback matching method</head><p>The logic-based test depends on semantic representations which are not available when the parser fails. To gain robustness against this case, an alternative matching-based method was implemented which determines the overlap between the concept and numerals contained in question and answer, and the concepts and numerals which occur in the supporting witness text. The number of concepts and numbers extracted from question and answer which cannot be matched with concepts and numbers in the representation of the snippet is stored in the indicator match-err-count. It should be remarked that the fallback matcher eliminates stopwords before attempting a match. Moreover the matching rules for numbers are rather liberal. Thus 'April' matches with '4' and vice versa. '1 828' matches with '1', with '828', and with '1.828' (assuming German number syntax). Moreover, English notation for numbers is accepted when it does not create ambiguity, e.g. '1.3' is correctly interpreted though it would normally be written as '1,3' in German. The simple matching method can also leverage lexical-semantic relations which serve for generating further options for the overlap check. It integrates all lexical-semantic resources also used by the prover, including the synonyms, but also some further resources: a larger list of 27,814 nominalization relationships, which also contains ambiguous cases<ref type="foot" coords="4,264.75,535.82,3.49,6.05" target="#foot_1">2</ref> as well as a list of 15,052 nominalizations of adjectives, like 'Kleinheit' (smallness) derived from 'klein' (small), which are not yet part of the logic-based subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.6">Determining entailment error levels</head><p>The validation and selection decision of MAVE is based on the computation of error levels. These include the logic-based synth-failed-literals count, the overlap-based match-err-count obtained by matching lexical concepts and numbers, and the following additional indicators for non-entailment.</p><p>• synth-proof-facticity: The facticity score of the proof, determined by collecting all involved entities and checking their facticity. A score of 2 signals that the proof involved a nonexisting entity or non-real event; a score of 1 signals that the proof involved a hypothetical entity with unknown facticity (this happens e.g. with certain modal embeddings).</p><p>• synth-dropped-names: The number of full person names (i.e. specifying first name and last name) mentioned in the question or answer string for which only the last name occurs in the witness text, but not the first name. In this case, the first name is eliminated from the logical hy-pothesis so that it does not prevent a logical proof, and the error is reported by incrementing the synth-dropped-names count.</p><p>• synth-name-conflicts: The number of person names in the logical hypothesis such that the last name of the queried person coincides with the last name of a person mentioned in the witness text, but the first name differs. This means that the text is likely about a different person which happens to share the last name with the person mentioned in the query, an indication of non-entailment.</p><p>• synth-missing-constraints: The number of numerals in the question and answer string which were lost in the semantic representation due to parsing errors.</p><p>• synth-nonbound-focus: This binary feature signals if the focus variable (i.e. the variable in the logical hypothesis which represents the queried information) was actually bound during the relaxation proof. If all hypothesis literals in which the focus variable occurs are skipped, the result is 1, which strongly indicates that an entailment relationship does not hold.</p><p>• synth-nonbound-vars: This novel feature counts the number of variables of the logical hypothesis which were not bound to entities in the representation of the witness text by the relaxation proof. Since none of the literals connecting such a variable to the remaining query could be proved, this indicates a serious problem concerning entailment.</p><p>When question, answer and witness text can be parsed, then the logic-based error level synth-err-count is defined as follows:</p><p>synth-err-count = synth-name-conflicts + synth-dropped-names + min(synth-failed-literals + synth-missing-constraints</p><formula xml:id="formula_0" coords="5,149.51,404.25,273.86,23.68">+ synth-proof-facticity + 2 synth-nonbound-focus + synth-nonbound-vars, 3) + match-err-count .</formula><p>Otherwise synth-err-count is undefined and match-err-count will serve as a fallback replacement for the logical entailment test which is robust against parsing failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.7">Computing error probabilities for individual validation items</head><p>In order make the logic-based synth-err-count levels and the fallback match-err-count levels comparable, we abstract from error levels by assigning corresponding error probabilities. Thus, what we are really interested in is not the concrete error count but rather the probability that the validation item is correct (or wrong) given this error count. For convenience, we assume that the failure probability can be expressed in the form min(αe -β , 1) where α, β ∈ R + 0 and is the error level. Using synth-err-count for , we then obtain the probability estimate synth-err-prob describing the failure probability of the considered validation item judging from its witness text in isolation. Similarly, using match-err-count for (with a different error model of α , β ) results in a fallback failure probability match-err-prob. A method for extracting the parameters α, β from the synth-err-count and match-err-count levels in an annotated validation set is described in <ref type="bibr" coords="5,292.11,605.26,10.58,8.64" target="#b3">[4]</ref>. There are many conceivable ways how these failure probabilities might be combined but for the time being MAVE makes an optimistic choice by selecting the minimum of the available values, i.e. err-prob = min(synth-err-prob, match-err-prob) if synth-err-count is defined, and err-prob = match-err-prob otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.8">Aggregation of evidence for clusters of validation items</head><p>In the next step, aggregation is used in order to compute the combined justification of an answer judging from all validation items which support the answer or a variant of it. A simplification function σ can be used for assigning a simplified answer string or 'cluster key' to each answer (see <ref type="bibr" coords="5,387.27,710.10,10.45,8.64" target="#b3">[4]</ref>), and validation items which share the same answer key are then grouped together. However, some configurations of MAVE also use the identity for σ; in this case aggregation is only possible for validation items which support identical answers. Generally speaking, the answer is logically justified if it is logically justified from at least one witness text, i.e. it is false only if all validation items in the cluster are false. Assuming independence of the validation items, we can then express the combined failure probability as mult-err-prob = c err-prob c , where c ranges over all validation items which share the answer key with the validation item of interest. However, independence is likely a wrong assumption in this context, and tends to underestimate the actual error. Therefore a less optimistic combination like min-err-prob = min c err-prob c makes sense. Moreover, since the validation item of interest must eventually be presented to the user as a convincing justification of the answer, its immediate failure probability err-prob should also be considered. Since the proper way of combining these probability estimates is not known, a simple average combined-err-prob = (err-prob + mult-err-prob + min-err-prob)/3 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.9">Judging witness quality</head><p>The basic quality of the witness is computed from the correctness probability (1-combined-err-prob) and a heuristic quality factor describing non-aggregable preferences on 'good' supporting text passages like high parsing quality, conciseness etc. Using the same set of heuristic features and weights as in <ref type="bibr" coords="6,471.24,276.41,10.58,8.64" target="#b3">[4]</ref>, we let wn-heuristic-quality = c(0.2, wn-occurrences) • c(0.2, wn-parse-quality)</p><p>• c(0.2, producer-score) • c(0.1, wn-num-sentences) • c(0.1, wn-num-chars)</p><p>• c(0.3, wn-special-chars) • c(0.2, wn-relativizing-words) • c(0.2, wn-qn-focusing).</p><p>The weighting function c(w, x) = 1 -w + wx adjusts the relative impact of each criterion on the final quality score. The aggregated logical justification and the heuristic criteria are then combined into the basic witness score wn-quality = wn-heuristic-quality • (1combined-err-prob).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.10">False positive tests and heuristic answer quality</head><p>Capturing witness quality is not sufficient for answer validation since there are cases when an answer must be rejected even if it is perfectly justified from the logical perspective. A typical example are trivial answers to questions 'Who is X?' which simply repeat X. MAVE therefore supports a number of false-positive checks. These criteria do not depend on the supporting text passages and are therefore not aggregable.</p><p>• aw-not-trivial: eliminates trivial answers. The feature is now defined as a disjunction of a logic-based triviality test which checks if the answer be proved from the question, and a matchingbased overlap test which checks if all lexical constants (or alternatives linked by lexical-semantic relations) and numerals that occur in the answer are also found in the question.</p><p>• aw-significant-def: This test for informative answers to a definition question is based on the observation that isolated nomina agentis (like 'the founder' compared to 'the founder of Microsoft') or isolated role terms (like 'the foreign minister' rather than 'the German foreign minister', or 'the brother' rather than 'Wladimir Klitschko's brother') are hardly suitable for answering 'Who is X?' style definition questions. MAVE uses a list of 2,856 nomina agentis (i.e. nouns denoting an agent of a verb) and relational nouns in order to recognize such cases. If the considered question is a definition question, then the lexical constants from this list will be deleted from the list of question concepts and the overlap-based triviality test is applied to the reduced list of question concepts. Thus an answer is penalized if it contains only nomina agentis and relational terms and no additional content beyond that already mentioned in the question.</p><p>• aw-not-circular: eliminates circular answers. The circularity test of <ref type="bibr" coords="6,424.05,643.88,11.62,8.64" target="#b3">[4]</ref> was changed such that it only applies to constituent-level answers (not to full-sentence answers which repeat part of the question). Thus, 'The inventor of the modern car is Carl Benz' is a legal answer to the question 'Who is the inventor of the car?', while 'the inventor of the modern car' is not.</p><p>• aw-eat-fat-compat: Checks if the expected answer type of the question and the found answer type of the answer are compatible. In order to preserve recall levels, the filter was designed to reject only answers which are clearly wrong with respect to the answer type criterion while unclear cases are not eliminated. The filter is implemented by a hand-coded decision tree abstracted from known annotations of QA@CLEF questions and answers for the years 2004-2006.</p><p>Apart from these sanity checks, there are also heuristic preferences on 'good' answers, encoded by the features aw-incompleteness (completeness of the answer as a function of its length), aw-overlength (penalty for overlong answers), and aw-parse-quality (the parse quality of the answer), see <ref type="bibr" coords="7,481.09,136.25,10.58,8.64" target="#b3">[4]</ref>. Note that variants of the same answer (i.e. answers with the same cluster key and thus the same 'content') can differ with respect to parse quality. This means that the answer quality factor relates to exact answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.11">Computing the validation score</head><p>The final validation score used for selection and validation decisions is given by validation-score = aw-heuristic-quality • (wn-quality + bonus-wn-quality)/2 ,</p><p>where wn-quality refers to the considered validation item and bonus-wn-quality is the maximal wn-quality achieved by any original or generated validation item in the enhanced validation set which supports the given exact answer (when using the ERA method) or a cluster-key variant of the answer (when using cluster variants). Using the bonus term makes sense because the number of 'unsupported' cases (i.e. correct answer though not entailed by the snippet) can almost be neglected compared to the number of wrong answers. Therefore correctness of an answer, as judged from the existence of a very good supporting text passage in the original validation set or actively found in the document collection, is also a strong indicator for validity of the considered validation item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.12">Applying thresholds for the selection and validation decision</head><p>The validation-score is used as the metric for selecting the best answer in the AVE07 validation set (candidate for SELECTED or REJECTED) and also as the basis for validation/rejection of the remaining answers (candidates for VALIDATED or REJECTED). For factual questions, there is often only one correct answer (e.g. 'In welchem Alter starb Elvis Presley?', En: 'At which age did Elvis Presley die?'). As soon as the best answer has been found, alternative answers should only be accepted if they are 'really convincing'. To achieve this, MAVE uses separate thresholds f-sel-thresh ≤ f-val-thresh in its decision rules for accepting/rejecting the best answer candidate and the remaining alternatives. The best validation item for a given question (i.e. the item in the test set which maximizes validation-score) is predicted SELECTED if validation-score ≥ f-sel-thresh, and REJECTED otherwise. If the best official witness was SELECTED and the validation-score of a non-best answer exceeds f-val-thresh, then the item is predicted VALIDATED, otherwise it is predicted REJECTED. The thresholds f-val-thresh and f-sel-thresh are determined from the development set. Two ways of optimizing the thresholds were considered. If the system targets at maximizing the f-measure (signaled by letter 'F' in the later experiments), then thresholds are optimized by considering all levels of validation-scores in the development set and selecting the two thresholds f-sel-thresh for the best validation item and f-val-thresh ≥ f-sel-thresh for the non-best items which maximize the f-measure over the development set. If emphasis is placed on optimizing successful selection, however (signaled by 'Q' for 'qa-accuracy' in later runs), then we let f-sel-thresh = 0, which forces the best answer to a question to be chosen in every case. Using f-sel-thresh = 0 for selection, f-val-thresh is then optimized for non-best answers such as to maximize the overall f-measure on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.13">Assigning confidence scores</head><p>For computing confidence scores, MAVE first extracts two more thresholds p-sel-thresh (for the best validation item) and p-val-thresh (for the remaining alternatives) which are chosen to maximize the likelihood of correct decision (i.e. the 'accuracy') on the assumed development set. Every validation item is associated a 'signature' (tv, bv, tpa, bpa) where tv = SELECTED|VALIDATED|REJECTED ('this value') is the predicted value for the validation item; bv = SELECTED|REJECTED ('best value') is the predicted value of the best 'official' validation item for the considered question (i.e. item in the original AVE07 test set); the value is obtained by applying the f-sel-thresh to the best item as explained above; tpa = YES|NO ('this p-accept') is the acceptance decision obtained by applying p-sel-thresh or p-val-thresh to the validation score of the current validation item (depending on whether the item is the best one or supports an potential alternative answer); bpa = YES|NO ('best p-accept') is the acceptance decision obtained by applying p-sel-thresh to the best witness for the question. Now suppose that every validation item in the AVE07 development set has been associated a signature (tv, bv, tpa, bpa). Items which share the same signature are grouped into classes. For each class, MAVE determines the empirical correctness probability p(c), i.e. the relative frequency that the item is either predicted SELECTED/VALIDATED and annotated VALIDATED, or predicted REJECTED and annotated REJECTED, considering only validation items which are not annotated UNKNOWN. After that, classes are arranged in ascending order of probability, i.e. c 1 , c 2 , . . . , c m such that p(c 1 ) ≤ • • • ≤ p(c m ). For each class, the 'radius' r(c) is determined as follows:</p><formula xml:id="formula_1" coords="8,152.48,311.20,291.37,35.53">r(c) =    (p(c 2 ) -p(c 1 ))/2 : c = c 1 min(p(c i ) -p(c i-1 ), p(c i+1 ) -p(c i ))/2 : c = c 1 , c = c m (p(c m ) -p(c m-1 ))/2 : c = c m</formula><p>Now let n(c i ) be the number of elements from the test set mapped to class c i . If tv = SELECTED or tv = VALIDATED, then a validation item with a high validation score is more reliable. We use this to improve the coarse ranking given by the empiricial correctness probabiliy of the class. Hence suppose that the elements are arranged in increasing order of validation score, i.e. e 1 , . . . , e n(ci) with validation-score(e k ) ≤ validation-score(e ) for k ≤ . We then associate with e j the confidence score C(e j ) = p(c i ) -r(c i ) + 2 j-1 n(ci)-1 r(c i ) or in the case of a singleton e 1 , C(e j ) = p(c i ). Thus, answers with the highest validation-score are also judged the most reliable within their signature class. If tv = REJECTED, by contrast, then a witness with a low validation score is a more reliable negative example. Therefore the elements e 1 , . . . , e n(ci) are arranged in decreasing order of validation score in this case, so that the elements with the highest validation scores will be assigned the least confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation</head><p>This section describes the two configurations of MAVE used for generating the runs submitted to AVE07. It then discusses the performance scores achieved by these configurations and presents a series of ablation studies which elucidate the effect of the main system components.</p><p>The following naming conventions are used for describing configurations of MAVE. The letter 'C' indicates the use of simplified answer strings (also dubbed 'cluster keys') for clustering answers. In this case, active enhancement of the validation set will add all validation items found by the external QA systems which support an exact answer in the original test set or a cluster-key variant of it. Use of the exact answer string for grouping answers is marked by 'N' for 'non-clustering' (when ERA is not used) or 'E', when ERA is used for enhancing the test set. Use of the plain validation items without active enhancement of validation sets is marked by 'P'. The letter 'F' indicates optimization of thresholds for maximizing f-measure on the development set while 'Q' indicates optimization of the thresholds for maximizing qaaccuracy and selection performance on the development set.</p><p>Following these conventions, the first run submitted to the AVE07 is named CF, i.e. it uses the clusterkey for grouping answers and optimizes thresholds for f-measure. Parameter estimation is based on the development set enhanced by cluster key variants which were filtered from earlier runs of the InSicht, QAP and MIRA QA systems on the CLEF questions <ref type="bibr" coords="8,285.43,703.24,10.58,8.64" target="#b3">[4]</ref>. In this way, the 504 validation items in the original AVE07 development set were extended by another 907 supporting items contributed by our QA systems. Similarly, the original AVE07 test set with its 282 validation items was extended by 1,386 additional supporting items filtered from candidate lists of InSicht, QAP and MIRA. The second submitted run (EQ) uses the ERA method both for enhancing the development set, which affects the determined error model and thresholds, and for extending the test set. Thresholds were optimized for qa-accuracy in order to achieve a high percentage of correct selections. Since ERA handles answer variants by reassignment of validation items, the actual aggregation (i.e. computation of mult-err-prob and min-err-prob) must be limited to validation items which support the same exact answer. The same applies to determining bonus-wn-quality where support for identical answers is required. In order to enhance the validation set using the ERA method, the four methods for spotting answer-answer relationships were applied to the total of 12,837 answer candidates with 30,432 supporting text passages generated by the InSicht, QAP and MIRA QA systems for the considered questions. This process resulted in 2,320 supporting items being added to the original test set.</p><p>The results obtained for the two submitted runs are shown in Table <ref type="table" coords="9,372.46,437.86,3.74,8.64" target="#tab_0">1</ref>. Here and in the following tables, 'f-meas' denotes the f-measure, 'f-gain' is the increase in f-measure compared to the 100% YES baseline <ref type="bibr" coords="9,90.00,461.77,10.58,8.64" target="#b6">[7]</ref>, 'prec' is precision, 'qa-acc' is qa-accuracy, i.e. the number of correct selections divided by the number of all questions, and 'sel-rate ' or selection rate is the relative performance of the system compared to optimal selection, i.e. the number of correct selections divided by the number of questions with at least one correct answer in the annotated test set. The results obtained, in particular the 93 percent selection rate in the second run, confirm the suitability of the proposed method for boosting answer selection performance. Moreover the system can also handle the basic validation task, as shown by the achieved f-measure scores around 70 percent.</p><p>In order to find out which of the proposed techniques contribute most to these results, a series of ablation experiments will now be presented. However, it makes more sense to relate these experiments to the current version of MAVE, which differs from the AVE07 system in two respects. First, a bug in the answer-type filter has been fixed in the meantime, which resulted in misclassification of COUNT/MEASURE questions. The second change is related to the ERA method. It was noticed that evidence reassignment might result in a disbalance of training examples when applied to the development set: In the extreme case, the method might add a large number of additional supporting items for only one of the answers in the validation set. To avoid the risk of disbalance, application of ERA has now been restricted to the test set, while the error model (i.e. the α, β parameters for determining synth-err-prob and match-err-prob) and the thresholds are determined from the original AVE07 development set enhanced by all validation items from the InSicht, QAP and MIRA runs which literally support the considered answer. <ref type="foot" coords="9,409.04,663.34,3.49,6.05" target="#foot_2">3</ref>For the current version of MAVE, we then obtain the reference results shown in Table <ref type="table" coords="9,448.43,676.97,3.74,8.64">2</ref>. Fixing the error in the answer-type filter and changing the parameter selection for the ERA runs has (slighly) improved overall performance, resulting in better qa-accuracy and selection rate for the f-measure oriented run, and improved f-measure in the qa-accuracy oriented run. In the following ablation studies, the reference con- The first experiment is concerned with the effect of active validation, i.e. of enhancing the test set or training set. In the ablation runs shown in Table <ref type="table" coords="10,281.89,378.49,3.74,8.64" target="#tab_1">3</ref>, the 'plain' development set and validation set was used, i.e. no additional supporting text passages were added. The models labeled 'N' do not use cluster-key simplification, i.e. only identical answers are grouped, which eliminates any opportunity for aggregation because the AVE07 development set and test set are free of answer redundancy. Compared to the EF* reference, the PNF run shows 5 percent difference in the f-measure and even 17 percent points loss in selection rate. The selection-oriented PNQ run loses 3 percent of f-measure and 4 percent of selection rate. These findings reveal a strong positive effect of active validation. It was also tried to use the cluster-key to group answer variants for aggregation (see runs labeled 'C'). This had no effect on results, however, since answer variants which share the same cluster key are not present in the AVE test set.</p><p>Finally it was tested if the decrease in validation and selection performence results from the smaller size of the development set (compared to the actively enhanced variant), which might result in less reliable probability estimates. The runs PCF+ and PCQ+ therefore used the development corpus enhanced by cluster-key variants for estimating parameters, while using the original test set without enhancements. As witnessed e.g. by the selection rate of PCF+, which is still very low, the smaller size of the development set when not using active enhancement is not responsible for the observed drop of selection rate. Thus active validation mainly profits from the additional supporting text passages in the enhanced test set and not so much from the more reliable parameter estimates due to increased size of the development collection.</p><p>Next let us consider the use of separate thresholds for selection/rejection of the best validation item and validation/rejection of alternative answers; see Table <ref type="table" coords="10,301.34,593.68,3.74,8.64" target="#tab_2">4</ref>. Both runs are based on the ERA method. EJF uses a joint threshold for selection and validation chosen to maximize f-measure on the development set, while EJQ uses a joint threshold chosen to maximize qa-accuracy on the development set. Compared to the EF* reference, we notice a loss of f-measure by 5 percent and a loss of selection rate by 7 percent in the EJF case. The EJQ variant which optimizes qa-accuracy still shows a selection rate of 0.93, but suffers a drastic loss of f-measure by 24 percent points compared to the EQ* reference. Thus, using separate thresholds for selecting the best answer and for accepting alternative answers pays off in the AVE context.</p><p>The next series of experiments is concerned with the effect of the various sanity checks used by MAVE. The results shown in Table <ref type="table" coords="10,202.08,689.32,4.98,8.64" target="#tab_3">5</ref> should be compared to the EF* and EQ* reference. The following naming scheme is used: 'A' denotes deactivation of the answer-type filter (aw-eat-fat-compat), 'T' means deactivation of the trivial answer filter (aw-not-trivial), 'S' means deactivation of the significant answer filter for definition questions (aw-significant-def), 'Z' means deactivation of the circular answer filter (aw-not-circular), and * means deactivation of all of the above sanity checks. Though none of these filters applies very often, they all show a consistent positive effect on the performance of MAVE. When deactivating all filters, f-measure drops by 5 percent points comparing E*F to EF* (or 4 percent points comparing E*Q to EQ*). Selection rate too drops by 3 percent compared to the reference. Due to the overlap of the 'T' and 'S' filters, simultaneous deactivation of both filters was also tested. As shown by the ESTF run, these filters contribute up to 4 percent points to the achieved f-measure compared to EF*, and 3 percent in the EQ* case. This means that the two variants of triviality filtering contribute most to the overall effect of sanity checking. Finally let us assess the effect of using a theorem prover and logical rules compared to the fallback matching method, and the effect of the lexical-semantic relations compared to using no background knowledge at all.</p><p>The introduction of the fallback matching method is justified by the following statistics: due to parsing failures, the proof-based validation feature synth-err-count was defined only for 168 of the 282 validation items in the original AVE07 test set; for 1,223 of the 1,668 validation items using cluster-key variants, and for 1,840 of the 2,602 validation items in the ERA-enhanced test set. The fallback method thus helps to better exploit the available evidence. In order to assess the quality of the fallback method, a number of experiments was conducted with the theorem prover switched off. The naming scheme for the ablation runs shown in Table <ref type="table" coords="11,222.37,655.87,4.98,8.64" target="#tab_4">6</ref> is as follows: 'L' indicates that all logic-based features are deactivated, while 'K' disables not only the prover, but also the use of any lexical-semantic relations in the fallback matching method. Notice that disabling the prover affects several features which involve the use of logic: the synth-err-count (which must be totally disabled), the triviality check (which is then determined by the fallback matching method only), the logic-based circularity test (which is not backed by a matching method and thus entirely switched off). Moreover the 'E' runs based on ERA use only the answer-answer analyzers which do not involve the use of logic (clustering, matching and sequential matching) but disable the logical inclusion test. The 'C' runs group answers by cluster-key as usual. Surprisingly, the performance of the f-measure oriented LEF run shows only a minimal decrease compared to the f-measure (1 percent point loss) and selection rate (2 percent points loss) of the EF* reference. For LEQ, the picture is similar, with 1 percent point loss compared to EQ* both for f-measure and selection rate. The effect of robust logical inference is positive but very small.</p><p>The picture changes when disabling not only the use of the prover, but also the use of lexical-semantic relations for fallback matching, as shown by KCF vs. CF* with a 17 percent difference in f-measure and 13 percent difference in the selection rate. Comparing KCQ to CQ*, f-measure decreases by 15 percent points while selection-rate loses 14 percent points. These findings demonstrate that lexical-semantic knowledge now makes a very strong contribution to the performance of MAVE. In particular, the massive extension of lexical-semantic knowledge compared to the first prototype pays off (no clear effect was found in earlier experiments based on a small repository of lexical-semantic relations <ref type="bibr" coords="12,367.78,400.04,10.45,8.64" target="#b2">[3]</ref>).</p><p>A few additional tests were made in order to better understand the success of the simple overlap-based method. Questions to be clarified include: Does redundancy explain the results of the fallback method? Does it profit too much from the structure-sensitive answer-type filter which involves some use of the prover for graph matching (but no background knowledge or lexical-semantic knowledge)?</p><p>Table <ref type="table" coords="12,129.30,459.82,4.98,8.64" target="#tab_5">7</ref> uses the letters 'L' (no logic), 'P' (no active enhancement of development and test corpus), 'C' (group answers by cluster keys), 'E' (evidence reassignment, in this case without logical inclusion recognizer), 'N' (no clustering except for exact answer strings), and 'A' (no answer-type filter); 'Q' and 'F' have the usual meaning. Comparing LCAF to LCF, LEAF to LEF, LCAQ to LCQ and LEAQ to LEQ, we notice that deactivating the answer-type filter has a minor effect on the achieved f-measure (at most 1 percent points loss) and selection rate (at most 2 percent points loss). This means that the success of the simple matching method is not explained by the use of a structure-sensitive answer-type filter. Concerning the effect of redundancy, we notice a difference of 7 percent in f-measure comparing PLCF (no redundancy) with LCF (which uses an enhanced validation set). Selection rate even drops by 15 percent points. A similar pattern becomes visible comparing PLNF and LEF, with a 7 percent points loss in f-measure and a 16 percent loss in selection rate when there is no redundancy. There is also a clear effect when optimizing thresholds for qa-accuracy. Thus, the success of the fallback matching method relies strongly on redundancy created by active enhancement of validation sets. However, the same dependency was observed with the combined method of using logic-based and matching-based information, cf. Table <ref type="table" coords="12,437.61,615.24,3.74,8.64" target="#tab_1">3</ref>. This means that the results of MAVE in its current configuration are mainly due to the active enhancement of the validation sets.</p><p>Turning to the ERA enhancement method, we consider the effect of different methods for spotting answer-answer relationships as the basis for evidence reassignment. Four methods were implemented for MAVE: variant answers which share the same cluster key (labeled 'clust' below); matching of an answer with a given one using the fallback matching method (labeled 'match' below); sequential matching of the tested answer with a given one (labeled 'seqm' below) where the word senses and numeric expressions must find matching expressions in the proper sequental order; and finally the logical inclusion test (labeled 'incl'). MAVE normally applies all four methods for spotting relationships between answers. In order to assess the relative merits of each technique, experiments using just one of these methods were conducted. As shown by these experiments, results for the four methods are very similar. Thus for the AVE07 data, shallow and deep methods for recognizing answer-answer relationships perform about the same. This again illustrates that axioms and inference are either not needed yet (due to simplicity of the test cases or due to redundancy created by active enhancement of the validation set); or that axioms are needed but not available at this time so that their potential can not yet be fully demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>The proposed changes the basic MAVE system have boosted f-measure beyond 70 percent and selection rate even into the 90 percent range. As shown by the ablation studies, these improvements can be attributed to the following methods: a) active enhancement of the validation set by additional supporting text passages generated by external QA systems; b) clustering of validation items for answer variants or use of the more flexible ERA method which offers a sound solution for leveraging inclusion of answers in an answer validation framework. In order to treat non-local and non-monotonic phenomena consistently, ERA handles inclusions of answers by reassigning evidence to all compatible validation items; c) integration of a large repository of lexical-semantic relations; d) use of various sanity checks for eliminating false positives; e) use of separate thresholds for the selection of the best answer and for acceptance/rejection of the remaining alternatives; and finally f) provision of robust fallback solutions for the main logic-based features. The excellent results of these fallback methods which do not involve any theorem proving or structural matching (only one percent loss in performance when the prover is switched off) are somewhat irritating. Obviously the AVE task which involves only a few end results of QA systems means a simplification and results might look different in an answer selection setting where the system must select from the top k answers for k 1. Moreover the QA systems represented in the AVE07 test set for German might have used some structure-sensitive validation themselves so that applying a similar validation method once again is no longer effective. In order to clarify these issues, future work will include experiments with variants of the robust inference method and a comparison with overlap methods and approximate graph matching. The first change to MAVE will be leveraging machine learning, however, which will replace the hand-coded formulas for the main validation decision and also for the answer-type check.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,104.94,148.20,275.92,8.64;7,93.46,169.34,377.79,8.74;7,115.59,184.28,393.95,8.74;7,115.59,199.22,266.33,8.74"><head></head><label></label><figDesc>The false-positive tests and soft preferences are combined as follows, aw-heuristic-quality = c(0.1, aw-incompleteness) • c(0.1, aw-overlength) • c(0.2, aw-parse-quality) • c(1.0, aw-not-trivial) • c(0.5, aw-significant-def) • c(0.6, aw-not-circular) • c(1.0, aw-eat-fat-compat) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,176.64,119.13,249.71,164.38"><head>Table 1 :</head><label>1</label><figDesc>Results of the two runs submitted to the AVE07</figDesc><table coords="9,176.64,138.60,249.71,144.91"><row><cell>model</cell><cell cols="5">f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell cols="2">CF/Run1 0.72</cell><cell>0.79</cell><cell>0.61 0.90</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell cols="2">EQ/Run2 0.68</cell><cell>0.69</cell><cell>0.54 0.94</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell cols="6">Table 2: Reference results for the ablation experiments</cell></row><row><cell cols="6">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>CF*</cell><cell>0.73</cell><cell>0.81</cell><cell>0.62 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>CQ*</cell><cell>0.70</cell><cell>0.74</cell><cell>0.56 0.94</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>EF*</cell><cell>0.73</cell><cell>0.82</cell><cell>0.62 0.91</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>EQ*</cell><cell>0.69</cell><cell>0.71</cell><cell>0.55 0.93</cell><cell>0.50</cell><cell>0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,169.44,119.13,264.13,100.63"><head>Table 3 :</head><label>3</label><figDesc>Results of MAVE without enhancement of validation sets</figDesc><table coords="10,182.97,138.60,237.07,81.17"><row><cell cols="6">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>PNF</cell><cell>0.68</cell><cell>0.67</cell><cell>0.61 0.76</cell><cell>0.41</cell><cell>0.75</cell></row><row><cell>PCF</cell><cell>0.68</cell><cell>0.67</cell><cell>0.61 0.76</cell><cell>0.41</cell><cell>0.75</cell></row><row><cell cols="2">PCF+ 0.68</cell><cell>0.68</cell><cell>0.60 0.78</cell><cell>0.42</cell><cell>0.77</cell></row><row><cell>PNQ</cell><cell>0.66</cell><cell>0.63</cell><cell>0.53 0.88</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell>PCQ</cell><cell>0.66</cell><cell>0.63</cell><cell>0.53 0.88</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell cols="2">PCQ+ 0.67</cell><cell>0.66</cell><cell>0.54 0.90</cell><cell>0.48</cell><cell>0.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,90.00,253.39,423.00,109.82"><head>Table 4 :</head><label>4</label><figDesc>Results of MAVE using a joint threshold for selection and validation the table will be varied by switching off one or more functional components of MAVE and observing the changes of obtained results.</figDesc><table coords="10,90.00,274.87,329.99,76.38"><row><cell cols="6">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>EJF</cell><cell>0.68</cell><cell>0.68</cell><cell>0.55 0.88</cell><cell>0.46</cell><cell>0.85</cell></row><row><cell>EJQ</cell><cell>0.45</cell><cell>0.11</cell><cell>0.29 0.97</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>figurations shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,183.01,119.13,236.98,174.38"><head>Table 5 :</head><label>5</label><figDesc>Results of MAVE without false-positive tests</figDesc><table coords="11,183.01,140.62,236.98,152.90"><row><cell cols="6">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>EAF</cell><cell>0.73</cell><cell>0.79</cell><cell>0.60 0.91</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>ETF</cell><cell>0.71</cell><cell>0.74</cell><cell>0.58 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>ESF</cell><cell>0.73</cell><cell>0.80</cell><cell>0.61 0.91</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell cols="2">ESTF 0.69</cell><cell>0.70</cell><cell>0.56 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>ECF</cell><cell>0.73</cell><cell>0.82</cell><cell>0.62 0.91</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>E*F</cell><cell>0.68</cell><cell>0.68</cell><cell>0.55 0.90</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell>EAQ</cell><cell>0.69</cell><cell>0.69</cell><cell>0.54 0.93</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>ETQ</cell><cell>0.67</cell><cell>0.66</cell><cell>0.54 0.91</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>ESQ</cell><cell>0.69</cell><cell>0.70</cell><cell>0.55 0.93</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell cols="2">ESTQ 0.66</cell><cell>0.63</cell><cell>0.52 0.91</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>EZQ</cell><cell>0.69</cell><cell>0.71</cell><cell>0.55 0.93</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>E*Q</cell><cell>0.65</cell><cell>0.61</cell><cell>0.51 0.91</cell><cell>0.49</cell><cell>0.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,178.86,327.14,245.28,102.65"><head>Table 6 :</head><label>6</label><figDesc>Results of MAVE without using logic-based features</figDesc><table coords="11,183.01,348.62,236.98,81.17"><row><cell cols="6">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>LCF</cell><cell>0.72</cell><cell>0.78</cell><cell>0.59 0.93</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell>LEF</cell><cell>0.72</cell><cell>0.79</cell><cell>0.59 0.94</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>KCF</cell><cell>0.56</cell><cell>0.39</cell><cell>0.44 0.78</cell><cell>0.42</cell><cell>0.77</cell></row><row><cell>LCQ</cell><cell>0.68</cell><cell>0.69</cell><cell>0.53 0.96</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>LEQ</cell><cell>0.68</cell><cell>0.68</cell><cell>0.53 0.96</cell><cell>0.50</cell><cell>0.92</cell></row><row><cell>KCQ</cell><cell>0.55</cell><cell>0.37</cell><cell>0.43 0.79</cell><cell>0.42</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,159.63,119.13,283.75,126.56"><head>Table 7 :</head><label>7</label><figDesc>Additional tests of MAVE with the logical prover switched off</figDesc><table coords="12,180.61,140.62,241.78,105.08"><row><cell cols="5">model f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>LCAF 0.71</cell><cell>0.76</cell><cell>0.58 0.93</cell><cell>0.47</cell><cell>0.87</cell></row><row><cell>LEAF 0.72</cell><cell>0.77</cell><cell>0.58 0.94</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell>PLCF 0.65</cell><cell>0.62</cell><cell>0.57 0.76</cell><cell>0.40</cell><cell>0.74</cell></row><row><cell>PLNF 0.65</cell><cell>0.62</cell><cell>0.57 0.76</cell><cell>0.40</cell><cell>0.74</cell></row><row><cell>LCAQ 0.68</cell><cell>0.67</cell><cell>0.52 0.96</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>LEAQ 0.67</cell><cell>0.66</cell><cell>0.52 0.96</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>PLCQ 0.64</cell><cell>0.58</cell><cell>0.51 0.87</cell><cell>0.46</cell><cell>0.85</cell></row><row><cell>PLNQ 0.64</cell><cell>0.58</cell><cell>0.51 0.87</cell><cell>0.46</cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,178.03,119.13,246.93,126.56"><head>Table 8 :</head><label>8</label><figDesc>Test of relationship-spotting methods for ERA</figDesc><table coords="13,178.03,140.62,246.93,105.08"><row><cell>model</cell><cell cols="5">f-meas f-gain prec recall qa-acc sel-rate</cell></row><row><cell>clust-F</cell><cell>0.73</cell><cell>0.81</cell><cell>0.62 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell cols="2">match-F 0.73</cell><cell>0.80</cell><cell>0.61 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>seqm-F</cell><cell>0.73</cell><cell>0.80</cell><cell>0.61 0.90</cell><cell>0.49</cell><cell>0.90</cell></row><row><cell>incl-F</cell><cell>0.73</cell><cell>0.80</cell><cell>0.62 0.88</cell><cell>0.48</cell><cell>0.89</cell></row><row><cell>clust-Q</cell><cell>0.70</cell><cell>0.72</cell><cell>0.56 0.93</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell cols="2">match-Q 0.69</cell><cell>0.71</cell><cell>0.55 0.93</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>seqm-Q</cell><cell>0.69</cell><cell>0.71</cell><cell>0.55 0.93</cell><cell>0.50</cell><cell>0.93</cell></row><row><cell>incl-Q</cell><cell>0.70</cell><cell>0.73</cell><cell>0.56 0.93</cell><cell>0.50</cell><cell>0.93</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,104.35,742.98,133.90,5.61"><p>http://www.openthesaurus.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,104.35,737.36,408.66,7.05;4,90.00,746.97,99.62,6.91"><p>e.g. anspannen.1.1 -'put (a cart) before a horse)' vs. anspannen.1.2 -'to strain', with only one available nominalization anspannung.1.1 at the moment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,104.35,732.00,408.65,7.05;9,90.00,741.61,286.57,6.91"><p>Another solution would be applying the ERA method to the development set as well, but avoiding disbalance by some weighting of examples which ensures that no single validation item dominates parameter estimation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,106.60,667.24,406.41,8.64;13,106.60,679.02,406.40,8.82;13,106.60,690.97,130.43,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,210.57,679.20,191.13,8.64">Learning to distinguish valid textual entailments</title>
		<author>
			<persName coords=""><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Rafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,420.28,679.02,92.72,8.59;13,106.60,690.97,100.86,8.59">Proc. of the 2nd Pascal RTE Challenge Workshop</title>
		<meeting>of the 2nd Pascal RTE Challenge Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,106.60,710.90,406.40,8.82;13,106.60,722.85,236.52,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,173.70,711.08,283.90,8.64">University of Hagen at QA@CLEF 2006: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,479.61,710.90,33.39,8.59;13,106.60,722.85,140.54,8.59">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.60,112.34,406.41,8.64;14,106.60,124.11,202.52,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,174.36,112.34,319.45,8.64">Filtering and fusion of question-answering streams by robust textual inference</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,106.60,124.11,98.30,8.59">Proceedings of KRAQ&apos;07</title>
		<meeting>KRAQ&apos;07<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.60,144.22,406.40,8.64;14,106.60,156.00,406.41,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,348.92,144.22,164.08,8.64;14,106.60,156.17,245.41,8.64">Logical validation, answer merging and witness selection: A study in multi-stream question answering</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,370.54,156.00,67.33,8.59">Proc. of RIAO-07</title>
		<meeting>of RIAO-07<address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.60,175.92,406.40,8.82;14,106.60,188.05,100.74,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,181.94,175.92,221.25,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.60,207.80,406.40,8.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,178.96,207.80,264.83,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,106.60,225.73,406.40,10.81;14,106.60,239.68,406.40,8.82;14,106.60,251.64,150.89,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,379.42,227.91,133.58,8.64;14,106.60,239.86,82.85,8.64">Testing the reasoning for question answering validation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,200.54,239.68,312.46,8.59;14,106.60,251.64,105.50,8.59">Journal of Logic and Computation, Special Issue on Natural Language and Knowledge Representation</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
