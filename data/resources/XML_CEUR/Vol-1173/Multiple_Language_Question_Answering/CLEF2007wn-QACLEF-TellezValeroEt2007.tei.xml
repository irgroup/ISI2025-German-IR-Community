<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.64,74.45,392.00,12.58">INAOE at AVE 2007: Experiments in Spanish Answer Validation</title>
				<funder ref="#_TNk2a9T">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder ref="#_rB9narF">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.80,103.44,91.20,9.02"><forename type="first">Alberto</forename><surname>Téllez-Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.74,103.44,106.14,9.02"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.45,103.44,97.99,9.02"><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.64,74.45,392.00,12.58">INAOE at AVE 2007: Experiments in Spanish Answer Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39F6B8549E5A00DE0F0CA239848D9960</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Answer Validation, Question Answering, Textual Entailment Recognition, and Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the INAOE's answer validation system evaluated at the Spanish track of the AVE 2007. This system is based on a supervised learning approach that considers two kinds of attributes. On the one hand, some attributes indicating the textual entailment between the given support text and the hypothesis constructed from the question and answer. On the other hand, some new features denoting certain answer restrictions as imposed by the question's type and format. In order to extract all these attributes the system uses different tools such as a lemmatizer, a POS tagger, a NER procedure and a superficial syntactic parser. Experimental results are encouraging; they show that the proposed system achieved a 52.91% of F-measure and that it outperformed the standard baseline by 15 percentage points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a question, a candidate answer and a support text, an answer validation system must decide whether accept or reject the candidate answer. In other words, it must determine if the specified answer is correct and supported.</p><p>Answer validation systems have been traditionally based on the idea of recognizing the textual entailment between the support text and an affirmative sentence (called hypothesis) created from the combination of the question and the answer. In order to accomplish this recognition they have probed several approaches, ranging from simple ones taking advantage of lexical overlaps to more complexes founded on the use of a logic representation <ref type="bibr" coords="1,70.92,553.89,10.66,8.74" target="#b5">[6]</ref>.</p><p>The approach based on lexical overlaps is quite simple, but surprisingly it has achieved very competitive results. Representative methods of this approach determine that H (the hypothesis) is entailed from T (the support text) only considering characteristics such as named entity overlaps <ref type="bibr" coords="1,347.78,588.39,10.65,8.74" target="#b7">[8]</ref>, n-gram overlaps <ref type="bibr" coords="1,433.67,588.39,10.65,8.74" target="#b3">[4]</ref>, as well as the size of the longest common subsequence (LCS) <ref type="bibr" coords="1,245.95,599.85,10.82,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,256.77,599.85,7.21,8.74" target="#b4">5]</ref>.</p><p>The simplicity is the strength of this approach, but also it is its weakness. For instance, <ref type="bibr" coords="1,440.74,611.37,11.70,8.74" target="#b7">[8]</ref> can easily recognize the textual entailment between "Lucy visits some friends" (H) and "Lucy goes with some friends" (T). However, it fails when there is a high word overlap between H and T, but there does not exist an entailment relation; for example between H and the text "Lucy has some wonderful friends". The method presented in <ref type="bibr" coords="1,480.33,645.87,11.68,8.74" target="#b3">[4]</ref> has the same problem. Nevertheless it is much restrictive on evaluating the overlap between H and T, and therefore it tends to produce better results.</p><p>The methods based on the use of LCSs <ref type="bibr" coords="1,244.22,680.37,11.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,255.74,680.37,7.68,8.74" target="#b4">5]</ref> are less sensible to the word overlap rate, but they are still very sensible to changes in the word order (like in the use of active and passive voices). For instance they will be unsuccessful in recognizing the entailment between "Lucy visits some friends" (H) and "Some friends are visited by Lucy" (T).</p><p>Finally, all overlap-based methods have problems to deal with situations where the answer does not satisfied simple type restrictions imposed by the question. For instance, in the example of Table <ref type="table" coords="2,421.32,84.99,3.76,8.74">1</ref>, the candidate answer is clearly incorrect, but it will be validated because the high lexical similarity between H and T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Incorrect answer validation using overlap-based methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>What is the world record in the high jump? Answer:</p><p>Javier Sotomayor Support text (T):</p><p>The world record in the high jump, obtained by Javier Sotomayor, is 2.45 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis (H):</head><p>Javier Sotomayor is the world record in the high jump</p><p>The system described in this paper adopts several ideas from recent systems (in particular from <ref type="bibr" coords="2,473.03,183.93,10.87,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,483.90,183.93,7.25,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,491.14,183.93,7.25,8.74" target="#b4">5]</ref>). It is based on a supervised learning approach that uses a combination of all previous used features (word overlaps and LCSs). In addition, it also includes some new characteristics that allow reducing previously discussed problems. In particular:</p><p>• It considers only content words for the calculus of word overlaps and the LCS. This represents a middle point between the usage of all words <ref type="bibr" coords="2,241.37,245.13,11.71,8.74" target="#b3">[4]</ref> and just named entities <ref type="bibr" coords="2,349.89,245.13,10.64,8.74" target="#b7">[8]</ref>. • It computes the LCS taking into consideration POS tags. This characteristic makes possible obtaining larger subsequences and also dealing with the synonymy phenomena. • It makes a simple syntactic transformation over the generated hypothesis in order to simulate the usage of active and passive voices. In other words, it generates two hypotheses (H and H´) that combine in a different way the given question and answer. The inclusion of this additional hypothesis improves in many cases the calculus of the LCS. • It uses some manually constructed lexical patterns to help treating support texts containing an apposition phrase. This idea was taken from the COGEX logic-based system <ref type="bibr" coords="2,362.41,339.27,10.65,8.74" target="#b8">[9]</ref>. Its goal is to make explicit the relation between the two elements of the apposition phase. • Finally, it includes some new features denoting certain answer restrictions as imposed by the question's class. This new features avoid validating answers that does not correspond to the expected semantic type of answer. The following sections give some details on the proposed system. In particular, section 2 describes the main characteristics of our system, whereas section 3 presents our evaluation results on the AVE 2007 Spanish track. Finally, section 4 discusses some general conclusions about the performance of the proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Answer Validation System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The main task of this initial phase is to construct two distinct hypotheses combining the given question and answer. In order to do that it firstly applies a superficial syntactic analysis over the question<ref type="foot" coords="3,436.44,102.14,3.00,5.23" target="#foot_0">1</ref> . Then, using the obtained syntactic tree, it generates both hypotheses. The first hypothesis (H) is constructed by replacing the nominal phrase that contains the interrogative particle by the given answer. For instance, given the question "How many inhabitants are there in Longyearbyen?" and the answer "180 millions of inhabitants", this approach allows generating the hypothesis H = "180 millions of inhabitants are there in Longyearbyen".</p><p>The second hypothesis (H´) is obtained doing a simple transformation on H. The idea is to detect the main verb phrase of the H (that is the main verb phrase of the question) and then interchange its surrounding nominal phrases. This way the second hypothesis for our example is H´ = "in Longyearbyen are there 180 millions of inhabitants".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>As we previously mentioned, our system considers two kinds of attributes. On the one hand, some attributes indicating the textual entailment between the support text and the constructed hypotheses. On the other hand, some new features that denote certain answer restrictions imposed by the question's type. These attributes are extracted by two different modules of our system, the textual entailment analysis and the question-answer analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Entailment Analysis</head><p>The textual entailment analysis of the pairs (T, H) and (T, H´) consists of two stages: (i) compute the term overlap, and (ii) calculate the term sequence overlap. In order to avoid a high matching caused by functional terms (such as prepositions and determiners), in both cases we only consider the occurrence of content terms (nouns, verbs, adjectives and adverbs). Besides, we use the word lemmas, which allow getting a better term overlap.</p><p>The term overlap between the pair (T, H) is computed by a simple counting of the common content words in the support text (T) and the hypothesis (H) <ref type="foot" coords="3,241.74,386.12,3.00,5.23" target="#foot_1">2</ref> . The following features are generated from this analysis:</p><p>(1) The rate of noun overlap between (T, H) (2) The rate of verb overlap between (T, H) (3) The rate of adjective overlap between (T, H) (4) The rate of adverb overlap between (T, H) <ref type="bibr" coords="3,85.14,449.97,11.68,8.74" target="#b4">(5)</ref> The rate of date overlap between (T, H) <ref type="bibr" coords="3,85.14,461.49,11.68,8.74" target="#b5">(6)</ref> The rate of number overlap between (T, H) Similar to <ref type="bibr" coords="3,124.63,478.95,11.50,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,136.13,478.95,7.67,8.74" target="#b4">5]</ref> we compute the term sequence overlap by extracting the longest common subsequence (LCS). However, different from these previous approaches, our method only considers the occurrence of content words and allows the inclusion of POS tags inside the sequence.</p><p>In this case, it is necessary to compute the LCS from (T, H) as well as from (T, H´). Nevertheless, only the longest subsequence is used. This way we generate the following feature from this analysis: <ref type="bibr" coords="3,85.14,542.49,11.68,8.74" target="#b6">(7)</ref> The size of the LCS between (T, H) or (T, H´) divided by the size of H It is important to remember that the presence of apposition phrases in the support text causes detriment on the LCS. In order to solve this problem we propose using some manually-constructed transformation patterns such as "The &lt;AGENT&gt;, &lt;AGENT&gt;, → The &lt;AGENT&gt; &lt;V&gt; &lt;AGENT&gt;".</p><p>The example of Table <ref type="table" coords="3,173.56,594.45,5.01,8.74">2</ref> illustrates the application of these patterns as well as the inclusion of POS tags inside the LCSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Answer Analysis</head><p>There are two common situations related with the presence of an incorrect answer. The first one is that the semantic class of the extracted answer does not correspond to the expected class of answer (in accordance to the given question). For instance, having the answer "yesterday" for the question "How many inhabitants are there in Longyearbyen?".</p><p>The second situation occurs when the question asks about a specific fact and the answer makes reference to another different one. For instance, answering "eight" to the example question, using as support text "…when eight animals parade by the principal street in Longyearbyen, a town of a thousand of inhabitants".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Application of transformation patterns and POS tags in the LCS calculus</head><p>Question:</p><p>What is the quinua? Answer:</p><p>Cereal Support text (T):</p><p>The quinua, an American cereal of great nutritional value, … Original analysis Hypotheses:</p><p>Cereal is the quinua (H), The quinua is cereal (H´) LCS (of size = 2):</p><p>Quinua cereal Using the transformation patterns Support Text (T´):</p><p>The quinua (V) an American cereal of great nutritional value … Hypotheses:</p><p>Cereal is the quinua (H), The quinua is cereal (H´) LCS (of size = 3): Quinua (V) cereal</p><p>Our system includes two new features that attempt to capture these situations: (8) A Boolean value indicating if a general-class restriction is satisfied, and (9) A Boolean value indicating if a specific-type restriction is satisfied</p><p>The general-class answer restriction is TRUE if the semantic class of the extracted answer and the expected class of the answer are equal; other case it is set to FALSE. We consider three general classes: quantity, date, and proper noun. The question classification (i.e., the definition of the expected class of the answer) is done using the KNN supervised algorithm 3 with K = 1.</p><p>In order to determine the specific target fact concerning the question it is necessary to perform the following procedure: (i) construct the syntactic tree of the question, and (ii) extract the principal noun from the noun phrase that contains the interrogative particle. Applying this procedure over the example question, the word "inhabitants" was selected as the specific target fact.</p><p>Once extracted the specific target fact from the question, it is possible to evaluate the specific-type answer restriction. Its value is set to TRUE if the specific target fact happens in the support text, in the immediate answer context (one content word to the right or left). In any other case its value is set to FALSE. Therefore, the candidate answer "eight" has its value set to FALSE since its immediate context ("eight animals") does not contains the noun "inhabitants". On the contrary, the candidate answer "thousand" will be have its value set to TRUE, since the noun "inhabitants" occurs in its immediate context ("town thousand inhabitants").</p><p>It is important to notice that not for all questions it is possible to establish a specific target fact (e.g., consider the question "When was Amintore Fanfani born?"). In these cases we considered -by default-that all candidate answers satisfied the specific-type restriction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>This final module generates the answer validation decision by means of a supervised learning approach, in particular, by a support vector machine classifier. This classifier decides if the answer is validated or rejected on the basis of the nine previously described features along with the following two additional ones: <ref type="bibr" coords="4,85.14,525.99,16.71,8.74" target="#b9">(10)</ref> The question category (i.e., factoid, definition, or list) (11) The question interrogative particle (i.e., who, where, when, etc.)</p><p>An evaluation of the proposed features during the development phase, using the information gain algorithm, shows us that the nouns overlap and the LCS size are the most discriminative features. The general ranking of the eleven features in decreasing order is as follows: 1, 7, 6, 11, 10, 8, 2, 5, 9, 4, and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Test Sets</head><p>The training set available for the AVE 2007 Spanish task consists of 1817 answers, where 15% are validated answers and the rest 85% are rejected. In order to avoid the low recall in the validated answers we assembled a more balanced training set. Basically, we joined some answers from the training sets of the AVE 2006 and 2007. This new training set contains 2022 answers, where 44% are validated and 56% rejected.</p><p>On the other hand, the evaluation set for the Spanish AVE 2007 contains 564 answers (22.5% validated and 77.5% rejected) corresponding to 170 different questions.</p><p>Details on these sets are described in <ref type="bibr" coords="4,232.04,711.75,10.64,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>This section describes the experimental results of our participation at the AVE 2007 Spanish task. This year we submitted two different runs. The first run (RUN 1) considered the system just as it was described in the previous section. On the other hand, the second run (RUN 2) used a different learning method; instead of using a single support vector machine classifier, it employed an ensemble of this classifier. This ensemble was implemented using the AdaBoostM1 algorithm in Weka <ref type="bibr" coords="5,275.68,137.01,10.65,8.74" target="#b2">[3]</ref>. Table <ref type="table" coords="5,107.94,148.53,5.01,8.74" target="#tab_0">3</ref> shows the evaluation results corresponding to our two submitted runs. It also shows (in the last row) the results for a 100% YES baseline (i.e., an answer validation system that validated all given answers). The results indicate that our methods achieved a very high recall and a middle level precision, which means that it validates must of the correct answers, but also some incorrect ones. It is important to point out that our best result (RUN 1) outperformed the baseline by 15 percentage points; the same proportion than the best evaluated system at the AVE 2006 <ref type="bibr" coords="5,171.34,206.01,10.61,8.74" target="#b5">[6]</ref>. This year the AVE organizers decide to include a new evaluation measure. This new measure, called qaaccuracy, aims to evaluate the influence of the answer validation systems to the question answering task. In order to compute this measure the answer validation systems must to select only one validated answer for each question. This way, the qa-accuracy expresses the rate of correct selected answers.</p><p>Table <ref type="table" coords="5,108.15,359.31,5.01,8.74" target="#tab_1">4</ref> presents the qa-accuracy results of our two runs. It also shows the results obtained by an "ideal" answer validation system (i.e., a system that, when possible, always selects a correct answer). Here, it is necessary to clarify that because only 101 questions (from the whole set of 170) has a correct candidate answer, it is impossible to obtain a 100% qa-accuracy.</p><p>The results of Table <ref type="table" coords="5,165.88,405.33,5.01,8.74" target="#tab_1">4</ref> are not conclusive. However, it is interesting to comment that our QA system (that was the best one in 2005 and the second best one in 2006 in the Spanish QA task) <ref type="bibr" coords="5,386.20,416.85,16.69,8.74" target="#b9">[10]</ref> obtains a 35.88% of accuracy on the same questions set and ignoring evaluate the correct NIL questions. This fact indicates -in some waythat answer validation is useful, and that it could produce interesting improvements over current QA systems. In order to do a detail evaluation of our system we also measured its precision over the subset of 101 questions that have a candidate corrected answer. In this case, RUN 1 validated the correct candidate answer for 75% of the questions, and RUN 2 for 61%. For the rest of the questions (a subset of 69 questions), where does not exist any correct candidate answer, the RUN 1 correctly answered NIL in 49% of the cases, whereas the RUN 2 correctly responded NIL in 61% of the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper described the INAOE's answer validation system that was evaluated at the Spanish track of the AVE 2007. This system adopts several ideas from recent overlap-based methods; basically, it is based on a supervised learning approach that uses a combination of all previous used features, in particular, the word overlaps and the longest common subsequences. However it includes some new notions that extend and improve these previous methods. For instance: (i) it considers only content words for the calculus of word overlaps and the LCS; (ii) it computes the LCS taking into consideration POS tags; (iii) it makes a syntactic transformation over the generated hypothesis in order to simulate the active and passive voices; (iv) it uses some manually constructed lexical patterns to help treating support texts containing an apposition phrase; (v) it includes some new features denoting certain answer restrictions as imposed by the question's class. The evaluation results are encouraging; they show that the proposed system achieved a 52.91% of F-measure and that it outperformed the standard baseline by 15 percentage points. Moreover, they indicate that our system is especially precise (75% of accuracy) in selecting the correct answer for a question when such answer exists inside the set of candidate answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,70.92,469.77,453.61,8.74;2,70.92,481.29,324.92,8.74"><head>Figure 1 Figure 1 .</head><label>11</label><figDesc>Figure1shows the general architecture of our system. It consists of three main phases: preprocessing, feature extraction and classification. The following subsections describe these processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,164.70,223.20,263.72,72.93"><head>Table 3 .</head><label>3</label><figDesc>General evaluation of the INAOE's system</figDesc><table coords="5,164.70,240.52,263.72,55.61"><row><cell></cell><cell cols="6">TP FP TN FN Precision Recall F-measure</cell></row><row><cell>RUN 1</cell><cell cols="3">109 176 248 18</cell><cell cols="2">38.25% 85.83%</cell><cell>52.91%</cell></row><row><cell>RUN 2</cell><cell cols="5">91 131 293 36 40.99% 71.65%</cell><cell>52.15%</cell></row><row><cell cols="2">100% YES 127 424</cell><cell>-</cell><cell>-</cell><cell>23.05%</cell><cell>100%</cell><cell>37.46%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,167.82,457.08,259.82,88.77"><head>Table 4 .</head><label>4</label><figDesc>Evaluation results obtained by the qa-accuracy measure</figDesc><table coords="5,177.90,474.34,237.29,71.51"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Selected Answers</cell><cell></cell></row><row><cell></cell><cell cols="5">Total Right Wrong Inexact QA-accuracy</cell></row><row><cell>RUN 1</cell><cell>129</cell><cell>76</cell><cell>47</cell><cell>6</cell><cell>44.71%</cell></row><row><cell>RUN 2</cell><cell>107</cell><cell>62</cell><cell>40</cell><cell>5</cell><cell>36.47%</cell></row><row><cell>IDEAL</cell><cell>101</cell><cell>101</cell><cell>-</cell><cell>-</cell><cell>59.41%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,76.20,732.22,300.22,7.85"><p>This analysis was done by Freeling<ref type="bibr" coords="3,205.25,732.22,9.52,7.85" target="#b1">[2]</ref>, an open source suite of language analyzers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,76.20,743.20,366.99,7.85"><p>It is not necessary to compute the term overlap between (T) and (H´) since it will be exactly the same.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was done under partial support of <rs type="funder">CONACYT</rs> (project grant <rs type="grantNumber">43990</rs> and scholarship <rs type="grantNumber">171610</rs>). We also like to thanks to the CLEF organizing committee as well as to the <rs type="institution">EFE agency</rs> for the resources provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TNk2a9T">
					<idno type="grant-number">43990</idno>
				</org>
				<org type="funding" xml:id="_rB9narF">
					<idno type="grant-number">171610</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.69,226.50,449.79,9.02;6,88.92,238.29,435.54,8.74;6,88.92,249.81,22.58,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,229.32,226.50,241.70,9.02">Paraphrase Substitution for Recognizing Textual Entailment</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Callison-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,488.88,226.77,35.60,8.74;6,88.92,238.29,307.57,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,261.00,449.81,9.02;6,88.92,272.79,435.66,8.74;6,88.92,284.31,61.42,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,286.50,261.00,233.82,9.02">FreeLing: An Open-Source Suite of Language Analyzers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,88.92,272.79,398.05,8.74">Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,295.50,449.80,9.02;6,88.92,307.29,307.43,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,204.54,295.50,177.06,9.02">Experiments with a new boosting algorithm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,389.04,295.77,135.45,8.74;6,88.92,307.29,71.79,8.74">Proc International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,318.54,449.81,9.02;6,92.22,330.27,385.43,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,288.90,318.54,107.35,9.02">UNED Submission to AVE</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.49,318.81,89.01,8.74;6,92.22,330.27,238.20,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,341.52,449.69,9.02;6,88.92,353.04,435.56,9.02;6,88.92,364.77,243.34,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,265.20,341.52,259.18,9.02;6,88.92,353.04,175.53,9.02">Adaptation of a Machine-learning Textual Entailment System to a Multilingual Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Montoyo</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,284.64,353.31,239.85,8.74;6,88.92,364.77,96.03,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,376.02,449.87,9.02;6,88.92,387.81,435.43,8.74;6,88.92,399.27,22.54,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,281.88,376.02,178.07,9.02">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,498.42,376.29,26.14,8.74;6,88.92,387.81,311.52,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,410.52,449.87,9.02;6,88.92,422.31,435.47,8.74;6,88.92,433.50,99.50,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,281.88,410.52,178.07,9.02">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,498.42,410.79,26.14,8.74;6,88.92,422.31,311.46,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2007)</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="6,74.69,445.02,449.72,9.02;6,88.92,456.81,435.54,8.74;6,88.92,468.27,22.54,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,248.58,445.02,221.01,9.02">The Effect of Entity Recognition in Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><forename type="middle">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,488.88,445.29,35.54,8.74;6,88.92,456.81,307.57,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,479.52,449.83,9.02;6,88.92,491.25,385.38,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,235.38,479.52,178.00,9.02">Automatic Answer Validation using COGEX</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Iles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moldovan</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,434.10,479.79,90.43,8.74;6,88.92,491.25,238.16,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, España</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.27,502.50,445.16,9.02;6,88.92,514.02,435.55,9.02;6,88.92,525.48,270.40,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,466.14,502.50,58.29,9.02;6,88.92,514.02,124.31,9.02">INAOE&apos;s Participation at QA@CLEF 2007</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Téllez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delicia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villatoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,234.79,514.29,289.68,8.74;6,88.92,525.75,51.82,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2007)</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
