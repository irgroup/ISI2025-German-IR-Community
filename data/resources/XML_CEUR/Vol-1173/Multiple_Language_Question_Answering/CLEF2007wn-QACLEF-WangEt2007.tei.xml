<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.56,75.23,400.56,12.58;1,241.44,92.69,112.46,12.58">DFKI-LT at AVE 2007: Using Recognizing Textual Entailment for Answer Validation</title>
				<funder ref="#_kAYhxKU">
					<orgName type="full">EU-</orgName>
				</funder>
				<funder ref="#_69pFtG6">
					<orgName type="full">German Federal Ministry of Education, Science, Research and Technology (BMBF)</orgName>
				</funder>
				<funder ref="#_5SkVwaQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.62,131.94,40.82,9.02"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>wang.rui@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<addrLine>Stuhlsatzenhausweg 3</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.89,131.94,69.16,9.02"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>neumann@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<addrLine>Stuhlsatzenhausweg 3</addrLine>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.56,75.23,400.56,12.58;1,241.44,92.69,112.46,12.58">DFKI-LT at AVE 2007: Using Recognizing Textual Entailment for Answer Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">598DAB16FA6A6C59E1C741BB1CC23BEC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Answer Validation, Recognizing Textual Entailment</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report is about our participation in the Answer Validation Exercise (AVE) 2007. Our system utilizes a Recognizing Textual Entailment (RTE) system as a component to validate answers. We first change the question and the answer into Hypothesis (H) and view the document as Text (T), in order to cast the AVE task into a RTE problem. Then, we use our RTE system to tell us whether the entailment relation holds between the documents (i.e. Ts) and question-answer pairs (i.e. Hs). Finally, we adapt the results for the AVE task. In all, we have submitted two runs and achieved f-measures of 0.46 and 0.55 respectively, which both outperform last year's best result for English. After detailed error analysis, we have found that both the recall and the precision of our system could be improved in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Question Answering (QA) is an important task in Natural Language Processing (NLP), which aims to mine answers to natural language questions from large corpora. Answer validation is to evaluate the answers obtained by the former stages of a QA system and select the most proper answers for the final output.</p><p>A lot of research has been done on this topic. In recent years, a new trend is to use Recognizing Textual Entailment (RTE-1 - <ref type="bibr" coords="1,159.32,440.76,72.66,9.02" target="#b2">Dagan et al., 2006</ref>; RTE-2 - <ref type="bibr" coords="1,277.67,440.76,86.92,9.02" target="#b0">Bar-Haim et al., 2006</ref>) to do answer validation, see the AVE 2006 Working Notes <ref type="bibr" coords="1,158.29,452.28,76.19,9.02" target="#b6">(Peñas et al., 2006)</ref>. Most of the groups use lexical or syntactic overlapping as features for machine learning; other groups derive the logic forms of natural language texts and perform proving.</p><p>We also developed our own RTE system, which proposed a new sentence representation extracted from the dependency structure, and utilized the Subsequence Kernel method <ref type="bibr" coords="1,353.59,486.78,123.14,9.02" target="#b1">(Bunescu and Mooney, 2006)</ref> to perform machine learning. We have achieved fairly high results on both the RTE-2 data set <ref type="bibr" coords="1,405.97,498.30,118.66,9.02">(Wang and Neumann, 2007a)</ref> and the RTE-3 data set <ref type="bibr" coords="1,168.85,509.76,119.15,9.02">(Wang and Neumann, 2007b)</ref>, especially on Information Extraction (IE) and QA pairs. Therefore, one of our motivations is to improve answer validation by using RTE, and the other is to test our RTE system in other NLP applications. This report will start with introducing our AVE system, which consists of the preprocessing part, the RTE component, and the post-processing part. Then, the results of our two submission runs will be shown, followed by a discussion on error sources. In the end, we will summarize our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our RTE-based AVE System</head><p>Our AVE system uses our RTE system (Tera -Textual Entailment Recognition for Application) as a core component, and includes preprocessing and post-processing modules. The preprocessing module mainly adapts questions, their corresponding answers, and supporting documents into Text(T)-Hypothesis(H) pairs, assisted by some manually designed patterns. The post-processing module (i.e. the Answer Validation in Figure <ref type="figure" coords="2,496.33,296.34,4.19,9.02">1</ref>) will validate each answer and select a most proper one based on the output of the RTE system. We will see the details of each component in the coming sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The given input of the AVE task is a list of questions, their corresponding answers and the documents containing these answers. Usually, we need to validate several answers for each question. For instance, the question is,</p><p>In which country was Edouard Balladur born? (id=178) The assumption here is that if the answer is relevant to the question, the document which contains the answer should entail the statement derived by combining the question and the answer. This section will mainly focus on the combination of the question and the answer and in the next sections the RTE system and how to deal with the output of the system will be described.</p><p>In order to combine the question and the answer into a statement, we need some language patterns. Normally, we have different types of questions, such as Who-questions asking about persons, What-questions asking about definitions, etc. Therefore, we manually construct some language patterns for the input questions. For the example given above (id=178), we will apply the following pattern,</p><p>Edouard Balladur was born in &lt;Answer&gt;. (id=178) Consequently, we substitute the "&lt;Answer&gt;" by each candidate answer to form Hs -hypotheses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Architecture of our AVE system</head><p>Hypothesis: Edouard Balladur was born in Frances.</p><p>These T-H pairs can be the input for any the generic RTE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The RTE Component</head><p>The RTE component is based on the RTE system we have used for RTE-3 Challenge <ref type="bibr" coords="3,416.30,140.76,103.85,9.02" target="#b3">(Giampiccolo et al., 2007)</ref>. The system contains a main approach with two backup strategies. The main approach extracts parts of the dependency structures to form a new representation, named Tree Skeleton, as the feature space and then applies Subsequence Kernels to represent TSs and perform Machine Learning. The backup strategies will deal with the T-H pairs which cannot be solved by the main approach. One backup strategy is called Triple Matcher, as it calculates the overlapping ratio on top of the dependency structures in a triple representation; the other is simply a Bag-of-Words (BoW) method, which calculates the overlapping ratio of words in T and H. We will begin with the main approach and briefly introduce the backup strategies at the end of this section.</p><p>If we take a broad view of the RTE task, in essence, we are asked to tell whether a particular relationship (i.e. entailment) holds between two text fragments. Notice that this kind of relationship is one-directional, which is from T to H. Generally, people start with T, do some processing, and then check whether H is reachable. However, we did it in the opposite direction, based on the observations: 1) H is the target we want to verify, which leads us to identify the relevant parts of T; and 2) H (i.e. a question and one of its candidate answers) is usually textually shorter than T (i.e. a document or a snippet). The T-H pair (id=178_1) is just an example of this. All the information we need in T is the part in bold of the first sentence.</p><p>Now the remaining problems are: 1) How do we identify the relevant parts of T based on H? 2) How do we combine them? 3) How do we represent them? The three steps of the main approach are aiming to solve these problems: extracting tree skeletons to obtain the most relevant parts, merging them to define the feature space, and applying subsequence kernels to represent the features and perform the machine learning procedure.</p><p>Tree Skeleton Extraction Since tree skeletons are extracted based on the dependency structures, we need to use some dependency parsers to obtain the dependency parse trees. We have used Minipar <ref type="bibr" coords="3,350.65,382.19,44.24,9.02" target="#b5">(Lin, 1998)</ref>. The following Figure <ref type="figure" coords="3,491.61,382.19,5.01,9.02" target="#fig_0">2</ref> shows the output given the previous H as the input sentence. As well as H is usually textually shorter than T, the dependency structure of H is also simpler. From Figure <ref type="figure" coords="3,308.70,405.23,3.77,9.02" target="#fig_0">2</ref>, we can easily identify the structure of the whole sentence: There are two nouns in the lower part of the parse tree, and they share a common parent node, which is a verb in the upper part. Since content words usually convey most of the meaning of the sentence, we will mark the two nouns as Topic Words and the verb as the Root Node. Together with the dependency paths in between, they form a subtree of the original dependency structure, which can be viewed as an extended version of Predicate-Argument Structure <ref type="bibr" coords="3,197.28,462.72,108.06,9.02" target="#b4">(Gildea and Palmer, 2002)</ref>. We call the subtree Tree Skeleton, the topic words Foot Nodes, and the dependency path from the noun to the root node Spine. If there are two foot nodes, the corresponding spines will be the Left Spine and the Right Spine.</p><p>On top of the tree skeleton of H, the tree skeleton of T can also be extracted. We assume that if the entailment holds from T to H, at least, they will share the same topics. Since in practice, there are different expressions for the same entity, we have applied some fuzzy matching techniques to correspond the topic words in T and H, like initialism, partial matching, etc. Once we successfully identify the topic words in T, we trace up along the dependency parse tree to find the lowest common parent node, which will be marked as the root node of the tree skeleton of T <ref type="foot" coords="4,124.26,95.98,3.00,5.40" target="#foot_0">2</ref> .</p><p>Notice that the prerequisite for performing this method is: topic words are identified and corresponded between T and H. Furthermore, if there are only two topic words, the whole tree skeleton can be viewed as a sequence of words and dependency relation tags in a flat structure. This is important because sequence structures are much less complex than the tree structures, which will greatly reduce the computational complexity. In practice, 37% of the RTE-2 test data <ref type="bibr" coords="4,223.58,153.72,120.12,9.02">(Wang and Neumann, 2007a)</ref> and 36% of the RTE-3 test data <ref type="bibr" coords="4,479.77,153.72,44.80,9.02;4,70.92,165.24,67.53,9.02">(Wang and Neumann, 2007b</ref>) meet these requirements and consequently can be dealt with by the main approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spine Generalization and Merging</head><p>Before moving on, some generalizations are necessary in order to avoid the scarcity of features in the data. Several steps will be performed: 1) We will collapse some of the dependency relation tags from the parsers to more generalized names, e.g., collapsing &lt;OBJ2&gt; and &lt;DESC&gt; to &lt;OBJ&gt;; 2) we will group together all nodes that have relation labels like &lt;CONJ&gt; or &lt;NN&gt;, since they are assumed to refer to the same entity or belong to one class of entities sharing some common characteristics; 3) lemmas are removed except for the topic words. Finally, we will get the generalized tree skeleton as follows (# is a separator to mark the root node), Edouard_Balladur:N &lt;SUBJ&gt; #born:A# &lt;MOD&gt; PREP &lt;PCOMP-N&gt; Frances:N We will do the same on the tree skeleton of T, and then merge the two tree skeletons by 1) excluding the longest common prefixes for left spines and 2) excluding the longest common suffixes for right spines. Finally, we will get the dissimilarity of the two tree skeletons and we call it Spine Differences, i.e. Left Spine Difference (LSD) and Right Spine Difference (RSD), like the following (## is to separate the parts from T and H, and null means an empty string),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSD: &lt;OBJ&gt; ## &lt;SUBJ&gt; RSD: N ## (null) Subsequence Kernels Application</head><p>After the spine generalization and merging, now all the remaining symbols are POS tags and (generalized) dependency relation tags. They altogether form a Closed-Class Symbol (CCS) set. The spine difference is thus a sequence of CCSs. To represent it, we have utilized a Subsequence Kernel and a Collocation Kernel. The definitions of the two kernels are as below, , ,</p><formula xml:id="formula_0" coords="4,179.28,408.03,279.56,62.07">| | |<label>, , | | , | | , , , , • , |</label></formula><p>whereby T and H refers to all spine differences from T and H, and |T| and |H| represent the cardinalities. The function K CCS (CCS,CCS') checks whether its arguments are equal.</p><p>As well as these two kernels, we have also considered the comparison between root nodes and their adjacent dependency relations. We have observed that some adjacent dependency relations of the root node (e.g. &lt;SUBJ&gt; or &lt;OBJ&gt;) can play important roles in predicting the entailment relationship. For instance, the verb "sell" has a direction of the action from the subject to the object. In addition, the verb "sell" and "buy" convey totally different semantics. Therefore, we assign them two extra simple kernels named Verb Consistence (VC) and Verb Relation Consistence (VRC). The former indicates whether two root nodes have a similar meaning, and the latter indicates whether the relations are contradictive (e.g. &lt;SUBJ&gt; and &lt;OBJ&gt; are contradictive).</p><p>On top of all these four kernels, we have used a composite kernel to combine them linearly with different weights, where γ and δ are learned from the training corpus, and α=β=1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backup Strategies</head><p>As well as the main approach, we have two backup strategies as well: one is called the Triple Similarity and the other is called the BoW Similarity.</p><p>Dependency structures can be represented in a form of a triple set, which expresses the local dependency relations. A triple is of the form &lt;node1, relation, node2&gt;, where node1 represents the head, node2 the modifier, and relation the dependency relation. Thus, each dependency parse tree consists of a set of such triples.</p><p>Chief requirements for the backup strategy are robustness and simplicity. Accordingly, we construct a similarity function, which operates on two triple sets and determines how many triples of H are contained in T.</p><p>The core assumption here is that the higher the number of matching triple elements, the more similar both sets are, and the more likely it is that T entails H. The function uses an approximate matching function. Different cases (i.e. ignoring either the parent node or the child node, or the relation between nodes) might provide different indications for the similarity of T and H. In all cases, a successful match between two nodes means that they have the same lemma and POS. We then sum them up using different weights and divide the result by the cardinality of H for normalization. The different weights learned from the corpus indicate that the "amount of missing linguistic information" affect entailment decisions differently.</p><p>The BoW similarity score is calculated by dividing the number of overlapping words between T and H by the total number of words in H after a simple tokenization according to the space between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-processing</head><p>The RTE component has given us several things: 1) for some of the T-H pairs, we directly know whether the entailment holds; 2) every T-H pair has a triple similarity score; 3) every T-H pair has a BoW similarity score. If the T-H pairs are covered by our main approach, we will directly use the answers; if not, we will use a threshold to decide the answer based on the two similarity scores. In practice, the threshold is learned from the training corpus and the two similarity scores are used in different submission runs.</p><p>For the adaption back to the AVE task, the "YES" entailment cases will be validated answers and the "NO" entailment cases will be rejected answers. In addition, the selected answers (i.e. the best answers) will naturally be the pairs covered by our main approach or (if not,) with the highest similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Error Analysis</head><p>The AVE task this year asks the system to judge whether an answer extracted from a document is a valid answer to the given question. The result can be either "VALIDATED" or "REJECTED", which mean it's a valid answer or not respectively. Furthermore, among all the "VALIDATED" answers to each question, one best answer will be marked as "SELECTED", but if there is no "VALIDATED" answers, there will be no "SELECTED" answer, either.</p><p>The AVE training data contains 200 questions, 1121 answers and 1121 supporting documents, among which there are 130 validated answers and 991 rejected answers. The AVE testing data contains 67 questions, 202 answers and supporting documents, among which there are 21 validated answers, 174 rejected answers, and 7 unknown answers according to the gold standard.</p><p>We have submitted two runs. Both of the two runs we have used the main approach and one backup strategy. The difference is that in the first run, the BoW similarity score is the backup, while in the second run, the triple similarity score is taken. Our machine learning process is performed by using the classifier SMO from the WEKA toolkit <ref type="bibr" coords="5,133.83,505.02,102.54,9.02" target="#b9">(Witten and Frank, 1999)</ref>. In the following, we will first show the table of the results and then present an error analysis.  <ref type="table" coords="5,259.99,577.74,5.01,9.02">1</ref> Results of our two runs Though the absolute scores are not very promising, they are still better than all the results for English from last year. The second run outperforms the first run in all respects, which shows advantages of the triple similarity score. The gold standard does not contain the "SELECTED" answers, thus, we will not discuss the QA accuracy for now. Instead, the error analysis will focus on the loss of recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head><p>As for recall, among all the errors, half of them belong to one type. For questions like "What is the occupation of Kiri Te Kanawa?" we have used the pattern "The occupation of Kiri Te Kanawa is &lt;Answer&gt;", which has caused problems, because "occupation" usually does not appear in the documents. Instead, a pattern like "Kiri Te Kanawa is &lt;Answer&gt;" might be much better. Some other errors are from the noise of web documents, on which the dependency parser could not work very well. A truly difficult example is the following one, Question: Which American President masterminded the Camp David Agreement? (id=160) Answer: Jimmy Carter. (id=160_2) Document: United States President Jimmy Carter invited both Sadat and Begin to a summit at Camp David to negotiate a final peace.</p><p>Not only the lexical semantics of "mastermind" and "negotiate" are necessary, but also some world knowledge like the name of an agreement is usually the place where people subscribe it.</p><p>The precision of our two runs are rather poor. After taking a closer look at the errors, we have found that most of the errors also belong to one type. In those answer-document pairs (e.g. id=119_2, id=125_1, id=133_1, etc.), the answers are usually very long, which consist of a large part of the documents. Some extreme cases (e.g. id=112_2, id=172_2, etc.), the answers are very long and exactly the same as the documents. Due to the characteristics of our method (i.e. using RTE for AVE), these answers will get high similarity scores, which are wrongly validated. Errors from the parser will also cause problems. For example, Question: Who is Thom Rotella? (id=106) Answer: Grant Geissman. (id=106_3) Document: As founder of Positive Music Records, Navarro is responsible for launching and furthering the recording careers of saxophonists Bob Militello and Brandon Fields, guitarists Grant Geissman, Thom Rotella and Pat Kelley, and keyboardists Gregg Karukas and Marcus Johnson. Some other errors like trivial answers (e.g. "one") could be avoided by adding some rules. As a whole, more fine-grained classification of answers could be helpful to improve the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In conclusion, we have described our participation of AVE 2007. The work presented is utilizing our RTE system to validate answers from QA systems. One the one hand, it is an effective way to improve the answer validation task; on the other hand, it is also a promising application for our developed RTE system. The results have shown the advantages of our method. After error analysis, the possible future directions are: 1) preprocessing the documents to clean the noisy web data; 2) improving the patterns or learning them automatically; 3) utilizing question analysis tools to acquire more useful information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,190.26,741.90,214.50,9.02;3,142.44,532.50,311.64,201.00"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Dependency Structure and Tree Skeleton</figDesc><graphic coords="3,142.44,532.50,311.64,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,82.26,398.08,406.98,147.27"><head></head><label></label><figDesc>1  The QA system gives out several candidate answers to this question, as follows,</figDesc><table coords="2,106.38,421.32,114.84,32.00"><row><cell>Frances (id=178_1)</cell></row><row><cell>12% jobless rate (id=178_3)</cell></row><row><cell>7 (id=178_5)</cell></row></table><note coords="2,106.38,456.09,8.91,8.74;2,82.26,467.34,356.44,9.02;2,106.38,479.07,382.86,8.74;2,106.38,490.59,382.70,8.74;2,106.38,502.11,382.77,8.74;2,106.38,513.57,382.82,8.74;2,106.38,524.82,220.83,9.02;2,106.38,536.34,10.02,9.02"><p>… Each answer will have one supporting document where the answer comes from, like this, Paris, Wednesday CONSERVATIVE Prime Minister Edouard Balladur, defeated in France's presidential election, resigned today clearing the way for President-elect Jacques Chirac to form his own new government. Balladur's move was a formality since outgoing President Francois Mitterrand hands over power next week to Chirac, the conservative Paris mayor who won last Sunday's run-off election... (parts) (id=178_1) …</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,78.42,748.59,442.14,8.10"><p>The Root Node of T is not necessary to be a verb, instead, it could be a noun, a preposition, or even a dependency relation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work presented here was partially supported by a research grant from the <rs type="funder">German Federal Ministry of Education, Science, Research and Technology (BMBF)</rs> to the <rs type="projectName">DFKI</rs> project <rs type="projectName">HyLap</rs> (<rs type="grantNumber">FKZ: 01 IW F02</rs>) and by the <rs type="funder">EU-</rs>funded project <rs type="projectName">QALL-ME</rs> (<rs type="grantNumber">FP6 IST-033860</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_69pFtG6">
					<orgName type="project" subtype="full">DFKI</orgName>
				</org>
				<org type="funded-project" xml:id="_kAYhxKU">
					<idno type="grant-number">FKZ: 01 IW F02</idno>
					<orgName type="project" subtype="full">HyLap</orgName>
				</org>
				<org type="funded-project" xml:id="_5SkVwaQ">
					<idno type="grant-number">FP6 IST-033860</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.30,533.31,450.14,8.10;6,82.26,543.63,442.18,8.10;6,82.26,553.95,169.84,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,444.40,533.31,80.04,8.10;6,82.26,543.63,165.43,8.10">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,271.39,543.63,253.05,8.10;6,82.26,553.95,115.01,8.10">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,564.33,450.18,8.10;6,82.26,574.65,126.95,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,219.07,564.33,166.39,8.10">Subsequence Kernels for Relation Extraction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,403.80,564.33,120.68,8.10;6,82.26,574.65,71.24,8.10">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,585.03,450.15,8.10;6,82.26,595.35,331.05,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,258.56,585.03,207.97,8.10">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,165.13,595.35,74.28,8.10">MLCW 2005, LNAI</title>
		<editor>
			<persName><surname>Quiñonero-Candela</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,605.73,450.16,8.10;6,82.26,616.05,418.38,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,323.74,605.73,200.72,8.10;6,82.26,616.05,34.86,8.10">The Third PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,133.07,616.05,252.29,8.10">Proceedings of the Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,626.43,450.24,8.10;6,82.26,636.75,373.49,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,208.26,626.43,229.31,8.10">The Necessity of Parsing for Predicate Argument Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.55,626.43,69.00,8.10;6,82.26,636.75,272.17,8.10">Proceedings of the 40th Meeting of the Association for Computational Linguistics (ACL 2002)</title>
		<meeting>the 40th Meeting of the Association for Computational Linguistics (ACL 2002)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,647.13,409.17,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,132.87,647.13,156.75,8.10">Dependency-based Evaluation of MINIPAR</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,306.70,647.13,172.73,8.10">Workshop on the Evaluation of Parsing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,657.45,450.21,8.10;6,82.26,667.83,77.71,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,291.70,657.45,164.58,8.10">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,492.25,657.45,32.26,8.10;6,82.26,667.83,18.00,8.10">the AVE 2006</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="6,74.30,678.15,450.27,8.10;6,82.26,688.53,45.02,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,217.60,678.15,258.89,8.10">Recognizing Textual Entailment Using a Subsequence Kernel Method</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,494.98,678.15,29.58,8.10;6,82.26,688.53,22.49,8.10">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.30,698.85,450.07,8.10;6,82.26,709.23,442.27,8.10;6,82.26,719.55,20.22,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,219.24,698.85,305.13,8.10;6,82.26,709.23,53.61,8.10">Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,154.24,709.23,263.10,8.10">Proceedings of the Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,78.44,729.93,446.16,8.10;6,82.26,740.25,62.70,8.10" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,182.28,729.93,307.30,8.10">Weka: Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
