<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.47,148.86,252.07,15.15">The UPV at QA@CLEF 2007</title>
				<funder ref="#_HG97rpc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-08-18">August 18, 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.54,182.75,70.62,8.74"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<email>dbuscaldi@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.85,182.75,74.41,8.74"><forename type="first">Yassine</forename><surname>Benajiba</surname></persName>
							<email>ybenajiba@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.96,182.75,52.69,8.74"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.35,182.75,64.11,8.74"><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
							<email>esanchis@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">de Sistemas Informticos y Computación (DSIC)</orgName>
								<orgName type="institution">Universidad Politcnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.47,148.86,252.07,15.15">The UPV at QA@CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-08-18">August 18, 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">0159F7C8DC62317F343DEF02D89A110D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Algorithms, Performance, Experimentation Question Answering, Passage Retrieval, Answer Extraction and Analysis, Anaphora Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done by the RFIA group at the Departamento de Sistemas Informáticos y Computación of the Universidad Politécnica of Valencia for the 2007 edition of the CLEF Question Answering task. We participated in the Spanish monolingual task only. A series of technical difficulties prevented us from completing all the tasks we subscribed. Moreover, an unnoticed indexing error resulted in the system working only on the Wikipedia collection. The system is a slightly revised version of the one we used in 2006. It was modified in order to comply with the 2007 guidelines, especially with regard to anaphora resolution, tackled with a web based anaphora resolution module. We obtained an accuracy of 11.5% despite the problems we experienced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QUASAR (QUestion AnSwering And Retrieval) is the name we gave to our Question Answering (QA) system. It is based on the JIRS Passage Retrieval (PR) system <ref type="bibr" coords="1,408.67,625.50,9.96,8.74" target="#b2">[3]</ref>, specifically oriented to this task. JIRS does not use any knowledege about the lexicon and the syntax of the target language, therefore it can be considered as a language-independent PR system. The system we used this year differs slightly from the one used in 2007. Its major improvement has been the insertion of an Anaphora Resolution module in order to comply with the guidelines of CLEF QA 2007. As evidenced in <ref type="bibr" coords="1,195.29,685.27,9.97,8.74" target="#b5">[6]</ref>, the correct resolution of anaphora is crucial and allows to improve accuracy of more than 10% with respect to a system that does not implement any method for anaphora resolution. The web is an important resource for QA <ref type="bibr" coords="1,370.45,709.18,10.52,8.74" target="#b3">[4]</ref> and has been already used to solve anaphora in texts <ref type="bibr" coords="1,196.75,721.14,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,211.16,721.14,7.01,8.74" target="#b0">1]</ref>. We took into account these works in order to build a web-based Anaphora Resolution module.</p><p>In the next section, we describe the structure and the building blocks of our QA system, including the new Anaphora Resolution module. In section 3 we discuss the results of QUASAR in the 2007 CLEF QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>In Fig. <ref type="figure" coords="2,120.05,190.70,4.40,8.74" target="#fig_0">1</ref> we show the architecture of the system used by our group at the CLEF QA 2007.  The user question is first examined by the Anaphora Resolver. This module will pass the question to QUASAR in order to obtain the answer that will be used in the following questions of the same group (i.e., questions over the same topic) for the anaphora resolution. The AR hands over the eventually reformulated question to the Question Analysis module, which is composed by a Question Analyzer that extracts some constraints to be used in the answer extraction phase, and by a Question Classifier that determines the class of the input question. At the same time, the question is passed to the Passage Retrieval module, which generates the passages used by the Answer Extraction (AE) module together with the information collected in the question analysis phase in order to extract the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anaphora Resolution Module</head><p>The anaphora resolution module is the first or last module of the system (depending on the viewpoint). It carries out its work in 5 basic steps:</p><p>• Step 1: Using patterns in this step the module induces the different entity names, temporal and number expressions occuring in the first question of a group.</p><p>• Step 2 : For each question of the group the module replaces the entity names which occured in the first question (which is most likely to contain the target) and occur only partially the others. For instance if the first question was "Cuál era el aforo del Estadio Santiago Bernabéu en los anos 80?" and another question within the same group was "Quién es el dueño del estadio?", "estadio" in the second question is replaced by "Estadio Santiago Bernabéu".</p><p>• Step 3: In this step the module focuses on pronominal anaphora and uses a web count to decide on weither to replace the anaphora with the answer of the previous question or with one of the entity names occuring in the first question of the group. For instance, if the first question was "Cómo se llama la mujer de Bill Gates?" and another question is "En qué universidad estudiaba él cuando creó Microsoft?" the module would check the Web counts of both "Bill Gates creó Microsoft" and "Melinda Gates creó Microsoft" and then replace the anaphora which whould change the second question to "En qué universidad estudiaba Bill Gates cuando creó Microsoft?". However, in the Spanish language pronominal anaphora tend to be absent in the sentence for instance "En que año se fundó?" and this kind of questions are handled in Step 5 (see below).</p><p>• Step 4: The other type of anaphora left to resolve are the possessive anaphora. Similarily to the previous step the module decides on how to change the question using web counts. For instance, if we take the same example mentioned in Step 2 and we say that a third question was "Cuánto dinero se gastó durante su ampliación entre 2001 y 2006?", the module would check the web counts of both "ampliación del Estadio Santiago Bernabéu" and "ampliación del Real Madrid Club de Fútbol" and change the question in order to become "Cuánto dinero se gastó durante ampliación del Estadio Santiago Bernabéu entre 2001 y 2006?".</p><p>• Step 5: For the questions which have not been changed during any of the previous steps the module adds at the end of the question the entity name which has been found in the first question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Analysis Module</head><p>This module obtains both the expected answer type (or class) and some constraints from the question. The different answer types that can be treated by our system are the same of 2006 <ref type="bibr" coords="3,499.71,512.97,9.97,8.74" target="#b1">[2]</ref>. Each category is defined by one or more patterns written as regular expressions. If a question matches more than one pattern, it is assigned the label of the longest matching pattern, on the basis of the assumption that longest patterns are less generic than shorter ones. Questions that do not match any defined pattern are labeled with OTHER. The patterns have been slightly modified since our 2006 participation.</p><p>The Question Analyzer has the purpose of identifying the constraints to be used in the AE phase. These constraints are made by sequences of words extracted from the POS-tagged query by means of POS patterns and rules, mostly language-dependent. For instance, in English, a pair adjective -noun (such as Russian president) is considered as a relevant pattern. The POS-taggers used were the SVMtool<ref type="foot" coords="3,191.92,630.95,3.97,6.12" target="#foot_0">1</ref> for English and Spanish, and the TreeTagger<ref type="foot" coords="3,395.15,630.95,3.97,6.12" target="#foot_1">2</ref> for Italian and French.</p><p>There are two classes of constraints: a target constraint, which is the word of the question that should appear closest to the answer string in a passage, and zero or more contextual constraints, keeping the information that has to be included in the retrieved passage in order to have a chance of success in extracting the correct answer. There is always only one target constraint for each question, but the number of contextual constraint is not fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Retrieval Module (JIRS)</head><p>Passages with the relevant terms (i.e., without stopwords) are found by the Search Engine using the classical IR system. Sets of unigrams, bigrams, ..., n-grams are extracted from the extended passages and from the user question. In both cases, n will be the number of question terms. With the n-gram sets of the passages and the user question we will make a comparison in order to obtain the weight of each passage. The weight of a passage will be heavier if the passage contains greater n-gram structures of the question. A complete description of JIRS is outside the scope of this paper and can be found in <ref type="bibr" coords="4,292.99,202.14,9.97,8.74" target="#b2">[3]</ref>. JIRS can be obtained at the following URL: http://leto.dsic.upv.es:8080/jirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Extraction</head><p>The input of this module is constituted by the n passages returned by the PR module and the constraints (including the expected type of the answer) obtained through the Question Analysis module. The positions of the passages in which occur the constraints are marked before passing them to the text analyzers (TextCrawlers). One of these analyzers is instantiated for each of the n passages with a set of patterns for the expected type of the answer and a pre-processed version of the passage text.</p><p>The TextCrawler begins its work by searching all the passage's substrings matching the expected answer pattern. Then a weight is assigned to each found substring s, depending on the positions of s with respect to the constraints, if s does not include any of the constraint words. If in the passage are present both the target constraint and one or more of the contextual constraints, then the product of the weights obtained for every constraint is used; otherwise, it is used only the weight obtained for the constraints found in the passage.</p><p>The Filter module takes advantage of a mini knowledge base in order to discard the candidate answers which do not match with an allowed pattern or that do match with a forbidden pattern. When the Filter module rejects a candidate, the TextCrawler provide it with the next bestweighted candidate, if there is one. Finally, when all TextCrawlers end their analysis of the text, the Answer Selection module selects the answer to be returned by the system. The following strategies apply:</p><p>• Simple voting (SV): The returned answer corresponds to the candidate that occurs most frequently as passage candidate.</p><p>• Weighted voting (WV): Each vote is multiplied for the weight assigned to the candidate by the TextCrawler and for the passage weight as returned by the PR module.</p><p>• Maximum weight (MW): The candidate with the highest weight and occurring in the best ranked passage is returned.</p><p>• Double voting (DV): As simple voting, but taking into account the second best candidates of each passage.</p><p>• Top (TOP): The candidate elected by the best weighted passage is returned.</p><p>We used the Confidence Weighted Score (CWS) to select the answer to be returned to the system. For each candidate answer we calculated the CWS by dividing the number of strategies giving the same answer by the total number of strategies <ref type="bibr" coords="4,346.80,658.88,11.63,8.74" target="#b4">(5)</ref>, multiplied for other measures depending on the number of passages returned (n p /N , where N is the maximum number of passages that can be returned by the PR module and n p is the number of passages actually returned) and the averaged passage weight. The final answer returned by the system is the one with the best CWS. Our system always return only one answer (or NIL), although 2006 rules allowed to return more answers per question. The weighting of NIL answers is slightly different, since is obtained as 1 -n p /N if n p &gt; 0, 0 elsewhere.</p><p>The snippet for answer justification is obtained from the portion of text surrounding the first occurrence of the answer string. The snippet size is always 300 characters (150 before and 150 after the answer) + the number of characters of the answer string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>This year we experienced some technical difficulties during the test period, making us able only to submit one run for the Spanish monolingual task. Moreover, we realized after the submission that JIRS indexed only a part of the collection, specifically the Wikipedia snippet. The results is that we obtained only an accuracy of 11.5%. The reason of the poor performance is due partly to the fact that the patterns were defined for a collections of news and not for an encyclopedia, and partly to the fact that many questions had their answer in the news collection. We obtained 54 NIL answers, more than the 25% of the total number of questions, and only one time correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Further Work</head><p>Our experience in the CLEF QA 2007 exercise was disappointing in terms of results, however we managed to develop an anaphora resolution module based on the web. The performance of this module cannot be evaluated from the results obtained at the CLEF, therefore we are considering to perform more experiments with the correct collections in order to improve the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,220.89,543.76,161.22,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the QA system</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,711.22,149.17,6.99"><p>http://www.lsi.upc.edu/ nlp/SVMTool/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,720.73,328.22,6.99"><p>http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the <rs type="grantNumber">TIN2006-15265-C06-04</rs> research project for partially supporting this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HG97rpc">
					<idno type="grant-number">TIN2006-15265-C06-04</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.50,474.55,407.49,8.74;5,105.50,486.50,304.31,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,162.19,474.55,241.56,8.74">Associative anaphora resolution: A web-based approach</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,424.97,474.55,88.03,8.74;5,105.50,486.50,183.18,8.74">EACL Workshop on the Computational Treatment of Anaphora</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,506.43,407.50,8.74;5,105.50,518.38,351.96,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,427.67,506.43,85.33,8.74;5,105.50,518.38,18.16,8.74">The upv at qa@clef 2006</title>
		<author>
			<persName coords=""><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,236.94,518.38,117.46,8.74">CLEF 2006 Working Notes</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,538.31,407.50,8.74;5,105.50,550.27,407.51,8.74;5,105.50,562.22,360.16,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,429.14,538.31,83.87,8.74;5,105.50,550.27,186.91,8.74">A passage retrieval system for multilingual question answering</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,316.03,550.27,196.97,8.74;5,105.50,562.22,123.63,8.74">8th International Conference of Text, Speech and Dialogue 2005 (TSD&apos;05)</title>
		<meeting><address><addrLine>Karlovy Vary, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,582.15,407.50,8.74;5,105.50,594.10,381.61,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,162.66,582.15,330.61,8.74">The web as a resource for question answering: Perspectives and challenges</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.50,594.10,264.61,8.74">Language Resource and Evaluation Conference (LREC 2002)</title>
		<meeting><address><addrLine>Las Palmas, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,614.03,407.50,8.74;5,105.50,625.98,396.16,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,291.84,614.03,203.57,8.74">Using the web for nominal anaphora resolution</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Modjeska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.50,625.98,275.03,8.74">EACL Workshop on the Computational Treatment of Anaphora</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.50,645.91,407.49,8.74;5,105.50,657.86,407.50,8.74;5,105.50,669.82,407.50,8.74;5,105.50,681.77,118.15,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,275.84,645.91,237.15,8.74;5,105.50,657.86,98.92,8.74">Importance of pronominal anaphora resolution in question answering systems</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ferrández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,226.37,657.86,286.62,8.74;5,105.50,669.82,130.81,8.74">ACL &apos;00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
