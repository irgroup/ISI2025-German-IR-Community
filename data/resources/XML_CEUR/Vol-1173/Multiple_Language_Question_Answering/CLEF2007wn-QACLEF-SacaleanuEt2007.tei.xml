<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,321.05,73.98,70.08,12.72">CLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.32,105.23,72.23,9.02"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.40,105.23,69.17,9.02"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
							<email>neumann@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.96,105.23,63.05,9.02"><forename type="first">Christian</forename><surname>Spurk</surname></persName>
							<email>cspurk@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">LT-Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,321.05,73.98,70.08,12.72">CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92A00AD9C0B2442774258C067D4522EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.7 [Document and Text Processing]: I.7.1 Document and Text Editing</term>
					<term>I.7.2 Document Preparation</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This Working note shortly presents QUANTICO, a cross-language open domain question answering system for German and English document collections. The main features of the system are: use of preemptive off-line document annotation with information like Named Entities, sentence boundaries and pronominal anaphora resolution; online extraction of abbreviation-extension pairs and appositional constructions for the answer extraction; use of online translation services for the cross-language scenarios and of English as interlingua for language combinations not supported directly; use of redundancy as an indicator of good answer candidates; selection of the best answers based on distance metrics defined over graph representations. Based on the question type two different strategies of answer extraction are triggered: for factoid questions answers are extracted from best IR-matched passages and selected by their redundancy and distance to the question keywords; for definition questions answers are considered to be the most redundant normalized linguistic structures with explanatory role (i.e., appositions, abbreviation's extensions). The results of evaluating the system's performance by QA@CLEF 2007 were as follows: for the German-German run we achieved an overall accuracy (ACC) of 30%; for the English-German run 18.5% (ACC); for the German-English run 7% (ACC), for the Spanish-English run 10% (ACC) and for the Portuguese-German run 7% (ACC).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>QUANTICO is a cross-language open domain question answering system developed mainly for both English and German factoid and definition question. It uses a common framework for both monolingual and cross-language scenarios that crosses the language barrier rather on the question than on the document side by using free online translation services, linguistic knowledge and alignment methods. Through the offline annotation of the document collection with several layers of linguistic information (named entities, sentence boundaries) and their use in the retrieval process, more accurate and reliable information units are being considered for answer extraction, which is based on the assumption that redundancy is a good indicator of information suitability. The answer selection component normalizes and represents the context of an answer candidate as a graph and computes its appropriateness in terms of the distance between the answer and question keywords. <ref type="bibr" coords="1,345.24,601.67,11.71,9.02" target="#b0">[1]</ref> For this year's participation we have extended the system to deal with two further source languages (Spanish and Portuguese) and a new document collection (Wikipedia), and opted for first translating the questions and then interpreting them instead of analyzing the source question and using alignment methods to cross the language barrier as in our previous evaluations. We will begin giving a short overview of the system and presenting its working for both factoid and definition questions in monolingual and cross-language scenarios. We will then continue with a short description of each component and close the paper with the presentation of the CLEF evaluation results and the outcome of an incipient error analysis. The topics as provided by the evaluation exercise are first annotated with named entities (NE) and personal pronouns are linked to their NE-references. Every question is then translated into the target language resulting in a set of possible translations, which are individually interpreted. The outcome of the question analysis is ranked according to linguistic well-formedness and its completeness with respect to the query information (question type, question focus, answer-type) and the best alternative is considered for further processing. Passages relevant to this best formal representation of the question are retrieved and possible answer candidates are extracted and ranked based on their redundant occurrence. Finally, the best candidate is chosen as answer according to a distance metric based on features of the question's keywords and potential candidates. The system is using online translation services (AltaVista<ref type="foot" coords="2,304.80,600.94,3.23,5.87" target="#foot_0">1</ref> , FreeTranslation<ref type="foot" coords="2,376.80,600.94,3.23,5.87" target="#foot_1">2</ref> and VoilaTranslation<ref type="foot" coords="2,468.24,600.94,3.23,5.87" target="#foot_2">3</ref> ) for crossing the language barrier from the source language of the question to the target language of the document collection. The difference in workflow results from the lack of translation services for pairs of languages from any other language than English to German. For this cases (i.e. Portuguese to German) an additional step of translating to English and then to German has been considered, in which case English is used as an Interlingua.</p><p>For the Answer Extraction component the distinction consists in different methods of computing the clusters of candidate answers (Group process): for factoid question, where the candidates are usually named entities or chunks, is based on co-reference (John ~ John Doe) and stop-word removal (of death ~ death), while for definition questions, where candidates can vary from chunks to whole sentences, is based on topic similarity (Italian designer ~ the designer of a new clothes collection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Component Descriptions</head><p>Following is a description of QUANTICO's individual components that have been used in this year's evaluation exercise along with some examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NE-Informed Translation</head><p>Since named entities can pose some problems in translation, especially proper names, by being translated when they should not be, the translation component has been developed with a substitution module that replaces some types of named entities with place holders before translating the question. The process is being reversed after translation, resulting in more accurate results. The outcome of this module is highly dependent on the accuracy of the named entity (NE) recognizer, since an inaccurate mark-up of the NE-terms might prevent from translating semantically relevant information. For the case of inexistent or inaccurate online translation services for pairs of languages like Portuguese to German an Interlingua solution has been approached: that of using English as an intermediate translation and having the question first translated from Portuguese to English and then from English to German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Analysis</head><p>The question parser computes for each question a syntactic dependency tree (which also contains recognized named entities) and semantic information like question type, the expected answer type, and the question focus, cf. <ref type="bibr" coords="3,514.32,411.35,11.71,9.02" target="#b1">[2]</ref> for details. The semantic information is determined on the basis of syntactic constraints applied on relevant NP and VP phrases of the dependency tree (e.g., considering agreement and functional roles), and by taking into account information from two small knowledge bases. They basically perform a mapping from linguistic entities to values of the questions tags, e.g., trigger phrases like name_of, type_of, abbreviation_of or a mapping from lexical elements to expected answer types, like town, person, and president. For German, we additionally perform a soft retrieval match to the knowledge bases taking into account on-line compound analysis and string-similarity tests. For example, assuming the lexical mapping Stadt → LOCATION for the lexeme town, then automatically we will also map the nominal compounds Hauptstadt (capital) and Großstadt (large city) to LOCATION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Passage Retrieval</head><p>The preemptive offline document annotation refers to the process of annotating the document collections with information that might be valuable during the retrieval process by increasing the accuracy of the hit list. Since the expected answer type for factoid questions is usually a named entity type, annotating the documents with named entities provides for an additional indexation unit that might help to scale down the range of retrieved passages only to those containing the searched answer type. The Generate Query process mediates between the question analysis result QAObj (answer type, focus, keywords) and the search engine serving the retrieval component with information units (passages). The Generate Query process builds on an abstract description of the processing method for every type of question to accordingly generate the IRQuery to make use of the advanced indexation units. For example given the question "What is the capital of Germany?", since named entities were annotated during the offline annotation and used as indexing units, the Query Generator adapts the IRQuery so as to restrict the search only to those passages having at least two locations: one as the possible answer (Berlin) and the other as the question's keyword (Germany), as the following example shows: +text:capital +text:Germany +neTypes:LOCATION +LOCATION:2.</p><p>It is often the case that the question has a semantic similarity with the passages containing the answer, but no lexical overlap. For example, for a question like "Who is the French prime-minister?", passages containing "prime-minister X of France", "prime-minister X … the Frenchman" and "the French leader of the government" might be relevant for extracting the right answer. The Extend process accounts for bridging this gap at the lexical level through look-up of unambiguous resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Extraction</head><p>The Answer Extraction component is based on the assumption that the redundancy of information is a good indicator for its suitability. The different configurations of this component for factoid and definition questions reflect the distinction of the answers being extracted for these two question types: simple chunks (i.e. named entities and basic noun phrases) and complex structures (from phrases through sentences) and their normalization. Based on the control information supplied by the Interpret Semantics component (q-type), different extraction strategies are being triggered (noun phrases, named entities, definitions) and even refined according to the a-type (definition as sentence in case of an OBJECT, definition as complex noun phrase in case of a PERSON). The Extract process for definition questions implies an online extraction of those passage-units only that might bear a resemblance to a definition. The extraction of these passages is attained by matching them against a lexicosyntactic pattern of the form: &lt;Searched Concept&gt; &lt;definition verb&gt; .+ whereby &lt;definition verb&gt; is being defined as a closed list of verbs like "is", "means", "signify", "stand for" and so on. For factoid questions having named entities or simple noun phrases as expected answer type the Group (normalization) process consists in resolving cases of coreference, while for definition questions with complex phrases and sentences as possible answers more advanced methods are being involved. The current procedure for clustering definitions consists in finding out the focus of the explanatory sentence or the head of the considered phrase. Each cluster gets a weight assigned based solely on its size (definition questions) or using additional information like the average of the IR-scores and the document distribution for each of its members (factoid questions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Selection</head><p>Using the most representative sample (centroid) of the answer candidates' best-weighed clusters, the Answer Selection component sorts out a list of top answers based on a distance metric defined over graph representations of the answer's context. The context is first normalized by removing all functional words and then represented as a graph structure. The score of an answer is defined in terms of its distance to the question concepts occurring in its context and the distance among these. In the context of the participation to CLEF a threshold of five best-weighed clusters has been chosen and all their instances, not only their centroids, have been considered for a thorough selection of the best candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Results</head><p>We participated in five tasks: DE2DE (German to German), EN2DE (English to German), DE2EN (German to English), ES2EN (Spanish to English) and PT2DE (Portuguese to German), with one run submitted for each of the tasks. A description of the achieved results can be seen in Table <ref type="table" coords="4,328.92,636.35,3.76,9.02" target="#tab_0">1</ref>. A preliminary error analysis of the results has uncovered two weak places in our system:</p><p>• the English parser has a coverage too small as initially assumed to have and therefore created a bottleneck effect for those runs with English as target language, • the NE-Informed Translation component, being highly dependent on the accuracy of the named entity (NE) recognizer, was disastrous for runs having Spanish and Portuguese as source language, for which the German NE-recognizer was used by default; in these cases a simple translation, without replacing any named entities, would have been more useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,247.56,411.47,128.25,9.02;2,70.92,70.80,469.68,324.24"><head>Figure 1</head><label>1</label><figDesc>Figure 1. System Architecture</figDesc><graphic coords="2,70.92,70.80,469.68,324.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,210.96,73.19,185.53,126.25"><head>Table 1 . System Performance</head><label>1</label><figDesc></figDesc><table coords="5,210.96,91.56,185.53,107.88"><row><cell>Run ID</cell><cell>#</cell><cell cols="2">Right %</cell><cell>W X U # # #</cell></row><row><cell cols="5">dfki061dede M 60 30 121 14 5</cell></row><row><cell cols="5">dfki061ende C 37 18.5 144 18 1</cell></row><row><cell cols="3">dfki061deen C 14</cell><cell>7</cell><cell>178 6 2</cell></row><row><cell cols="3">dfki062esen C 10</cell><cell>5</cell><cell>180 10 0</cell></row><row><cell cols="5">dfki062ptde C 5 2.5 189 4 2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,76.68,674.63,116.09,9.02"><p>http://babelfish.altavista.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,76.68,686.15,117.41,9.02"><p>http:// ets.freetranslation.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,76.68,697.55,77.35,9.02"><p>http:// trans.voila.fr</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,92.32,355.07,448.81,9.02;5,106.92,366.59,326.61,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,225.96,355.12,299.87,8.88">DFKI-LT at the CLEF 2006 Multiple Language Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,106.92,366.59,181.54,9.02">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date>20-22 September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.32,378.11,448.81,9.02;5,106.92,389.63,434.11,9.02;5,106.92,401.15,269.47,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,239.88,378.16,301.25,8.88;5,106.92,389.68,300.81,8.88">Experiments on Robust NL Question Interpretation and Multilayered Document Annotation for a Cross-Language Question/Answering System</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,523.92,389.63,17.11,9.02;5,106.92,401.15,18.12,9.02">Clef 2004</title>
		<title level="s" coord="5,132.00,401.15,25.33,9.02">LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
