<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.43,148.86,286.15,15.15">Making RAPOSA (FOX) smarter</title>
				<funder>
					<orgName type="full">POSI</orgName>
				</funder>
				<funder ref="#_2BDUsux">
					<orgName type="full">Fundação para a Ciência e Tecnologia (Portugal)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.30,188.73,63.05,8.74"><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.03,188.73,72.67,8.74"><forename type="first">Eugénio</forename><surname>Oliveira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.43,148.86,286.15,15.15">Making RAPOSA (FOX) smarter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF7370C15F170EBB31DDA3A0EFC69FB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe RAPOSA, a question answering system for Portuguese, and we present detailed results of the participation in the QA@CLEF 2007 evaluation task. We explain how we improved our system since the last participation in QA@CLEF, by expanding the set of rules associated with Question Parsing and Answering Extraction. We will finish by pointing lines for future work, and by introducing the concept of "tagging by example" that allows improving the answer extraction stage by learning how identify in text elements that are potentially useful in answering certain types of questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>The main goal of our participation in QA@CLEF for Portuguese with the RAPOSA system was to be able to gain further insights into the problem of semantic analysis of text over a realistic and application-oriented such as is the Question Answering (QA) scenario. Our previous participation in the QA track <ref type="bibr" coords="1,162.82,606.61,10.52,8.74" target="#b8">[9]</ref> (our first) was mainly intended to be an alternate evaluation scenario for our wide-scope named-entity recognizer (NER) system, SIEM ÊS <ref type="bibr" coords="1,356.35,618.57,9.97,8.74" target="#b5">[6]</ref>, which was, and still is, the main analysis component of RAPOSA: both analysis of questions and extraction of answers rely heavily in the NER system. However, in last year's participation RAPOSA suffered from several limitations in specific point of its pipeline that degraded the final performance of the system, in levels sometimes superior to those that resulted from problems in the NER system itself. Therefore, our participation in 2006, although successful in enabling us to detect interesting problems in our NER system, and in helping us to define new directions for future work, lacked the representativeness that we wished for testing our NER.</p><p>The main concern this year was to expand the set of different types questions that the system attempts to answer, by improving both the question parsing and the answer extraction stages, where the role of NER system is crucial. However, there were two challenges that we decided not to face yet this year. First, we did not attempt to to answer list/enumeration questions (10 in this year's test set). Additionally, we did not try to answer context dependent questions because no co-reference resolution module was available for that purpose. When a group of questions was found in the test set, only the first question of the group, which had not resolution dependencies, was considered. The co-reference resolution problem is still out of the scope of our current work.</p><p>Another interesting point in this years participation was related with using Wikipedia as the source collection for extracting answers. There has been a great interest recently in using Wikipedia for all sorts of information extraction tasks, and given the continuous growth of this resource, the trend is that such type of works become even more common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RAPOSA</head><p>RAPOSA is a question answering system for Portuguese (mono-lingual). Contrary to many other systems that clearly separate the information extraction stage (where facts are extracted from text bases and stored in some intermediate fact database) and retrieval stage (where, given a question, the system retrieves candidate answers from the fact database and generates an answer), RAPOSA tries to provide continuous on-line processing chain from the question to the answer.</p><p>There are three important advantages in following such an approach. First, during development time, any change in the RAPOSA can be immediately tested, making it possible to speed-up the entire development process. Second, and since the focus is on direct processing of text information, adapting RAPOSA to answer questions based on a different text collection is simply a matter of developing the interface for querying the new text sources. And third, various question-answering heuristics for specific types of questions can be implemented and used almost instantaneously, making it very simple to expand the set of question that RAPOSA can answer.</p><p>The downside of this approach, however, is that it usually takes much more time to find a possible answer than what would be acceptable for a real-time system. For this reason we are considering in future versions of RAPOSA, to develop an hybrid system which relies on previously semantically-tagged text to try to answer a question, only using plain text sources (which will be annotated on demand) whenever the answer is not found by searching the tagged text.</p><p>The architecture of RAPOSA is currently a pipeline consisting of 6 blocks (see figure <ref type="figure" coords="2,479.51,465.67,3.88,8.74" target="#fig_0">1</ref>).</p><p>1. Question Parser: receives a question in raw text, identifies the type of question, its arguments, possible restrictions and other relevant keywords, and transforms it into a canonical form. The admissible types of answer for the question are also identified at this stage. The question parser uses the SIEM ÊS NER for identifying mentions to named-entities and other related phrases in the question.</p><p>2. Query Generator: this module is responsible for preparing a set of query objects from the previously parsed question so that the following Snippet Searcher module can invoke the required search engines. The Query Generator selects which words must necessarily occur in target text snippets (for examples, a name that was identified by the Question Parser), and which words are optional. Some query expansion techniques could be applied in this module, but currently only very simple regular expression generalizations are made (stripping suffixes and changing them by wild-cards).</p><p>3. Snippet Searcher: the Snippet Searcher uses the information given by the Query Generator for querying the available text bases and retrieving text snippets where candidate answer may eventually be found. The collections that RAPOSA is currently able to query are the CLEF collection for Portuguese, a MySQL encoded version of the XML dump of the portuguese Wikipedia<ref type="foot" coords="2,214.97,693.23,3.97,6.12" target="#foot_0">1</ref> , and the BACO <ref type="bibr" coords="2,297.68,694.81,10.52,8.74" target="#b6">[7]</ref> text base (we plan to develop an interface to web search engines soon). After being retrieved, all text snippets are annotated by SIEM ÊS (NER and related phrases). The Answer Extractor has two possible strategies to find candidate answers. The first one is based on a set of context evaluation rules. These rules check the presence of certain specific contexts (words or semantic tags) around the position of the argument of the question (note that the argument of the question must be present in the text snippets retrieved by the Snippet Extractor), and extracts specific elements for being considered as candidate answers. Context evaluation rules lead to high precision results, but they are usually too restrictive, which affects the recall. These rules have been developed manually, which is a severe limitation on the scalability of this strategy.</p><p>For this reason we developed a second strategy for answer extraction is much simpler: to consider as possibly answer any element found in the annotated text snippets that is semantically compatible to the expected semantic category of the answer for question at stake (as identified by the Question Parser). Although more than one compatible element may exist in the snippet being analyzed, this strategy can actually lead to good results. provided that there is enough redundancy in the answer. In such cases, the most frequently found candidate in all the analyzed text-snippets has a good chance of being the correct answer. We call this the simple type checking strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Answer Fusion: the role of the Answer Fusion module is to cluster lexically different but semantically equivalent (or overlapping) answers in to a single "answer group". At this moment, this module is not yet developed: it simply outputs what it receives from the Answer Extractor.</p><p>6. Answer Selector: selects one of the candidate answers produced by the Answer Fusion module and choses the best supporting text / answer justification among previously extracted text snippets. Finally, the Answer Selector also assigns a confidence value to that answer.</p><p>There are other question answering systems for Portuguese that follow either a similar shallow parsing and on-line approach <ref type="bibr" coords="4,239.98,169.80,9.97,8.74" target="#b1">[2]</ref>, or that make use of more deeper parsing tecnhologies and knowledge bases ( <ref type="bibr" coords="4,167.87,181.76,10.80,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="4,201.36,181.76,10.30,8.74" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Improving RAPOSA</head><p>Last year, the Question Analyzer had a set of 25 rules to analyze the type of question and to identify its elements. These rules were able to address only a few types of questions, mostly (i) factoid questions about dates, places, and quantities, and (ii) questions about people, both factoid and simple definition question. This year, a significant amount of work was invested in improving and creating rules for question parsing, both for definition and for factoid question.</p><p>We manually extended the set of rules for processing factoid questions to 72 rules. RAPOSA is now able to process factoid questions for which answers can be dates, several type of numeric values (quantities, dimensions and money) or entities (location, organization, geo-political entities, people, facilities and infrastructures, or other undefined entities). Also, the set of rules to process definition questions was expanded to 30 rules to deal with definitions regarding people, acronyms and miscellaneous objects. Additionally, we added some chunking capabilities to the Question Analyzer module, that allow the identification of some compound words and specific noun phrases, prior to the actual parsing of the question. This enables RAPOSA to deal with questions that do not directly mention named-entities.</p><p>The Answer Extraction module was also improved with the addition of several new extraction rules. Last year, RAPOSA had 25 context evaluation rules for dealing with question of the type "Who is &lt; job title &gt;?" and 6 to deal with questions of the type"Who is &lt; person name &gt;?". For all other question addressed last year we would extract answer candidates using the simple type checking strategy. This year, we were able to make some minor corrections on the context evaluation rule that we already had for "Who is...?" questions, and we developed additional context evaluation rules for dealing with acronyms (15 rules), organizations (2 rules), miscellaneous definitions (9 rules). For the factoid questions we kept the same type checking strategy that we followed last year. The only difference is that we developed more associations between questions and answer types because the Question Parser is now able to deal with more types of factoid questions.</p><p>The final improvement made this year was the possibility of searching Wikipedia text. We converted the XML version of the portuguese Wikipedia into a tabular format which was then loaded into a MySQL database. From all the information that is provided in the XML dumps only the text contained in the paragraphs of Wikipedia articles was kept, i.e. links, category information, related information boxes were ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We submitted for evaluation only one run, for the monolingual Portuguese question set. The number of questions that RAPOSA attempted to answer was less than the 200 questions included in the test set. As mentioned before we skipped all list/enumeration questions and we did not try to answer dependent question since we had no method for tackling the co-reference problem. In fact, for any group of dependent questions RAPOSA only tried to parse the first question: the rest of the questions were ignored, which immediately excluded 50 dependent questions. From the remaining question RAPOSA was only able to parse 107 questions. The unparsed questions received the NIL answer, which was almost always the incorrect answer.</p><p>We are mainly concerned in analyzing the performance of RAPOSA over the 107 question that it was able to parse, because those are the ones for the whole information extraction pipeline was activated. We will divide the analysis of results in two sections, which correspond in practice to the two types of strategies that were used for extracting answer candidates: (i) definitions questions (29), for which we used context evaluation rules, and (ii) factoid questions (78) for which we used the simple type checking strategy.</p><p>Also, for several reasons, as explained in <ref type="bibr" coords="5,280.84,159.84,9.96,8.74" target="#b7">[8]</ref>, the Snippet Extractor of RAPOSA was frequently unable to find text snippets for extracting possible answers. The text search capabilities of RA-POSA are still in a very early stage of development, and they are basically not more than simple SQL queries over MySQL encoded text databases. Thus, many of the answers produced by RA-POSA are NIL (48 %) for the simple reason that it was not able to find any text snippet. This issue will be matter of future work, so for now we will focus mainly in the precision of RAPOSA when it is able to find an answer (i.e. answer is not NIL). The results we present in the next tables will address the following parameters for each type of question:</p><p>• #: number of question that RAPOSA attempted to answer</p><p>• T: number of question for which a Text answers was found (i.e. not NIL)</p><p>• NIL: number of question answered NIL • R, X, W: Right, ineXact and Wrong answers</p><p>• R | T: Right questions given that a Text answered was provided Table <ref type="table" coords="5,117.00,363.08,4.98,8.74" target="#tab_0">1</ref> on presents the aggregated results over all the question. The last two columns contain the values of the overall precision (right answers / total questions attempted) and the precision when a non-NIL answer was produced. In the two following sections we will detail the results for the case of definition questions (DEF) and factoid questions (F). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Definitions Questions</head><p>We consider Definition Questions all questions for which the answer is a phrase or a sentence that provides an explanation about a named-entity or concept. These include questions about:</p><p>• people of the form "Who is &lt;person name&gt;?", such as "Quem é George Vassiliou?" / "Who is George Vassiliou?" . We will refer to such questions by DEF HUM.</p><p>• acronyms (similar to the acronym expansion task) of the form "What is &lt;acronym&gt;?" such as "O que é a FIDE?" / "What is FIDE?". These questions will be identified by DEF ACR.</p><p>• miscellaneous objects of the form "What is &lt;something&gt;?" such as "O que é a Granja do Torto?" / "What is Granja do Torto?". We will refer to this questions as DEF MISC.</p><p>RAPOSA successfully parsed and attempted to answer 29 definition questions. There were 31 definition question in this year's test set, so RAPOSA was able to parse nearly all of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Factoid Questions</head><p>Factoid questions include all sorts of question for which the answer is a simple factual elements, such the name of a named-entity (person, organization, location, etc), a date, a numerical value, etc. In this years test set there where 159 questions that were classified as factoid questions. RA-POSA specializes the general category of factoid questions into different sub-categories according to the type of answers that they require:</p><p>• person (F PER): the correct answer will be a name of a person (real or fictional) or a character, or in some cases, a given group (such as the name of a rock band). E.g.: "Quem foi o último rei da Bulgária?" / "Who was the last King of Bulgaria?"</p><p>• geo-political entities (F GPE): these questions have as admissible answers the names of geo political entities such as countries, states, cities, etc. E.g.: "Qual é a capital de Timor Ocidental?" / "What is the capital of East Timor?"</p><p>• organization (F ORG): the answer for these questions must the name of an organization E.g.: "A que partido pertencia Spadolini?" / "Which party did Spadolini belong to?"</p><p>• entity (F ENT): this is a more general case of the previous question where the type of the entity that corresponds to the correct answer is not very well defined (or requires deeper semantics analysis for its definition) and can be any either a person, organization or a GPE.</p><p>• location (F LOC): these questions have as an answer the name of a geographic location. E.g. "Onde é que nasceu o Vítor Baía?" / "Where was Vítor Baía born?"</p><p>• infra-structures (F INFRA): the expected answer is a reference to a infra-structures, such as a brigde, a road, etc. E.g.: "Qual o túnel ferroviário mais comprido do mundo?" / "Which is the world's longest train tunnel?"</p><p>• date / time (F DATE): the expected answer is a date, a specific time or a time period. E.g. "Em que ano foi fundada a Bertrand?" / "In which year was Bertrand established?"</p><p>• dimensions (F DIM): the answer must be a value along with an measurement unit. E.g. "Qual o diâmetro de Ceres?" / "What is the diameter of Ceres?"</p><p>• F MNY: the answer for these questions is an amount of money. E.g. "Quanto custou a Mars Observer?" /"What as the cost of the Mars Observer?"</p><p>• F NUM: the expected answer is simply a number. E.g.: "Quantas ilhas tem Cabo Verde?" / "How many islands does Cabo Verde have?"</p><p>• F RVAL: the type of answer for this question will be a relative value such as a ratio or a percentage. E.g.: "Que percentagem dos finlandeses fala sueco?" / "What is the percentage of Finnish that can speak Swedish?" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Definitions Questions</head><p>Considering first the definitions questions (31 in the test set), we can see that RAPOSA had a reasonably good performance, both from a recall and from a precision point of view. The most remarkable result is the extremely high precision of RAPOSA, in the cases where a non-nil answer is produced. This indicates that current context evaluation rules, when successful in extracting an answer, provide almost always the correct answer.</p><p>However, in about 45% of cases no candidate answer was extracted, and hence a NIL answer was produced. An incorrect NIL answer can occur in two cases: (i) when no snippet was found from which we could extract an answer candidate, or (ii) when the context evaluation rules were not able to match any of the snippets found. As explained before problem (i) will be subject of future work, and we will now focus on problem (ii). Since most of the incorrect nil answers were produced while attempting to answer miscellaneous definition questions, for which we only had 9 evaluation rules and a small related vocabulary, we have a strong indication that the problem here is the lack of enough context evaluation rules, which results in the low recall values for these cases. Clearly, there is a huge room for improvement by simply adding more evaluation rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Factoid Questions</head><p>The precision obtained in answering factoid question was significantly inferior to that obtained definition questions. One immediate reason for this is related to how answers are extracted from retrieved text snippets, i.e. by using the simple type checking mechanism. While such a relaxed strategy seems to work slightly better for dates, GPE and location factoids, it has clearly very low performance for the other types of questions, and seems especially bad in dealing with numeric factoids. Also surprising, but compatible with last years results was the relatively low performance of RAPOSA for factoid questions regarding people.</p><p>One recurrent problem in answering factoid questions using the type checking strategy is that it is relatively frequent to find in a given text snippet more than one element (named-entity / numerical item) compatible with the type we are looking for. When there is enough redundancy in data (i.e. we may find more than one different text snippet that can provide us the answer), we might sill be able to extract the correct element just by checking the most frequent one over all retrieved snippets. This works relatively well in many occasions, and RAPOSA has been relying on this so far. But the chances of choosing the correct answer decrease when there is less text available related to the question at stake. For less frequently documented subjects, for which less text snippets can be found, we end up choosing more or less randomly and obtaining many incorrect answers. This seems to be specially the case for certain factoid question that focus on numeric facts, either because these facts are usually related to very specific subjects (e.g. technical data) with less text available for search, or because there are many distinct ways of expressing the same (or approximate) numeric quantity. We believe that these two factor combined make it quite difficult to extract the correct answer in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with 2006</head><p>From a global point of view, we were able to significantly improve the results obtained last year both in recall and in precision. In 2006, we were submitted two rounds, R1 2006 that only had answers extracted using the context evaluation rules, and R2 2006 which, for several types of factoid questions, we also used the simple type checking mechanism to extract answers.</p><p>One of the most important improvements this year is that we greatly increased the number of different types of questions that RAPOSA can now try to answer. Last year, in round R1 2006 RAPOSA was only able to answer 34 questions, because it only had context evaluation for extracting answers for "Who is &lt; job title &gt;?" and "Who is &lt; person name &gt;?" questions. For R2 2006 , RAPOSA was configured also to extract answer for factoid question using the simple type checking strategy, so it attempted to answer 74 questions. This year, mostly because of the work invested in (i) creating new question parsing rules for several types of factoid questions (which involves being able to assign one or more possible answer types to each type of parsed question) and (ii) creating rules to parse more definition questions (namely acronyms and miscellaneous), we were able to greatly increase the number of question that RAPOSA tried to answer (109 against 79).</p><p>If we consider questions for which RAPOSA used context evaluation rules (i.e. comparing current results regarding definition question), we might be lead to think that there was no improvement at all because last in year's R1 2006 run RAPOSA attempted two answer 34 question while this year RAPOSA only attempted to answer 29. However, in this year's question set there was a huge reduction in the "Who is &lt; person name &gt;? questions, which were the majority of definition questions in last year's set. On the other hand, this year we were able to answer 18 miscellaneous a 3 acronyms definition questions, which were completely ignored types last year.</p><p>As far as precision is concerned, we were able to significantly improve the results. Last year, RAPOSA achieved and global precision of 0.18 in run R1 2006 and 0.23 in run R2 2006 and a precision of 0.31 in run R1 2006 and 0.29 in run R2 2006 if we consider only non-nil answers. The overall results obtained this year (Table <ref type="table" coords="8,202.56,516.95,4.43,8.74" target="#tab_0">1</ref>) are much higher in both cases: 0.33 and 0.54. The increase is mostly justified by the high precision that RAPOSA was able to obtain for definition questions (for which several context evaluation rules are available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>While there are certainly many possibilities for improvement there are two points where we should invest more in the near future, and which are somehow connected. The first point is concerned with reducing the number of NIL answers that RAPOSA produces because of the insufficient number text snippets it extracts from text sources, sometimes none at all. In many cases the answer to a question can be found in the available text sources but the context surrounding that answer is expressed in different words of those contained in the questions. Currently, the Query Generator is not able to deal with situations where the interesting texts snippets would be found via generalization, specialization or paraphrasing of the words in the question.</p><p>Since we lack the lexical-semantic resources for Portuguese, such as thesauri or paraphrase dictionaries, to improve query generation in an efficient a scalable way, we have to find automatic ways of dealing with the problem. We are thus considering an approach that involves the automatic generation of paraphrases such as the ones described in <ref type="bibr" coords="8,337.06,727.14,10.52,8.74" target="#b3">[4]</ref> and <ref type="bibr" coords="8,370.78,727.14,9.96,8.74" target="#b2">[3]</ref>. One interesting "side effect" of increasing the number of snippets retrieved from the text sources is that we are also potentially increasing the redundancy contained in the text to be analyzed, making it also more probable to find a correct answer, especially when using the simple type checking mechanisms for answering factoid questions.</p><p>Our second concern is the identification in free text of elements that, isolated or in specific contexts might be relevant to the answer certain types of questions. For example, for answering like questions like "Who is/was &lt; person name &gt;?", e.g. "Who was Fernando Pessoa?" text snippets such "... the poet Fernando Pessoa...", would contain of) the correct answer. Therefore we would like to semantically annotate such text snippets -"... the[poet] JOB [Fernando Pessoa] HU M ..." -in order to perform answer extraction based on such high-level semantic tags. Our NER system already executes part of this job but it becomes very difficult to expand its semantic annotation rules, and related lexicon, to accommodate all variations needed for efficient open-domain question answering.</p><p>Therefore, our current line of research is on developing systems that perform tagging by example. Our goal is to have a system that, starting from a few examples of what elements would be good sources for answering a given type of question (the tagged snippet showed previously) would learn by bootstrapping how to detect and tag similar instance in text. For example, starting from "... the [poet] JOB [Fernando Pessoa] HU M ...", we would like the system to be able to annotate "similar" text instances such as "... the [painter] JOB [Pablo Picasso] HU M ..." or "...the [Queen of the Netherlands] JOB [Beatrix Wilhelmina Armgard van Oranje-Nassau] HU M ... ", which would all be a good source of information for answering "Who is/was &lt; person name &gt;?". Ideally, the system would generalize and be able to also to tag other variations than convey similar and additional information such as "... For other types of questions, one would also (manually) point the learning system to other elements in text where the corresponding answers could be found. This would allow to semantically annotate other elements that could be then used to answer those types of questions. The underlying learning system follows a lightly-supervised learning strategy, which starting from a set of seed examples is able to learn similar instances by a bootstrapping mechanism. The core of the system is the ability to identify type similarity between elements (names, nouns, noun-phrases, adjectives), so as to be able to find in text instances that convey parallel or nearly-parallel information. Our recent experiments <ref type="bibr" coords="9,173.13,470.68,15.50,8.74" target="#b9">[10]</ref> in trying to identify type similar named-entities look promising, and suggest that the tagging by example approach may be viable and scalable to help answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We were able to significantly increase the performance of RAPOSA, both on recall and precision by improving two fundamental blocks of the QA pipeline: Question Parsing and Answer Extraction. In both cases improvements were achieved by manually expanding the corresponding set rules. We were able to achieve high precision results in answering Definition Questions, mostly due to the good performance of the context evaluation rules. The results obtained in answering factoid questions using the more relaxed simple type checking strategy are still modest, but are slightly better the ones obtained last year.</p><p>We have pointed out ways for improving the global performance of the system. The first is including more efficient query expansion techniques in the text snippet retrieval stage: this will help reducing the number of NIL answers and increase the redundancy among the candidate answers, and thus also the overall precision of RAPOSA. The second, is based on the concept of "tagging by example" which would allow us to identify in text possible answers for several types of questions, based on a few seed examples of what text snippets that can contain such answers look like. We believe that this is a promising approach for improving the performance of RAPOSA in a scalable way, both for definitions and for factoid questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,202.70,421.97,197.60,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The basic components of RAPOSA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,250.26,363.08,262.74,9.65;9,90.00,375.03,360.81,9.65"><head></head><label></label><figDesc>the [[german] [composer] JOB ] [Richard Wagner] HU M ..." or "...[António Guterrez] HU M , the [[former] [[portuguese] [prime-minister] JOB ]], ...".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,132.06,422.16,338.88,42.95"><head>Table 1 :</head><label>1</label><figDesc>Global result for the 107 questions that RAPOSA was able to parse.</figDesc><table coords="5,145.05,422.16,311.33,21.09"><row><cell>type</cell><cell>#</cell><cell cols="5">T NIL %NIL R X W R|T P all P R|T</cell></row><row><cell cols="3">DEF + F 107 56</cell><cell>51</cell><cell>48.0</cell><cell>35 2 71</cell><cell>30</cell><cell>0.33 0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,653.53,423.00,68.51"><head>Table 2 (</head><label>2</label><figDesc>similar to Table1) presents an overview of the results obtained. Since RAPOSA could use either Wikipedia or the Portuguese CLEF corpus for extracting the answer, it is also interesting to check how many times each of them was actually used for that purpose. For the 16 questions for which RAPOSA found 7 non-nil answer in Wikipedia a 9 non-nil answers in CLEF corpus.</figDesc><table coords="6,143.47,110.82,314.50,58.15"><row><cell>type</cell><cell cols="9"># T NIL %NIL R X W R|T P all P R|T</cell></row><row><cell>DEF HUM</cell><cell>8</cell><cell>8</cell><cell>0</cell><cell>0.0</cell><cell>7</cell><cell>1</cell><cell>0</cell><cell>7</cell><cell>0.88 0.88</cell></row><row><cell cols="3">DEF MISC 18 6</cell><cell>12</cell><cell>67</cell><cell>7</cell><cell cols="2">0 11</cell><cell>6</cell><cell>0.39 1.00</cell></row><row><cell>DEF ACR</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>33</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>0.67 1.00</cell></row><row><cell></cell><cell cols="2">29 16</cell><cell>13</cell><cell>45</cell><cell cols="3">16 1 12</cell><cell>15</cell><cell>0.59 0.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,136.26,182.10,330.49,8.74"><head>Table 2 :</head><label>2</label><figDesc>The performance of RAPOSA in answering 29 Definition Question</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,710.52,423.00,32.65"><head>Table 3 (</head><label>3</label><figDesc>similar to previous) contains the results by type of factoid question. RAPOSA extracted most of his non-nil answers from WIkipedia -35 answers -while the CLEF corpus was only used for extracting 5 answers.</figDesc><table coords="7,147.20,110.82,307.03,156.98"><row><cell>type</cell><cell cols="10"># T NIL %NIL R X W R|T P all P R|T</cell></row><row><cell>F DATA</cell><cell cols="2">15 9</cell><cell>6</cell><cell>40</cell><cell>5</cell><cell>1</cell><cell>9</cell><cell>4</cell><cell cols="2">0.33 0.44</cell></row><row><cell>F GPE</cell><cell cols="2">15 9</cell><cell>6</cell><cell>40</cell><cell>6</cell><cell>0</cell><cell>9</cell><cell>6</cell><cell cols="2">0.40 0.67</cell></row><row><cell>F ORG</cell><cell>6</cell><cell>1</cell><cell>5</cell><cell>83</cell><cell>1</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell cols="2">0.17 0.00</cell></row><row><cell>F HUM</cell><cell>7</cell><cell>4</cell><cell>3</cell><cell>43</cell><cell>1</cell><cell>0</cell><cell>6</cell><cell>1</cell><cell cols="2">0.14 0.25</cell></row><row><cell>F ENT</cell><cell>9</cell><cell>3</cell><cell>6</cell><cell>67</cell><cell>1</cell><cell>0</cell><cell>8</cell><cell>1</cell><cell cols="2">0.11 0.33</cell></row><row><cell cols="2">F INFRA 2</cell><cell>0</cell><cell>2</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0.00</cell><cell>-</cell></row><row><cell>F LOC</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>50</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>1</cell><cell cols="2">0.50 0.50</cell></row><row><cell>F DIM</cell><cell>5</cell><cell>4</cell><cell>1</cell><cell>20</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell></cell><cell cols="2">0.00 0.00</cell></row><row><cell>F MNY</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>00</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell cols="2">0.00 0.00</cell></row><row><cell>F NUM</cell><cell cols="2">11 7</cell><cell>4</cell><cell>36</cell><cell>2</cell><cell>0</cell><cell>9</cell><cell>2</cell><cell cols="2">0.18 0.29</cell></row><row><cell>F RVEL</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>0.00</cell><cell>-</cell></row><row><cell></cell><cell cols="2">78 40</cell><cell>38</cell><cell>49</cell><cell cols="3">18 1 59</cell><cell>15</cell><cell cols="2">0.23 0.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,90.00,280.93,376.75,41.41"><head>Table 3 :</head><label>3</label><figDesc>The performance of RAPOSA in answering 78 Definition Question 5 Discussion of the Results</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,738.20,296.20,6.99"><p>Available from the University of Amsterdam: http:ilps.science.uva.nl/WikiXML</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work was partially supported by grant <rs type="grantNumber">SFRH/BD/ 23590/2005</rs> from <rs type="funder">Fundação para a Ciência e Tecnologia (Portugal)</rs>, co-financed by <rs type="funder">POSI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2BDUsux">
					<idno type="grant-number">SFRH/BD/ 23590/2005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.48,200.56,402.51,8.74;10,110.48,212.52,402.52,8.74;10,110.48,224.47,402.52,8.74;10,110.48,236.43,299.68,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,195.49,212.52,312.78,8.74">Priberam&apos;s question answering system in a cross-language environment</title>
		<author>
			<persName coords=""><forename type="first">Adán</forename><surname>Cassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helena</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Afonso</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cláudia</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,415.48,224.47,97.52,8.74;10,110.48,236.43,196.05,8.74">Working Notes of the Cross-Language Evalaution Forum Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,256.35,402.52,8.74;10,110.48,268.31,402.53,8.74;10,110.48,280.26,402.51,8.74;10,110.48,292.22,402.51,8.74;10,110.48,304.17,252.39,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,165.72,256.35,244.31,8.74">Question answering beyond CLEF document collections</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.90,280.26,206.09,8.74;10,110.48,292.22,83.83,8.74">7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<title level="s" coord="10,452.79,292.22,60.21,8.74;10,110.48,304.17,88.72,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-09">September 2006. 2007</date>
		</imprint>
	</monogr>
	<note>Revised Selected papers</note>
</biblStruct>

<biblStruct coords="10,110.48,324.10,402.52,8.74;10,110.48,336.05,402.52,8.74;10,110.48,348.01,402.52,8.74;10,110.48,359.97,311.33,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,294.84,324.10,218.16,8.74;10,110.48,336.05,224.41,8.74">Answering the question you wish they had asked: The impact of paraphrasing for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,356.02,336.05,156.98,8.74;10,110.48,348.01,318.50,8.74">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,379.89,402.52,8.74;10,110.48,391.85,402.52,8.74;10,110.48,403.80,181.32,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,281.42,379.89,231.58,8.74;10,110.48,391.85,45.61,8.74">Learning paraphrases to improve a question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Collin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,182.23,391.85,330.77,8.74;10,110.48,403.80,150.53,8.74">Proceedings of the 10th Conference of EACL Workshop Natural Language Processing for Question-Answering</title>
		<meeting>the 10th Conference of EACL Workshop Natural Language Processing for Question-Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,423.73,402.52,8.74;10,110.48,435.68,402.52,8.74;10,110.48,447.64,402.52,8.74;10,110.48,459.59,327.87,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,331.37,423.73,181.63,8.74;10,110.48,435.68,67.63,8.74">A logic programming based approach to qa@clef05 track</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irene</forename><forename type="middle">Pimenta</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,145.07,459.59,152.10,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Rijke</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="351" to="360" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,477.00,402.52,11.26;10,110.48,491.47,402.52,8.74;10,110.48,503.43,402.52,8.74;10,110.48,515.38,22.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,178.30,477.00,330.81,11.26">SIEM ÊS -a named entity recognizer for Portuguese relying on similarity rules</title>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,123.65,491.47,389.35,8.74;10,110.48,503.43,70.68,8.74">PROPOR 2006 -Encontro para o Processamento Computacional da Língua Portuguesa Escrita e Falada</title>
		<meeting><address><addrLine>ME -RJ / Itatiaia; Rio de Janeiro -Brasil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-17">13 a 17 de Maio 2006</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,535.31,402.51,8.74;10,110.48,547.26,402.52,8.74;10,110.48,559.22,402.52,8.74;10,110.48,571.17,324.44,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,180.32,535.31,227.53,8.74">BACO -A large database of text and co-occurrences</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,180.79,559.22,332.21,8.74;10,110.48,571.17,106.64,8.74">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)</title>
		<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Odjik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</editor>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,591.10,402.52,8.74;10,110.48,603.05,402.52,8.74;10,110.48,615.01,220.32,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,185.10,591.10,174.86,8.74">Hunting answers with RAPOSA (FOX)</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,266.03,603.05,246.97,8.74;10,110.48,615.01,40.84,8.74">Working Notes of the Cross-Language Evalaution Forum Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">20-22 September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,634.93,402.52,8.74;10,110.48,646.89,402.52,8.74;10,110.48,658.84,402.52,8.74;10,110.48,670.80,402.52,8.74;10,110.48,682.75,316.53,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,179.32,634.93,290.21,8.74">A first step to address biography generation as an iterative QA task</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.24,658.84,159.76,8.74;10,110.48,670.80,137.38,8.74">7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<title level="s" coord="10,110.48,682.75,152.88,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain; Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-09">September 2006. 2007</date>
		</imprint>
	</monogr>
	<note>Revised Selected papers</note>
</biblStruct>

<biblStruct coords="10,110.48,702.68,402.52,8.74;10,110.48,714.64,402.52,8.74;10,110.48,726.59,272.44,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,434.62,702.68,78.37,8.74;10,110.48,714.64,156.22,8.74">More Like These&quot;: Growing Entity Classes from Seeds</title>
		<author>
			<persName coords=""><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugénio</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,292.90,714.64,220.10,8.74;10,110.48,726.59,241.89,8.74">Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM 2007)</title>
		<meeting>the Sixteenth ACM Conference on Information and Knowledge Management (CIKM 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
