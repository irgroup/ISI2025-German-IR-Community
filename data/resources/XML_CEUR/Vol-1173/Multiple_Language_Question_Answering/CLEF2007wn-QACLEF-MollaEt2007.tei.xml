<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.39,148.62,372.34,15.51;1,121.44,170.53,360.20,15.51">AnswerFinder at QAst 2007: Named Entity Recognition for QA on Speech Transcripts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.87,204.00,50.91,9.96"><forename type="first">Diego</forename><surname>Mollá</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Macquarie University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.69,204.00,57.65,9.96"><forename type="first">Steve</forename><surname>Cassidy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Macquarie University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.11,204.00,83.03,9.96"><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Macquarie University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.39,148.62,372.34,15.51;1,121.44,170.53,360.20,15.51">AnswerFinder at QAst 2007: Named Entity Recognition for QA on Speech Transcripts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ACC8E00C10CADAB254DDB68D2ED278F5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2.7 [Natural Language Processing]: Text Analysis Measurement, Performance, Experimentation Question answering, Speech transcripts, Named Entity Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Macquarie University's contribution to the QAst track of CLEF is centered on a study of Named Entity (NE) recognition on speech transcripts, and how such NE recognition impacts on the accuracy of the final question answering system. We have ported AFNER, the NE recogniser of the AnswerFinder question-answering project, to the types of answer types expected in the QAst track. AFNER uses a combination of regular expressions, lists of names (gazetteers) and machine learning. The machine learning component is a Maximum Entropy classifier and was trained on a development set of the AMI corpus. Problems with scalability of the system and errors of the extracted annotation lead to relatively poor performance in general, though the system was second (out of three participants) in one of the QAst subtasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>AnswerFinder is a question answering system that is being developed focusing on shallow semantic representations of questions and text <ref type="bibr" coords="1,251.36,621.89,10.52,9.96" target="#b3">[4,</ref><ref type="bibr" coords="1,264.65,621.89,7.01,9.96" target="#b4">5]</ref>. The underlying idea is that these semantic representations reduce the impact of paraphrases (different wordings of the same information). Overall, the system uses symbolic algorithms to find exact answers to questions in large document collections.</p><p>The design and implementation of the AnswerFinder system has been driven by requirements that the system should be easy to configure, extend, and, therefore, port to new domains. To measure the success of the implementation of AnswerFinder in these respects, we decided to participate in the QAst competition. The task in this competition is different from that for which AnswerFinder was originally designed. Applying the system to a new task would illustrate potential problems with respect to configurability and extensibility.</p><p>In addition, in our contribution we focused on the localisation of AFNER, our Named Entity Recogniser (NER), for speech transcripts and its application for Question Answering. Named Entity (NE) recognition is the task of finding instances of specific types of entities in free text. This module is typically one of the most important sources of possible answers available to QA systems and therefore an improvement on its accuracy should result on an improvement of the accuracy of the complete QA system.</p><p>The AFNER system, just like the AnswerFinder system, was designed with flexibility in mind. Since the properties of the NE recognition task in this competition are quite different in several respects to that of which AFNER was originally designed, the QAst competition also allowed us to measure the success of our AFNER implementation according to the configurability and extensibility criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Answering for Speech Transcripts</head><p>The task of Text-Based Question Answering (TBQA) has been very active during the last decade, mostly thanks to the Question Answering track of the Text REtrieval Conference (TREC) <ref type="bibr" coords="2,478.37,273.02,9.96,9.96" target="#b8">[9]</ref>. The kinds of questions being asked range from fact-based questions (also known as factoid questions) to questions whose answer is a list of facts, or definitions. The methods and techniques used have converged to a prototypical, pipeline-based architecture like the one we will describe here, and only recently the task has been diversified to more complex tasks such as TREC's QA task of complex interactive question answering <ref type="bibr" coords="2,264.98,332.80,10.52,9.96" target="#b1">[2]</ref> or the Document Understanding Conference (DUC)'s track of query-driven summarisation <ref type="bibr" coords="2,252.14,344.76,9.96,9.96" target="#b2">[3]</ref>.</p><p>The present CLEF pilot track QAst presents an interesting and challenging new application of question answering, and in this contribution we have attempted to re-use as much as we could of AnswerFinder, a TBQA system that is designed for configurability, flexibility and portability to other domains. Part of our interest in participating in QAst was to test AnswerFinder's portability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AnswerFinder</head><p>The AnswerFinder question answering system is essentially a framework consisting of several phases that work in a sequential manner. For each of the phases, a specific algorithm has to be selected to create a particular instantiation of the framework. The aim of each of the phases is to reduce the amount of data the system has to handle from then on. This allows later phases to perform computationally more expensive operations on the remaining data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Analysis</head><p>Sentence Selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type</head><p>Answer Selection Final Answer(s)</p><p>Figure <ref type="figure" coords="2,243.70,652.93,3.87,9.96">1</ref>: AnswerFinder system overview Figure <ref type="figure" coords="2,136.30,685.20,4.98,9.96">1</ref> provides an overview of the AnswerFinder framework. The first phase is a document retrieval phase that selects relevant documents. AnswerFinder was developed to work on large document collections and this phase typically reduces a great amount of text that will be handled in subsequent steps.</p><p>Next is the sentence selection phase. This can actually be a sequence of steps, each of which selects a subset of sentences from the relevant documents selected in the previous phase. During sentence selection, all sentences that are still left (e.g. all sentences in the selected documents in the first step) are scored against the question using a relevance metric. The most relevant sentences according to this metric are kept for further processing.</p><p>After sentence selection, the remaining sentences are passed to the answer selection phase. The answer selection phase aims at selecting the best of the possible answers to return to the user. In the experiments described here, the list of possible answers is provided by a NER. <ref type="foot" coords="3,463.01,170.36,3.97,6.37" target="#foot_0">1</ref> Thus, the question is analysed, providing information about the kind of answer that is required. From the possible answers, those that match the type of answer (required by the question) are selected and scored.</p><p>Finally, the best answer is returned to the user. Best answer in this context is considered to be the answer that has both the highest score and an answer type that matches the question, or simply the answer with the highest score if none of the possible answers fits the expected answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Applying AnswerFinder to Speech Transcripts</head><p>Question answering on speech transcripts introduces specific challenges compared to TBQA due to the nature of the genre and the process of transcription. AnswerFinder has been initially developed to work on news articles. News articles are typically well-written pieces of text. Analysing the documents in the QAst competition, it is clear that speech transcripts are different. For example:</p><p>• There are frequent false starts and sentences that are interrupted in the discourse.</p><p>• There are filling words that usually do not appear in free text (and in particular news text), such as "er", "uh", etc. In our experiments, this is particularly problematic when these words appear inside a named entity, e.g. "Rufford, um, Sanatorium, that's right".</p><p>• The grammatical structure of the transcription does not conform with that of free text. Consequently most tools, such as parsers and chunkers, which would normally be used in specific AnswerFinder phases, produce very poor results.</p><p>• If the transcript is an automatic transcript (produced by a speech recogniser) there are errors of transcription and missing information, most notably punctuation characters and capitalised characters. This information is used in many phases of AnswerFinder to answer questions on news data.</p><p>• When using a corpus annotated with named entities, the density of named entities in free speech is much smaller than in usual corpora.</p><p>Many of the above features make it difficult to do traditional linguistic processing such as parsing and semantic interpretation. For this reason, many of the instantiations of the phases we have implemented, which typically use complex linguistic processing (which are described in <ref type="bibr" coords="3,90.00,592.00,10.79,9.96" target="#b4">[5]</ref>) would not perform well. We consequently decided not to use AnswerFinder's syntactic and graph-semantic information. Instead we focused on attempting to increase the accuracy of the task of recognition of named entities. Thus, the question answering method used for QAst is entirely based on the task of finding and selecting the right entities.</p><p>In particular, the AnswerFinder framework that generated the QAst 2007 results consists of the following instantiations of the phases:</p><p>• The document selection component returns the full list of documents provided for the complete list of questions. The total number of documents is fairly small and therefore the other components of AF are able to handle all documents. We do not attempt to rank the documents in any way.</p><p>• The sentence selection component is based on the word overlap between the question and the document sentences. This metric counts the number of words that can be found in both question and sentence after removing stop words. A simple sentence splitter method is used, which relies on the existence of punctuation marks when available, or on speech turns. Only sentences that contain NEs of the required type are considered.</p><p>• The question classification component is based on a decision list of hand-constructed patterns of regular expressions.</p><p>• The answer extraction component selects five NEs that are of the expected answer type and have the highest scores. If four or less NEs are found, then a NIL answer is returned as an option after presenting all found NEs. If no NEs of the expected type are found at all, the returned answer is NIL. The score of a NE is the sum of the individual scores of each occurrence of a NE. The individual score of a NE is the confidence of AFNER to label the answer candidate with the particular NE label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AFNER</head><p>Within the AnswerFinder project, we recently incorporated a purpose-built NER, called AFNER <ref type="bibr" coords="4,504.25,325.52,9.96,9.96" target="#b5">[6]</ref>. This NER has been specifically designed for the task of TBQA. AFNER differs from other NERs in that it aims to increase recall of recognition of entities, at the expense of a possible loss of precision <ref type="bibr" coords="4,117.88,361.38,10.52,9.96" target="#b5">[6,</ref><ref type="bibr" coords="4,131.87,361.38,7.01,9.96" target="#b7">8]</ref>. Crucially, it allows the allocation of multiple tags to the same string, thus handling the case of ambiguous entities or difficult entities by not committing to a single tag. The rationale is that we do not want to weed out the right answer at this stage. Instead we let the final answer extraction mechanism make the final decision about what is a good answer. AFNER is ultimately based on machine learning. We use a maximum entropy classifier, and the implementation of this classifier is adapted from Franz Josef Och's YASMET<ref type="foot" coords="4,440.53,420.39,3.97,6.37" target="#foot_2">2</ref> . Obviously, the selection of the features used in the classifier is very important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>The features used by AFNER combine three kinds of information: regular expressions, gazetteers, and properties internal and external to the token.</p><p>The regular expressions used in AFNER are manually created and are useful for identifying strings that match patterns that are characteristic to entity types such as dates, times, percentages, and monetary expressions. These types of named entities are relatively standardised and therefore easy to find with high precision. However, the range of entities that can be discovered using regular expressions is limited. Matching a particular regular expression is a key feature used in identifying entities of these particular types.</p><p>Gazetteers are useful for finding commonly referenced entities such as names. If one or more words are found in one of the gazetteers, which are lists of names, locations, etc., then it is likely that the expression is of the type indicated by the list. However, this this is not always the case. For example, common person names may also be regular words (Smith, Baker ). We use gazetteers as additional features in the classifier to increase the likelihood of these kinds of named entities. It also allows the classifier to use other features that combined may be more determinant for the categorisation of a specific token in particular entities. AFNER uses three lists (locations, person names, and organisations), with a total of about 55,000 entries.</p><p>Finally, there are three types of features that relate to specific aspects of the separate tokens. Firstly, we identify features that illustrate internal token properties including capitalisation, alpha/numeric information, etc. Some specific features are listed in Table <ref type="table" coords="4,414.36,694.59,3.87,9.96">1</ref>.</p><p>Secondly, AFNER incorporates some contextual features. These are features that concentrate on the token in the surrounding text, or relate a token to tokens in surrounding text. These features are implemented through a set of regular expressions that are matched against neighbouring tokens within a context window of the token under consideration. When a regular expression matches the context, this is recorded. These regular expressions detect patterns such as whether the neighbouring token is made of two digits, or whether it is a currency name. Features that consider the class assigned to the previous tokens and all of its class probabilities are also part of this type of feature.</p><p>Thirdly, there is a set of features that measure global information of the tokens. These features are mainly inspired on features described by <ref type="bibr" coords="5,287.18,195.04,9.96,9.96" target="#b0">[1]</ref>. Currently AFNER only checks whether a token is always capitalised in a passage of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regular Expressions Specific patterns for dates, times, etc FoundInList</head><p>The token is a member of a gazetteer InitCaps</p><p>The first letter is a capital letter AllCaps</p><p>The entire word is capitalised MixedCaps</p><p>The word contains upper case and lower case letters IsSentEnd</p><p>The token is an end of sentence character InitCapPeriod Starts with capital letter and ends with period OneCap</p><p>The word is a single capitalised letter ContainDigit</p><p>The word contains a digit NumberString</p><p>The word is a number word ('one', 'thousand', etc.) PrepPreceded</p><p>The word is preceded by a preposition (in a window of 4 tokens) PrevClass</p><p>The class assigned to the previous token ProbClass</p><p>The probability assigned to a particular class in the previous token AlwaysCapped</p><p>The token is capitalised every time it appears Table <ref type="table" coords="5,254.11,413.94,3.87,9.96">1</ref>: Features used in AFNER</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">General Method</head><p>The features described in the previous section are used in a maximum entropy classifier which for each token and for each category computes the probability of the token belonging to the category. Categories in this case are the named entity types prepended with 'B' and 'I' (indicating whether the token is at the beginning or inside a NE respectively), and a general 'OUT' category for tokens not in any entity. So for n named entities, n * 2 + 1 categories are used. The classifier returns a list of tags for each token ordered based on probability. We select only those tags that have a probability of more than half of the probability of the next tag in order. This initial threshold already removes tags that have a low probability. However, we also only allow a certain maximum number of tags to pass through. Preliminary experiments revealed that often the top two or three tag probabilities have similar values, but that tags lower down the list still pass the initial threshold, while they are not correct. By setting a threshold that limits the maximum number of tags to be returned we also filter those out. The results presented in this paper are generated by setting the second threshold to allow two tags per token. Initial experiments showed that this increases recall considerably. Allowing more tags increases recall only slightly while decreasing precision considerably.</p><p>AFNER assigns multiple tags to tokens. By doing this, we aim for high recall. The presence of multiple tags also means that NEs can be nested, meaning that NEs may exist within other NEs.</p><p>Once tokens are assigned tags, they are combined to produce the final list of NEs. Each token that has a 'B' tag assigned to it is considered the beginning of a new NE of that type. All 'I' tags continue a NE if the previous token already had either a 'B' or 'I' tag of the same type assigned to it. If there was no such tag assigned to the previous token, the 'I' tag is taken to be a 'B' tag and indicates the start of a new NE of that type. Additionally, if a 'B' tag is preceded by a token with an 'I' tag, it will be taken to be both as a separate entity (with the previous entity ending) and as a continuation of the previous token.  To compute the probability of a sequence of tokens (with corresponding named entity types), we use the geometric mean. This is done to normalise over the length of the named entities. The computation works as follows. Take P i to be the probability of token i and P 1...n the probability of the entire list of tokens (from begin to end). The geometric mean of the probabilities is computed as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><formula xml:id="formula_0" coords="6,259.15,488.48,63.68,21.09">P 1...n = e n i=1</formula><p>log P i n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptation of AFNER to QAst</head><p>AFNER has been developed to work on news data, and as such, we had to modify parts of the system to allow it to be used in the QAst task. The first adaptation of AFNER is the selection of NE types. Originally AFNER focused on a limited set of entities similar to those defined in the Message Understanding Conferences <ref type="bibr" coords="6,251.61,581.00,9.96,9.96" target="#b6">[7]</ref>, and listed in Table <ref type="table" coords="6,353.25,581.00,3.87,9.96" target="#tab_0">2</ref>.</p><p>For QAst we used a set of entity types that closely resembles the kinds of answers expected, as described by the QAst 2007 specification. The types used by the modified AFNER are listed in Table <ref type="table" coords="6,129.03,616.87,3.87,9.96" target="#tab_1">3</ref>.</p><p>The regular expressions that are used in AFNER to find MUC-type named entities were extended to cover the new types of entities. This process did not require much additional work, other than adding a few common names of shapes and colours. The lists of names that was part of the initial AFNER was left untouched.</p><p>The general machine learning mechanism was left unmodified, and the set of features was also left untouched. The only difference was the choice of training corpus. We mapped the annotated entities of the BBN corpus that we had used previously, and added a fragment of the development set of the AMI corpus.</p><p>However, due to problems of scalability during training (the intermediate files produced were very large due to the increased number of classes the classifier can return) we were not able to  <ref type="table" coords="7,195.10,471.99,4.98,9.96" target="#tab_1">3</ref> shows the total number of entities annotated in the BBN and the AMI parts of the training set. The entity types of each kind of corpus complement each other, though some of the entity types had few instances in the corpora, most notably, the type Language only occurred nine times. We decided to use the BBN corpus to complement the annotations of AMI because some entity types were very scarce in AMI but very common in BBN. Also, the entity types annotated in AMI are not the sort of types that would typically be annotated as named entities. For example, the entity type "Person" would have instances like industrial designer. Furthermore, the quality of some of the annotations of the AMI corpus was very bad, to the point that, for example, the entity type "Color" would have instances like fancy or uh, and even just punctuation marks such as commas or periods. The later errors of annotation make us suspect that perhaps the process to extract the entities from the AMI corpus, which was very laborious, had one mistake or two. We plan to revise the full process of extraction and re-do the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We participated in all the QAst tasks and provided two runs per task. The first run used the full AFNER system, whereas the second run used a version of AFNER that had the machine learning component disabled. The results are shown in Table <ref type="table" coords="7,321.64,694.13,3.87,9.96" target="#tab_2">4</ref>.</p><p>The results returned by CLEF indicate, as expected, comparatively poor performance with respect to the other participants. We are pleased to notice, however, that the results of task 3 are second best (from a group of three participants). Task 3 is the task that used the AMI transcripts and it was the task that we used to develop and fine-tune the system. The other tasks 1, 2, and 4 simply used the same settings. We are particularly pleased to learn that the results of task 3 are higher than the results we obtained during development time. This is possibly due to the nature of our experiments, since we automatically applied the answer patterns to the answers found, and it could have been the case that correct answers which happened not to match the patterns were automatically marked as incorrect in our experiments. The evaluations carried by CLEF used human judges so they would be able to detect correct answers that had an unusual format.</p><p>Our preliminary experiments indicated that the machine learning component was not helping the question answering process at all. The CLEF results show some increase of correct answers in the first run (with machine learning) in the tasks based on the CHIL corpus (tasks 1 and 2) but a decrease of correct answers in the tasks based on the AMI corpus (tasks 3 and 4). Our preliminary experiments used the AMI corpus only, and therefore the results are consistent with our experiments. Given the poor overall results with the CHIL corpus it is reasonable to suspect that the patterns and lists do not do well with the CHIL corpus and therefore the machine learning component can help. The patterns and lists were not fine-tuned either for CHIL or for AMI, they were simply the ones we used for the original, news-based BBN text corpus. We will investigate the relative impact of the patterns and lists on one side and the machine learning component on the other side for the speech transcripts.</p><p>Our method to handle NIL questions is simple yet relatively effective to the point that correct NIL answers were a significant part of the correct answers. Task 4 in particular, which has 15 NIL questions, results in a halved MRR (from 14.10% down to 7.05% in our second run) when all NIL questions are removed. Still, task 3 has relatively good results after removing all NIL questions (from 26.39% down to 22.38% in our second run). The results of the non-NIL questions are shown in Table <ref type="table" coords="8,129.03,524.10,3.87,9.96" target="#tab_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Further Work</head><p>In our contribution to QAst we reused as much as we could of AnswerFinder, our question answering system, and AFNER, our Named Entity recogniser. Due to the nature of the speech corpus we simplified the processing done by AnswerFinder and made it rely more heavily on the entities found by AFNER. The whole experiment showed that both AnswerFinder and AFNER are flexible and can be adapted easily to new tasks.</p><p>The small training corpus and the presence of annotation errors in the AMI corpus made the machine learning component of AFNER ineffective. An immediate line of further research is to investigate the cause of the errors, and correct them. Other lines of research are:</p><p>• Revise the machine learning component of AFNER, possibly replace it with another more scalable method, so that larger training corpora can be used.</p><p>• Review the features used for identifying the entities. Most of the current features rely on information about capitalisation, presence of digits, or punctuation marks but none of those are available on speech transcripts.</p><p>• Use additional corpora. There are a few corpora of speech transcriptions available with annotations of named entities that we could use. Among the options is the corpus of speech transcripts within the SQUAD project with the UK Data Archive at the University of Edinburgh.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,90.00,375.34,422.99,9.96;6,90.00,387.30,174.11,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Named entities as multiple labels. The token-based labels appear above the words. The final NE labels appear below the words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,110.66,422.99,253.18"><head>Table 2 :</head><label>2</label><figDesc>Entity types used in the original version of AFNERThe result of this algorithm is an assignment of named entities to the sequence of tags where the named entities may overlap, as is illustrated in Figure2.</figDesc><table coords="6,122.67,110.66,357.65,253.18"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Type</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ENAMEX Organization</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Person</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Location</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TIMEX</cell><cell></cell><cell>Date</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">NUMEX</cell><cell>Money</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Percent</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BPER</cell><cell>ILOC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IPER</cell><cell>BLOC</cell><cell></cell><cell></cell><cell>BLOC</cell><cell></cell><cell>BDATE</cell><cell></cell></row><row><cell>BLOC</cell><cell>IPER</cell><cell cols="2">OUT OUT</cell><cell>IPER</cell><cell cols="3">OUT IDATE OUT</cell></row><row><cell>Jack</cell><cell>London</cell><cell>lived</cell><cell>in</cell><cell>Oakland</cell><cell>in</cell><cell>1885</cell><cell>.</cell></row><row><cell cols="2">PERSON LOCATION</cell><cell></cell><cell></cell><cell>LOCATION</cell><cell></cell><cell>DATE</cell><cell></cell></row><row><cell cols="2">PERSON</cell><cell></cell><cell></cell><cell>PERSON</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LOCATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,110.66,422.97,307.59"><head>Table 3 :</head><label>3</label><figDesc>Named Entities used for QAst. The numbers of entities listed in the two last columns refer to the actual training set (a subset of BBN and AMI).</figDesc><table coords="7,158.06,110.66,286.86,307.59"><row><cell cols="2">Class</cell><cell>Type</cell><cell cols="3"># in BBN # in AMI</cell></row><row><cell cols="3">ENAMEX Language</cell><cell>9</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>Location</cell><cell>2,468</cell><cell></cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>Organization</cell><cell>4,421</cell><cell></cell><cell>27</cell></row><row><cell></cell><cell></cell><cell>Person</cell><cell>2,149</cell><cell></cell><cell>196</cell></row><row><cell></cell><cell></cell><cell>System</cell><cell>0</cell><cell></cell><cell>448</cell></row><row><cell></cell><cell></cell><cell>Color</cell><cell>0</cell><cell></cell><cell>283</cell></row><row><cell></cell><cell></cell><cell>Shape</cell><cell>0</cell><cell></cell><cell>147</cell></row><row><cell></cell><cell></cell><cell>Material</cell><cell>0</cell><cell></cell><cell>267</cell></row><row><cell cols="2">TIMEX</cell><cell>Date</cell><cell>3,006</cell><cell></cell><cell>9</cell></row><row><cell></cell><cell></cell><cell>Time</cell><cell>96</cell><cell></cell><cell>147</cell></row><row><cell cols="2">NUMEX</cell><cell>Measure</cell><cell>2,568</cell><cell></cell><cell>293</cell></row><row><cell></cell><cell></cell><cell>Cardinal</cell><cell>0</cell><cell></cell><cell>646</cell></row><row><cell>Run</cell><cell cols="5">Questions Correct Answers MRR Accuracy</cell></row><row><cell>clt1-t1</cell><cell></cell><cell>98</cell><cell>17.35%</cell><cell>9.98%</cell><cell>6.12%</cell></row><row><cell>clt2-t1</cell><cell></cell><cell>98</cell><cell>16.33%</cell><cell>9.44%</cell><cell>5.10%</cell></row><row><cell>clt1-t2</cell><cell></cell><cell>98</cell><cell>14.29%</cell><cell>7.16%</cell><cell>3.06%</cell></row><row><cell>clt2-t2</cell><cell></cell><cell>98</cell><cell>12.24%</cell><cell>5.88%</cell><cell>2.04%</cell></row><row><cell>clt1-t3</cell><cell></cell><cell>96</cell><cell cols="2">35.42% 24.51%</cell><cell>16.67%</cell></row><row><cell>clt2-t3</cell><cell></cell><cell>96</cell><cell cols="2">33.33% 26.39%</cell><cell>20.83%</cell></row><row><cell>clt1-t4</cell><cell></cell><cell>93</cell><cell cols="2">19.35% 11.24%</cell><cell>6.45%</cell></row><row><cell>clt2-t4</cell><cell></cell><cell>93</cell><cell cols="2">22.58% 14.10%</cell><cell>8.60%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,430.14,422.99,51.80"><head>Table 4 :</head><label>4</label><figDesc>Results of the CLEF runs use all the files. For these experiments we used 26 documents from the AMI corpus and 16 from the BBN corpus. Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,158.06,110.66,286.86,128.56"><head>Table 5 :</head><label>5</label><figDesc>Results of non-NIL questions</figDesc><table coords="8,158.06,110.66,286.86,106.70"><row><cell>Run</cell><cell cols="4">Questions Correct Answers MRR Accuracy</cell></row><row><cell>clt1-t1</cell><cell>88</cell><cell>12.50%</cell><cell>8.56%</cell><cell>6.82%</cell></row><row><cell>clt2-t1</cell><cell>88</cell><cell>11.36%</cell><cell>7.95%</cell><cell>5.68%</cell></row><row><cell>clt1-t2</cell><cell>87</cell><cell>5.75%</cell><cell>4.06%</cell><cell>3.45%</cell></row><row><cell>clt2-t2</cell><cell>87</cell><cell>3.45%</cell><cell>2.87%</cell><cell>2.30%</cell></row><row><cell>clt1-t3</cell><cell>86</cell><cell cols="2">29.07% 22.33%</cell><cell>18.60%</cell></row><row><cell>clt2-t3</cell><cell>86</cell><cell cols="2">25.58% 22.38%</cell><cell>19.77%</cell></row><row><cell>clt1-t4</cell><cell>79</cell><cell>6.33%</cell><cell>3.90%</cell><cell>2.53%</cell></row><row><cell>clt2-t4</cell><cell>78</cell><cell>8.97%</cell><cell>7.05%</cell><cell>5.13%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.25,727.08,407.68,7.17;3,90.00,736.55,422.92,7.17;3,90.00,746.00,72.13,7.17"><p>In general, some sentence selection methods have the ability to generate possible answers that can also be selected during the answer selection phase. However, these algorithms are not used in these experiments as will be discussed in section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="3,164.96,746.00,13.16,7.17"><p>2.2.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,105.25,738.01,135.30,7.34"><p>http://www.fjoch.com/YASMET.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,105.50,201.98,407.50,9.96;9,105.50,213.94,271.60,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,266.59,201.98,246.42,9.96;9,105.50,213.94,104.40,9.96">Named entity recognition: A maximum entropy approach using global information</title>
		<author>
			<persName coords=""><forename type="first">Haoi</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chieu</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,231.21,214.28,115.28,9.18">Proceedings COLING 2002</title>
		<meeting>COLING 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,233.86,407.49,9.96;9,105.50,245.82,370.39,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,228.63,233.86,284.36,9.96;9,105.50,245.82,246.97,9.96">Different structures for evaluating answers to complex questions: Pyramids won&apos;t topple, and neither will human assessors</title>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.37,246.16,70.69,9.18">Proceedings ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,265.75,407.48,9.96;9,105.50,277.70,407.47,9.96;9,105.50,289.65,249.48,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,224.68,265.75,233.01,9.96">Evaluation of question-focused summarization systems</title>
		<author>
			<persName coords=""><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tran</forename><forename type="middle">Dang</forename><surname>Duc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,477.09,266.09,35.89,9.18;9,105.50,278.04,344.64,9.18">Proceedings of the Workshop on Task-Focused Summarization and Question Answering</title>
		<meeting>the Workshop on Task-Focused Summarization and Question Answering<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,309.58,407.49,9.96;9,105.50,321.53,252.75,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,270.54,309.58,100.94,9.96">Answerfinder at TREC</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,221.15,321.87,76.59,9.18">Proc. TREC 2005</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>TREC 2005</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2005">2005. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,341.46,407.50,9.96;9,105.50,353.42,373.13,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,339.57,341.46,112.15,9.96">Answerfinder at trec 2006</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Molla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luiz</forename><surname>Pizzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.94,353.76,103.58,9.18">Proceedings TREC 2006</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>TREC 2006</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,373.35,407.50,9.96;9,105.50,385.30,259.99,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,350.16,373.35,162.85,9.96;9,105.50,385.30,41.16,9.96">Named entity recognition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pizzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,168.11,385.64,105.27,9.18">Proceedings ALTW 2006</title>
		<meeting>ALTW 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,405.22,407.51,9.96;9,105.50,417.18,338.77,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,199.80,405.22,199.41,9.96">Overview of results of the MUC-6 evaluation</title>
		<author>
			<persName coords=""><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,423.94,405.56,89.07,9.18;9,105.50,417.51,147.49,9.18">Proc. Sixth Message Understanding Conference MUC-6</title>
		<meeting>Sixth Message Understanding Conference MUC-6</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,437.10,407.50,9.96;9,105.50,449.06,151.40,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,275.72,437.10,217.44,9.96">A named entity recogniser for question answering</title>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,105.50,449.40,120.77,9.18">Proceedings PACLING 2007</title>
		<meeting>PACLING 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,468.98,407.49,9.96;9,105.50,480.94,407.51,9.96;9,105.50,492.89,52.57,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,199.45,468.98,204.33,9.96">The TREC-8 question answering track report</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,246.62,481.28,60.11,9.18">Proc. TREC-8</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC-8</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
