<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.24,148.63,206.66,15.51">Overview of QAST 2007</title>
				<funder>
					<orgName type="full">Spanish Ministry of Science (TEXTMESS project)</orgName>
				</funder>
				<funder ref="#_4tNWG5X">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">LIMSI AI/ASP Ritel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,96.00,182.13,54.67,9.96"><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
							<email>turmo@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre (UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.28,182.13,52.04,9.96"><forename type="first">Pere</forename><surname>Comas</surname></persName>
							<email>pcomas@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre (UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.80,182.13,76.55,9.96"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
							<email>ayache@elda.org</email>
							<affiliation key="aff1">
								<orgName type="institution">ELDA/ELRA. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.80,182.13,68.82,9.96"><forename type="first">Djamel</forename><surname>Mostefa</surname></persName>
							<email>mostefa@elda.org</email>
							<affiliation key="aff1">
								<orgName type="institution">ELDA/ELRA. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.81,182.13,60.63,9.96"><forename type="first">Sophie</forename><surname>Rosset</surname></persName>
							<email>rosset@limsi.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">LIMSI. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,494.53,182.13,48.01,9.96"><forename type="first">Lori</forename><surname>Lamel</surname></persName>
							<email>lamel@limsi.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">LIMSI. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.24,148.63,206.66,15.51">Overview of QAST 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4BEE940BBDAF96EB1D5E14A69FE7B95B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Experimentation, Performance, Measurement Question Answering, Spontaneous Speech Transcripts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes QAST, a pilot track of CLEF 2007 aimed at evaluating the task of Question Answering in Speech Transcripts. The paper summarizes the evaluation framework, the systems that participated and the results achieved. These results have shown that question answering technology can be useful to deal with spontaneous speech transcripts, so for manually transcribed speech as for automatically recognized speech. The loss in accuracy from dealing with manual transcripts to dealing with automatic ones implies that there is room for future reseach in this area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Question Answering (QA) consists of providing short, relevant answers to natural language questions. Most Question Answering research has focused on extracting information from text sources, providing the shortest relevant text in response to a question <ref type="bibr" coords="1,463.07,619.89,10.45,9.96" target="#b3">[4,</ref><ref type="bibr" coords="1,478.56,619.89,6.97,9.96" target="#b4">5]</ref>. For example, the correct answer to the question How many groups participate in the CHIL project? is 16. Whereas the response to the question of who are the partners in CHIL? is a list of the partners. This simple example illustrates the two main advantages of QA has over current search engines: first, the input is a natural language question rather a keyword query, and second, the answer provides the desired information content and not a potentially large set of documents or URLs that the user must plow through.</p><p>Most of current QA systems handle independent questions and produce one answer to each question, extracted from textual data, for both open domain and limited domain tasks. However, a large portion of human interactions involve spontaneous speech, e.g. meetings, seminars, lectures, telephone conversations, and are beyond the capacities of current text-based factual QA systems. Most of the recent QA research has been undertaken by natural language groups who have typically applied techniques to written texts, and assume that these texts have a correct syntactic and semantic structure. The grammatical structure of spoken language is different from that of written language, and some of the anchor points used in text processing such as punctuation must be inferred and are therefore error prone. Other spoken language phenomena include disfluencies, repetitions, restarts and corrections. In the case that automatic processing is used to create the speech transcripts, an additional challenge is dealing with the recognition errors. The lecture and interactive meeting data are particularly difficult due to run-on sentences (where the distance between the first part of an utterance and its end one can be very long) and interruptions. Therefore current techniques for text-based QA need substantial adaptation in order to access the information contained in audio data.</p><p>This paper provides an overview of a pilot evaluation track at CLEF 2007 for Question Answering in Speech Transcriptions, named QAST. Section 2 describes the principles of this evaluation track. Sections 3 and 4 present the evaluation framework and the systems that participated, respectively. Section 5 shows the results achieved and the main implications. Finally, Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The QAST task</head><p>The objective of this pilot track is to provide a framework in which QA systems can be evaluated when the answers have to be found in spontaneous speech transcripts (manual and automatic transcripts). There are three main objectives to this evaluation:</p><p>• Comparing the performances of the systems dealing with both types of transcripts.</p><p>• Measuring the loss of each system due to the inaccuracies in state of the art ASR technology.</p><p>• Motivating and driving the design of novel and robust factual QA architectures for automatic speech transcripts.</p><p>In this evaluation, the QA systems have to return answers found in the audio transcripts to questions presented in a written natural language form. The answer is the minimal sequence of words that includes the correct exact answer in the audio stream. For the purposes of this evaluation, instead of pointers in the audio signal, the recognized words covering the location of the exact answer have to be returned. For example, consider the question which organisation has worked with the University of Karlsruhe on the meeting transcription system?, and the following extract of an automatically recognized document: breath fw and this is , joint work between University of Karlsruhe and coming around so fw all sessions , once you find fw like only stringent custom film canals communicates on on fw tongue initials .</p><p>corresponding to the following exact manual transcript: uhm this is joint work between the University of Karlsruhe and Carnegie Mellon, so also here in these files you find uh my colleagues and uh Tanja Schultz.</p><p>The answer found in the manual transcript is Carnegie Mellon whereas in the automatic transcript it is coming around. This example illustrates the two principles that guide this track:</p><p>• The questions are generated considering the exact information in the audio stream regardless of how this information is transcribed, because the transcription process is transparent to the user.</p><p>• The answer to be extracted is the minimal sequence of words that includes the correct exact answer in the audio stream (i.e., in the manual transcripts). In the above example, the answer to be extracted from the automatic transcript is coming around, because this text gives the start/end pointers to the correct answer in the audio stream.</p><p>Four tasks have been defined for QAST:</p><p>• T1: QA in manual transcriptions of lectures.</p><p>• T2: QA in automatic transcriptions of lectures.</p><p>• T3: QA in manual transcripts of meetings.</p><p>• T4: QA in automatic transcriptions of meetings.</p><p>3 Evaluation protocol</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collections</head><p>The data for the QAST pilot track consists of two different resources, one for dealing with the lecture scenario and the other for dealing with the meeting scenario:</p><p>• The CHIL corpus<ref type="foot" coords="3,190.44,355.66,3.97,4.84" target="#foot_0">1</ref> : it consists of around 25 hours (around 1 hour per lecture) both manually and automatically transcribed (LIMSI produced the ASR transcriptions with around 20% of word error rate -WER- <ref type="bibr" coords="3,217.45,380.37,10.00,9.96" target="#b1">[2]</ref>, while the manual ones were done by ELDA). In addition, the set of lattices and confidences for each lecture has been provided. The domain of the lectures is speech and language processing. The language is European English (mostly spoken by non native speakers). Lectures have been provided with simple tags. Seminars are formatted as plain text files (ISO-8859-1) <ref type="bibr" coords="3,240.01,428.25,10.00,9.96" target="#b2">[3]</ref>.</p><p>• The AMI corpus<ref type="foot" coords="3,188.64,446.98,3.97,4.84" target="#foot_1">2</ref> : it consists of around 100 hours (168 meetings) both manually and automatically transcribed (the Univeristy of Edimburgh produced the ASR trasncripts with around 38% of WER <ref type="bibr" coords="3,207.14,471.81,10.31,9.96" target="#b0">[1]</ref>). The domain of this meetings is design of television remote control.</p><p>The language is European English. Meetings (as lectures) have been produced with simple tags. Meetings are formatted as plain text files (ISO-8859-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Questions and answer types</head><p>For each one of the scenarios, two sets of questions will be provided to the participants:</p><formula xml:id="formula_0" coords="3,105.00,560.97,169.94,9.96">• Development set (1 February 2007) :</formula><p>-Lectures: 10 seminars and 50 questions.</p><p>-Meetings: 50 meetings and 50 questions.</p><p>• the Evaluation set (18 June 2007):</p><p>-Lectures: 15 seminars and 100 questions.</p><p>-Meetings: 118 meetings and 100 questions.</p><p>Question sets have been formatted as plain text files, with one question per line as defined in the Guidelines 3 . All the questions in the QAST task are factual questions, whose expected answer is a Named Entity (person, location, organization, language, system, method, measure, time, color, shape and material). No definition questions have been proposed. The two data collections (CHIL and AMI corpus) were first tagged with Named Entities. Then, an English native speaker created questions for each NE tagged session. So each answer is a tagged Named Entity.</p><p>An answer is basically structured as an [answer-string, document-id] pair, where the answerstring contains nothing more than a complete and exact answer (a Named Entity) and the document-id is the unique identifier of a document that supports the answer. There are no particular restrictions on the length of an answer-string (which is usually very short), but unnecessary pieces of information will be penalised, since the answer will be marked as non-exact. Assessors will focus mainly on the responsiveness and usefulness of the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human judgement</head><p>The files submitted by participants have been manually judged by native speaking assessors. Assessors considered correctness and exactness of the returned answers. They have also checked that the document labelled with the returned docid supports the given answer. One assessor evaluated the results. Then, another assessor manually checked each judgement evaluated by the first one. Any doubts about an answer was solved through various discussions.</p><p>To evaluate the data, assessors used an evaluation tool developed in Perl (at ELDA) named QASTLE <ref type="foot" coords="4,131.04,312.22,3.97,4.84" target="#foot_3">4</ref> . A simple interface permits easy access of the question, the answer and the document associated with the answer (all in one window only).</p><p>For T2 and T4 (QA on automatic transcripts) the manual transcriptions were aligned to the automatic ASR outputs to find the answer in the automatic transcripts. The alignments between the automatic and the manual transcription were done using time information for most of the seminars and meetings. Unfortunately for some AMI meetings time information were not available and only word alignments were used.</p><p>After each judgement the submission files have been modified. A new element appears in the first column: the answer's evaluation (or judgement). The four possible judgements (also used at TREC <ref type="bibr" coords="4,111.44,420.69,16.08,9.96" target="#b4">[5]</ref>) correspond to a number ranging between 0 and 3:</p><p>• 0 correct: the answer-string consists of the relevant information (exact answer), and the answer is supported by the returned document.</p><p>• 1 incorrect: the answer-string does not contain a correct answer or the answer is not responsive.</p><p>• 2 non-exact: the answer-string contains a correct answer and the docid supports it, but the string has bits of the answer missing or is longer than the required length of the answer.</p><p>• 3 unsupported: the answer-string contains a correct answer but the docid does not support it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measures</head><p>The two following metrics used in CLEF have been used in the QAST evaluation:</p><p>1. Mean Reciprocal Rank (MRR) measures how well ranked is the right answer, as defined in Section 2, in the list of 5 possible answers in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Accuracy:</head><p>The fraction of correct answers ranked in the first position in the list of 5 possible answers.</p><p>A total of five groups from five different countries submitted results for one or more of the proposed QAST tasks. Due to various reasons (technical, financial, etc.), three other registered groups were not be able to submit any results.</p><p>The five participating groups are the following:</p><p>• CLT, Center for Language Technology, Australia;</p><p>• DFKI, Germany;</p><p>• LIMSI, Laboratoire d'Informatique et de Mécanique des Sciences de l'Ingénieur, France;</p><p>• TOKYO, Tokyo Institute of Technology, Japan;</p><p>• UPC, Universitat Politècnica de Catalunya, Spain.</p><p>Five groups participated in both T1 and T2 tasks (CHIL corpus) and three groups participated in both T3 and T4 tasks (AMI corpus).</p><p>The participants could submit up to 2 submissions per task and up to 5 answers per question. The systems used in the submissions are described in Table <ref type="table" coords="5,365.51,324.45,3.90,9.96" target="#tab_0">1</ref>. In total, 28 submissions were evaluated: 8 submissions from 5 participating sites for T1, 9 submission files from 5 different sites for T2, 5 submissions from 3 participants for T3 and 6 submissions from 3 participants for T4. The lattices provided for task T2 were not finally used by any participant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results for the four QAST tasks are presented in tables 2, 3, 4 and 5. Due to some problems (typo, answer type) some questions have been deleted from the scoring results in tasks T1, T2 and T3. In total, the results have been calculated on the basis of 98 questions for tasks T1 and T2, and 96 for T3. In addition, and due to also missing time information at word level for some AMI meetings, seven questions have been deleted from the scoring results of T4. The results for this task have been calculated on the basis of 93 questions.    The results are very encouraging. First, the best result in accuracy achieved in tasks involving manual transcripts (0.51 for task T1) is closed to the best two results for factual questions in TREC 2006 (0.58 and 0.54), in which monolingual English QA was evaluated. Second, this behaviour is also observed in average: the accuracy in average achieved in tasks T1 and T3 is 0.22, which is comparable with 0.18 achieved in TREC 2006. Although no direct comparisons between QAST and TREC are possible due to the use of different data, questions and answer types, these facts show that QA technology can be useful to deal with spontaneous speech transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Finally, the accuracy values are 0.22 and 0.15 in average for the tasks involving lectures (T1 and T2, respectively), and 0.21 and 0.14 for those involving meetings (T3 and T4, respectively). These values show that the accuracy decreases in average more than 36% when dealing with automatic transcripts. The reduction of this difference between accuracy values have to be taken as a main goal in the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have described the QAST 2007 (Question Answering in Speech Transcripts) task. A set of five groups participated in this track with a total of 28 submitted runs among four specific tasks. In general, the results achieved show that, first, QA technology can be useful to deal with spontaneous speech transcripts, and second, the loss in accuracy when dealing with automatically transcribed speech is high. These results are very encouraging and suggest that there is room for future research in this area.</p><p>Future work aims at including in the evaluation framework other languages than English, oral questions, and other question types different than factual ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,96.00,383.78,423.40,280.15"><head>Table 1 :</head><label>1</label><figDesc>Systems that participated in QAST</figDesc><table coords="5,96.00,383.78,423.40,248.57"><row><cell>System</cell><cell>Enrichment</cell><cell>Question</cell><cell>Doc/Pass</cell><cell>Answer</cell><cell>NERC</cell></row><row><cell></cell><cell></cell><cell>classification</cell><cell>Retrieval</cell><cell>Extraction</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>pass. ranking</cell><cell>candidate ranking</cell><cell>hand-crafted patterns,</cell></row><row><cell>clt1</cell><cell>words</cell><cell>hand-crafted</cell><cell>based on word</cell><cell>based on frequency</cell><cell>gazeetters</cell></row><row><cell></cell><cell>and NEs</cell><cell>patterns</cell><cell>similarities between</cell><cell>and the NER</cell><cell>and ME models</cell></row><row><cell>clt2</cell><cell></cell><cell></cell><cell>pass. and query</cell><cell>confidence</cell><cell>No ME models</cell></row><row><cell></cell><cell>words</cell><cell>hand-crafted</cell><cell></cell><cell>candidate ranking</cell><cell>gazeeteers and</cell></row><row><cell>dfki1</cell><cell>and NEs</cell><cell>sint.-sem.</cell><cell>Lucene</cell><cell>based on frequency</cell><cell>not tuned statistical</cell></row><row><cell></cell><cell></cell><cell>rules</cell><cell></cell><cell></cell><cell>models</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pass. ranking based</cell><cell></cell><cell></cell></row><row><cell>limsi1</cell><cell></cell><cell></cell><cell>on hand-crafter</cell><cell>candidate ranking</cell><cell></cell></row><row><cell></cell><cell>words</cell><cell>hand-crafted</cell><cell>back-off queries</cell><cell>based on frequency,</cell><cell>hand-crafted</cell></row><row><cell></cell><cell>and NEs</cell><cell>patterns</cell><cell>cascaded doc/pass</cell><cell>keyword distance and</cell><cell>patterns</cell></row><row><cell>limsi2</cell><cell></cell><cell></cell><cell>ranking based on</cell><cell>retrieval confidence</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>search descriptors</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>pass. retrieval with</cell><cell></cell><cell></cell></row><row><cell>tokyo1</cell><cell></cell><cell cols="2">non-linguistic interpolated doc/pass</cell><cell>candidate ranking</cell><cell></cell></row><row><cell></cell><cell>words</cell><cell>statistical</cell><cell>statistical models</cell><cell>based on statistical</cell><cell>no</cell></row><row><cell></cell><cell></cell><cell>multi-word</cell><cell>addition of word</cell><cell>multi-word</cell><cell></cell></row><row><cell>tokyo2</cell><cell></cell><cell>model</cell><cell>classes to the</cell><cell>model</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>statistical models</cell><cell></cell><cell></cell></row><row><cell></cell><cell>words, NEs</cell><cell></cell><cell>pass. ranking based</cell><cell>candidate ranking</cell><cell></cell></row><row><cell>upc1</cell><cell>lemmas and</cell><cell></cell><cell>on iterative query</cell><cell>based on keyword</cell><cell>hand-crafted patterns,</cell></row><row><cell></cell><cell>POS</cell><cell>perceptrons</cell><cell>relaxation</cell><cell>distance and density</cell><cell>gazeetters</cell></row><row><cell>upc2</cell><cell>also</cell><cell></cell><cell cols="2">addition of approximated phonetic matching</cell><cell>and perceptrons</cell></row><row><cell></cell><cell>phonetics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,167.28,157.22,268.62,243.65"><head>Table 2 :</head><label>2</label><figDesc>Results for T1 (QA on CHIL manual transcriptions)</figDesc><table coords="6,176.52,157.22,249.86,243.65"><row><cell></cell><cell># Questions</cell><cell cols="3">#Correct answers MRR Accuracy</cell></row><row><cell>clt1 t1</cell><cell>98</cell><cell>16</cell><cell>0.09</cell><cell>0.06</cell></row><row><cell>clt2 t1</cell><cell>98</cell><cell>16</cell><cell>0.09</cell><cell>0.05</cell></row><row><cell>dfki1 t1</cell><cell>98</cell><cell>19</cell><cell>0.17</cell><cell>0.15</cell></row><row><cell>limsi1 t1</cell><cell>98</cell><cell>43</cell><cell>0.37</cell><cell>0.32</cell></row><row><cell>limsi2 t1</cell><cell>98</cell><cell>56</cell><cell>0.46</cell><cell>0.39</cell></row><row><cell>tokyo1 t1</cell><cell>98</cell><cell>32</cell><cell>0.19</cell><cell>0.14</cell></row><row><cell>tokyo2 t1</cell><cell>98</cell><cell>34</cell><cell>0.20</cell><cell>0.14</cell></row><row><cell>upc1 t1</cell><cell>98</cell><cell>54</cell><cell>0.53</cell><cell>0.51</cell></row><row><cell>System</cell><cell>#Questions</cell><cell cols="3">#Correct answers MRR Accuracy</cell></row><row><cell>clt1 t2</cell><cell>98</cell><cell>13</cell><cell>0.06</cell><cell>0.03</cell></row><row><cell>clt2 t2</cell><cell>98</cell><cell>12</cell><cell>0.05</cell><cell>0.02</cell></row><row><cell>dfki1 t2</cell><cell>98</cell><cell>9</cell><cell>0.09</cell><cell>0.09</cell></row><row><cell>limsi1 t2</cell><cell>98</cell><cell>28</cell><cell>0.23</cell><cell>0.20</cell></row><row><cell>limsi2 t2</cell><cell>98</cell><cell>28</cell><cell>0.24</cell><cell>0.21</cell></row><row><cell>tokyo1 t2</cell><cell>98</cell><cell>17</cell><cell>0.12</cell><cell>0.08</cell></row><row><cell>tokyo2 t2</cell><cell>98</cell><cell>18</cell><cell>0.12</cell><cell>0.08</cell></row><row><cell>upc1 t2</cell><cell>96</cell><cell>37</cell><cell>0.37</cell><cell>0.36</cell></row><row><cell>upc2 t2</cell><cell>97</cell><cell>29</cell><cell>0.25</cell><cell>0.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,161.28,423.09,280.59,93.22"><head>Table 3 :</head><label>3</label><figDesc>Results for T2 (QA on CHIL automatic transcriptions)</figDesc><table coords="6,169.92,456.98,263.07,59.33"><row><cell>System</cell><cell>#Questions</cell><cell>#Correct answers</cell><cell>MRR</cell><cell>Accuracy</cell></row><row><cell>clt1 t3</cell><cell>96</cell><cell>31</cell><cell>0.23</cell><cell>0.16</cell></row><row><cell>clt2 t3</cell><cell>96</cell><cell>29</cell><cell>0.25</cell><cell>0.20</cell></row><row><cell>limsi1 t3</cell><cell>96</cell><cell>31</cell><cell>0.28</cell><cell>0.25</cell></row><row><cell>limsi2 t3</cell><cell>96</cell><cell>40</cell><cell>0.31</cell><cell>0.25</cell></row><row><cell>upc1 t3*</cell><cell>95</cell><cell>23(27)</cell><cell>0.22(0.26)</cell><cell>0.20(0.25)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,538.41,422.99,124.54"><head>Table 4 :</head><label>4</label><figDesc>Results for T3 (QA on AMI manual transcriptions). *Due to a bug with the output format script, UPC asked to the assessors to reevaluate their unique run for T3. The results in brackets must be regarded as a non official run.</figDesc><table coords="6,179.40,593.78,244.10,69.17"><row><cell>System</cell><cell>#Questions</cell><cell>#Correct answers</cell><cell cols="2">MRR Accuracy</cell></row><row><cell>clt1 t4</cell><cell>93</cell><cell>17</cell><cell>0.10</cell><cell>0.06</cell></row><row><cell>clt2 t4</cell><cell>93</cell><cell>19</cell><cell>0.13</cell><cell>0.08</cell></row><row><cell>limsi1 t4</cell><cell>93</cell><cell>21</cell><cell>0.19</cell><cell>0.18</cell></row><row><cell>limsi2 t4</cell><cell>93</cell><cell>21</cell><cell>0.19</cell><cell>0.17</cell></row><row><cell>upc1 t4</cell><cell>91</cell><cell>22</cell><cell>0.22</cell><cell>0.21</cell></row><row><cell>upc2 t4</cell><cell>92</cell><cell>17</cell><cell>0.15</cell><cell>0.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,169.44,685.17,264.30,9.96"><head>Table 5 :</head><label>5</label><figDesc>Results for T4 (QA on AMI manual transcriptions)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,727.46,74.29,7.97"><p>http://chil.server.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,736.94,101.03,7.97"><p>http://www.amiproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,746.42,110.16,7.97"><p>http://www.lsi.upc.edu/˜qast</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,105.24,679.34,106.07,7.97"><p>http://www.elda.org/qastle/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are very grateful to <rs type="person">Thomas Hain</rs> from the <rs type="affiliation">University of Edimburgh</rs>, who provide us with the AMI transcripts automatically generated by their ASR. This work has been jointly funded by the <rs type="funder">European Commission</rs> (<rs type="projectName">CHIL</rs> project <rs type="grantNumber">IP-506909</rs>), the <rs type="funder">Spanish Ministry of Science (TEXTMESS project)</rs> and the <rs type="funder">LIMSI AI/ASP Ritel</rs> grant.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_4tNWG5X">
					<idno type="grant-number">IP-506909</idno>
					<orgName type="project" subtype="full">CHIL</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.48,478.89,407.53,9.96;7,105.48,490.89,356.89,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,495.87,478.89,17.15,9.96;7,105.48,490.89,192.01,9.96">The ami system for the transcription of meetings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vepa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,318.84,490.89,111.91,9.96">Proceedings of ICASSP&apos;07</title>
		<meeting>ICASSP&apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.49,510.81,407.54,9.96;7,105.48,522.69,155.17,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,340.74,510.81,152.27,9.96">Transcribing lectures and seminars</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.48,522.69,124.51,9.96">Proceedings of Interspeech&apos;05</title>
		<meeting>Interspeech&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.49,542.61,407.64,9.96;7,105.48,554.61,407.50,9.96;7,105.48,566.61,407.61,9.96;7,105.48,578.49,367.57,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,254.06,566.61,259.03,9.96;7,105.48,578.49,81.04,9.96">The chil audiovisual corpus for lecture and meeting analysis inside smart rooms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tobia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pnvmatikakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mylonakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Talantzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rochet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,251.51,578.49,190.89,9.96">Language Resources and Evaluation Journal</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.49,598.41,407.56,9.96;7,105.48,610.41,407.40,9.96;7,105.48,622.41,99.39,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,221.16,610.41,287.59,9.96">Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<editor>, J. Karlgren, B. M</editor>
		<editor>agnini, D.W. Oard, M. de Rijke, and M. Stempfhuber</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.48,642.33,407.49,9.96;7,105.48,654.21,110.29,9.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="7,297.84,642.33,215.14,9.96;7,105.48,654.21,79.27,9.96">The Fifteenth Text Retrieval Conference Proceedings (TREC 2006)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
