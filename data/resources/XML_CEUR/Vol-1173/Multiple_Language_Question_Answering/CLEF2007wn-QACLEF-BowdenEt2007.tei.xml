<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.83,113.78,295.64,15.06;1,142.27,131.72,330.78,15.06;1,239.62,149.65,136.09,15.06">Multilingual Question Answering through Intermediate Translation: LCC&apos;s PowerAnswer at QA@CLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.82,188.09,70.65,10.46"><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.11,188.09,67.32,10.46"><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.41,188.09,89.89,10.46"><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.10,188.09,66.11,10.46;1,275.79,200.04,18.13,10.46"><forename type="first">Dan</forename><surname>Thomas D'silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.23,200.04,42.35,10.46"><surname>Moldovan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.83,113.78,295.64,15.06;1,142.27,131.72,330.78,15.06;1,239.62,149.65,136.09,15.06">Multilingual Question Answering through Intermediate Translation: LCC&apos;s PowerAnswer at QA@CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">504EE127937516D34A37F8466D5A3216</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on Language Computer Corporation's QA@CLEF 2007 preparation, participation and results. For this exercise, LCC integrated its open-domain PowerAnswer Question Answering system with its statistical Machine Translation engine. For 2007, LCC participated in the English-to-French and English-to-Portuguese crosslanguage tasks. The approach is that of intermediate translation, only processing English within the QA system regardless of the input or source languages. The output snippets were then mapped back into the source language documents for the final output of the system and submission. What follows is a description of the improved system and methodology and updates from QA@CLEF 2006.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2006, Language Computer Corporation's open-domain question answering system PowerAnswer <ref type="bibr" coords="1,230.53,475.16,10.52,10.46" target="#b5">[6]</ref> participated in QA@CLEF for the first time <ref type="bibr" coords="1,443.68,475.16,9.96,10.46" target="#b0">[1]</ref>, 2007 is a continuation of this exercise. PowerAnswer has previously participated in many other evaluations, notably NIST's TREC <ref type="bibr" coords="1,351.64,499.07,10.52,10.46" target="#b6">[7]</ref> workshop series, however, QA@CLEF is the first Multilingual QA evaluation the system has entered. Additionally, LCC has developed its own statistical machine translation system, which is integrated with PowerAnswer for this evaluation. Since PowerAnswer is a very modular and extensible system, the integration required only a minimum of modifications for the approach chosen.</p><p>The goals for participating in QA@CLEF are <ref type="bibr" coords="1,345.04,570.87,12.73,10.46" target="#b0">(1)</ref> to examine how well the QA system performs when given noisy data, such as that from automatic translation and (2) to examine and evaluate the performance and utility of the machine translation system in a question answering environment. To that end, LCC has adopted an approach of intermediate translation instead of adapting the QA system to process target languages natively.</p><p>The paper presents a summary of the PowerAnswer system, the machine translation engine, the integration of the two for QA@CLEF 2007, and then follows with a discussion of results and challenges in the CLEF question topics. For 2007, LCC participated in the following bilingual tasks: English → French, and English → Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of LCC's PowerAnswer</head><p>Automatic question answering requires a system that has a wide range of tools available. There is no one monolithic solution for all question types or even data sources. In realization of this, LCC developed PowerAnswer as a fully-modular and distributed multi-strategy question answering system that integrates semantic relations, advanced inferencing abilities, syntactically constrained lexical chains, and temporal contexts. This section presents an outline of the system and how it was modified to meet the challenges of QA@CLEF 2007.</p><p>PowerAnswer comprises a set of strategies that are selected based on advanced question processing, and each strategy is developed to solve a specific class of questions either independently or together. A Strategy Selection module automatically analyzes the question and chooses a set of strategies with the algorithms and tools that are tailored to the class of the given question. PowerAnswer can distribute the strategies across workers in the case of multiple strategies being selected, alleviating the increase in the complexity of the question answering process by splitting the workload across machines and processors.  Each strategy is a collection of components, (1) Question Processing (QP), (2) Passage Retrieval (PR), and (3) Answer Processing (AP). Each of these components constitute one or more modules, which interface to a library of generic NLP tools. These NLP tools are the building blocks of the PowerAnswer 2 system that, through a well-defined set of interfaces, allow for rapid integration and testing of new tools and third-party software such as IR systems, syntactic parsers, named entity recognizers, logic provers, semantic parsers, ontologies, word sense disambiguation modules, and more. Furthermore, the components that make up each strategy can be interchanged to quickly create new strategies, if needed, they can also be distributed <ref type="bibr" coords="3,305.10,129.47,14.61,10.46" target="#b11">[12]</ref>.</p><p>As illustrated in Figure <ref type="figure" coords="3,259.45,141.88,3.87,10.46" target="#fig_0">1</ref>, the role of the QP module is to determine (1) temporal constraints, ( <ref type="formula" coords="3,237.24,153.84,4.24,10.46">2</ref>) the expected answer type, (3) to process any question semantics necessary such as roles and relations, (4) to select the keywords used in retrieving relevant passages, and (5) perform any preliminary questions as necessary for resolving question ambiguity. The PR module ranks passages that are retrieved by the IR system, while the AP module extracts and scores the candidate answers based on a number of syntactic and semantic features such as keyword density, count, proximity, semantic ordering, roles and entity type. All modules have access to a syntactic parser, semantic parser, a named entity recognizer and a reference resolution system through LCC's generic NLP tool libraries. To improve the answer selection, PowerAnswer takes advantage of redundancy in large corpora, specifically in this case, the Internet. As the size of a document collection grows, a question answering system is more likely to pinpoint a candidate answer that closely resembles the surface structure of the question. These features have the role of correcting the errors in answer processing that are produced by the selection of keywords, by syntactic and semantic processing and by the absence of pragmatic information. Usually, the final decision for selecting answers is based on logical proofs from the inference engine COGEX <ref type="bibr" coords="3,175.74,357.08,9.96,10.46" target="#b8">[9]</ref>. For QA@CLEF, however, the logic prover is disabled in order to better evaluate the individual components of the QA architecture. COGEX's evaluation on multilingual data was performed in the 2006 CLEF Answer Validation Exercise <ref type="bibr" coords="3,208.30,392.94,14.61,10.46" target="#b14">[15]</ref>, where the system was the top performer in both Spanish and English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Translation Engine</head><p>The translation system used at LCC -MeTRe -implements phrase-based statistical machine translation <ref type="bibr" coords="3,259.62,477.30,9.96,10.46" target="#b2">[3]</ref>; the core translation engine is the open-source Phramer <ref type="bibr" coords="3,177.01,489.25,15.50,10.46" target="#b13">[14]</ref> system, developed by one of LCC's engineers. Phramer in turn implements and extends the phrase-based machine translation algorithms described by Koehn <ref type="bibr" coords="3,215.63,513.17,9.96,10.46" target="#b2">[3]</ref>. A more detailed description of the MT solution adopted for Multilingual QA@CLEF can be found in <ref type="bibr" coords="3,342.09,525.12,14.61,10.46" target="#b12">[13]</ref>. The translation system is trained using the European Parliament Proceedings Parallel Corpus 1996-2003 (EUROPARL) <ref type="bibr" coords="3,203.01,549.03,9.96,10.46" target="#b3">[4]</ref>, which provides between 600,000 and 800,000 pairs of sentences (sentences in English paired with the translation in another European language). LCC followed the training procedure described in the Pharaoh <ref type="bibr" coords="3,470.08,572.94,10.52,10.46" target="#b4">[5]</ref> training manual<ref type="foot" coords="3,204.26,583.82,3.97,7.32" target="#foot_0">1</ref> to generate the phrase table required for translation.</p><p>In order to translate entire documents, the core translation engine is augmented with (1) tokenization, (2) capitalization, and (3) de-tokenization.</p><p>The tokenization process is performed on the original documents (in French or Portuguese), in order to convert the sentences to space-separated entities, in which the punctuation and the words are isolated. The step is required because the statistical machine translation core engine accepts only lowercased tokenized input.</p><p>The capitalization process follows the translation process and it restores the casing of the words, due to using models trained on lowercase text. The capitalization tool uses three-gram statistics extracted from 150 million words from the English GigaWord Second Edition<ref type="foot" coords="4,301.23,184.43,3.97,7.32" target="#foot_1">2</ref> corpus, augmented with two heuristics:</p><p>1. First word will always be uppercased; 2. If the words appear also in the foreign documents, the casing is preserved (this rule is very effective for proper nouns and named entities)</p><p>4 PowerAnswer-MeTRe Integration LCC's cross-language solution for Question Answering is based on automatic translation of the documents in the source language (English). QA is performed on a collection consisting only of English documents. The answers were converted back into the target language (the original language of the documents) by aligning the translation with the original document (finding the original phrase in the original document that generated the answer in English); when this method failed, the system falls back to machine translation (source → target). While this fallback method provides excellent usability in a real-world situation, as discussed in the Errors discussion, the method produces answers judged inexact in an evaluation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Passage Retrieval</head><p>Making use of PowerAnswer's modular design, for last year's QA@CLEF, LCC developed three different retrieval methods, settling on the first of these for the final experiment.</p><p>1. use an index of English words, created from the translated documents 2. use an index of foreign words (French, Spanish or Portuguese), created from the original documents 3. use an index of English words, created from the original documents in correlation with the translation table</p><p>The first solution is the default solution, and for 2007, the only method used. LCC selected this as the sole method this year because it gave the best performance in terms of quality versus runtime effort. Moreover, LCC has improved the speed of the automatic translator since the 2006 QA@CLEF. In addition to an algorithmic speed improvement of over 100% per execution core, and a decrease in the impact of network latency, the translator also now takes advantage of multiple processors, greatly increasing the time performance of the system. On dual-core machines, the translation speedup is more than 300%.</p><p>The entire target language document collection is translated into English, processed through the set of LCC's NLP tools and indexed for querying. Its major disadvantage is the computational effort required to translate the entire collection. It also requires updating the English version of the collection when one improves the quality of the translation. For 2007, we created all new indexes of the collection. Its major advantage is that there are no additional costs during question answering (the documents are already translated). This passage retrieval method is illustrated in Figure <ref type="figure" coords="5,311.10,201.20,3.87,10.46" target="#fig_1">2</ref>. As a main source of errors last year, for 2007 LCC made improvements to the Answer Aligner as described in Section 5. The second solution, as seen in Figure <ref type="figure" coords="5,333.98,225.11,3.87,10.46" target="#fig_2">3</ref> indexing (the document collection is indexed in its native language). In order to retrieve the relevant documents, the system translates the keywords of the IR query (the query submitted by PowerAnswer to the Lucene-based<ref type="foot" coords="5,441.64,428.32,3.97,7.32" target="#foot_2">3</ref> IR system) with alternations as the new IR query (step 1). The translation of keywords is performed using MeTRe, by generating n-best translations. This translated query is submitted to the target language index (step 2). The documents retrieved by this query are then dynamically translated into English using MeTRe (step 3). The system uses a cache to store translated documents so that IR query reformulations and other questions that might retrieve the same documents will not need to be translated again. The set of translated documents is indexed into a mini-collection (step 4) and the mini-collection is re-queried using the original English-based IR query (step 5). For example, the boolean IR query in English ("poem" AND "love" AND "1922") is translated into French as ("poeme" AND ("aiment" OR "aimer" OR "aimez" OR "amour") AND "1922") with the alternations. This new query will return 85 French documents. Some of them do not contain "love" in their automatic translation (but the original document contains "aiment", "aimer", "aimez" or "amour"). Thus, by re-querying the translated sub-collection (that contains only the translation of those 85 documents) the system retrieves only 72 English documents that will be passed to PowerAnswer.  The advantage of the second method is that minimum effort is required during collection preparation. Also, the collection preparation might not be under the control of the QA system (i.e. it can be web-based). Also, improvements in the MT engine can be reflected immediately in the output of the integrated system. The disadvantage is that more computation is required at run-time for translating the IR query and the documents dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PowerAnswer</head><p>The third alternative extracts during indexing the English words that might be part of the translation and indexes the collection accordingly. The process doesn't involve lexical choice -all choices are considered possible. The set of keywords is determined using the translation table, and collects all words that are part of the translation lattice ( <ref type="bibr" coords="6,283.44,395.89,10.52,10.46" target="#b4">[5]</ref>). Determining only the words according to the translation table (semi-translation) is approximately 10 times faster than the full translation. The index is queried using the original IR query generated by PowerAnswer (with English keywords). After the initial retrieval, the algorithm is similar to the second method: translate the retrieved documents, re-query the mini-collection. The advantage is the much smaller indexing time when compared with the first method, besides all the advantages of the second method. Also, it has all the disadvantages of the second method, except that it doesn't require IR query translation.</p><p>Because preliminary testing proved that there aren't significant differences in recall between the three methods and because the first method is fastest after the document collection is prepared, only the first method was used for the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Processing</head><p>For each of the above methods, PowerAnswer returns the exact answer and the supporting source sentence (all in English). These answers are aligned to the corresponding text in the target language documents. The final output of the system is the response list in the target language with the appropriate supporting snippets. If the alignment method fails, the English answers are converted directly into the target language as the final </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Updates from QA@CLEF 2006</head><p>As 2006 was LCC's first year participating in CLEF, there were some substantial errors that were corrected for 2007 as well as some other improvements to various components of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PowerAnswer improvements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer type detection</head><p>We extended PowerAnswer's answer type detection module by moving it to a hybrid system which takes advantage of precise heuristics as well as machine learning algorithms for ambiguous questions. A maximum entropy model was trained to detect both answer type terms and answer types. The learner's features for answer type terms include part-of-speech, lemma, head information, parse path to WH-word, and named entity information. Answer type detection uses a variety of attributes such as additional answer type term features and set-to-set lexical chains derived from eXtended WordNet<ref type="foot" coords="7,380.72,387.86,3.97,7.32" target="#foot_3">4</ref> which links the set of question keywords to the set of potential answer type nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal processing</head><p>Dates for documents and the temporal context of the answer are maintained through question answering and after initial ranking, answers are given a boosting factor on top of their current relevance score that is intended to give greater priority to strong answers that are more recent than other strong answers. Answers that appear further down the response list and have lower relevance scores will not be affected by this boosting.</p><p>Because temporal answers can have a range of granularity, when pre-processing the data collection, the named entities stored in the IR index are extracted in a greedy fashion, so both "March 14, 1592" and "2000" will be tagged as date to give PowerAnswer the best flexibility for entity selection. During answer processing, if the question is seeking just a month, or a year, then the excess information from the date entity selected is removed after a more fine-grained NE recognition is performed on the answer nugget. EN → FR Q27 In what year was Richard Nixon born? demonstrates the utility of this method, where the answer is given in the text ... naît le 9 janvier 1913 .... Otherwise, if a simple "When was ..." question is asked, the entity with the most detailed temporal information would be the final answer. This method operated on 4 EN → FR and 3 EN → PT questions seeking year, or day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Machine translation improvements</head><p>Since last year (QA@CLEF 2006 evaluation), we improved the Answer Aligner module: (1) we fixed bugs that altered the order of the answer in the output and (2) we improved the alignment heuristics.</p><p>In terms of Machine Translation quality, we added modules in MeTRe designed to better preserve the structure of the sentence. The add-ons were focused on rules that can be easily derived from punctuation: numeric values, currency amounts, insertions through quotation marks and through brackets, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Wikipedia document conversion</head><p>PowerHarvest is a tool developed by Language Computer Corp. that is used for document harvesting and preprocessing for Question Answering. One of the features of PowerHarvest is to convert XML database dumps<ref type="foot" coords="8,396.02,304.27,3.97,7.32" target="#foot_4">5</ref> into a format that is used by PowerAnswer's document collection indexing module.</p><p>Prior to QA@CLEF 07, PowerHarvest was limited to the English version of the Wikipedia collection -it only knew how to interpret English Wikipedia markup (e.g.: Talk, User, User talk, Template, Category, ...). We extended Pow-erHarvest to work also on the targeted languages -French and Portugueseby introducing support for French markup (e.g.: Discuter, Utilisateur, Discussion Utilisateur, Modèle, Catégorie, ...) and Portuguese markup (e.g.: Discussão, Usuário, Usuário Discussão, Predefinição, Categoria, ...).</p><p>The documents resulting from PowerHarvest (in French and in Portuguese) were translated using MeTRe and indexed, using the same procedures that were used for the Newswire parts of the collection (Le Monde and French SDA for French; Público and Folha de São Paulo for Portugese -according to the Guidelines for Participants in QA@CLEF 2007 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The integrated multilingual PowerAnswer system was tested on 200 English → French and 200 English → Portuguese factoid, list and definition questions. For QA@CLEF, the main score is the overall accuracy, the average of SCORE(q), where SCORE(q) is defined for factoids and definition questions as 1 if the top answer for q is assessed as correct, 0 otherwise. Also included is the Confidence Weighted Score (CWS) that judges how well a system confidently returns correct answers.</p><p>Table <ref type="table" coords="8,178.51,618.49,4.98,10.46" target="#tab_2">1</ref> illustrates the final results of Language Computer's efforts in its participation at QA@CLEF for 2007.</p><p>While LCC saw a substantial improvement in errors over last year's results, there remain challenges that offer interesting research and engineering opportunities. The major sources of errors include: translation misalignments, tokenization errors, and data processing errors -questions and passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Translation misalignments</head><p>Because the version of PowerAnswer used is monolingual, the system design for multilingual question answering involves translating documents dynamically for processing through the QA system and later mapping the responses back into the source language documents. This results in several opportunities for error. While the translation of the documents into English did introduce noise into the data such as mistranslations, words that were not translated and should have been or words that should not have been translated and were, aggressive keyword expansion techniques diminish the impact of these mistranslations. Errors from misalignments still occured due to For the French source results, PowerAnswer returned 14 inexact answers, and for Portuguese source 7 inexact, 7% and 3.5% of the total response. Many of these inexact responses are definition-style questions that either (1) did not have enough information, such as EN → FR Q158: Who is Amira Casar?, actrice née le 1erjuillet 1971 à Londres, d'une mère russe chanteuse d'opéra et d'un père d'origine kurde. or (2) the alignment module was unable to correctly align the English answer within the given source language document, and so fell back to translating the English answer. While this particular default behavior is positive for the user since the answer is readable and still correct in nature, the language is not exact from the document and so warrants an inexact judgment in the evaluation. This failure is caused by translation errors when trying to map back from noisy text to the original source.</p><p>An example of this is EN → FR Q154: Who is Allan Frederick Jacobsen?. The source document is the Wikipedia "Allan Jacobsen" entry. While the final answer is readable and comprehensible, it is not the answer as it appears in the source document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Returning NIL as answer</head><p>The version of PowerAnswer used for QA@CLEF uses parameters that relax some of the semantic and syntactic restrictions on answers that PowerAnswer uses when running on more stable and less noisy data. A result of this is that zero NIL answers were returned because the system always attempts to return an answer. An example of this is EN → PT Q13: When did the blue whale become extinct?, the answer to which is NIL because the blue whale has never become extinct. PowerAnswer selected the translated answer When the hunting of whale blue has finally been banned in the 1960s, 350000 whales Blue had been killed.</p><p>with the exact answer the 1960s, but with a low relative confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Other error sources</head><p>Other error sources are less specific to the methodology of intermediate translation and more general question answering errors such as answer type detection, keyword selection and expansion, passage retrieval and answer selection/ranking. An example of an answer selection error is EN → PT Q24 What department is Caen the capital of ?. The correct answer string is Caen é uma comuna francesa na região administrativa da Baixa-Normandia, no departamento Calvados but PowerAnswer selected " Baixa-Normandia" as the correct answer instead of Calvados due to proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">English accuracy</head><p>As we also included for last year's results <ref type="bibr" coords="10,322.07,399.59,9.96,10.46" target="#b0">[1]</ref>, Table <ref type="table" coords="10,367.05,399.59,4.98,10.46" target="#tab_4">2</ref> compares the PowerAnswer English accuracy versus the mapped submission accuracy. This table also demonstrates that the system did obtain the expected improvements after the correction of misalignment errors present in the submission for QA@CLEF 2006. QA@CLEF 2007 proved to be a valuable learning exercise. We have been able to correct some of the errors that were present in last year's results and achieve the kind of performance we expected from PowerAnswer. Intermediate translation for question answering provides the opportunity for additional errors in processing, but we believe that our results in this evaluation show that such a methodology can be practical and accurate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,234.60,535.92,146.13,9.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. PowerAnswer 2 Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,191.92,368.71,231.42,9.41"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Passage Retrieval on English documents (default)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,191.93,241.26,231.41,9.41"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Passage Retrieval on Target Language documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,199.66,116.47,212.93,45.03"><head>Table 1 .</head><label>1</label><figDesc>LCC's QA@CLEF 2007 Results</figDesc><table coords="7,199.66,116.47,212.93,34.12"><row><cell>Source</cell><cell cols="2">Accuracy CWS Improv. from 2006</cell></row><row><cell>French</cell><cell>41.75% 0.22234</cell><cell>98.34%</cell></row><row><cell cols="2">Portuguese 29.32% 0.10484</cell><cell>244.54%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.51,511.18,346.10,130.00"><head></head><label></label><figDesc>The source language answer is Allan Frederick Jacobsen, né le 22 septembre 1978 à Edimbourg ( Écosse) est un joueur de rugby à XV qui joue avec l'équipe d' Écosse depuis 2002, évoluant au poste de pilier (1,78m et 109kg). The answer returned by PowerAnswer over the English translated Wikipedia article is born on 22 September 1978 to Edinburgh (Scotland -is a player rugby to XV is playing with the team of Scotland since 2002 swimming as pillar (1.78 me and 109 kg). The final submitted result, which was translated as the default was 22 nés sur édimbourg à 1978 septembre un joueur -est (scotland est rugby xv à jouez avec écosse l ' équipe depuis 2002 de baigner (1.78 comme pilier 109 kg) moi et.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,477.33,282.33,107.85"><head>Table 2 .</head><label>2</label><figDesc>LCC's Factoid/Definition Results in English</figDesc><table coords="10,134.77,477.33,281.86,107.85"><row><cell>Source</cell><cell cols="2">Submission Acc. Eng. Position 1 Acc.</cell></row><row><cell>French</cell><cell>41.75%</cell><cell>52.06%</cell></row><row><cell>Portuguese</cell><cell>29.32%</cell><cell>39.23%</cell></row><row><cell>8 Conclusions</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,655.49,203.75,9.41"><p>http://www.iccs.inf.ed.ac.uk/˜pkoehn/training.tgz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,655.49,318.01,9.41"><p>http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,655.49,103.92,9.41"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,144.73,655.49,127.04,9.41"><p>http://xwn.hlt.utdallas.edu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,144.73,655.49,125.66,9.41"><p>http://download.wikimedia.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.95,141.27,337.50,9.41;11,151.52,152.24,329.03,9.41;11,151.52,163.19,129.00,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,197.78,152.24,163.29,9.41">LCC&apos;s PowerAnswer at QA@CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,381.59,152.24,94.91,9.41">CLEF 2006 Proceedings</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>LNCS in press</note>
</biblStruct>

<biblStruct coords="11,142.95,174.15,337.51,9.41;11,151.52,185.11,328.93,9.41;11,151.52,196.07,133.20,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,215.41,185.11,245.91,9.41">Employing Two Question Answering Systems in TREC-2005</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,196.07,104.88,9.41">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,207.03,337.51,9.41;11,151.52,217.98,275.53,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,355.36,207.03,125.11,9.41;11,151.52,217.98,21.93,9.41">Statistical Phrase-Based Translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franz</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,181.20,217.98,137.90,9.41">Proceedings of HLT/NAACL 2003</title>
		<meeting>HLT/NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,228.95,337.51,9.41;11,151.52,239.90,49.16,9.41;11,228.45,239.90,20.98,9.41" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,218.36,228.95,258.06,9.41">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>MT Summit</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,250.86,337.50,9.41;11,151.52,261.83,253.12,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,217.14,250.86,263.31,9.41;11,151.52,261.83,96.59,9.41">Pharaoh: A beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.11,261.83,108.20,9.41">Proceedings of AMTA 2004</title>
		<meeting>AMTA 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,272.78,337.51,9.41;11,151.52,283.74,329.07,9.41;11,151.52,294.70,20.98,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,440.90,272.78,39.56,9.41;11,151.52,283.74,202.16,9.41">PowerAnswer 2: Experiments and Analysis over TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,372.31,283.74,104.01,9.41">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,305.66,337.51,9.41;11,151.52,316.61,230.79,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,340.29,305.66,140.17,9.41;11,151.52,316.61,77.76,9.41">A Temporally-Enhanced PowerAnswer in TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marta</forename><surname>Tatu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,249.11,316.61,104.89,9.41">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,327.58,337.50,9.41;11,151.52,338.54,317.33,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,377.11,327.58,103.34,9.41;11,151.52,338.54,96.94,9.41">Temporal Context Representation and Reasoning</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.48,338.54,83.25,9.41">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,349.49,337.50,9.41;11,151.52,360.45,329.05,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,446.23,349.49,34.22,9.41;11,151.52,360.45,154.70,9.41">COGEX A Logic Prover for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,324.90,360.45,126.33,9.41">Proceedings of the HLT/NAACL</title>
		<meeting>the HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,371.41,337.84,9.41;11,151.52,382.37,221.07,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,306.84,371.41,154.97,9.41">Lexical chains for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,382.37,93.83,9.41">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,393.33,337.84,9.41;11,151.52,404.29,303.44,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,284.78,393.33,195.68,9.41;11,151.52,404.29,145.36,9.41">Logic Form Transformation of WordNet and its Applicability to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,317.00,404.29,76.13,9.41">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,415.25,337.84,9.41;11,151.52,426.20,329.05,9.41;11,151.52,437.17,20.98,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,175.85,426.20,160.42,9.41">Synergist: Tools for Intelligence Analysis</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Munirathnam</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Altaf</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,343.62,426.20,71.03,9.41">NIMD Conference</title>
		<meeting><address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,448.13,337.84,9.41;11,151.52,459.08,329.01,9.41;11,151.52,470.05,109.78,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,392.28,448.13,88.17,9.41;11,151.52,459.08,143.58,9.41">Language Models and Reranking for Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,319.87,459.08,160.66,9.41;11,151.52,470.05,81.70,9.41">NAACL 2006 Workshop On Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,481.00,337.84,9.41;11,151.52,491.96,329.01,9.41;11,151.52,502.91,152.36,9.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,422.94,481.00,57.51,9.41;11,151.52,491.96,195.87,9.41">Phramer -An Open Source Statistical Phrase-Based Translator</title>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ionut</forename><surname>Volosen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,367.11,491.96,113.42,9.41;11,151.52,502.91,124.28,9.41">NAACL 2006 Workshop On Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,513.88,337.84,9.41;11,151.52,524.84,241.86,9.41" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,333.71,513.88,146.75,9.41;11,151.52,524.84,30.65,9.41">Automatic Answer Validation using COGEX</title>
		<author>
			<persName coords=""><forename type="first">Marta</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Iles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.40,524.84,171.96,9.41">Cross-Language Evaluation Forum (CLEF)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
