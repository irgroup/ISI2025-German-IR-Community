<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.52,74.45,296.28,12.58">Overview of the Answer Validation Exercise 2007</title>
				<funder>
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund</orgName>
				</funder>
				<funder ref="#_yhKbNJu">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.88,102.00,60.13,9.02"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.85,102.00,61.11,9.02"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.83,102.00,58.68,9.02"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
							<email>felisa@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.60,126.12,66.14,9.02"><forename type="first">Dpto</forename><surname>Lenguajes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.52,74.45,296.28,12.58">Overview of the Answer Validation Exercise 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B53ABF0A67330289D552AEF4ECB51EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Evaluation</term>
					<term>Textual Entailment</term>
					<term>Answer Validation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Answer Validation Exercise at the Cross Language Evaluation Forum is aimed at developing systems able to decide whether the answer of a Question Answering system is correct or not. We present here the exercise description, the changes in the evaluation methodology with respect to the first edition, and the results of this second edition (AVE 2007). The changes in the evaluation methodology had two objectives: the first one was to quantify the gain in performance when more sophisticated validation modules are introduced in QA systems. The second objective was to bring systems based on Textual Entailment to the Automatic Hypothesis Generation problem which is not part itself of the Recognising Textual Entailment (RTE) task but a need of the Answer Validation setting. 9 groups have participated with 16 runs in 4 different languages. Compared with the QA systems, the results show an evidence of the potential gain that more sophisticated AV modules introduce in the task of QA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The first Answer Validation Exercise (AVE 2006) <ref type="bibr" coords="1,297.12,413.34,11.69,9.02" target="#b6">[7]</ref> was activated last year in order to promote the development and evaluation of subsystems aimed at validating the correctness of the answers given by QA systems. In some sense, systems must emulate human assessment of QA responses and decide whether an answer is correct or not according to a given text. This automatic Answer Validation is expected to be useful for improving QA systems performance <ref type="bibr" coords="1,221.92,459.31,10.64,9.02" target="#b4">[5]</ref>. However, the evaluation methodology in AVE 2006 did not permit to quantify this improvement and thus, the exercise has been modified in AVE 2007.</p><p>Figure <ref type="figure" coords="1,138.14,482.28,5.01,9.02" target="#fig_0">1</ref> shows the relationship between the QA main track and the Answer Validation Exercise. The main track provides the questions made by the organization and the responses given by the participant systems once they are judged by humans. Another difference in the exercise with respect to the AVE 2006 is the input to the participant systems. Last year we promoted an architecture based on Textual Entailment trying to bring research groups working on machine learning to Question Answering. Thus, we provided the hypothesis already built from the questions and answers <ref type="bibr" coords="2,70.91,107.76,11.68,9.02" target="#b5">[6]</ref> (see Figure <ref type="figure" coords="2,133.01,107.76,3.62,9.02" target="#fig_1">2</ref>). Then, the exercise was similar to the RTE Challenges <ref type="bibr" coords="2,368.79,107.76,11.73,9.02" target="#b0">[1]</ref> [2] <ref type="bibr" coords="2,397.67,107.76,10.62,9.02" target="#b2">[3]</ref>, where systems must decide if there is entailment or not between the supporting text and the hypothesis. In this edition, on the contrary, we left open the problem of Automatic Hypothesis Generation for those systems based on Textual Entailment. In this way, the task is more realistic and close to the Answer Validation problem, where systems receive a triplet (Question, Answer, Supporting text) instead a pair (Hypothesis, Text) (see Figure <ref type="figure" coords="2,118.17,165.24,3.61,9.02" target="#fig_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Validation</head><p>Section 2 describes the exercise in more detail. The development and testing collections are described in Section 3. Section 4 discusses the evaluation measures. Section 5 offers the results obtained by the participants and finally Section 6 present some conclusions and future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exercise Description</head><p>In this edition, participant systems received a set of triplets (Question, Answer, Supporting Text) and they must return a value for each triplet rejecting or accepting it. More in detail, the input format was a set of pairs (Answer, Supporting Text) grouped by Question (see Figure <ref type="figure" coords="3,344.36,120.24,3.62,9.02">3</ref>). Systems must consider the Question and validate each of the (Answer, Supporting Text) pairs. The number of answers to be validated per question depended on the number of participant systems at the Question Answering main track.</p><p>Participant systems must return one of the following values for each answer according to the response format (see Figure <ref type="figure" coords="3,147.34,177.72,3.71,9.02" target="#fig_2">4</ref>): Indicates that the answer is correct and supported by the given text. There is no restriction in the number of VALIDATED answers (from zero to all).</p><p>• SELECTED indicates that the answer is VALIDATED and it is the one chosen as the output of a hypothetical QA system. The SELECTED answers are evaluated against the QA systems of the Main Track. No more than one answer per question can be marked as SELECTED. At least one of the VALIDATED answers must be marked as SELECTED.</p><p>• REJECTED indicates that the answer is incorrect or there is no enough evidence of its correctness.</p><p>There is no restriction in the number of REJECTED answers (from zero to all).</p><p>This configuration permitted us to compare the AV systems responses with the QA ones, and obtain some evidences about the gain in performance that sophisticated AV modules can give to QA systems (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Collections</head><p>Since our objective was to compare AVE results with the QA main track results, we must ensure that we give to AV systems no extra information. The fact of grouping all the answers to the same question could lead to provide extra information based on counting answer redundancies that QA systems might not be considering. For this reason we removed duplicated answers inside the same question group. In fact, if an answer was contained in another answer, the shorter one was removed. Finally, NIL answers, void answers and answers with a supporting snippet larger than 700 characters (maximum permitted in the main track) were discarded for building the collections. This processing lead to a reduction in the number of answers to be validated (see Tables <ref type="table" coords="3,70.92,531.68,5.01,9.02" target="#tab_3">1</ref> and<ref type="table" coords="3,95.46,531.68,3.70,9.02" target="#tab_4">2</ref>): from 11.2% in the Italian test collection to 88.3% in the Bulgarian development collection.</p><p>For the assessments, we reused the QA judgements because they were done considering the supporting snippets in a similar way the AV systems must do. The relation between QA assessments and AVE judgements was the following:</p><p>• Answers judged as Correct have a value equal to VALIDATED • Answers judged as Wrong or Unsupported have a value equal to REJECTED • Answers judged as Inexact have a value equal to UNKNOWN and are ignored for evaluation purposes.</p><p>• Answers not evaluated at the QA main track (if any) are also tagged as UNKNOWN and they are also ignored in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Development Collections</head><p>Development collections were obtained from the QA@CLEF 2006 <ref type="bibr" coords="3,345.45,674.04,11.68,9.02" target="#b5">[6]</ref> main track questions and answers. Table <ref type="table" coords="3,70.92,685.50,5.01,9.02" target="#tab_3">1</ref> shows the number of questions and answers for each language together with the percentage that these answers represent over the number of answers initially available, and the number of answers with VALIDATED and REJECTED values. These collections were available for participants after their registration at CLEF at http://nlp.uned.es/QA/ave/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Test Collections</head><p>Test collections were obtained from the QA@CLEF 2007 main track. In this edition, questions were grouped by topic <ref type="bibr" coords="4,146.37,291.06,10.61,9.02" target="#b3">[4]</ref>. The first question of a topic was self contained in the sense that there is no need of information outside the question to answer it. However, the rest of the topic questions can refer to implicit information linked to the previous questions and answers of the topic group (anaphora, co-reference, etc.).</p><p>For the AVE 2007 test collections we only made use of the self-contained questions (the first one of each topic group) and their respective answers given by the participant systems in QA.</p><p>The change of the task produced a lower participation in the main track because systems were not tuned on time and this fact, together with the consideration of less number of questions and the elimination of redundancies led to a reduction of the evaluation corpora in AVE 2007.</p><p>Table <ref type="table" coords="4,132.33,383.10,5.01,9.02" target="#tab_4">2</ref> shows the number of questions and the number of answers to be validated (or rejected) in the test collections together with the percentage that these answers represent over the answers initially available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation of the Answer Validation Exercise</head><p>In <ref type="bibr" coords="4,117.69,611.70,11.71,9.02" target="#b6">[7]</ref> was argued why the AVE evaluation is based on the detection of the correct answers. Instead of using an overall accuracy as the evaluation measure, we proposed the use of precision (1), recall (2) and Fmeasure (3) (harmonic mean) over answers that must be VALIDATED. In other words, we proposed to quantify systems ability to detect whether there is enough evidence to accept an answer.</p><p>Results can be compared between systems but always taking as reference the following baselines: 1. A system that accepts all answers (return VALIDATED or SELECTED in 100% of cases) 2. A system that accepts 50% of the answers (random) 1 Assessments not available at the this report was submited (2)</p><formula xml:id="formula_0" coords="5,169.44,73.95,267.04,25.40">| _ _ _ _ | | _ _ _ _ _ | VALIDATED</formula><formula xml:id="formula_1" coords="5,222.54,164.64,152.35,26.51">precision recall precision recall F + = • • 2<label>(3)</label></formula><p>However, this is an intrinsic evaluation that is not enough for comparing AVE results with QA results in order to obtain some evidence about the goodness of incorporating more sophisticated validation systems into the QA architecture. Some recent works <ref type="bibr" coords="5,243.34,231.06,11.69,9.02" target="#b4">[5]</ref> have shown how the use of textual entailment can improve the accuracy of QA systems. Our aim was to obtain evidences of this improvement in a comparative and shared evaluation.</p><p>For this reason, a new measure (4), very easy to understand, was applied in AVE 2007. Since answers were grouped by questions and AV systems were requested to SELECT one or none of them, the resulting behaviour is comparable to a QA system: for each question there is no more than one SELECTED answer. The proportion of correctly selected answers is a measure comparable to the accuracy used in the QA Main Track and, therefore, we can compare AV systems taking as reference the QA systems performance over the questions involved in AVE test collections.</p><formula xml:id="formula_2" coords="5,168.96,345.42,259.02,24.57">| | | _ _ | _ questions correctly SELECTED answers accuracy qa =<label>(4)</label></formula><p>This measure has an upper bound given by the proportion of questions that have at least one correct answer (in its corresponding group). This upper bound corresponds to a perfect selection of the correct answers given by all the QA systems at the main track. The normalization of qa_accuracy with this upper bound is given in <ref type="bibr" coords="5,81.26,421.26,10.64,9.02" target="#b4">(5)</ref>. We will refer to this measure also as percentage of the perfect selection (normalized_qa_accuracy x 100). </p><formula xml:id="formula_3" coords="5,183.65,445.92,235.60,24.57">| _ _ _ | | _ _ | _ _</formula><p>Besides the upper bound, results of qa_accuracy can be compared with the following baseline system: A system that validates 100% of the answers and selects randomly one of them. Thus, this baseline can be seen as the average proportion of correct answers per question group <ref type="bibr" coords="5,317.35,510.30,10.63,9.02" target="#b5">(6)</ref>.</p><formula xml:id="formula_5" coords="5,115.85,534.90,365.22,25.48">∑ ∈ = questions q q of answers q of answers correct questions accuracy qa random | ) ( _ | | ) ( _ _ | | | 1 _ _<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Nine groups (2 less than the past edition) have participated in four different languages. Table <ref type="table" coords="5,474.89,612.48,5.01,9.02" target="#tab_7">3</ref> shows the participant groups and the number of runs they submitted per language. Again, English and Spanish were the most popular with 8 and 5 runs respectively.</p><p>Tables <ref type="table" coords="5,112.21,646.98,4.45,9.02" target="#tab_8">4</ref><ref type="table" coords="5,116.66,646.98,4.45,9.02" target="#tab_9">5</ref><ref type="table" coords="5,116.66,646.98,4.45,9.02" target="#tab_10">6</ref><ref type="table" coords="5,121.11,646.98,4.45,9.02" target="#tab_11">7</ref>show the results for all participant systems in each language. Results cannot be compared between languages since the number of answers to be validated and the proportion of the correct ones are different for each language (due to the real submission of the QA systems). Together with the systems precision, recall and Fmeasure, the two baselines values are shown: the results of a system that always accept all answers (validates 100% of the answers), and the results of a hypothetical system that validates the 50% of answers. In our opinion, F-measure is an appropriate measure to identify the systems that perform better, measuring their ability to detect the correct answers and only them. However, we wanted to obtain some evidence about the improvement that more sophisticated AV systems could provide to QA systems. Tables 8-11 show the rankings of systems (merging QA and AV systems) according to the QA accuracy calculated only over the subset of questions considered in AVE 2007. With the exception of Portuguese were there is only one participant group, there are AV systems for each language able to achieve more than 70% of the perfect selection. In German and English, the best AV systems obtained better results than the QA systems, achieving a 93% of the perfect selection in the case of German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head><p>In general, the groups that participated in both QA Main Track and AVE, obtained better results with the AV system than with the QA one. This can be due to two factors: Or they need to extract more and better candidate answers, or they do not use their own AV module to rank them properly in the QA system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,163.68,729.54,268.04,9.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relationship between the QA Track and the AV Exercise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,70.92,352.92,453.49,9.02;2,70.92,364.38,212.47,9.02"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. From an Answer Validation architecture based on Textual Entailment in AVE 2006 to the complete Answer Validation systems evaluation in AVE 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,216.72,228.54,161.93,9.02;3,162.48,206.10,299.96,7.88"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Response format in AVE 2007 q_id a_id [SELECTED|VALIDATED|REJECTED] confidence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,127.38,440.42,320.34,96.91"><head>&lt;q id="116" lang="EN"&gt; &lt;q_str&gt;What</head><label></label><figDesc></figDesc><table /><note coords="2,220.37,449.48,91.17,6.27;2,162.78,458.53,110.37,6.27;2,198.18,467.59,182.37,6.27;2,198.18,476.71,86.38,6.27;2,198.18,485.77,230.37,6.27;2,198.18,495.07,230.35,6.27;2,198.18,504.12,220.75,6.27;2,198.18,513.18,249.55,6.27;2,198.18,522.00,67.18,6.27;2,162.78,531.06,4.80,6.27"><p>is Zanussi?&lt;/q_str&gt; &lt;a id="116_1" value=""&gt; &lt;a_str&gt;was an Italian producer of home appliances&lt;/a_str&gt; &lt;t_str doc="Zanussi"&gt;Zanussi For the Polish film director, see Krzysztof Zanussi. For the hot-air balloon, see Zanussi (balloon). Zanussi was an Italian producer of home appliances that in 1984 was bought&lt;/t_str&gt; &lt;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,162.78,531.06,275.36,96.85"><head>/a&gt; &lt;a id="116_2" value=""&gt; &lt;a_str&gt;who had also been in Cassibile since August 31&lt;/a_str&gt; &lt;t_str doc="en/p29/2998260.xml"&gt;Only after</head><label></label><figDesc></figDesc><table coords="2,162.78,567.29,265.75,60.62"><row><cell>the</cell></row><row><cell>signing had taken place was Giuseppe Castellano</cell></row><row><cell>informed of the additional clauses that had been</cell></row><row><cell>presented by general Ronald Campbell to another</cell></row><row><cell>Italian general, Zanussi, who had also been in</cell></row><row><cell>Cassibile since August 31.&lt;/t_str&gt;</cell></row><row><cell>&lt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,127.38,621.63,296.37,98.20"><head>/a&gt; &lt;a id="116_4" value=""&gt; &lt;a_str&gt;3&lt;/a_str&gt; &lt;t_str doc="1618911.xml"&gt;(1985) 3</head><label></label><figDesc></figDesc><table coords="2,127.38,648.80,296.37,71.03"><row><cell></cell><cell>Out of 5 Live</cell></row><row><cell>(1985)</cell><cell>What Is This?&lt;/t_str&gt;</cell></row><row><cell>&lt;/a&gt;</cell><cell></cell></row><row><cell>&lt;/q&gt;</cell><cell></cell></row><row><cell cols="2">Figure 3. Excerpt of the English test collection in AVE 2007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,78.48,79.19,438.46,127.41"><head>Table 1 .</head><label>1</label><figDesc>Number of questions and answers in the AVE 2007 development collections</figDesc><table coords="4,78.48,79.19,438.46,115.40"><row><cell></cell><cell></cell><cell>English</cell><cell>Spanish</cell><cell>French</cell><cell>Italian</cell><cell>Dutch</cell><cell>Portuguese</cell><cell>Bulgarian</cell></row><row><cell>Questions</cell><cell>187</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>192</cell><cell>198</cell><cell>200</cell><cell>56</cell></row><row><cell>Answers (final)</cell><cell>504</cell><cell>1121</cell><cell>1817</cell><cell>1503</cell><cell>476</cell><cell>528</cell><cell>817</cell><cell>70</cell></row><row><cell>% over available answers</cell><cell>31.5%</cell><cell>62.28%</cell><cell>53.44%</cell><cell>50.1%</cell><cell>47.6%</cell><cell>44%</cell><cell>40.85%</cell><cell>11.67%</cell></row><row><cell>VALIDATED</cell><cell>135</cell><cell>130</cell><cell>265</cell><cell>263</cell><cell>86</cell><cell>100</cell><cell>153</cell><cell>49</cell></row><row><cell>REJECTED</cell><cell>369</cell><cell>991</cell><cell>1552</cell><cell>1240</cell><cell>390</cell><cell>428</cell><cell>664</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,77.70,423.47,440.33,137.79"><head>Table 2 .</head><label>2</label><figDesc>Number of questions and answers in the AVE 2007 test collections</figDesc><table coords="4,77.70,423.47,440.33,125.72"><row><cell></cell><cell></cell><cell>English</cell><cell>Spanish</cell><cell>French</cell><cell>Italian</cell><cell>Dutch</cell><cell>Portuguese</cell><cell>Romanian</cell></row><row><cell>Questions</cell><cell>113</cell><cell>67</cell><cell>170</cell><cell>122</cell><cell>103</cell><cell>78</cell><cell>149</cell><cell>100</cell></row><row><cell>Answers (final)</cell><cell>282</cell><cell>202</cell><cell>564</cell><cell>187</cell><cell>103</cell><cell>202</cell><cell>367</cell><cell>127</cell></row><row><cell>% over available answers</cell><cell>48.62%</cell><cell>60.3%</cell><cell>66.35%</cell><cell>75.4%</cell><cell>88.79%</cell><cell>51.79%</cell><cell>30.58%</cell><cell>52.05%</cell></row><row><cell>VALIDATED</cell><cell>67</cell><cell>21</cell><cell>127</cell><cell>(1)</cell><cell>16</cell><cell>31</cell><cell>148</cell><cell>45</cell></row><row><cell>REJECTED</cell><cell>197</cell><cell>174</cell><cell>424</cell><cell>(1)</cell><cell>84</cell><cell>165</cell><cell>198</cell><cell>58</cell></row><row><cell>UNKNOWN</cell><cell>18</cell><cell>7</cell><cell>13</cell><cell>( ) 1</cell><cell>3</cell><cell>6</cell><cell>21</cell><cell>24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,157.86,89.81,274.33,294.26"><head>Table 3 .</head><label>3</label><figDesc>Participants and runs per language in AVE 2007</figDesc><table coords="6,344.76,89.81,87.44,47.83"><row><cell>English</cell><cell>Spanish</cell><cell>Portuguese</cell><cell>Total</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,148.92,387.06,297.52,81.68"><head>Table 4 .</head><label>4</label><figDesc>Precision, Recall and F measure over correct answers for Spanish</figDesc><table coords="6,177.96,411.84,237.42,56.90"><row><cell>Group</cell><cell>System</cell><cell>F</cell><cell cols="2">Precision Recall</cell></row><row><cell>FUH</cell><cell>iglockner_1</cell><cell>0.72</cell><cell>0.61</cell><cell>0.9</cell></row><row><cell>FUH</cell><cell>iglockner_2</cell><cell>0.68</cell><cell>0.54</cell><cell>0.94</cell></row><row><cell cols="2">100% VALIDATED</cell><cell>0.4</cell><cell>0.25</cell><cell>1</cell></row><row><cell cols="2">50% VALIDATED</cell><cell>0.34</cell><cell>0.25</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,148.68,471.72,297.99,153.62"><head>Table 5 .</head><label>5</label><figDesc>Precision, Recall and F measure over correct answers for German</figDesc><table coords="6,157.80,496.44,276.06,128.90"><row><cell>Group</cell><cell>System</cell><cell>F</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>DFKI</cell><cell>ltqa_2</cell><cell>0.55</cell><cell>0.44</cell><cell>0.71</cell></row><row><cell>DFKI</cell><cell>ltqa_1</cell><cell>0.46</cell><cell>0.37</cell><cell>0.62</cell></row><row><cell>U. Alicante</cell><cell>ofe_1</cell><cell>0.39</cell><cell>0.25</cell><cell>0.81</cell></row><row><cell cols="2">Text-Mess Project Text-Mess_1</cell><cell>0.36</cell><cell>0.25</cell><cell>0.62</cell></row><row><cell>Iasi</cell><cell>adiftene</cell><cell>0.34</cell><cell>0.21</cell><cell>0.81</cell></row><row><cell>UNED</cell><cell>rodrigo</cell><cell>0.34</cell><cell>0.22</cell><cell>0.71</cell></row><row><cell cols="2">Text-Mess Project Text-Mess_2</cell><cell>0.34</cell><cell>0.25</cell><cell>0.52</cell></row><row><cell>U. Alicante</cell><cell>ofe_2</cell><cell>0.29</cell><cell>0.18</cell><cell>0.81</cell></row><row><cell cols="2">100% VALIDATED</cell><cell>0.19</cell><cell>0.11</cell><cell>1</cell></row><row><cell cols="2">50% VALIDATED</cell><cell>0.18</cell><cell>0.11</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,149.52,628.32,296.42,69.68"><head>Table 6 .</head><label>6</label><figDesc>Precision, Recall and F measure over correct answers for English</figDesc><table coords="6,177.96,653.10,237.42,44.90"><row><cell>Group</cell><cell>System</cell><cell>F</cell><cell cols="2">Precision Recall</cell></row><row><cell>UE</cell><cell>jsaias</cell><cell>0.68</cell><cell>0.91</cell><cell>0.55</cell></row><row><cell cols="2">100% VALIDATED</cell><cell>0.6</cell><cell>0.43</cell><cell>1</cell></row><row><cell cols="2">50% VALIDATED</cell><cell>0.46</cell><cell>0.43</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="6,142.56,700.98,310.30,9.02"><head>Table 7 .</head><label>7</label><figDesc>Precision, Recall and F measure over correct answers for Portuguese</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="7,147.60,189.96,300.21,320.54"><head>Table 8 .</head><label>8</label><figDesc>Comparing AV systems performance with QA systems in Spanish</figDesc><table coords="7,170.64,189.96,253.91,320.54"><row><cell></cell><cell cols="2">System System</cell><cell>QA</cell><cell>% of perfect</cell></row><row><cell></cell><cell></cell><cell>Type</cell><cell>accuracy</cell><cell>selection</cell></row><row><cell cols="2">Perfect selection</cell><cell>QA</cell><cell>0.59</cell><cell>100%</cell></row><row><cell>Priberam</cell><cell></cell><cell>QA</cell><cell>0.49</cell><cell>83.17%</cell></row><row><cell cols="2">INAOE tellez_1</cell><cell>AV</cell><cell>0.45</cell><cell>75.25%</cell></row><row><cell cols="2">UNED rodrigo</cell><cell>AV</cell><cell>0.42</cell><cell>70.3%</cell></row><row><cell cols="2">UJA magc_1</cell><cell>AV</cell><cell>0.41</cell><cell>68.32%</cell></row><row><cell>INAOE</cell><cell></cell><cell>QA</cell><cell>0.38</cell><cell>63.37%</cell></row><row><cell cols="2">INAOE tellez_2</cell><cell>AV</cell><cell>0.36</cell><cell>61.39%</cell></row><row><cell cols="2">Random</cell><cell>AV</cell><cell>0.25</cell><cell>41.45%</cell></row><row><cell>MIRA</cell><cell></cell><cell>QA</cell><cell>0.15</cell><cell>25.74%</cell></row><row><cell>UPV</cell><cell></cell><cell>QA</cell><cell>0.13</cell><cell>21.78%</cell></row><row><cell cols="2">UJA magc_2</cell><cell>AV</cell><cell>0.08</cell><cell>13.86%</cell></row><row><cell>TALP</cell><cell></cell><cell>QA</cell><cell>0.07</cell><cell>11.88%</cell></row><row><cell>Group</cell><cell>System</cell><cell>System</cell><cell>QA</cell><cell>% of perfect</cell></row><row><cell></cell><cell></cell><cell>Type</cell><cell>accuracy</cell><cell>selection</cell></row><row><cell cols="2">Perfect selection</cell><cell>QA</cell><cell>0.54</cell><cell>100%</cell></row><row><cell cols="2">FUH iglockner_2</cell><cell>AV</cell><cell>0.50</cell><cell>93.44%</cell></row><row><cell cols="2">FUH iglockner_1</cell><cell>AV</cell><cell>0.48</cell><cell>88.52%</cell></row><row><cell cols="2">DFKI dfki071dede</cell><cell>QA</cell><cell>0.35</cell><cell>65.57%</cell></row><row><cell cols="2">FUH fuha071dede</cell><cell>QA</cell><cell>0.32</cell><cell>59.02%</cell></row><row><cell cols="2">Random</cell><cell>AV</cell><cell>0.28</cell><cell>51.91%</cell></row><row><cell cols="2">DFKI dfki071ende</cell><cell>QA</cell><cell>0.25</cell><cell>45.9%</cell></row><row><cell cols="2">FUH fuha072dede</cell><cell>QA</cell><cell>0.21</cell><cell>39.34%</cell></row><row><cell cols="2">DFKI dfki071ptde</cell><cell>QA</cell><cell>0.05</cell><cell>9.84%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="7,135.90,513.48,328.01,225.14"><head>Table 9 .</head><label>9</label><figDesc>Comparing AV systems performance with QA systems in German</figDesc><table coords="7,135.90,538.26,328.01,200.36"><row><cell>Group</cell><cell>System</cell><cell>System</cell><cell>QA</cell><cell>% of perfect</cell></row><row><cell></cell><cell></cell><cell>Type</cell><cell>accuracy</cell><cell>selection</cell></row><row><cell cols="2">Perfect selection</cell><cell>QA</cell><cell>0.3</cell><cell>100%</cell></row><row><cell cols="2">DFKI Itqa_2</cell><cell>AV</cell><cell>0.21</cell><cell>70%</cell></row><row><cell cols="2">Iasi adiftene</cell><cell>AV</cell><cell>0.21</cell><cell>70%</cell></row><row><cell>UA ofe_2</cell><cell></cell><cell>AV</cell><cell>0.19</cell><cell>65%</cell></row><row><cell cols="2">U.Indonesia CSUI_INEN</cell><cell>QA</cell><cell>0.18</cell><cell>60%</cell></row><row><cell>UA ofe_1</cell><cell></cell><cell>AV</cell><cell>0.18</cell><cell>60%</cell></row><row><cell cols="2">DFKI Itqa_1</cell><cell>AV</cell><cell>0.16</cell><cell>55%</cell></row><row><cell cols="2">UNED rodrigo</cell><cell>AV</cell><cell>0.16</cell><cell>55%</cell></row><row><cell cols="2">Text-Mess Project Text-Mess_1</cell><cell>AV</cell><cell>0.15</cell><cell>50%</cell></row><row><cell cols="2">DFKI DFKI_DEEN</cell><cell>QA</cell><cell>0.13</cell><cell>45%</cell></row><row><cell cols="2">Text-Mess Project Text-Mess_2</cell><cell>AV</cell><cell>0.12</cell><cell>40%</cell></row><row><cell>Random</cell><cell></cell><cell>AV</cell><cell>0.1</cell><cell>35%</cell></row><row><cell cols="2">DFKI DFKI_ESEN</cell><cell>QA</cell><cell>0.04</cell><cell>15%</cell></row><row><cell cols="2">Macquarie MQAF_NLEN_1</cell><cell>QA</cell><cell>0</cell><cell>0%</cell></row><row><cell cols="2">Macquarie MQAF_NLEN_2</cell><cell>QA</cell><cell>0</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="7,145.62,741.66,304.14,9.02"><head>Table 10 .</head><label>10</label><figDesc>Comparing AV systems performance with QA systems in English</figDesc><table coords="8,167.88,73.86,272.51,152.36"><row><cell>Group</cell><cell>System</cell><cell>System</cell><cell>QA</cell><cell>% of perfect</cell></row><row><cell></cell><cell></cell><cell>Type</cell><cell>accuracy</cell><cell>selection</cell></row><row><cell cols="2">Perfect selection</cell><cell>QA</cell><cell>0.74</cell><cell>100%</cell></row><row><cell>Priberam</cell><cell></cell><cell>QA</cell><cell>0.61</cell><cell>82.73%</cell></row><row><cell cols="2">UE jsaias</cell><cell>AV</cell><cell>0.44</cell><cell>60%</cell></row><row><cell>Random</cell><cell></cell><cell>AV</cell><cell>0.44</cell><cell>60%</cell></row><row><cell cols="2">U. Evora diue</cell><cell>QA</cell><cell>0.41</cell><cell>55.45%</cell></row><row><cell cols="2">LCC lcc_ENPT</cell><cell>QA</cell><cell>0.3</cell><cell>40%</cell></row><row><cell cols="2">U. Porto feup</cell><cell>QA</cell><cell>0.23</cell><cell>30.91%</cell></row><row><cell cols="2">INESC-ID CLEF07-2_PT</cell><cell>QA</cell><cell>0.13</cell><cell>17.27%</cell></row><row><cell cols="2">INESC-ID CLEF07_PT</cell><cell>QA</cell><cell>0.11</cell><cell>15.45%</cell></row><row><cell cols="2">SINTEF esfi_1</cell><cell>QA</cell><cell>0.07</cell><cell>10%</cell></row><row><cell cols="2">SINTEF esfi_2</cell><cell>QA</cell><cell>0.04</cell><cell>5.45%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="8,70.93,229.20,453.67,67.71"><head>Table 10 .</head><label>10</label><figDesc>Comparing AV systems performance with QA systems in Portuguese All the participant groups in AVE 2007 reported the use of an approach based on Textual Entailment. 5 of the 9 groups (FUH, U. Iasi, INAOE, FUH, U. Évora and DFKI) have also participated in the Question Answering Track, showing that techniques developed for Textual Entailment are in the process of being incorporated in the QA systems participating at CLEF.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> within the <rs type="projectName">Text-Mess-INES</rs> project (<rs type="grantNumber">TIN2006-15265-C06-02</rs>), the <rs type="funder">Education Council of the Regional Government of Madrid</rs> and the <rs type="funder">European Social Fund</rs>. We are grateful to all the people involved in the organization of the QA track (specially to the coordinators at CELCT, <rs type="person">Danilo Giampiccolo</rs> and <rs type="person">Pamela Forner</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_yhKbNJu">
					<idno type="grant-number">TIN2006-15265-C06-02</idno>
					<orgName type="project" subtype="full">Text-Mess-INES</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Theorem prover X X X Semantic similarity X X Table <ref type="table" coords="8,170.39,649.32,8.34,9.02">12</ref>. Techniques, resources and methods used by the AVE participants.</p><p>Table <ref type="table" coords="8,132.04,672.36,9.99,9.02">12</ref> shows the techniques used by AVE participant systems. In general, the groups that performed some kind of syntactic or semantic analysis worked in the Automatic Hypothesis Generation as a combination of the question and the answer. However, in some cases the hypothesis generated was directly in a logic form instead of a textual sentence.</p><p>All the participants reported the use of lexical processing. Lemmatization and part of speech tagging were commonly used. In the other side, only few systems used first order logic representations, performed semantic analysis and took the validation decision with a theorem prover.</p><p>Lexical similarity was the feature most used for taking the validation decision. In general, systems that performed syntactic or semantic processing used this processing as similarity features. None of the systems reported the use of semantic frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this second edition of the Answer Validation Exercise, techniques developed for Recognizing Textual Entailment have been employed widely, although the exercise was defined more closely to the real answer validation application.</p><p>We have refined the evaluation methodology in order to consider the QA systems performance as a reference for AV systems evaluation. Thus, new measures have been defined together with their respective baselines: qa_accuracy and the percentage of the perfect selection (normalized_qa_accuracy).</p><p>With respect to the development of test collections, the new evaluation framework led us to reduce redundancies in the sets of answers. This process reduces the size of the testing collections discarding around 50% of candidate answers. The training and testing collections resulting from AVE 2006 and 2007 are available at http://nlp.uned.es/QA/ave for researchers registered at CLEF.</p><p>Results show that AV systems are able to detect correct answers improving the results of QA systems. In fact, except for Portuguese (where there is only one participant at AVE), all the systems are far from the random behaviour and closer to the perfect selection (from 70% to 93%).</p><p>All systems utilize lexical processing, most of them introduce a syntactic level and only few make use of semantics and logic. Groups that participated in both QA and AVE tracks show better performance in the selection of answers than the results obtained by the whole QA system. This fact points to the need of considering the evidences given by the AV modules in order to generate more and better candidate answers. In this way, the approach of looping the AV module with the generation of candidate answers should be considered instead of the solely approach based on the ranking of candidate answers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,74.69,516.24,449.86,9.02;9,88.93,527.70,435.66,9.02;9,88.93,539.22,365.04,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,157.28,527.70,269.77,9.02">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.35,527.70,77.24,9.02;9,88.93,539.22,304.21,9.02">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,74.69,550.74,449.79,9.02;9,88.93,562.20,360.67,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,330.93,550.74,193.56,9.02;9,88.93,562.20,38.76,9.02">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,134.52,562.20,141.11,9.02">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2006-01">2006. Jan 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,74.69,573.72,449.74,9.02;9,88.93,585.18,421.86,9.02" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,383.29,573.72,141.14,9.02;9,88.93,585.18,392.71,9.02">The Third PASCAL Recognizing Textual Entailment Challenge. ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,74.69,596.70,449.74,9.02;9,88.93,608.22,124.24,9.02" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,229.01,596.70,291.12,9.02">Overview of the CLEF 2007 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>Working Notes of CLEF</note>
</biblStruct>

<biblStruct coords="9,74.69,619.68,449.77,9.02;9,88.93,631.20,435.52,9.02;9,88.94,642.72,158.40,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,190.41,619.68,317.38,9.02">Methods for Using Textual Entailment in Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,88.93,631.20,435.52,9.02;9,88.94,642.72,31.65,9.02">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,74.69,654.18,449.69,9.02;9,88.93,665.70,435.49,9.02;9,88.93,677.22,435.56,9.02;9,88.93,688.68,148.12,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,457.42,665.70,67.00,9.02;9,88.93,677.22,219.05,9.02">Overview of the CLEF 2006 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.90,677.22,47.18,9.02">CLEF 2006</title>
		<title level="s" coord="9,373.15,677.22,151.34,9.02;9,88.93,688.68,25.64,9.02">Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Berlín</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,74.69,700.20,449.68,9.02;9,88.93,711.72,407.21,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,379.65,700.20,144.72,9.02;9,88.93,711.72,34.46,9.02">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentín</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.90,711.72,44.94,9.02">CLEF 2006</title>
		<title level="s" coord="9,202.86,711.72,170.69,9.02">Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Berlín</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2007. 2006</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
