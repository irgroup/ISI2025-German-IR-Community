<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,441.00,148.86,72.00,15.15">QA 2007</title>
				<funder>
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_TdXFnE7">
					<orgName type="full">Valentin Jijkoun</orgName>
				</funder>
				<funder ref="#_nebGknx #_pWzbSVp">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_m6zG7KP #_6aQBPKe #_VuvGxKF #_BypSU8T #_D4XNuKH #_8eUbmcz #_eH8PVeP #_4afXXVf #_sxqa8nM #_PpTG6fb #_PC4NfeW #_THXhFwx #_9Gp4ADW #_a2Ec8KM">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,127.85,182.75,71.54,8.74;1,219.31,182.75,24.63,8.74"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Katja</surname></persName>
						</author>
						<author>
							<persName coords="1,247.26,182.75,39.85,8.74;1,307.04,182.75,25.87,8.74"><forename type="first">Hofmann</forename><surname>David</surname></persName>
							<email>khofmann@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,336.23,182.75,18.55,8.74;1,374.70,182.75,40.96,8.74"><forename type="first">Ahn</forename><surname>Mahboob</surname></persName>
							<email>mahboob@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,418.97,182.75,56.18,8.74"><forename type="first">Alam</forename><surname>Khalid</surname></persName>
						</author>
						<author>
							<persName coords="1,157.40,196.70,39.24,8.74"><forename type="first">Joris</forename><surname>Van</surname></persName>
						</author>
						<author>
							<persName coords="1,199.96,196.70,39.44,8.74;1,259.32,196.70,75.99,8.74"><forename type="first">Rantwijk</forename><surname>Maarten De Rijke</surname></persName>
							<email>rantwijk@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,355.25,196.70,47.76,8.74"><forename type="first">Erik</forename><surname>Tjong</surname></persName>
							<email>erikt@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,406.33,196.70,43.18,8.74"><forename type="first">Kim</forename><surname>Sang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam at CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,441.00,148.86,72.00,15.15">QA 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D7E5C1A45EDAE5BE64534D4F38009349</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a new version of our question answering system, which was applied to the questions of the 2007 CLEF Question Answering Dutch monolingual task. This year, we made three major modifications to the system: (1) we added the contents of Wikipedia to the document collection and the answer tables; (2) we completely rewrote the module interface code in Java; and (3) we included a new table stream which returned answer candidates based on information which was learned from questionanswer pairs. Unfortunately, the changes did not lead to improved performance. Unsolved technical problems at the time of the deadline have led to missing justifications for a large number of answers in our submission. Our single run obtained an accuracy of only 8% with an additional 12% of unsupported answers (last year, our best run achieved 21%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For our earlier participations in the CLEF question answering track <ref type="bibr" coords="1,400.00,451.84,23.06,8.74">(2003)</ref><ref type="bibr" coords="1,423.06,451.84,4.61,8.74">(2004)</ref><ref type="bibr" coords="1,423.06,451.84,4.61,8.74">(2005)</ref><ref type="bibr" coords="1,427.68,451.84,23.06,8.74">(2006)</ref>, we have developed a parallel question answering architecture in which candidate answers to a question are generated by different competing strategies, QA streams <ref type="bibr" coords="1,341.56,475.75,9.97,8.74" target="#b3">[4]</ref>. Although our streams use different approaches to answer extraction and generation, they share the mechanism for accessing the collection data: we have converted all of our data resources (text, linguistic annotations, and tables of automatically extacted facts) to fit in an XML database in order to standardize the access <ref type="bibr" coords="1,499.72,511.62,9.96,8.74" target="#b3">[4]</ref>. For the 2007 version of the system, we have focused on three tasks:</p><p>1. Add to the data resources of the system material derived from the Dutch Wikipedia (previously only derived from Dutch newspaper text).</p><p>2. Rewrite the out-of-date code which takes care of the communication between the different modules (previously in Perl) in Java. In the long run we are aiming at a system which is completely written in Java and is easily maintainable.</p><p>3. Add a new question answering stream to our parallel architecture: a stream that generates answers from pre-extracted relational information based on learned associations between questions and answers; a similar stream in last year's system used manual rules for identifying such associations.</p><p>This paper is divided in seven sections. In section 2, we give an overview of the current system architecture. In the next three sections, we describe the changes made to our system for this year: resource adaptation (section 3), code rewriting (4), and the new table stream <ref type="bibr" coords="1,438.20,702.90,11.63,8.74" target="#b4">(5)</ref>. We describe our submitted runs in section 6 and conclude in section 7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The architecture of our Quartz QA system is an expanded version of a standard QA architecture consisting of parts dealing with question analysis, information retrieval, answer extraction, and answer post-processing (clustering, ranking, and selection). The Quartz architecture consists of multiple answer extraction modules, or streams, which share common question and answer processing components. The answer extraction streams can be divided into three groups based on the text corpus that they employ: the CLEF-QA corpus, Dutch Wikipedia, or the Web. Below, we describe these briefly.</p><p>The Quartz system (Figure <ref type="figure" coords="2,225.31,466.78,4.43,8.74">1</ref>) contains four streams that generate answers from the two CLEF data sources, the CLEF newspaper corpus and Wikipedia. The Table Lookup stream searches for answers in specialized knowledge bases which are extracted from the corpus offline (prior to question time) by predefined rules. These information extraction rules take advantage of the fact that certain answer types, such as birthdays, are typically expressed in one of a small set of easily identifiable ways. The stream uses the analysis of a question to determine how a candidate answer should be looked up in the database using a manually defined mapping from question to database queries. Our new stream, ML Table <ref type="table" coords="2,288.72,550.47,28.46,8.74">Lookup</ref>, performs the answer lookup task by using a mapping learned automatically from a set of training questions (see section 5 for a more elaborate description). The Ngrams stream looks for answers in the corpus by searching for most frequent word ngrams in a list of text passages retrieved from the collection using a standard retrieval engine (Lucene) using a text query generated from the question.</p><p>The most advanced of the four streams is XQuesta. For a given question, it automatically generates XPath queries for answer extraction, and executes them on an XML version of the corpus which contains both the corpus text and additional annotations. The annotations include information about part-of-speech, syntactic chunks, named entities, temporal expressions, and dependency parses (from the Alpino parser <ref type="bibr" coords="2,282.89,658.06,10.30,8.74" target="#b5">[6]</ref>). For each question, XQuesta only examines text passages relevant to the question (as identified by Lucene).</p><p>There is one stream which employs textual data outside the CLEF document collection defined for the task: the Ngrams stream also retrieves answers by submitting automatically generated web queries to the Google web search engine and collecting most common ngrams from the returned snippets. The answers candidates found by this approach are not backed up by documents from the CLEF collection as required by the task. For this reason such candidates are never returned as actual answers, but only used at the answer merging stage to adjust the ranking of answers that are found by other QA streams.</p><p>3 Wikipedia as a QA Resource</p><p>Our system uses Dutch Wikipedia in the same way as the Dutch newspaper corpus. We used an XML dump of Wikipedia<ref type="foot" coords="3,221.95,177.17,3.97,6.12" target="#foot_0">1</ref> that provides basic structural markup and additionally annotated it with sentence boundaries, part-of-speech tags, named entities and temporal expressions. Wikipedia was consulted by the XQuesta and NGram streams and was also used for offline information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rewriting Interface Code in Java</head><p>The QA system we used in previous years consisted of many components but was mostly developed ad-hoc, i.e., without a consistent system architecture. As a result, the system was difficult to maintain and change. To address this problem we re-implemented large parts of the system following a modular system design. The goal is to develop a self-contained system that is consistent and can be maintained more easily. The main feature of the newly developed system architecture is that it consists of several modules which are cleanly separated by interfaces. This allows us to minimize dependencies between components.</p><p>Figure <ref type="figure" coords="3,136.52,353.06,4.98,8.74" target="#fig_0">2</ref> gives an overview of the main components of our QA system. The core modules are document collection, text analysis, data, question answering, and answer selection. The package apps contains several small programs that combine functionality of the core modules into complete applications, such as the CLEF batch system, an interactive command-line program, and an online demo of our QA system.  Central to our system is the data module which abstracts all textual data within the system as AnnotatedText. AnnotatedText objects maintain an XML representation of the data and allow access through both Java methods and XQuery. XML annotations can be added as necessary. At each processing step the data objects can be serialized to their XML representation for logging or data exchange with external programs.</p><p>The question answering module contains our QA streams. Each stream implements the interface QuestionAnsweringStream which allows applications to run streams in a unified way. This allows us to add new QA streams to the system on the fly, without changing any of the existing components.</p><p>To make question answering streams independent of the specifics of different document collections -SQL tables, the CLEF newspaper corpus, Wikipedia, and the web -the document collection module provides access via the DocumentCollection and CollectionElement interfaces. DocumentCollection provides methods for retrieving elements from a collection. Implementations for different IR engines and web search engines were developed. To create answer candidates from CollectionElements, the elements are converted into AnnotatedText.</p><p>Text analysis tools, such as part-of-speach tagger, named-entity tagger, and question classifier, are part of the text analysis module and implement the AnalysisTool interface. Analysis tools are run on demand by a mechanism provided by the data module. Consumers of data objects, such as QA streams, specify which tools they require. The tools produce XML annotations which are added to the data objects and can be queried, for example using XQuery. We attempt to use standard XML annotation formats whenever possible.</p><p>The answer selection module contains filters for post-processing lists of answer candidates. Each post-processing tool implements the interface AnswerFilter so that applications using these tools are independent of implementation details. Sequences of post-processing tools can be assembled by higher-level applications as necessary. The tools can be applied either per-stream, or to the combined output of the system as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Machine Learning for QA from Tabular Data</head><p>As described in section 2, our offline information extraction module creates a database of simple relational facts to be used during question answering. A TableLookup QA stream uses a set of manually defined rules to map an analyzed incoming question into a database query. A new stream, MLTableLookup, uses supervised machine learning to train a classifier that performs this mapping. In this section we give an overview of our approach. We refer to <ref type="bibr" coords="4,419.72,477.62,10.52,8.74" target="#b4">[5]</ref> for further details.</p><p>Essentially, the purpose of the table lookup stream is to map an incoming question to an SQLlike query "select AF from T where sim(QF, Q)", where T is the table that contains the answer in field AF and its other field QF has a high similarity with the input question Q. Executing such a query for a given question results in a list of answer candidates-the output of the MLTableLookup stream.</p><p>In the query formalism described above, the task of generating the query can be seen as the task of mapping an incoming question Q to a triple T, QF, AF (a table-lookup label ) and defining an appropriate similarity function sim(QF, Q).</p><p>The database of facts extracted from the CLEF QA collection consists of of 16 tables containing 1.4M rows in total. For example, the Definitions(name, definition) table contains the definition of Soekarno as president of Indonesia, the table Birthdates(name,birthdate) contains the information that Aleksandr Poesjkin was born in 1799. Then, for a question such as Wie was de eerste Europese commissaris uit Finland? (Who was the first European Commissioner from Finland?) the classifier may assign the table-lookup label T : Def initions, QF : Def inition, AF : name . In this case, the question would be mapped to the SQL like query "select name from Def initions where sim(def inition, {eerste, European, commissaris, F inland})".</p><p>For an incoming question, we first extract features and apply a statistical classifier that assign a table-lookup label, i.e., a triple T, QF, AF . We then use a retrieval engine to locate values of field QF in table T which are most similar to the text of the question Q (according to a retrieval function sim(•, •)), and return values of corresponding AF fields. Figure <ref type="figure" coords="4,407.31,716.72,4.98,8.74" target="#fig_2">3</ref> shows the architecture of our system.  Our architecture depends on two modules: the classifier that predicts table lookup labels and the retrieval model sim(•, •) along with the text representation and the retrieval query formulation. For the later task we selected Lucene's vector space model as retrieval model, and used a combination of two types of text representation, exact and stemmed forms of the question words, to formulate a retrieval query, i.e., to translate an incoming to question to a retrieval query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>The interesting and novel part of the new QA system is the second stage of our query formulation, i.e., training a classifier to predict table lookup labels. This stage, in turn, can be split in two parts: generating training data and actually training the classifier. We generate the training data using the selected retrieval model. We index the values of all fields of all rows in our database as separate documents. For each question q, we translate the question into the retrieval query and use the selected retrieval model to generate a ranked list of field values from our database. We select the document, table name T and the field name QF , such that it occurs first time in the ranked list and the value of some other field AF contains the answer of the question. In other words we find T , QF and AF such that the query "select AF from T where sim(QF, Q)" returns a correct answer to question q at the top rank. We use the label T, QF, AF as a correct class for question q. For example, we translate the question In welk land in Afrika is een nieuwe grondwet aangenomen? (whose answer is Zuid-Afrika) into a retrieval query that is composed of the question words and words retrieved from the process of filtering out stopwords and stem the remaining the question words. Then we run the query against the retrieval engine's index; for this particular example our system finds the triplet T : Locations, QF : location b , AF : location a as the optimal table-lookup label for this question.</p><p>Next, in order to generate training data, we represent each question as a set of features. We use the existing module of <ref type="bibr" coords="5,211.87,584.51,10.52,8.74" target="#b1">[2]</ref> to construct the set of features. Finally we train a memory-based classifier TiMBL <ref type="bibr" coords="5,165.54,596.46,10.52,8.74" target="#b0">[1]</ref> and use a parameter optimization tool to find the best setting for Timbl; see Figure <ref type="figure" coords="5,121.43,608.42,4.98,8.74">4</ref> for an overview.</p><p>We used a set of question/answer pairs from the CLEF-QA tasks 2003-2006 and a knowledge base with tables extracted from the CLEF-QA corpus using the information extraction tools of QUARTZ system. We split our training corpus of 644 questions with answers into 10 sets and run a 10-fold cross-validation. The performance of the system is measured using the Mean Reciprocal Rank (MRR, the inverse of the rank of the first correct answer, averaged over all questions) and accuracy at n (a@n, the number of question answered at rank ≤ n). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Runs</head><p>We have submitted a single Dutch monolingual run: uams071qrtz. The associated evaluation results can be found in Table <ref type="table" coords="6,222.68,437.65,3.88,8.74" target="#tab_3">2</ref>. In this run, we have treated list questions as factoid questions: always returning the top answer. The planned updates of the system proved to be more time consuming than was expected. The system was barely finished at the time of the deadline. Because of this there was no time for an elaborate test or for compiling alternative runs. The performance of the system has suffered from this: only about 8% of the questions were answered correctly. The previous version achieved 21% correct on the CLEF-2006 questions.</p><p>The prime cause of the performance drop can be found in the submitted answer file. No less than 81 (41%) of the 200 answers did not contain the required answer snippet. This problem caused all but 4 of the unsupported assessments. 22 of these 81 answers mentioned a document id for the missing snippet but the other 59 lacked the id as well. The problem was caused by a mismatch between the new java code and the justification module which caused all justifications associated with answers from the two table streams to be lost.</p><p>When examining the answers for the factoid and definition questions, we noticed that a major problem is a mismatch between the expected answer type and the type of the answer. Here are a few examples: 0003. How often was John Lennon hit? Answer: Yoko Ono 0136. What is an antipope? Answer: Anacletus II 0160. Who is Gert Muller? Answer: 1947 As many as 61 of the 161 incorrectly answered displayed such a type mismatch. The question classification part of the system (accuracy: 80%) generates an expected type for each answer but it is not used in the postprosessing phase. Indeed, the addition of a type-based filter at the end of the processing phase is one of the most urgent tasks for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described the fifth iteration of our system for the CLEF Question Answering Dutch mono-lingual track (2007). While keeping the general multi-stream architecture, we re-designed and re-implemented the system in Java. This was an important update, which however did not lead to improved performance, mainly due to many technical problems that were not solved by the time of the deadline. In particular, these problems led to originating snippet being lost for many of the answer candidates extracted from collection, leading to a large number of answers in our submission. Our single run obtained an accuracy of only 8% with an additional 12% of unsupported answers (last year, our best run achieved 21%). Addressing these issues, performing a more systematic error analysis and answer extraction step in XQuesta stream and learning step in MLTableLookup are the most important items for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,196.68,399.31,3.97,6.12"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,90.00,630.55,423.00,8.74;3,90.00,642.51,423.00,8.74;3,90.00,654.46,67.56,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: UML Component diagram showing modules and module dependencies. Boxes represent individual modules, circles represent interfaces, and half-circles indicate that a module depends on an interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,173.91,289.67,255.18,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the MLTableLookup QA stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,116.87,423.01,221.31"><head></head><label></label><figDesc>Quartz-2007:  the University of Amsterdam's Dutch Question Answering System. After question analysis, a question is forwarded to two table modules and two retrieval modules, all of which generate candidate answers. These four question processing streams use the two data sources for this task, the Dutch CLEF newspaper corpus and Dutch Wikipedia, as well as fact tables which were generated from these data sources, and the Web. Related candidate answers are combined and ranked by a postprocessing module, which produces the final list of answers to the question.</figDesc><table coords="2,90.00,116.87,388.85,149.58"><row><cell>question</cell><cell>analysis question</cell><cell></cell><cell></cell></row><row><cell>Lookup Table</cell><cell>Lookup ML Table</cell><cell>XQuesta</cell><cell>NGram</cell></row><row><cell></cell><cell></cell><cell></cell><cell>candidate answers</cell><cell>processing post !</cell><cell>ranked answers</cell></row><row><cell></cell><cell>Dutch</cell><cell>Dutch</cell><cell></cell></row><row><cell>Tables</cell><cell>CLEF</cell><cell>Wiki !</cell><cell>Web</cell></row><row><cell></cell><cell>corpus</cell><cell>pedia</cell><cell></cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,680.15,423.01,20.69"><head>Table 1 :</head><label>1</label><figDesc>Table 1 shows the evaluation results averaged over the 10 folds. Evaluation of the ML Table-lookup QA stream applied to the CLEF 2003-2006 question answer pairs.</figDesc><table coords="6,158.32,101.43,267.82,245.93"><row><cell>Docs</cell><cell cols="2">IE tools</cell><cell>facts</cell><cell>DB Tables</cell></row><row><cell>Questions</cell><cell cols="2">retrieval query</cell><cell cols="2">add normalized layers</cell></row><row><cell></cell><cell></cell><cell cols="2">Retrieval Engine</cell></row><row><cell></cell><cell></cell><cell cols="2">&lt;T 1 ,QF, AF [...]&gt; &lt;T 2 ,QF, AF [...]&gt;</cell></row><row><cell cols="2">Feature Extractor</cell><cell cols="2">Label generator</cell><cell>Answer patterns</cell></row><row><cell cols="2">feature vector</cell><cell></cell><cell>&lt;T,QF,AF&gt;</cell></row><row><cell></cell><cell></cell><cell cols="2">Train Classifier</cell></row><row><cell></cell><cell cols="4">Figure 4: Learning table lookup labels</cell></row><row><cell></cell><cell>a@1</cell><cell>a@5</cell><cell cols="2">a@10 MRR</cell></row><row><cell></cell><cell cols="4">13.1% 21.4% 24.1% 0.593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,90.00,110.79,423.01,161.74"><head>Table 2 :</head><label>2</label><figDesc>Assessment counts for the 200 top answers in the Amsterdam run submitted for Dutch monolingual Question Answering (NLNL) in CLEF-2007. About 8% of the questions were answered correctly. Another 12% were correct but insufficiently supported. The run did not contain NIL answers.</figDesc><table coords="7,97.08,110.79,408.84,94.05"><row><cell>Question type</cell><cell cols="6">Total Right Unsupported Inexact Wrong % Correct</cell></row><row><cell>factoid</cell><cell>156</cell><cell>14</cell><cell>17</cell><cell>0</cell><cell>125</cell><cell>9%</cell></row><row><cell>definition</cell><cell>28</cell><cell>1</cell><cell>5</cell><cell>0</cell><cell>22</cell><cell>4%</cell></row><row><cell>factoid+definition</cell><cell>184</cell><cell>15</cell><cell>22</cell><cell>0</cell><cell>147</cell><cell>8%</cell></row><row><cell>list</cell><cell>16</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>14</cell><cell>0%</cell></row><row><cell>temporarily restricted</cell><cell>41</cell><cell>2</cell><cell>3</cell><cell>0</cell><cell>36</cell><cell>5%</cell></row><row><cell>unrestricted</cell><cell>159</cell><cell>13</cell><cell>20</cell><cell>1</cell><cell>125</cell><cell>8%</cell></row><row><cell>all</cell><cell>200</cell><cell>15</cell><cell>23</cell><cell>1</cell><cell>161</cell><cell>8%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,719.28,167.00,7.21"><p>URL: http://ilps.science.uva.nl/WikiXML</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,728.78,407.75,7.21;3,90.00,738.25,75.91,6.99"><p>The Dutch version of our QA demo can be accessed at http://cs-ilps.science.uva.nl:20500/. Uses Table Lookup stream only.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by various grants from the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs>. <rs type="funder">Valentin Jijkoun</rs> was supported under project numbers <rs type="grantNumber">220.80.001</rs>, <rs type="grantNumber">600.065.120</rs> and <rs type="grantNumber">612.000.106</rs>. <rs type="person">Joris van Rantwijk</rs> and <rs type="person">David Ahn</rs> were supported under project number <rs type="grantNumber">612.066.302</rs>. <rs type="person">Erik Tjong Kim Sang</rs> was supported under project number <rs type="grantNumber">264.70.050</rs>. Maarten de Rijke was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220.80.001</rs>, <rs type="grantNumber">264.70.050</rs>, <rs type="grantNumber">354.20.005</rs>, <rs type="grantNumber">600.065.120</rs>, <rs type="grantNumber">612.13.001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, and <rs type="grantNumber">640.-002.501</rs>. <rs type="person">Mahboob Alam Khalid</rs> and <rs type="person">Katja Hofmann</rs> were supported by <rs type="funder">NWO</rs> under project number <rs type="grantNumber">612.066.512</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TdXFnE7">
					<idno type="grant-number">220.80.001</idno>
				</org>
				<org type="funding" xml:id="_m6zG7KP">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_6aQBPKe">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_VuvGxKF">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_nebGknx">
					<idno type="grant-number">264.70.050</idno>
				</org>
				<org type="funding" xml:id="_BypSU8T">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_D4XNuKH">
					<idno type="grant-number">220.80.001</idno>
				</org>
				<org type="funding" xml:id="_8eUbmcz">
					<idno type="grant-number">264.70.050</idno>
				</org>
				<org type="funding" xml:id="_eH8PVeP">
					<idno type="grant-number">354.20.005</idno>
				</org>
				<org type="funding" xml:id="_4afXXVf">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_sxqa8nM">
					<idno type="grant-number">612.13.001</idno>
				</org>
				<org type="funding" xml:id="_PpTG6fb">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_PC4NfeW">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_THXhFwx">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_9Gp4ADW">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_pWzbSVp">
					<idno type="grant-number">640.-002.501</idno>
				</org>
				<org type="funding" xml:id="_a2Ec8KM">
					<idno type="grant-number">612.066.512</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,628.28,407.51,8.74;7,105.50,640.23,407.51,8.74;7,105.50,652.19,188.76,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,443.64,628.28,69.36,8.74;7,105.50,640.23,236.88,8.74">TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide</title>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakub</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antal</forename><surname>Ko Van Der Sloot</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bosch</surname></persName>
		</author>
		<idno>ILK-0402</idno>
		<ptr target="http://ilk.uvt.nl/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Tilburg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">ILK Technical Report</note>
</biblStruct>

<biblStruct coords="7,105.50,672.11,407.51,8.74;7,105.50,684.07,137.02,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,281.74,672.11,226.69,8.74">Building infrastructure for Dutch question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,117.95,684.07,93.52,8.74">Proceedings DIR-2003</title>
		<meeting>DIR-2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,703.99,407.50,8.74;7,105.50,715.95,407.51,8.74;7,105.50,727.90,249.48,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,286.91,703.99,226.08,8.74;7,105.50,715.95,77.18,8.74">Retrieving answers from frequently asked questions pages on the web</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,210.14,715.95,302.86,8.74;7,105.50,727.90,163.03,8.74">Proceedings of the Fourteenth ACM conference on Information and knowledge management (CIKM 2005)</title>
		<meeting>the Fourteenth ACM conference on Information and knowledge management (CIKM 2005)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,112.02,407.51,8.74;8,105.50,123.98,407.50,8.74;8,105.50,135.93,144.47,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,105.50,123.98,224.26,8.74">The University of Amsterdam at QA@CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joris</forename><surname>Van Rantwijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,355.87,123.98,157.13,8.74;8,105.50,135.93,40.84,8.74">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,155.86,407.50,8.74;8,105.50,167.81,407.51,8.74;8,105.50,179.77,169.98,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,307.74,155.86,205.26,8.74;8,105.50,167.81,54.14,8.74">Machine learning for question answering from tabular data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,187.06,167.81,325.94,8.74;8,105.50,179.77,139.41,8.74">FlexDBIST-07 Second International Workshop on Flexible Database and Information Systems Technology</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,199.69,407.51,8.74;8,105.50,211.65,64.62,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,193.78,199.69,144.39,8.74">At last parsing is now operational</title>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,358.27,199.69,113.29,8.74">Proceedings of TALN 2006</title>
		<meeting>TALN 2006<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
