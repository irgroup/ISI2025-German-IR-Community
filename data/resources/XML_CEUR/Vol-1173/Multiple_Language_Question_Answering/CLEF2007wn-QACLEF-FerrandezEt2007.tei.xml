<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.78,146.21,411.42,18.08">The contribution of the University of Alicante to AVE 2007</title>
				<funder>
					<orgName type="full">Spanish Generalitat Valenciana</orgName>
				</funder>
				<funder ref="#_kBQWDmX">
					<orgName type="full">European Union</orgName>
					<orgName type="abbreviated">EU</orgName>
				</funder>
				<funder ref="#_mHX5cjF">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder ref="#_pHcuTvT">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Qxj884d">
					<orgName type="full">Spanish Ministry of Education and Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.60,178.75,67.95,12.98"><forename type="first">Óscar</forename><surname>Ferrández</surname></persName>
						</author>
						<author>
							<persName coords="1,230.43,181.27,53.78,10.46"><forename type="first">Daniel</forename><surname>Micol</surname></persName>
							<email>dmicol@dlsi.ua.es</email>
						</author>
						<author>
							<persName coords="1,292.00,181.27,57.86,10.46"><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
							<email>rafael@dlsi.ua.es</email>
						</author>
						<author>
							<persName coords="1,377.93,181.27,71.86,10.46"><forename type="first">Manuel</forename><surname>Palomar</surname></persName>
							<email>mpalomar@dlsi.ua.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Languages</orgName>
								<orgName type="laboratory">Natural Language Processing and Information Systems Group</orgName>
								<orgName type="institution">Systems</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante San Vicente del</orgName>
								<address>
									<addrLine>Raspeig</addrLine>
									<postCode>03690</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.78,146.21,411.42,18.08">The contribution of the University of Alicante to AVE 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00EA83A89E5658C7A456EB9E8A51D910</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Semantic Similarity</term>
					<term>Experimentation</term>
					<term>Measurement</term>
					<term>Performance Question Answering</term>
					<term>Answer Validation</term>
					<term>Recognizing Textual Entailment</term>
					<term>Lexical Similarity</term>
					<term>Syntactic Trees</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we discuss a system used to recognize entailment relations within the AVE framework. This system creates representations of text snippets by means of a variety of lexical measures and syntactic structures. Once these representations have been created, we compare the corresponding to the text and to the hypothesis and we try to determine if there is an entailment relation between the text and the hypothesis. The hypotheses have been generated by merging the answers with their corresponding questions, applying a set of regular expression aimed at this issue. In the performed experiments our system obtained a maximum F-measure score of 0.40 and 0.39 for the development and test English corpora, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Answer Validation Exercise (AVE) is a two-year-old track within the Cross-Language Evaluation Forum (CLEF) 2007. AVE provides an evaluation framework for answer validations in Question Answering (QA) systems. This automatic answer validation would be useful for improving the performance of QA systems, helping humans in the assessment of QA systems output and developing better criteria for collaborative ones.</p><p>Systems must emulate human assessment of QA responses and decide if an answer to a question is correct or not according to a given text. This year, the participants receive a set of triplets (Question, Answer, Supporting Text) and they must return a boolean value for each triplet showing whether the answer is supported by the text. This shows that the AVE task is very related to the recognition of textual entailments, since it can be considered as a kind of such relations.</p><p>With our participation, we want to evaluate our system within the very realistic environment that AVE provides. In addition, AVE boots the direct applicability of our system in the field of QA, which is very appealing. Our system is designed to recognize textual entailment relations. In fact, we have participated in ACL-PASCAL Third Recognising Textual Entailment (RTE) Challenge <ref type="bibr" coords="2,90.01,134.45,10.52,10.46" target="#b2">[3]</ref> this year <ref type="bibr" coords="2,146.70,134.45,9.96,10.46" target="#b1">[2]</ref>. To apply our system to the AVE competition we had to do some adjustments that will be explained in detail later.</p><p>The remainder of this paper is structured as follows. The following section presents our approach for our participation in AVE. Third section illustrates the experiments carried out and the results obtained. Finally, fourth section shows the conclusions and proposes future work based on our actual research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The AVE Approach</head><p>The proposed approach attempts to detect when the text, which could be consider as a passage returned by a QA system, entails or implies the answer given and, if this occurs, the answer is then justified. To determine if this relation appears, our approach will detect lexical and syntactic implications between two text snippets (the text or the passage and the hypothesis that will be created by both the question and the answer). We propose several methods that mainly rely on lexical and syntactic inferences in order to address the recognition task. Next subsections summarize the procedure followed to apply our approach to AVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpora Processing</head><p>The corpora provided by the AVE organizers has the following format: &lt;q id=1 lang=EN&gt; &lt;q_str&gt;Who was Yasser Arafat?&lt;/q_str&gt; &lt;a id=1 value=XXX&gt; &lt;a_str&gt;Palestine Liberation Organization Chairman&lt;/a_str&gt; &lt;t_str doc=XXX&gt;President Clinton appealed ... &lt;/t_str&gt; &lt;/a&gt; &lt;a id=2 value=XXX&gt; ... &lt;/a&gt; .... &lt;/q&gt; where each question (tag q) contains a string (q_str), which is the question formulated in natural language. In addition, q can have one or more answers and each answer (a_str) is associated with a text (t_str) that will be required to determine if the answer is entailed to the question.</p><p>Since our system is designed to determine implications between two text snippets, the best way to adapt the AVE corpus to our system seems to be the following: for each answer and question, convert them into an affirmative sentence and detect if there is entailment with its associated text.</p><p>Therefore, we generated a set of regular expressions to manage these situations. Table <ref type="table" coords="2,479.82,582.20,4.98,10.46" target="#tab_0">1</ref> shows such regular expressions together with the number and percentage of solved question-answer pairs. AVE organizers provide a set of patterns intended for this purpose, although we used our own due to when these patterns were published we have already adapted our system to the aforementioned regular expressions.</p><p>Our system was applied to the English corpora from AVE. These corpora contain 1121 questionanswer pairs for the development corpus and 202 for the test one.</p><p>Finally, to complete the explanation of the corpora processing, we would like to mention what occurs when a pair does not match any of the generated regular expressions. We propose two solutions: in the first one, called automatic, the tokens of the answer are linked together with the tokens of the corresponding question, while for the second solution, called semi-automatic, we have done a review of these pairs manually creating the affirmative sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regular expression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Core of the System</head><p>The core of our system is composed of two modules, each of which attempts to recognize the textual entailment relation from different perspectives, which are the lexical one and the syntactic. In this section we will describe both of them in little detail. For further information, please refer to <ref type="bibr" coords="3,502.49,317.70,10.52,10.46" target="#b0">[1]</ref> and <ref type="bibr" coords="3,109.36,329.67,9.96,10.46" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Lexical module</head><p>The performance of this method relies on the computation of a wide variety of lexical measures, which basically consists of overlap metrics. Some researchers have already used this kind of metrics <ref type="bibr" coords="3,90.01,397.86,9.96,10.46" target="#b6">[7]</ref>. However, our approach does not use any semantic knowledge.</p><p>Prior to the calculation of the measures, all texts and the hypotheses created merging the question-answer pairs by means of regular expressions are tokenized and lemmatized. Later on, a morphological analysis is performed as well as a stemmization. Once these steps are completed, we create several data structures that contain the tokens, stems, lemmas, functional<ref type="foot" coords="3,460.00,444.61,3.97,7.32" target="#foot_0">1</ref> words and the most relevant<ref type="foot" coords="3,168.96,456.56,3.97,7.32" target="#foot_1">2</ref> ones corresponding to the text and the hypothesis. The lexical measures will be applied over these structures and this will allow us to know which of them are more suitable to recognize entailment relations. The followings paragraphs describe the lexical measures implemented in our system.</p><p>• Simple matching: word overlap between text and hypothesis is initialized to zero. If a word in the hypothesis appears also in the text, an increment of one unit is added. The final weight is normalized dividing it by the length of the hypothesis.</p><p>• Levenshtein distance: it is similar to simple matching. However, in this case we use the mentioned distance as the similarity measure between words. When the distance is zero, the increment value is one. On the other hand, if such value is equal to one, the increment is 0.9. Otherwise, it will be the inverse of the obtained distance.</p><p>• Consecutive subsequence matching: this measure assigns the highest relevance to the appearance of consecutive subsequences. In order to perform this, we have generated all possible sets of consecutive subsequences, from length two until the length in words, from the text and the hypothesis. If we proceed as mentioned, the sets of length two extracted from the hypothesis will be compared to the sets of the same length from the text. If the same element is present in both the text and the hypothesis set, then a unit is added to the accumulated weight. This procedure is applied to all sets of different length extracted from the hypothesis. Finally, the sum of the weight obtained from each set of a specific length is normalized by the number of sets corresponding to this length, and the final accumulated weight is also normalized by the length of the hypothesis in words minus one. One should note that this measure does not consider non-consecutive subsequences. In addition, it assigns the same relevance to all consecutive subsequences with the same length. Furthermore, the more length the subsequence has, the more relevant it will be considered.</p><p>• Tri-grams: two sets containing tri-grams of letters belonging to the text and the hypothesis were created. All the occurrences in the hypothesis' tri-grams set that also appear in the text's will increase the accumulated weight in a factor of one unit. The calculated weight is then normalized dividing it by the total number of tri-grams within the hypothesis.</p><p>• ROUGE measures: ROUGE measures have already been tested for automatic evaluation of summaries and machine translation <ref type="bibr" coords="4,287.69,231.41,9.96,10.46" target="#b3">[4]</ref>. For this reason, and considering the impact of n-gram overlap metrics in textual entailment, we believe that the idea of integrating these measures<ref type="foot" coords="4,154.34,254.25,3.97,7.32" target="#foot_2">3</ref> in our system is very appealing. We have implemented these measures as defined in <ref type="bibr" coords="4,126.52,267.27,9.96,10.46" target="#b3">[4]</ref>.</p><p>In order to detect entailment relations, several machine learning classifiers were considered, being the Bayesian Network the best for our needs. We have used the Bayesian Network implementation from Weka <ref type="bibr" coords="4,188.70,309.77,9.96,10.46" target="#b8">[9]</ref>, considering each lexical measure as a feature for the training and test stages of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Syntactic module</head><p>This module aims to provide a good accuracy rate by using few syntactic modules that behave collaboratively. These include tree construction, filtering and graph node matching.</p><p>• Tree generation: the first module constructs the corresponding syntactic dependency trees.</p><p>For this purpose, MINIPAR <ref type="bibr" coords="4,239.71,407.93,10.52,10.46" target="#b4">[5]</ref> output is generated and afterwards parsed for each text and hypothesis of our corpus. Phrase tokens, along with their grammatical information, are stored in an on-memory data structure that represents a tree, which is equivalent to the mentioned syntactic dependency tree.</p><p>• Tree filtering: once the tree has been constructed, we may want to discard irrelevant data in order to reduce our system's response time and noise. For this purpose we have generated a database of relevant grammatical categories (see Table <ref type="table" coords="4,359.78,486.30,4.43,10.46" target="#tab_1">2</ref>) that will allow us to remove from the tree all those tokens whose category does not belong to such list. The resulting tree will have the same structure as the original, but will not contain any stop words nor irrelevant tokens, such as determinants or auxiliary verbs.</p><p>• Graph node matching: in this stage we proceed to perform a graph node matching process, termed alignment, between both the text and the hypothesis <ref type="foot" coords="4,385.17,551.64,3.97,7.32" target="#foot_3">4</ref> . This operation consists in finding pairs of tokens in both trees whose lemmas are identical, no matter whether they are in the same position within the tree. Some authors have already designed similar matching techniques, such as the ones described in <ref type="bibr" coords="4,299.99,588.58,9.96,10.46" target="#b7">[8]</ref>. However, these include semantic constraints that we have decided not to consider. The reason of this decision is that we desired to overcome the recognition task from an exclusively syntactic perspective.</p><p>Let τ and λ represent the text's and hypothesis' syntactic dependency trees, respectively. We assume we have found a word, namely β, present in both τ and λ. Now let γ be the weight assigned to β's grammatical category (Table <ref type="table" coords="4,308.01,651.68,3.87,10.46" target="#tab_1">2</ref>), σ the weight of β's grammatical relationship (Table <ref type="table" coords="4,146.81,663.63,3.87,10.46" target="#tab_2">3</ref>), µ an empirically calculated value that represents the weight difference between tree levels, and δ β the depth of the node that contains the word β in λ. We define the function φ(β) = γ • σ • µ -δ β as the one that calculates the relevance of a word in our system. The experiments performed reveal that the optimal value for µ is 1.1. For a given pair (τ , λ), we define the set ξ as the one that contains all words present in both trees, being ξ = τ ∩λ ∀α ∈ τ, β ∈ λ. Therefore, the similarity rate between τ and λ, denoted by the symbol ψ, would be ψ(τ, λ) = ν∈ξ φ(ν). One should note that a requirement of our system's similarity measure would be to be independent of the hypothesis length. Thus, we must define the normalized similarity rate, as ψ(τ, λ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammatical category</head><formula xml:id="formula_0" coords="5,366.13,420.46,53.51,23.29">= ν∈ξ φ(ν) β∈λ φ(β)</formula><p>. Once the similarity value has been calculated, it will be provided to the user together with the corresponding text-hypothesis pair identifier. It will be his responsibility to choose an appropriate threshold that will represent the minimum similarity rate to be considered as entailment between text and hypothesis. All values that are under such a threshold will be marked as not entailed. The development corpus will help us to establish this threshold properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In AVE, all pairs must be tagged with one of the following values:</p><p>• VALIDATED indicates that the answer is correct and supported although not the one selected.</p><p>• SELECTED indicates that the answer is VALIDATED and it is the one chosen as the output of an hypothetical QA system. One of the VALIDATED answers per question should be marked as SELECTED.</p><p>• REJECTED indicates that the answer is incorrect or there is not enough evidence of its correctness.</p><p>Since our system returns a numeric value to determine the entailment, we decided to mark as SELECTED the pair with the highest true entailment score among all pairs that belong to the same question. If it is the case that two or more pairs have the highest score, then one of them is randomly chosen.</p><p>Regarding the framework that the AVE organizers propose to evaluate the systems, apart from the well-known measures of Precision, Recall and F over the YES pairs 5 , we would like to point out a new measure, called Q-A accuracy. This measure only considers the accuracy obtained from correct SELECTED values and attempts to simulate the decision that could be made by a QA system. However, for our system it is quite difficult to establish one of the VALIDATED values as a SELECTED since differences between true entailment scores are usually minimal. This happens due to the fact that no semantic knowledge is considered. Therefore, although the system is able to determine lexical and syntactic implications, in the case of SELECTED values this does not seem to be enough.</p><p>Table <ref type="table" coords="6,132.76,194.22,4.98,10.46" target="#tab_3">4</ref> shows the different experiments carried out and the results obtained for our system. The proposed baseline was generated setting all pairs as VALIDATED, which was useful to evaluate the gain of the remainder experiments. Two main experiments were carried out for our participation in AVE. The first one applies the lexical module to detect VALIDATED and SELECTED pairs, whereas the second one only uses syntactic information (obtained by the syntactic module) to solve implications. These runs were named lex and syn respectively. A simple combination of both modules, for instance to decide the judgment depending on the accuracy of each one for true and false implications, does not improve the results. Therefore we believe that subsequent work could be the combination of these modules in a collaborative way rather than by means of other simpler techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Moreover, each run (lex or syn) was processed with the two types of corpus, automatic and semi-automatic, created from the original AVE corpora (see section 2.1). Table <ref type="table" coords="6,449.34,494.64,4.98,10.46" target="#tab_3">4</ref> reveals that, although the semi-automatic experiments obtain better results, the effort needed to generate this corpus is not worth in comparison with the gain of accuracy obtained.</p><p>The approach that achieved better results is lex. This is due to the fact that there are some cases where the hypothesis' construction does not make sense and consequently the syntactic tree is incorrectly generated. These situations occur when the answer has a grammatical category inconsistent with the one expected by the question (for instance, if the answer is a quantity or date when the question asks for a person name).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>This paper presents two independent approaches considering mainly lexical and syntactic information. Throughout this paper we expose and analyze a wide variety of lexical measures as well as syntactic structure comparisons that attempt to recognize the textual implications required for the AVE task.</p><p>The approach that obtained the best results was the lexical one, being the optimal for our participation, and obtaining an F-measure score of 0.40 and 0.39 for the development and test corpus, respectively. However, we would like to point out that the results obtained in challenges or competitions about recognizing entailment relations depend on the idiosyncrasies of the corpora used. For instance, whereas AVE generates its corpora directly from the output of several QA systems, the RTE challenge constructs the corpora by means of a review process of several anno-tators and from different sources (see RTE-3 overview <ref type="bibr" coords="7,330.65,110.53,10.52,10.46" target="#b2">[3]</ref> and our participation in this challenge <ref type="bibr" coords="7,90.00,122.49,10.30,10.46" target="#b1">[2]</ref>).</p><p>Future work can be related to the development of a semantic module. This module will be able to construct characterized representations based on the text using named entities and role labeling in order to extract semantic information from the pair of text snippets. In addition, once the semantic module was implemented, subsequent work will be to combine these modules in an efficient way. Each module should perform the recognition individually as well as support it together with the rest of the modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,95.61,111.34,411.79,142.71"><head>Table 1 :</head><label>1</label><figDesc>Regular expressions used to convert questions and answers into affirmative sentences.</figDesc><table coords="3,128.87,111.34,345.26,108.89"><row><cell></cell><cell cols="2">Number of Q-A pairs solved (%)</cell></row><row><cell></cell><cell cols="2">English development corpus English test corpus</cell></row><row><cell>(What) (\S+) (.+)</cell><cell>350 (31.22%)</cell><cell>92 (45.54%)</cell></row><row><cell>(Which) (\S+) (\S+) (.+)</cell><cell>165 (14.72%)</cell><cell>10 (4.95%)</cell></row><row><cell>(Who) (\S+) (.+)</cell><cell>179 (15.97%)</cell><cell>36 (17.82%)</cell></row><row><cell>(Where) (\S+) (.+)</cell><cell>76 (6.78%)</cell><cell>8 (3.96%)</cell></row><row><cell>(How many) (\S+) (.+)</cell><cell>96 (8.56%)</cell><cell>14 (6.93%)</cell></row><row><cell>(How much) (.+)</cell><cell>12 (1.07%)</cell><cell>0 (0.0%)</cell></row><row><cell>Total</cell><cell>878 (78.32%)</cell><cell>160 (79.21%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,157.99,108.94,287.01,213.15"><head>Table 2 :</head><label>2</label><figDesc>Weights assigned to the relevant grammatical categories.</figDesc><table coords="5,372.90,108.94,35.56,10.46"><row><cell>Weight</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,170.72,343.05,261.57,10.46"><head>Table 3 :</head><label>3</label><figDesc>Weights assigned to the grammatical relationships.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,101.27,242.78,400.46,144.71"><head>Table 4 :</head><label>4</label><figDesc>RunPrec. YES Rec. YES F-measure Q-A acc. Results obtained for the AVE 2007 track.</figDesc><table coords="6,101.27,257.14,387.39,96.53"><row><cell></cell><cell>baseline</cell><cell>0.12</cell><cell>1.0</cell><cell>0.21</cell><cell>-</cell></row><row><cell></cell><cell>lex automatic</cell><cell>0.26</cell><cell>0.78</cell><cell>0.39</cell><cell>-</cell></row><row><cell>Development</cell><cell>lex semi-automatic</cell><cell>0.27</cell><cell>0.78</cell><cell>0.40</cell><cell>-</cell></row><row><cell></cell><cell>syn automatic</cell><cell>0.31</cell><cell>0.03</cell><cell>0.06</cell><cell>-</cell></row><row><cell></cell><cell>syn semi-automatic</cell><cell>0.17</cell><cell>0.17</cell><cell>0.17</cell><cell>-</cell></row><row><cell></cell><cell>baseline</cell><cell>0.11</cell><cell>1.0</cell><cell>0.19</cell><cell>-</cell></row><row><cell>Test</cell><cell>lex semi-automatic</cell><cell>0.25</cell><cell>0.81</cell><cell>0.39</cell><cell>0.18</cell></row><row><cell></cell><cell>syn semi-automatic</cell><cell>0.18</cell><cell>0.81</cell><cell>0.29</cell><cell>0.19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,729.95,366.58,8.37"><p>As functional words we consider nouns, verbs, adjectives, adverbs and figures (number, dates, etc).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,739.45,127.93,8.37"><p>Considering only nouns and verbs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,105.24,717.42,407.86,8.37;4,90.00,726.89,32.99,8.37"><p>The considered measures were ROUGE-N with n=2 and n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,105.24,736.39,407.87,8.37;4,90.00,745.85,103.54,8.37"><p>One should remember that the hypothesis has been created from the pair question-answer by means of regular expressions (see section 2.1)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research has been partially funded by the <rs type="programName">QALL-ME</rs> consortium, which is a <rs type="programName">6 th Framework Research Programme of the</rs> <rs type="funder">European Union (EU)</rs>, contract number <rs type="grantNumber">FP6-IST-033860</rs> and by the <rs type="funder">Spanish Government</rs> under the project <rs type="projectName">CICyT</rs> number <rs type="grantNumber">TIN2006-1526-C06-01</rs>. It has also been supported by the undergraduate research fellowships financed by the <rs type="funder">Spanish Ministry of Education and Science</rs>, and the project <rs type="grantNumber">ACOM06/90</rs> financed by the <rs type="funder">Spanish Generalitat Valenciana</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pHcuTvT">
					<orgName type="program" subtype="full">QALL-ME</orgName>
				</org>
				<org type="funding" xml:id="_kBQWDmX">
					<idno type="grant-number">FP6-IST-033860</idno>
					<orgName type="program" subtype="full">6 th Framework Research Programme of the</orgName>
				</org>
				<org type="funded-project" xml:id="_mHX5cjF">
					<idno type="grant-number">TIN2006-1526-C06-01</idno>
					<orgName type="project" subtype="full">CICyT</orgName>
				</org>
				<org type="funding" xml:id="_Qxj884d">
					<idno type="grant-number">ACOM06/90</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,106.88,349.06,406.13,12.98;7,105.50,363.54,407.50,10.46;7,105.50,375.49,407.50,10.46;7,105.50,387.44,65.37,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,426.72,351.58,86.28,10.46;7,105.50,363.54,220.04,10.46">DLSITE-1: Lexical analysis for solving textual entailment recognition</title>
		<author>
			<persName coords=""><forename type="first">Óscar</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,349.66,363.54,163.35,10.46;7,105.50,375.49,315.50,10.46">Proceedings of the 12th International Conference on Applications of Natural Language to Information Systems</title>
		<meeting>the 12th International Conference on Applications of Natural Language to Information Systems<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.88,404.85,406.14,12.98;7,105.50,419.32,407.51,10.46;7,105.50,431.27,407.50,10.46;7,105.50,443.24,236.59,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,424.40,407.37,88.62,10.46;7,105.50,419.32,229.94,10.46">A perspective-based approach for solving textual entailment recognition</title>
		<author>
			<persName coords=""><forename type="first">Óscar</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,363.10,419.32,149.91,10.46;7,105.50,431.27,227.83,10.46">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,463.16,407.51,10.46;7,105.50,475.12,407.51,10.46;7,105.50,487.07,407.50,10.46;7,105.50,499.02,118.14,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,410.35,463.16,102.65,10.46;7,105.50,475.12,149.09,10.46">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,274.25,475.12,238.76,10.46;7,105.50,487.07,126.02,10.46">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,518.95,407.50,10.46;7,105.50,530.90,407.51,10.46;7,105.50,542.86,407.50,10.46;7,105.50,554.82,22.69,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,177.52,518.95,271.56,10.46">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,287.24,530.90,225.77,10.46;7,105.50,542.86,243.67,10.46">Text Summarization Branches Out: Proceedings of the Association for Computational Linguistics Workshop</title>
		<editor>
			<persName><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,574.74,407.50,10.46;7,105.50,586.70,174.60,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,163.81,574.74,188.55,10.46">Dependency-based Evaluation of MINIPAR</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,376.30,574.74,136.70,10.46;7,105.50,586.70,69.64,10.46">Workshop on the Evaluation of Parsing Systems</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,604.10,407.52,12.98;7,105.50,618.58,407.50,10.46;7,105.50,630.53,407.50,10.46;7,105.50,642.48,400.39,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,419.27,606.62,93.75,10.46;7,105.50,618.58,335.02,10.46">DLSITE-2: Semantic similarity based on syntactic dependency trees applied to textual entailment</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Óscar</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,463.34,618.58,49.66,10.46;7,105.50,630.53,130.56,10.46">Proceedings of the TextGraphs-2 Workshop</title>
		<meeting>the TextGraphs-2 Workshop<address><addrLine>Rochester, New York, United States of America</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
	<note>The North American Chapter of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="7,105.50,662.41,407.51,10.46;7,105.50,674.37,407.51,10.46;7,105.50,686.33,407.51,10.46;7,105.50,698.28,74.19,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,373.70,662.41,139.31,10.46;7,105.50,674.37,278.69,10.46">Detecting Entailment Using an Extended Implementation of the Basic Elements Overlap Metrics</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timonthy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,403.76,674.37,109.25,10.46;7,105.50,686.33,296.17,10.46">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,718.20,407.50,10.46;7,105.50,730.16,407.50,10.46;7,105.50,742.11,341.28,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,325.20,718.20,187.80,10.46;7,105.50,730.16,44.28,10.46">Effectively using syntax for recognizing false entailment</title>
		<author>
			<persName coords=""><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.44,730.16,336.42,10.46">Proceedings of the North American Association of Computational Linguistics</title>
		<meeting>the North American Association of Computational Linguistics<address><addrLine>New York City, New York, United States of America</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,110.53,407.50,10.46;8,105.50,122.49,235.07,10.46" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,242.93,110.53,265.79,10.46">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
