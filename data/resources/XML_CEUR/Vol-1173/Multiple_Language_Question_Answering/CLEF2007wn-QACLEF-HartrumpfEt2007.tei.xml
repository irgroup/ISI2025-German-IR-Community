<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.45,148.79,302.10,15.48;1,148.06,170.71,306.88,15.48;1,239.67,192.62,123.66,15.48">University of Hagen at QA@CLEF 2007: Coreference Resolution for Questions and Answer Merging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.63,226.91,62.35,8.64"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.93,226.91,55.56,8.64"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.34,226.91,74.03,8.64"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS</orgName>
								<orgName type="institution">University of Hagen (FernUniversität in Hagen</orgName>
								<address>
									<postCode>58084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.45,148.79,302.10,15.48;1,148.06,170.71,306.88,15.48;1,239.67,192.62,123.66,15.48">University of Hagen at QA@CLEF 2007: Coreference Resolution for Questions and Answer Merging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4402999E29CEA7F87CDEBC48AB82CA09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Linguistic processing H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Search process H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Semantic networks I.2.7 [Artificial Intelligence]: Natural Language Processing-Language parsing and understanding Experimentation</term>
					<term>Measurement</term>
					<term>Performance Question answering</term>
					<term>Deep semantic processing of questions and documents</term>
					<term>Follow-up questions</term>
					<term>Coreference resolution</term>
					<term>Answer merging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The German question answering (QA) system InSicht participated in QA@CLEF for the fourth time. InSicht realizes a deep QA approach: it builds on full sentence parses, rulebased inferences on semantic representations, and matching semantic representations derived from questions and document sentences. InSicht was improved for QA@CLEF 2007 in the following main areas: questions containing pronominal or nominal anaphors are treated by a coreference resolver; the shallow QA methods are improved; and finally, our system for the CLEF Answer Validation Exercise is employed for answer merging. Results showed a performance drop compared to last year mainly due to unstable and incomplete handling of the newly added Wikipedia corpus. However, dialog treatment by coreference resolution delivered very accurate results so that follow-up questions can be handled similar to isolated questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ferent data streams with answer candidates (answer streams), logical answer validation (MAVE), and natural language generation. Three different approaches to create answer candidates are employed. These approaches are applied to the two corpora for QA@CLEF 2007, namely CLEF-NEWS and Wikipedia, actually resulting in six different streams of answer candidates, which are merged in the answer validation phase.</p><p>The first answer producer is InSicht <ref type="bibr" coords="2,251.71,172.12,126.96,8.64" target="#b6">(Hartrumpf and Leveling, 2006)</ref>, a precision-oriented QA system using a semantic network representation of questions and documents (see Sect. 2). It realizes a deep (semantic) QA approach because it tries to employ deep methods in many natural language processing (NLP) areas: it builds on full sentence parses for documents and questions, rule-based inferences on semantic representations, matching semantic representations derived from questions and documents, and natural language generation of answers from semantic representations of documents. Specialized NLP modules resolve temporal deixis and coreferences.</p><p>The other two answer producers are QAP (Question Answering by Pattern matching) and MIRA (Modified Information Retrieval (IR) Approach for question answering), see Sect. 3. They employ shallow NLP methods and aim at a high recall to provide a fallback strategy for InSicht. These shallow approaches rely on preprocessed document corpora with sentence boundaries detected. Text segments are transformed into XML and indexed in a database management system supporting the tf-idf IR model. Both the QAP and the MIRA module rely on the WOCADI parser for resolving ellipsis and anaphoric references in questions.</p><p>2 Changes of InSicht for QA@CLEF 2007</p><p>The QA@CLEF task was considerably changed in 2007: The document collection was increased in size and diversity by adding the Wikipedia documents from 2006-11-30 (for German, the number of sentences grew from 5.0 million sentences to 16.5 million sentences); and follow-up questions were included in the test set. These two issues are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Wikipedia</head><p>For document processing in InSicht, all documents are parsed by WOCADI and intratextual coreferences are resolved by CORUDIS (COreference resolution by RUles and DIsambigation Statistics, <ref type="bibr" coords="2,470.95,466.49,42.05,8.64;2,90.00,478.44,21.44,8.64" target="#b3">Hartrumpf (2001</ref><ref type="bibr" coords="2,118.31,478.44,20.75,8.64" target="#b4">Hartrumpf ( , 2003))</ref>). Because the time between guideline release and test set release was too short to process all Wikipedia documents in the normal way, we had to rely on an older parse of the <ref type="bibr" coords="2,419.91,490.40,93.09,8.64;2,90.00,502.35,24.90,8.64">Wikipedia (from 2006-09-25)</ref>. Although this Wikipedia version is only 2 months older than the recommended snapshot, it turned out to be considerably different in many articles (e.g. 6,813 articles disappeared by renaming, merging, or complete removal, while 38,478 articles were added); this is just another witness of the very dynamic nature of Wikipedia. To save time, coreferences were not resolved.</p><p>The German Wikipedia is 2.3 times larger than the traditional German QA@CLEF collection, CLEF-NEWS. In general, InSicht was able to deal with the size increase, but unfortunately the quite unrestricted form of article names (and in InSicht's context, file names) led to an inconsistent concept index that rendered many Wikipedia articles inaccessible to InSicht. So, InSicht's answers came too rarely from Wikipedia, which seems to be the main reason for the performance drop. Aggravating this situation, most test set questions seem to target the Wikipedia subcollection only. Fortunately, the performance drop in InSicht was in part compensated by the improved shallow QA subsystems (see Sect. 3) and the newly integrated answer validator MAVE (see Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dialog Treatment</head><p>In the years before 2007, all questions could be answered in isolation without any reference to the context, like previous questions or answers. The guidelines for QA@CLEF 2007 removed this restriction by allowing coreferences to the topic expressed in the first question/answer pair. In the test set, one question (165) is even more unrestricted: it contains a pronominal anaphor that corefers with an antecedent from the second question of its topic context.</p><p>To treat such context-dependent questions, the basic idea was to keep a dialog history containing semantic representations of questions and answers. The dialog history is initialized (i.e. deleted), if the start of a new topic is encountered in the test set. (We have not tried to detect topic boundaries automatically, yet.) On these semantic representations in the dialog history, coreferences are resolved by the general coreference resolver CORUDIS. This module has already been used successfully for coreference resolution on documents since QA@CLEF-2005 <ref type="bibr" coords="3,232.28,172.12,71.80,8.64" target="#b5">(Hartrumpf, 2006)</ref>.</p><p>CORUDIS is a hybrid coreference resolver because it contains both symbolic, linguistically motivated coreference rules that license possible coreferences and a statistical multi-dimensional back-off model derived from a manually annotated corpus for selecting among licensed alternatives. In addition, CORUDIS employs a whole range of bonus factors like syntactic parallelism, semantic parallelism, and maximality of noun phrases. The back-off model for CORUDIS, which was derived from annotated newspaper articles (from the Süddeutsche Zeitung), was taken without any modifications. First, we considered retraining on a dialog corpus with anaphors in questions, which would have been more similar to the application type in QA@CLEF 2007. But to save time, we kept the old model and only added some positive (negative) scores for specific coreference rules that are more (less) likely to be correct in question sequences.</p><p>Table <ref type="table" coords="3,129.72,291.67,4.98,8.64">1</ref> contains all 29 questions from the 200 German questions (of QA@CLEF 2007) where coreference resolution is required to find an answer. So, only 34.5% of all 84 follow-up questions for the 47 topics with more than one question require coreference resolution. The second column in the table shows which dependency types occur in the test set; q 1 (a 1 ) stands for the first question (answer) of a topic. Only two questions (046, 107) contain an anaphor that corefers with an answer; therefore, an answer should only be an alternative to an antecedent candidate from a preceding question. As no answer could be found for the corresponding topic-first questions (045, 105) it did not matter whether one includes or excludes the semantic representation of the first answer from subsequent coreference resolutions for the same topic.</p><p>The pronominal anaphor sie in question 165 corefers with Hanamachi from the second question of the topic 163-165). To handle such references to non-first questions, we adapted the dialog processing as follows: a subsequent question is deleted from the dialog history only if it contains an anaphor which was successfully resolved.</p><p>The answer producers used only representations where coreferences had been resolved. For the deep producer InSicht, it suffices to integrate the antecedents (and remove the anaphors) on the semantic network level. For the shallow producers to profit from coreference resolution most easily, the integration is performed on the surface level. This leads to question formulations that can be answered without any further context. The original questions and the automatically revised questions can be seen in the first column of Table <ref type="table" coords="3,125.01,494.90,3.74,8.64">1</ref>. The average size of the resulting semantic networks (measured by the number of relations after coreference resolution) was 11.39; this shows that QA@CLEF's questions stayed astonishingly stable in terms of semantic size (and approximately specificity) over the last 5 years: 11.30 <ref type="bibr" coords="3,426.25,518.82,86.75,8.64;3,90.00,530.77,100.32,8.64">(2006), 11.30 (2005), 9,73 (2004), 10.98 (2003)</ref>. But the question difficulty increased because of added phenomena like temporal restrictions, temporal deixis, and anaphors in questions.</p><p>Related work for coreference resolution on the question side and on the document side is presented in an overview by <ref type="bibr" coords="3,142.10,566.64,115.71,8.64" target="#b12">Vicedo and Ferrández (2006)</ref>. Some systems for English that employ coreference resolution on questions in the Context Task of TREC-10 are described by <ref type="bibr" coords="3,354.45,578.59,94.31,8.64">Harabagiu et al. (2001)</ref>; <ref type="bibr" coords="3,456.54,578.59,56.46,8.64;3,90.00,590.55,25.14,8.64" target="#b10">Lin and Chen (2001)</ref>; <ref type="bibr" coords="3,122.10,590.55,62.25,8.64" target="#b11">Oh et al. (2001)</ref>. All three approaches handle coreferences in order to add the keywords from the question (or answer) containing the antecedent for an anaphor in the current question. In our approach, we try to construct a question that can be answered as a question without context. Elliptical questions, although not occurring in this year's test set, were implemented because they are frequent and central for QA systems with dialog handling. A simple heuristic was applied to detect an elliptical question: if no verb is contained in the parse of the question and the question is short, the question will be treated as elliptical. Ellipsis resolution simply replaces the question focus of the previous question by the question focus of the current question, e.g. the elliptical question Wo? (Where?) after the question Seit wann X? (Since when X?) becomes Wo X? (Where X?).</p><p>Table 1: Analysis of coreferences in the German questions of QA@CLEF 2007. The question in the first column contains in parentheses the correct antecedent for an anaphor. The English translations are from the English-German test set. The parenthesized item in the second column (dependency type) indicates that this resolution alternative is less likely to lead to an answer. The resolution result of CORUDIS in the third column can be right (R; 26 cases), missing (M; 3 cases), or wrong (W; 0 cases). QAP (Question Answering by Pattern matching, see <ref type="bibr" coords="5,301.38,134.61,63.21,8.64" target="#b8">Leveling (2006)</ref>) employs pattern matching on a persentence basis. For this approach, about 30 classes of questions were defined, based on an analysis of the QA@CLEF questions from 2003 to 2006. These classes correspond to relational triples of the form &lt;relname&gt;(&lt;keyword&gt;, &lt;answer&gt;), where relname is the name of a relation, keyword is a term describing the question topic, and answer is a string representing the answer. QAP returns answers exactly as they occur in the document. Several large resources were utilized to create question-answer pairs for training this method, including the PND data as used in the German Wikipedia (PND -Personennamendatei, see <ref type="bibr" coords="5,90.00,218.29,103.68,8.64" target="#b7">Hengel and Pfeifer (2005)</ref>). An entry in the PND data contains information about a famous person such as his/her place of birth (relation born in), date of birth (born on), place of death (died in), date of death (died on), aliases (has pseudonym), and profession (has role). This data can be transformed to represent question-answer pairs. In addition to explicit information, additional question-answer pairs can be derived, e.g. the age at death (died at age) can be computed from the date of birth and the date of death.</p><p>QA in QAP consists of determining the question type (relname), extracting the main keywords from the question, and retrieving a set of document sentences containing answer candidates. The patterns corresponding to the relation identified are applied to retrieved sentences. Pattern matching instantiates the answer variables in the patterns. Their values are returned as answer candidates and sent to the MAVE module for validation. QAP was introduced in QA@CLEF 2006, while the following approach (MIRA) is new in QA@CLEF 2007.</p><p>MIRA (Modified Information Retrieval Approach for QA, see <ref type="bibr" coords="5,364.27,349.80,64.29,8.64" target="#b9">Leveling (2007)</ref>) is a recall-oriented approach to QA based on information retrieval combined with the selection of the most frequent word sequence of the expected answer type. Question processing in MIRA consists of the following steps: The natural language question is tokenized and stopwords are eliminated to identify the keywords. A naïve Bayesian classifier is applied on shallow features of the question such as the first four word forms. The classifier returns a ranking of expected answer types, of which the most probable type is selected. The classes for expected answer type include DEFINITION, LOCATION, MEASURE, ORGANIZATION, PERSON, SUBSTANCE, and TIME. An IR query is created utilizing all morphologic variants of keywords and submitted to the database system. The top 250 documents (sentences) are retrieved and tokenized. The tokens are categorized according to the expected answer types, i.e. named entities are tagged with LOCATION, ORGANIZATION, or PERSON; temporal expressions (dates) are annotated with TIME, and numeric expressions followed by a unit are associated with MEASURE. Answer candidates are then selected by choosing the most frequent word sequences tagged with the expected answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Merging Answer Streams by Validation</head><p>The answer producers delivered six answer streams that had to be merged by a new component, the answer validator MAVE (Multinet-based Answer VErification). The first MAVE prototype <ref type="bibr" coords="5,444.38,561.11,68.62,8.64" target="#b0">(Glöckner, 2006)</ref> originated from the Answer Validation Exercise (AVE) at CLEF 2006 and was later extended to handle not only the basic validation task (i.e. checking the correctness of an answer with respect to a supporting text passage), but answer selection as well <ref type="bibr" coords="5,247.09,596.97,90.47,8.64" target="#b2">(Glöckner et al., 2007)</ref>. The system accepts streams of validation items composed of the question string, the answer string, and a supporting witness text extracted from the document collection. It then uses deep linguistic processing and logical reasoning for validating the correctness of answers, i.e. by checking if they are verified by the witness texts. This is usually the case if the hypothesis expressed by the answer given the question (i.e. by the answer in affirmative form) can be proved from the representation of the witness and from the assumed background knowledge.</p><p>In order to gain more robustness, the theorem prover of MAVE is embedded in a feedback loop which subsequently skips literals until a proof of the reduced set of query literals succeeds. The number of skipped literals then serves as a robust indicator for (non)entailment. The system is backed with additional tests for false positives which reject trivial or circular answers. Answer selection needs some more effort because it involves a re-ranking of answers which permits a selection of the best one, rather than a clear-cut validation decision. MAVE exploits the aggregated evidence of all witness texts supporting a given answer in order to assign a useful validation score. The version of MAVE used for filtering the QA@CLEF 2007 results was mostly identical to the system described by <ref type="bibr" coords="6,142.04,226.78,84.71,8.64" target="#b2">Glöckner et al. (2007)</ref>. The system even reused the error model extracted in these experiments, which determine the probability estimates needed for aggregation. However, there were two main changes compared to the published method: First, extraction of a threshold which makes it possible to reject rather than select the best answer candidate in the case that the evidence for selection of the best answer is still too weak. Second, integration of large lexical-semantic resources (like GermaNet and OpenThesaurus) which allow more flexible inferences.</p><p>The current state of the system is detailed by <ref type="bibr" coords="6,289.22,298.51,69.74,8.64" target="#b1">Glöckner (2007b)</ref>. In particular, the system now avoids several errors which still deteriorated results in QA@CLEF 2007. The improvements achieved in the meantime include an answer type filter which compares the expected answer type of the question and the found answer type; an improved informativeness test for definition questions; combination of the skipped literal count (as a special kind of an edge overlap metric) with a simple lexical overlap matcher, etc. These changes resulted in very satisfying performance in the Answer Validation Exercise 2007, though the AVE task of answer selection from a few end results of QA systems is by no means comparable to a realistic problem of answer selection from hundreds of answer candidates, as produced by our six streams for QA@CLEF 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Discussion</head><p>We submitted two runs for the German monolingual task in QA@CLEF 2007 (see Table <ref type="table" coords="6,461.06,450.04,3.60,8.64" target="#tab_0">2</ref>). The first run was generated from all six answer streams by applying MAVE for answer selection. Noticing the heterogenity of the involved QA systems (i.e. the precision-oriented InSicht system and the shallow QAP and MIRA systems), the second run was compiled from the QAP and MIRA streams only in order to obtain a baseline for the shallow QA subsystems. As noted above, the results dropped in comparison to previous years, mainly because of the addition of Wikipedia and several problems in adapting system components (see Sect. 2.1). In part, the shallow QA subsystems managed to back-up the performance of the deep QA system: 18 correct answers that InSicht did not find came from the shallow streams.</p><p>Compared to previous years with positive K1 values, our system somewhat lost the ability to judge its own answers by assigning accurate scores and hence the ability to identify questions with no answers in the collection (NIL questions). This effect was partly due to remaining bugs in the answer validator -an error which disabled applicability of important axioms and an error in the processing of numerals which spoiled results for COUNT and MEASURE questions. Moreover the system used improper parameter settings based on the QA@CLEF tasks of previous years and the characteristics of earlier versions of InSicht, QAP, and MIRA, which were no longer valid due to the extension of the text collection by the German Wikipedia and due to changes in the underlying QA systems to which the error model was not yet adapted.</p><p>The dialog treatment was very successful as can be seen in the third column of Table <ref type="table" coords="6,449.71,641.33,3.74,8.64">1</ref>. 89.6% of the questions with anaphors were correctly treated by the coreference resolver CORUDIS.</p><p>Due to time constraints, patterns for the Wikipedia data were not produced in time by the shallow QA methods. Instead, the patterns created from CLEF-NEWS were utilized for Wikipedia documents as well. As articles sometimes follow a template-like style in the Wikipedia (e.g. for biographical information like the place and date of birth), QAP missed important patterns applicable to reliably find answers to factoid questions in Wikipedia. The shallow QA systems profited from resolving anaphoric references (by the coreference resolver CORUDIS in the WOCADI parser). But for three follow-up questions, an anaphor stayed unresolved. This led to missing bits of information for precise results from the IR phase.</p><p>The system for QA@CLEF 2007 showed a performance <ref type="bibr" coords="7,321.85,134.61,160.93,8.64">drop compared to 2004, 2005, and 2006</ref>. Error analysis hinted at the massive change in the size and type of the document collection caused by the addition of Wikipedia. On the positive side, the system architecture (under the umbrella of IRSAW) matured by the solid integration of two shallow QA subsystems beside the main, deep QA system, InSicht. Finally, the addition of MAVE as an answer validator completed the system by an important component. Some cutting edge QA systems participating in CLEF or TREC employ components fulfilling a similar function.</p><p>In the future, the document processing (especially preprocessing and parsing) of the Wikipedia subcollection should be improved by adjusting to some frequent peculiarities of Wikipedia. The successful dialog handling should be tested on more diverse discourse dependency types and structures linking questions and answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,95.98,119.13,411.05,68.34"><head>Table 2 :</head><label>2</label><figDesc>Results for the German question set from QA@CLEF 2007.</figDesc><table coords="6,95.98,133.17,411.05,54.31"><row><cell>Run</cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4"># Right # Unsupported # Inexact # Wrong</cell><cell>K1</cell></row><row><cell>fuha071DEDE</cell><cell>48</cell><cell>2</cell><cell>4</cell><cell>146</cell><cell>-0.1789</cell></row><row><cell>fuha072DEDE</cell><cell>30</cell><cell>2</cell><cell>4</cell><cell>164</cell><cell>-0.3180</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,104.35,732.60,408.65,6.91;1,90.00,742.07,257.76,6.91"><p>The IRSAW project (Intelligent Information Retrieval on the Basis of a Semantically Annotated Web; LIS 4 -554975(2) Hagen, BIB 48 HGfu 02-01) is funded by the DFG (Deutsche Forschungsgemeinschaft).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Dep. type Res.</p><p>Gegen wen ist sie (Steffi Graf) im Halbfinale der French Open im Jahr 1994 ausgeschieden? (038) Against whom did she drop out in the semifinal of the French Open in 1994?</p><p>In welchem Jahr hat er (Goethe) die Krönung von Kaiser Joseph dem Zweiten beobachtet? (040) In which year did he watch the coronation of the Emperor Joseph the second?</p><p>In welchem Jahr wurde er (Goethe) geboren? (041) When was he born?</p><p>In welcher Stadt ist er (Goethe) gestorben? (042) In which city did Goethe die?</p><p>In welchem Jahr erhielt er (answer 045: Victor Fleming; question 045: der Regisseur von "Vom Winde verweht") einen Oscar für "Vom Winde verweht"? (046) In which year did he receive the Oscar for "Gone with the Wind"?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M</head><p>In welchem Jahr ist er (Freddy Mercury) gestorben? (051) In which year did he die?</p><p>Welches Musical von ihm (Andrew Lloyd Webber) führt der ökumenische Jugendchor Friedrichsdorf 1994 auf? (061) Which musical by him is performed by the ecumenical youth choir Friedrichsdorf in 1994?</p><p>Für wie viel Millionen Dollar hat er (Andrew Lloyd Webber) ein Picasso-Gemälde ersteigert? (062) For how much million dollars did he purchase a painting by Picasso at auction?</p><p>Wer war er (Al Capone)? ( <ref type="formula" coords="4,176.20,365.37,10.36,6.91">069</ref>) Who was he?</p><p>Wann wurde er (Al Capone) erschossen? (070) When was he shot?</p><p>Wie alt war er (Al Capone), als er seine Familie nach Chicago brachte? (071) How old was he when he brought his family to Chicago?</p><p>Wie viele Mitgliedsstaaten hatte die Organisation (die UNESCO) 1995? (074) How many member states did the organization have in 1995?</p><p>Wie viele Soldaten hatte die USA während des Krieges (Vietnamkriegs) in den 60er Jahren in Vietnam stationiert? (088) How many soldiers did the USA base in the sixties in Vietnam during the war?</p><p>Wer war US-Präsident, als der Krieg (der Vietnamkrieg) zu Ende ging? (090) Who was President of the USA when the war came to an end?</p><p>Nenne drei Alben dieser Rockband (Red Hot Chili Peppers). ( <ref type="formula" coords="4,289.17,488.41,10.36,6.91">107</ref>) Name three albums of the rock band.</p><p>Von welcher Organisation wurde sie (Audrey Hepburn) als Sonderbotschafterin ernannt? (111) Which organization appointed she as a special ambassador?</p><p>An welcher Universität studierte er (Martin Scorsese) 1960 Filmkunst? (114) At which university did Martin Scorsese study cinematography in 1960?</p><p>Für wie viele Oscars wurde er (Martin Scorsese) bereits nominiert? (115) For how many Academy Awards he has been already nominated?</p><p>Wie heißt der Kriminalkommissar in seinem (Henning Mankells) Roman "Mörder ohne Gesicht"? ( <ref type="formula" coords="4,409.11,564.12,10.36,6.91">134</ref>) What is the name of the police inspector in his novel "Faceless Killers"?</p><p>Nenne drei Staaten, die das Protokoll (Kyoto-Protokoll) unterzeichnet haben. ( <ref type="formula" coords="4,341.71,583.05,10.36,6.91">153</ref>) Name three countries which signed the Protocol?</p><p>Wieviele Mitglieder hat die Organisation (die Organisation Pro Familia)? ( <ref type="formula" coords="4,329.12,601.98,10.36,6.91">156</ref>) How many members does the organization have?</p><p>Wie heißt ihr (Angela Merkels) Bruder? ( <ref type="formula" coords="4,223.68,620.91,10.36,6.91">161</ref>) What is the name of her brother?</p><p>In welchen japanischen Städten existieren sie (Hanamachi) noch? (165) In which japanese cities do Hanamachi still exist?</p><p>Auf welchem Treffen ließ er (Gerhard Schröder) sich von Chirac vertreten? (174) At which event was he represented by Chirac?</p><p>Wie heißt die vierte Ehefrau von ihm (Gerhard Schröder)? ( <ref type="formula" coords="4,281.80,677.70,10.36,6.91">175</ref>) What is the name of his fourth wife?</p><p>Wie heißen die drei großen Wasserfälle im Canyon (Grand Canyon)? ( <ref type="formula" coords="4,315.84,696.63,10.36,6.91">177</ref>) What are the names of the three waterfalls of the Canyon?</p><p>Wie hieß ihr (Cate Blanchetts) erster Kinofilm? (185) What is the name of her first cinema film?</p><p>Wohin wanderte er <ref type="bibr" coords="4,152.36,734.49,114.68,6.91">(Alfred Hitchcock) 1939 aus? (197)</ref> Where did he emigrate to in 1939?</p><p>Wie heißt sein (Alfred Hitchcocks) erster amerikanischer Film? <ref type="bibr" coords="4,295.12,753.41,17.26,6.91">(198)</ref> What is the name of his first American film?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,90.00,297.61,423.00,8.82;7,99.96,309.56,413.04,8.59;7,99.96,321.52,373.68,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,186.53,297.79,278.30,8.64">University of Hagen at QA@CLEF 2006: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,484.22,297.61,28.78,8.59;7,99.96,309.56,413.04,8.59;7,99.96,321.52,39.48,8.59">Results of the CLEF 2006 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><forename type="middle">;</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,340.76,423.00,8.82;7,99.96,352.72,413.04,8.59;7,99.96,364.67,123.18,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,190.05,340.94,275.89,8.64">University of Hagen at QA@CLEF 2007: Answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,484.22,340.76,28.78,8.59;7,99.96,352.72,413.04,8.59;7,99.96,364.67,37.31,8.59">Results of the CLEF 2007 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,384.10,423.00,8.64;7,99.96,395.87,413.03,8.82;7,99.96,407.83,413.04,8.59;7,99.96,419.78,413.03,8.82;7,99.96,431.92,244.59,8.64;7,90.00,451.16,423.00,8.64;7,99.96,463.12,413.04,8.64;7,99.96,474.90,398.55,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,353.78,384.10,159.22,8.64;7,99.96,396.05,278.57,8.64;7,394.01,463.12,118.99,8.64;7,99.96,475.07,223.43,8.64">Logical validation, answer merging and witness selection -a case study in multi-stream question answering</title>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.48,395.87,110.51,8.59;7,99.96,407.83,413.04,8.59;7,99.96,419.78,256.15,8.59;7,341.51,474.90,97.20,8.59">Proceedings of RIAO 2007 (Recherche d&apos;Information Assistée par Ordinateur -Computer assisted information retrieval), Large-Scale Semantic Access to Content (Text, Image, Video and Sound)</title>
		<editor>
			<persName><forename type="first">Rȃzvan</forename><surname>Bunescu</surname></persName>
		</editor>
		<meeting>RIAO 2007 (Recherche d&apos;Information Assistée par Ordinateur -Computer assisted information retrieval), Large-Scale Semantic Access to Content (Text, Image, Video and Sound)<address><addrLine>Pittsburgh, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Le Centre de Hautes Etudes Internationales</publisher>
			<date type="published" when="2001">2007. 2001</date>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
	<note>Proceedings of TREC-10</note>
</biblStruct>

<biblStruct coords="7,90.00,494.32,423.00,8.64;7,99.96,506.10,413.03,8.82;7,99.96,518.23,90.79,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,195.16,494.32,300.72,8.64">Coreference resolution with syntactico-semantic rules and corpus statistics</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,99.96,506.10,371.42,8.59">Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)</title>
		<meeting>the Fifth Computational Natural Language Learning Workshop (CoNLL-2001)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,537.30,423.00,8.82;7,99.96,549.43,77.73,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,195.69,537.30,218.81,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,568.68,423.00,8.64;7,99.96,580.45,413.04,8.82;7,99.96,592.41,413.04,8.82;7,99.96,604.54,413.03,8.64;7,99.96,616.32,332.93,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,195.30,568.68,317.70,8.64;7,99.96,580.63,90.62,8.64">Extending knowledge and deepening linguistic processing for the question answering system InSicht</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,207.37,580.45,305.63,8.59;7,99.96,592.41,162.51,8.59">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<title level="s" coord="7,165.35,616.32,139.13,8.59">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><forename type="middle">;</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Fredric</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><surname>Henning Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Austria; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,635.74,423.00,8.64;7,99.96,647.52,413.04,8.82;7,99.96,659.47,413.04,8.82;7,99.96,671.61,186.73,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,287.59,635.74,225.41,8.64;7,99.96,647.70,168.42,8.64">University of Hagen at QA@CLEF 2006: Interpretation and normalization of temporal expressions</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,287.10,647.52,225.90,8.59;7,99.96,659.47,261.89,8.59">Results of the CLEF 2006 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">Alessandro</forename><forename type="middle">;</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">José</forename><forename type="middle">Luis</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,690.85,423.00,8.64;7,99.96,702.63,152.28,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,271.65,690.85,236.95,8.64">Kooperation der Personennamendatei (PND) mit Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Christel</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Pfeifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,99.96,702.63,93.98,8.59">Dialog mit Bibliotheken</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,722.05,423.00,8.64;7,99.96,733.83,413.04,8.82;7,99.96,745.96,94.79,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,201.67,722.05,305.72,8.64">On the role of information retrieval in the question answering system IRSAW</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.79,733.83,250.25,8.59">Proceedings of the LWA 2006, Workshop Information Retrieval</title>
		<meeting>the LWA 2006, Workshop Information Retrieval<address><addrLine>Hildesheim, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="119" to="125" />
		</imprint>
		<respStmt>
			<orgName>Universität Hildesheim</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,112.34,423.00,8.64;8,99.96,124.12,413.03,8.82;8,99.96,136.25,110.94,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,210.75,112.34,302.26,8.64;8,99.96,124.29,92.40,8.64">A modified information retrieval approach to produce answer candidates for question answering</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,217.29,124.12,194.27,8.59">Proceedings of the LWA 2007, Workshop FGIR</title>
		<meeting>the LWA 2007, Workshop FGIR<address><addrLine>Halle/Saale, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Gesellschaft für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,156.00,423.00,8.82;8,99.96,167.95,124.26,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,263.26,156.17,195.56,8.64">Description of NTU system at TREC-10 QA task</title>
		<author>
			<persName coords=""><forename type="first">Chuan-Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,476.94,156.00,36.06,8.59;8,99.96,167.95,64.45,8.59">Proceedings of TREC-10</title>
		<meeting>TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="406" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,188.06,423.00,8.64;8,99.96,199.83,413.03,8.82;8,99.96,211.97,37.36,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,484.22,188.06,28.78,8.64;8,99.96,200.01,268.77,8.64">TREC-10 experiments at KAIST: Batch filtering and question answering</title>
		<author>
			<persName coords=""><forename type="first">Jong-Hoon; Kyung-Soon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">; Du-Seong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chung</forename><forename type="middle">Won</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Key-Sun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.18,199.83,99.45,8.59">Proceedings of TREC-10</title>
		<meeting>TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,231.71,423.00,8.82;8,99.96,243.67,413.04,8.82;8,99.96,255.62,237.21,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,289.86,231.89,89.98,8.64">Coreference in Q &amp; A</title>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.06,231.71,109.94,8.59;8,99.96,243.67,81.25,8.59">Advances in Open Domain Question Answering</title>
		<title level="s" coord="8,484.94,243.67,28.06,8.59;8,99.96,255.62,103.04,8.59">Speech and Language Technology</title>
		<editor>
			<persName><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="71" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
