<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_M2frMv3">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.60,182.13,82.93,9.96"><forename type="first">Georgiana</forename><surname>Puşcaşu</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,313.09,182.13,82.59,9.96"><forename type="first">Constantin</forename><surname>Orȃsan</surname></persName>
							<email>c.orasan@wlv.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<postCode>2007</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Research Group in Computational Linguistics</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A4D212AED9A9836F4EDD1EAC581C363E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Question Answering, Cross-lingual Question Answering, Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on the participation of the University of Wolverhampton in the Multiple Language Question Answering (QA@CLEF) track of the CLEF 2007 campaign. We approached the Romanian to English cross-lingual task with a Question Answering (QA) system that processes a question in the source language (i.e. Romanian), translates the identified keywords into the target language (i.e. English), and finally searches for answers in the English document collection. We submitted one run of our system that has achieved an overall accuracy of 14%. Besides the difficulties posed by developing a monolingual QA system, the bottleneck in building a crosslingual one is the lack of a reliable translation methodology from the source into the target language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) <ref type="bibr" coords="1,209.29,602.01,10.57,9.96" target="#b6">[7]</ref> is defined as the task of providing an exact answer to a question formulated in natural language. Cross-lingual QA capabilities enable systems to retrieve the answer in one language (the target language) to a question posed in a different language (the source language).</p><p>Last year, a new Romanian-to-English (RO-EN) cross-lingual QA task was organised for the first time within the context of the CLEF campaign <ref type="bibr" coords="1,317.18,661.77,14.60,9.96" target="#b9">[10]</ref>. The task consisted in retrieving answers to Romanian questions in an English document collection. Four types of questions were considered: factoid, definition, list and temporally restricted (see <ref type="bibr" coords="1,317.91,685.65,15.45,9.96" target="#b9">[10]</ref> for a detailed description of each question type). This year's task was organised in a similar manner, with the exception that all questions were clustered in classes related to same topic, some of which even contain anaphoric references to other questions from the same topic class or to their answers. Besides the usual news collections employed in the search for answers, this year's novelty was the fact that Wikipedia articles could also be used as answer source, which significantly increased the search space, making the task more difficult. This is the first time a Romanian-English cross-lingual QA system fully developed at the University of Wolverhampton has participated in the QA@CLEF competition. This system adheres to the classical architecture of QA systems which includes three stages: question processing, information retrieval and answer extraction <ref type="bibr" coords="2,351.75,171.09,9.88,9.96" target="#b6">[7]</ref>. In addition, the cross-lingual capabilities are provided by a Romanian-to-English term translation module. This paper describes the development stages and evaluation results of our system. The rest of the paper is organised as follows: Section 2 provides an overall description of the system, while Sections 3, 4, 5 and 6 present the four embedded modules -question processing, term translation, passage retrieval and answer extraction respectively. Section 7 captures the evaluation results and their analysis. Finally, in Section 8, conclusions are drawn and future directions of system development are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System overview</head><p>Question Answering systems normally adhere to a pipeline architecture consisting of three main stages: question analysis, passage retrieval and answer extraction <ref type="bibr" coords="2,381.84,309.57,9.91,9.96" target="#b6">[7]</ref>. For cross-lingual systems, the language barrier is usually crossed by employing free online translation services for translating the question from the source language into the target language <ref type="bibr" coords="2,372.35,333.45,10.57,9.96" target="#b7">[8,</ref><ref type="bibr" coords="2,386.63,333.45,11.62,9.96" target="#b13">14]</ref>. The QA process is then entirely performed in the target language by a monolingual QA system. There are also crosslingual systems that automatically translate the document collection in the source language and then perform monolingual QA in the source language <ref type="bibr" coords="2,328.00,369.33,10.00,9.96" target="#b1">[2]</ref>. Another alternative approach involves monolingual QA in the source language and then translating the answer, but this approach is feasible only when document collections covering the same material are available both in the source and target languages <ref type="bibr" coords="2,214.69,405.21,10.00,9.96" target="#b0">[1]</ref>.</p><p>Since we could not identify neither reliable translation services from Romanian into English for translating complete questions, nor English-Romanian full document translation tools, the first two approaches could not be adopted. In the case of the third approach, the impediment was the lack of a Romanian document collection equivalent to the English one. Therefore we adopted a slightly different approach where the question analysis is performed in the original source language without any translation in order to overcome the negative effect of full question translation on the overall accuracy of the system. Afterwards, in order to link the two languages involved in the crosslingual QA setting, term translation is performed by means of bilingual resources and linguistic rules. The search for passages and answers is then performed in the target language documents using modules designed for that particular language. This approach has been previously adopted by Sutcliffe et al. <ref type="bibr" coords="2,168.85,536.73,15.49,9.96" target="#b12">[13]</ref> and Tanev et al. <ref type="bibr" coords="2,263.54,536.73,14.60,9.96" target="#b14">[15]</ref>.</p><p>The architecture of our system consists of a four-module pipeline, where each module is responsible for a different stage in answering a question. These four modules are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Question Processing Module</head><p>This module receives as input a question in Romanian, parses it with a statistical part-ofspeech (POS) tagger and with a shallow parser, and then uses this linguistic information to identify the type of the question and of the expected answer, the question focus, as well as the relevant keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Term Translation Module</head><p>This module is responsible for identifying all translation equivalents of each term identified in the question. The translation equivalents are generated by consulting bilingual resources and then assembled into terms in the target language by means of linguistic rules.</p><p>3) Passage Retrieval Module At this stage candidate snippets of text are retrieved from the English document collection on the basis of a query that includes the translation equivalents of all terms identified in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Answer Extraction Module</head><p>This module, on the basis of the information extracted by the Question Processor, processes the snippets of text retrieved at the previous stage and identifies candidate answers restricted to the expected answer type. Then one answer is selected after ranking the resulting list of candidate answers.</p><p>Figure <ref type="figure" coords="3,226.45,432.09,3.90,9.96">1</ref>: System Architecture and Functionality Figure <ref type="figure" coords="3,136.45,457.77,4.98,9.96">1</ref> illustrates the system architecture and functionality. The following four sections will present in more detail the functionality of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Processing</head><p>This stage is mainly concerned with the identification of the semantic type of the entity sought by the question (expected answer type). In addition, it also provides the question focus, the question type and the set of keywords relevant for the question. To achieve these goals, our question analyser performs the following steps: a) POS-tagging, NP-chunking, Named Entity (NE) Extraction, Temporal Expression Identification The questions are first morpho-syntactically pre-processed using the TnT statistical part-ofspeech tagger <ref type="bibr" coords="3,165.71,617.61,10.57,9.96" target="#b2">[3]</ref> trained on Romanian <ref type="bibr" coords="3,274.45,617.61,14.60,9.96" target="#b16">[17]</ref>. On the basis of this morpho-syntactic annotation, a rule-based shallow noun phrase (NP) chunker was implemented. A rule-based NE recogniser identifies the NEs which appear in the questions. Temporal expressions (TEs) are also detected using a Romanian TE identifier and normalizer based on the one previously developed for English by Puscasu <ref type="bibr" coords="3,192.14,665.49,14.60,9.96" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Question Focus Identification</head><p>The question focus is the word or word sequence that defines or disambiguates the question, in the sense that it pinpoints what the question is searching for or what it is about. The question focus is considered to be either the noun determined by the question stem (for example in the question What city hosted the Olympic Games in 2000?, the focus is city) or the head noun of the first question NP if this NP comes before the question's main verb or if it follows the verb "to be" (for example in the question Who is the inventor of the polygraph?, the focus is inventor ). c) Distinguishing the Expected Answer Type At this stage the category of the entity expected as an answer to the analysed question is identified. Our system's answer type taxonomy distinguishes the following classes: PERSON, LOCATION, ORGANIZATION, TEMPORAL, NUMERIC, DEFINITION and GENERIC, and it was derived on the basis of questions asked in previous CLEF campaigns. The assignment of a class to an analysed question is performed using the question stem and the type of the question focus. The question focus type is detected using WordNet <ref type="bibr" coords="4,399.96,214.65,10.45,9.96" target="#b4">[5]</ref> sub-hierarchies specific to the categories PERSON / LOCATION / ORGANIZATION. We employ a pre-defined correspondence between each category and the ILI (InterLingual Index) codes of WordNet root nodes heading category-specific noun sub-trees. These ILI codes guide the extraction of category specific noun lists from the Romanian WordNet <ref type="bibr" coords="4,360.85,262.41,15.61,9.96" target="#b17">[18,</ref><ref type="bibr" coords="4,380.42,262.41,11.60,9.96" target="#b18">19]</ref>. In the case of ambiguous question stems (e.g. What ), the resulted lists are searched for the head of the question focus, and the expected answer type is identified with the category of the corresponding list (for example, in the case of the question In which country was Swann born?, the question focus is country, noun found in the LOCATION list, therefore the associated expected answer type is LOCATION).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d) Inferring the Question Type</head><p>This year, the QA@CLEF main task distinguishes among four question types: factoid, definition, list and temporally restricted questions<ref type="foot" coords="4,326.76,364.63,3.97,5.11" target="#foot_0">1</ref> . As temporal restrictions can constrain any type of question, we proceed by first detecting whether the question has the type factoid, definition or list and then test the existence of temporal restrictions. The question type is identified using two simple rules: for questions which ask for definitions of concepts, the assigned question type is definition; if the question focus is a plural noun, then the question type is list, otherwise the consider the question to be factoid. The temporal restrictions are identified using several patterns and the information provided by the TE identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e) Keyword Set Generation</head><p>The set of keywords is automatically generated by listing the question terms in decreasing order of their relevance, as follows: the question focus, the identified NEs and TEs, the remaining noun phrases, and all the non-auxiliary verbs present in the question. Given the grouping of questions into topics and the presence of anaphoric expressions pointing to terms situated in other questions belonging to the same topic, a shallow anaphora resolution mechanism was employed to expand the set of question keywords with other possibly relevant terms as described below. The expanded set of keywords is then passed on to the Term Translation module, in order to obtain English keywords for passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>f) Resolution of anaphoric expressions</head><p>One novelty introduced in this year's competition was that questions were organised in clusters of related questions. In a number of cases, the links between questions were realised using anaphoric pronouns which meant that in order to obtain a more complete list of keywords, anaphora resolution was necessary. Given the difficulty of anaphora resolution it was not possible to employ a fully fledged anaphora resolution system. Instead, the set of keywords related to a question was expanded with the list of named entities present in the cluster. This was done for two reasons. On the one hand, investigation of the question clusters revealed that pronouns quite often refer to named entities in the cluster. On the other hand, given that the questions are related, it is possible that named entities present in the questions also cooccur in the same document. As a result, it is more likely to extract relevant documents with this expanded query. A number of questions referred to the result of the previous question. Currently, we took no steps to address this problem due to the fact that in our present system there is no way to feed the answer to a question back into the system.</p><p>At this stage two processes are carried out: term translation and query generation. The keywords extracted at the question processing stage are first translated with an approach similar to the one we employed last year when we participated together with two Romanian research groups in the same task at CLEF 2006 <ref type="bibr" coords="5,197.87,169.05,14.60,9.96" target="#b11">[12]</ref>. It does also resemble the one employed by Ferrandez et al. <ref type="bibr" coords="5,472.33,169.05,10.45,9.96" target="#b5">[6]</ref> within the same CLEF campaign, but in the English to Spanish cross-lingual task. After the process of term translation has finished, a query is generated by making a conjunction of all keywords. Each keyword is represented by all its translation equivalents grouped using the disjunction operator. Term translation is achieved by employing WordNet and more specifically the ILI alignment between the English WordNet and all other WordNets developed as part of the EuroWordNet and BalkaNet projects. The underlying idea is that, given a Romanian word, the Romanian WordNet and its alignment to the English one, we identify all possible translations of the word by finding all the synsets containing it and crossing through the ILI alignment to the English side where the equivalent synsets are found. If the word to be translated does not appear in the Romanian WordNet, as is quite frequently the case, we search for it in other available dictionaries and preserve the first three translations. If still no translation is found, the word itself is considered as translation, an approach which works reasonably well for named entities.</p><p>In the case of multi-word terms, like most of the question noun phrases (NP), each NP word is translated individually using the method described above. After that, rules are employed to convert the Romanian syntax to English syntax, and to obtain the translation equivalents of a given term.</p><p>One drawback of this term translation method is that it proposes too many translations for a word due to the fact that it does not employ any word sense disambiguation. In order to address this problem, we implemented a ranking method which relies on information from parallel English-Romanian Wikipedia pages related to the question to be answered, but not necessary containing the actual answer. The assumption of this method was that the two sets of pages will contain more or less the same information, so it will be possible to find the most likely translation for the noun-verb pairs present in the question. Unfortunately, preliminary experiments revealed that the inclusion of this approach lead to the retrieval of a very small number of passages, many of which did not contain the answer to the question. Due to the time restrictions with this task, we were unable to properly tune the method to improve the quality of the passage retrieval module, and for this reason we did not employ it in this year's submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Passage Retrieval</head><p>The purpose of the passage retrieval module is to extract a list of passages from the document collection which may contain the answer to the question asked. This year's document collection consists of three distinct collections: English Wikipedia pages collected in November 2006, Los Angeles Times from 1994 and Glasgow Herald from 1995. This is the first time that Wikipedia has been included in the document collection and, as a result of the fact that it is several orders of magnitude bigger than the other two collections, the search space was significantly larger than in previous years, making the task more difficult. Given that the documents in each collection are formatted in different ways, each had to be indexed individually and processed in a slightly different manner. For indexing and retrieval, we used Lucene <ref type="bibr" coords="5,372.48,642.21,10.00,9.96" target="#b8">[9]</ref>, an open source information retrieval library appropriate for local document collections and intranets.</p><p>The query proposed by the term translation module, including all possible translations of the question keywords, was used as starting point in extracting passages. In the initial experiments we tried to limit the number of translations used for each original keyword, but as a result, the number of retrieved snippets was too low. This can be explained by the fact that no disambiguation was performed and therefore it was possible that some of the translations were highly ranked and therefore included in the query, even though they were not appropriate. As explained before, attempts to order the keyword translations according to the likeliness of them being the correct translation of the keyword did not lead to satisfactory results and therefore it was not used in this year's submission. In light of this, we decided to use all the translations identified for a keyword and linked them using the OR operator provided by Lucene.</p><p>We indexed the document collection in order to retrieve documents which contain the keywords, and not actual passages. We decided to take this approach because it offers more flexibility and allows better control of the methods which retrieve candidate passages. However, the drawback of this approach is that it needs to process each document individually and extract relevant passages. For this year's system we decided to retrieve only sentences. In order to do this, each sentence from the retrieved documents was scored on the basis of how many keywords, temporal expressions and named entities they contained. At present, up to 25 sentences with the highest scores are retrieved from each document, provided that their score is higher than a predefined threshold. This set of sentences is fed into the next module, the answer extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer Extraction</head><p>Once candidate answer-bearing document passages (in our case sentences) have been selected, the answer extraction module starts with a merging of all passages retrieved for questions belonging to a certain topic. All retrieved passages are morpho-syntactically analysed and annotated with functional dependency information by employing Conexor's FDG Parser <ref type="bibr" coords="6,422.64,333.45,14.70,9.96" target="#b15">[16]</ref>. They are also parsed with the Named Entity Identifier embedded in the GATE (General Architecture for Text Engineering) toolkit <ref type="bibr" coords="6,182.90,357.45,10.00,9.96" target="#b3">[4]</ref>, which recognises and classifies multi-or one-word strings as names of companies, persons, locations, etc.</p><p>Afterwards a question-based passage ranking is applied to the merged set of passages retrieved in response to queries derived from all topic specific questions. This set is ranked by using information that refers to the presence of the question focus, presence of question NEs, as well as of NEs belonging to the unified topic NE set, presence of other question elements (noun phrases and verb phrases), and presence of temporal and numeric expressions pertaining to the question.</p><p>The answer extraction process then addresses each type of expected answer type in a different manner, as follows:</p><p>a) Expected answer type is a Named Entity such as PERSON, LOCATION, ORGANIZATION or MISCELLANEOUS (any other type of named entity)</p><p>When the expected answer type is either a Named Entity or a NUMERIC / TEMPORAL entity, a text unit should be identified in the retrieved passages whose semantic type matches that of the expected answer. Named entities having the desired answer type are identified in the retrieved passages and added to the set of candidate answers. For each candidate answer, another score is computed on the basis of the passage score, the distance to other keywords and its frequency in the set of candidate answers. The candidate answer featuring the highest score is presented as the final answer. In the case of no candidate answer being found in the retrieved passages, the system returns NIL.</p><p>b) Expected answer type is NUMERIC In the case of NUMERIC answers, there are several sub-categories we consider in our search for an answer: MONEY, PERCENTAGE, MEASURE and NUMERIC-QUANTITY (any other type of NUMERIC entity). Various patterns are defined for exact candidate answer identification, patterns that take into consideration either the format of certain numeric expressions or the presence of the question focus in the neighbourhood of a numeric expression. The process of ranking candidate answers relies on the same parameters as in the case of the Named Entity answer type.</p><p>c) Expected answer type is TEMPORAL (i.e. a Temporal Expression)</p><p>The sub-categories of TEMPORAL entities that guide the answer extraction process are: MILLENNIUM, CENTURY, DECADE, YEAR, MONTH, DATE, TIME, DURATION (this category also applies to questions asking about age) and FREQUENCY. Patterns have been defined to extract from a certain temporal expression only that part having the required granularity (e.g. extracting from a temporal expression of granularity DATE like 25th of January 1993 only the YEAR, that is 1993 ).</p><p>d) Expected answer type is GENERIC When the expected answer type is neither a Named Entity, nor a NUMERIC or TEMPORAL entity, the question focus is essential in finding the answer. The candidate answers are constrained to be hyponyms of the question focus head.</p><p>e) Expected answer type is DEFINITION</p><p>When the expected answer is the definition of a concept, the processing is done in a different manner. Instead of using the passage extractor described in the previous section, it was decided to use a simpler approach. Wikipedia defines a large number of concepts, and therefore it was decided to first try to obtain the definition from the Wikipedia page associated to the concept. To this end, Lucene was used to return Wikipedia pages which contain the words from the concept to be defined in their title. Because this approach returned more than one document, a scoring method was implemented in order to rank the retrieved documents. If the document title contained words from the concept to be defined, the score of the document was boosted. In the case of words from the title not present in the concept, the score was penalised. Once the documents were ranked, regular expressions such as X [is|are|was|were]</p><p>[a|an|the] [possible definition] were used to locate the answer to a question. A common problem with the documents extracted from Wikipedia is that quite often they do not have any real content and they are redirections to other pages which contain the real description of the concept. This problem had to be addressed before documents were scored. Whenever no answer could be located in Wikipedia, passages were extracted from the other two document collections using the passage retrieval module described in Section 5 and the regular expressions were then applied to them. Unfortunately, this fall-back approach performed quite poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation Results</head><p>This section describes the results we obtained in our CLEF-2007 participation. We submitted only one run for the Romanian to English cross-lingual QA task. The methodology we employed targeted precision at the cost of recall, therefore we always chose to provide NIL answers for those questions we could not reliably locate a candidate answer in the retrieved passages. Apart from this, we have never returned more than one answer per question, but only the first ranked answer, when this could be identified. Table <ref type="table" coords="7,132.36,540.69,4.98,9.96" target="#tab_0">1</ref> illustrates the detailed results achieved by our system. It is to be mentioned that our system is able to recognise at the Question Processing stage questions asking for LISTs, but the answer extractor does not tackle this type of questions. The overall accuracy of our cross-lingual QA system was evaluated at a generic score over all questions of 14%. An analysis of our system output revealed the fact that our system was unable to locate an answer and thus returned the answer NIL for 117 questions. It retrieved 83 answers, out of which 28 were correct, 49 were wrong, 4 unsupported and 2 inexact.</p><p>Unsupported answers are correct answers to a question, but the judge who evaluated the run considered the passage returned as a source for the answer not relevant enough for that particular question. Given that at this moment we do not have access to the correct answers and the expected support passages, it is difficult to judge whether the four retrieved passages are appropriate or not. For example, in the case of the Romanian question Ce tip de animal a incercat Victor Bernal sa cumpere pe 25 ianuarie 1993? (which translates into English as What kind of animal did Victor Bernal try to buy on the 25th of January 1993? ), our returned answer was gorilla extracted from the following support passage:</p><p>The sting took place on Jan. 25, 1993, when Bernal and the others were escorted onto a DC-3 cargo plane parked in a remote corner of a small Miami airport to see the gorilla, crated for shipment.</p><p>which seems correct and justified by the presence of both Bernal's name and the date mentioned in the question, as well as the presence of the noun gorilla, which is a type of animal.</p><p>In the case of inexact answers, the answer-string contains a correct answer and the provided text-snippet supports it, but the answer-string is incomplete/truncated or is longer than the minimum amount of information required. For example, given the Romanian question Ce meserie are Michael Barrymore? (which translates into English as What is the occupation of Michael Barrymore? ), our answer, evaluated as inexact, was troubled comic and the passage supporting it was:</p><p>Troubled comic Michael Barrymore last night received an ovation as his show, Strike It Lucky, was named Quiz Programme of the Year at the National Television Awards.</p><p>These errors can be corrected by improving the answer extractor with more specific rules as to the extent of the required answer.</p><p>A preliminary analysis of the incorrect and NIL answers showed that their main cause was the poor translation of the question keywords, this yielding either irrelevant or no passages being retrieved from the English document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper described the development stages of our cross-lingual Romanian to English QA system, as well as our participation in the QA@CLEF campaign. Adhering to the generic QA system architecture, our system implements the three essential stages (question processing, passage retrieval and answer extraction), as well as a term translation module which provides cross-lingual capabilities by translating question terms from Romanian into English. It should be pointed out that this year our emphasis was less on fine tuning the system, and more on exploring the issues posed by the task and developing a complete system that can participate in the competition. Therefore, all four modules are still in a preliminary stage of development.</p><p>Our participation in the QA@CLEF campaign included only one run for the Romanian to English cross-lingual QA task. Our cross-lingual QA system achieved an overall accuracy of 14%. An in-depth analysis of our results at different stages in the QA process has revealed a number of future system improvement directions. The term translation module has a crucial influence over the performance of our system, and therefore will receive most of our attention. Apart from this, we will further investigate the ranking method for translation equivalents which relies on information from parallel English-Romanian Wikipedia pages in order to improve its performance, as we believe it is a promising research direction. We also intend to improve our answer extraction module by identifying a better answer ranking strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,131.49,186.34,339.85,231.17"><head></head><label></label><figDesc></figDesc><graphic coords="3,131.49,186.34,339.85,231.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,122.52,594.89,357.96,87.64"><head>Table 1 :</head><label>1</label><figDesc>Detailed evaluation results</figDesc><table coords="7,122.52,594.89,357.96,64.54"><row><cell></cell><cell>FACTOID</cell><cell>LIST</cell><cell>DEFINITION</cell><cell>TEMPORALLY RESTRICTED</cell></row><row><cell>RIGHT</cell><cell>15</cell><cell>0</cell><cell>13</cell><cell>0</cell></row><row><cell>WRONG</cell><cell>140</cell><cell>9</cell><cell>17</cell><cell>2</cell></row><row><cell>UNSUPPORTED</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>INEXACT</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>TOTAL</cell><cell>161</cell><cell>9</cell><cell>30</cell><cell>3</cell></row><row><cell>ACCURACY</cell><cell>9.32%</cell><cell>0.00%</cell><cell>43.33%</cell><cell>0.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,746.42,389.63,7.97"><p>For more details please refer to the track guidelines available at http://clef-qa.itc.it/2007/guidelines.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">Acknowledgements</head><p>The work presented in this paper has been partially supported by the <rs type="funder">EU</rs> funded project <rs type="projectName">QALL-ME</rs> (<rs type="grantNumber">FP6 IST-033860</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_M2frMv3">
					<idno type="grant-number">FP6 IST-033860</idno>
					<orgName type="project" subtype="full">QALL-ME</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.52,199.77,402.50,9.96;9,110.52,211.65,402.49,9.96;9,110.52,223.65,53.31,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,249.02,199.77,247.61,9.96">Cross-Lingual Question Answering by Answer Translation</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.52,211.65,355.28,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,243.21,402.66,9.96;9,110.52,255.09,402.51,9.96;9,110.52,267.09,289.11,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,110.52,255.09,187.42,9.96">LCC&apos;s PowerAnswer at QA@CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marian</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasin</forename><surname>Suriyentrakorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,330.48,255.09,182.55,9.96;9,110.52,267.09,185.01,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,286.65,402.52,9.96;9,110.52,298.53,378.14,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,195.85,286.65,182.17,9.96">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,405.35,286.65,107.69,9.96;9,110.52,298.53,287.84,9.96">Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP-2000)</title>
		<meeting>the Sixth Conference on Applied Natural Language Processing (ANLP-2000)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,318.09,402.54,9.96;9,110.52,330.09,402.52,9.96;9,110.52,341.97,402.37,9.96;9,110.52,353.97,22.93,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,470.42,318.09,42.64,9.96;9,110.52,330.09,385.68,9.96">GATE: A framework and graphical development environment for robust NLP tools and applications</title>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.52,341.97,398.23,9.96">Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Anniversary Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,373.53,402.72,9.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,235.92,373.53,174.55,9.96">WordNet: An Eletronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,393.09,402.55,9.96;9,110.52,404.97,402.49,9.96;9,110.52,416.97,402.24,9.96;9,110.52,428.85,95.43,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,325.37,404.97,187.64,9.96">AliQAn and BRILI QA Systems at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Ferrandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pilar</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandra</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Ferrandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Peral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Noguera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.00,416.97,357.68,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,448.41,402.53,9.96;9,110.52,460.41,402.75,9.96;9,110.52,472.41,22.93,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,277.14,448.41,85.30,9.96">Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,483.83,448.41,29.22,9.96;9,110.52,460.41,167.59,9.96">Oxford Handbook of Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="560" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,491.85,402.66,9.96;9,110.52,503.85,402.48,9.96;9,110.52,515.85,309.62,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,147.37,503.85,214.76,9.96">The University of Amsterdam at QA@CLEF2004</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,386.28,503.85,126.72,9.96;9,110.52,515.85,229.17,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2004 Workshop</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,535.29,211.48,9.96" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,554.85,402.72,9.96;9,110.52,566.85,402.39,9.96;9,110.52,578.73,402.36,9.96;9,110.52,590.73,377.43,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,110.52,578.73,298.65,9.96">Overview of the CLEF 2006 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,431.51,578.73,81.36,9.96;9,110.52,590.73,273.33,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,610.29,402.52,9.96;9,110.52,622.17,310.09,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,208.33,610.29,175.07,9.96">A Framework for Temporal Resolution</title>
		<author>
			<persName coords=""><forename type="first">Georgiana</forename><surname>Puscasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,412.56,610.29,100.49,9.96;9,110.52,622.17,278.37,9.96">Proceedings of the 4th Conference on Language Resources and Evaluation (LREC2004)</title>
		<meeting>the 4th Conference on Language Resources and Evaluation (LREC2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,641.73,402.64,9.96;9,110.52,653.73,402.52,9.96;9,110.52,665.61,402.38,9.96;9,110.52,677.61,402.50,9.96;9,110.52,689.61,22.93,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,148.20,665.61,286.24,9.96">Cross-Lingual Romanian to English Question Answering at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Georgiana</forename><surname>Puscasu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ionut</forename><surname>Pistol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Trandabat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alin</forename><surname>Ceausu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Stefanescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,475.92,665.61,36.99,9.96;9,110.52,677.61,323.00,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.52,709.17,402.58,9.96;9,110.52,721.05,402.51,9.96;9,110.52,733.05,402.24,9.96;9,110.52,744.93,98.31,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,149.89,721.05,363.14,9.96;9,110.52,733.05,18.34,9.96">Cross-Language French-English Question Answering using the DLT System at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Aoife</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kieran</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darina</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.00,733.05,357.68,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,111.33,402.63,9.96;10,110.52,123.33,402.65,9.96;10,110.52,135.21,402.35,9.96;10,110.52,147.21,171.51,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,110.52,123.33,402.65,9.96;10,110.52,135.21,97.66,9.96">Exploiting Linguistic Indices and Syntactic Structures for Multilingual Question Answering: ITC-irst at CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiril</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.76,135.21,285.11,9.96;10,110.52,147.21,64.53,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2005 Workshop</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,167.13,402.56,9.96;10,110.52,179.13,402.50,9.96;10,110.52,191.01,265.46,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,443.05,167.13,70.03,9.96;10,110.52,179.13,189.10,9.96">The DIOGENE question answering system at CLEF-2004</title>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.80,179.13,181.23,9.96;10,110.52,191.01,185.01,9.96">Working Notes for the Cross Language Evaluation Forum (CLEF) 2004 Workshop</title>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,210.93,402.34,9.96;10,110.52,222.93,325.93,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,279.75,210.93,163.23,9.96">A Non-Projective Dependency Parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Jaervinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.31,210.93,49.55,9.96;10,110.52,222.93,268.33,9.96">Proceedings of the 5th Conference of Applied Natural Language Processing</title>
		<meeting>the 5th Conference of Applied Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,242.85,402.41,9.96;10,110.52,254.85,402.43,9.96;10,110.52,266.73,364.08,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,166.33,242.85,346.60,9.96;10,110.52,254.85,139.69,9.96">Using a Large Set of EAGLES-compliant Morpho-Syntactic Descriptors as a Tagset for Probabilistic Tagging</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,273.24,254.85,239.72,9.96;10,110.52,266.73,153.95,9.96">Proceedings of the Second International Conference on Language Resources and Evaluation</title>
		<meeting>the Second International Conference on Language Resources and Evaluation<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,286.65,402.49,9.96;10,110.52,298.65,402.32,9.96;10,110.52,310.65,343.57,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,327.40,286.65,185.62,9.96;10,110.52,298.65,148.75,9.96">BalkaNet: Aims, Methods, Results and Perspectives. A General Overview</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sofia</forename><surname>Stamou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,359.64,298.65,153.21,9.96;10,110.52,310.65,100.61,9.96">Romanian Journal on Information Science and Technology</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Romanian Academy</publisher>
		</imprint>
	</monogr>
	<note>Special Issue on BalkaNet</note>
</biblStruct>

<biblStruct coords="10,110.52,330.57,402.53,9.96;10,110.52,342.45,402.29,9.96;10,110.52,354.45,219.01,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,174.14,342.45,194.88,9.96">New developments of the Romanian WordNet</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbu</forename><surname>Verginica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luigi</forename><surname>Ceausu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Catalin</forename><surname>Bozianu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Magda</forename><surname>Mihaila</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.76,342.45,123.05,9.96;10,110.52,354.45,186.94,9.96">Proceedings of the Workshop on Resources and Tools for Romanian NLP</title>
		<meeting>the Workshop on Resources and Tools for Romanian NLP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
