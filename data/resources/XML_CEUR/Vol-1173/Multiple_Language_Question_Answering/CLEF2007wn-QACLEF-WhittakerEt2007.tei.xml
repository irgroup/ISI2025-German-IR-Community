<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,174.39,170.78,254.21,15.15">CLEF2007 Question Answering Experiments at Tokyo Institute of Technology</title>
				<funder ref="#_nSJsSjb">
					<orgName type="full">Japanese government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.76,204.67,78.87,8.74"><forename type="first">E</forename><forename type="middle">W D</forename><surname>Whittaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.68,204.67,46.22,8.74"><forename type="first">J</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
							<email>novakj@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.20,204.67,34.31,8.74"><forename type="first">M</forename><surname>Heie</surname></persName>
							<email>heie@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.21,204.67,35.04,8.74"><forename type="first">S</forename><surname>Furui</surname></persName>
							<email>furui@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.04,148.86,402.93,15.15;1,174.39,170.78,254.21,15.15">CLEF2007 Question Answering Experiments at Tokyo Institute of Technology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C58F28CEFBEFF7FF44BA3777CC0BCA6C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question answering, Language modeling, Speech recognition, Spoken document retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the experiments carried out at Tokyo Institute of Technology for the CLEF 2007 QAst (Question Answering in speech transcripts) pilot task, as well as our results from the official evaluation. We apply a non-linguistic, data-driven approach to Question Answering (QA), based a noisy channel model. The system we use for the QAst evaluation comprises an Information Retrieval (IR) module which uses an LM-based approach to sentence retrieval, and an Answer Extraction (AE) module which identifies and ranks the exact answer candidates in the retrieved sentences. Our team participated in the CLEF 2007 QAst pilot track, task T1: QA in manual transcriptions of lectures, and task T2: QA in automatic transcriptions of lectures. On the official evaluation our system achieved a best run MRR of 0.20 and a top1 score of 0.14 on task T1, and a best run MRR of 0.12 and a top1 score of 0.08 on task T2, placing us 3rd in a field of 5 teams that submitted results for these tasks. All experiments and evaluations descibed in this paper were conducted using the CHIL corpus (transcriptions of lectures) which was supplied to all track participants by the QAst track coordinators. ASR lattices were also provided by LIMSI, however we did not use these during the official evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we explain our experimental setup and general approach to automatic Question Answering (QA), and report our official evaluation results for the CLEF 2007 QAst (Question Answering in speech transcripts) pilot track. We employed an entirely data-driven, non-linguistic and largely language independent QA framework for the QAst track, which was similar but not identical to that which we used in previous QA evaluations such <ref type="bibr" coords="2,385.67,123.98,64.34,8.74">as TREC 2006</ref><ref type="bibr" coords="2,450.02,123.98,58.44,8.74">, CLEF 2006</ref><ref type="bibr" coords="2,508.46,123.98,4.54,8.74;2,90.00,135.93,55.38,8.74">, NTCIR 2006</ref>, etc. This approach, which is detailed in <ref type="bibr" coords="2,341.01,135.93,15.49,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,360.93,135.93,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,378.09,135.93,12.73,8.74" target="#b12">13]</ref> centers on a noisy-channel model of the QA problem and generally speaking relies on the redundancy of answer data in the target corpus in order to identify and extract correct answers.</p><p>Our QAst system comprises two major components, an Information Retrieval (IR) module used to identify and retrieve relevant sentences from a large corpus, and an Answer Extraction module which is used to identify and rank exact answers in the sentences returned by the IR module. Our approach, which is data-driven and does not require human-guided interaction except for the development of a short list of frequent stop words and common question words, makes it possible to rapidly develop new systems for a wide variety of different languages. Furthermore performance accuracy is roughly comparable even across very disparate languages such as English and Japanese, and developers need not have more than a perfunctory acquaintance with the language <ref type="bibr" coords="2,474.63,255.48,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,488.56,255.48,12.73,8.74" target="#b13">14]</ref> in order to build and deploy a new system.</p><p>Our data-driven approach differs substantially from conventional rule-based approaches, yet it does share certain features with other approaches in the literature <ref type="bibr" coords="2,385.40,291.35,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,399.60,291.35,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,411.03,291.35,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,422.46,291.35,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,433.88,291.35,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,445.31,291.35,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,456.75,291.35,11.62,8.74" target="#b9">10]</ref>. Systems which employ similar answer-typing approaches have lately begun to appear <ref type="bibr" coords="2,435.05,303.30,9.96,8.74" target="#b6">[7]</ref>, however most of these systems still utilize some form of specific linguistic knowledge in contrast to our alldata driven, non-linguistic, classification approach. Although our approach requires that a small number of parameters be optimized to minimize the effects of data sparsity, these parameters are all determined at system initialization time and are invariant across different questions. This means that new data or system settings can be applied without the need for wearisome model re-training.</p><p>Due to its data-driven nature our QA system performs best when there are numerous redundant sentences containing the correct answer and question words. This reliance on data redundancy to help identify correct answers has seldom been a source of difficulty in past evaluations, however the QAst pilot track presented a unique challenge due to the relatively small size of the CHIL lectures target corpus. In other closed domain evaluations with medium-sized corpora we have opted to utilize web data, however this did not seem entirely appropriate for the QAst track due to the spoken nature of the data and very small corpus size. In part to help combat the resulting data sparsity, we employed a new language-modeling based sentence retrieval IR module as a precursor to the Answer Extraction (AE) stage. This sentence retrieval module acts as an intermediate filter and helps to eliminate noise usually contained in the larger original documents.</p><p>The rest of the paper is structured as follows. Section 2 describes our QA architecture in detail. In section 3 we detail our experimental setup. Section 4 describes our results and Section 5 presents a brief discussion of the results. Finally, section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QA Architecture for QAst</head><p>The answer to a question depends primarily on the question itself but also on many other factors such as the identity and location of the questioner, previous questions, social context and so on. Although such factors are clearly relevant in many situations, they are difficult to model and also to test. In our approach to QA we therefore limit ourselves to modeling the most straightforward dependence, the probability of an answer A given the question Q. In the system used for the QAst evaluation, we divide the work of identifying answers between two major modules, the Information Retrieval (IR) module which employs an LM-based approach to sentence retrieval, and the Answer Extraction (AE) module. We briefly describe the IR module, the AE module and the Query Expansion process below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information Retrieval module</head><p>The general approach to IR for QA is to treat the question as a standard search query, but discard question-type words such as "what", "when", "who", etc., and possibly also a set of stop words.</p><p>We employ a language modeling approach to this problem where an individual LM is estimated for each document. The documents are then ranked according to the conditional probability P (Q|D), the probability of generating the query Q given the document D.</p><p>In our system we employ a sentence-based retrieval approach similar to that described in <ref type="bibr" coords="3,499.72,147.89,9.96,8.74" target="#b4">[5]</ref>, where each document comprises only one sentence. Due to lack of data to train the sentence specific LMs, all words are treated as independent, and a unigram model is applied,</p><formula xml:id="formula_0" coords="3,253.39,192.54,259.61,31.18">P (Q|S) = |Q| i=1 P (q i |S),<label>(1)</label></formula><p>where q i is the ith query term in the query Q = (q 1 ...q |Q| ) composed of |Q| query terms. Throughout this paper we calculate the probability of a query term q given a sentence S in three different ways: P 1 (q|S), P 2 (q|S) and P 3 (q|S), as explained below.</p><p>We use absolute discounting in order to smooth the otherwise sparse LMs, where the probability of a query term q given a sentence S is calculated as:</p><formula xml:id="formula_1" coords="3,177.99,314.30,335.02,22.31">P 1 (q|S) = max{tf (q, S) -δ, 0} l(S) + δ • h(S, δ) l(S) • P (q|B),<label>(2)</label></formula><p>where tf (q, S) is the term frequency of q in S, l(S) is the length (number of words) of S, δ is the discount parameter, h(S, δ) is the count of how many unique words in S have a term frequency higher than δ, and P (q|B) is the unigram probability of the query term q according to the background collection model. Note that if δ &lt; 1 then h(S, δ) is equal to the number of unique words in S.</p><p>A problem with the model presented in <ref type="bibr" coords="3,284.27,408.15,10.51,8.74" target="#b4">[5]</ref> is that words relevant to the sentence might not occur in the sentence itself, but in the surrounding text. For example, for the question "Who is Tom Cruise married to?", the sentence "He is married to Katie Holmes" in an article about Tom Cruise should ideally be assigned a high probability, despite the sentence missing the words "Tom" and "Cruise". To account for this, we train document LMs, P 1 (q|D), in the same manner as for P 1 (q|S) in Eq. ( <ref type="formula" coords="3,187.29,467.92,3.88,8.74" target="#formula_1">2</ref>), and perform a linear interpolation between P 1 (q|S) and P 1 (q|D):</p><formula xml:id="formula_2" coords="3,211.10,489.42,301.90,9.65">P 2 (q|S) = (1 -α) • P 1 (q|S) + α • P 1 (q|D),<label>(3)</label></formula><p>where 0 ≤ α ≤ 1 is an interpolation parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query expansion</head><p>In order to help further improve QA performance we experiment with a global query expansion method in which words are grouped into a set C = {c 1 ...c |C| } of |C| overlapping classes beforehand, and calculate the unigram class model probability of a query term q given a sentence S as follows:</p><formula xml:id="formula_3" coords="3,231.75,601.78,281.25,31.18">P C (q|S) = |C| j=1 P (q|c j ) • P (c j |S),<label>(4)</label></formula><p>where P (q|c j ) = 1/|c j | if q ∈ c j , else P (q|c j ) = 0, where |c j | is the number of words in c j . P (c j |S) can be re-written as a sum over the |V | words in the vocabulary V = {w 1 ...w |V | }:</p><formula xml:id="formula_4" coords="3,227.13,679.47,285.87,31.41">P (c j |S) = |V | k=1 P (c j |w k ) • P (w k |S),<label>(5)</label></formula><p>where The word LM in Eq.( <ref type="formula" coords="4,198.74,112.02,4.59,8.74" target="#formula_1">2</ref>) and the class LM in Eq.( <ref type="formula" coords="4,319.79,112.02,4.59,8.74" target="#formula_3">4</ref>) are combined using linear interpolation:</p><formula xml:id="formula_5" coords="3,119.57,721.73,157.22,9.65">P (c j |w k ) = 1/N (w k , C) if w k ∈ c j ,</formula><formula xml:id="formula_6" coords="4,207.77,133.94,305.24,9.65">P int (q|S) = (1 -β) • P 1 (q|S) + β • P C (q|S),<label>(6)</label></formula><p>where 0 ≤ β ≤ 1 is an interpolation parameter. P int (q|D) is calculated in a similar manner. Eq.( <ref type="formula" coords="4,316.24,167.81,4.59,8.74" target="#formula_1">2</ref>) is then adjusted to give P 3 (q|S) as follows:</p><formula xml:id="formula_7" coords="4,205.02,189.73,307.98,9.65">P 3 (q|S) = (1 -γ) • P int (q|S) + γ • P int (q|D),<label>(7)</label></formula><p>where 0 ≤ γ ≤ 1 is an interpolation parameter. For all QAst evaluation runs, either P 2 or P 3 were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>The AE module models the probability of an answer A given a question Q as:</p><formula xml:id="formula_8" coords="4,252.47,291.80,256.29,8.74">P (A|Q) = P (A|W, X), (<label>8</label></formula><formula xml:id="formula_9" coords="4,508.76,291.80,4.24,8.74">)</formula><p>where W is a set of features describing the question-type part of Q, such as "when", "why" and "how", etc., while X is a set of features describing the information-bearing part of of Q, i.e. what the question is about and what it refers to. For example, in the questions "Where was Tom Cruise married?" and "When was Tom Cruise married", the information-bearing parts are identical while the question-type parts differ. Finding the best answer Â involves a search over all A for the one which maximizes the probability of the above model:</p><formula xml:id="formula_10" coords="4,250.25,392.89,262.75,17.10">Â = arg max A P (A|W, X).<label>(9)</label></formula><p>Using Bayes' rule and making various conditional independence and uniform prior distribution assumptions, Eq. ( <ref type="formula" coords="4,171.76,432.66,4.24,8.74" target="#formula_10">9</ref>) can be rearranged to give:</p><formula xml:id="formula_11" coords="4,242.90,454.58,270.10,14.58">arg max A P (A|X) • P (W |A),<label>(10)</label></formula><p>where P (A|X) is termed the answer retrieval model and P (W |A) the answer filter model. P (A|X) essentially models the proximity of A to features in X. P (W |A) can be viewed as a LM that models the probability of the question-type features W given a candidate answer A.</p><p>We will not examine the answer retrieval model and the answer filter model further, see <ref type="bibr" coords="4,497.51,516.30,15.49,8.74" target="#b14">[15]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup for QAst</head><p>We participated in task T1: QA in manual transcriptions of lectures, and task T2: QA in automatic transcriptions of lectures. For the official evaluation we used the data released for the QAst evaluation task T1 and task T2. This data comprised a development set and an evaluation set with characteristics described in Table <ref type="table" coords="4,264.59,618.89,3.88,8.74" target="#tab_0">1</ref>. The development set consisted of manual transcripts (MAN) and ASR-based transcripts (ASR) for 10 lectures, a set of questions, and a set of answers for each transcript set. The evaluation set consisted of MAN and ASR for 15 lectures, and a set of 100 questions. The development and evaluation data did not overlap. All questions were of one of the following answer types: person, location, organization, language, system/method, measure, time, color, shape, and material. Word lattices were also made available however, after preliminary experiments with the development data revealed minor inconsistencies between the lattices and ASR, we chose not to use any of the lattices in the actual evaluation. No audio was provided.</p><p>We cleaned the data by automatically removing fillers and pauses, and performed simple text processing of abbreviations and numerical expressions using perl's Lingua CPAN module to ensure consistency between ASR, MAN, questions and answers. ASR documents were sentence segmented according to the sentence boundaries provided, and MAN was sentence segmented using an inhouse segmenter developed by one of the authors. Our system is not able to identify whether the answer to a question can be found in the corpus, thus we chose never to return a "nil" response for any question.</p><p>For retrieval purposes we filtered out question-type words and stop words (in total 28 words) from the questions. Using the remaining words as query terms, we ranked sentences according to either P 2 (q|S) or P 3 (q|S), depending on the run. We optimized weights on the development set and used these weights for the official evaluation.</p><p>Classes for query expansion were generated based on the overlap in features, which are computed using standard mutual information techniques, for each word in the vocabulary based on a large text corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QAst Evaluation Results</head><p>Question sets for both task T1 and task T2 comprised the same 100 factoid questions, however 2 of these questions were deemed faulty by the coordinators following submissions and were removed prior to making assessments, resulting in a total of 98 evaluation questions. Our system returned a maximum of 5 answer candidates per question per run. We submitted two (2) runs each for task T1 and task T2. For both tasks, P 2 (q|S) was used for the first run and P 3 (q|S) was used for the second run. In addition to our group, four other teams participated. Table <ref type="table" coords="5,426.49,458.78,4.98,8.74" target="#tab_1">2</ref> details the official best-run results for the entire field for task T1. participants on the manual transcription task, T1. Our team name is tokyo2.</p><p>As can be seen in table <ref type="table" coords="5,204.87,622.06,4.98,8.74" target="#tab_1">2</ref> our system achieved a best run MRR of 0.20, and was able to correctly answer 34 of 98 questions on the manual data set, placing us third overall. Results for the ASR transcripts were lower, as expected, at 18 correct answers for 98 questions, however other systems showed similar losses on the ASR data. Table <ref type="table" coords="5,291.20,657.93,4.98,8.74" target="#tab_2">3</ref> shows a comparison of our group's manual versus ASR results by submission. P 2 (q|S) was used for runs tokyo1 t1 and tokyo1 t2, while P 3 (q|S) was used for runs tokyo2 t1 and tokyo2 t2. As can be seen, query expansion employed by P 3 (q|S) slightly improved our Top5 scores, but had no effect on Top1 accuracy. There was a performance drop of approximately 44% for results based on the top 5 answers using P 3 (q|S) and a drop of approximately 43% for results based on the top 1 answer for both P 2 (q|S) and P 3 (q|S). Similar drops were reflected in other participants results however, and we suspect that this primarily reflects ASR errors. Table <ref type="table" coords="6,133.42,247.90,4.98,8.74" target="#tab_3">4</ref> gives an accuracy break-down by answer type for task T1 and T2 for both of our submissions. Table <ref type="table" coords="6,175.40,259.86,4.98,8.74" target="#tab_3">4</ref> also shows that performance loss is generally consistent regardless of answer type, which serves as further evidence that the loss is due primarily to ASR errors. In several of these cases the answers to questions in the ASR transcripts were represented by mis-recognized tokens, this considerably magnified the difficulty of extracting the proper token for the question. </p><formula xml:id="formula_12" coords="6,156.48,318.35,17.45,8.77">run</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Analysis</head><p>Our results from task T1 compare favorably with results from previous CLEF and TREC evaluations, despite the size and relative lack of redundancy in the target CHIL lectures corpus. Additional experiments on this corpus which are documented in a paper that is currently pending publication show that our system is able to correctly select the sentence containing the answer over 50% of the time, indicating that there is upwards of a 20% performance loss between the sentence retrieval and answer extraction stages. While performance across different answer types was fairly consistent, there was a conspicuous gap for the time type, where we did not answer any of the related questions correctly. Analysis of the data indicates that this was caused by multiple factors. There were two time questions for which there was no appropriate answer in the document corpus. There was also a problem with automatically normalizing complex dates which the perl Lingua module was not particular consistent, and as our system generally performs better when times and dates are represented as digits, this made it difficult to correctly extract answers such as "nineteen ninety-eight". Finally, there was at least one time question for which the question itself did not clearly specify the type.</p><p>Finally, we observed a considerable drop in performance between task T1 and task T2, which was similarly mirrored in all other participants' results. We surmise that in our case this was mainly due to answer typing issues resulting from ASR errors since answer words of the correct answer type are crucial for good AE performance in our system. This can be explained by the way the answer filter model (Section 2.3) works: if the answer words in ASR are of the wrong answer type, then P (W |A) will assign a low probability to the correct answer candidate.</p><p>In this paper we have presented our results from the CLEF 2007 QAst pilot track for task T1 and T2, and described our system and experimental setup for the evaluation. In general our results compare favorably with past evaluations, and place us in the middle of the field for this evaluation. We noticed considerable performance drops between the manual transcripts and ASR transcripts, but because these drops were consistent across submissions and participants we are led to believe that this is mainly a result of ASR errors. In future evaluations we think it would be preferable to supply both recognition lattices which consistently match the ASR transcripts, and to be able to use the actual audio. Given that the real aim of this track is to find answers to natural language, factoid questions in spoken documents, having access to these resources might provide greater opportunities for teams to directly exploit the source data in more interesting way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Online demonstration</head><p>A demonstration of the system using model ONE supporting questions in English, Japanese, Chinese, Russian, French, Spanish and Swedish can be found online at http://www.inferret.com/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,281.07,721.73,231.94,9.65;3,90.00,733.69,423.00,9.65;3,90.00,745.64,49.08,8.74"><head></head><label></label><figDesc>else P (c j |w k ) = 0 where N (w k , C) is the number of classes in C where w k occurs. P (w k |S) is the unigram probability of the word w k given the sentence S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,110.82,423.01,68.86"><head>Table 1 :</head><label>1</label><figDesc>Number of lectures, number of sentences, number of words, word error rate and number of questions for each data set after preprocessing.</figDesc><table coords="5,172.89,110.82,257.23,35.04"><row><cell cols="6">Data Set #Lect. #Sent. #Words WER #Quest.</cell></row><row><cell>Dev. Set</cell><cell>10</cell><cell>2966</cell><cell>54633</cell><cell>32%</cell><cell>45</cell></row><row><cell>Eval. Set</cell><cell>15</cell><cell>2917</cell><cell>50986</cell><cell>28%</cell><cell>86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,491.46,423.00,92.77"><head>Table 2 :</head><label>2</label><figDesc>Out of 98, number of correct answers in the top 5, MRR, and top1 answer accuracy for all</figDesc><table coords="5,177.06,491.46,248.88,70.90"><row><cell cols="5">Team ID Questions Returned Top5 MRR %Top1</cell></row><row><cell>clt1</cell><cell>98</cell><cell>16</cell><cell>0.09</cell><cell>0.06</cell></row><row><cell>dfki1</cell><cell>98</cell><cell>19</cell><cell>0.17</cell><cell>0.15</cell></row><row><cell>limsi2</cell><cell>98</cell><cell>56</cell><cell>0.46</cell><cell>0.39</cell></row><row><cell>tokyo2</cell><cell>98</cell><cell>34</cell><cell>0.20</cell><cell>0.14</cell></row><row><cell>upc1</cell><cell>98</cell><cell>54</cell><cell>0.53</cell><cell>0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,110.82,422.50,103.53"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our manual and ASR results, and relative performance loss due to ASR using P2</figDesc><table coords="6,90.00,110.82,291.74,103.53"><row><cell>tokyo2</cell><cell cols="3">MAN ASR Perf.Loss</cell></row><row><cell>Top5(P 2 )</cell><cell>32</cell><cell>17</cell><cell>47%</cell></row><row><cell>Top5(P 3 )</cell><cell>34</cell><cell>18</cell><cell>44%</cell></row><row><cell>Top1(P 2 )</cell><cell>14</cell><cell>8</cell><cell>43%</cell></row><row><cell>Top1(P 3 )</cell><cell>14</cell><cell>8</cell><cell>43%</cell></row><row><cell>MRR</cell><cell>0.20</cell><cell>0.12</cell><cell>N/A</cell></row><row><cell>(tokyo1) and P3 (tokyo2).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,318.35,423.00,79.65"><head>Table 4 :</head><label>4</label><figDesc>Break-down of results by answer type and task. org=organization, per=person, loc=location, tim=time, mea=measure, met=method, lan=language.</figDesc><table coords="6,164.77,318.35,281.75,47.02"><row><cell cols="9">tokyo2 org per loc tim mea met lan total</cell></row><row><cell>totals</cell><cell>20</cell><cell>9</cell><cell>9</cell><cell>10</cell><cell>28</cell><cell>18</cell><cell>4</cell><cell>98</cell></row><row><cell>task T1</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>0</cell><cell>12</cell><cell>5</cell><cell>2</cell><cell>34</cell></row><row><cell>task T2</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>0</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>18</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>This research was supported in part by the <rs type="funder">Japanese government</rs>'s <rs type="programName">21st century COE programme</rs>: "<rs type="projectName">Framework for Systematization and Application of Large-scale Knowledge Resources</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_nSJsSjb">
					<orgName type="project" subtype="full">Framework for Systematization and Application of Large-scale Knowledge Resources</orgName>
					<orgName type="program" subtype="full">21st century COE programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,429.33,402.52,7.86;7,110.48,440.29,402.52,7.86;7,110.48,451.25,354.91,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,364.37,429.33,148.63,7.86;7,110.48,440.29,145.00,7.86">Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,277.44,440.29,235.56,7.86;7,110.48,451.25,260.60,7.86">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,461.91,402.52,7.86;7,110.48,472.86,402.52,7.86;7,110.48,483.82,21.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,263.60,461.91,230.29,7.86">An Analysis of the AskMSR Question-answering System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.48,472.86,397.09,7.86">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,494.48,402.51,7.86;7,110.48,505.44,173.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,227.49,494.48,206.58,7.86">A Noisy-Channel Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,455.89,494.48,57.10,7.86;7,110.48,505.44,144.15,7.86">Proceedings of the 41st Annual Meeting of the ACL</title>
		<meeting>the 41st Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,516.09,402.51,7.86;7,110.48,527.05,167.90,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,232.42,516.09,227.63,7.86">IBM&apos;s Statistical Question Answering System-TREC-11</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,479.82,516.09,33.17,7.86;7,110.48,527.05,139.58,7.86">Proceedings of the TREC 2002 Conference</title>
		<meeting>the TREC 2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,537.70,402.52,7.86;7,110.48,548.66,169.59,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,217.42,537.70,295.58,7.86;7,110.48,548.66,40.15,7.86">Comparing Improved Language Models for Sentence Retrieval in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,170.70,548.66,80.24,7.86">Proceedings of CLIN</title>
		<meeting>CLIN</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,559.32,402.52,7.86;7,110.48,570.28,402.53,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,357.43,559.32,155.57,7.86;7,110.48,570.28,197.70,7.86">NTCIR-6 CLQA Question Answering Experiments at the Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.04,570.28,157.97,7.86">Proceedings of the NTCIR-6 Conference</title>
		<meeting>the NTCIR-6 Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,580.93,402.52,7.86;7,110.48,591.89,77.08,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,215.26,580.93,147.28,7.86">A Probabilistic Answer Type Model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinchak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.38,580.93,121.25,7.86">European Chapter of the ACL</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,602.54,402.52,7.86;7,110.48,613.50,288.85,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,314.10,602.54,181.45,7.86">Probabilistic Question Answering on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.48,613.50,206.57,7.86">Proc. of the 11th international conference on WWW</title>
		<meeting>of the 11th international conference on WWW<address><addrLine>Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,624.16,402.51,7.86;7,110.48,635.12,402.52,7.86;7,110.48,646.08,68.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,310.87,624.16,202.12,7.86;7,110.48,635.12,54.83,7.86">Statistical QA-Classifier vs. Re-ranker: What&apos;s the difference</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,187.59,635.12,325.40,7.86;7,110.48,646.08,40.12,7.86">Proceedings of the ACL Workshop on Multilingual Summarization and Question Answering</title>
		<meeting>the ACL Workshop on Multilingual Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,656.73,402.52,7.86;7,110.48,667.69,180.23,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,209.51,656.73,211.36,7.86">Automatic Question Answering: Beyond the Factoid</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,440.86,656.73,72.14,7.86;7,110.48,667.69,151.90,7.86">Proceedings of the HLT/NAACL 2004: Main Conference</title>
		<meeting>the HLT/NAACL 2004: Main Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,678.34,402.53,7.86;7,110.48,689.30,365.79,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,320.48,678.34,192.53,7.86;7,110.48,689.30,119.35,7.86">TREC2005 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,249.78,689.30,198.16,7.86">Proceedings of the 14th Text Retrieval Conference</title>
		<meeting>the 14th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,699.96,402.53,7.86;7,110.48,710.92,263.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,281.40,699.96,231.60,7.86;7,110.48,710.92,108.15,7.86">A Statistical Pattern Recognition Approach to Question Answering using Web Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,238.71,710.92,106.76,7.86">Proceedings of Cyberworlds</title>
		<meeting>Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.47,721.57,402.53,7.86;7,110.48,732.53,185.11,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,285.91,721.57,227.09,7.86;7,110.48,732.53,40.15,7.86">A Unified Approach to Japanese and English Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hamonic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,170.70,732.53,95.73,7.86">Proceedings of NTCIR-5</title>
		<meeting>NTCIR-5</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,112.70,402.53,7.86;8,110.48,123.66,377.86,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,387.00,112.70,126.01,7.86;8,110.48,123.66,184.32,7.86">CLEF2006 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.75,123.66,45.22,7.86">CLEF 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,134.31,402.53,7.86;8,110.48,145.27,264.75,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,316.16,134.31,196.84,7.86;8,110.48,145.27,119.35,7.86">TREC 2006 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,249.78,145.27,96.22,7.86">Proceedings of TREC-15</title>
		<meeting>TREC-15</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
