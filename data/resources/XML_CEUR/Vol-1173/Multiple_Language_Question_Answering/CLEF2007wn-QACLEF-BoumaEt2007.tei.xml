<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.58,148.86,391.86,15.15;1,497.42,146.25,5.98,10.48">Question Answering with Joost at CLEF 2007 *</title>
				<funder>
					<orgName type="full">Dutch Organisation for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,203.33,182.75,56.43,8.74"><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
							<email>g.bouma@rug.nl</email>
						</author>
						<author>
							<persName coords="1,268.68,182.75,81.37,8.74"><forename type="first">Geert</forename><surname>Kloosterman</surname></persName>
						</author>
						<author>
							<persName coords="1,358.24,182.75,36.10,8.74"><forename type="first">Jori</forename><surname>Mur</surname></persName>
						</author>
						<author>
							<persName coords="1,163.50,196.70,80.37,8.74"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
						</author>
						<author>
							<persName coords="1,252.17,196.70,93.62,8.74"><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
						</author>
						<author>
							<persName coords="1,368.49,196.70,71.02,8.74"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Information Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.58,148.86,391.86,15.15;1,497.42,146.25,5.98,10.48">Question Answering with Joost at CLEF 2007 *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B25D4D01FD38244A67962DC0C6B2A58A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>J.5 [Arts and Humanities]: Language translation</term>
					<term>Linguistics Algorithms, Measurement, Performance, Experimentation Question Answering, Dutch, Wikipedia, Information Retrieval, Query Expansion, Anaphora Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our system for the monolingual Dutch and multilingual English to Dutch QA tasks. First, we present a brief overview of our QA-system, which makes heavy use of syntactic information. Next, we describe the modules that were developed especially for CLEF 2007, i.e. preprocessing of Wikipedia, inclusion of query expansion in IR, anaphora resolution in follow-up questions, and a question classification module for the multilingual task. We achieved 25.5% accuracy for the Dutch monolingual task, and 13.5% accuracy for the multilingual task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Question Answering task for CLEF 2007 contained two innovations. First, the document collection was extended with Wikipedia, the online encyclopedia that is available for many different languages. As described in section 3, we preprocessed the XML source files for this document collection so that we could index it adequately for the purposes of Information Retrieval. In addition, we extracted all relevant plain text, and parsed it.</p><p>Second, the test questions were grouped in topics. Within a topic, questions might refer to or presuppose information from previous questions or answers to these questions. We developed a simple anaphora resolution system (described in section 5) that detects anaphoric elements in a question, and tries to find a suitable antecedent in the first question of a topic, or in the answer to that question.</p><p>In addition to these innovations, we also improved the Information Retrieval component of our QA system. In section 4, we show that query expansion based on (automatically acquired) synonym-lists and blind relevance feedback improves the mean reciprocal rank of the IR module.</p><p>In section 6 we describe a question classification module for the multilingual QA system, which uses both the question class assigned to the English source question and the class assigned to the automatically translated Dutch target question. This leads to a modest improvement.</p><p>The results of our system are discussed in section 7, and some suggestions for future work are given in section 8.</p><p>2 Joost: A QA system for Dutch Joost <ref type="bibr" coords="2,116.08,250.47,88.97,8.74">(Bouma et al., 2005)</ref> is a question answering system for Dutch which is characterized by the fact that it relies on syntactic analysis of the question as well as the documents in which answers need to be found. The complete document collection is parsed by Alpino <ref type="bibr" coords="2,405.67,274.38,107.33,8.74;2,90.00,286.34,59.02,8.74" target="#b3">(Bouma, van Noord, and Malouf, 2001)</ref>, a wide-coverage dependency parser for Dutch. The resulting depedency trees are stored as XML. Answers are extracted by pattern matching over syntactic dependency relations, and potential answers are ranked, among others, by computing the syntactic similarity between the question and the sentence from which the answer is extracted.</p><p>The architecture of our system is depicted in figure <ref type="figure" coords="2,335.27,334.16,3.88,8.74" target="#fig_0">1</ref>. Apart from the standard components question analysis, passage retrieval, answer extraction and answer ranking, the system also contains a component called Qatar, which collects all answers to questions of a specific type (i.e. birthdates) off-line. Answers to questions for which a Qatar-table exists are found by means of table look-up.  The first processing stage is question analysis. The input to the question analysis component is a natural language question in Dutch, which is parsed by Alpino. The goal of question analysis is to determine the question type and to identify keywords in the question. Depending on the question type the next stage is either passage retrieval or table look-up (using Qatar). If the question type matches one of the table categories, it will be answered by Qatar. Qatar consists of a number of manually written syntactic patterns for extraction of interesting relations (i.e. creator-object tuples such as Heinrich Mann -Der Untertan). Recall is improved by using a set of equivalence rules for syntactic dependency patterns <ref type="bibr" coords="3,329.62,159.84,159.44,8.74">(Bouma, Mur, and van Noord, 2005)</ref>, and by using anaphora resolution <ref type="bibr" coords="3,222.46,171.80,51.55,8.74" target="#b9">(Mur, 2006)</ref>. Using these patterns, the parsed corpus is searched exhaustively, and all extracted relation tuples are stored in tables. We defined patterns for 20 relations. Using these patterns, almost 400K relation instances were extracted from the Dutch Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Analysis</head><p>For all questions that cannot be answered by Qatar, we follow the other path through the QAsystem to the passage retrieval component. Instead of retrieving full documents, the IR module retrieves passages (see section 3). The 40 most relevant passages retrieved by IR are passed on to the answer extraction module. Here, we use patterns similar to those used by Qatar, but now slightly more general, to find the actual answer strings. Per sentence, at most one potential answer string is selected.</p><p>The final step is answer selection. Given a list of potential answers from Qatar or the IRbased QA module, the most promising answer is selected. Answers are ranked using various features, such as syntactic overlap between question and answer sentence, word overlap, proper name overlap, the reliability of the pattern used to extract the answer, and the frequency of the answer. The answer ranked first is returned to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preprocessing Wikipedia</head><p>New in this year's CLEF QA tracks was the inclusion of Wikipedia in the corpus. The Wikipedia corpus is different from the newspaper texts that were used so far in a number of ways. First of all, whereas the newspaper collection is relatively redundant (there are two newspapers covering the same period in the Dutch collection, and news stories tend to contain a fair amount of repetition), this is far less the case for the encyclopedia, which contains many facts that are mentioned only in one article. Thus, we expect redundancy-based techniques (typically using patterns that are noisy but provide high recall in combination with frequency-based ranking of results) to less effective for Wikipedia. Second, Wikipedia consists of structured web-documents, containing many lists, tables, and cross-references. In the newspaper collection, only article titles, and paragraphs are provided. By mining the structure of Wikipedia documents, it is possible to extract a large number of facts that cannot be found using syntactic patterns. Due to time constraints, we applied only the syntactic patterns that were developed for the newspaper collection.</p><p>An XML-version of the Dutch Wikipedia was provided by the University of Amsterdam.<ref type="foot" coords="3,489.60,535.82,3.97,6.12" target="#foot_0">1</ref> For IR and parsing, we were interested in obtaining just the text in each of the articles. We developed a series of stylesheets which removes material that was irrelevant for our task (i.e. navigation and pictures), and which returns the remaining content as highly simplified XML, containing only information that is required to identify the segmentation of the text into titles, sections, and lists. The segmentation is used in the IR index. From the simplified XML, plain text can be extracted easily. The result is tokenized and split into 4.7 million sentences. The sentences were parsed with the Alpino-parser.</p><p>The Qatar relation extraction module searches the corpus exhaustively for interesting facts, and stores these facts in a database. For Wikipedia, we used the patterns as they were developed for the newspaper corpus, with only minor modifications. In particular, we did not try to extract facts from lists, or using the XML structure.</p><p>Our IR system retrieves passages rather than complete articles. For previous CLEF tasks, we used the existing paragraph markup in the newspaper data to split documents into passages to be retrieved. For Wikipedia, similar markup exists but often refers to very small units, in many cases only single sentences. The Dutch Wikipedia corpus contains about 4.7 million sentences split into about 2 million units. Single sentence units usually correspond to headers and subsection headers which often contain important keywords that match well with given queries. Unfortunately, including these as separate passages results in a strong preference for these units when retrieving passages. To avoid this we implemented a simple solution that merges consecutive units until they are bigger than a pre-defined size of 200 characters.</p><p>Despite its simplicity this approach works sufficiently well and made it possible to easily integrate the new Wikipedia corpus into our IR index. The same approach has also been applied to the newspaper corpus in order to create and index with a uniform segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Passage Retrieval with and without query expansion</head><p>In CLEF 2007 we submitted two runs of our system, applying two different settings of the information retrieval (IR) component which is used to retrieve relevant passages for a given question. The main difference between these two settings is the inclusion of query expansion techniques in one of them.</p><p>Common to both settings is the approach, previously described in <ref type="bibr" coords="4,391.68,298.29,77.67,8.74" target="#b12">Tiedemann (2005)</ref>, in which linguistic features have been integrated in the IR index. IR queries are constructed from questions using various features and feature combinations. Furthermore, keyword selection constraints are introduced using part-of-speech tags (POS) and dependency relation types (rel). For each keyword type a separate weight is used to optimize retrieval performance. Furthermore, we also use proximity queries requiring terms within a given text window. Keyword weights and window sizes have been trained on questions and answers from the CLEF QA tracks in 2003 and 2005 using a genetic algorithm. The mean reciprocal rank of relevant passages retrieved has been improved from 0.52 (using standard plain text keywords) to 0.62 (including linguistic features and optimized settings) for questions from the training set and from 0.49 to 0.57 on unseen evaluation data <ref type="bibr" coords="4,459.03,405.89,53.97,8.74;4,90.00,417.85,42.62,8.74">(CLEF 2004 questions)</ref>. Details of the optimization procedure are discussed in <ref type="bibr" coords="4,379.47,417.85,78.12,8.74" target="#b12">Tiedemann (2005)</ref>.</p><p>The second run includes various forms of query expansion. The main purpose of expanding the query is to increase recall of the passage retrieval component in order to minimize the risk of missing relevant information. We experimented with two general techniques for query expansion: global methods using fixed lists and local techniques using blind relevance feedback. For the latter we applied an implementation of the Rocchio algorithm for Lucene, LucQE <ref type="bibr" coords="4,447.87,477.62,65.14,8.74" target="#b11">(Rubens, 2007;</ref><ref type="bibr" coords="4,90.00,489.58,62.51,8.74" target="#b10">Rubens, 2006)</ref>, which we adapted to our purposes. Relevance feedback is known to be most useful for increasing recall. In blind relevance feedback (also called pseudo-relevance feedback) user interaction is simulated by simply selecting the highest ranked documents as the positive examples and ignoring negative ones. Rocchio is used to re-weight existing keywords and also to add new terms from the positive examples. We restricted this type of re-weighting and keyword expansion for the plain text field only and a maximum of 10 new keywords. The top five documents were used as positive examples and the Rocchio parameters where set to common values used in the literature (α = 1 and β = 0.75). Furthermore, we used a fixed decay value of 0.1 for decreasing the importance of documents selected for feedback.</p><p>Furthermore, we used global expansion techniques using several lists of expansion terms. Firstly, we used redirects from the Dutch Wikipedia. Redirects link search terms similar to existing Wikipedia lemmas to corresponding articles. Redirects mainly cover spelling variations but also include various synonyms. Secondly, we used synonyms of nouns, verbs, and adjectives automatically extracted from word-aligned parallel corpora (van der <ref type="bibr" coords="4,390.55,644.99,118.02,8.74">Plas and Tiedemann, 2006)</ref>. For this, we aligned the Europarl corpus <ref type="bibr" coords="4,273.96,656.95,63.03,8.74" target="#b6">(Koehn, 2003)</ref> with its 11 languages and used aligned translations of Dutch words as features in a distributional similarity approach. Using this technique we obtained 13,896 near-synonyms for 6,968 Dutch nouns, 3,207 near-synonyms for 1,203 verbs and 3,556 near-synonyms for 1,621 adjectives. More details about the algorithm used for the extraction can be found in van der <ref type="bibr" coords="4,241.05,704.77,117.80,8.74">Plas and Tiedemann (2006)</ref>. Thirdly, we included isa-relations of named entities extracted from syntactically annotated monolingual corpora (van der Plas and <ref type="bibr" coords="4,90.00,728.68,58.93,8.74">Bouma, 2005)</ref>. The parsed document collection contains over 2 million instances of an apposition relation between a noun and a named entity (i.e. the composer Aaron Copland), where the noun provides an isa-label for the named entity. After filtering infrequent combinations (often caused by parsing errors), we are left with almost 400K unique tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Anaphora Resolution for Follow-Up Questions</head><p>A new feature in the 2007 QA task are follow-up questions. Questions are grouped in topics, consisting of a number of questions. Answering non-initial questions may require information from previous questions or answers to previous questions. The TREC QA task has included follow-up questions for a number of years. As no development data was available for the CLEF task, we used English examples from previous TREC QA tasks for inspiration.<ref type="foot" coords="5,442.10,224.99,3.97,6.12" target="#foot_1">2</ref> Note however that in TREC descriptive topics are explicitly provided, whereas in CLEF only an numeric topic id is given.</p><p>The most important aspect of follow-up questions is anaphora resolution, i.e. the process of detecting anaphoric phrases that depend on a previous antecedent expression for their interpretation, and assigning a correct antecedent to them.</p><p>A noun phrase was considered to be anaphoric if it was a personal (1-b) or impersonal (2-b) pronoun, a possessive pronoun (1-c), a deictic pronoun (3-b), an NP introduced by a deictic determiner (4-b), or an NP introduced by a definite determiner and not containing any modifiers (5-b).</p><p>(1) a. When was Napoleon born? b. Which title was introduced by him? c. Who were his parents?</p><p>(2) a. What is the KNMI? b. When was it founded?</p><p>(3) a. What is an ecological footprint? b. When was this introduced?</p><p>(4) a. Who lead the Russian Empire during the Russion-Turkish War of 1787-1792? b. Who won this war?</p><p>(5) a. Since when is Cuba ruled by Fidel Castro? b. When was the flag of the country designed?</p><p>Antecedents were restricted to named entities from the first question/answer pair of a topic. The answer was chosen as antecedent if the initial question was one of a limited number of question types which ask for a named entity (i.e. what is the capital of, who wrote/founded/.. , who is the chair/president/.. of ). In other cases, the first named entity from the question was chosen. We adopted this naive approach mostly because we lacked data to test and evaluate more sophisticated approaches. Note also that quite a few TREC systems limit anaphora resolution to resolving anaphoric expressions to the topic of the question (see <ref type="bibr" coords="5,373.20,580.44,80.51,8.74" target="#b5">Hickl et al. (2006)</ref> for a notable exception), apparently with reasonable success. Our anaphora resolution system operates on the result of the syntactic dependency parse of the sentence. If anaphora resolution applies, and an antecedent is found, the set of dependency relations for the question is extended with dependency relations for the antecedent. That is, given an Anaphor resolved to Antecedent, for each dependency relation Head, Rel, Anaphor in the question, we add a dependency relation Head, Rel, Antecedent . Note that, as the IR system described in the previous section constructs queries on the basis of the dependency parse of the question, this ensures that the Antecedent is also included in the IR query.</p><p>According to our inspection of the best monolingual run, there were 56 questions which required anaphora resolution. For 29 questions (52%), a correct antecedent for an anaphoric expression was found. In 15 cases (27%), a wrong antecedent was given. An important source of errors were cases where the answer to the initial question was correctly chosen as antecedent, but the answer was wrong. Incorrect antecedents also occurred when the intended antecedent was not (analysed by the parser as) a named entity. In cases such as (3-b) above, the antecedent is a common noun (ecological footprint). In cases such as ((4-b)) the antecedent (Russian-Turkish War) is analysed by the parser as an NP headed by a noun (war), and thus not recognized as a named entity. There were only a few cases where the antecedent was not in the first question/answer pair of a topic but in a later question. 12 cases (21%) were missed altogether by the anaphora module. These are due to the fact that no attempt was made to treat temporal anaphora such as toen, destijds (during that moment/period), and daarvoor (before this date), to treat locative uses of er (there), and a number of highly implicit anaphoric relations (i.e. given a question about the theme park de Efteling, the question which attraction opened in 1993? should be interpreted as asking about an attraction in de Efteling). A few antecedents were missed because we resolved at most one anaphoric element per question, whereas some questions actually contain two anaphoric elements (i.e. Who was he in the eighties version of the cartoon?).</p><p>Finally, there were 4 cases where anaphora resolution was triggered by an element that was not anaphoric (false alarms). These were all caused by the fact that relative pronouns were misclassified as deictic pronouns by the resolution component.</p><p>An interesting feature of CLEF is the fact that similar tasks are being executed for different languages. Follow-up questions were included in all tasks, but table 1 shows that the number of such questions varies considerably per target language. This suggests that the number of anaphoric expressions is also likely to vary considerably between tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Question Classification in Multilingual QA</head><p>Our system for multilingual QA performs English to Dutch QA, i.e. questions are in English, and answers are to be found in the Dutch document collection. English questions are translated to Dutch using Babelfish/Systran. As explained in <ref type="bibr" coords="6,328.85,561.81,94.86,8.74" target="#b1">Bouma et al. (2006)</ref>, one problem with this approach is the fact that proper names and concepts are often mistranslated (i.e. they are translated whereas they should remain unchanged, or a special translation exists in Dutch, or a complex name is not recognized as a syntactic unit, and is split up in the translated string). As the presence of names and concepts directly influences the performance of the QA system, we tried to reduce the number of errors using Wikipedia. For each name or concept in the English question, we check if there is a Wikipedia lemma with that title. If so, we check if a link to a corresponding Dutch page exists. If this is the case, the title of the Dutch lemma is used in the translation. Otherwise, the English name is used in the translation.</p><p>This year, we improved the system by using newer (and much expanded) versions of Wikipedia, inclusion of redirect pages, and the online geographical database geonames 3 for translation of geographical locations. Inspection of the translation results suggests that the coverage of these resources is quite good, although some problems remain. The use of redirects, for instance, causes Google to be mapped to the less frequent term Google Inc. Also, abbreviations tend to be replaced A second important aspect of QA-systems is question classification. As many automatically generated translations are grammatically poor, parsing may lead to unexpected results, and, as a consequence, question classification is often incorrect or impossible. To remedy this problem, we also included a question classifier for English, which we ran on the English source questions. We manually constructed a mapping from the question types used for English to the question types used in Joost. We expected that such a mapping might give more accurate results than classification of the automatically translated questions. Both the (mapped) English question type and the Joost type assigned to the translated are used to find an answer to the question. Note that question classification of the source language question is used in many MLQA systems (see <ref type="bibr" coords="7,90.00,423.92,90.80,8.74" target="#b8">Ligozat et al. (2006)</ref> for an overview), but usually the classification used for the source question is the same as that used by the answer extraction components.</p><p>There are various question classifiers for English which use the question classes of <ref type="bibr" coords="7,460.24,447.83,52.76,8.74;7,90.00,459.78,26.10,8.74" target="#b7">Li and Roth (2002)</ref>. They propose a classification consisting of 6 coarse question types and 50 fine-grained types. Each question is assigned a label consisting of both a coarse and a fine question type. We used the automatically trained classifier described by <ref type="bibr" coords="7,324.19,483.69,115.09,8.74" target="#b4">Hacioglu and Ward (2003)</ref> <ref type="foot" coords="7,439.28,482.12,3.97,6.12" target="#foot_2">4</ref> , which uses the Li and Roth classification.</p><p>Joost uses over 40 question types, some of which correspond quite well to those of Li and Roth. Mismatches are problematic especially in those cases where Joost expects a more fine-grained class than the class produced by Li and Roth. For instance, Li and Roth classify what is the capital of Togo as loc:city whereas Joost has the class capital. Furthermore, the question classes assigned by Joost are not just labels, but typically consist of a label combined with one or more phrases from the question that are crucial for answering the question. I.e. the question what does NASA stand for? is assigned the type abbr:exp by Quest, whereas it is assigned the label abbreviation(NASA) by Joost. The mapping therefore tries to fill in missing arguments (usually names) on the basis of the syntactic parse of the Dutch translated question.</p><p>In many cases, the question class assigned by Joost is more helpful than the class assigned after mapping the English question class. An important exception, however, are questions that were assigned no class by Joost itself (usually, because the translated question contained syntactic errors that made the parser fail). In those cases, using a mapped question class is preferable over using no class at all. We tested our approach on data from previous years. The effect of including question classification based on the original question turned out to be small, however, as can be seen in figure <ref type="figure" coords="7,150.96,686.93,3.88,8.74">2</ref>. For the 2006 dataset, the effect was even negative. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>The results from the CLEF evaluation are given in table <ref type="table" coords="8,348.45,388.65,3.88,8.74" target="#tab_3">3</ref>. Table <ref type="table" coords="8,391.43,388.65,4.98,8.74" target="#tab_4">4</ref> gives results per question type for the best Dutch monolingual run. For 20 questions no answer was given (i.e. nil was returned by the system as answer).<ref type="foot" coords="8,244.66,410.99,3.97,6.12" target="#foot_3">5</ref> There are two main reasons for this: mistakes in anaphora resolution, which made it impossible to find documents or answers matching the question and lack of coverage of the question analysis component. Although there were 28 definition questions, only 18 were classified as such by Joost. List questions were an important source of errors.</p><p>The impact of adding Wikipedia to the document collection was significant. Although the text version of the Dutch Wikipedia is smaller than the newspaper text collection (approximately 50M and 80M words respectively), 150 of the 180 questions (i.e. over 80%) that received an answer were answered using Wikipedia.</p><p>Definition questions are answered using a relation-table that was created of-line. In addition to these, 24 questions were assigned a question type for which a relation-table existed. This number is lower than for previous CLEF tasks.</p><p>The system normally checks that answers suggested by the system do not occur in the question. It turned out that, in the context of follow-up questions, this filter needs to take into account anaphora resolution as well. That is, if a question contained an anaphor that was resolved to antecedent A, in some cases the system would still suggest A as an answer to the question, and such answers were not filtered (as A did not occur in the question string).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>The inclusion of Wikipedia in the CLEF QA-task has made the task more realistic and attractive. We believe that performance on this task can be improved by taking the structure of Wikipedia more seriously, and by developing methods for relation and answer extraction that combine NLP with XML-based extraction.</p><p>Follow-up questions required the incorporation of a anaphora resolution component for questions. The current version of this module performs reasonably well, but its coverage should be extended (to cover locative anaphors and multiple anaphors). The proper treatment of lexical</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,216.17,695.09,170.67,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture of Joost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,206.49,402.62,187.09,274.54"><head>Table look</head><label>look</label><figDesc></figDesc><table coords="2,206.49,402.62,187.09,274.54"><row><cell></cell><cell>Question</cell></row><row><cell>Alpino</cell><cell></cell></row><row><cell></cell><cell>Question Type and Keywords</cell></row><row><cell></cell><cell>Off-line answer extraction relation</cell></row><row><cell></cell><cell>Tables</cell></row><row><cell>Passage Retrieval</cell><cell>-up</cell></row><row><cell></cell><cell>Qatar</cell></row><row><cell>Answer Extraction</cell><cell>Potential Answers</cell></row><row><cell></cell><cell>Answer Ranking</cell></row><row><cell></cell><cell>Answer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,112.94,423.00,108.01"><head>Table 1 :</head><label>1</label><figDesc>Number of follow-up questions (FQs) per target language. All tasks consist of 200 questions.</figDesc><table coords="6,237.13,112.94,128.74,61.56"><row><cell cols="3">Target FQs Target FQs</cell></row><row><cell>EN</cell><cell>133 RO</cell><cell>78</cell></row><row><cell>NL</cell><cell>122 FR</cell><cell>76</cell></row><row><cell>DE</cell><cell>84 PT</cell><cell>50</cell></row><row><cell>IT</cell><cell>84 ES</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,112.94,423.00,200.16"><head>Table 2 :</head><label>2</label><figDesc>Mean Reciprocal Rank (for the first 5 answers) and CLEF-score (1st answer correct) for English to Dutch QA, using Joost and a combination of Joost and Quest (union).by their expanded meanings. Although both IR and the linguistic QA modules recognize many abbreviations and expanded terms as synonyms, this may still cause problems. An obvious case is a question asking for the meaning of an abbreviation. Another problem is the fact that many common words, which are not concepts occur as lemmas in Wikipedia. If no corresponding Dutch page exists, this causes some terms to show up untranslated in the Dutch question (i.e. for the adjectives French and Eastern, an English Wikipedia page exists, but no Dutch counterpart).</figDesc><table coords="7,202.23,112.94,198.54,61.56"><row><cell></cell><cell>Joost</cell><cell>union</cell><cell></cell></row><row><cell>Testset</cell><cell>Qs MRR</cell><cell>1st MRR</cell><cell>1st</cell></row><row><cell>2003</cell><cell cols="3">377 0.329 0.292 0.347 0.310</cell></row><row><cell>2004</cell><cell cols="3">200 0.406 0.350 0.429 0.375</cell></row><row><cell>2006</cell><cell cols="3">200 0.225 0.195 0.213 0.185</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.00,112.94,423.00,197.33"><head>Table 3 :</head><label>3</label><figDesc>Official CLEF scores for the monolingual Dutch task and the bilingual English to Dutch task (200 questions), with and without Query Expansion (QE) .</figDesc><table coords="8,114.26,112.94,374.48,197.33"><row><cell>Run</cell><cell cols="6">Accuracy (%) Right ineXact Unsupported Wrong</cell></row><row><cell>Dutch-mono</cell><cell></cell><cell>24.5</cell><cell>49</cell><cell>11</cell><cell>4</cell><cell>136</cell></row><row><cell cols="2">Dutch-mono + QE</cell><cell>25.5</cell><cell>51</cell><cell>10</cell><cell>4</cell><cell>135</cell></row><row><cell>En-Du</cell><cell></cell><cell>13.0</cell><cell>26</cell><cell>8</cell><cell>7</cell><cell>159</cell></row><row><cell>En-Du + QE</cell><cell></cell><cell>13.5</cell><cell>27</cell><cell>7</cell><cell>5</cell><cell>161</cell></row><row><cell>Q type</cell><cell cols="6"># q's Accuracy (%) Right ineXact Unsupported Wrong</cell></row><row><cell>Factoids</cell><cell>156</cell><cell>25.6</cell><cell>40</cell><cell>5</cell><cell>4</cell><cell>107</cell></row><row><cell>List</cell><cell>16</cell><cell>6.3</cell><cell>1</cell><cell>0</cell><cell>5</cell><cell>10</cell></row><row><cell>Definition</cell><cell>28</cell><cell>35.7</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>18</cell></row><row><cell>Temp. Restricted</cell><cell>41</cell><cell>19.5</cell><cell>8</cell><cell>3</cell><cell>3</cell><cell>27</cell></row><row><cell>NIL</cell><cell>20</cell><cell>0.0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,145.40,335.47,312.19,8.74"><head>Table 4 :</head><label>4</label><figDesc>Results per question type for the best Dutch monolingual run.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,736.23,148.20,6.64"><p>http://ilps.science.uva.nl/WikiXML/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,742.83,232.39,7.21"><p>i.e. trec.nist.gov/data/qa/2006 qadata/QA2006 testset.xml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,105.24,705.86,192.65,7.21"><p>Available until recently at sds.colorado.edu/QUEST</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="8,105.24,737.54,407.76,7.44;8,90.00,747.00,55.81,6.99"><p>At the moment of writing, it is not clear to us whether there actually were questions for which nil was the correct answer.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This research was carried out as part of the research program for Interactive Multimedia Information Extraction, imix, financed by nwo, the <rs type="funder">Dutch Organisation for Scientific Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>knowledge within the system remains an issue that requires more attention. The performance of the IR-module was improved using automatically acquired synonyms, but this knowledge has not been integrated yet in the relation and answer extraction modules.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,90.00,190.10,423.00,8.74;9,104.94,202.06,408.05,8.74;9,104.94,214.01,94.25,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,183.83,202.06,195.40,8.74">Linguistic knowledge and question answering</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,388.93,202.06,124.06,8.74;9,104.94,214.01,33.06,8.74">Traitement Automatique des Langues</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="15" to="39" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,232.56,422.99,8.74;9,104.94,244.51,408.06,8.74;9,104.94,256.47,278.12,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,163.47,244.51,349.53,8.74;9,104.94,256.47,12.00,8.74">The University of Groningen at QA@CLEF 2006: Using syntactic knowledge for QA</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Fahmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,139.80,256.47,196.10,8.74">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,275.01,423.00,8.74;9,104.94,286.97,31.46,8.74;9,167.67,286.97,345.33,8.74;9,104.94,298.92,393.03,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,337.90,275.01,175.10,8.74;9,104.94,286.97,12.00,8.74">Reasoning over dependency relations for QA</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,361.49,286.97,151.51,8.74;9,104.94,298.92,277.13,8.74">Proceedings of the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)</title>
		<editor>
			<persName><forename type="first">Patrick</forename><surname>Benamara</surname></persName>
		</editor>
		<editor>
			<persName><surname>Saint-Dizier</surname></persName>
		</editor>
		<meeting>the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ)<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,317.47,422.99,8.74;9,104.94,329.42,408.06,8.74;9,104.94,341.38,53.50,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,375.75,317.47,137.24,8.74;9,104.94,329.42,112.72,8.74">Alpino: Wide-coverage computational analysis of Dutch</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Malouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.42,329.42,204.94,8.74">Computational Linguistics in The Netherlands</title>
		<meeting><address><addrLine>Rodopi, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2001. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,359.92,422.99,8.74;9,104.94,371.88,408.06,8.74;9,104.94,383.83,75.31,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,279.87,359.92,233.12,8.74;9,104.94,371.88,115.64,8.74">Question classification with support vector machines and error correcting codes</title>
		<author>
			<persName coords=""><forename type="first">Kadri</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,246.27,371.88,148.72,8.74">Proceedings of HLT-NACCL 2003</title>
		<meeting>HLT-NACCL 2003<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,402.37,423.00,8.74;9,104.94,414.33,408.06,8.74;9,104.94,426.28,302.08,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,104.94,414.33,253.57,8.74">Question answering with LCC&apos;s Chaucer at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,187.36,426.28,103.84,8.74">TREC 2006 Proceedings</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,444.83,423.00,8.74;9,104.94,456.78,387.77,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,197.43,444.83,311.42,8.74">Europarl: A multilingual corpus for evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://people.csail.mit.edu/koehn/publications/europarl/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>unpublished draft</note>
</biblStruct>

<biblStruct coords="9,90.00,475.33,423.00,8.74;9,104.94,487.28,302.85,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,217.05,475.33,119.31,8.74">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,355.65,475.33,157.35,8.74;9,104.94,487.28,229.06,8.74">Proceedings of the 19th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,505.83,423.00,8.74;9,104.94,517.78,408.06,8.74;9,104.94,529.74,371.92,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,445.30,505.83,67.71,8.74;9,104.94,517.78,258.31,8.74">Evaluation and improvement of cross-lingual question answering strategies</title>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabella</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Vilat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,181.68,529.74,229.72,8.74">EACL workshop on Multilingual Question Answering</title>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</editor>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,548.28,423.00,8.74;9,104.94,560.24,400.90,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,168.88,548.28,339.98,8.74">Increasing the coverage of answer extraction by applying anaphora resolution</title>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,117.40,560.24,384.43,8.74">Fifth Slovenian and First International Language Technologies Conference (IS-LTC &apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,578.78,423.00,8.74;9,104.94,590.74,380.94,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,181.06,578.78,331.94,8.74;9,104.94,590.74,124.94,8.74">The application of fuzzy logic to the construction of the ranking function of information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Rubens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,238.85,590.74,186.20,8.74">Computer Modelling and New Technologies</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,609.28,381.57,8.74" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Rubens</surname></persName>
		</author>
		<ptr target="http://lucene-qe.sourceforge.net/" />
		<title level="m" coord="9,180.39,609.28,135.12,8.74">Lucqe -lucene query expansion</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,627.82,404.00,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,211.26,627.82,276.93,8.74">Improving passage retrieval in question answering using NLP</title>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,503.87,627.82,9.14,8.74;9,104.94,639.78,408.05,8.74;9,104.94,651.73,408.06,8.74;9,104.94,663.69,261.77,8.74" xml:id="b13">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="9,295.62,639.78,217.38,8.74;9,104.94,651.73,305.62,8.74">Progress in Artificial Intelligence -Selected papers from the 12th Portuguese Conference on Artificial Intelligence (EPIA)</title>
		<title level="s" coord="9,488.10,651.73,24.91,8.74;9,104.94,663.69,24.74,8.74">LNAI Series</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bento</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cardoso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Dias</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Covilhã, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3808</biblScope>
			<biblScope unit="page" from="634" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,682.23,422.99,8.74;9,104.94,694.19,408.06,8.74;9,104.94,706.14,111.47,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,303.63,682.23,209.36,8.74;9,104.94,694.19,115.44,8.74">Automatic acquisition of lexico-semantic knowledge for question answering</title>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.32,694.19,268.18,8.74">Proceedings of Ontolex 2005 -Ontologies and Lexical Resources</title>
		<meeting>Ontolex 2005 -Ontologies and Lexical Resources<address><addrLine>Jeju Island, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,724.69,423.00,8.74;9,104.94,736.64,362.94,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,330.54,724.69,182.46,8.74;9,104.94,736.64,220.84,8.74">Finding synonyms using automatic word alignment and measures of distributional similarity</title>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,346.56,736.64,116.20,8.74">Proceedings of ACL/Coling</title>
		<meeting>ACL/Coling</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
