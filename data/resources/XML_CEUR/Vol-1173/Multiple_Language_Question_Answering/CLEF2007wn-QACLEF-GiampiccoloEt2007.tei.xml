<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,242.04,69.60,127.23,11.96;1,113.88,84.72,383.57,11.96">OVERVIEW OF THE CLEF 2007 MULTILINGUAL QUESTION ANSWERING TRACK</title>
				<funder ref="#_EqkfAQS">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.00,129.43,76.31,10.37"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
							<email>giampiccolo@celct.it</email>
							<affiliation key="aff0">
								<orgName type="institution">CELCT</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.12,129.43,58.23,10.37"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.24,129.43,67.39,10.37"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
							<email>ayache@elda.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">ELDA/ELRA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.60,129.43,43.23,10.37"><forename type="first">Dan</forename><surname>Cristea</surname></persName>
						</author>
						<author>
							<persName coords="1,412.80,129.43,55.07,10.37"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff0">
								<orgName type="institution">CELCT</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute for Computer Science</orgName>
								<orgName type="institution">University &quot;Al. I. Cuza&quot; of Iaşi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Romanian Academy</orgName>
								<address>
									<settlement>Iaşi</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,139.80,140.11,62.27,10.37"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
							<email>jijkoun@science.uva.nl</email>
							<affiliation key="aff5">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.88,140.11,56.11,10.37"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">BTB</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.84,140.11,47.59,10.37"><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
							<email>paulo.rocha@alfa.di.uminho.pt</email>
							<affiliation key="aff7">
								<orgName type="department">Linguateca</orgName>
								<orgName type="institution">SINTEF ICT and Portugal</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.40,140.11,69.59,10.37"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
							<email>bogdan.sacaleanu@dfki.de</email>
							<affiliation key="aff8">
								<orgName type="institution">DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,422.64,140.11,64.51,10.37"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff9">
								<orgName type="department">DLTG</orgName>
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<address>
									<settlement>Gatwick, Stansted, Heathrow, Luton and City</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,242.04,69.60,127.23,11.96;1,113.88,84.72,383.57,11.96">OVERVIEW OF THE CLEF 2007 MULTILINGUAL QUESTION ANSWERING TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A635C13EA404E72D6F4DDE2D6E9EB9CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fifth QA campaign at CLEF, the first having been held in 2006. was characterized by continuity with the past and at the same time by innovation. In fact, topics were introduced, under which a number of Question-Answer pairs could be grouped in clusters, containing also co-references between them. Moreover, the systems were given the possibility to search for answers in Wikipedia. In addition to the main task, two other tasks were offered, namely the Answer Validation Exercise (AVE), which continued last year's successful pilot, and QUAST, aimed at evaluating the task of Question Answering in Speech Transcription.</p><p>As general remark, it must be said that the task proved to be more difficult than expected, as in comparison with last year's results the Best Overall Accuracy dropped from 49,47% to 41,75% in the multilingual subtasks, and, more significantly, from 68,95% to 54% in the monolingual subtasks.</p><p>PERSON, e.g.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The fifth QA campaign at CLEF [1], the first having been held in 2003, was characterized by continuity with the past, maintaining the focus on cross-linguality and covering as many European languages as possible (with the addition of Indonesian); and by innovation 1) by introducing a number of Question-Answer pairs, grouped in clusters, which referred to a same topic and which contained co-references between them, and 2) by giving the possibility to search for answers in Wikipedia. In this way, the newcomers had the possibility to test themselves with the classic task, and those who had participated in the previous campaigns had a new challenging factor to test their systems. In addition to the main task, two other tasks were offered, namely the Answer Validation Exercise (AVE), which continued last year's successful pilot, and the Question Answering for Speech Transcripts (QAST), aimed at evaluating the task of Question Answering in Speech Transcription. In the following sections, the main task and its preparation will be described. A presentation of the participants and the runs submitted will be also given, together with a description of the evaluation method and the results achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>Following the procedure consolidated in previous years, in the 2007 campaign several different tasks were proposed:</p><p>1. a main task, divided into several monolingual and bi-lingual sub-tasks; 2. the Answer Validation Exercise (AVE), which continued the successful experiment proposed in 2006.</p><p>Systems were required to emulate human assessment of QA responses and decide whether an Answer to a Question is correct or not according to a given Text. Participating systems were given a set of triplets (Question, Answer, Supporting Text) and they had to return a boolean value for each triplet. Results were evaluated against the QA human assessments [1]; 3. the QA Answering on Speech Transcripts (QAST), a pilot task which aimed at providing a framework in which factual. Relevant points of this pilot were: a. Comparing the performances of the systems dealing with both types of transcriptions. b. Measuring the loss of each system due to the state of the art ASR technology. c. In general, motivating and driving the design of novel and robust factual QA architectures for automatic speech transcriptions <ref type="bibr" coords="2,281.16,142.39,10.03,10.37">[2]</ref>.</p><p>The AVE and QAST tasks are described in details in dedicated papers in this Working Notes.</p><p>As far as the main task is concerned, the consolidated procedure was followed, although some relevant innovations were introduced.</p><p>The systems were given a set of 200 questions -which could concern facts or events (F-actoid questions), definitions of people, things or organisations (D-efinition questions), or lists of people, objects or data (L-ist questions)-and were asked to return one exact answer, where exact meant that neither more nor less than the information required was given. Following the example of TREC, this year the exercise consisted of topicrelated questions, i.e. clusters of questions which were related to the same topic and possibly contained coreferences between one question and the others. Neither the question types (F, D, L) or the topics were given to the participants.</p><p>The answer needed to be supported by the docid of the document in which the exact answer was found, and by portion(s) of text, which provided enough context to support the correctness of the exact answer. Supporting texts could be taken from different sections of the relevant documents, and had to sum up to a maximum of 700 bytes. There were no particular restrictions on the length of an answer-string, but unnecessary pieces of information were penalized, since the answer was marked as ineXact. As in previous years, the exact answer could be exactly copied and pasted from the document, even if it was grammatically incorrect (e.g.: inflectional case did not match the one required by the question). Anyway, this year systems were also allowed to use NL generation in order to correct morpho-syntactical inconsistencies (e.g., in German, changing "dem Presidenten" into "der President" if the question implies that the answer is in Nominative case), and to introduce grammatical and lexical changes (e.g., QUESTION: What nationality is X? TEXT: X is from the Netherlands =&gt; EXACT ANSWER: Dutch). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RO</head><p>The subtasks were both:</p><p>• monolingual, where the language of the question (Source language) and the language of the news collection (Target language) were the same; • cross-lingual, where the questions were formulated in a language different from that of the news collection.</p><p>Ten source languages were considered, namely, Bulgarian, Dutch , English, French, German, Indonesian, Italian, Portuguese, Romanian and Spanish. All these languages were also considered as target languages, except for Indonesian, which had no news collections available for the queries and, as was done in the previous campaigns, used the English question set translated into Indonesian (IN). Anyway, as Table <ref type="table" coords="3,163.20,253.15,4.67,10.37" target="#tab_2">2</ref> shows, not all the proposed tasks were then carried out by the participants. As customary in recent campaigns, a monolingual English (EN) task was not available as it seems to have been already thoroughly investigated in TREC campaigns. English was still both source and target language in the cross-language tasks. As the format is concerned, this year both input and output files were formatted as an XML file (for more details see <ref type="bibr" coords="3,107.04,478.15,9.90,10.37">[4]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Set Preparation</head><p>The procedure followed to prepare the test set was much different from that used in the previous campaigns. First at all, each organizing group, responsible for a target language, freely chose a number of topics. For each topic, one to four questions were generated. Topics could be not only named entities or events, but also other categories such as objects, natural phenomena, etc. (e.g. George W. Bush; Olympic Games; notebooks; hurricanes; etc.). The set of ordered questions were related to the topic as follows:</p><p>The topic was named either in the first question or in the first answer The following questions can contain co-references to the topic expressed in the first question/answer pair.</p><p>Topics were not given in the test set, but could be inferred from the first question/answer pair. For example, if the topic was George W. Bush, the cluster of questions related to it could have been:</p><p>Q1: Who is George W. Bush? Q2: When was he born? Q3: Who is his wife?</p><p>The The questions in the set were numbered from 1 to 200, with no indication about whether they were part of a cluster belonging to the same topic.</p><p>Another major innovation of this year's campaign concerned the corpora at which the questions were aimed at.</p><p>In fact, beside the data collections composed of news articles provided by ELRA/ELDA, also Wikipedia was considered, capitalizing on the experience of the WiQA pilot task proposed in 2006. The Wikipedia pages in the target languages, as found in the version of the Wikipedia of November, 2006 could be used. XML and the HTML versions were available for download, even though any other versions of the Wikipedia files could be used as long as they dated back to the end of November / beginning of December 2006. All the answers to the questions had to be taken from "actual entries" or articles of Wikipedia pages -the ones whose filenames normally correspond to the topic of the article. Other types of data ("image", "discussion", "category", "template", "revision histories", any files with user information, and any "meta-information" pages), had to be excluded.</p><p>As far as the question types are concerned, as in previous years of QA@CLEF, the three following categories were still considered: a) Factoid questions, fact-based questions, asking for the name of a person, a location, the extent of something, the day on which something happened, etc. We consider the following 8 answer types for factoids:</p><p>As only one answer was allowed, all the items had to be presented in sequence, one next to the other, in one document of the target collections. Some questions could have no answer in the document collection, and in that case the exact answer was "NIL" and the answer and support docid fields were left empty. A question is assumed to have no right answer when neither human assessors nor participating systems could find one. The distribution of the questions among these categories is described in Table <ref type="table" coords="6,387.60,174.79,3.45,10.37" target="#tab_4">4</ref>.</p><p>Each of the question sets was finally then translated into English, so that each group could translate another set into their own language, when preparing the cross-lingual data sets which had been activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants</head><p>After years of constant growth, the number of participants has decreased in 2007 [see Table <ref type="table" coords="6,439.80,252.55,3.13,10.37" target="#tab_5">5</ref>].. Also the number of submitted runs has decreased sensibly, from a total of 77 registered last year to 22 (see table <ref type="table" coords="6,92.76,594.19,3.34,10.37" target="#tab_6">6</ref>). As in previous campaigns, a larger number of people chose to participate in the monolingual tasks, which once again demonstrated to be more approachable.</p><p>No changes were made as far the evaluation process is concerned-Human judges assessed the exact answer (i.e. the shortest string of words which is supposed to provide the exact amount of information to answer the question) as:</p><p>• R (Right) if correct;</p><p>• W (Wrong) if incorrect;</p><p>• X (ineXact) if contained less or more information than that required by the query;</p><p>• U (Unsupported) if either the docid was missing or wrong, or the supporting snippet did not contain the exact answer.</p><p>Most assessor-groups managed to guarantee a second judgment of all the runs. As regards the evaluation measures the following measures:</p><p>• accuracy, as the main evaluation score, defined as the average of SCORE(q) over all 200 questions q;</p><p>• the K1 measure <ref type="bibr" coords="7,182.81,249.91,12.26,10.37" target="#b1">[6]</ref>:</p><formula xml:id="formula_0" coords="7,213.00,262.84,165.47,71.23">( ) ( ) ( ) ( ) ( ) ( ) [ ] 1 , 1 1 IR 1 # sys 1 K - ∈ ∧ ∈ • = ∑ ∈ sys K sys K questions r eval r score sys answers r</formula><p>where: score (r) is the confidence score assigned by the system to the answer r and eval(r) depends on the judgment given by the human assessor. • the Confident Weighted Score (CWS), designed for systems that give only one answer per question.</p><p>Answers are in a decreasing order of confidence and CWS rewards systems that give correct answers at the top of the ranking [2] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>As far as accuracy is concerned, scores were generally far lower this year than usual, as Figure <ref type="figure" coords="7,472.92,565.99,4.67,10.37" target="#fig_1">1</ref> shows. In detail, Best accuracy in the monolingual task decreased by almost 15 points, passing from last year's 68.95% to 54%, while Best accuracy in cross-language tasks passed from 49.47% to 41.75% recording. As far as average performances are concerned, this year a neat decrease has been recorded in the biligual tasks, which went from 22.8% to 10.9%. This was due also due to the presence of systems which participated for the first time, achieving very low score in tasks which are quite difficult also for veterans. As a general remark, it can be said that the new factors introduced this year appear to have had an impact on the performances of the systems. As more than one participant has noticed, there has been not enough time to adjust the systems to the new requirements.</p><p>Here below a more detailed analyses of the results in each language follows, giving more specific information on the performances of systems in the single sub-tasks and on the different types of questions, providing the relevant statistics and comments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dutch as Target</head><p>For the Dutch subtask of the CLEF 2007 QA task, three annotators generated 200 questions organized in 78 groups so that there were 16 groups with one question, 21 groups with two, 22 with three and 19 groups with four questions. Among the 200 questions 156 were factoids, 28 definitions and 16 list questions. In total, 41 questions had temporal restrictions. Table XXX below shows the distributions of topic types for groups and expected answer types for questions. Annotators were asked to create questions with answers either in Dutch Wikipedia or in the Dutch newspaper corpus, as well as questions without known answers. Of 200 questions, 186 had answers in Wikipedia, and 14 in the newspaper corpus. Annotators did not create NIL questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7: Results</head><p>This year, two teams took part in the QA track with Dutch as the target language: the University of Amsterdam and the University of Groningen. The latter submitted both monolingual and crosslingual (English to Dutch) runs. The 5 submitted runs were assessed independently by 3 Dutch native speakers in such a way that each question group was assessed by at least two assessors. In case of conflicting assessments, assessors were asked to discuss the judgements and come to an agreement. Most of the occured conflicts were due to difficulties in distinguishing between inexact and correct answers. Table <ref type="table" coords="9,325.44,268.75,4.67,10.37">7</ref> below shows the evaluation results for the five submitted runs (three monolingual and two cross-lingual). The table shows the number of Right, Wrong, ineXact and Unsupported answers, as well as the percentage of correctly answered Factoids, Temporally restricted questions, Definition and List questions.</p><p>The best monolingual run (gron072NLNL) achieved accuracy of 25.5%, which is slightly less that the best results in the 2006 edition of the QA task. The same tendency holds for the performance on factoid and definition questions. We interpret this an indication of the increased difficulty of the task due the newly introduced Wikipedia collection.</p><p>One of the runs contained as many as 23 unsupported answers-this might indicate a bug in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">English as Target</head><p>Creation of questions. This year the questions set were radically different from last year. Instead of 200 independent questions, we were required to devise questions in groups. Each group had a declared topic (e.g. "Polygraph") but unlike in TREC, this topic was not communicated to the participants. As at CLEF last year, the type of question (e.g. definition, factoid or list) was not declared to participants either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8: Results</head><p>160 Factoids (in groups) were requested, together with 30 definitions and ten lists. The numbers of temporally restricted factoids and questions with NIL answers was at our discretion. In the end we submitted 161 factoids, 30 definitions and nine lists. In previous years we have been obliged to devise a considerable number of temporally restricted questions and this has proved very difficult to do with the majority of them being very contrived and artificial. For this reason it was intended to set no such questions this year. However, one reasonable one was spotted during the data entry process and so was flagged as such. Two others were also flagged accidentally during data entry. Unfortunately, therefore, the statistics can not tell us anything about temporally restricted questions.</p><p>Concerning NIL questions, we have long argued that they tell us very little about the performance of a system unless it can report the reason why there is no answer. For example, this is a useful system:</p><formula xml:id="formula_1" coords="10,92.76,153.07,122.59,21.06">Q: Who is the Queen of France? A:</formula><p>France is a Republic! By contrast, answering NIL would not tell us whether there was an answer which was simply not found, or whether no answer in fact exists. Another important point following from this is that NIL questions artificially boost the performance of a system which returns many NIL answers. For these reasons we decided not to include any questions with NIL anwers. However, we would like to see 'Queen of France' answers being returned in future workshops.</p><p>The grouped nature of the questions had a considerable effect on their difficulty; instead of a series of 'trivia' type questions, each with a simple, clear answer, a single topic was effectively investigated in much more detail.</p><p>To achieve the goals set by the organisers it was necessary to find topics about which several questions could be asked and then to devise as many questions as possible from that topic. Each task was surprisingly hard, and an inevitable consequence was that the questions are much harder this year than in previous years. We had no wish to set especially difficult or convoluted questions, but unfortunately this arose as a side-effect of the new procedures.</p><p>The requirement for related questions on a topic necessarily implies that the questions will refer to common concepts and entities within the domain in question. In a series of questions this is accomplished by co-reference -a well known phenomenon within Natural Language Processing which nevertheless has not been a major factor in the success of QA systems at previous CLEF workshops. The most common form is anaphoric reference to the topic declared implicitly in the first question, e.g.:</p><formula xml:id="formula_2" coords="10,92.76,401.71,98.47,21.17">Q: What is a Polygraph? Q: When was it invented?</formula><p>However, other forms of co-reference occurred in the questions. Here is an example:</p><p>Q: Who wrote the song "Dancing Queen"? Q: How many people were in the group?</p><p>Here the group refers to the category of entity into which the answer to the first question is known by the questioner to belong. However, the QA system does not know this and has to infer it, a task which can be very complex and indirect, especially where the topic is concealed from the participants.</p><p>In addition to the issue of question grouping, it was decided at a very late stage to use not only the two collections from last year (the LA Times and Glasgow Herald) but also the English Wikipedia. The latter is extremely large and greatly increases the task complexity for the participants in terms of both indexing and IR searching. In addition, some questions had to be heavily qualified in order to reduce the ambiguity introduced by alternative readings in the Wikipedia. Here is an example: Q: What is the "KORG" on which Niky Orellana is a soccer commentator?</p><p>Thirdly, we should bear in mind that the Wikipedia varies considerably in size depending on the language, with the English one being by far the largest. We have not controlled for this fact in CLEF and the consequence could be that the addition of Wikipedia had a greater effect on difficulty for English than it did for other languages.</p><p>Summary Statistics. Eight cross-lingual runs with English as target were submitted this year, as compared with thirteeen for last year. Five groups participated in six source languages, Dutch, French, German, Indonesian, Romanian and Spanish. DFKI submitted runs for two source languages, German and Spanish, while all other groups worked in only one. Cindi Group and Macquarie University both submitted two runs for a language pair (French-English and Dutch-English respectively) but unfortunately there was no language for which more than one group submitted a run. This means that no direct comparisons can be made between QA systems this year, because the task being solved by each was different.</p><p>Assessment Procedure. An XML format was used for the submission of runs this year, by constrast with previous years when fairly similar plain text formats were adopted. This meant that our evaluation tools were no longer usable. However, last year we also participated in the evaluation of the WiQA task organised by University of Amsterdam. For this they developed an excellent web-based tool which was subsequently adapted for this year's Dutch CLEF evaluations. We are extremely grateful to Martin de Rijke and Valentin Jijkoun for allowing us to use it and for setting it up in Amsterdam especially for us. It allows multiple assessors to work independently, shows runs anonymised, allows all answers to a particular question to be judged at the same time (like the TREC software), and includes the supporting snippets for each submitted answer as well as the 'correct'</p><p>(reference) answer. It also shows inter-assessor disagreement, and, once this has been eliminated, can produce the assessed runs in the correct XML format. Overall, this software worked perfectly for us and saved us a considerable amount of time.</p><p>All answers were double-judged. The first assessor was Richard Sutcliffe and the second was Udo Kruschwitz from University of Essex to whom we are indebted for his invaluable help. Where assessors differed, the case was discussed between us and a decision taken. We measured the agreement level by two methods. For Agreement 1 we take agreement on each group of 8 answers to a question as a whole as either exactly the same for both assessors or not exactly the same. This is a very strict measure. There were disagreements for 30 questions out of the 200, i.e. 15%, which equates to an agreement level of 85%.</p><p>For Agreement Level 2 we taking each decision made on one of the eight answers to a question and count how many decisions were the same for both assessors and how many were not the same. There were 39 differences of decision and a total of 1600 decisions (200 questions by eight runs). This is 2.4%, which equates to an agreement level of 97.6%. This is the measure we used in previous years. Last year the agreement level was 89% and the previous year it was 93%. We conclude from these figures that the assessment of our CLEF runs is quite accurate and that double judging is sufficient.</p><p>Results Analysis. As in previous years there were three types of question within the question groups, Factoids, Definitions and Lists. Considering all question types together, the best performance is University of Wolverhampton with 28 R and 2 X, (14% strict or 15% lenient) closely followed by the CINDI Group at Concordia University with 26 R and 1 X (13% strict or 13.50% lenient). Note that these systems are working on different tasks (RO-EN and FR-EN respectively) as noted above, so the results are not directly comparable. The best performance last year for English targets was 25.26%. Nevertheless, considering the extreme difficulty of the questions, this represents a remarkable achievement for these systems.</p><p>For Factoids alone, the best system was CINDI (FR-EN) at 11.18% followed by University of Indonesia (IN-EN) with 10.56%. For Definitions the best result was University of Wolverhampton (RO-EN) with 43.33% correct, followed equally by CINDI (FR-EN) and DFKI (DE-EN) both with 23.33%. It is interesting that this year the best Definition score is almost four times the best Factoid score, whereas last year they were nearly equal. One reason for this may be that the definitions either occurred first in a group of questions or on their own in a 'singleton' group. This was not specifically intended but seems to be a consequence of the relationship between Factoids and Definitions, namely that the latter are somehow epistemologically prior to the former<ref type="foot" coords="11,502.68,540.82,3.04,6.75" target="#foot_0">1</ref> . In consequence, Definitions may be more simply phrased than Factoids and in particular may avoid co-reference in the vast majority of cases.</p><p>Nine lists questions were set but only CINDI was able to answer any of them correctly (11.11% accuracy). (University of Indonesia was ineXact on one list question.) Perhaps the problem here was recognising the list question in the first place -unlike at TREC they are not explicitly flagged. We believe this is not necessarily reasonable since in a real dialogue a questioner would surely make it quite clear whether they expected a list of answers or just one. They would not come up with a list question out of the blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">French as Target</head><p>This year two groups took part in evaluation tasks using French as target language: one French group: Synapse Développement ; and one American group: Language Computer Corporation (LCC).</p><p>In total, only two runs have been returned by the participants: one monolingual run (FR-to-FR) from Synapse Développement and one bilingual run (EN-to-FR) from LCC.</p><p>It appears that the number of participants for the French task has clearly decreased this year, certainly due to the many changes that appeared in the 2007 Guidelines for the participants: adding to a large new answer source (Wikipedia 2006) and adding to a large number of topic-related questions, i.e. clusters of questions which are related to the same topic and possibly contain anaphoric references between one question and the other questions. These changes explain certainly the cause of the strong decrease of participation this year.</p><p>Three types of questions were proposed: factual, definition and closed list questions. The participating teams could return one exact answer per question and up to two runs. Some questions (10%) had no answer in the document collection, and in this case the exact answer is "NIL".</p><p>Table <ref type="table" coords="12,116.40,263.35,4.67,10.37" target="#tab_10">9</ref> shows the final results of the assessment of runs for the two participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="41.75">%</head><p>Figure <ref type="figure" coords="12,119.52,442.87,4.67,10.37" target="#fig_2">2</ref> shows the best scores for systems using French as target in the last four CLEF QA campaigns. The French test set was composed of 200 questions: 163 Factual (F), 27 Definition (D) and 10 closed List questions (L). Among these 200 questions, 41 were Temporally restricted questions (T). The accuracy has been calculated over all the answers of F, D, T and L questions and also the Confidence Weighted Score (CWS) and the K1 measure.</p><p>For the monolingual task, the Synapse Développement' system returned 108 correct answers i.e. 54 % of correct answers (as opposed to 67,89 % last year).</p><p>For the bilingual task, the LCC's system returned 81 correct answers i.e. 41,75 % of correct answers (as opposed to 49,47 % for the best bilingual system last year).</p><p>We can observe that the two systems obtained different results according to the answer types. The monolingual system obtained better results for Definition questions (74,07 %) than for Factoid (52,76 %) and Temporally questions (46,34 %) whereas the bilingual system obtained better results for Temporally (46,34 %) and Factoid questions (44,17 %) than for Definition questions (22,22 %).</p><p>We can note that the bilingual system has not returned NIL answer, whereas the monolingual one returned 40 NIL answers (out of 9 expected NIL answers in the French test set). As there were only 9 NIL answers in the French test set and as the monolingual system returned 40 NIL answers, his final score is not very high (even if this system returned the 9 expected correct NIL answers).</p><p>The main difficulties encountered this year by the systems were the new type of questions: topic-related questions and the adding of a new large answer source (Wikipedia 2006). The participants had to adapt their system in a few weeks to be able to deal with this new type of questions. Moreover, larger is the corpus, more difficult is the expected exact answer to be extracted from the corpus source for a system (even if very often, there are several possible answers in the corpus).</p><p>In conclusion, despite the important changes in the Guidelines for the participants, the monolingual system obtained the best results of all the participants at CLEF@QA track this year (108 correct answers out of 200). We can note that the American group (LCC) participated only for the second time in the Question Answering track using French in target and has already obtained good results that can let us imagine it will improve again in the future. In addition, we can still observe this year the increasing interest in Question Answering for the tasks using French as target language from the non-European research community due to the second participation of an American team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">German as Target</head><p>Two research groups submitted runs for evaluation in the track having German as target language: The German Research Center for Artificial Intelligence (DFKI) and the Fern Universität Hagen (FUHA). Both provided system runs for the monolingual scenario and just DFKI submitted runs for the cross-language English-German and Portuguese-German scenario. The assessment was conducted by two native German speakers with fair knowledge of information access systems. Compared to the previous editions of the evaluation forum, this year a decrease in the accuracy of the best performing system and of an aggregated virtual system for both monolingual and cross-language tasks was registered. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Results evolution</head><p>The details of systems' results can be seen in Table <ref type="table" coords="14,304.44,287.59,7.80,10.37" target="#tab_12">11</ref>. There were no NIL questions tested in this year's evaluation. The results submitted by DFKI did not provide a normalized value for the confidence score of an answer and therefore both CWS and K1 values could not be computed. The number of topics covered by the questions was of 116 distributed as it follows: 69 topics consisting of 1 question, 19 topics of 2 related questions, and each 19 topics of 3 and 4 related questions. The most frequents topic types were PERSON (40), OBJECT (33) and ORGANIZATION (23). As regards the source of the answers, 101 questions from 68 topics asked for information out of the CLEF document collection and the rest of 99 from 48 topics for information from Wikipedia. The distribution of the topics over the document collections (CLEF vs. Wikipedia) is as follows: 53 vs. 16 topics of 1 question, 4 vs. 15 topics of each 2 and 3 questions and 7 vs. 2 topics of 4 questions. Question disagreement reflects the number of questions on which the assessors delivered different judgments. Along the total figures for the disagreement, a breakdown at the question type level (Factoid, Definition, List) and at the assessment value level (ineXact, Unsupported, Wrong/Right) is listed. The answer disagreements of type Wrong/Right are trivial errors during the assessment process when a right answers was considered wrong by mistake and the other way around, while those of type X or U reflect different judgments whereby an assessor considered an answer inexact or unsupported while the other marked it as right or wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Italian as Target</head><p>Only one group took part in this year to the monolingual Italian task, i.e. FBK-irst, submitting only one run. The results are shown in table <ref type="table" coords="15,190.68,209.35,7.72,10.37" target="#tab_14">13</ref>. As Figure <ref type="figure" coords="15,132.72,346.63,4.67,10.37" target="#fig_4">4</ref> shows, the results were much lower than both best and average performances in monolingual Italian tasks in the achieved in the previous campaigns. The Italian question consisted of 147 factoid questions, 41 definition questions and 12 list questions. 38 questions contained a temporal restriction, and 11 had no answer in the Gold Standard. In the Gold Standard, 108 answers were retrieved from Wikipedia, the remains from the news collections.</p><p>The submitted run was assessed by two judges; the inter-annotator agreement was 92,5%.</p><p>The system achieved low accuracy in all types of questions, performing anyway better in factoids questions. Definition questions, with 2,63% of accuracy and list questions, for which no correct answer was retrieved, were particularly challenging. A relevant number of questions (about 6%) was judged unsupported, meaning that the correct answer was retrieved by the system, which did not provided enough context to support it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Portuguese as Target</head><p>Six research groups took part in tasks with Portuguese as target language, submitting eigth runs: seven in the monolingual task, and one with English as source; unlike last year, no group presented Spanish as source. One new group (INESC) participated this. The group of University of Évora (UE) returned this year, while the group from NILC, the sole Brazilian group to take part to date, was absent.</p><p>Again, Priberam presented the best result for the third year in a row; the group of the University of Évora wasn't however far behind. As last year, we added the classification X-, meaning incomplete, while keeping the classification X+ for answers with extra text or other kinds of inexactness. In Table <ref type="table" coords="16,424.32,166.03,4.67,10.37" target="#tab_3">3</ref> we present the overall results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 14: Results of the runs with Portuguese as target: all 200 questions</head><p>A direct comparison with last year's results is not fully possible, due to the existance of multiple questions to each topic. Therefore, 14 presents results regarding the first question of each topic, which we believe is more readily comparable to the results of previous years. its last participation in 2005. We leave it to the participants to comment on whether it might have been caused by harder questions or changes (or lack thereof) in the systems. Question 94 was reclassified as NIL due to a spelling error, and question 135 because of the use of a word with a rare meaning. On the other hand, one system saw through that rare meaning, providing a correct answer; we decided to keep the question as NIL, considering correct both the system's answer and any NIL answer from other systems. The same system also found a correct answer to a NIL question, not discovered during the question creating process; that question was therefore reclassified as non-NIL. In the end, there were 13 NIL questions.</p><p>Table <ref type="table" coords="17,117.36,163.87,9.47,10.37" target="#tab_16">16</ref> shows the results for each answer type of definition questions, while Table <ref type="table" coords="17,425.52,163.87,9.35,10.37" target="#tab_17">17</ref> shows the results for each answer type of factoid questions (including list questions). As it can be seen, four out of six systems perform clearly better when it comes to definitions than to factoids. This may well have been helped by the use of Wikipedia texts, where a large proportion of articles begin with a definition.  We included in both Table <ref type="table" coords="17,202.68,598.15,9.47,10.37" target="#tab_16">16</ref> and in Table <ref type="table" coords="17,273.48,598.15,9.47,10.37" target="#tab_17">17</ref> a virtual run, called combination, in which one question is considered correct if at least one participating system found a valid answer. The objective of this combination run is to show the potential achievement when combining the capacities of all the participants. The combination run can be considered, somehow, state-of-the-art in monolingual Portuguese question answering. The system with best results, Priberam, answered correctly 72.7% the questions with at least one correct answer, not as dominating as last year. Despite being a bilingual run, LCC answered correctly 14 questions not answered by any of the monolingual systems.</p><p>In Table <ref type="table" coords="17,126.60,684.67,7.80,10.37" target="#tab_18">18</ref>, we present some values concerning answer and snippet size (in number of words). Temporally restricted questions: Table <ref type="table" coords="18,251.64,223.39,9.47,10.37" target="#tab_19">19</ref> presents the results of the 20 temporally restricted questions. As in previous years, the effectiveness of the systems to answer those questions is visibly lower than for non-TRQ questions (and indeed several systems only answered correctly question 160, which is a NIL TRQ). List questions: a total of twelve questions were defined as list questions; unlike last year, all these questions were closed list factoids, with two twelve answers each<ref type="foot" coords="18,320.28,429.70,3.04,6.75" target="#foot_1">2</ref> . The results were, in general, weak, with UE and LCC getting two correct answers, Priberam five, and all other system zero. There was a single case of incomplete answer (i.e., answering some elements of the list only), but it was judged W since, besides incomplete, it was also unsupported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Romanian as Target</head><p>At CLEF 2007 Romanian was addressed as a target language for the first time, based on the collection of Wikipedia Romanian pages frozen in November 2006, and as a source language for the second time, using the English news collection (Los Angeles Times, 1994 and Glasgow Herald, 1995) and the Wikipedia English pages.</p><p>Creation of Questions. The creation of the questions was realized at the Faculty of Computer Science, Al.I. Cuza University of Iasi. The group<ref type="foot" coords="18,226.92,572.38,3.04,6.75" target="#foot_2">3</ref> was very well instructed with respect to this task, using the Guidelines for Question Generation and based on a good feedback received from the organizers at IRST<ref type="foot" coords="18,430.20,583.06,3.04,6.75" target="#foot_3">4</ref> . The final 200 created questions are distributed according to table 20. </p><p>Most difficulties in this task were raised by deciding on the supporting snippets, especially for questions belonging to the same topic. We found unnatural to include answers through "copy-paste" from the text: if the question requires an answer in the Nominative case, but the text includes the answer in the Genitive case, then we had to include the Genitive in the answer, even if it is more natural to have the answer in Nominative.</p><p>Participants. This year two Romanian groups took part in the monolingual task with Romanian as a target language: the Faculty of Computer Science from the Al. I. Cuza University of Iasi, and the Research Institute for Artificial Intelligence from the Romanian Academy, Bucharest. Three runs were submitted -one by the first group and two by the second group, with the differences between due to the way they treated the questionprocessing and the answer-extraction. The 2007 results are presented in Tables 21 below. One system with Romanian as a source language and English as target was submitted by the Computational Linguistics Group from the University of Wolverhampton, United Kingdom. All three systems "crashed" on the LIST questions. The NIL questions are hard to classify, starting from the question-classifier (the classifier should "know" that the QA system has no possibility, no knowledge to find the answer). It would be better to have a clear separation between the NIL answers due to impossibility to find answer and the NIL answers classified as such by the system. None of the three systems could handle the questions related under one same topic: the systems returned at most the answer to the first question in a topic.</p><p>Assessment Procedure. Due to time restrictions, all three runs where judged by only one assessor at the Faculty of Computer Science in Iasi, so an inter-annotator agreement was not possible. Based on the Guidelines, all three systems were judged in parallel. The same evaluation criteria, especially with respect to the UNSUPPORTED and INEXACT answers, were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Spanish as Target</head><p>The participation at the Spanish as Target subtask has decreased from 9 groups in 2006 to 5 groups this year. All the runs were monolingual. We think that the changes in the task (linked questions and wikipedia) led to a lower participation and worse overall results because systems could not be tuned on time. Table <ref type="table" coords="19,432.24,584.11,9.35,10.37" target="#tab_22">22</ref> shows the summary of systems results with the number of Right (R), Wrong (W), Inexact (X) and Unsupported (U) answers. The table shows also the accuracy (in percentage) of factoids (F), factoids with temporal restriction (T), definitions (D) and list questions (L). Best values are marked in bold face. All the runs were assessed by two assessors. Only a 1.5% of the judgements were different and the resulting kappa value was 0,966, which corresponding to "almost perfect" assessment <ref type="bibr" coords="19,201.00,638.11,10.03,10.37" target="#b2">[7]</ref>. Best performing systems have obtained worse results than last year due mainly to the low performance in answering linked questions (15% of the questions) and due to the questions with answer only in Wikipedia.</p><p>Table <ref type="table" coords="20,117.00,232.63,9.23,10.37" target="#tab_23">23</ref> shows that considering only self-contained questions (the first one of each topic group) the results are closer to the ones obtained last year. In fact the accuracy for the linked questions is less than 20%. Table <ref type="table" coords="20,116.76,372.79,9.23,10.37" target="#tab_24">24</ref> shows some evidence on the effect of Wikipedia in the performance. When the answer appears only in Wikipedia the accuracy is reduced in more than 35% in all the cases. Regarding NIL questions, Table <ref type="table" coords="20,222.00,522.79,9.35,10.37" target="#tab_25">25</ref> shows the harmonic mean (F) of precision and recall for self-contained, linked and all questions. The best performing system has decreased their overall performance with respect to the last edition (see Table <ref type="table" coords="20,189.12,544.39,8.23,10.37" target="#tab_26">26</ref>) in NIL questions. However, the performance considering only self-contained questions is closer to the one obtained last year. The correlation coefficient r between the self-score and the correctness of the answers (shown in Table <ref type="table" coords="21,506.40,172.99,8.31,10.37" target="#tab_27">27</ref>) has been similar to the obtained last year, being not good enough yet, and explaining the low results in CWS and K1 <ref type="bibr" coords="21,106.44,194.59,11.03,10.37" target="#b1">[6]</ref> measures.</p><p>Since a supporting snippet is requested in order to assess the correctness of the answer, we have evaluated the systems capability to extract the answer when the snippet contains it. The first column of table <ref type="table" coords="21,469.08,216.31,9.47,10.37" target="#tab_27">27</ref> shows the percentage of cases where the correct answer was present in the snippet and correctly extracted. This information is very useful to diagnose if the lack of performance is due to the passage retrieval or to the answer extraction process. As shown in the table, the best systems are also better in the task of answer extraction, whereas the rest of systems still have a lot of room for improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Final considerations</head><p>This year the task was changed considerably and this affected the general level of results and also the level of participation in the task. The grouped questions could be regarded as more realistic and more searching but in consequence they were much more difficult. The policy of not declaring the question type means that if this is deduced incorrectly then the answer is bound to be wrong. Moreover, the policy of not even declaring the topic of a question group, but leaving it implicit (usually within the first question) means that if a system infers the topic wrongly, then all questions in the group will be answered wrongly. This should be probably re-considered, as it is not 'realistic'. In a real dialogue, if a question is answered inappropriately we do not dismiss all subsequent answers from that person, we simply re-phrase the question instead. The level of ambiguity concerning question type in a real dialogue is not fixed at some arbitrary value but varies according to many factors which the questioner estimates. In CLEF we are not modelling this process at all accurately and this affects the validity of our results. Finally, co-reference has now entered CLEF. This is interesting and useful but it might be preferable if we could separate the effect of co-reference resolution from other factors in analysing results. This could be done by marking up the co-references in the question corpus and allowing participants to use this information under certain circumstances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,298.32,425.80,17.54,12.28;7,254.52,420.34,12.26,36.71;7,391.32,425.80,30.78,12.21;7,375.96,425.80,9.20,12.21;7,331.80,425.80,30.10,12.21;7,317.28,425.80,7.40,12.21;7,303.96,425.80,3.66,12.21;7,284.28,425.80,6.78,12.21;7,275.52,425.80,5.50,12.21;7,341.04,450.40,23.36,12.21;7,306.84,450.40,22.74,12.21;7,291.72,450.40,8.62,12.21;7,284.64,450.40,5.50,12.21;7,277.20,452.71,6.04,9.39;7,200.52,431.37,17.45,15.98;7,205.80,431.65,4.70,15.67;7,170.64,431.65,23.49,15.67;7,229.44,435.36,9.67,15.03;7,126.00,487.51,149.34,10.37"><head></head><label></label><figDesc>) = 0 is established as a baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,188.88,296.93,233.67,8.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Best and average scores in CLEF QA campaigns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,160.68,657.44,289.85,7.66"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Best scores for systems using French as target in CLEF QA campaigns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="15,145.08,591.53,321.39,8.48"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Best and Average performance in the Monolingual and Bilingual tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,191.04,431.09,227.99,211.99"><head>Table 1 : Tasks activated in 2007 (in green)</head><label>1</label><figDesc></figDesc><table coords="2,191.04,450.23,227.99,192.84"><row><cell></cell><cell cols="8">TARGET LANGUAGES (corpus and answers)</cell></row><row><cell></cell><cell>BG</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>NL</cell><cell>PT</cell><cell>RO</cell></row><row><cell></cell><cell>BG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SOURCE LANGUAGES (questions)</cell><cell>DE EN ES FR IN IT NL PT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,109.68,197.71,409.02,44.09"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,109.68,197.71,409.02,44.09"><row><cell>,37 tasks were proposed:</cell></row><row><cell>• 8 Monolingual -i.e. Bulgarian (BG), German (DE), Spanish (ES), French (FR), Italian (IT), Dutch (NL),</cell></row><row><cell>Portuguese (PT) and Romanian (RO;</cell></row><row><cell>• 29 Cross-lingual.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,158.76,282.41,293.82,134.60"><head>Table 2 : Tasks chosen by at least 1 participant in QA@CLEF campaigns.</head><label>2</label><figDesc></figDesc><table coords="3,167.16,310.73,257.27,106.28"><row><cell></cell><cell>MONOLINGUAL</cell><cell>CROSS-LINGUAL</cell></row><row><cell>CLEF 2004</cell><cell>6</cell><cell>13</cell></row><row><cell>CLEF 2005</cell><cell>8</cell><cell>15</cell></row><row><cell>CLEF 2006</cell><cell>7</cell><cell>17</cell></row><row><cell>CLEF 2007</cell><cell>7</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,160.56,68.36,292.07,365.05"><head>Table 3 : Document collections used in CLEF 2007.</head><label>3</label><figDesc></figDesc><table coords="4,160.56,84.22,292.07,349.19"><row><cell>TARGET LANG..</cell><cell>COLLECTION</cell><cell>PERIOD</cell><cell>SIZE</cell></row><row><cell>Bulgarian (BG)</cell><cell>Sega</cell><cell>2002</cell><cell>120 MB (33,356 docs)</cell></row><row><cell></cell><cell>Standart</cell><cell>2002</cell><cell>93 MB (35,839 docs)</cell></row><row><cell></cell><cell>Frankfurter Rundschau</cell><cell>1994</cell><cell>320 MB (139,715 docs)</cell></row><row><cell>Germany (DE)</cell><cell>Der Spiegel</cell><cell>1994/1995</cell><cell>63 MB (13,979 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1994</cell><cell>144 MB (71,677 docs)</cell></row><row><cell></cell><cell>German SDA</cell><cell>1995</cell><cell>141 MB (69,438 docs)</cell></row><row><cell>English (EN)</cell><cell>Los Angeles Times</cell><cell>1994</cell><cell>425 MB (113,005 docs)</cell></row><row><cell></cell><cell>Glasgow Herald</cell><cell>1995</cell><cell>154 MB (56,472 docs)</cell></row><row><cell>Spanish (ES)</cell><cell>EFE</cell><cell>1994</cell><cell>509 MB (215,738 docs)</cell></row><row><cell></cell><cell>EFE</cell><cell>1995</cell><cell>577 MB (238,307 docs)</cell></row><row><cell></cell><cell>Le Monde</cell><cell>1994</cell><cell>157 MB (44,013 docs)</cell></row><row><cell>French (FR)</cell><cell>Le Monde</cell><cell>1995</cell><cell>156 MB (47,646 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1994</cell><cell>86 MB (43,178 docs)</cell></row><row><cell></cell><cell>French SDA</cell><cell>1995</cell><cell>88 MB (42,615 docs)</cell></row><row><cell>Italian (IT)</cell><cell>La Stampa</cell><cell>1994</cell><cell>193 MB (58,051 docs)</cell></row><row><cell></cell><cell>Itallian SDA</cell><cell>1994</cell><cell>85 MB (50,527 docs)</cell></row><row><cell></cell><cell>Itallian SDA</cell><cell>1995</cell><cell>85 MB (50,527 docs)</cell></row><row><cell>Dutch (NL)</cell><cell>NRC Handelsblad</cell><cell>1994/1995</cell><cell>299 MB (84,121 docs)</cell></row><row><cell></cell><cell>Algemeen Dagblad</cell><cell>1994/1995</cell><cell>241 MB (106,483 docs)</cell></row><row><cell></cell><cell>Público</cell><cell>1994</cell><cell>164 MB (51,751 docs)</cell></row><row><cell>Portuguese (PT)</cell><cell>Público</cell><cell>1995</cell><cell>176 MB (55,070 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1994</cell><cell>108 MB (51,875 docs)</cell></row><row><cell></cell><cell>Folha de São Paulo</cell><cell>1995</cell><cell>116 MB (52,038 docs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,92.76,521.81,425.99,174.43"><head>Table 4 : Test set breakdown according to question type</head><label>4</label><figDesc></figDesc><table coords="5,92.76,538.73,425.99,157.51"><row><cell></cell><cell>F</cell><cell>D</cell><cell>L</cell><cell>T</cell><cell>NIL</cell></row><row><cell>BG</cell><cell>158</cell><cell>32</cell><cell>10</cell><cell>12</cell><cell>0</cell></row><row><cell>DE</cell><cell>164</cell><cell>28</cell><cell>8</cell><cell>27</cell><cell>0</cell></row><row><cell>EN</cell><cell>161</cell><cell>30</cell><cell>9</cell><cell>3</cell><cell>0</cell></row><row><cell>ES</cell><cell>148</cell><cell>42</cell><cell>10</cell><cell>40</cell><cell>21</cell></row><row><cell>FR</cell><cell>148</cell><cell>42</cell><cell>10</cell><cell>40</cell><cell>20</cell></row><row><cell>IT</cell><cell>147</cell><cell>41</cell><cell>12</cell><cell>38</cell><cell>11</cell></row><row><cell>NL</cell><cell>147</cell><cell>40</cell><cell>13</cell><cell>30</cell><cell>20</cell></row><row><cell>PT</cell><cell>143</cell><cell>47</cell><cell>9</cell><cell>23</cell><cell>18</cell></row><row><cell>RO</cell><cell>160</cell><cell>30</cell><cell>10</cell><cell>52</cell><cell>7</cell></row><row><cell cols="6">Besides, all types of questions could contain a temporal restriction, i.e. a temporal specification that provided</cell></row><row><cell cols="4">important information for the retrieval of the correct answer, for example:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,92.76,282.05,406.39,121.63"><head>Table 5 : Number of participating groups</head><label>5</label><figDesc></figDesc><table coords="6,92.76,297.79,406.39,105.89"><row><cell></cell><cell cols="4">America Europe Asia Australia</cell><cell>TOTAL</cell></row><row><cell>CLEF 2003</cell><cell>3</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>8</cell></row><row><cell>CLEF 2004</cell><cell>1</cell><cell>17</cell><cell>-</cell><cell>-</cell><cell>18</cell></row><row><cell>CLEF 2005</cell><cell>1</cell><cell>22</cell><cell>1</cell><cell>-</cell><cell>24</cell></row><row><cell>CLEF 2006</cell><cell>4</cell><cell>24</cell><cell>2</cell><cell>-</cell><cell>30</cell></row><row><cell>CLEF 2007</cell><cell>3</cell><cell>17</cell><cell>1</cell><cell>1</cell><cell>22</cell></row><row><cell cols="6">The geographical distribution has anyway remained almost the same, recording a new entry of a group from</cell></row><row><cell cols="3">Australia. No participants took part to any Bulgarian tasks.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,234.96,422.81,194.12,49.03"><head>Table 6 . Number of submitted runs</head><label>6</label><figDesc></figDesc><table coords="6,246.36,439.87,182.72,31.97"><row><cell>Number of submitted runs</cell><cell>Monolingual</cell><cell>Cross-lingual</cell></row><row><cell>#</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,176.16,481.37,232.31,85.88"><head>CLEF 2003 17 6 11 CLEF 2004 48 20 28 CLEF 2005 67 43 24 CLEF 2006 77 42 35 CLEF 2007 37 20 17</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,130.44,486.29,339.35,131.35"><head>Table 6 : Distribution of topic types and expected answer for questions. Topic type Number of topics Expected answer type</head><label>6</label><figDesc></figDesc><table coords="8,426.24,508.37,43.55,19.16"><row><cell>Number of</cell></row><row><cell>questions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,92.64,292.52,422.14,138.16"><head>Table 9 : Results of the monolingual and bilingual French runs.</head><label>9</label><figDesc></figDesc><table coords="12,92.64,319.85,422.14,110.83"><row><cell></cell><cell>Assesse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>d Answer s #</cell><cell cols="2">R W X # # #</cell><cell cols="5">U % F % T % D % L # [163] [41] [27] [10] #</cell><cell cols="2">NIL [9] %</cell><cell>CWS</cell><cell>K1</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell>syn07frfr</cell><cell>200</cell><cell>108 82</cell><cell>9</cell><cell>1 52.76</cell><cell>46.3 4</cell><cell>74.07</cell><cell>20</cell><cell cols="2">40</cell><cell>22. 5</cell><cell>-</cell><cell>-</cell><cell>54 %</cell></row><row><cell>lcc07enfr</cell><cell>194</cell><cell cols="2">81 95 14</cell><cell>4 44.17</cell><cell>46.3 4</cell><cell>22.22</cell><cell>30</cell><cell>0</cell><cell></cell><cell cols="2">0 0.2223</cell><cell>-0.1235</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="13,141.00,568.13,318.88,72.91"><head>Table 10 : Results through the years.</head><label>10</label><figDesc></figDesc><table coords="13,141.00,581.33,318.88,59.71"><row><cell>Year</cell><cell>Best Mono</cell><cell cols="2">Aggregated Mono Best Cross</cell><cell>Aggregated Cross</cell></row><row><cell>2007</cell><cell>30</cell><cell>45</cell><cell>18.5</cell><cell>18.5</cell></row><row><cell>2006</cell><cell>42.33</cell><cell>64.02</cell><cell>32.98</cell><cell>33.86</cell></row><row><cell>2005</cell><cell>43.5</cell><cell>58.5</cell><cell>23</cell><cell>28</cell></row><row><cell>2004</cell><cell>34.01</cell><cell>43.65</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="14,96.60,332.81,416.47,147.27"><head>Table 11 . System Performance -Details</head><label>11</label><figDesc></figDesc><table coords="14,96.60,362.21,416.47,117.87"><row><cell>Run</cell><cell>R #</cell><cell>W #</cell><cell cols="2">X U % F # # [164]</cell><cell>% T [27]</cell><cell>% D [28]</cell><cell>% L [8]</cell><cell>#</cell><cell>NIL % [0]</cell><cell>CWS</cell><cell>K1</cell><cell>accuracy</cell><cell>Overall</cell></row><row><cell>dfki071dede M</cell><cell>60</cell><cell cols="2">121 14 5</cell><cell>29.8</cell><cell cols="2">14.81 39.29</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">30</cell></row><row><cell cols="2">fuha071dede M 48</cell><cell cols="5">146 4 2 24.39 18.52 28.57</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.086 -0.17</cell><cell cols="2">24</cell></row><row><cell cols="2">fuha072dede M 30</cell><cell cols="4">164 4 2 17.07 14.81</cell><cell>7.14</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0.048 -0.31</cell><cell cols="2">15</cell></row><row><cell>dfki071ende C</cell><cell>37</cell><cell cols="4">144 18 1 17.68 14.81</cell><cell>25</cell><cell>12,5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">18.5</cell></row><row><cell>dfki071ptde C</cell><cell>10</cell><cell cols="2">180 10 0</cell><cell>3.66</cell><cell>7.41</cell><cell>14.29</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="14,169.68,575.09,271.95,123.15"><head>Table 12 : Inter-Assessor Agreement/Disagreement (breakdown)</head><label>12</label><figDesc>Table12describes the inter-rater disagreement on the assessment of answers in terms of question and answer disagreement.</figDesc><table coords="14,169.68,598.85,271.95,99.39"><row><cell>Run ID</cell><cell># Questions</cell><cell cols="6"># Q-Disagreements Total F D L X U W/R</cell></row><row><cell>dfki071dede M</cell><cell>200</cell><cell>20</cell><cell>16</cell><cell>4</cell><cell cols="2">0 15 4</cell><cell>1</cell></row><row><cell>fuha071dede M</cell><cell>200</cell><cell>13</cell><cell>10</cell><cell>3</cell><cell>0</cell><cell>7 3</cell><cell>3</cell></row><row><cell>fuha072dede M</cell><cell>200</cell><cell>7</cell><cell>6</cell><cell>1</cell><cell>0</cell><cell>2 2</cell><cell>3</cell></row><row><cell>dfki071ende C</cell><cell>200</cell><cell>13</cell><cell>7</cell><cell>5</cell><cell cols="2">1 12 1</cell><cell>0</cell></row><row><cell>dfki071ptde C</cell><cell>200</cell><cell>8</cell><cell>3</cell><cell>5</cell><cell>0</cell><cell>8 0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="15,104.16,233.09,413.95,101.52"><head>Table 13 :</head><label>13</label><figDesc>Results.</figDesc><table coords="15,384.00,251.57,16.55,8.48"><row><cell>NIL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="16,92.76,248.03,425.94,452.77"><head>Table 15 : Results of the runs with Portuguese as target: answers to the first question of the 149 topics</head><label>15</label><figDesc></figDesc><table coords="16,92.76,494.39,425.84,174.01"><row><cell>Run Name</cell><cell>R (#)</cell><cell>W (#)</cell><cell>X+ (#)</cell><cell>X-(#)</cell><cell>U (#)</cell><cell>Overall Accuracy (%)</cell></row><row><cell>diue071ptpt</cell><cell>61</cell><cell>77</cell><cell>1</cell><cell>9</cell><cell>1</cell><cell>40,9%</cell></row><row><cell>esfi071ptpt</cell><cell>11</cell><cell>132</cell><cell>0</cell><cell>4</cell><cell>2</cell><cell>7,4%</cell></row><row><cell>esfi072ptpt</cell><cell>6</cell><cell>141</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>4,0%</cell></row><row><cell>feup071ptpt</cell><cell>34</cell><cell>113</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>22,8%</cell></row><row><cell>ines071ptpt</cell><cell>17</cell><cell>125</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>11,4%</cell></row><row><cell>ines072ptpt</cell><cell>21</cell><cell>122</cell><cell>0</cell><cell>4</cell><cell>2</cell><cell>14,1%</cell></row><row><cell>prib071ptpt</cell><cell>92</cell><cell>86</cell><cell>3</cell><cell>5</cell><cell>1</cell><cell>61,7%</cell></row><row><cell>lcc_071enpt</cell><cell>44</cell><cell>48</cell><cell>7</cell><cell>3</cell><cell>9</cell><cell>29,5%</cell></row><row><cell cols="7">As it can be seen, the removal of subsequent questions to each topic doesn't cause a big change on the overal</cell></row></table><note coords="16,92.76,668.83,425.94,10.37;16,92.76,679.63,425.82,10.37;16,92.76,690.43,425.83,10.37"><p><p><p>results, apart from a clear improvement by Priberam. On the whole, compared to last year</p>(Vallin et al., 2007)</p>, Priberam saw a slight drop on its results, Raposa (FEUP) a clear improvement from an admitedly low level, Esfinge (SINTEF) a clear drop, and LCC kept last year's levels. Senso (UE) shows a marked improvement since</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="17,138.72,225.65,333.87,165.04"><head>Table 16 : Results of the assessment of the monolingual Portuguese runs: definitions</head><label>16</label><figDesc></figDesc><table coords="17,204.84,253.16,201.88,137.53"><row><cell>Run</cell><cell>obj 6</cell><cell>org 6</cell><cell>oth 9</cell><cell cols="3">per TOT % 9 30</cell></row><row><cell>diue071ptpt</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>19</cell><cell>63%</cell></row><row><cell>esfi071ptpt</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3%</cell></row><row><cell>esfi072ptpt</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3%</cell></row><row><cell>feup071ptpt</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>7</cell><cell>16</cell><cell>53%</cell></row><row><cell>ines071ptpt</cell><cell>4</cell><cell>4</cell><cell>6</cell><cell>0</cell><cell>14</cell><cell>47%</cell></row><row><cell>ines072ptpt</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>2</cell><cell>18</cell><cell>60%</cell></row><row><cell>prib071ptpt</cell><cell>6</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>23</cell><cell>77%</cell></row><row><cell>combination</cell><cell>6</cell><cell>5</cell><cell>8</cell><cell>9</cell><cell>27</cell><cell>87%</cell></row><row><cell>lcc_071enpt</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>8</cell><cell>27%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="17,140.28,420.89,330.76,166.32"><head>Table 17 : Results of the assessment of the Portuguese runs: factoids, including lists</head><label>17</label><figDesc></figDesc><table coords="17,154.08,448.64,303.19,138.57"><row><cell>Run</cell><cell>cou 21</cell><cell>loc 31</cell><cell cols="2">mea obj 16 5</cell><cell>org 21</cell><cell>oth 26</cell><cell>per 21</cell><cell cols="3">tim TOT % 19 160</cell></row><row><cell>diue071ptpt</cell><cell>11</cell><cell>17</cell><cell>4</cell><cell>3</cell><cell>6</cell><cell>8</cell><cell>7</cell><cell>9</cell><cell cols="2">65 39%</cell></row><row><cell>esfi071ptpt</cell><cell>3</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>7</cell><cell>15</cell><cell>9%</cell></row><row><cell>esfi072ptpt</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>11</cell><cell>7%</cell></row><row><cell>feup071ptpt</cell><cell>4</cell><cell>8</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell cols="2">24 15%</cell></row><row><cell>ines071ptpt</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>8</cell><cell>5%</cell></row><row><cell>ines072ptpt</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>10</cell><cell>6%</cell></row><row><cell>prib071ptpt</cell><cell>9</cell><cell>15</cell><cell>10</cell><cell>1</cell><cell>11</cell><cell>14</cell><cell>8</cell><cell>10</cell><cell cols="2">78 46%</cell></row><row><cell>combination</cell><cell>16</cell><cell>24</cell><cell>12</cell><cell>3</cell><cell>12</cell><cell>17</cell><cell>12</cell><cell cols="3">13 109 68%</cell></row><row><cell>lcc_071enpt</cell><cell>7</cell><cell>11</cell><cell>6</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>4</cell><cell>6</cell><cell cols="2">48 29%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="18,128.76,68.81,348.47,151.36"><head>Table 18 : average size of answers</head><label>18</label><figDesc></figDesc><table coords="18,128.76,85.19,348.47,134.98"><row><cell></cell><cell>Non-NIL</cell><cell>Average</cell><cell>Average answer</cell><cell>Average</cell><cell>Average</cell></row><row><cell>Run name</cell><cell>Answers</cell><cell>answer</cell><cell>size</cell><cell>snippet</cell><cell>snippet size</cell></row><row><cell></cell><cell>(#)</cell><cell>size</cell><cell>(R only)</cell><cell>size</cell><cell>(R only)</cell></row><row><cell>diue071ptpt</cell><cell>89</cell><cell>2.8</cell><cell>2.9</cell><cell>25.0</cell><cell>24.3</cell></row><row><cell>esfi071ptpt</cell><cell>57</cell><cell>2.4</cell><cell>2.8</cell><cell>56.3</cell><cell>29.3</cell></row><row><cell>esfi072ptpt</cell><cell>19</cell><cell>2.4</cell><cell>2.8</cell><cell>59.7</cell><cell>29.1</cell></row><row><cell>feup071ptpt</cell><cell>56</cell><cell>2.7</cell><cell>3.3</cell><cell>59.8</cell><cell>32.9</cell></row><row><cell>ines071ptpt</cell><cell>49</cell><cell>3.7</cell><cell>4.8</cell><cell>60.7</cell><cell>33.6</cell></row><row><cell>ines072ptpt</cell><cell>47</cell><cell>3.8</cell><cell>5.3</cell><cell>61.7</cell><cell>34.2</cell></row><row><cell>prib071ptpt</cell><cell>182</cell><cell>3.5</cell><cell>4.4</cell><cell>49.6</cell><cell>32.4</cell></row><row><cell>lcc_071enpt</cell><cell>191</cell><cell>3.4</cell><cell>4.2</cell><cell>45.2</cell><cell>32.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="18,150.96,263.45,306.35,142.84"><head>Table 19 : accuracy of temporally restricted questions</head><label>19</label><figDesc></figDesc><table coords="18,150.96,278.27,306.35,128.01"><row><cell>Run name</cell><cell>Correct answers (#)</cell><cell>(%)</cell><cell>Non-T.R.Q correctness (%)</cell><cell>Total correctness (%)</cell></row><row><cell>diue071ptpt</cell><cell>4</cell><cell>20.0</cell><cell>44.4</cell><cell>42.0</cell></row><row><cell>esfi071ptpt</cell><cell>1</cell><cell>5.0</cell><cell>8.3</cell><cell>8.0</cell></row><row><cell>esfi072ptpt</cell><cell>1</cell><cell>5.0</cell><cell>6.1</cell><cell>6.0</cell></row><row><cell>feup071ptpt</cell><cell>1</cell><cell>5.0</cell><cell>21.7</cell><cell>20.0</cell></row><row><cell>ines071ptpt</cell><cell>1</cell><cell>5.0</cell><cell>11.7</cell><cell>11.0</cell></row><row><cell>ines072ptpt</cell><cell>1</cell><cell>5.0</cell><cell></cell><cell>14.0</cell></row><row><cell>prib071ptpt</cell><cell>8</cell><cell>40.0</cell><cell>51.7</cell><cell>28.0</cell></row><row><cell>lcc_071enpt</cell><cell>6</cell><cell>30.0</cell><cell>27.8</cell><cell>50.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="19,94.32,68.81,427.01,96.44"><head>Table 20 : Question types distribution in Romanian</head><label>20</label><figDesc></figDesc><table coords="19,94.32,88.67,427.01,76.57"><row><cell></cell><cell>PERSON</cell><cell>TIME</cell><cell>LOCATION</cell><cell>ORGA-NIZATION</cell><cell>MEASURE</cell><cell>COUNT</cell><cell>OBJECT</cell><cell>OTHER</cell><cell>TOTAL</cell></row><row><cell>FACTOID</cell><cell>22</cell><cell>17</cell><cell>21</cell><cell>19</cell><cell>17</cell><cell>20</cell><cell>16</cell><cell>21</cell><cell>153</cell></row><row><cell>DEFINITION</cell><cell>9</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>6</cell><cell>10</cell><cell>30</cell></row><row><cell>LIST</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>NIL QUESTIONS</cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="19,80.52,319.49,450.38,82.60"><head>Tables 21: Results in the monolingual task, Romanian as target language</head><label></label><figDesc></figDesc><table coords="19,80.52,344.24,450.38,57.85"><row><cell>Run</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>Overall Accuracy</cell><cell>NIL RETURNED</cell><cell>NIL correct</cell><cell>CWS</cell></row><row><cell>outputRoRo (1)</cell><cell>24</cell><cell>171</cell><cell>4</cell><cell>1</cell><cell>12</cell><cell>100</cell><cell>5</cell><cell>0.02889</cell></row><row><cell>ICIA071RORO (2)</cell><cell>60</cell><cell>105</cell><cell>34</cell><cell>1</cell><cell>30</cell><cell>54</cell><cell>7</cell><cell>0.09522</cell></row><row><cell>ICIA072RORO (3)</cell><cell>60</cell><cell>101</cell><cell>39</cell><cell>0</cell><cell>30</cell><cell>54</cell><cell>7</cell><cell>0.09522</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="20,106.68,68.81,409.36,130.20"><head>Table 22 : Results at the Spanish as target.</head><label>22</label><figDesc></figDesc><table coords="20,106.68,96.24,409.36,102.76"><row><cell>Run</cell><cell>R #</cell><cell>W #</cell><cell>X #</cell><cell>U #</cell><cell>% F [115]</cell><cell>% T [43]</cell><cell cols="2">% D % L [32] [10]</cell><cell>#</cell><cell>NIL</cell><cell>F [8]</cell><cell>CWS</cell><cell>K1</cell><cell>accuracy</cell><cell>Overall</cell><cell>%</cell></row><row><cell>Priberam</cell><cell>89</cell><cell>87</cell><cell>3</cell><cell>21</cell><cell cols="3">47,82 23,25 68,75</cell><cell>20</cell><cell>3</cell><cell cols="2">0,29</cell><cell>-</cell><cell>-</cell><cell cols="3">44,5</cell></row><row><cell>Inaoe</cell><cell>69</cell><cell>118</cell><cell>7</cell><cell>6</cell><cell cols="3">28,69 18,60 87,50</cell><cell>-</cell><cell>3</cell><cell cols="4">0,12 0,175 -0,287</cell><cell cols="3">34,5</cell></row><row><cell>Miracle</cell><cell>30</cell><cell>158</cell><cell>4</cell><cell>8</cell><cell>20</cell><cell cols="2">13,95 3,12</cell><cell>-</cell><cell>1</cell><cell cols="4">0,07 0,022 -0,452</cell><cell></cell><cell>15</cell></row><row><cell>UPV</cell><cell>23</cell><cell>166</cell><cell>5</cell><cell>6</cell><cell cols="2">13,08 9,30</cell><cell>12,5</cell><cell>-</cell><cell>1</cell><cell cols="4">0,03 0,015 -0,224</cell><cell cols="3">11,5</cell></row><row><cell>TALP</cell><cell>14</cell><cell>183</cell><cell>1</cell><cell>2</cell><cell>6,08</cell><cell cols="2">2,32 18,65</cell><cell>-</cell><cell>3</cell><cell cols="4">0,07 0,007 -0.34</cell><cell></cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="20,142.80,267.44,325.87,93.81"><head>Table 23 . Results for self-contained and linked questions, compared with overall accuracy.</head><label>23</label><figDesc></figDesc><table coords="20,151.56,283.16,308.30,78.09"><row><cell></cell><cell>% accuracy over</cell><cell>% accuracy over</cell><cell>% Overall Accuracy</cell></row><row><cell>Run</cell><cell>Self-contained questions</cell><cell>Linked questions</cell><cell>[200]</cell></row><row><cell></cell><cell>[170]</cell><cell>[30]</cell><cell></cell></row><row><cell>Priberam</cell><cell>49,41</cell><cell>16,66</cell><cell>44,5</cell></row><row><cell>Inaoe</cell><cell>37,64</cell><cell>16,66</cell><cell>34,5</cell></row><row><cell>Miracle</cell><cell>15,29</cell><cell>13,33</cell><cell>15</cell></row><row><cell>UPV</cell><cell>12,94</cell><cell>3,33</cell><cell>11,5</cell></row><row><cell>TALP</cell><cell>7,05</cell><cell>6,66</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" coords="20,190.56,407.60,227.54,103.65"><head>Table 24 : Results for questions with answer in Wikipedia</head><label>24</label><figDesc></figDesc><table coords="20,190.56,423.44,227.54,87.81"><row><cell></cell><cell>% accuracy over</cell><cell>% accuracy over</cell></row><row><cell>Run</cell><cell>questions with answer only in wikipedia</cell><cell>questions with answer in both EFE and wikipedia</cell></row><row><cell></cell><cell>[114]</cell><cell>[71]</cell></row><row><cell>Priberam</cell><cell>40.35%</cell><cell>54.93%</cell></row><row><cell>Inaoe</cell><cell>29.82%</cell><cell>42.25%</cell></row><row><cell>Miracle</cell><cell>7.89%</cell><cell>28.17%</cell></row><row><cell>UPV</cell><cell>7.02%</cell><cell>19.72%</cell></row><row><cell>TALP</cell><cell>0%</cell><cell>14.08%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="20,186.24,589.73,239.07,99.84"><head>Table 25 : Results at the Spanish as target for NIL questions</head><label>25</label><figDesc></figDesc><table coords="20,200.64,611.60,210.17,77.97"><row><cell></cell><cell>F-measure</cell><cell>F-</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell></cell><cell>(Self-</cell><cell>measure</cell><cell>(Overall)</cell><cell>(Overall)</cell></row><row><cell></cell><cell>contained)</cell><cell>(Overall)</cell><cell></cell><cell></cell></row><row><cell>Priberam</cell><cell>0.4</cell><cell>0.29</cell><cell>0.23</cell><cell>0.38</cell></row><row><cell>Inaoe</cell><cell>0.13</cell><cell>0.12</cell><cell>0.07</cell><cell>0.38</cell></row><row><cell>Miracle</cell><cell>0.07</cell><cell>0.07</cell><cell>0.05</cell><cell>0.13</cell></row><row><cell>UPV</cell><cell>0.04</cell><cell>0.03</cell><cell>0.02</cell><cell>0.13</cell></row><row><cell>TALP</cell><cell>0.06</cell><cell>0.07</cell><cell>0.04</cell><cell>0.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26" coords="21,212.04,68.48,187.27,92.32"><head>Table 26 . Evolution of best results in NIL questions.</head><label>26</label><figDesc></figDesc><table coords="21,267.72,86.93,73.15,73.87"><row><cell cols="2">Year F-measure</cell></row><row><cell>2003</cell><cell>0,25</cell></row><row><cell>2004</cell><cell>0,30</cell></row><row><cell>2005</cell><cell>0,38</cell></row><row><cell>2006</cell><cell>0,46</cell></row><row><cell>2007</cell><cell>0,29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="21,128.16,283.25,355.07,98.64"><head>Table 27 . Answer Extraction and correlation coefficient r results at the Spanish as target</head><label>27</label><figDesc></figDesc><table coords="21,241.44,305.00,122.30,76.89"><row><cell></cell><cell>% Answer</cell><cell>r</cell></row><row><cell>Run</cell><cell>Extraction</cell><cell></cell></row><row><cell>Priberam</cell><cell>93,68</cell><cell>-</cell></row><row><cell>INAOE</cell><cell>75</cell><cell>0,1170</cell></row><row><cell>Miracle</cell><cell>49,18</cell><cell>0,237</cell></row><row><cell>UPV</cell><cell>54,76</cell><cell>-0,1003</cell></row><row><cell>TALP</cell><cell>53,84</cell><cell>0,134</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="11,98.28,700.75,328.79,10.37"><p>Perhaps it is just a consequence of setting too many undergraduate examination papers!</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="18,98.28,657.55,392.34,10.37"><p>There were some open list questions as well, but they were classified and evaluated as ordinary factoids.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="18,98.28,668.35,420.59,10.37;18,92.76,679.03,199.59,10.37"><p>Three Computational Linguistics Master students: Anca Onofraşc, Ana-Maria Rusu, Cristina Despa, supervised and working in collaboration with the two organizers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="18,100.08,689.95,418.67,10.37;18,92.76,700.75,37.14,10.37"><p>Without the help received from Danilo Giampicolo and Pamela Forner, we wouldn't have solved all our problems.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>A special thank to <rs type="person">Bernardo Magnini</rs> (<rs type="affiliation">FBK-irst, Trento, Italy</rs>), who has given his precious advise and valueble support at many levels for the preparation and realization of the QA track at CLEF 2007.</p><p><rs type="person">Anselmo Peñas</rs> has been partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> within the <rs type="projectName">Text-Mess-INES</rs> project (<rs type="grantNumber">TIN2006-15265-C06-02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EqkfAQS">
					<idno type="grant-number">TIN2006-15265-C06-02</idno>
					<orgName type="project" subtype="full">Text-Mess-INES</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="22,96.21,132.19,237.45,10.37;22,109.68,143.19,277.37,9.37" xml:id="b0">
	<monogr>
		<ptr target="http://clef-qa.itc.it/2007/download/QA@CLEF07_Guidelines-for-Participants.pdf" />
		<title level="m" coord="22,109.68,132.19,152.86,10.37">QA@CLEF 2007 Organizing Committee</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,96.21,152.71,422.61,10.37;22,109.68,163.63,408.90,10.37;22,109.68,174.31,408.99,10.37;22,109.68,185.11,84.23,10.37" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,243.24,152.71,169.48,10.37">Question answering pilot task at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,356.04,163.63,162.54,10.37;22,109.68,174.31,70.73,10.37">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="22,187.20,174.31,132.90,10.37">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Hidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,96.21,195.91,422.37,10.37;22,109.68,206.83,93.06,10.37" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,265.80,195.91,249.21,10.37">The measurements of observer agreement for categorical data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,109.68,206.83,39.54,10.37">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
