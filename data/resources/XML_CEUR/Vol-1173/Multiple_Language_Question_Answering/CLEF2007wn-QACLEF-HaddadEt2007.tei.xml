<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,116.10,138.88,363.23,14.42;1,226.74,157.30,142.09,14.42">Cross Lingual Question Answering using CINDI_QA for QA@CLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,234.42,185.58,60.22,9.02"><forename type="first">Chedid</forename><surname>Haddad</surname></persName>
							<email>c_haddad@cs.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 De Maisonneuve Blvd. W</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.95,185.58,59.13,9.02"><forename type="first">Bipin</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
							<email>bcdesai@cs.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 De Maisonneuve Blvd. W</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,116.10,138.88,363.23,14.42;1,226.74,157.30,142.09,14.42">Cross Lingual Question Answering using CINDI_QA for QA@CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E473B4C1D9292FD8809B96D58FCB3F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Question answering, Questions beyond factoids, Bilingual, French, English</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the first participation of the CINDI group in the Multiple Language Question Answering Cross Language Evaluation Forum (QA@CLEF). We participated in a track using French as the source language and English as the target language. CINDI_QA first uses an online translation tool to convert the French input question into an English sentence. Second, a Natural Language Parser extracts keywords such as verbs, nouns, adjectives and capitalized entites from the query. Third, synonyms of those keywords are generated thanks to a Lexical Reference module. Fourth, our integrated Searching and Indexing component localises the answer candidates from the QA@CLEF data collection provided to us. Finally, the candidates are matched against our existing set of templates to decide on the best answer to return to the user. Two runs were submitted. Having missed using a large part of the corpora, CINDI_QA was only able to generate correct and exact answers for 13% of the questions it received.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Concordia Index for Navigation and Discovery on the Internet (CINDI <ref type="bibr" coords="1,385.64,645.54,11.24,9.02" target="#b0">[1]</ref>) group has been founded at the Department of Computer Science and Software Engineering of Concordia in the late 1990s. Its purpose is the continuous enhancement of information discovery and retrieval. Hence, CINDI_QA has been created to tackle bilingual question answering within the spectrum of QA@CLEF 2007.</p><p>The paper is organized in the following way: we first go through the system overview with an emphasis on architecture. We then list the tools incorporated in CINDI_QA. Afterwards, we take a look at the template matching mechanism that drives the system and its application for the QA@CLEF participation and the results obtained. The conclusion follows where we highlight what we learned and how we intend to improve CINDI_QA's performance for the future editions of QA@CLEF. The system's "brain" is located in the CINDI_QA Processor where all the logical inferences are made. Since CINDI_QA communicates with several tools, the information it retrieves from those modules -which is meaningless on its own -is analyzed in the Processor in a structured way that helps build the eventual answer. This process is done in a pre-defined order, with the Processor getting data from one module, sorting it out then using it to probe the next module.</p><p>One of the peripheral components is the PHP Script that acts as an interface between the Online Translator and the Processor. Its purpose is to send the French question to the Online Translator and bring its English equivalent back. The other peripheral unit is the Graphical User Interface (GUI) which is the fa√ßade of our system for the user; it will prompt for a French question and return the English answer.</p><p>In a typical scenario, the question inputted in French by the user is translated to English then parsed to extract the keywords. Afterward, the synonyms of the keywords are obtained to form an internal query sent to the Search engine that already has the CLEF data indexed. The answer candidates are localized at which point they are matched against a pre-existing set of templates that enables the selection of the best answer. That answer, which is in English, is sent back to the user.</p><p>In order to improve performance, CINDI_QA allows user interaction to direct its flow of operations. Actually, CINDI_QA can be run in two modes. In the automatic mode, it acts as a complete black box by only taking a question and returning an answer. In the feedback mode, it has the possibility of disambiguating the query by prompting the user after each stage. For example, since certain synonyms of a word often don't suit a particular context, the user can choose to eliminate them and only retain relevant ones.</p><p>The process flow of CINDI_QA in the feedback mode is shown in the following diagram. The next section lists the tools used to build CINDI_QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tools Integration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online Translation</head><p>Since we are working in a bilingual environment, the system is queried in a language different from the data collection it is using as reference. A translation tool is needed for this reason. After researching the available tools, we noticed that the Google <ref type="bibr" coords="4,181.02,148.80,10.57,9.02" target="#b1">[2]</ref>, Babel Fish <ref type="bibr" coords="4,243.20,148.80,11.61,9.02" target="#b2">[3]</ref> and Systran [4] translators appear to be powered by the same engine because they offer the same result when asked to translate from French to English.</p><p>We chose to use Google Translate in our system due to its better interface and speed of processing. A PHP script is responsible of delivering the French question typed in by the user to the Google Translate webpage and bring back the translated English equivalent to the CINDI_QA Processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Natural Language Parsing</head><p>We need a way to understand the question asked by the user in order to single out the keywords. This was achieved thanks to the Link Grammar Parser <ref type="bibr" coords="4,253.99,254.58,10.61,9.02" target="#b3">[5]</ref>, a syntactic parser of English based on link grammar, an original theory of English syntax. Given a sentence, the Link Parser assigns to it a syntactic structure which consists of a set of labeled links connecting pairs of words. Each structure is called a linkage and several linkages are generated for the same input. This tool is written in generic C code, but since CINDI_QA has been programmed in Java, we used the Java Native Code Link Grammar Interface <ref type="bibr" coords="4,279.42,300.61,10.61,9.02" target="#b4">[6]</ref>.</p><p>The Link Parser is plugged into our system to generate linkages of the English question. Using one linkage, we are able to determine which words form nouns, verbs, adjectives and capitalized entities. If those keywords appear wrong or incomplete, we go on to the next linkage. Remember that the user has the option of choosing the most appropriate linkage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical Reference</head><p>To increase the chances of finding candidate answers among the data collection, we include synonyms of the keywords in addition to the keywords themselves. WordNet <ref type="bibr" coords="4,324.09,406.38,11.70,9.02" target="#b5">[7]</ref> is a lexical database for the English language developed at Princeton University and has been used in other CINDI related projects <ref type="bibr" coords="4,428.48,417.84,16.69,9.02" target="#b8">[10]</ref> so its selection was pretty obvious.</p><p>WordNet was used in concordance with Lucene. Lucene enables us to create an index composed strictly of synonyms defined by WordNet that can be queried like a regular index so we can actually get a list of synonyms of a specific word. This strategy is highlighted in pages 292-296 of Lucene in Action <ref type="bibr" coords="4,411.54,463.86,10.61,9.02" target="#b6">[8]</ref>.</p><p>After defining the keywords using the Link Parser, we queried the WordNet index to obtain the synonyms of each keyword, except for capitalized entities. Since some of those synonyms are irrelevant or out of context, the user has the choice to discard them and select only the appropriate ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Indexing and Searching</head><p>We needed a tool that could not only index all the documents that make up QA@CLEF's data collection but also search the created index with some level of intelligence such as ranking results and highlighting query terms. A perfect match for this requirement is the Apache Software Foundation's Lucene <ref type="bibr" coords="4,407.87,569.64,10.61,9.02" target="#b7">[9]</ref>, a high-performance, fullfeatured text search engine library written entirely in Java.</p><p>CINDI_QA makes extensive use of Lucene. As mentioned before, it is used in concordance with WordNet to get synonyms of keywords. Lucene also creates the CLEF index and ranks the results found. The following features are of great importance to our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Lucene Query Building using Proximity Search</head><p>Once we have identified the keywords and their synonyms, the building of the query takes place. The query is constructed by putting together each keyword or its synonym with the other keywords or their synonyms. The crucial point here is to add the proximity search flag to the built query so that Lucene will not look for a sentence that has our keywords adjacent to each other, but rather one where the keywords are close to each other but spread out in a paragraph. This is done by adding the tilde character '~' and a number to the end of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Highlighting query terms in Answer Candidates</head><p>Once the Lucene query is built, it is searched against the index of the document collection. Lucene then returns a list of filenames ranked according to the frequency of occurrence of the words in the query. At this point, we take advantage of the Lucene Highlighter, a wonderful tool that actually displays snippets of text with the query terms highlighted. This allows us not only to know which document has the answer, but also to obtain a sentence in that document that displays the actual answer. This mechanism is mentioned on page 300 of Lucene in Action <ref type="bibr" coords="5,494.32,141.91,10.61,9.02" target="#b6">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Template Matching</head><p>CINDI_QA's template module comes into play in the final stages, between the identification of the answer candidates and the return of an answer to the user. Indeed, after the Lucene component's job is done, we are left with a set of sentences one of which holds the actual answer. To determine which one to choose, we match them against our set of pre-defined templates. We chose to use templates because a previous project <ref type="bibr" coords="5,467.97,238.51,16.68,9.02" target="#b8">[10]</ref> done by a member of our group was pretty successful at parsing English questions using mainly templates.</p><p>According to the CLEF Working Notes <ref type="bibr" coords="5,247.83,261.48,15.33,9.02" target="#b9">[11]</ref>, there are three different types of questions: Factoid, Definition and List. Factoid and Definition kinds are further broken down into sub-categories: Person, Time, Location, Organization, Measure, Count, Object and Other for the former and Person, Organization, Object and Other for the latter. Our templates take advantage of this approach to differentiate among the questions that are inputted in our system, and also of the keywords we identified in the Link Parser module. The capitalized entity, made up of a word whose first letter is uppercase, identifies proper nouns. It is an important keyword because of its high frequency of occurrence; this is due to the fact that most questions at QA@CLEF ask about famous people, locations or events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Person Definition Template: Who is X?</head><p>This is without a doubt the easiest template to consider. To discover that a question is of that type, it must start with the word "who", have the verb "is" or its derivative "was" and must be followed by a capitalized entity X. X could be one word or a composition of words; as long as adjacent words are capitalized, CINDI_QA will define them as belonging to one and the same capitalized entity. The following illustrates the query given to CINDI_QA, the candidate returned by the Highlighter of the Lucene Module and the answer returned after template matching.</p><p>Example: -query: Who is Robert Altman? -candidate: Robert Bernard Altman (February 20, 1925 -November 20, 2006) was an American film director known for making films that are highly naturalistic, but with a stylized perspective. -answer: American film director.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Count Factoid Template: How many N V?</head><p>This template is selected if the question starts with the words "how many" and has at least one verb V and one noun N, capitalized entities being optional. In the following example, had the expression "Solar System" not been capitalized, the Link Parser module would have identified "solar" as an adjective and "system" as a noun, yet the template would still be selected because "planets" is a noun. Example: -query: How many planets does the Solar System have? -candidate: The Solar System or solar system consists of the Sun and the other celestial objects gravitationally bound to it: the eight planets, their 165 known moons, three currently identified dwarf planets (Ceres, Eris, and Pluto) and their four known moons, and billions of small bodies. -answer:</p><p>Eight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Time Factoid Template: What year V1 X V2? / When V1 X V2?</head><p>This template is selected if the following occurs: the question starts with the words "what year" or "when", then has two verbs, V1 and V2 as well as one capitalized entity X. The Link Parser module will identify two separate verbs even if they belong to the participate form of the same verb, i.e. it will flag "was" as V1 and "murdered" as V2, even though technically they both define the verb "murder". In the following example, notice how the word "assassinate", synonym of the inputted verb "murder", is used to return the correct answer.</p><p>Example: -query:</p><p>What year was Martin Luther King murdered? -candidate: On April 4, 1968, King was assassinated in Memphis, Tennessee.</p><p>-answer:</p><p>1968.</p><p>In the feedback mode, the user will be prompted one last time to approve the template chosen by CINDI_QA before being shown an answer. In the black-box mode, the system chooses the least costly linkage from the Link Parser, retains only the first two synonyms of a keyword, uses the best ranked document from Lucene with the most highlighted sentence and constructs the answer based on the first template matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The CINDI group participated in the FR to EN track of the QA@CLEF edition of 2007. We produced two runs that were sent: cind071fren and cind072fren. The two runs ended up with the same overall accuracy value of 13%. The following figure details the different assessments both runs obtained. In addition to the usual news collections, articles from Wikipedia were also considered as answer source for the first time this year. But we became aware of that very late in our answering process and were only able to use a small part of the corpora available. This is the main reason for our system's low performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right</head><p>Since our two runs are equivalent, the following figure shows the accuracy by question type only of cind072fren. CINDI_QA's score on definition questions is much higher than any other type of question. This result is attributed to the robustness of template 4.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factoids</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>Although the approach we used to tackle bilingual question-answering at CLEF looks good in theory, the results obtained are a bit disappointing, as is shown by our 13% success rate. Because this is our first participation in QA@CLEF and the CINDI group only invested 1 man/year on the project, we have high hope for the future, especially since we missed a large part of the source corpora. We learned that because CINDI_QA relies on so many external tools, it is only as strong as its weakest link. For instance, if from the start, the translation of the question isn't a successful one, there is nothing the system can do after that stage to come up with a correct answer.</p><p>Future works include the addition of new templates to handle the multitude of question sub-categories as well as a mechanism to identify questions whose answer isn't located in the CLEF data collection and return NIL for those questions. We hope we can have these enhancements ready for the next edition of QA@CLEF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,226.02,483.60,148.16,9.02"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CINDI_QA Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,225.30,714.24,149.51,9.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CINDI_QA Process Flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,112.02,263.88,371.47,63.86"><head>CINDI_QA Results at QA@CLEF 2007</head><label></label><figDesc></figDesc><table coords="6,112.02,263.88,371.47,63.86"><row><cell></cell><cell></cell><cell>Wrong</cell><cell cols="4">Inexact Unsupported Unassessed Accuracy</cell></row><row><cell>cind071fren</cell><cell>26</cell><cell>171</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>13%</cell></row><row><cell>cind072fren</cell><cell>26</cell><cell>170</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>13%</cell></row><row><cell></cell><cell cols="2">Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,169.14,424.80,252.48,126.50"><head>Figure 4: CINDI_QA Results by Question Type</head><label></label><figDesc></figDesc><table coords="6,169.14,424.80,252.48,100.94"><row><cell></cell><cell></cell><cell>Lists</cell><cell>Definitions</cell></row><row><cell>Total</cell><cell>161</cell><cell>9</cell><cell>30</cell></row><row><cell>Right</cell><cell>18</cell><cell>1</cell><cell>7</cell></row><row><cell>Wrong</cell><cell>140</cell><cell>8</cell><cell>22</cell></row><row><cell>Unsupported</cell><cell>2</cell><cell>0</cell><cell>0</cell></row><row><cell>Inexact</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>Accuracy</cell><cell>11.18%</cell><cell>11.11%</cell><cell>23.33%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,97.30,100.50,271.11,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://cindi.encs.concordia.ca/about_cindi.html" />
		<title level="m" coord="7,97.30,100.50,75.63,9.02">The CINDI System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.31,123.48,225.80,9.02" xml:id="b1">
	<monogr>
		<ptr target="http://translate.google.com/translate_t" />
		<title level="m" coord="7,97.31,123.48,67.67,9.02">Google Translate</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.29,146.46,214.93,9.02" xml:id="b2">
	<monogr>
		<ptr target="http://babelfish.altavista.com/" />
		<title level="m" coord="7,97.29,146.46,89.60,9.02">Babel Fish Translation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.30,192.48,239.61,9.02" xml:id="b3">
	<monogr>
		<ptr target="http://www.link.cs.cmu.edu/link" />
		<title level="m" coord="7,97.30,192.48,103.76,9.02">The Link Grammar Parser</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.29,215.46,290.40,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,97.29,215.46,169.42,9.02">Java Native Code Link Grammar Interface</title>
		<ptr target="http://chrisjordan.ca/projects" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.25,238.50,334.86,9.02" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,97.25,238.50,211.50,9.02">WordNet, a lexical database for the English language</title>
		<ptr target="http://wordnet.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.26,261.48,266.32,9.02" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<title level="m" coord="7,227.22,261.48,66.22,9.02">Lucene in Action</title>
		<imprint>
			<publisher>Manning</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.29,284.46,133.29,9.02" xml:id="b7">
	<monogr>
		<ptr target="http://lucene.apache.org/" />
		<title level="m" coord="7,97.29,284.46,27.40,9.02">Lucene</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,97.21,307.50,432.46,9.02;7,70.56,318.96,69.78,9.02" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,149.76,307.50,262.08,9.02">NLPQC: A Natural Language Processor for Querying CINDI</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Stratica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Concordia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master Thesis</note>
</biblStruct>

<biblStruct coords="7,96.75,341.93,432.84,9.02;7,70.56,353.46,459.00,9.02;7,70.56,364.98,459.03,9.02;7,70.56,376.44,67.83,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,190.80,353.46,291.61,9.02">Overview of the CLEF 2006 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Former</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,506.81,353.46,22.75,9.02;7,70.56,364.98,361.36,9.02">Cross Language Evaluation Forum: Working Notes for the CLEF 2006 Workshop (CLEF 2006)</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">20-22 September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
