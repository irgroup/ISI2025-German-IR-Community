<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.88,74.45,259.65,12.58">INAOE&apos;s Participation at QA@CLEF 2007</title>
				<funder ref="#_TT6n7Ug">
					<orgName type="full">CONACYT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.62,103.44,59.32,9.02"><forename type="first">Alberto</forename><surname>Téllez</surname></persName>
							<email>albertotellezv@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.48,103.44,63.87,9.02"><forename type="first">Antonio</forename><surname>Juárez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.34,103.44,82.73,9.02"><forename type="first">Gustavo</forename><surname>Hernández</surname></persName>
							<email>ghernandez@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.57,103.44,66.84,9.02"><forename type="first">Claudia</forename><surname>Denicia</surname></persName>
							<email>cdenicia@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.60,114.96,60.39,9.02"><forename type="first">Esaú</forename><surname>Villatoro</surname></persName>
							<email>villatoroe@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.52,114.96,64.53,9.02"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
							<email>mmontesg@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.43,114.96,65.26,9.02"><forename type="first">Luis</forename><surname>Villaseñor</surname></persName>
							<email>villasen@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratorio de Tecnologías del Lenguaje Instituto Nacional de Astrofísica</orgName>
								<orgName type="laboratory" key="lab2">Óptica y Electrónica (INAOE)</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.88,74.45,259.65,12.58">INAOE&apos;s Participation at QA@CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84773A646F1B4F201435C17F22441A07</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question Answering for Spanish, Lexical Information, Information Retrieval, Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the system developed by the Language Technologies Lab of INAOE for the Spanish Question Answering task at CLEF 2007. The presented system is centered in a full datadriven architecture that uses information retrieval and machine learning techniques to identify the most probable answers to definition and factoid questions respectively. The major quality of our system is that it mainly relies on the use of lexical information and avoids applying any complex language processing resource such as POS taggers, named entity classifiers, parsers or ontologies. Experimental results indicate that our approach is very effective for answering definition questions from Wikipedia. In contrast, they also reveal that it is very difficult to respond factual questions from this resource solely based on the use of lexical overlaps and redundancy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) has become a promising research field whose aim is to provide more natural access to information than traditional document retrieval techniques. In essence, a QA system is a kind of search engine that allows users to pose questions using natural language instead of an artificial query language, and that returns exact answers to the questions instead of a list of entire documents.</p><p>Current developments in QA tend to use a variety of linguistic resources to help in understanding the questions and the documents. The most common linguistic resources include: part-of-speech taggers, parsers, named entity extractors, dictionaries, and WordNet <ref type="bibr" coords="1,254.38,551.85,10.87,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,268.78,551.85,7.54,8.74" target="#b2">3,</ref><ref type="bibr" coords="1,279.88,551.85,7.50,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,290.92,551.85,7.21,8.74" target="#b5">6]</ref>. Despite of the promising results of these approaches, they have two main inconveniences. On the one hand, the construction of such linguistic resources is a very complex task, and on the other hand, their performance rates are usually not optimal.</p><p>In contrast to these recent developments that point to knowledge rich methods (that are intrinsically language and domain dependent), in this paper we present a straightforward QA approach that avoids using any kind of linguistic resource, and therefore, that can be -in theory-applied to answer questions in several languages. This approach is mainly supported on two simple ideas. First, questions and answers are commonly expressed using the same set of words, and second, different kind of questions requires different kind of methods for adequate answer extraction.</p><p>In particular, the developed QA system is based on a full data-driven approach that exclusively uses lexical information in order to determine relevant passages as well as candidate answers. Furthermore, this system is divided in two basic components; one of them focuses on definition questions and applies traditional information retrieval techniques, whereas the other one centers on factoid questions and uses a supervised machine learning strategy. This system continues our last year work <ref type="bibr" coords="1,274.89,701.37,10.87,8.74" target="#b4">[5]</ref>; however it incorporates some new elements. For instance, it takes advantage of the structure of the document collection (Wikipedia in this case) to easily locate definition phrases, and it also applies a novel technique for query expansion based on association rule mining <ref type="bibr" coords="1,477.93,724.35,11.62,8.74" target="#b0">[1]</ref> in order to enhance the recovery of relevant passages.</p><p>The following sections give some details on the proposed system. Sections 2 and 3 describe the subsystems for answering definition and factoid questions respectively. Then, section 4 describes the system's adaptations required to deal with group of related questions. Later on, section 5 presents our evaluation results. Finally, section 6 discusses some general conclusions about our participation at QA@CLEF 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Answering Definition Questions</head><p>Our method for answering definition questions uses Wikipedia as target document collection. It takes advantage of two known facts: (1) Wikipedia organizes information by topics, that is, each document concerns one single subject and, (2) the first paragraph of each document tend to contain a short description of the topic at hand. This way, it simply retrieves the document(s) describing the target term of the question and then returns some part of its initial paragraph as answer.</p><p>Figure <ref type="figure" coords="2,111.18,208.77,5.01,8.74" target="#fig_0">1</ref> shows the general process for answering definition questions. It consists of three main modules: target term extraction, document retrieval and answer extraction. The following sections briefly describe these modules.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finding Relevant Documents</head><p>In order to search in Wikipedia for the most relevant document to the given question, it is necessary to firstly recognize the target term. For this purpose our method uses a set of manually constructed regular expressions such as: "What|Which|Who|How"+"any form of verb to be"+&lt;TARGET&gt;+"?", "What is a &lt;TARGET&gt; used for?", "What is the purpose of &lt;TARGET&gt;?", "What does &lt;TARGET&gt; do?", etc.</p><p>Then, the extracted target term is compared against all document names and the document having the greatest similarity is recovered and delivered to the answer extraction module. It is important to mention that, in order to favor the retrieval recall, we decided using the document names instead of the document titles since they also indicate their subject but normally they are more general (i.e., titles tend to be a subset of document names).</p><p>In particular, our system uses the Lucene 1 information retrieval system for both indexing and searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extracting the Target Definition</head><p>As we previously mentioned, most Wikipedia's documents tend to contain a brief description of its topic in the first paragraph. Based on this fact, our method for answer extraction is defined as follows:</p><p>born in Euskirchen, near Cologne, the son of a businessman. After graduating he wished to study natural sciences, but his father compelled him to work in the family business until determining that his son was unsuitable".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answering Factoid Questions</head><p>Figure <ref type="figure" coords="3,99.90,139.77,5.01,8.74" target="#fig_1">2</ref> shows the general process for answering factoid questions. This process considers three main modules: passage retrieval, where the passages with the major probability to contain the answer are recovered from the document collections; question classification, where the type of expected answer is determined; and answer extraction, where candidate answers are selected using a machine-learning approach, and the final answer recommendation of the system is produced. The following sections describe each of these modules.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passage Retrieval</head><p>This module aims, as we previously mentioned, to recover a set of relevant passages from all target document collections, in this particular case, the EFE news collection and Wikipedia. It is primary based on a traditional vector-space-model retrieval system, but also incorporates a novel query expansion approach. Figure <ref type="figure" coords="3,477.32,391.83,5.01,8.74" target="#fig_2">3</ref> shows the general scheme of this module. It considers four main processes: association rule mining, query generation, passage retrieval, and passage integration. Association rule mining. This process is done offline. Its purpose is to obtain all pairs of highly related concepts (i.e., named entities) from a given document collection. It considers that a concept A is related or associated to some other concept B (i.e., A → B), if B occurs in σ% of the documents that contains A.</p><p>In order to discover all association rules satisfying a specified σ-threshold this process applies the well-known</p><p>Apriori algorithm <ref type="bibr" coords="3,146.15,671.91,10.65,8.74" target="#b0">[1]</ref>. Using this algorithm it was possible to discover association rules such as "Churchill → Second World War" and "Ernesto Zedillo → Mexico". Query generation. This process uses the discovered association rules to automatically expand the input question. Basically, it constructs four different queries from the original question. The first query is the set of key-words (for instance, the set of named entities) from the original question, whereas the other three queries expand the first one by including some associated concept <ref type="foot" coords="4,272.22,84.62,3.00,5.23" target="#foot_2">3</ref> .</p><p>For instance, given a question such as "Who was the president of Mexico during the Second World War?", this process generates the following four queries: (1) "Mexico Second World War", (2) "Mexico Second World War 1945", (3) "Mexico Second World War United States", and (4) "Mexico Second World War Pearl Harbor".</p><p>Passage retrieval. <ref type="foot" coords="4,158.10,133.64,3.00,5.23" target="#foot_3">4</ref> The purpose of this process is to recover the greatest number of relevant passages from the target document collections (EFE and Wikipedia). In order to do that it retrieves passages using all generated queries.</p><p>Passage integration. This process combines the retrieved passages into one single set. Its objective is to sort all passages in accordance with a homogeneous weighting scheme. The new weight passages is calculated as follows:</p><formula xml:id="formula_0" coords="4,244.56,210.69,106.34,30.54">∑ ∑ = ∈ ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ = n i G y i p i y p C G n w 1 ) , ( 1 1</formula><p>Where w p is the new weight of passage p, n indicates the number of words of the reference question, G i is the set of all n-grams of size i from the question, and C(p,y) is equal to 1 if the question n-gram y occurs in the passage p, otherwise it is equal to 0. This new weighting scheme favors those passages sharing the greatest number of n-grams with the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Classification</head><p>This module is responsible to define the semantic class of the answer of the given question. The idea is to know in advance the type of the expected answer in order to reduce the searching space to only those information fragments related to this specific semantic class.</p><p>Our prototype implements this module following a direct approach based on regular expressions. It only considers three general semantic classes for the type of expected answer: date, quantity and name (i.e., a proper noun).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Extraction</head><p>Answer extraction aims to establish the best answer for a given question. It is based on a supervised machine learning approach. It consists of two main modules, one for attribute extraction and other one for answer selection. These modules were taken from our last year prototype <ref type="bibr" coords="4,315.10,451.53,10.61,8.74" target="#b4">[5]</ref>.</p><p>Attribute extraction. First, the set of recovered passages are processed. The purpose is to identify all text fragments related to the semantic class of the expected answer. This process is done using a set of regular expression that allows identifying proper names, dates and quantities. Each identified text fragment is considered a "candidate answer".</p><p>In a second step, the lexical context of each candidate answer is analyzed with the aim of constructing its formal representation. In particular, each candidate answer is represented by a set of 17 attributes, clustered in the following groups:</p><p>1. Attributes that describe the complexity of the question. For instance, the length of the question (number of non-stopwords). 2. Attributes that measure the similarity between the context of the candidate answer and the given question.</p><p>Basically, these attributes considers the number of common words, word lemmas and named entities (proper names) between the context of the candidate answer and the question. They also take into consideration the density of the question words in the answer context. 3. Attributes that indicate the relevance of the candidate answer in reference to the set of recovered passages. For instance, the relative position of passage that contains the candidate answer as well as the redundancy of the answer in the whole set of passages.</p><p>Answer Selection. This module selects from the set of candidate answers the one with the maximum probability of being the correct answer. This selection is done by a machine learning method, in particular, by a Naïve Bayes classifier.</p><p>It is important to mention that the classification model (actually, we have three classifiers, one for each kind of answer) was constructed using as training set the questions and documents from previous CLEFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Answering Lists of Questions</head><p>This year's evaluation includes a new challenge: groups of related questions, where the first one indicates the focus of the group and the rest of them are somehow dependent from it. For instance, the pair of questions "When was Amintore Fanfani born?", and "where was he born?".</p><p>Our approach for answering this kind of questions is quite simple. It basically considers the enrichment of dependent questions by adding some keywords as well as the answer from the first (head) question.</p><p>The process for answering list of related questions is as follows:</p><p>1. Handle head questions as usual (refer to sections 2 and 3). 2. Extract the set of keywords (in our case the set of named entities) from the head question. This process is done using a set of regular expressions. 3. Add to all dependent questions the set of keywords and the extracted answer from the head question. 4. Handle enriched dependent questions as usual (refer to sections 2 and 3).</p><p>For instance, after this process the example question "where was he born?" was transformed to the enriched question "where he was born? + Amintore Fanfani + 6 February 1908".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>This section presents the experimental results about our participation at the monolingual Spanish QA track at CLEF 2007. This evaluation exercise considers two basic types of questions, definition and factoid. However, as we mentioned in section 4, this year there were also included some groups of related questions.</p><p>From the given set of 200 test question, our QA system treated 34 as definition questions and 166 as factoid. | It is very interesting to notice that our method for answering definition questions is very precise. It could answer almost 90% of the questions; moreover, it never replies wrong or unsupported answers. This result evidenced that Wikipedia has some inherent structure, and that our method could effectively take advantage of it.</p><p>On the other hand, Table <ref type="table" coords="5,188.78,514.89,5.01,8.74" target="#tab_2">1</ref> also shows that our method for answering factoid questions was not completely adequate (it only could answer 23% of this kind of questions). Taking into consideration that this method obtained 40% of accuracy on last year exercise <ref type="bibr" coords="5,256.18,537.87,10.62,8.74" target="#b4">[5]</ref>, we presume that this poor performance was caused by the inclusion of Wikipedia. Two characteristics of Wikipedia damage our system's behavior. First, it is much less redundant that general news collections; and second, its style and structure makes lexical contexts of candidate answers less significant that those extracted from other free-text collections. Finally, Table <ref type="table" coords="5,141.36,658.65,5.01,8.74" target="#tab_4">2</ref> shows some results about the treatment of groups of related questions. It is clear that the proposed approach (refer to section 4) was not useful for dealing with dependent questions. The reason of this poor performance is that only 37% of head questions were correctly answered, and therefore, in the majority of the cases dependent questions were enriched with erroneous information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented a QA system that allows answering factoid and definition questions. This system is based on a lexical data-driven approach. Its main idea is that the questions and their answers are commonly expressed using almost the same set of words, and therefore, it simply uses lexical information to identify the relevant passages as well as the candidate answers.</p><p>The proposed method for answering definition questions is quite simple; nevertheless it allowed achieving very high precision rates. We consider that its success is mainly attributable to its capability to take advantage of the style and structure of Wikipedia (the used target document collection). On the contrary, our method for answering factoid question was not equally successful. Paradoxically, the style and structure of Wikipedia caused detriment in most of its internal processes, since they are mainly based on lexical overlap and redundancy.</p><p>With respect to the treatment of groups of related questions our conclusion is that the achieved poor performance (16% in dependent questions) was consequence of a cascade error, in view of the fact that only 37% of head questions were correctly answered, and therefore, most dependent questions were expanded using incorrect information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,202.86,398.16,198.05,9.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Process for answer definition questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,196.02,315.66,199.73,9.02"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Process for answering factoid questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,210.72,599.04,193.04,9.02"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. General Process for Passage Retrieval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.92,371.07,175.83,8.74"><head>Table 1</head><label>1</label><figDesc>details our general accuracy results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,187.68,388.32,218.75,74.22"><head>Table 1 .</head><label>1</label><figDesc>System's general evaluation</figDesc><table coords="5,187.68,409.27,218.75,53.26"><row><cell></cell><cell cols="5">Right Wrong Inexact Unsupported Accuracy</cell></row><row><cell>Definition</cell><cell>30</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>88.23%</cell></row><row><cell>Factoid</cell><cell>39</cell><cell>118</cell><cell>3</cell><cell>6</cell><cell>23.49%</cell></row><row><cell>TOTAL</cell><cell>69</cell><cell>118</cell><cell>7</cell><cell>6</cell><cell>34.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,111.12,589.62,371.16,53.94"><head>Table 2 .</head><label>2</label><figDesc>Evaluation details about answering groups of related questions</figDesc><table coords="5,443.40,607.75,14.21,7.18"><row><cell>NIL</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,99.24,628.23,424.28,8.74"><p>Eliminate all text between parenthesis (the goal is to eliminate comments and less important information).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,99.24,639.32,425.24,9.10;2,99.30,651.21,278.59,8.74;2,82.26,668.46,442.27,9.02;2,70.92,679.92,453.55,9.02;2,70.92,691.44,453.54,9.02;2,70.92,721.24,97.49,7.85;2,70.92,731.18,453.63,8.88;2,79.56,743.20,191.16,7.85"><p>If the constructed answer is shorter than a given specified threshold 2 , then aggregate as many sentences of the first paragraph as necessary to obtain an answer of the desire size.For instance, the answer for the question "Who was Hermann Emil Fischer?" (refer to Figure1) was extracted from the first paragraph of the document "Hermann_Emil_Fischer": "Hermann Emil Fischer (October 9, 1852 -July 15, 1919) was a German chemist and recipient of the Nobel Prize for Chemistry in 1902. Emil Fischer was 1 http://lucene.apache.org/<ref type="bibr" coords="2,70.92,731.18,3.00,5.23" target="#b1">2</ref> For the experiments reported in section 4 we defined this threshold equal to 70 characters. This number was estimated after a manual analysis of several Wikipedia's documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,76.20,732.22,272.02,7.85"><p>These concepts must be associated with all keywords of the given question.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,76.20,742.95,261.69,8.10"><p>This process was carried out by the Lucene information retrieval system.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was done under partial support of <rs type="funder">CONACYT</rs> (Project Grant <rs type="grantNumber">43990</rs>). We also like to thanks to the CLEF organizing committee as well as to the <rs type="institution">EFE agency</rs> for the resources provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TT6n7Ug">
					<idno type="grant-number">43990</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.69,317.34,449.80,9.02;6,88.92,329.07,176.47,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,202.80,317.34,185.31,9.02">Fast Algorithms for Mining Association Rules</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,395.28,317.61,129.22,8.74;6,88.92,329.07,44.20,8.74">Proceedings of the 20th. VLDB Conference</title>
		<meeting>the 20th. VLDB Conference<address><addrLine>Santiago de Chile, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,340.59,449.85,8.74;6,88.92,351.84,435.59,9.02;6,88.92,363.57,355.71,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,124.38,351.84,267.62,9.02">MIRACLE&apos;s 2005 Approach to Cross-Lingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De-Pablo-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>González-Ledesma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martinez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Guirao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,410.27,352.11,114.24,8.74;6,88.92,363.57,212.87,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,374.82,449.87,9.02;6,88.92,386.34,435.64,9.02;6,88.92,398.07,126.12,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,388.14,374.82,136.42,9.02;6,88.92,386.34,56.46,9.02">The TALP-QA System for Spanish at CLEF-2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kanaan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ageno</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,165.52,386.61,338.04,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,409.32,449.80,9.02;6,88.92,420.84,435.55,9.02;6,88.92,432.57,340.22,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,453.42,409.32,71.07,9.02;6,88.92,420.84,275.25,9.02">Monolingual and Cross-language QA using a QA-oriented Passage Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gómez-Soriano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bisbal-Asensi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanchos-Arnal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.06,421.11,140.42,8.74;6,88.92,432.57,197.30,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,444.09,449.78,8.74;6,88.92,455.34,435.52,9.02;6,88.92,467.07,158.29,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,88.92,455.34,275.06,9.02">INAOE at CLEF 2006: Experiments in Spanish Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juárez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Téllez-Valero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Denicia-Carral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,371.46,455.61,152.98,8.74;6,88.92,467.07,17.15,8.74">Working Notes of CLEF-2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,478.32,449.79,9.02;6,88.92,489.78,435.55,9.02;6,88.92,501.57,164.46,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,475.87,478.32,48.62,9.02;6,88.92,489.78,89.94,9.02">Spanish QA System at CLEF-2005</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Aliqan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,201.30,490.05,323.17,8.74;6,88.92,501.57,21.52,8.74">Working notes for the Cross Language Evaluation Forum Workshop (CLEF 2005)</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
