<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.34,136.61,309.52,12.58;1,252.96,152.69,89.47,12.58">Cross Lingual Question Answering using QRISTAL for CLEF 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.76,202.80,76.85,9.02"><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
							<email>dlaurent@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.28,202.80,60.79,9.02"><forename type="first">Patrick</forename><surname>Séguéla</surname></persName>
							<email>p.seguela@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.88,202.80,54.68,9.02"><forename type="first">Sophie</forename><surname>Nègre</surname></persName>
							<email>sophie.negre@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Synapse Développement</orgName>
								<address>
									<addrLine>33 rue Maynard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.34,136.61,309.52,12.58;1,252.96,152.69,89.47,12.58">Cross Lingual Question Answering using QRISTAL for CLEF 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2F63BAEC7C979E91B9760A8223CFB44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2 [Database Management]: H.2.3 Languages-Query Languages Measurement, Performance, Experimentation Question answering, French, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>QRISTAL is a commercial question answering system making intensive use of natural language processing both for indexing documents and extracting answers. It ranked first in the EQueR evaluation campaign (Evalda, Technolangue <ref type="bibr" coords="1,283.92,342.78,11.25,9.02" target="#b0">[1]</ref>) and in CLEF 2005 [8] and CLEF 2006 [6]  for monolingual task (French-French) and multilingual task (English-French and Portuguese-French). This year Synapse Développement only took part to the French monolingual run with a 54% of overall accuracy. This paper describes the improvements and changes implemented in Synapse Développement QA system since last CLEF 2006 campaign and details new features added to meet the challenges of this year's evaluation, that is sequence of question and the Wikipedia corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Synapse Développement took part in the 2005 <ref type="bibr" coords="1,259.53,655.98,11.69,9.02" target="#b3">[4]</ref> and 2006 <ref type="bibr" coords="1,313.59,655.98,11.74,9.02" target="#b4">[5]</ref> campaign of CLEF. During last year's campaign, we provided results both for monolingual (French to French) and bilingual tasks (English to French and Portuguese to French). This year, however, we just submitted our run for the monolingual French to French campaign.</p><p>The QA system we used for the campaign was an adaptation of our QA commercial product Qristal <ref type="bibr" coords="1,479.99,707.94,10.64,9.02" target="#b2">[3]</ref>. Qristal is a cross lingual question answering system for French, English, Italian, Portuguese, Polish and Czech. It was designed to extract answers both from documents stored on a hard disk and from Web pages by using traditional search engines (Google, MSN, AOL, etc.). Qristal is currently used in the M-CAST European project of Econtent (22249, Multilingual Content Aggregation System based on TRUST Search Engine). Anyone can assess Qristal online to ask questions on Wikipedia on : http://synapse.servehttp.com/ASP/ClientAsp_QristalSearch/WebFormWikipedia.aspx Qristal integrates sophisticated Natural Language Processing components such as embedded ontologies, synonyms dictionaries, semantic relation extraction, coreference resolution, temporal contexts. The main originality of the architecture of our system is that it uses multi-criteria indexes <ref type="bibr" coords="2,416.48,139.38,10.63,9.02" target="#b4">[5]</ref>. While indexing, each document is divided into block of 1 kilobyte. Then, each block is analyzed syntactically and semantically, anaphora and metaphor are resolved. As a result, for each block, 8 different indexes are built : heads of derivation for common nouns, proper nouns, idioms, named entities, concepts, fields, QA extraction patterns and keywords.</p><p>Compared to last year, the architecture of our QA system remains roughly the same : after the question is submitted, it is categorised according to a question typology and pivots are identified. An automatic search in the index retrieves a set of potentially relevant blocks, containing a list of sentences related with the question. Blocks are sorted with weights and only 100 blocks are linguistically analysed for the selection of best sentences. Sentences are weighted according to their semantic relevance and similarity with the question. Next, through specific answer patterns, these sentences are examined once again and the parts containing possible answers are extracted. Finally, a single answer is chosen among all candidates.</p><p>The next section gives an overview of the main improvements of our QA technology over last year. Section 3 addresses the various adjustments we made to adapt to both novelties of this year's QA@CLEF campaign, namely the use of Wikipedia and the handling of topic related questions. Section 4 presents and discusses our results and section 5 concludes with future guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Improvement of the technology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Improvement of linguistic resources</head><p>As the product is being marketed, the linguistic resources need to be permanently updated.</p><p>Proper names dictionary was dramatically increased from 44 000 to 117 000 entries. This work exploited Wikipedia resources to list candidates, mostly works, enterprises and people's name. Thus, candidates were manually validated. We also add to proper names relevant information. For example, for a person, we mentioned its nationality, birthplace, birth date, date of death and main function. For a town, we mentioned its country, its region, its surface, its population and if its a capital or not. Note we only used the link between towns and countries and between persons and nationalities for this QA@CLEF campaign. Actually, a deeper use of those resources could have lead us to provide "unjustified" answers as it encourages the system to rank first a text including some answers even if the system did not find any clear justification of that answer in the text.</p><p>Together with our resources, our French syntactic analyser has noticeably been improved as we received the detailed results of the Easy campaign <ref type="bibr" coords="2,225.94,531.96,11.71,9.02" target="#b6">[7]</ref> where our analyser was awarded as one of the best tools available for French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Handling of meta data</head><p>Our system now handles meta data. For a given document it stores its title, creation date, author and keywords. Note that dividing texts into blocks made it compulsory to store keywords for a given document. Isolated blocks cannot explicitly mention main subjects of the original text although sentences of these blocks relate to these subjects. Consequently, titles are considered as belonging to all blocks of texts.</p><p>We revised our algorithm for index search to take those meta data into account, that is to filter scanned blocks. For example, the analysis of the a question Qui a reçu le prix Goncourt en 1995? (Who was awarded the Goncourt Prize in 1995?) infers that the question is temporally restricted and its time environment is a date, i.e. the year 1995. Thus, the index search algorithm will give focus on documents created in 1995 while documents created before 1995 will be lowered.</p><p>Additionally, other small improvements were made, such as the implementation of new algorithms for searching noun phrases and related words in the index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Coping with changes</head><p>This year's campaign introduced some new elements participants had to cope with : topic-related questions and articles from Wikipedia considered as an answer source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Wikipedia searching</head><p>First decision was to simplify the Wikipedia corpus in removing XML tags and, on top of suggested exclusion proposed in the guidelines, suppress REDIRECT files. As previous campaigns were based on news collections,0 we here list some elements we identified as specific to Wikipedia.</p><p>Texts have very few redundancy. An information is often mentioned once in the overall corpus, the author making extensive use of hyperlinks. Basically, Wikipedia is influenced by the fact that it is meant to be read in a browser in a web environment. Moreover, it is very important to consider titles as the main subject of an article is often referred using anaphora.</p><p>Additionally, the document layout, nonexistent in corpus, is very important in Wikipedia. Lots of information are in tables that are particularly difficult to handle for technology based on the analysis of lists of sentences. Typically, for a town, the population, land area or country will be stated in tables and not referred in the text of the article anymore. For example, table titles are often missing or replaced by dedicated codes (the tag "datedeces" is displayed "date de décès" (date of death)).</p><p>Finally, there are more spelling mistakes in Wikipedia than in news text. Diacritic signs are particularly neglected. The name of persons stands often in the title and in the very beginning of an article. Unfortunately, all first names of the person are mentioned where only one first name is commonly used. The page "Marivaux" is entitled and begins with "Pierre Carlet de Chamblain de Marivaux". The page about "Victor Hugo" begins as follows "Victor-Marie Hugo, né le 26 février 1802 à Besançon, mort..." ("Victor-Marie Hugo, born the 26th of February 1802 in Besançon, died..."). On both those pages our technology will recognise and index the complete named entity making it difficult for the algorithm to find the common usage (Marivaux, Victor Hugo) in this block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Topic-related questions</head><p>Topic-related questions are clusters of questions which are related to the same topic and possibly contain anaphoric references between one question and the other questions. 76 questions out of 200 were topic-related for the French QA@CLEF 2007 run.</p><p>We implemented a special treatment to handle the fact that the question are topic-related. For example, last dates and geographic places (country, towns) encountered in a sequence were systematically added in the next question.</p><p>But our effort focuses on the detection of anaphora within a topic-related question set. Our overall technology encompass an anaphora resolution technique. The idea was both to organise an interface to treat questions and answers as a flow in a topic-related question set and improve the coverage of our anaphora resolution technique for those data. The anaphora resolution aimed at replacing reference by the word or noun phrase it referred to. This generated question is then treated normally by the QA process.</p><p>Qristal already resolved anaphora for pronouns and possessive adjectives. For this campaign, we developed an algorithm to handle demonstrative adjectives. When the syntactic analysis of a question detect a demonstrative adjective, we select the noun phrase it introduces and look at previous questions and answers for this noun phrase or synonyms. If we find one, we collect its extension and replace the demonstrative adjective and the noun phrase of the current question by this extended noun phrase. For example, in the question Qui a réalisé ce film ? (Who directed this movie?) the demonstrative adjective is "ce" (this), the noun phrase introduced is "film" (movie). Now let us consider previous answers and questions in this topic-related set : previous answer is la palme d'or (Golden palm) and previous question is Quelle récompense le film Pulp Fiction a-t-il reçu lors du festival de Cannes? (What award did the film "Pulp Fiction" get at the Cannes Film Festival?). The noun phrase "film" is then found and its extension is "film Pulp Fiction". Thus, we replace the demonstrative adjective and the noun phrase of the current question by the extended version to get the following question Qui a réalisé ce film Pulp fiction? (Who directed this film "pulp Fiction"?).</p><p>If the noun phrase and its synonyms are not found in the previous answers and questions set, we take the last noun phrase with the same semantic type. Semantic types can be abstract, concrete, human, animal or animated.</p><p>Anyway, either with this additional treatment, some anaphora were judged as too complicated to be handled as Lequel d'entre eux découvrit l'uranium 235 ? (Which one of them discovered uranuim 235?) coming after Comment se prénommaient les deux frères Piccard ? (What were the first names of the two Piccard brothers?).</p><p>Unfortunately the introduction of topic-related question makes it difficult for us to participate to cross language runs for Portuguese to French. As previously mentioned, we changed the interface of our linguistic modules that analyses the question. and thus made it incompatible with the Portuguese module of our partner Priberam <ref type="bibr" coords="4,510.33,150.90,10.64,9.02" target="#b1">[2]</ref>. Making it compatible was not much of a work but we did not managed to do it for the due time. Moreover, to resolve anaphora we had to consider both questions and answers. And thus, translate those answers in a cross language environment for topic-related questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The results of this QA@CLEF campaign for our monolingual French run are as follows : The general results of Table <ref type="table" coords="4,188.55,330.18,5.01,9.02" target="#tab_0">1</ref> show that there was a decrease regarding last year's French monolingual task. In 2006 campaign, we achieved 68% of correct answers and 64% in 2005.</p><p>As we think this drop could be related to the use of Wikipedia and checked whether questions had answers in the news corpus or in Wikipedia. It was clear that some questions could have answers only in one collection. Thus, we decided to separate questions in 4 sets, NIL questions, questions whose answer can be found in the news corpus, in Wikipedia or both.</p><p>Specific developments for the NIL questions had been implemented in CLEF 2006 resulting in a spectacular result of a 100% accuracy for the 9 NIL questions of CLEF 2007. So, NIL questions were not considered and only 191 questions are listed below.</p><p>Corpus Table <ref type="table" coords="4,97.65,559.68,5.01,9.02" target="#tab_1">2</ref> shows that redundancy is very important for a QA system based on linguistic analysis of sentences.</p><p>When the answer appeared at least twice, the system reached an encouraging 76% of correct answer. Results on the news corpus, with a 61% of good results are comforting as well and much closer to last years campaigns. But, as foreseen in a previous section, the Wikipedia encyclopaedia revealed less suited for our QA based on pattern extractions than the news corpus.</p><p>Then we decided to check the distribution of errors along the main stages of our QA system.</p><p>Stage W+X+U Failure %</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document retrieval 11 12%</head><p>Selection of best sentences 57 61%</p><p>Extraction of the answer 15 17% Anaphora 9 10%</p><p>Total 92 Table <ref type="table" coords="4,180.22,749.16,3.76,9.02">3</ref>. Distribution of errors along the main stages of our QA system.</p><p>Table <ref type="table" coords="5,96.58,87.42,5.01,9.02">3</ref> shows that for 11 questions, The Document Retrieval stage does not extract the blocks where an answer is mentioned. This is related with several issues :</p><p>-the number of one kilobyte blocks extracted is limited to 100. Therefore, for a common category of question like the definition category and very common words, numerous blocks get the same score. The question Quelle est la plus grande banque du Japon (Which is the biggest bank in Japan?), is a typical example of this. Here the category of the question is definition and searched pivots are the common noun bank and the proper noun Japan. Just for Wikipedia, 1166 blocks contain those 3 elements. Those blocks are ranked with meta data, occurrences of pivots and as a result 36 blocks are chosen. But then, the 64 following blocks are somehow randomly selected out of the 1130 remaining.</p><p>-some noun phrase or named entities are difficult to extract. This is particularly penalizing as words in the noun phrase or named entity are very common. For the question Qui remporta le Tournoi des Cinq nations en 1994 ? (Who won the 5 nations championship in 1994?), we do not recognize the named entity Tournoi des Cinq nations but the proper nouns Tournoi and Cinq and the noun Nation. Note that here capital letters on Tournoi and Cinq, very surprising in French indeed, surely make the correct recognition impossible.</p><p>-some links between questions and answers are too subtle. Table <ref type="table" coords="5,98.09,523.86,5.01,9.02" target="#tab_2">4</ref> points out that the system described previously resolved 74% of the proposed anaphora. For those questions, results were similar to the overall results. Surprisingly, an answer was correctly found for an unrecognised anaphora. This question is Qui était , avant cette chute, le président du régime communiste en Afghanistan ? (Who was the communist president before the fall?). Here the anaphora is on the fall that should be replaced by Communist regime collapse in Afghanistan. However, all referred words in the anaphora are mentioned in the question (Afghanistan, communiste, chute, régime), thus, making the anaphora resolution not that important for the document retrieval state. And finally, for the Extraction of Answer stage, as the adjunct of time is not necessary here to find the answer.</p><p>If our overall results are globally inferior this year, with 54% to be compared to the 68% of 2006, it can be interesting to compare both results on the same category of questions, that is questions out of sequences where answers can be found in the news corpus.</p><p>Corpus Table <ref type="table" coords="6,97.00,189.90,5.01,9.02" target="#tab_3">5</ref> highlights that the decrease observed in our result is due to both the introduction of sequences and the Wikipedia corpus. If we consider the question out of sequences with possible answers on the news corpus, the overall accuracy reaches 65%. Actually, if we add the so called NIL question to those questions, we reach an encouraging 69% (55+9/84+9) of correct answer improving slightly last year's result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>The analysis of the results lead us to identify a few issues that need to be solved.</p><p>We think that a measure between words could prevent us to miss some blocks if a noun phrase or a named entity was badly recognised.</p><p>In addition, we have to improve our results on the Wikipedia corpus. To begin with, we will try to take the most of the XML enrichment of the corpus we removed somehow too rapidly. For example, REDIRECT links could be used to enlarge our acronym databases and thus resolve automatically this category of questions.</p><p>Moreover, sequence treatment should be revised to consider all questions of the sequence as related to the same topic and therefore consider in a first place the blocks found for the previous question in the Document Retrieval stage of a new question. This work will be of particular interest for the introduction of QA treatment in a real (monolingual) oral dialogue with a user often seen as the future of QA systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,175.20,258.66,236.43,63.02"><head>Table 1 .</head><label>1</label><figDesc>Results for a monolingual French run.</figDesc><table coords="4,175.20,258.66,236.43,45.02"><row><cell></cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell></row><row><cell>Total answers</cell><cell>108</cell><cell>82</cell><cell>9</cell><cell>1</cell></row><row><cell>Percentage</cell><cell>54%</cell><cell>41%</cell><cell>4,5%</cell><cell>0.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,99.30,452.16,394.73,99.02"><head>Table 2 .</head><label>2</label><figDesc>Results for different corpus.</figDesc><table coords="4,99.30,452.16,394.73,81.02"><row><cell>of the answer</cell><cell>Answers</cell><cell>R</cell><cell>W</cell><cell>X</cell><cell>U</cell><cell>Overall accuracy</cell></row><row><cell>News</cell><cell>96</cell><cell>59</cell><cell>30</cell><cell>6</cell><cell>1</cell><cell>61%</cell></row><row><cell>News + Wikipedia</cell><cell>21</cell><cell>16</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>76%</cell></row><row><cell>Wikipedia</cell><cell>74</cell><cell>24</cell><cell>47</cell><cell>3</cell><cell>0</cell><cell>32%</cell></row><row><cell>Total</cell><cell>191</cell><cell>99</cell><cell>81</cell><cell>10</cell><cell>1</cell><cell>51%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.92,266.40,453.64,248.96"><head>Table 4 .</head><label>4</label><figDesc>If we consider the question Sur une montre, quelle aiguille représente les heures ? (On a watch, which needle represents the hour?) the answer is "...l'heure indiquée par la petite aiguille d'une horloge" (...the hour indicated by the small needle of a clock). Our synonym dictionary provides us with the information that représente (represent) is a synonym of indiquer with a proximity of 10%. Unfortunately, montre (watch) is not mentioned as a synonym of horloge (clock). As all pivots are very frequent in the corpus, the block containing the answer is not possibly selected here.As for the Selection of best sentences and the extraction of answer stage, many errors are connected with patterns of extraction that are difficult to fine tuned. Moreover we encounter some difficulties in writing extraction patterns for answers in long sentences or in a particular document layout like tables or enumeration.Finally, 9 questions failed due to a bad resolution of anaphora resulting in a lack of core pivots making it impossible to find answer blocks. As a matter of fact, out of the 76 questions in sequence, only 43 contained an anaphora. Results for question with anaphora.</figDesc><table coords="5,163.26,422.81,264.49,74.54"><row><cell></cell><cell>Total</cell><cell>R</cell><cell>W+X+U</cell><cell>Overall Accuracy</cell></row><row><cell>Anaphora recognised</cell><cell>33</cell><cell>19</cell><cell>14</cell><cell>56%</cell></row><row><cell>Anaphora missed</cell><cell>10</cell><cell>1</cell><cell>9</cell><cell>18%</cell></row><row><cell>Total</cell><cell>43</cell><cell>20</cell><cell>23</cell><cell>46%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,92.40,662.82,375.67,89.00"><head>Table 5 .</head><label>5</label><figDesc>Comparing with last year's result.</figDesc><table coords="5,92.40,662.82,375.67,89.00"><row><cell></cell><cell>of the answer</cell><cell>Total</cell><cell>R</cell><cell>W+X+U</cell><cell>Overall Accuracy</cell></row><row><cell>Out of</cell><cell>News and News + Wikipedia</cell><cell>84</cell><cell>55</cell><cell>29</cell><cell>65%</cell></row><row><cell>sequence</cell><cell>Wikipedia only</cell><cell>31</cell><cell>10</cell><cell>21</cell><cell>32%</cell></row><row><cell></cell><cell>Total</cell><cell>115</cell><cell>65</cell><cell>50</cell><cell>56%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,87.60,450.48,436.85,9.02;6,70.92,461.94,424.71,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,276.36,450.48,248.09,9.02;6,70.92,461.94,66.69,9.02">Campagne d&apos;évaluation EQueR-EVALDA : Évaluation en question-réponse</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,144.30,461.94,23.87,9.02">TALN</title>
		<editor>
			<persName><forename type="first">Tutoriels</forename><surname>Ateliers</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.42,479.46,438.12,9.02;6,70.92,490.98,453.50,9.02;6,70.92,502.44,64.22,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,70.92,490.98,295.33,9.02">Priberam&apos;s question answering system in a cross-language environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,374.64,490.98,46.47,9.02">CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. september 2006</date>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.14,519.96,439.29,9.02;6,70.92,531.48,246.94,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,227.16,519.96,175.17,9.02">QRISTAL, système de Questions-Réponses</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,409.74,519.96,23.85,9.02">TALN</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2005">2005. 2005. juin 2005</date>
			<pubPlace>Dourdan, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,84.60,548.94,433.44,9.02;6,70.92,560.46,419.44,9.02;6,70.92,571.98,161.72,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,265.86,548.94,252.18,9.02;6,70.92,560.46,18.06,9.02">Cross-Lingual Question Answering using QRISTAL for CLEF 2005</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nègre</forename><forename type="middle">S</forename><surname>Séguéla P</surname></persName>
		</author>
		<ptr target="http://clef.isti.cnr.it/2005/workingnotes/WorkingNotes2005/laurent05.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="6,96.00,560.46,44.91,9.02">CLEF 2005</title>
		<meeting><address><addrLine>Wien, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">2005. september 2005</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.20,600.96,439.22,9.02;6,70.92,612.48,235.57,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,268.80,600.96,255.62,9.02">Cross-Lingual Question Answering using QRISTAL for CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nègre</forename><forename type="middle">S</forename><surname>Séguéla P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,96.00,612.48,44.91,9.02">CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. 2006. september 2006</date>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.82,629.94,437.56,9.02;6,70.92,641.46,453.61,9.02;6,70.92,652.98,351.08,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,233.82,641.46,286.40,9.02">Overview of the CLEF 2006 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.92,652.98,185.46,9.02">Working Notes of the Workshop of CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. september 2006</date>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.12,670.44,427.13,9.02;6,70.92,681.96,453.55,9.02;6,70.92,693.48,307.20,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,317.65,670.44,194.60,9.02;6,70.92,681.96,150.63,9.02">Les résultats de la campagne EASY d&apos;évaluation desanalyseurs syntaxiques du français</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Paroubek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,227.82,681.96,296.65,9.02;6,70.92,693.48,108.86,9.02">Actes de la 14ème conférence annuelle sur le Traitement Automatique des Langues Naturelles (TALN)</title>
		<meeting>s de la 14ème conférence annuelle sur le Traitement Automatique des Langues Naturelles (TALN)<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">juin 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,85.38,716.46,439.08,9.02;6,70.92,727.92,453.57,9.02;6,70.92,739.44,420.21,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,298.98,727.92,225.51,9.02;6,70.92,739.44,67.90,9.02">Overview of the CLEF 2005 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">]</forename><surname>Magnini B</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Rijke M</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sutcliffe R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,145.62,739.44,185.50,9.02">Working Notes of the Workshop of CLEF 2005</title>
		<meeting><address><addrLine>Wien, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">2004. september 2005</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
