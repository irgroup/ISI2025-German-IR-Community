<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.66,98.15,317.54,12.58">Stemming Approaches for East European Languages</title>
				<funder ref="#_dVnAdeX">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.62,121.83,66.03,8.74"><forename type="first">Ljiljana</forename><surname>Dolamic</surname></persName>
							<email>ljiljana.dolamic@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.79,121.83,58.80,8.74"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.66,98.15,317.54,12.58">Stemming Approaches for East European Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5ECF825EF3E7A9BF1AFE21D9A6E8F455</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Content Analysis and Indexing]: Indexing methods</term>
					<term>Linguistic processing. I.2.7 [Natural Language Processing]: Language models. H.3.3 [Information Storage and Retrieval]: Retrieval models. H.3.4 [Systems and Software]: Performance evaluation Experimentation</term>
					<term>Performance</term>
					<term>Measurement</term>
					<term>Algorithms Natural Language Processing with East European Languages</term>
					<term>Stemmer</term>
					<term>Stemming Strategy</term>
					<term>Czech Language</term>
					<term>Hungarian Language</term>
					<term>Bulgarian Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our participation in this CLEF evaluation campaign, the first objective is to propose and evaluate various indexing and search strategies for the Czech language in order to hopefully produce better retrieval effectiveness than that of the language-independent approach (n-gram). Based on our stemming strategy used with other languages, we propose two light stemmers for this Slavic language and a third one based on a more aggressive suffix-stripping scheme that removes some derivational suffixes. Our second objective is to obtain a better picture of the relative merit of various search engines in exploring Hungarian and Bulgarian documents. Moreover for the Bulgarian language we developed a new and more aggressive stemmer. To evaluate these solutions we use our various IR models, including the Okapi, Divergence from Randomness (DFR) and statistical language model (LM) together with the classical tf . idf vectorprocessing approach. Our experiments tend to show that for the Bulgarian language removing certain frequently used derivational suffixes may improve mean average precision. For the Hungarian corpus, applying an automatic decompounding procedure improves the MAP. For the Czech language, a comparison between a light (inflectional only) and a more aggressive stemmer that removes both inflectional and some derivational suffixes reveals small performance differences. For this language only, the performance difference between a word-based or a 4gram indexing strategy is also rather small, while for the Hungarian or Bulgarian corpora, a wordbased approach tend to produce better MAP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the last few years, the IR group at University of Neuchatel has been involved in designing, implementing and evaluating IR systems for various natural languages, including both European <ref type="bibr" coords="1,460.66,628.77,38.76,8.74;1,70.92,640.05,55.92,8.74">(Savoy &amp; Abdou, 2007)</ref> and popular Asian <ref type="bibr" coords="1,205.63,640.05,57.00,8.74" target="#b12">(Savoy, 2005)</ref>  <ref type="bibr" coords="1,265.17,640.05,96.95,8.74">(Abdou &amp; Savoy, 2007a</ref>) languages (namely, Chinese, Japanese, and Korean). In this context our main objective is to promote effective monolingual IR in those languages. For our participation in the CLEF 2007 evaluation campaign we decided to review our stemming strategy by including some very frequently used derivational suffixes. When defining our stemming rules however we still focus only on nouns and adjectives.</p><p>The rest of this paper is organized as follows: Section 2 describes the main characteristics of the CLEF-2007 test-collections. Section 3 outlines the main aspects of our stopword lists and stemming procedures. Section 4 analyses the principal features of different indexing and search strategies, and evaluates their use with the available corpora. The data fusion approaches adapted in our experiments are explained in Section 5, and Section 6 depicts our official results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Test-Collections</head><p>The corpora used in our experiments include newspaper articles, namely Magyar Hirlap <ref type="bibr" coords="2,442.02,98.91,72.44,8.74">(2002, Hungarian)</ref>, <ref type="bibr" coords="2,70.92,109.92,91.90,9.02">Sega (2002, Bulgarian)</ref>, <ref type="bibr" coords="2,169.50,109.92,106.97,9.02">Standart (2002, Bulgarian)</ref>, <ref type="bibr" coords="2,283.14,109.92,57.03,9.02">Novinar (2002</ref><ref type="bibr" coords="2,340.17,110.19,169.37,8.74;2,70.92,121.47,21.67,8.74">, a new Bulgarian sub-collection in CLEF 2007)</ref>, Mladná fronta <ref type="bibr" coords="2,160.28,121.20,78.23,9.02">Dnes (2002, Czech)</ref>, Lidove <ref type="bibr" coords="2,275.27,121.20,85.90,9.02">Noviny (2002, Czech)</ref>. As shown in Table <ref type="table" coords="2,448.76,121.47,3.76,8.74" target="#tab_0">1</ref>, the Bulgarian corpus is relatively large compared to the others, both in size and in the number of documents. As for average article length, the Czech corpus is longer (212.6), while for the Bulgarian (135.9) and Hungarian (152.3) languages the lengths are relatively similar. It is interesting to note that even though the Hungarian collection is the smallest (105 MB), it contains a larger number of distinct indexing terms <ref type="bibr" coords="2,381.08,166.58,98.92,8.74;2,70.92,177.86,42.66,8.74">(191,738 computed after stemming)</ref> when compared to the Bulgarian and Czech corpuses.</p><p>During the indexing process we retained only the following logical sections from the original documents: &lt;TITLE&gt;, &lt;LEAD&gt;, and &lt;TEXT&gt;. From the topic descriptions we automatically removed certain phrases such as "Relevant document report …", "Подходящ е всеки документ" or "Keressünk olyan cikkeket, amelyek …", etc. All our runs were fully automatic.</p><p>As shown in the Appendix 2, the available topics cover various subjects (e.g., Topic #409: "Bali Car Bombing," Topic #414: "Beer Festivals," Topic #436: "VIP Divorces," or Topic #443: "World Swimming Records"), including both regional (Topic #445: "Prince Harry and Drugs") and more international coverage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stopword Lists and Stemming Procedures</head><p>During this evaluation campaign, our stopword list and stemmer for Hungarian were the same as that used in our CLEF 2006 participation <ref type="bibr" coords="2,189.75,629.43,95.38,8.74">(Savoy &amp; Abdou, 2007)</ref>. For this language our suggested stemmer mainly includes inflectional removals (gender, number and 23 grammatical cases, as for example in "házakat" → "ház" (house)) as well as some pronouns (e.g., "házamat" (my house) → "ház") and a few derivational suffixes (e.g., "temetés" (burial) → "temet" (to bury)). See <ref type="bibr" coords="2,254.84,663.27,54.39,8.74" target="#b13">Savoy (2007)</ref> for more information. Moreover, the Hungarian language uses compound constructions (e.g., "hétvégé" (weekend) = "hét" (week / seven) + "vég" (end)). In order to increase the matching possibilities between search keywords and document representations, we automatically decompounded Hungarian words using our decompounding algorithm <ref type="bibr" coords="2,412.55,697.11,55.15,8.74" target="#b11">(Savoy, 2004)</ref>, leaving both compound words and their component parts in the documents and queries. The stopword list retained contains 737 words. The stemmer and stopword list are freely available www.unine.ch/info/clef.</p><p>For the Bulgarian language we decided to modify the transliteration procedure we used previously to convert Cyrillic characters into Latin letters. By correcting an error and adapting it for the new transliteration scheme, we modified last year's stemmer and denoted it the light Bulgarian stemmer. In this language, definite articles and plural forms are represented by suffixes and the general noun pattern is the following: &lt;stem&gt; &lt;plural&gt; &lt;article&gt;. Our light stemmer contains eight rules for removing plurals and five for removing articles. Additionally we applied seven grammatical normalization rules plus three others to remove palatalization (changing a stem's final consonant when followed by a suffix beginning with certain vowels), as is very common in most Slavic languages (see Appendix 3 for all the rules). We also proposed a new and more aggressive Bulgarian stemmer that also removes some derivational suffixes (e.g., "страшен" (fearfull) → "страх" (fear)). The stopword list used for this language contains 309 words, somewhat bigger than that of last year (258 items).</p><p>For the Czech language, we proposed a new stopword list containing 467 forms (determinants, prepositions, conjunctions, pronouns, and some very frequent verb forms). We also designed and implemented three Czech stemmers. The first one is a light stemmer that removes only those inflectional suffixes attached to nouns or adjectives in order to conflate to the same stem those morphological variations related to gender (feminine, neutral vs. masculine), number (plural vs. singular) and various grammatical cases (seven in the Czech language). For example, the noun "město" (city) appears as such in its singular form (nominative, vocative or accusative) but varies with other cases, "města" (genitive), "městu" (dative), "městem" (instrumental) or "městě" (locative). The corresponding plural forms are "města", "měst", "městům", "městy" or "městech". In the Czech language all nouns have a gender, and with a few exceptions (indeclinable borrowed words), they are declined for both number and case. For Czech nouns, the general pattern is the following: &lt;stem&gt; &lt;possessive&gt; &lt;case&gt; in which &lt;case&gt; ending includes both gender and number. Adjectives are declined to match the gender, case and number of the nouns to which they are attached. To remove these various case endings from nouns and adjectives we devised 52 rules, and then before returning the computed stem, we added five normalization rules in order to control palatalization and certain vowel changes in the basic stem (see Appendix 4 for all details).</p><p>Our second Czech stemmer denoted "light+" also includes rules for removing comparative forms from adjectives (e.g., "krásný", "krásnější", "nejkrásnější" → "krásn" (beautiful, more beautiful, the most beautiful)). We do not however expect this light stemmer variation to result in any significant changes in retrieval performance.</p><p>Finally, we designed and implemented a more aggressive stemmer that includes certain rules to remove frequently used derivational suffixes (e.g., "členství"(membership) → "člen"(member)). In applying this third more aggressive stemmer (denoted "derivational") we hope to improve mean average precision (MAP). Finally and unlike other languages, we do not remove the diacritics when building Czech stemmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IR models and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Indexing and Searching Strategies</head><p>In order to obtain a high MAP values, we might adopt different weighting schemes applied to terms that occur in the documents or in the query. This weighting would allow us to account for term occurrence frequency (denoted tf ij for indexing term t j in document D i ), as well as their inverse document frequency (denoted idf j ). Moreover, we might normalize each indexing weight using the cosine to obtain the classical tf . idf formulation, rather than the more recent normalization approaches that account for document length.</p><p>In addition to this vector-space approach, we also considered probabilistic models such as the Okapi (or BM25) <ref type="bibr" coords="3,102.40,613.68,91.35,9.02" target="#b9">(Robertson et al. 2000)</ref>. As a second probabilistic approach, we implemented three variants of the DFR (Divergence from Randomness) family of models suggested by <ref type="bibr" coords="3,326.67,625.23,125.99,8.74" target="#b2">Amati &amp; van Rijsbergen (2002)</ref>. In this framework, the indexing weight w ij attached to term t j in document D i combines two information measures as follows:</p><formula xml:id="formula_0" coords="3,126.90,662.89,218.38,11.65">w ij = Inf 1 ij • Inf 2 ij = -log 2 [Prob 1 ij (tf)] • (1 -Prob 2 ij (tf))</formula><p>As a first model, we implemented the PB2 scheme, defined by the following equations:</p><formula xml:id="formula_1" coords="3,126.90,697.21,396.99,32.23">Inf 1 ij = -log 2 [(e -λ j • λ j tf ij )/tf ij !] with λ j = tc j / n (1) Prob 2 ij = 1 -[(tc j +1) / (df j • (tfn ij + 1))] with tfn ij = tf ij • log 2 [1 + ((c•mean dl) / l i )]<label>(2)</label></formula><p>where tc j indicates the number of occurrences of term t j in the collection, l i the length (number of indexing terms) of document D i , mean dl the average document length, n the number of documents in the corpus, and c a constant (the corresponding values are given in the Appendix 1).</p><p>For the second model called GL2, the implementation of Prob 1 ij is given by Equation <ref type="formula" coords="4,428.72,113.19,3.77,8.74">3</ref>, and Prob 2 ij is given by Equation <ref type="formula" coords="4,122.21,124.47,3.77,8.74" target="#formula_2">4</ref>, as follows:</p><formula xml:id="formula_2" coords="4,126.90,140.65,396.98,30.91">Prob 1 ij = [1 / (1+λ j )] • [λ j / (1+λ j )] tfn ij (3) Prob 2 ij = tfn ij / (tfn ij + 1)<label>(4)</label></formula><p>where λ j and tfn ij were defined previously.</p><p>For the third model called IneC2, the implementation is given by the following two equations:</p><formula xml:id="formula_3" coords="4,126.90,215.01,396.99,31.61">Inf 1 ij = tfn ij • log 2 [(n+1) / (n e +0,5)] with n e = n • [1 -[(n-1)/n] tc j ] (5) Prob 2 ij = 1 -[(tc j +1) / (df j • (tfn ij +1))]<label>(6)</label></formula><p>where n, tc j and tfn ij were defined previously, and df j indicates the number of documents in with the term t j occurs.</p><p>Finally, we also considered an approach based on a statistical language model (LM) <ref type="bibr" coords="4,424.48,284.07,67.90,8.74" target="#b5">(Hiemstra, 2000;</ref><ref type="bibr" coords="4,494.88,284.07,21.63,8.74" target="#b6">2002)</ref>, known as a non-parametric probabilistic model (the Okapi and DFR are viewed as parametric models). Probability estimates would thus not be based on any known distribution (e.g., as in Equation <ref type="formula" coords="4,448.52,306.63,5.01,8.74">1</ref>or 3), but rather be estimated directly based on occurrence frequencies in document D i or corpus C. Within this language model paradigm, various implementations and smoothing methods might be considered, although in this study we adopted a model proposed by <ref type="bibr" coords="4,191.59,340.47,64.29,8.74" target="#b6">Hiemstra (2002)</ref>, as described in Equation <ref type="formula" coords="4,363.57,340.47,3.77,8.74">7</ref>, combining an estimate based on document (P[t j | D i ]) and on corpus (P[t j | C]).</p><formula xml:id="formula_4" coords="4,126.90,367.93,396.99,24.91">P[D i | Q] = P[D i ] . ∏ t j ∈Q [λ j . P[t j | D i ] + (1-λ j ) . P[t j | C]] with P[t j | D i ] = tf ij /l i and P[t j | C] = df j /lc with lc = ∑ k df k (7)</formula><p>where λ j is a smoothing factor (constant for all indexing terms t j , and usually fixed at 0.35) and lc an estimate of the size of the corpus C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Evaluation</head><p>To measure retrieval performance, we adopted MAP values computed on the basis of 1,000 retrieved items per request as calculated with the new TREC-EVAL program. Using this evaluation tool, some evaluation differences may occur in the values computed according to the official measure (the latter always takes 50 queries into account while in our presentation we do not account for queries having no relevant items). In the following tables, the best performance under the given conditions (with the same indexing scheme and the same collection) is listed in bold type. Table <ref type="table" coords="4,110.46,711.27,5.01,8.74" target="#tab_1">2</ref> shows the MAP achieved by various probabilistic models using the Bulgarian collection with two different query formulations (TD or TDN) and the two stemmers. The last two columns show the MAP achieved by using a 4-gram indexing scheme (without applying a stemming approach). An analysis of this data shows that the best performing IR model corresponds to the DFR IneC2 model with all stemming approaches or query sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head><p>In the last lines we reported the MAP average over these 5 IR models together with percentage of variation compared to the medium (TD) query formulation or to the derivational stemmer (TD query). As depicted in the last lines, increasing the query size improves the MAP (around +9%). According to the average performance, the best indexing approach seems to be a word-based approach using our derivational stemmer. In this case, the MAP with TD query formulation is, in average, 0.3467 vs. 0.3021 for the 4-gram approach, a relative difference of 12.9%. The performance difference with the light stemmer is smaller in average (0.3467 vs. 0.3265), a relative difference of 5.8%. Table <ref type="table" coords="5,110.51,370.83,5.01,8.74" target="#tab_2">3</ref> reports the evaluations done with the Hungarian language (word-based and 4-gram indexing) and with the classical tf idf vector-space scheme. For the most part the same conclusions can be drawn for this language as those shown for Bulgarian (Table <ref type="table" coords="5,258.08,393.39,3.62,8.74" target="#tab_1">2</ref>). Firstly, the DFR In2C2 probabilistic model provides the best IR performance and secondly when compared to the TD query formulation the retrieval effectiveness is improved (around 11.6%). As depicted in the last three lines, the best indexing strategy seems to be a wordbased approach with an automatic decompounding procedure. Using this strategy as baseline and with TD query formulation, the average performance difference with an indexing strategy without a decompounding procedure is around 9.4% (0.3492 vs. 0.3166), while a 4-gram indexing scheme depicts an average MAP of 0.3220 having a percentage of degradation of around 7.8%. The evaluations done on the Czech language are depicted in Table <ref type="table" coords="5,354.17,651.33,3.76,8.74" target="#tab_3">4</ref>. In this case, we compared three stemmers and the 4-gram indexing approach (without stemming). The best performing IR models corresponds to either the DFR GL2 or the Okapi probabilistic model. The performance differences between these two IR models are usually rather small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head><p>As shown in the last three lines of Table <ref type="table" coords="5,249.38,702.45,3.76,8.74" target="#tab_3">4</ref>, the best indexing strategy seems to be the word-based indexing strategy using the light stemming approach. As expected, performance differences between the "light" and "light+" stemmers are rather small (2.14% when using the TD query formulation). Moreover, the performance differences between the 4-gram and the light stemming approach seem to be statistically not significant (in parametric (Okapi and DFR) and non-parametric (language model or LM) approaches. On the other hand, we also combined both word-based and n-gram indexing strategies. To perform such combination we evaluated various fusion operators (see Table <ref type="table" coords="7,214.66,95.91,5.01,8.74">8</ref> for a detailed list of their descriptions). The "Sum RSV" operator for example indicates that the combined document score (or the final retrieval status value) is simply the sum of the retrieval status value (RSV k ) of the corresponding document D k computed by each single indexing scheme <ref type="bibr" coords="7,501.20,118.47,18.97,8.74;7,70.92,129.75,59.26,8.74" target="#b4">(Fox &amp; Shaw, 1994)</ref>. Table <ref type="table" coords="7,164.94,129.75,5.01,8.74">8</ref> thus illustrates how both the "Norm Max" and "Norm RSV" apply a normalization procedure when combining document scores. When combining the retrieval status value (RSV k ) for various indexing schemes and in order to favor certain more efficient retrieval schemes, we could multiply the document score by a constant α i (usually equal to 1) reflecting the differences in retrieval performance.</p><formula xml:id="formula_5" coords="7,112.56,183.63,370.00,54.41">Sum RSV SUM (α i . RSV k ) Norm Max SUM (α i . (RSV k / Max i )) Norm RSV SUM [α i . ((RSV k -Min i ) / (Max i -Min i ))] Z-Score α i . [((RSV k -Mean i ) / Stdev i ) + δ i ] with  δ i = [(Mean i -Min i ) / Stdev i ]</formula><p>Table <ref type="table" coords="7,198.84,249.48,3.91,9.02">8</ref>: Data fusion combination operators used in this study</p><p>In addition to using these data fusion operators, we also considered the round-robin approach, wherein we took one document in turn from each individual list and removed any duplicates, retaining only the highest ranking occurrence. Finally we suggest merging the retrieved documents according to the Z-Score, computed for each result list. Within this scheme, for each ith result list we needed to compute the average RSV k value (denoted Mean i ) and the standard deviation (denoted Stdev i ). Based on these we could then normalize the retrieval status value for each document D k provided by the ith result list by computing the deviation of RSV k with respect to the mean (Mean i ). In Table <ref type="table" coords="7,246.50,334.71,3.76,8.74">8</ref>, Min i (Max i ) lists the minimal (maximal) RSV value in the ith result list. Of course, we might also weight the relative contribution of each retrieval scheme by assigning a different α i value to each retrieval model.  Table <ref type="table" coords="7,110.51,545.31,5.01,8.74" target="#tab_5">9</ref> depicts the evaluation of various data fusion operators, comparing them to the single approach using the language model (LM), Okapi or the DFR probabilistic models (PB2 or GL2). From this data, we can see that combining three IR models might improve retrieval effectiveness, only slightly for the Bulgarian collection, moderately for the Czech and noticeably for the Hungarian corpus. When combining different retrieval models, the Z-Score scheme tended to perform the best, or at least it had one of the best performing MAP (e.g., for the Hungarian corpus). Except for the Hungarian corpus, when compared to the best single search model, the performance achieved by the various data fusion approaches did not seem statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Official Results</head><p>Table <ref type="table" coords="7,110.51,667.83,10.03,8.74" target="#tab_6">10</ref> shows the exact specifications of our 12 official monolingual runs, based mainly on the probabilistic models (Okapi, DFR and statistical language model (LM)). For all languages we submitted three runs with the TD query formulation and one with the TDN. All runs are fully automatic and the same data fusion approach (Z-score) was applied in all cases. For the Hungarian corpus however we sometimes applied our decompounding approach (denoted by "dec" in the "Index" column) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this eighth CLEF evaluation campaign we evaluated various probabilistic IR models using three different test-collections written in three different East European languages, namely the Hungarian, Bulgarian and Czech languages. We suggested a new stemmer for the Bulgarian language that removed some very frequent derivational suffixes. For the Czech language, we designed and implemented three different stemmers.</p><p>Our various experiments tend to demonstrate that the Okapi model or the IneC2 model derived from Divergence from Randomness (DFR) paradigm tend to produce the best overall retrieval performances (see Tables <ref type="table" coords="8,100.20,637.05,21.49,8.74" target="#tab_3">2 to 4</ref>). The statistical language model (LM) used in our experiments usually results in retrieval performance inferior to that obtained with the Okapi or DFR approach.</p><p>For the Bulgarian language (Table <ref type="table" coords="8,225.96,665.61,3.63,8.74" target="#tab_1">2</ref>), our new and more aggressive stemmer tends to produce a better MAP when compared to a light stemming approach (5.8% in relative difference) and better than the 4-gram indexing scheme (-12.9%). For the Hungarian language (Table <ref type="table" coords="8,290.64,688.16,3.63,8.74" target="#tab_2">3</ref>), applying an automatic decompounding procedure seems to improve the MAP around 9.4% when compared to a word-based approach, or around 7.8% when compared to a 4-gram indexing scheme. For the Czech language however performance differences between a light (inflectional only) and a more aggressive stemmer removing both inflectional and some derivational suffixes were rather small (Table <ref type="table" coords="8,206.05,733.28,3.62,8.74" target="#tab_3">4</ref>). Moreover, the performance differences were also small when compared to those achieved with a 4-gram approach. Pseudo-relevance feedback (Rocchio's model) improves the MAP depending on the parameter settings (Tables <ref type="table" coords="9,250.95,73.35,21.50,8.74">5 to 7</ref>). A data fusion strategy may clearly enhance the retrieval performance for the Hungarian language (Table <ref type="table" coords="9,265.54,84.63,4.18,8.74">8</ref>) and slightly for the two other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,141.84,287.55,286.19,284.51"><head>Table 1 :</head><label>1</label><figDesc>CLEF 2007 test-collection statistics</figDesc><table coords="2,141.84,287.55,286.19,263.62"><row><cell></cell><cell>Bulgarian</cell><cell>Hungarian</cell><cell>Czech</cell></row><row><cell>Size (in MB)</cell><cell>261 MB</cell><cell>105 MB</cell><cell>178 MB</cell></row><row><cell># of documents</cell><cell>87,281</cell><cell>49,530</cell><cell>81,735</cell></row><row><cell># of distinct terms</cell><cell>169,394</cell><cell>191,738</cell><cell>194,500</cell></row><row><cell cols="3">Number of distinct indexing terms per document</cell><cell></cell></row><row><cell>Mean</cell><cell>99.5</cell><cell>105.4</cell><cell>117.7</cell></row><row><cell>Standard deviation</cell><cell>93.86</cell><cell>91.08</cell><cell>105.79</cell></row><row><cell>Median</cell><cell>70</cell><cell>75</cell><cell>90</cell></row><row><cell>Maximum</cell><cell>1,193</cell><cell>1,284</cell><cell>2,350</cell></row><row><cell>Minimum</cell><cell>0</cell><cell>2</cell><cell>1</cell></row><row><cell cols="2">Number of indexing terms per document</cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>135.9</cell><cell>152.3</cell><cell>212.6</cell></row><row><cell>Standard deviation</cell><cell>143.58</cell><cell>145.86</cell><cell>193</cell></row><row><cell>Median</cell><cell>91</cell><cell>102</cell><cell>160</cell></row><row><cell>Maximum</cell><cell>2,837</cell><cell>6,008</cell><cell>4,846</cell></row><row><cell>Minimum</cell><cell>0</cell><cell>5</cell><cell>1</cell></row><row><cell>Number of queries</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>Number rel. items</cell><cell>1,012</cell><cell>911</cell><cell>762</cell></row><row><cell>Mean rel./ request</cell><cell>20.24</cell><cell>18.22</cell><cell>15.24</cell></row><row><cell>Standard deviation</cell><cell>14.23</cell><cell>14.08</cell><cell>12.08</cell></row><row><cell>Median</cell><cell>17.5</cell><cell>14</cell><cell>10.5</cell></row><row><cell>Maximum</cell><cell>62 (T#438)</cell><cell>66 (T#415)</cell><cell>47 (T#415)</cell></row><row><cell>Minimum</cell><cell>2 (T#419)</cell><cell>1 (T#411)</cell><cell>2 (T#411)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,76.56,539.55,445.40,163.19"><head>Table 2 :</head><label>2</label><figDesc>MAP of various IR models and query formulations (Bulgarian language)</figDesc><table coords="4,76.56,539.55,445.40,143.26"><row><cell></cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell>Bulgarian</cell><cell>Bulgarian</cell></row><row><cell>Query</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Stemmer / indexing unit</cell><cell cols="6">light / word light / word deriv./word deriv./word none/4-gram none/4-gram</cell></row><row><cell>Model \ # of queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>Okapi</cell><cell>0.3155</cell><cell>0.3462</cell><cell>0.3425</cell><cell>0.3720</cell><cell>0.3022</cell><cell>0.3342</cell></row><row><cell>DFR GL2</cell><cell>0.3307</cell><cell>0.3653</cell><cell>0.3541</cell><cell>0.3909</cell><cell>0.3100</cell><cell>0.3250</cell></row><row><cell>DFR PB2</cell><cell>0.3266</cell><cell>0.3476</cell><cell>0.3394</cell><cell>0.3637</cell><cell>0.2960</cell><cell>0.3116</cell></row><row><cell>DFR IneC2</cell><cell>0.3423</cell><cell>0.3696</cell><cell>0.3606</cell><cell>0.3862</cell><cell>0.3156</cell><cell>0.3409</cell></row><row><cell>LM (λ=0.35) tf . idf</cell><cell>0.3175 0.2103</cell><cell>0.3580 0.2264</cell><cell>0.3368 0.2143</cell><cell>0.3782 0.2293</cell><cell>0.2868 0.2105</cell><cell>0.3294 0.2271</cell></row><row><cell>Average</cell><cell>0.3265</cell><cell>0.3573</cell><cell>0.3467</cell><cell>0.3782</cell><cell>0.3021</cell><cell>0.3282</cell></row><row><cell>% change over TD</cell><cell></cell><cell>+9.4%</cell><cell></cell><cell>+9.09%</cell><cell></cell><cell>+8.6%</cell></row><row><cell>% change</cell><cell>-5.8%</cell><cell></cell><cell>baseline</cell><cell></cell><cell>-12.9%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,76.56,199.11,440.92,163.19"><head>Table 3 :</head><label>3</label><figDesc>MAP of various IR models and query formulations (Hungarian language)</figDesc><table coords="5,76.56,199.11,440.92,143.26"><row><cell></cell><cell cols="6">Hungarian Hungarian Hungarian Hungarian Hungarian Hungarian</cell></row><row><cell>Query</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Indexing unit</cell><cell cols="2">decompound decompound</cell><cell>word</cell><cell>word</cell><cell>4-gram</cell><cell>4-gram</cell></row><row><cell>Model \ # of queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>Okapi</cell><cell>0.3629</cell><cell>0.3959</cell><cell>0.3255</cell><cell>0.3763</cell><cell>0.3445</cell><cell>0.3797</cell></row><row><cell>DFR GL2</cell><cell>0.3615</cell><cell>0.3994</cell><cell>0.3324</cell><cell>0.3809</cell><cell>0.3495</cell><cell>0.3702</cell></row><row><cell>DFR PB2</cell><cell>0.3799</cell><cell>0.4106</cell><cell>0.3428</cell><cell>0.3910</cell><cell>0.3355</cell><cell>0.3599</cell></row><row><cell>DFR IneC2</cell><cell>0.3897</cell><cell>0.4271</cell><cell>0.3525</cell><cell>0.4031</cell><cell>0.3527</cell><cell>0.3828</cell></row><row><cell>LM (λ=0.35) tf . idf</cell><cell>0.3482 0.2532</cell><cell>0.3921 0.2887</cell><cell>0.3118 0.2344</cell><cell>0.3669 0.2806</cell><cell>0.3153 0.2345</cell><cell>0.3555 0.2506</cell></row><row><cell>Average</cell><cell>0.3492</cell><cell>0.3856</cell><cell>0.3166</cell><cell>0.3665</cell><cell>0.3220</cell><cell>0.3498</cell></row><row><cell>% change over TD</cell><cell></cell><cell>+10.4%</cell><cell></cell><cell>+15.8%</cell><cell></cell><cell>+8.6%</cell></row><row><cell>% change</cell><cell>baseline</cell><cell></cell><cell>-9.4%</cell><cell></cell><cell>-7.8%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,76.56,490.59,443.64,152.21"><head>Table 4 :</head><label>4</label><figDesc>MAP of various IR models and query formulations (Czech language)</figDesc><table coords="5,76.56,490.59,443.64,132.28"><row><cell></cell><cell>Czech</cell><cell>Czech</cell><cell>Czech</cell><cell>Czech</cell><cell>Czech</cell><cell>Czech</cell></row><row><cell>Query</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TD</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Stemmer</cell><cell>light</cell><cell>light</cell><cell>light+</cell><cell>4-gram</cell><cell cols="2">derivational derivational</cell></row><row><cell>Model \ # of queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell><cell>50 queries</cell></row><row><cell>Okapi</cell><cell>0.3355</cell><cell>0.3616</cell><cell>0.3255</cell><cell>0.3401</cell><cell>0.3255</cell><cell>0.3669</cell></row><row><cell>DFR GL2</cell><cell>0.3437</cell><cell>0.3678</cell><cell>0.3323</cell><cell>0.3365</cell><cell>0.3342</cell><cell>0.3678</cell></row><row><cell>DFR PB2</cell><cell>0.3233</cell><cell>0.3434</cell><cell>0.3144</cell><cell>0.3188</cell><cell>0.3164</cell><cell>0.3472</cell></row><row><cell>LM (λ=0.35) tf . idf</cell><cell>0.3263 0.2050</cell><cell>0.3626 0.2338</cell><cell>0.3182 0.2105</cell><cell>0.3204 0.2126</cell><cell>0.3109 0.1984</cell><cell>0.3594 0.2303</cell></row><row><cell>Average</cell><cell>0.3068</cell><cell>0.3338</cell><cell>0.3002</cell><cell>0.3057</cell><cell>0.2971</cell><cell>0.3343</cell></row><row><cell>% change over TD</cell><cell></cell><cell>+8.83%</cell><cell></cell><cell></cell><cell></cell><cell>+12.54%</cell></row><row><cell>% change</cell><cell>baseline</cell><cell></cell><cell>-2.14%</cell><cell>-0.35%</cell><cell>-3.16%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,92.88,527.76,409.17,9.02"><head>Table 9 :</head><label>9</label><figDesc>Mean average precision using different combination operators (with blind-query expansion)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,76.56,74.79,441.88,442.49"><head>Table 10 :</head><label>10</label><figDesc>Description and mean average precision (MAP) of our official monolingual runs</figDesc><table coords="8,76.56,74.79,441.88,421.60"><row><cell>Run name</cell><cell>Query</cell><cell>Index</cell><cell>Stem</cell><cell>Model</cell><cell>Query expansion</cell><cell cols="2">Single MAP Comb MAP</cell></row><row><cell>UniNEbg1</cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>Okapi</cell><cell>Roc 3 docs / 150 terms</cell><cell>0.3169</cell><cell>Z-score</cell></row><row><cell>BG</cell><cell>TD</cell><cell>word</cell><cell>light</cell><cell>PB2</cell><cell>idf 5 docs / 60 terms</cell><cell>0.3750</cell><cell>0.4128</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">word deriva.</cell><cell>LM</cell><cell>Roc 10 docs / 50 terms</cell><cell>0.4098</cell><cell></cell></row><row><cell>UniNEbg2</cell><cell>TD</cell><cell cols="2">word deriva.</cell><cell>LM</cell><cell>Roc 10 docs / 120 terms</cell><cell>0.4004</cell><cell>Z-Score</cell></row><row><cell>BG</cell><cell>TD</cell><cell>word</cell><cell>light</cell><cell>IneC2</cell><cell>idf 5 docs / 60 terms</cell><cell>0.3740</cell><cell>0.4108</cell></row><row><cell>UniNEbg3</cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>LM</cell><cell>idf 3 docs / 120 terms</cell><cell>0.3336</cell><cell>Z-Score</cell></row><row><cell>BG</cell><cell>TD</cell><cell>word</cell><cell>light</cell><cell>LM</cell><cell>Roc 5 docs / 40 terms</cell><cell>0.3624</cell><cell>0.3999</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">word deriva.</cell><cell>LM</cell><cell>idf 10 docs / 50 terms</cell><cell>0.4013</cell><cell></cell></row><row><cell>UniNEbg4</cell><cell cols="3">TDN 4-gram none</cell><cell>Okapi</cell><cell>Roc 3 docs / 150 terms</cell><cell>0.3406</cell><cell>Z-score</cell></row><row><cell>BG</cell><cell>TDN</cell><cell>word</cell><cell>light</cell><cell>PB2</cell><cell>idf 5 docs / 60 terms</cell><cell>0.4038</cell><cell>0.4422</cell></row><row><cell></cell><cell>TDN</cell><cell cols="2">word deriva.</cell><cell>LM</cell><cell>Roc 10 docs / 50 terms</cell><cell>0.4418</cell><cell></cell></row><row><cell>UniNEhu1</cell><cell>TD</cell><cell>dec</cell><cell>stem</cell><cell>LM</cell><cell>Roc 5 docs / 100 terms</cell><cell>0.4323</cell><cell>Z-score</cell></row><row><cell>HU</cell><cell>TD</cell><cell>word</cell><cell>stem</cell><cell>GL2</cell><cell>Roc 5 docs / 70 terms</cell><cell>0.4375</cell><cell>0.4606</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>PB2</cell><cell>idf 3 docs / 80 terms</cell><cell>0.3886</cell><cell></cell></row><row><cell>UniNEhu2</cell><cell>TD</cell><cell>dec</cell><cell>stem</cell><cell>LM</cell><cell>Roc 5 docs / 70 terms</cell><cell>0.4315</cell><cell>Z-score</cell></row><row><cell>HU</cell><cell>TD</cell><cell>word</cell><cell>stem</cell><cell>GL2</cell><cell>idf 5 docs / 100 terms</cell><cell>0.4376</cell><cell>0.4716</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>Okapi</cell><cell>idf 3 docs / 120 terms</cell><cell>0.4233</cell><cell></cell></row><row><cell>UniNEhu3</cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>LM</cell><cell>idf 3 docs / 120 terms</cell><cell>0.3842</cell><cell>Z-score</cell></row><row><cell>HU</cell><cell>TD</cell><cell>word</cell><cell>stem</cell><cell>GL2</cell><cell>Roc 5 docs / 100 terms</cell><cell>0.4379</cell><cell>0.4586</cell></row><row><cell></cell><cell>TD</cell><cell>dec</cell><cell>stem</cell><cell>PB2</cell><cell>idf 5 docs / 20 terms</cell><cell>0.4366</cell><cell></cell></row><row><cell>UniNEhu4</cell><cell>TDN</cell><cell>dec</cell><cell>stem</cell><cell>LM</cell><cell>Roc 5 docs / 100 terms</cell><cell>0.4604</cell><cell>Z-score</cell></row><row><cell>HU</cell><cell>TDN</cell><cell>word</cell><cell>stem</cell><cell>GL2</cell><cell>Roc 5 docs / 70 terms</cell><cell>0.4664</cell><cell>0.4773</cell></row><row><cell></cell><cell cols="3">TDN 4-gram none</cell><cell>PB2</cell><cell>idf 3 docs / 80 terms</cell><cell>0.4108</cell><cell></cell></row><row><cell>UniNEcz1</cell><cell>TD</cell><cell cols="2">word light+</cell><cell>Okapi</cell><cell>idf 5 docs / 20 terms</cell><cell>0.4013</cell><cell>Z-score</cell></row><row><cell>CZ</cell><cell>TD</cell><cell cols="2">word deriva.</cell><cell>LM</cell><cell>Roc 5 docs / 50 terms</cell><cell>0.4002</cell><cell>0.4167</cell></row><row><cell>UniNEcz2</cell><cell>TD</cell><cell>word</cell><cell>light</cell><cell>Okapi</cell><cell>Roc 5 docs / 20 terms</cell><cell>0.3560</cell><cell>Z-score</cell></row><row><cell>CZ</cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>GL2</cell><cell>idf 5 docs / 70 terms</cell><cell>0.3798</cell><cell>0.4134</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">word light+</cell><cell>PB2</cell><cell>Roc 5 docs / 50 terms</cell><cell>0.3632</cell><cell></cell></row><row><cell>UniNEcz3</cell><cell>TD</cell><cell>word</cell><cell>light</cell><cell>LM</cell><cell>idf 5 docs / 20 terms</cell><cell>0.4070</cell><cell>Z-score</cell></row><row><cell>CZ</cell><cell>TD</cell><cell cols="2">4-gram none</cell><cell>Okapi</cell><cell>Roc 5 docs / 70 terms</cell><cell>0.3672</cell><cell>0.4225</cell></row><row><cell></cell><cell>TD</cell><cell cols="2">word light+</cell><cell>GL2</cell><cell>Roc 5 docs / 50 terms</cell><cell>0.4085</cell><cell></cell></row><row><cell>UniNEcz4</cell><cell>TDN</cell><cell cols="2">word deriva.</cell><cell>Okapi</cell><cell>Roc 5 docs / 20 terms</cell><cell>0.3627</cell><cell>Z-score</cell></row><row><cell>CZ</cell><cell cols="3">TDN 4-gram none</cell><cell>LM</cell><cell>Roc 5 docs / 100 terms</cell><cell>0.3953</cell><cell>0.4242</cell></row><row><cell></cell><cell>TDN</cell><cell cols="2">word light+</cell><cell>GL2</cell><cell>idf 5 docs / 50 terms</cell><cell>0.4048</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to also thank the CLEF-2007 task organizers for their efforts in developing various European language test-collections. The authors would also thank <rs type="person">Samir Abdou</rs> for his help during the implementations of the different stemmers within the Lucene system. This research was supported in part by the <rs type="funder">Swiss National Science Foundation</rs> under Grant #<rs type="grantNumber">200021-113273</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dVnAdeX">
					<idno type="grant-number">200021-113273</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>average, 0.3068 vs. 0.3057 with TD query formulation). As for the other corpora, increasing the query size improves the MAP (around +10%).</p><p>An analysis showed that pseudo-relevance feedback (PRF or blind-query expansion) seemed to be a useful technique for enhancing retrieval effectiveness. In this study, we adopted Rocchio's approach (denoted "Roc") <ref type="bibr" coords="6,70.92,125.28,87.80,9.02" target="#b3">(Buckley et al., 1996)</ref> with α = 0.75, β = 0.75, whereby the system was allowed to add m terms extracted from the k best ranked documents from the original query. From our previous experiments we learned that this type of blind query expansion strategy does not always work well. More particularly, we believe that including terms occurring frequently in the corpus (because they also appear in the top-ranked documents) may introduce more noise, and thus be an ineffective means of discriminating between relevant and non-relevant items <ref type="bibr" coords="6,467.16,170.66,30.94,8.74;6,70.92,181.94,54.58,8.74" target="#b8">(Peat &amp; Willett, 1991)</ref>. Consequently we chose to also apply our idf-based query expansion model (denoted "idf" in Tables <ref type="table" coords="6,100.23,193.22,5.01,8.74">9</ref> and<ref type="table" coords="6,124.83,193.22,8.93,8.74">10</ref>) <ref type="bibr" coords="6,140.77,193.22,100.31,8.74" target="#b1">(Abdou &amp; Savoy, 2007b)</ref>.</p><p>To evaluate these propositions, we applied certain probabilistic models and enlarged the query by the 20 to 150 (indexing words or n-grams) retrieved from the 3 to 10 best-ranked articles within the Bulgarian (Table <ref type="table" coords="6,99.66,233.06,3.62,8.74">5</ref>), Hungarian (Table <ref type="table" coords="6,186.71,233.06,4.18,8.74">6</ref>) and Czech corpora (Table <ref type="table" coords="6,304.18,233.06,3.62,8.74">7</ref>). For the Bulgarian corpus (Table <ref type="table" coords="6,216.48,498.75,3.63,8.74">5</ref>), enhancement increased from +1.47% (4-gram, Okapi, 0.3022 vs. 0.3065) to +21.7% <ref type="bibr" coords="6,115.41,510.03,48.37,8.74">(LM model,</ref><ref type="bibr" coords="6,166.32,510.03,73.53,8.74">0.3368 vs. 0.4098)</ref>. For the Hungarian collection (Table <ref type="table" coords="6,396.07,510.03,3.63,8.74">6</ref>), percentage improvement varied from +6.1% (4-gram, Okapi model, 0.3445 vs. 0.3654) to +10.1% <ref type="bibr" coords="6,366.24,521.31,48.38,8.74">(LM model,</ref><ref type="bibr" coords="6,417.15,521.31,73.55,8.74">0.3913 vs. 0.4323)</ref>. For the Czech language (Table <ref type="table" coords="6,180.76,532.59,3.62,8.74">7</ref>), the percentages of variation range from -2.6% (4-gram, Okapi model, 0.3401 vs. 0.3314) to +21.6% (DFR GL2 model, 0.3437 vs. 0.4179). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Fusion</head><p>It is assumed that combining different search models should improve retrieval effectiveness, due to the fact that each document representation might not retrieve the same pertinent items and thus increase the overall recall <ref type="bibr" coords="6,70.92,745.29,93.58,8.74" target="#b15">(Vogt &amp; Cottrell, 1999)</ref>. In this current study we combined three probabilistic models representing both the RemoveArticle(word) { if (word ends with "-ът") then remove "-ът" return; # masculine if (word ends with "-ят") then # masculine if (word ends with " V+ят") then replace by "-й" # V -any vowel else remove "-ят" return; if (word ends with "-то") then remove "-то" return; # neutral if (word ends with "-те") then remove "-те" return; # neutral if (word ends with "-та") then remove "-та" return; # feminine return; } RemovePlural(word) { if (word ends with "-ища") then remove "-ища" return; # for adjectives if (word ends with "-ище") then remove "-ище" return; # for adjectives if (word ends with "-овци") then replace by "-о" return; # for adjectives if (word ends with "-евци") then replace by "-е" return; # for adjectives if (word ends with "-ове") then remove "-ове" return; # masculine if (word ends with "-еве") then # masculine if (word ends with " V+ еве") then replace by "-й" else remove "-еве" return; if (word ends with "-та") then remove "-та" return;</p><p># feminine if (word ends with "-..е.и") then replace by "-.я." return; # rewriting rule return;</p><p># with . any character } Normalize(word) { if (word ends with "-еи" or "-ии") then remove "-еи" or "-ии"; if (word ends with "-я") then # normalize if (word ends with " V+ я") then replace by "-й" # adjectives else remove "-я"; if (word ends with "-[аой]") then remove "-[аой]"; if (word ends with "-[еи]") then remove "-[еи]"; if (word ends with "-йн") then replace by "-н" return;</p><p># rewriting rule if (word ends with "-LеC") then replace by "-LC";</p><p># L-any letter if (word ends with "-LъL") then replace by "-LL";</p><p># C-any consonant return; } Palatalization(word) { if (word ends with "-ц" or "-ч") then replace by "-к" return; if (word ends with "-з" or "-ж") then replace by "-г" return; if (word ends with "-с" or "-ш") then replace by "-х" return; return; } RemovePossessives(word) { if (word ends with "-ov") then remove "-ov" return; if (word ends with "-in") then remove "-in" return; if (word ends with "-ův") then remove "-ův" return; return; } Normalize(word) { if (word ends with "čt") then replace by "ck" return; if (word ends with "št") then replace by "sk" return; if (word ends with "c" or "č") then replace by "k" return; if (word ends with "z" or "ž") then replace by "h" return; if (word ends with ".ů.") then replace by ".o." return; return; } -12 -RemoveCase(word) { if (word ends with "-atech") then remove "-atech" return; if (word ends with "-ětem") then remove "-ětem" return; if (word ends with "-etem") then remove "-etem" return; if (word ends with "-atům") then remove "-atům" return; if (word ends with "-ech") then remove "-ech" return; if (word ends with "-ich") then remove "-ich" return; if (word ends with "-ích") then remove "-ích" return; if (word ends with "-ého") then remove "-ého" return; if (word ends with "-ěmi") then remove "-ěmi" return; if (word ends with "-emi") then remove "-emi" return; if (word ends with "-ému") then remove "-ému" return; if (word ends with "-ěte") then remove "-ěte" return; if (word ends with "-ete") then remove "-ete" return; if (word ends with "-ěti") then remove "-ěti" return; if (word ends with "-eti") then remove "-eti" return; if (word ends with "-ího") then remove "-ího" return; if (word ends with "-iho") then remove "-iho" return; if (word ends with "-ími") then remove "-ími" return; if (word ends with "-ímu") then remove "-ímu" return; if (word ends with "-imu") then remove "-imu" return; if (word ends with "-ách") then remove "-ách" return; if (word ends with "-ata") then remove "-ata" return; if (word ends with "-aty") then remove "-aty" return; if (word ends with "-ých") then remove "-ých" return; if (word ends with "-ama") then remove "-ama" return; if (word ends with "-ami") then remove "-ami" return; if (word ends with "-ové") then remove "-ové" return; if (word ends with "-ovi") then remove "-ovi" return; if (word ends with "-ými") then remove "-ými" return; if (word ends with "-em") then remove "-em" return; if (word ends with "-es") then remove "-es" return; if (word ends with "-ém") then remove "-ém" return; if (word ends with "-ím") then remove "-ím" return; if (word ends with "-ům") then remove "-ům" return; if (word ends with "-at") then remove "-at" return; if (word ends with "-ám") then remove "-ám" return; if (word ends with "-os") then remove "-os" return; if (word ends with "-us") then remove "-us" return; if (word ends with "-ým") then remove "-ým" return; if (word ends with "-mi") then remove "-mi" return; if (word ends with "-ou") then remove "-ou" return; if (word ends with "-[aeiouyáéíýě]") then remove "-[aeiouyáéíýě]" return; return; } </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.92,203.58,452.34,9.02;9,92.16,214.86,304.82,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,197.22,203.85,254.84,8.74">Monolingual experiments with Far-East Languages in NTCIR-6</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,473.10,203.58,50.16,9.02;9,92.16,214.86,34.37,9.02">Proceedings NTCIR-6</title>
		<meeting>NTCIR-6<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>National Institute of Informatics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,228.39,422.35,8.74;9,92.16,239.67,252.93,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,197.75,228.39,295.52,8.74;9,92.16,239.67,40.26,8.74">Searching in Medline: Stemming, query expansion, and manual indexing evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,141.50,239.67,156.46,8.74">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007">2007b</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,70.92,252.93,446.36,8.74;9,92.16,263.94,365.41,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,241.55,252.93,275.73,8.74;9,92.16,264.21,114.35,8.74">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,216.12,263.94,171.68,9.02">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,277.47,408.27,8.74;9,92.16,288.48,299.01,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,302.91,277.47,157.28,8.74">New retrieval approaches using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<idno>#500-236</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.16,288.48,93.33,9.02">Proceedings of TREC-4</title>
		<meeting>TREC-4<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,301.74,432.22,9.02;9,92.16,313.29,152.79,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,203.22,302.01,134.14,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>#500-215</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,357.30,301.74,83.01,9.02">Proceedings TREC-2</title>
		<meeting>TREC-2<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,326.55,368.95,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,159.30,326.55,193.01,8.74">Using language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="9,70.92,339.81,448.76,8.74;9,92.16,350.82,268.66,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,159.25,339.81,343.42,8.74">Term-specific smoothing for the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.16,350.82,124.68,9.02">Proceedings of the ACM-SIGIR</title>
		<meeting>the ACM-SIGIR<address><addrLine>Tempere</addrLine></address></meeting>
		<imprint>
			<publisher>The ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,364.08,439.61,9.02;9,92.16,375.36,92.54,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,223.58,364.35,268.90,8.74">Character n-gram tokenization for European language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,501.06,364.08,9.47,9.02;9,92.16,375.36,29.54,9.02">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,388.89,438.01,8.74;9,92.16,399.90,367.64,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,202.57,388.89,306.36,8.74;9,92.16,400.17,65.71,8.74">The limitations of term co-occurrence data for query expansion in document retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Peat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,167.16,399.90,225.53,9.02">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="383" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,413.43,410.74,8.74;9,92.16,424.44,223.16,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,283.51,413.43,193.05,8.74">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,92.16,424.44,157.71,9.02">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,437.70,411.01,9.02;9,92.16,448.98,120.43,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,144.29,437.97,223.16,8.74">Statistical inference in retrieval effectiveness evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,376.50,437.70,105.43,9.02;9,92.16,448.98,49.95,9.02">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="512" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,462.51,425.53,8.74;9,92.16,473.52,431.21,9.02;9,92.16,484.80,367.26,9.02" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,144.34,462.51,352.11,8.74;9,92.16,473.79,85.41,8.74">Report on CLEF-2003 monolingual tracks: Fusion of probabilistic models for effective monolingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,414.06,473.52,109.31,9.02;9,92.16,484.80,220.89,9.02">Comparative Evaluation of Multilingual Information Access Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="322" to="336" />
		</imprint>
	</monogr>
	<note>LNCS #3237</note>
</biblStruct>

<biblStruct coords="9,70.92,498.33,415.39,8.74;9,92.16,509.34,369.02,9.02" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,141.79,498.33,344.52,8.74;9,92.16,509.61,38.38,8.74">Comparative study of monolingual and multilingual search models for use with Asian languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,139.86,509.34,256.57,9.02">ACM Transactions on Asian Languages Information Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.92,522.60,447.30,9.02;9,92.16,534.15,29.28,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,144.37,522.87,191.47,8.74">Searching strategies for the Hungarian language</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,345.18,522.60,157.71,9.02">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,70.92,547.41,446.78,8.74;9,92.16,558.69,419.30,8.74;9,92.16,569.97,251.58,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,192.75,547.41,246.85,8.74">Experiments with monolingual, bilingual, and robust retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,444.60,558.69,66.86,8.74;9,92.16,569.97,72.02,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin; Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,70.92,582.96,429.98,9.02" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,221.55,583.23,164.13,8.74">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,394.68,582.96,41.55,9.02">IR Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
