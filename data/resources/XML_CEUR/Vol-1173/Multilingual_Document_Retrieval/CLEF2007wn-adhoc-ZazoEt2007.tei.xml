<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.22,148.86,302.56,15.15">REINA at CLEF 2007 Robust Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.82,182.73,68.47,8.77"><forename type="first">Angel</forename><forename type="middle">F</forename><surname>Zazo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">REINA Research Group</orgName>
								<orgName type="institution" key="instit2">University of Salamanca C</orgName>
								<address>
									<addrLine>Francisco Vitoria 6-16</addrLine>
									<postCode>37008</postCode>
									<settlement>Salamanca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.43,182.73,96.93,8.77"><forename type="first">Carlos</forename><forename type="middle">G</forename><surname>Figuerola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">REINA Research Group</orgName>
								<orgName type="institution" key="instit2">University of Salamanca C</orgName>
								<address>
									<addrLine>Francisco Vitoria 6-16</addrLine>
									<postCode>37008</postCode>
									<settlement>Salamanca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.31,182.73,119.86,8.77"><forename type="first">José</forename><forename type="middle">L Alonso</forename><surname>Berrocal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">REINA Research Group</orgName>
								<orgName type="institution" key="instit2">University of Salamanca C</orgName>
								<address>
									<addrLine>Francisco Vitoria 6-16</addrLine>
									<postCode>37008</postCode>
									<settlement>Salamanca</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.22,148.86,302.56,15.15">REINA at CLEF 2007 Robust Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D63FE9446AC61C686B86132DAF077077</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Content Analysis and Indexing]: Indexing methods, Thesauruses</term>
					<term>H.3.3 [Information Search and Retrieval]: Query formulation, Relevance feedback</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation</term>
					<term>I.2.7 [Natural Language Processing]: Machine Translation Measurement, Performance, Experimentation Robust Retrieval, Query Expansion, Term Windows, Association Thesauri, CLIR, Machine Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our work at CLEF 2007 Robust Task. We have participated in the monolingual (English, French and Portuguese) and the bilingual (English to French) subtask. At CLEF 2006 our research group obtained very good results applying local query expansion using windows of terms in the robust task. This year we have used the same expansion technique, but taking into account some criteria of robustness: MAP, GMAP, MMR, GS@10, P@10, number of failed topics, number of topics bellow 0.1 MAP, and number of topics with P@10=0. In bilingual retrieval experiments three machine translation programs were used to translate topics. For the target language, translations were merged before performing a monolingual retrieval. We also applied the same local expansion technique. This year the results were disappointing. We think out that the reason is the difficulty to select the best measurement for robustness. Perhaps the problem is that all measurements are average results over all topics, but the hard topics are inherently hard and must be analyze separately. This year all our runs also ends up in good ranking, both base runs and expanded ones. We think that the reason is that we used a good information retrieval system, and the expansion technique is robust because it does not deteriorate significantly the retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust retrieval tries to obtain stable performance over all topics by focusing on poorly performing topics. Robust tracks were carried out at <ref type="bibr" coords="1,270.64,697.82,49.65,8.74">TREC 2003</ref><ref type="bibr" coords="1,327.87,697.82,19.93,8.74">TREC , 2004</ref><ref type="bibr" coords="1,369.81,697.82,19.93,8.74">TREC and 2005</ref> for monolingual retrieval <ref type="bibr" coords="1,502.48,697.82,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,90.00,709.78,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,100.71,709.78,7.01,8.74" target="#b4">5]</ref>, and at CLEF 2006, including monolingual, bilingual and multilingual retrieval <ref type="bibr" coords="1,454.85,709.78,9.97,8.74" target="#b0">[1]</ref>. This year only monolingual (English, French and Portuguese) and bilingual (English to French) subtask were carried out. Our research group has participated in all the subtasks. For a complete description of this task, please, see the CLEF 2007 Ad-hoc Track Overview, also published in this volume.</p><p>The system's robustness ensures that all topics obtain minimum effectiveness levels. In information retrieval the mean of the average precision (MAP) is used to measure systems' performance. But, poorly performing topics have little influence on MAP. At TREC, geometric average (GMAP), rather than MAP, turned out to be the most stable evaluation method for robustness <ref type="bibr" coords="2,90.00,159.84,9.97,8.74" target="#b3">[4]</ref>. The GMAP has the desired effect of emphasizing scores close to 0.0 (the poor performers) while minimizing differences between higher scores. Nevertheless, at the CLEF 2006 Workshop the submitted runs showed high correlations between MAP and GMAP, so at CLEF 2007 other criteria of robustness have been suggested: MAP, GMAP, P@10, number of failed topics, number of topics bellow 0.1 MAP, and number of topics with P@10=0. In our experiments we have also considered other two user-related measurements: the Generalized Success@10 (GS@10) <ref type="bibr" coords="2,499.71,219.62,9.97,8.74" target="#b1">[2]</ref>, and the mean reciprocal rank (MRR). Both ones indicate the rank of the top retrieved relevant document.</p><p>Our main focus was monolingual retrieval. The steps followed are explained below. For bilingual retrieval experiments we used machine translation (MT) programs to translate topics into document language, and then we performed a monolingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>For the monolingual experiments we used the well-known vector space model, using the dnu-ntc term weighting scheme. For documents, letter u stands for the pivoted document normalization: we adjusted pivot to the average document length and slope set to 0.1 for all the collections. We decided to remove the terms present in more than 25 percent of documents. For the English and French languages we verified that stemming improve retrieval. Last year we saw that stemming does not deteriorate the retrieval performance of hard topics, so we also decided to apply stemming for the Portuguese language. For English we used the Porter stemmer, and for French and Portuguese the stemmers from the University of Neuchatel in the web page http://www.unine.ch/info/clef/. From the descriptions and narratives of the topics we automatically removed certain phrases such as "Find documents that . . . ", "Les documents pertinents relatent . . . " or "Encontrar documentos sobre . . . ".</p><p>At CLEF 2006 Robust Task our research group obtained very good results applying local query expansion using windows of terms <ref type="bibr" coords="2,240.91,477.62,9.96,8.74" target="#b5">[6]</ref>. This year we have used the same expansion technique, but taking into account the new criteria. This technique uses co-occurrence relations in windows of terms from the first retrieved documents to build a thesaurus to expand the original query. Our interest was to use sort and long queries in our experiments, i.e., use the title field of the topics for sort queries, and title and description fields for long ones. A lot of tests were carried out to obtain the best performance using the training collections, but we found no settings that improve retrieval for all measurements. Then we decided to select the settings that improve the greatest number of measurements for both sort and long queries. For English the highest improvement achieved with this expansion technique was by using a distance value of 1, taking the first 15 retrieved documents to build the thesauri, and adding about 10 terms to the original query. For French, the highest improvement achieved was by using a distance value of 1, taking the first 20 retrieved documents, and adding 40 terms to the original query.</p><p>For Portuguese we decided to use the best combination obtained last year for the Spanish experiments, due two reasons. First, the Portuguese language is more similar to Spanish than English or French are. Second, the average number of terms per sentence in the Portuguese collection is very similar to the Spanish one. We use a distance value of 2, taking the first 10 documents, and adding 30 terms to the original query.</p><p>For the bilingual experiments the CLIR system was the same as that used in monolingual retrieval. A previous step was carried out before searching, to translate English topics into French. We used three MT programs: L&amp;H Power Translator Pro 7.0, Systran<ref type="foot" coords="2,400.93,703.20,3.97,6.12" target="#foot_0">1</ref> and Reverso<ref type="foot" coords="2,462.84,703.20,3.97,6.12" target="#foot_1">2</ref> . For each topic we combined the terms of the translations in a single topic: this is another expansion process, although in most cases the three translations were identical. Finally, a monolingual retrieval was performed. The local query expansion using co-occurrence based thesauri built with terms windows was also applied.</p><p>For each subtask and topic language five runs were submitted for the test and training topics. The name of the run begins with "reina", follows the abbreviation of the language (EN, FR or PT for the monolingual runs, and E2F to indicate the English to French bilingual runs), follows the fields of the topics used in the run (t: title, td: title and description, tdn: title, description and narrative), follows with the letter "e" to indicate if expansion of terms was used and/or the letter "T" to indicate if the run is a test run. For example, the run "reinaENtdeT" stands for the test run submitted for the English collection using the title and descriptions fields of the topics, and applying term expansion. We send the "tdn" runs only for internal testing purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We only analyze results of our test runs, i.e., for the test topics of the robust task. Table <ref type="table" coords="3,508.02,717.16,4.98,8.74" target="#tab_0">1</ref> shows the results of the runs. We can see that term expansion no improves performance for all measurements.</p><p>At CLEF 2006 Robust Task our research group obtained very good results applying local query expansion using windows of terms in the robust task. This year at CLEF 2007 the results were disappointing. We think out that the reason is the difficulty to select the best measurement for robustness. Perhaps the problem is that all measurements are average results over all topics, but the hard topics are inherently hard and must be analyse separately. When a topic becomes hard depends on the document collection, the topic collection, the information retrieval system and the topic itself. Therefore general directives to improve performance of hard topics are difficult to suggest.</p><p>This year all our runs also ends up in good ranking, both base runs and expanded ones. We think that the reason is that we used a good information retrieval system, and the expansion technique is robust because it does not deteriorate significantly the retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,118.16,118.94,363.61,388.91"><head>Table 1 :</head><label>1</label><figDesc>Results of the runs submitted at CLEF 2007 Robust Task.</figDesc><table coords="3,118.16,134.33,363.61,373.52"><row><cell></cell><cell></cell><cell cols="2">Basis Expansion*</cell><cell cols="2">Basis Expansion*</cell><cell>Basis</cell></row><row><cell></cell><cell></cell><cell>t</cell><cell>t</cell><cell>td</cell><cell>td</cell><cell>tdn</cell></row><row><cell>English</cell><cell>MAP</cell><cell>0.3226</cell><cell>0.3205</cell><cell>0.3897</cell><cell>0.3855</cell><cell>0.3897</cell></row><row><cell></cell><cell>GMAP</cell><cell>0.1190</cell><cell>0.1045</cell><cell>0.1850</cell><cell>0.1762</cell><cell>0.1850</cell></row><row><cell>(*)Settings</cell><cell>MRR</cell><cell>0.5602</cell><cell>0.5379</cell><cell>0.6922</cell><cell>0.6792</cell><cell>0.6922</cell></row><row><cell cols="2">for expansion: GS@10</cell><cell>0.7613</cell><cell>0.7219</cell><cell>0.8506</cell><cell>0.8422</cell><cell>0.8506</cell></row><row><cell>distance=1</cell><cell>P@10</cell><cell>0.3200</cell><cell>0.3240</cell><cell>0.3620</cell><cell>0.3640</cell><cell>0.3620</cell></row><row><cell>docs=15</cell><cell># failed</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>terms=10</cell><cell># &lt;0.1 MAP</cell><cell>16</cell><cell>20</cell><cell>7</cell><cell>8</cell><cell>7</cell></row><row><cell></cell><cell># P@10=0</cell><cell>16</cell><cell>23</cell><cell>10</cell><cell>11</cell><cell>10</cell></row><row><cell>French</cell><cell>MAP</cell><cell>0.3382</cell><cell>0.3481</cell><cell>0.3773</cell><cell>0.3804</cell><cell>0.3773</cell></row><row><cell></cell><cell>GMAP</cell><cell>0.0940</cell><cell>0.0947</cell><cell>0.1289</cell><cell>0.1218</cell><cell>0.1289</cell></row><row><cell>(*)Settings</cell><cell>MRR</cell><cell>0.5749</cell><cell>0.5972</cell><cell>0.6564</cell><cell>0.6564</cell><cell>0.6564</cell></row><row><cell cols="2">for expansion: GS@10</cell><cell>0.7555</cell><cell>0.7445</cell><cell>0.7940</cell><cell>0.7959</cell><cell>0.7940</cell></row><row><cell>distance=1</cell><cell>P@10</cell><cell>0.3710</cell><cell>0.3740</cell><cell>0.4140</cell><cell>0.4280</cell><cell>0.4140</cell></row><row><cell>docs=20</cell><cell># failed</cell><cell>9</cell><cell>9</cell><cell>8</cell><cell>9</cell><cell>8</cell></row><row><cell>terms=40</cell><cell># &lt;0.1 MAP</cell><cell>18</cell><cell>19</cell><cell>12</cell><cell>12</cell><cell>12</cell></row><row><cell></cell><cell># P@10=0</cell><cell>23</cell><cell>24</cell><cell>19</cell><cell>18</cell><cell>19</cell></row><row><cell cols="2">Portuguese MAP</cell><cell>0.3387</cell><cell>0.3533</cell><cell>0.4083</cell><cell>0.4121</cell><cell>0.4140</cell></row><row><cell></cell><cell>GMAP</cell><cell>0.0825</cell><cell>0.0911</cell><cell>0.1369</cell><cell>0.1301</cell><cell>0.1287</cell></row><row><cell>(*)Settings</cell><cell>MRR</cell><cell>0.5711</cell><cell>0.5950</cell><cell>0.6286</cell><cell>0.6273</cell><cell>0.6419</cell></row><row><cell cols="2">for expansion: GS@10</cell><cell>0.7307</cell><cell>0.7277</cell><cell>0.7855</cell><cell>0.7718</cell><cell>0.7787</cell></row><row><cell>distance=2</cell><cell>P@10</cell><cell>0.3013</cell><cell>0.3027</cell><cell>0.3320</cell><cell>0.3347</cell><cell>0.3360</cell></row><row><cell>docs=10</cell><cell># failed</cell><cell>15</cell><cell>12</cell><cell>10</cell><cell>10</cell><cell>11</cell></row><row><cell>terms=30</cell><cell># &lt;0.1 MAP</cell><cell>28</cell><cell>29</cell><cell>22</cell><cell>26</cell><cell>23</cell></row><row><cell></cell><cell># P@10=0</cell><cell>36</cell><cell>39</cell><cell>29</cell><cell>30</cell><cell>30</cell></row><row><cell>EN → FR</cell><cell>MAP</cell><cell>0.3035</cell><cell>0.3278</cell><cell>0.3385</cell><cell>0.3455</cell><cell>0.3583</cell></row><row><cell></cell><cell>GMAP</cell><cell>0.0821</cell><cell>0.0872</cell><cell>0.1005</cell><cell>0.0997</cell><cell>0.1228</cell></row><row><cell>(*)Settings</cell><cell>MRR</cell><cell>0.5819</cell><cell>0.6084</cell><cell>0.6219</cell><cell>0.6164</cell><cell>0.6794</cell></row><row><cell cols="2">for expansion: GS@10</cell><cell>0.7555</cell><cell>0.7580</cell><cell>0.7833</cell><cell>0.7769</cell><cell>0.8096</cell></row><row><cell>distance=1</cell><cell>P@10</cell><cell>0.3242</cell><cell>0.3535</cell><cell>0.3770</cell><cell>0.3870</cell><cell>0.3830</cell></row><row><cell>docs=20</cell><cell># failed</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>8</cell></row><row><cell>terms=40</cell><cell># &lt;0.1 MAP</cell><cell>16</cell><cell>16</cell><cell>15</cell><cell>14</cell><cell>11</cell></row><row><cell></cell><cell># P@10=0</cell><cell>22</cell><cell>20</cell><cell>19</cell><cell>18</cell><cell>16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,736.23,110.08,6.64"><p>http://www.systransoft.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,745.73,93.15,6.64"><p>http://www.reverso.net</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.50,308.16,407.50,8.74;4,105.50,320.11,138.37,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,351.06,308.16,157.45,8.74">CLEF 2006: Ad hoc track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,105.50,320.11,48.94,8.74">CLEF 2006</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,340.04,407.50,8.74;4,105.50,352.00,407.51,8.74;4,105.50,363.95,407.50,8.74;4,105.50,375.91,72.06,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,175.76,340.04,332.55,8.74">Comparing the robustness of expansion techniques and retrieval measures</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,324.75,352.00,164.16,8.74">ABSTRACTS CLEF 2006 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Vicedo</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">20-22 September. 2006</date>
		</imprint>
	</monogr>
	<note>Results of the CLEF 2006 Cross-Language System Evaluation Campaign</note>
</biblStruct>

<biblStruct coords="4,105.50,395.83,407.50,8.74;4,105.50,407.79,397.56,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,185.02,395.83,225.78,8.74">Overview of the TREC 2003 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,435.95,395.83,77.05,8.74;4,105.50,407.79,155.11,8.74">The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,427.71,407.50,8.74;4,105.50,439.67,407.50,8.74;4,105.50,451.62,115.27,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,183.71,427.71,223.93,8.74">Overview of the TREC 2004 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,431.73,427.71,81.28,8.74;4,105.50,439.67,155.34,8.74">The Thirteen Text REtrieval Conference (TREC 2004)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2004">November 16-19. 2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,471.55,407.50,8.74;4,105.50,483.50,402.28,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,181.11,471.55,220.27,8.74">Overview of the TREC 2005 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,423.39,471.55,89.61,8.74;4,105.50,483.50,155.36,8.74">The Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2005">November 15-18. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.50,503.43,407.49,8.74;4,105.50,515.38,305.46,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,359.39,503.43,153.60,8.74;4,105.50,515.38,121.18,8.74">Local query expansion using terms windows for robust retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Alonso Berrocal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Figuerola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,234.95,515.38,48.94,8.74">CLEF 2006</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
