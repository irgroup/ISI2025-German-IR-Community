<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,101.91,154.00,67.10,9.96"><forename type="first">Debasis</forename><surname>Mandal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.44,154.00,84.47,9.96"><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.33,154.00,63.35,9.96"><forename type="first">Mayank</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.09,154.00,78.91,9.96"><forename type="first">Pratyush</forename><surname>Banerjee</surname></persName>
							<email>pratyushb@gmail.comsandipan</email>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.84,154.00,72.36,9.96"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
							<email>sudeshna@cse.iitkgp.ernet.in</email>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE1DF19A52D81EF9112236C04DFC3A88</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries.</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Bengali, Hindi, Transliteration, Cross-language Text Retrieval, CLEF Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our experiment on two cross-lingual and one monolingual English text retrievals at CLEF 1 in the ad-hoc track. The cross-language task includes the retrieval of English documents in response to queries in two most widely spoken Indian languages, Hindi and Bengali. For our experiment, we had access to a Hindi-English bilingual lexicon, 'Shabdanjali', consisting of approx. 26K Hindi words. But neither we had any effective Bengali-English bilingual lexicon nor any parallel corpora to build the statistical lexicon. Under this limited resources, we mostly depended on our phoneme-based transliterations to generate equivalent English query from Hindi and Bengali topics. We adopted Automatic Query Generation and Machine Translation approach for our experiment. Other language-specific resources included a Bengali morphological analyzer, a Hindi stemmer and a set of 200 Hindi and 273 Bengali stopwords. Lucene framework was used for stemming, indexing, retrieval and scoring of the corpus documents. The CLEF results suggested the need for a rich bilingual lexicon for CLIR involving Indian languages. The best MAP values for Bengali, Hindi and English queries for our experiment were 7.26, 4.77 and 36.49 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-language (or cross-lingual) Information Retrieval (CLIR) involves the study of retrieving the documents in a language other than the query language. Since the language of query and documents to be retrieved are different, either the documents or queries need to be translated in CLIR. But this translation step tends to cause a reduction in the retrieval performance of CLIR as compared to monolingual information retrieval. A study in <ref type="bibr" coords="2,406.99,61.36,10.51,9.96" target="#b0">[1]</ref> showed that missing specialized vocabulary, missing general terms, wrong translation due to ambiguity and correct identical translation are the four most important factors for the difference in performance for over 70% queries between monolingual and cross-lingual retrievals. This puts the importance on effective translation in CLIR research. Again, the document translation requires a lot of memory and processing capacity than its counterpart and therefore the query translation is more popular in the IR research community involving multiple languages <ref type="bibr" coords="2,343.36,133.08,12.78,9.96" target="#b4">[5]</ref>.</p><p>Oard <ref type="bibr" coords="2,129.96,145.04,10.51,9.96" target="#b6">[7]</ref> presents an overview of the Controlled Vocabulary and Free Text retrieval approaches followed in CLIR research within the query translation framework. But the present research in CLIR are mainly concentrated around three approaches: Dictionary based Machine Translation (MT), Parallel Corpora based statistical lexicon and Ontology-based methods. The basic idea in Machine Translation is to replace each term in the query with an appropriate term or a set of terms from the lexicon. In current MT systems the quality of translations is very low and the high quality is achieved only when the application is domain-specific <ref type="bibr" coords="2,373.27,216.77,9.96,9.96" target="#b4">[5]</ref>. The Parallel Corpora-based method utilizes the broad repository of multi-lingual corpora to build the statistical lexicon from the simliar training data as of the target collection. Knowledge-based approaches use ontology or thesauri to replace the source language word by all of its target language equivalents. Some of the CLIR models built on these approaches or on their hybrids can be found in <ref type="bibr" coords="2,422.47,264.59,10.65,9.96" target="#b4">[5]</ref>[6] <ref type="bibr" coords="2,443.77,264.59,24.86,9.96">[8][10]</ref>.</p><p>This paper presents two cross-lingual and one English monolingual text retrieval. The crosslanguage task includes English document retrieval in response to queries in two Indian languages: Hindi and Bengali. Although Hindi is mostly spoken in north India and Bengali in the Eastern India and Bangladesh only, the former is the fifth most widely spoken language in the world and Bengali the seventh. This requires attention on CLIR involving these languages. In this paper, we restrict ourselves to Cross-Language text retrieval applying Machine Translation approach.</p><p>The rest of the paper is structured as follows. Section 2 briefly presents some of the works on CLIR involving Indian languages. The next section provides the language specific and open source resources used for our experiment. Section 4 builds our CLIR model on the resources and explains our approach. CLEF evaluations of our results and their discussions are presented in the subsequent section. We conclude this paper with a set of inferences and scope of future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cross-language retrieval is a budding field in India and the works are still in its primitive state. The first major work involving Hindi occurred during TIDES Surprise Language exercise in a one month period. The objective of the exercise was to retrieve Hindi documents, provided by LDC (Linguistic Data Consortium), in response to English queries. The participants used parallel corpora based approach to build the statistical lexicon <ref type="bibr" coords="2,328.77,498.68,10.71,9.96" target="#b2">[3]</ref>[4] <ref type="bibr" coords="2,350.19,498.68,14.28,9.96" target="#b11">[12]</ref>. <ref type="bibr" coords="2,372.43,498.68,10.51,9.96" target="#b3">[4]</ref> assigned statistical weightage on query and expansion terms using the training corpora and this improved their cross-lingual results over monolingual runs. <ref type="bibr" coords="2,227.70,522.60,21.02,9.96">[3][9]</ref> indicated some of the language-specific obstacles for Indian languages, viz., propritary encodings of much of the web text, lack of availability of parallel corpora, variability in Unicode encoding etc. But all of these works were the reverse of our problem statement for CLEF. The related work of Hindi-English retrieval can be found in <ref type="bibr" coords="2,486.03,558.46,9.96,9.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resources used</head><p>We used various language specific resources and open source tools for our Cross Language Infomation Retrieval (CLIR) experiments. For the processing of English query and corpus, we used the stop-word list (33 words) and porter stemmer of Lucene framework. For Bengali query, a Bengali-English transliteration (ITRANS) tool<ref type="foot" coords="2,296.19,648.24,3.97,4.84" target="#foot_1">2</ref>  <ref type="bibr" coords="2,304.92,649.09,14.60,9.96" target="#b10">[11]</ref>, a set of Bengali stop-words 3 (273 words), an open source Bengali-English bio-chemical lexicon ( 9k Bengali words) and a Bengali morphological analyzer of moderate performance were used. Hindi language specific resources included a Hindi-English Transliteration tool (wx and ITRANS), a Hindi stop-word list of 200 words, a Hindi-English bilingual lexicon 'Shabdanjali' containing approximately 26K Hindi words and a Hindi Stemmer<ref type="foot" coords="3,156.49,108.33,3.97,4.84" target="#foot_3">4</ref> . We also manually built a named entity list of 1510 entries mainly drawn from the names of countries and cities, abbreviations, companies, medical terms, rivers, seven wonders, global awards, tourist spots, diseases, events of 2002 from wiki etc. Finally, the open source Lucene framework was used for indexing and retrieval of the documents with their corresponding scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Model</head><p>The objective of Ad-Hoc Bilingual (X2EN) and Monolingual English tasks was to retrieve the relevant documents from English target collection and submit the results in ranked order. The topic sets for these two tasks consist of 50 topics and the participant is asked to retrieve at least 1000 documents from the corpus per query for each of the source languages. Each topic consists of three fields: a brief 'title', almost equivalent to a query provided by the end-user to a search engine; a one-sentence 'description', specifying more accurately what kind of documents the user is looking for from the search and a 'narrative' for relevance judgements, describing what is relevant to the the topic and what is not. Our approach to the problem can be broken into 3 phases: corpus processing, query generation and document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus Processing</head><p>The English news corpus of LA Times 2002, provided by CLEF, contained 1,35,153 documents of 433.5 MB size. After removing stop words and stemming the documents, they were indexed using the Lucene indexer to obtain the index terms corresponding to the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Generation</head><p>We adopted Automatic Query Generation method to immitate the possible application of CLIR on the web. The language-specific stop-words were first removed from the topics. To remove the most frequent suffixes, we used a morphological analyzer for Bengali, a stemmer for Hindi and the Lucene stemmer for English topics. We considered all possible stems for a single term as no training data was available to pick the most relevant stem. This constitutes the final query for English monolingual run. For Indian languages, the stemmed terms were then looked up in the bilingual lexicon for their translations into English. All the translations for the term were used for the query generation (Structured Query Translation), if the term was found in the lexicon. But many terms did not occur in the lexicon due to its limitation in size or the improper stemming or as the term is a named entity <ref type="bibr" coords="3,243.21,519.51,9.96,9.96" target="#b1">[2]</ref>. Those terms were first transliterated into ITRANS and then matched against the named entity list with the help of an approximate string matching algorithm, edit-distance algorithm. The algorithm returns the best match of the term for the pentagram statistics. This produces the final query terms for cross-lingual runs. The queries were constructed from the topics consisting of one or more of the topic fields.</p><p>Note that we did not expand the query using the Pseudo Relevance Feedback (PRF). This is due to the fact that it does not improve the retrieval significantly for CLIR, rather hurts by increasing noise <ref type="bibr" coords="3,161.54,603.19,14.60,9.96" target="#b12">[13]</ref>, or increases queries in which no relevant documents are returned <ref type="bibr" coords="3,469.13,603.19,9.96,9.96" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Retrieval</head><p>The query generated in the above phase is fed into Lucene search engine and the documents were retrieved along with their normalized scores. Lucene scorer follows the Vector Space Model (VSM) of Information Retrieval.</p><p>The evaluation document set for the ad-hoc bilingual and monolingual tracks consists of 1,35,153 documents from LA Times 2002. For the 50 topics originally provided by CLEF, there were manually selected 2247 relevant documents which were matched against the retrieved documents of the participants. We provided the set of 50 queries to the system for each run of our experiments. Six official runs were submitted for the Indian langauges to English bilingual retrieval, three for Hindi queries and three for Bengali queries. Three monolingual English runs were also submitted to compare the results between bilingual and monolingul retrievals. The runs were performed using only &lt;title&gt; field, &lt;title+desc&gt; fields and &lt;title+desc+narr&gt; fields per topic for each of these languages. The performance metrics for the nine runs of our experiments are presented in the following tables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Discussions</head><p>Table <ref type="table" coords="4,118.93,472.09,4.98,9.96" target="#tab_0">1</ref> presents four basic primary metrics for CLIR, viz., MAP (Mean Average Precision), GMAP (Geometric Mean Average Precision), B-Preference and Precision at 10 retrieved documents (P@10) for all of our official runs. The lower values of the GMAP corresponding to MAP clearly specifies the poor performance of our retrievals in the lower end of the average precision scale. Also, lower values of the MAP for Hindi than the work of <ref type="bibr" coords="4,387.48,519.91,10.51,9.96" target="#b1">[2]</ref> clearly suggests the need for query expansion at the source language end. It is evident from the monolingual English and bilingual Bengali runs that adding extra information to query through &lt;title+desc&gt; increases the performance of the system. But adding the &lt;narr&gt; field has not improved the result significantly. This is probably due to the fact that this field was meant for the relevance judgement in the retrieval and we have not made any effort in preventing the retrieval of irrelevant documents in our IR model. This, in turn, has also affected the MAP value for all the runs. However, the improvement in the result for &lt;title+desc&gt; run over &lt;title&gt; run is not significant for Hindi. This is probably due to the fact that using Structured Query Translation (SQT) increased too much noise in the query to compensate the effect of a better lexicon. Also, we used morphological analyzer for bengali rather than stemmer (for hindi) which was suggested by <ref type="bibr" coords="4,368.21,639.46,10.51,9.96" target="#b1">[2]</ref> and this may have contributed to the better result for Bengali. Table <ref type="table" coords="4,132.42,663.37,4.98,9.96" target="#tab_1">2</ref> shows the results of the topicwise score breakup for the relevant 2247 documents. As seen from the table, number of failed topics (with no relevant retrieval) and topics with MAP ≤ 10% gradually decreased with more fields from the topic, thus establishing the fact again mentioned in the earlier paragraph. Also, the better result for Hindi than Bengali is clearly attributed to its better lexicon. But when it comes to the number of topics with MAP ≥ 50%, Bengali clearly outperforms Hindi due to the ńoise factor , mentioned in the previous paragraph.</p><p>A careful analysis of the queries revealed that the queries with named entities provided better results for all the runs, whereas the queries without named entities performed very poor due to poor bilingual lexicons and thus brininging down the overall performance metrics. This clearly implies the importance of a very good bilingual lexicon and transliteration tool in the CLIR for Indian languages. Recall is a very important performance metric for CLIR specifically for the case when the number of relevant documents is significantly low compared to the target collection (in this case, it is 1.66% only). It is noteworthy that the recall value has improved even for the &lt;title+desc+narr&gt; field compared to other runs and Bengali has again outperformed Hindi due to ńoise factor .</p><p>Figure <ref type="figure" coords="5,124.53,631.12,3.87,9.96">1</ref>: Recall Vs Precision for Bengali and Hindi to English Cross-language runs for &lt;Title+Desc&gt; fields.</p><p>The recall vs average precision graphs in Figure <ref type="figure" coords="5,313.58,664.58,4.98,9.96">1</ref> and retrieved documents vs precision graphs in Figure <ref type="figure" coords="5,134.76,676.53,4.98,9.96" target="#fig_0">2</ref> suggest the need for refinement of important query terms (e.g. named entity) and weigh them more than the translated terms. Also, we used Structured Query Translation and assigned uniform weight on all of them. But this has affected the precision values for some queries even when the recall is significantly high. Again, we used all possible stems for a term when multiple stems are possible and this has also added to the lower precision values for some queries. A proper named entity recognizer is also important to prune out the named entities from other non-lexical terms. All of these will decrease the noise in the final query and thereby help the lucene ranking algorithm to push the relevant documents to the top. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Works</head><p>This was our first participation in CLEF and we performed our experiment under a limited resource scenario with a very basic Machine Translation approach. But the experiment pointed out the necessity of good language-specific resources, specifically a rich bilingual lexicon. A close analysis between cross-lingual and monolingual retrievals clarly pointed out the importance of four factors in CLIR, mentiond earlier <ref type="bibr" coords="6,206.52,659.11,9.96,9.96" target="#b0">[1]</ref>. Apart from the above language-specific requirements, a number of good computational approaches like query expansion by Pseudo Relevance Feedback (PRF) both at the source and target language ends, query refinement by assigning various weightage to query terms, proper stemming, irrelevance judgement to improve ranking, parallel corpus to build the statistical lexicon, named entity recognizer to prune them out of non-lexical terms, Multi-word Expression (MWE) detection, Word Sense disambiguation to avoid multiple translations (SQT) will also increase the performance of the system. Also, pruning out the irrelevant documents from retrieveing will increase the precision of the results. We will make attempt to experiment and verify their effects in CLIR involving Indian languages in our future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,90.00,245.31,423.00,9.96;6,90.00,257.26,108.20,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Retrieved documents Vs Precision for Bengali and Hindi to English Cross-language runs for &lt;Title+Desc&gt; fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,90.00,534.41,423.01,9.96;6,90.00,546.37,108.20,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recall Vs Precision and Retrieved documents Vs Precision monolingual English runs for &lt;Title+Desc&gt; fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,164.85,222.10,285.28,195.39"><head>Table 1 :</head><label>1</label><figDesc>Primary Metrics (in %) for the Official Runs.</figDesc><table coords="4,164.85,248.75,285.28,168.73"><row><cell>Lang</cell><cell>Run</cell><cell cols="4">MAP GMAP B-Pref P@10</cell></row><row><cell>Bengali</cell><cell>&lt;title&gt;</cell><cell>4.98</cell><cell>0.11</cell><cell>5.43</cell><cell>6.60</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>7.26</cell><cell>0.50</cell><cell>10.38</cell><cell>10.20</cell></row><row><cell></cell><cell cols="2">&lt;title+desc+narr&gt; 7.19</cell><cell>0.57</cell><cell>11.21</cell><cell>10.80</cell></row><row><cell>Hindi</cell><cell>&lt;title&gt;</cell><cell>4.77</cell><cell>0.21</cell><cell>9.95</cell><cell>6.40</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>4.39</cell><cell>0.32</cell><cell>11.58</cell><cell>8.60</cell></row><row><cell></cell><cell cols="2">&lt;title+desc+narr&gt; 4.77</cell><cell>0.34</cell><cell>12.02</cell><cell>8.40</cell></row><row><cell>English</cell><cell>&lt;title&gt;</cell><cell>30.56</cell><cell>19.51</cell><cell>29.51</cell><cell>37.80</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>36.49</cell><cell>27.34</cell><cell>34.54</cell><cell>46.00</cell></row><row><cell></cell><cell cols="2">&lt;title+desc+narr&gt; 36.12</cell><cell>23.51</cell><cell>35.65</cell><cell>45.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,120.00,68.27,374.95,194.84"><head>Table 2 :</head><label>2</label><figDesc>Result of Querywise Score breakup.</figDesc><table coords="5,120.00,94.37,374.95,168.74"><row><cell>Language</cell><cell>Run</cell><cell cols="4">Failed MAP≤ 10% MAP≥ 50% Recall (in %)</cell></row><row><cell>Bengali</cell><cell>&lt;title&gt;</cell><cell>14</cell><cell>22</cell><cell>19</cell><cell>27.06</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>8</cell><cell>13</cell><cell>22</cell><cell>37.87</cell></row><row><cell></cell><cell>&lt;title+desc+narr&gt;</cell><cell>5</cell><cell>14</cell><cell>23</cell><cell>40.32</cell></row><row><cell>Hindi</cell><cell>&lt;title&gt;</cell><cell>10</cell><cell>18</cell><cell>20</cell><cell>31.51</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>6</cell><cell>14</cell><cell>18</cell><cell>30.57</cell></row><row><cell></cell><cell>&lt;title+desc+narr&gt;</cell><cell>4</cell><cell>11</cell><cell>17</cell><cell>30.97</cell></row><row><cell>English</cell><cell>&lt;title&gt;</cell><cell>0</cell><cell>1</cell><cell>43</cell><cell>72.54</cell></row><row><cell></cell><cell>&lt;title+desc&gt;</cell><cell>0</cell><cell>0</cell><cell>46</cell><cell>78.95</cell></row><row><cell></cell><cell>&lt;title+desc+narr&gt;</cell><cell>0</cell><cell>0</cell><cell>45</cell><cell>78.19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.25,688.58,225.54,7.97"><p>Cross Language Evaluation Forum. http://clef-campaign.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.25,668.71,407.69,7.97;2,90.00,678.17,199.19,7.97"><p>ITRANS is an encoding standared specifically for Indian languages. It converts the Indian language letters into Roman (English) mostly using its phoneme structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.25,687.67,205.31,7.97"><p>The list was provided by Jadavpur University, Kolkata.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.25,690.51,255.24,7.97"><p>'Shabdanjali' and the Hindi stemmer were built by IIIT, Hyderabad.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,163.94,402.53,9.96;7,110.47,175.90,179.21,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,199.30,163.94,265.82,9.96">Translation Events in Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Anna</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,489.79,163.94,23.22,9.96;7,110.47,175.90,57.91,9.96">ACM SIGIR Forum</title>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,195.82,402.52,9.96;7,110.47,207.78,393.27,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,277.04,195.82,235.95,9.96;7,110.47,207.78,123.35,9.96">Hindi and Telegu to English Cross Language Information Retrieval at CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.78,207.78,186.14,9.96">Cross Language Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,227.70,402.52,9.96;7,110.47,239.65,395.96,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,388.57,227.70,119.58,9.96">Hindi CLIR in Thirty Days</title>
		<author>
			<persName coords=""><forename type="first">Leah</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasreen</forename><surname>Abduljaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,123.49,239.65,272.33,9.96">ACM Transactions on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="142" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,259.58,402.53,9.96;7,110.48,271.53,345.03,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,247.68,259.58,144.65,9.96">Cross-Lingual Retrieval for Hindi</title>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,415.65,259.58,97.36,9.96;7,110.48,271.53,172.11,9.96">ACM Transactions on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,291.46,402.51,9.96;7,110.48,303.42,402.52,9.96;7,110.48,315.37,402.56,9.96;7,110.48,327.32,26.00,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,232.50,291.46,280.49,9.96;7,110.48,303.42,124.62,9.96">Querying across languages: A dictionary-based approach to multilingual informaion retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,257.32,303.42,255.68,9.96;7,110.48,315.37,289.64,9.96">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information retrieval<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,347.25,402.50,9.96;7,110.48,359.21,147.26,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,368.62,347.25,139.53,9.96">Cross-Lingual Relevance Models</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Victor Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,123.49,359.21,39.34,9.96">SIGIR&apos;02</title>
		<imprint>
			<date type="published" when="2002">August 11-15, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,379.14,402.52,9.96;7,110.48,391.09,402.53,9.96;7,110.48,403.04,109.21,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,200.20,379.14,260.49,9.96">Alternative Approaches for Cross-Language Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,486.94,379.14,26.06,9.96;7,110.48,391.09,244.49,9.96">AAAI Symposium on Cross-Language Text and Speech Retrieval</title>
		<imprint>
			<publisher>American Association for Artificial Intelligence</publisher>
			<date type="published" when="1997-03">March 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,422.97,402.51,9.96;7,110.48,434.93,290.76,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,326.07,422.97,186.92,9.96;7,110.48,434.93,125.26,9.96">Evaluating a Probablistic Model for Crosslingual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chanh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,257.34,434.93,39.34,9.96">SIGIR&apos;01</title>
		<imprint>
			<date type="published" when="2001">September 9-12, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,454.85,402.51,9.96;7,110.48,466.81,402.52,9.96;7,110.48,478.76,52.02,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,394.35,454.85,118.64,9.96;7,110.48,466.81,167.44,9.96">Webkhoj: Indian language IR from Multiple Character Encodings</title>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,300.25,466.81,185.64,9.96">Internatioanl World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2006">May 23-26, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,498.68,402.52,9.96;7,110.48,510.64,143.18,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,277.68,498.68,216.86,9.96">Resolving Ambiguity for Cross-language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.48,510.64,39.34,9.96">SIGIR&apos;98</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,530.56,171.63,9.96" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Avinash</forename><surname>Chopde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,189.97,530.56,72.45,9.96">ITRANS version</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,550.49,402.52,9.96;7,110.48,562.45,402.54,9.96;7,110.48,574.40,74.16,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,366.37,550.49,146.63,9.96;7,110.48,562.45,75.61,9.96">A Month to Topic Detection and Tracking in Hindi</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,208.62,562.45,217.45,9.96">ACM Transactions on Asian Language Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,594.33,402.57,9.96;7,110.48,606.28,142.06,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,276.27,594.33,213.20,9.96">Measuring Pseudo Relevance Feedback &amp; CLIR</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.48,606.28,38.56,9.96">SIGIR&apos;04</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">July 25-29, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
