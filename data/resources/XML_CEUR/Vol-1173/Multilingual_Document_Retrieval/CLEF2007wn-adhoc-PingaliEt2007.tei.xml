<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.25,148.86,396.51,15.15;1,213.90,170.78,175.20,15.15">IIIT Hyderabad at CLEF 2007 -Adhoc Indian Language CLIR task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.72,204.67,64.01,8.74"><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Centre IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.42,204.67,72.87,8.74"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Research Centre IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.25,148.86,396.51,15.15;1,213.90,170.78,175.20,15.15">IIIT Hyderabad at CLEF 2007 -Adhoc Indian Language CLIR task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A19982CA8366DDC26ECEF6DD6C85B7F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Ad-hoc cross language text retrieval, Indian languages, Hindi, Telugu</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the experiments of Language Technologies Research Centre (LTRC) 1 as part of their participation in CLEF 2 2007 Indian language to English ad-hoc cross language document retrieval task. In this paper we discuss our Hindi and Telugu to English CLIR system and the experiments using CLEF 2007 dataset. We used a variant of TFIDF algorithm in combination with a bilingual lexicon for query translation. We also explored the role of a document summary in fielded queries and two different boolean formulations of query translations. We find that a hybrid boolean formulation using a combination of boolean AND and boolean OR operators improves ranking of documents. We also find that simple disjunctive combination of translated query keywords results in maximum recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-language information retrieval (CLIR) research involves the study of systems that accept queries (or information needs) in one language and return objects of a different language. These objects could be text documents, passages, images, audio or video documents. Cross-language information retrieval focused on the cross-language issues from information retrieval (IR) perspective rather than the machine translation (MT) perspective. The motivation for a separate research into such systems was that CLIR was not merely coupling of IR and MT, and a lot of processing usually performed in machine translation systems may not be necessary for CLIR. Also on the other hand, machine translation systems rely on syntactically well formed sentences as input to the system, which may not be a realistic assumption for an IR system, as most of the IR queries tend to be very short and many times without any syntactic correctness and hence very little context to perform syntactic parsing or disambiguate automatically. However, some times keyword based queries might also contain valid phrases which could be the level of language syntax one could rely on for CLIR systems. Some of the key technical issues <ref type="bibr" coords="2,247.63,159.84,10.52,8.74" target="#b2">[3]</ref> for cross language information retrieval can be thought of as</p><p>• How can a query term in L 1 be expressed in L 2 ?</p><p>• What mechanisms determine which of the possible translations of text from L 1 to L 2 should be retained?</p><p>• In cases where more than one translation are retained, how can different translation alternatives be weighed?</p><p>In order to address these issues, many different techniques were tried in various CLIR systems in the past. These techniques can be broadly classified <ref type="bibr" coords="2,341.48,287.36,10.52,8.74" target="#b4">[5]</ref> as controlled vocabulary based and free text based systems at a very high level. However, it is very difficult to create, maintain and scale a controlled vocabulary for CLIR systems in a general domain for a large corpus. Therefore very quickly researchers realized it would be essential to come up with models that can be built from the full text of the corpus. The free text based system research can be broadly classified on the corpus-based and knowledge-based aspects. This classification comes from the type of information resources used by the CLIR systems in order to address the above mentioned issues. For example, knowledge based systems might use bilingual dictionaries or ontologies which form the hand-crafted knowledge readily available for the systems to use. On the other hand corpusbased systems may use parallel or comparable corpora which are aligned at word level, sentence level or passage level to learn models automatically. Hybrid systems were also built combining the knowledge based and corpus based approaches. Apart from these approaches, the extension of monolingual IR techniques such as vector based models, relevance modeling techniques <ref type="bibr" coords="2,481.14,430.82,10.52,8.74" target="#b3">[4]</ref> etc., to cross language IR were also explored.</p><p>In this paper we discuss our experiments on CLIR for Indian languages to English, where the queries are in Indian languages and the documents to be retrieved are in English. Experiments were conducted using queries in two Indian languages using the CLEF 2007 experimental setup. The two languages chosen were Hindi which is predominantly spoken in north India and Telugu which is predominantly spoken in southern part of India. In the rest of the paper we discuss CLIR and related work in these Indian languages and also our own experiments at CLEF 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Very little work has been done in the past in the areas of IR and CLIR involving Indian languages. We participated in CLEF 2006 Indian language to English CLIR task, which is to the best of our knowledge, the first evaluation of CLIR where queries were provided in Hindi and Telugu and the documents to be retrieved were in English. In the year 2003 a surprise language exercise <ref type="bibr" coords="2,90.00,617.10,10.52,8.74" target="#b5">[6]</ref> was conducted at ACM TALIP<ref type="foot" coords="2,243.80,615.52,3.97,6.12" target="#foot_2">3</ref> . The task was to build CLIR systems for English to Hindi and Cebuano, where the queries were in English and the documents were in Hindi and Cebuano. Five teams participated in this evaluation task at ACM TALIP providing some insights into the issues involved in processing Indian language content. A few other information access systems were built apart from this task such as cross language Hindi headline generation <ref type="bibr" coords="2,450.74,664.92,9.96,8.74" target="#b1">[2]</ref>, English to Hindi question answering system <ref type="bibr" coords="2,238.06,676.87,15.50,8.74" target="#b9">[10]</ref> etc. We previously built a monolingual web search engine for various Indian languages which is capable of retrieving information from multiple character encodings <ref type="bibr" coords="2,135.55,700.78,9.97,8.74" target="#b6">[7]</ref>. However, no work was found related to CLIR involving Telugu or any other Indian language other than Hindi. Some research was previously done in the areas of machine translation involving Indian languages <ref type="bibr" coords="3,122.38,123.98,9.97,8.74" target="#b0">[1]</ref>. Most of the Indian language MT efforts involve studies on translating various Indian languages amongst themselves or translating English into Indian language content. Hence most of the Indian language resources available for our work are largely biased to these tasks. This led to the challenge of using resources which enabled translation from English to Indian languages for a task involving translation from Indian languages to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>The problem statement of CLIR task discussed in this paper is as defined in the ad-hoc track of CLEF 2007. The ad-hoc track tests mono-and cross-language textual document retrieval. The bilingual task on target collections in English would test systems where the topics are supplied in a variety of languages including Amharic, Afaan Oromo, Hindi, Telugu, Bengali, Marathi, Tamil and Indonesian. In this paper we discuss our system for Hindi and Telugu languages therefore the system will be provided with a set of 50 topics in Hindi and Telugu where each topic represents an information need for which English text documents need to be retrieved and ranked. An example topic in Telugu would look as shown below.</p><p>Each topic comes with a unique number identifying the topic, a title, a description and a narrative. A title is typically a few words in length and is characteristic of a real world IR query. The description of a topic contains more detailed description of what the user is looking for, as a natural language statement. A narrative contains a little more information than the description in the sense that it also give additional information of what is relevant and what is not relevant. Such information would be very useful for systems which use both relevance as well as irrelevance information into their models. The system should use these topics as input or manually a set of keywords can be generated by a human and provided to the system. In this paper we restrict our problem to automatically retrieving the relevant documents with the input topics. The system is expected to provide an output of 1000 documents for each topic in a ranked order which are evaluated against a set of manually created relevance judgements. The possible judgements for each retrieved documents could either be relevant or irrelevant. In other words the relevance judgements are binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>Our submission to CLEF 2007 uses a vector based ranking model with bilingual lexicon using word translations. Out of vocabulary words or OOVs are handled using a probabilistic algorithm as mentioned in <ref type="bibr" coords="3,166.55,556.30,9.97,8.74" target="#b7">[8]</ref>. Document retrieval is achieved using an extended boolean model where queries are constructed using boolean operators among keywords and the occurrence of keywords in various types of metadata is given a different weight. The various fields that were used while retrieving the documents are mentioned in section 4.2. The ranking is achieved using a vector based ranking model using a variant of TFIDF ranking algorithm. We used the lucene framework to index the English documents. All the English documents were stemmed and stop words were eliminated to obtain the index terms. These terms were indexed using the Lucene<ref type="foot" coords="3,447.35,626.46,3.97,6.12" target="#foot_3">4</ref> search engine using the TFIDF similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Query Translation</head><p>A query translation need not be a true translation of the given source language query. In other words, the target language output produced need not be well formed and human readable and the only goal of such a translation is to obtain a topically similar translation in the target language to enable proper retrieval. In our system, a given source language query is translated using word by word translation. However, the design of our system does support phrasal or multi-word expression lookup as well. Each source language word is looked up in the bilingual dictionaries for exact match as well as all words having the same prefix as the given source language word. If a given source language word does not occur in the bilingual dictionary, one character at each time is removed from the end of the word until a matching word with same prefix is found in the dictionary. This heuristic for dictionary lookup helps in translating source language words even if their morphological variants or compound words are present in the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dictionaries</head><p>From our CLEF 2006 experiments <ref type="bibr" coords="4,245.54,227.85,10.52,8.74" target="#b8">[9]</ref> we found that, using existing dictionaries which are built from human readable dictionaries may not yield good results in a CLIR task. We therefore created a new dictionary for Telugu-English and Hindi-English language pairs. While creating the dictionaries, we used the TFIDF measure of a Telugu or Hindi word from a large corpus collected from a monolingual web search engine <ref type="bibr" coords="4,258.68,275.68,9.96,8.74" target="#b6">[7]</ref>. The words having higher TFIDF would be chosen first while manually creating entries into the dictionary. The motivation for such a choice is based on our findings that dictionaries are never complete and the optimal way of building a new dictionary would be by maximizing the probability of finding the most appropriate concepts for a given cross language query word. A group of about five native language speakers of Telugu and Hindi with a reasonably good knowledge of English were chosen to create the dictionaries. The task was defined as to quickly create bilingual dictionaries for as many source language words as possible in seven days. We created a Hindi-English dictionary with 5,175 entries and Telugu-English dictionary with 26,182 entries. These entries might contain variants of a same source language word, since we do not make any effort to construct only a root word dictionary. The words chosen are as-is from the index of a web-search engine, ordered by their TFIDF scores. The guidelines given to the dictionary creators while creating the dictionaries were • To key-in utmost five English keywords for each source language word • To key-in entries only for the words where the creator's confidence level was very high</p><p>• To choose words in such a way that they would be topically related to the source language word and need not be an exact synonym</p><p>The third guideline mentioned above was found to help the most in building highly suitable dictionaries for a CLIR task. The idea was to assume a user giving a source language keyword for searching and imagine the most likely keywords found in a relevant target language document. In other words the created dictionary may not be an exact synonym dictionary, but a dictionary which returns topically similar keywords in the target language. Such dictionaries can also be automatically built using large comparable bilingual corpora. However, due to absence of such a resource we manually created the above said dictionaries. Also, since the word list was chosen from an unstemmed word list of a web search engine, it may also contain proper names, foreign language words or morphological variants of words as separate entries. In many such cases the ditionary creator may end up keying in the same English keywords for all the morphological variants of a given source language word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Indexing, Retrieval and Ranking</head><p>Traditionally IR systems use the notion of fields to treat different types of metadata related to a document with different weights. Queries are constructed to search for keywords and weigh them using some prior weights assigned intuitively to a given metadata type. For example, the title of a document can be viewed as a metadata of the given article and a keyword found in the title might be deemed to be more important as against one found in the document's body. In our system, we used three such fields for each document. Two of the fields title, body were explicitly provided in the given corpus, while we derived a new field called summary of a document and chose the first 50 words of the document body as a summary. Different boost scores were given to each of these fields such that title of the document is deemed most important followed by summary, then followed by the body of the document. In order to provide different weights to terms based on fields, a combination of term and the field name is treated as a unique entry in the inverted index. In other words, a given term occuring in both title and body are treated as two different terms and their term frequencies and document frequencies are computed individually.</p><p>As mentioned in the beginning of this section, our retrieval approach is to translate the given source language query keywords using a bilingual dictionary. We translate one term at a time and do not handle multi-word expressions and phrases in queries. However, the algorithm itself does not require any modification to be able to handle multi-term queries. It would suffice if the multi-word expressions along with their meanings are also stored in the same dictionary as the single word dictionary. Our lookup algorithm first tries to lookup entries containing longest source language expressions. This is achieved since the lookup program also internally represents the dictionary using an inverted index data structure.</p><p>Once the source language queries are translated and transliterated, the resultant English keywords used to construct boolean queries using boolean AND and OR operators. Assume the index model to contain the set of fields/metadata as F = f 1 , f 2 ...f m and the source language query S = s 1 , s 2 , ..., s n , and every source language keyword s i results in multiple target language keywords. Let t ij be the j th translation of source language keyword s i . In our experiments we primarily construct a disjunctive type query Q disj and a hybrid query Q hyb for every k th field from F as</p><formula xml:id="formula_0" coords="5,256.02,373.04,256.98,19.91">Q disj,k = w k . i,j t ij<label>(1)</label></formula><formula xml:id="formula_1" coords="5,244.83,401.59,268.18,19.91">Q hyb,k = w k . i j t ij (2)</formula><p>where w k is the boost weight given to the k th field. Finally the multiple field queries are again combined using a boolean OR operator. We report various runs based on the boolean operations on the queries and the fields on which retrieval is performed in the evaluation section.</p><p>It is evident from our approach that we do not make any efforts were made to identify the irrelevant documents in the search process. For this reason we did not use the narrative information in the topics for any of our runs. It is also evident that we did not make any efforts to weigh the various terms in the possible translations which is the third issue for CLIR as mentioned in section 1 and treat all the translated keywords to be equally likely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Discussion</head><p>The evaluation document set consists of news articles and reports from Los Angeles Times of 2002. A set of 50 topics representing the information need were given in Hindi and Telugu. A set of human relevance judgements for these topics were generated by assessors at CLEF. These relevance judgements are binary relevance judgements and are decided by a human assessor after reviewing a set of pooled documents using the relevant document pooling technique. The system evaluation framework is similar to the Cranfield style system evaluations and the measures are similar to those used in TREC<ref type="foot" coords="5,180.44,643.73,3.97,6.12" target="#foot_4">5</ref>  <ref type="bibr" coords="5,188.73,645.30,14.62,8.74" target="#b10">[11]</ref>. Two runs were submitted related to the Indian languages, two with Hindi queries and two with Telugu queries. A monolingual run was also submitted to CLEF. After CLEF released the relevance judgements we conducted some more experiments. Table <ref type="table" coords="5,465.90,669.21,4.98,8.74">5</ref> describes the various runs we report in this section. MONO and MDISJ are the monolingual English runs. TETD, TNOSUM and TDISJ are Telugu-English CLIR runs, while HITD, HNOSUM and HDISJ are Hindi-English CLIR runs. These runs differ primarily in the fields that are used for searching and the way in which the translated keywords are combined using boolean operators. The third column in table 5 describes each of these runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CLEF 2007 Evaluation for Hindi-English and Telugu-English CLIR</head><p>The average metrics for each of the runs mentioned in table 5 are described in table <ref type="table" coords="6,455.72,437.15,3.88,8.74">5</ref>.1. The first column in table <ref type="table" coords="6,160.20,449.10,4.24,8.74">5</ref>.1 mentiones the metric and the remaining of each column represents the various runs. Each row gives a comparison of a given metric across all the runs. The metrics listed are as provided by the TREC evaluation package <ref type="foot" coords="6,289.45,471.44,3.97,6.12" target="#foot_5">6</ref> . Of these metrics, we find num rel ret, map, bpref and P5 values to be interesting. Apart from these metrics we also report the number of relavent documents (num rel), mean reciprocal rank (MRR) and interpolated recall at various precision levels (ircl prn) metrics. From the run statistics it can be observed that the Hindi-English CLIR performs reasonably well even when the dictionary is very small around 5,000 words. Also from P5 it can be observed that systems using boolean AND operator with appropriate boosting of metadata results in better ranking. However, such systems result in lower recall. It can be observed from num rel ret of disjunctive runs MDISJ, TDISJ and HDISJ that the system is able to retrieve more number of relavent documents when queries are combined using boolean OR operator. It can also be observed that using a summary as a metadata in retrieval might help when the translation quality is low. This fact can be observed from better performance of HNOSUM run for Hindi, which performs better than HITD. However, use of a summary results in lower performance when the translation quality is better, which can be observed from TETD and TNOSUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Our experiments suggest that the performance of a CLIR system heavily depends on the type and quality of the resources being used. While the underlying IR model is also important and can play a role in the quality of a CLIR output, we found the main contribution to performance coming from the ability to convert a source language information need into the target language. We showed that, by using simple techniques to quickly create dictionaries, one can maximize the probability of retrieving the relavent documents. This was evident from the fact that our Hindi-English CLIR system used a very small dictionary of the size of 5,175 words, many of them containing variants of same words. However, the reason for success of this resource in a CLIR task is that, the choice of source language words in the dictionary is motivated by the TFIDF measure of the words from a sufficiently large corpus. Moreover the dictionary creators keyed-in meanings with an IR application in mind, instead of attempting to create an exact synonym dictionary. Also, the restriction on the number of keywords one can type for a give source language word enabled us to capture the homonyms instead of many polysemous variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,119.67,118.94,357.02,274.68"><head>Table 1 :</head><label>1</label><figDesc>Run Descriptions</figDesc><table coords="6,119.67,130.61,357.02,263.01"><row><cell>Run ID</cell><cell cols="2">Language Pair Description</cell></row><row><cell>MONO</cell><cell>English</cell><cell>Monolingual run, using title, body and summary</cell></row><row><cell></cell><cell></cell><cell>fields. Title keywords are combined using hybrid</cell></row><row><cell></cell><cell></cell><cell>query as described in the previous section. CLEF</cell></row><row><cell></cell><cell></cell><cell>official submission.</cell></row><row><cell>MDISJ</cell><cell>English</cell><cell>Monolingual run, using title, body and summary</cell></row><row><cell></cell><cell></cell><cell>fields. All keywords are combined using boolean</cell></row><row><cell></cell><cell></cell><cell>OR operator.</cell></row><row><cell>TETD</cell><cell cols="2">Telugu -English Uses title, body and summary fields. Title key-</cell></row><row><cell></cell><cell></cell><cell>words are combined using boolean AND across</cell></row><row><cell></cell><cell></cell><cell>translations. CLEF official submission.</cell></row><row><cell>HITD</cell><cell>Hindi -English</cell><cell>Uses title, body and summary fields. Title key-</cell></row><row><cell></cell><cell></cell><cell>words are combined using boolean AND across</cell></row><row><cell></cell><cell></cell><cell>translations. CLEF official submission.</cell></row><row><cell cols="3">TNOSUM Telugu -English Title and body fields are used. Title keywords are</cell></row><row><cell></cell><cell></cell><cell>combined using boolean AND.</cell></row><row><cell cols="2">HNOSUM Hindi -English</cell><cell>Title and body fields are used. Title keywords are</cell></row><row><cell></cell><cell></cell><cell>combined using boolean AND.</cell></row><row><cell>TDISJ</cell><cell cols="2">Telugu -English Only body text is used. All translated keywords</cell></row><row><cell></cell><cell></cell><cell>combined using boolean OR.</cell></row><row><cell>HDISJ</cell><cell>Hindi -English</cell><cell>Only body text is used. All translated keywords</cell></row><row><cell></cell><cell></cell><cell>combined using boolean OR.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,95.98,245.79,482.64,376.76"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,95.98,245.79,482.64,376.76"><row><cell></cell><cell></cell><cell></cell><cell cols="2">: Run Statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">METRIC/RUN MDISJ MONO TETD TNOSUM TDISJ HITD HNOSUM HDISJ</cell></row><row><cell>num q</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>num ret</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell></row><row><cell>num rel</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell><cell>2247</cell></row><row><cell>num rel ret</cell><cell>1952</cell><cell>1629</cell><cell>1275</cell><cell>1456</cell><cell>1517</cell><cell>958</cell><cell>1132</cell><cell>1123</cell></row><row><cell>map</cell><cell>0.4003</cell><cell>0.3687</cell><cell>0.2155</cell><cell>0.2370</cell><cell>0.2170</cell><cell>0.1560</cell><cell>0.1432</cell><cell>0.1331</cell></row><row><cell>gm ap</cell><cell>0.3335</cell><cell>0.2526</cell><cell>0.0834</cell><cell>0.0950</cell><cell>0.0993</cell><cell>0.0319</cell><cell>0.0321</cell><cell>0.0321</cell></row><row><cell>R-prec</cell><cell>0.4084</cell><cell>0.3979</cell><cell>0.2467</cell><cell>0.2702</cell><cell>0.2478</cell><cell>0.1689</cell><cell>0.1557</cell><cell>0.1566</cell></row><row><cell>bpref</cell><cell>0.3980</cell><cell>0.3850</cell><cell>0.2868</cell><cell>0.3083</cell><cell>0.2750</cell><cell>0.2104</cell><cell>0.2026</cell><cell>0.2005</cell></row><row><cell>recip rank</cell><cell>0.7161</cell><cell>0.6584</cell><cell>0.5160</cell><cell>0.4814</cell><cell>0.5419</cell><cell>0.3778</cell><cell>0.3270</cell><cell>0.3416</cell></row><row><cell>ircl prn.0.00</cell><cell>0.7860</cell><cell>0.7346</cell><cell>0.5798</cell><cell>0.5424</cell><cell>0.5948</cell><cell>0.4084</cell><cell>0.3634</cell><cell>0.3756</cell></row><row><cell>ircl prn.0.10</cell><cell>0.6777</cell><cell>0.6289</cell><cell>0.4154</cell><cell>0.4320</cell><cell>0.4603</cell><cell>0.3139</cell><cell>0.2676</cell><cell>0.2592</cell></row><row><cell>ircl prn.0.20</cell><cell>0.6224</cell><cell>0.5839</cell><cell>0.3678</cell><cell>0.3889</cell><cell>0.3904</cell><cell>0.2789</cell><cell>0.2111</cell><cell>0.2298</cell></row><row><cell>ircl prn.0.30</cell><cell>0.5538</cell><cell>0.5180</cell><cell>0.2995</cell><cell>0.3251</cell><cell>0.3051</cell><cell>0.2114</cell><cell>0.1878</cell><cell>0.1807</cell></row><row><cell>ircl prn.0.40</cell><cell>0.4975</cell><cell>0.4607</cell><cell>0.2674</cell><cell>0.2892</cell><cell>0.2461</cell><cell>0.1870</cell><cell>0.1735</cell><cell>0.1571</cell></row><row><cell>ircl prn.0.50</cell><cell>0.4372</cell><cell>0.4153</cell><cell>0.2213</cell><cell>0.2575</cell><cell>0.2159</cell><cell>0.1549</cell><cell>0.1585</cell><cell>0.1416</cell></row><row><cell>ircl prn.0.60</cell><cell>0.3395</cell><cell>0.3013</cell><cell>0.1589</cell><cell>0.2073</cell><cell>0.1430</cell><cell>0.1095</cell><cell>0.1379</cell><cell>0.1108</cell></row><row><cell>ircl prn.0.70</cell><cell>0.2842</cell><cell>0.2421</cell><cell>0.1287</cell><cell>0.1601</cell><cell>0.1088</cell><cell>0.0907</cell><cell>0.1128</cell><cell>0.0809</cell></row><row><cell>ircl prn.0.80</cell><cell>0.2066</cell><cell>0.1830</cell><cell>0.0773</cell><cell>0.0912</cell><cell>0.0723</cell><cell>0.0650</cell><cell>0.0609</cell><cell>0.0529</cell></row><row><cell>ircl prn.0.90</cell><cell>0.1430</cell><cell>0.1308</cell><cell>0.0404</cell><cell>0.0562</cell><cell>0.0469</cell><cell>0.0396</cell><cell>0.0281</cell><cell>0.0358</cell></row><row><cell>ircl prn.1.00</cell><cell>0.0884</cell><cell>0.0765</cell><cell>0.0230</cell><cell>0.0257</cell><cell>0.0173</cell><cell>0.0222</cell><cell>0.0118</cell><cell>0.0147</cell></row><row><cell>P5</cell><cell>0.5520</cell><cell>0.4920</cell><cell>0.3480</cell><cell>0.3640</cell><cell>0.3440</cell><cell>0.2240</cell><cell>0.2040</cell><cell>0.1920</cell></row><row><cell>P10</cell><cell>0.4920</cell><cell>0.4520</cell><cell>0.3060</cell><cell>0.3060</cell><cell>0.3080</cell><cell>0.1820</cell><cell>0.2000</cell><cell>0.1960</cell></row><row><cell>P15</cell><cell>0.4453</cell><cell>0.4053</cell><cell>0.2720</cell><cell>0.2720</cell><cell>0.2733</cell><cell>0.1693</cell><cell>0.1773</cell><cell>0.1680</cell></row><row><cell>P20</cell><cell>0.4040</cell><cell>0.3680</cell><cell>0.2450</cell><cell>0.2520</cell><cell>0.2590</cell><cell>0.1510</cell><cell>0.1570</cell><cell>0.1540</cell></row><row><cell>P30</cell><cell>0.3567</cell><cell>0.3260</cell><cell>0.2127</cell><cell>0.2213</cell><cell>0.2240</cell><cell>0.1340</cell><cell>0.1307</cell><cell>0.1293</cell></row><row><cell>P100</cell><cell>0.2188</cell><cell>0.1880</cell><cell>0.1164</cell><cell>0.1178</cell><cell>0.1378</cell><cell>0.0808</cell><cell>0.0782</cell><cell>0.0826</cell></row><row><cell>P200</cell><cell>0.1447</cell><cell>0.1190</cell><cell>0.0773</cell><cell>0.0847</cell><cell>0.0949</cell><cell>0.0552</cell><cell>0.0554</cell><cell>0.0580</cell></row><row><cell>P500</cell><cell>0.0712</cell><cell>0.0592</cell><cell>0.0429</cell><cell>0.0501</cell><cell>0.0523</cell><cell>0.0322</cell><cell>0.0344</cell><cell>0.0349</cell></row><row><cell>P1000</cell><cell>0.0390</cell><cell>0.0326</cell><cell>0.0255</cell><cell>0.0291</cell><cell>0.0303</cell><cell>0.0192</cell><cell>0.0226</cell><cell>0.0225</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,737.50,272.94,6.99"><p>LTRC is a research centre at IIIT, Hyderabad, India. http://ltrc.iiit.ac.in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,747.00,227.92,6.99"><p>Cross Language Evaluation Forum. http://clef-campaign.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,731.67,361.58,6.99"><p>ACM Transactions on Asian Language Information Processing. http://www.acm.org/pubs/talip/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,741.06,91.27,6.99"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,105.24,745.98,176.89,6.99"><p>Text Retrieval Conferences, http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,105.24,737.99,230.12,6.99"><p>TREC provides a trec eval package for evaluating IR systems.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,110.48,250.47,402.52,8.74;8,110.48,262.43,402.52,8.74;8,110.48,274.38,276.69,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,426.68,250.47,86.33,8.74;8,110.48,262.43,122.43,8.74">Machine translation activities in India: A survey</title>
		<author>
			<persName coords=""><forename type="first">Akshar</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Sangal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dipti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amba P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,268.75,262.43,244.25,8.74;8,110.48,274.38,246.16,8.74">the Proceedings of workshop on survey on Research and Development of Machine Translation in Asian Countries</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,294.31,402.52,8.74;8,110.48,306.26,402.52,8.74;8,110.48,318.22,43.73,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,341.05,294.31,171.95,8.74;8,110.48,306.26,20.76,8.74">Cross-language headline generation for hindi</title>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,142.79,306.26,320.29,8.74">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="289" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,338.14,402.52,8.74;8,110.48,350.10,208.14,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,305.09,338.14,166.14,8.74">Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,370.02,402.52,8.74;8,110.48,381.98,402.52,8.74;8,110.48,393.93,402.52,8.74;8,110.48,405.89,25.74,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,364.62,370.02,130.57,8.74">Cross-lingual relevance models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.48,381.98,402.52,8.74;8,110.48,393.93,178.68,8.74">SIGIR &apos;02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,425.82,402.51,8.74;8,110.48,437.77,252.98,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,176.97,425.82,238.10,8.74">Alternative approaches for cross language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,434.83,425.82,78.17,8.74;8,110.48,437.77,196.25,8.74">AAAI Symposium on Cross Language Text and Speeck Retrieval</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,457.70,402.51,8.74;8,110.48,469.65,223.32,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,196.02,457.70,136.54,8.74">The surprise language exercises</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,342.06,457.70,170.94,8.74;8,110.48,469.65,140.63,8.74">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="84" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,489.58,402.51,8.74;8,110.48,501.53,402.53,8.74;8,110.48,513.49,387.03,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,395.02,489.58,117.97,8.74;8,110.48,501.53,159.59,8.74">Webkhoj: Indian language ir from multiple character encodings</title>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.31,501.53,218.70,8.74;8,110.48,513.49,135.87,8.74">WWW &apos;06: Proceedings of the 15th international conference on World Wide Web</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,533.41,402.52,8.74;8,110.48,545.37,375.66,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,375.91,533.41,137.10,8.74;8,110.48,545.37,72.83,8.74">Hindi, Telugu, Oromo, English CLIR Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kula</forename><surname>Kekeba Tune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,192.24,545.37,263.52,8.74">Lecture Notes in Computer Science: CLEF 2006 Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,565.29,402.51,8.74;8,110.48,577.25,402.52,8.74;8,110.48,589.20,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,276.51,565.29,236.48,8.74;8,110.48,577.25,101.94,8.74">Hindi and Telugu to English Cross Language Information Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,255.41,577.25,230.87,8.74">Working Notes of Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2006">2006. 2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,609.13,402.52,8.74;8,110.48,621.08,402.52,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,276.38,609.13,231.98,8.74">Hindi-english cross-lingual question-answering system</title>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,110.48,621.08,310.75,8.74">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="181" to="192" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,641.01,402.52,8.74;8,110.48,652.96,402.53,8.74;8,110.48,664.92,186.64,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,285.06,641.01,158.47,8.74">The text retrieval conferences (trecs)</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.34,641.01,49.66,8.74;8,110.48,652.96,108.31,8.74">Proceedings of a workshop on held at</title>
		<meeting>a workshop on held at<address><addrLine>Baltimore, Maryland; Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="241" to="273" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
