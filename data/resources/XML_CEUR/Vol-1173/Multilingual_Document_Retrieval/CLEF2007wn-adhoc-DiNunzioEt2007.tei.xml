<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2007: Ad Hoc Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.32,152.97,61.11,9.96"><forename type="first">Giorgio</forename><forename type="middle">M</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.04,152.97,53.71,9.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.24,152.97,66.14,9.96"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
							<email>mandl@uni-hildesheim.de</email>
							<affiliation key="aff1">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.37,152.97,54.33,9.96"><forename type="first">Carol</forename><surname>Peters</surname></persName>
							<email>carol.peters@isti.cnr.it</email>
							<affiliation key="aff2">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<addrLine>Area di Ricerca</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.24,115.72,260.83,12.93">CLEF 2007: Ad Hoc Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6DF6E1E2546970AFA94BF7DEFE1A782A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation Experimentation, Performance, Measurement, Algorithms Multilingual Information Access, Cross-Language Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the objectives and organization of the CLEF 2007 ad hoc track and discuss the main characteristics of the tasks offered to test monolingual and cross-language textual document retrieval systems. The track was divided into two streams. The main stream offered mono-and bilingual tasks on target collections for central European languages (Bulgarian, Czech and Hungarian). Similarly to last year, a bilingual task encouraging system testing with non-European languages against English documents was also offered; this year, particular attention was given to Indian languages. The second stream, designed for more experienced participants, offered mono-and bilingual "robust" tasks with the objective of privileging experiments which achieve good stable performance over all queries rather than high average performance. These experiments re-used CLEF test collections from previous years in three languages (English, French, and Portuguese). The performance achieved for each task is presented and a statistical analysis of results is given.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ad hoc retrieval track is generally considered to be the core track in the Cross-Language Evaluation Forum (CLEF). The aim of this track is to promote the development of monolingual and cross-language textual document retrieval systems. Similarly to last year, the CLEF 2007 ad hoc track was structured in two streams. The main stream offered mono-and bilingual retrieval tasks on target collections for central European languages plus a bilingual task encouraging system testing with non-European languages against English documents. The second stream, designed for more experienced participants, was the "robust task", aimed at finding documents for very difficult queries. It used test collections developed in previous years.</p><p>The Monolingual and Bilingual tasks were principally offered for Bulgarian, Czech and Hungarian target collections. Additionally, a bilingual task was offered to test querying with non-European language queries against an English target collection. As a result of requests from a number of Indian research institutes, a special sub-task for Indian languages was offered with topics in Bengali, Hindi, Marathi, Tamil and Telugu. The aim in all cases was to retrieve relevant documents from the chosen target collection and submit the results in a ranked list.</p><p>The Robust task proposed mono-and bilingual experiments using the test collections built over the last six CLEF campaigns. Collections and topics in English, Portuguese and French were used. The goal of the robust analysis is to improve the user experience with a retrieval system. Poor performing topics are more serious for the user than performance losses in the middle and upper interval. The robust task gives preference to systems which achieve a minimal level for all topics. The measure used to assure this, is the geometric mean over all topics. The robust task intends to evaluate stable performance over all topics instead of high average performance.</p><p>This was the first year since CLEF began that we have not offered a Multilingual ad hoc task (ie searching a target collection in multiple languages).</p><p>In this paper we describe the track setup, the evaluation methodology and the participation in the different tasks (Section 2), present the main characteristics of the experiments and show the results (Sections 3 -5). Statistical testing is discussed in Section 6 and the final section provides a brief summing up. For information on the various approaches and resources used by the groups participating in this track and the issues they focused on, we refer the reader to the other papers in the Ad Hoc section of these Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Setup</head><p>The ad hoc track in CLEF adopts a corpus-based, automatic scoring method for the assessment of system performance, based on ideas first introduced in the Cranfield experiments in the late 1960s. The test collection used consists of a set of "topics" describing information needs and a collection of documents to be searched to find those documents that satisfy these information needs. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures. The distinguishing feature of CLEF is that it applies this evaluation paradigm in a multilingual setting. This means that the criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted to satisfy the particular requirements of the multilingual context. All language dependent tasks such as topic creation and relevance judgment are performed in a distributed setting by native speakers. Rules are established and a tight central coordination is maintained in order to ensure consistency and coherency of topic and relevance judgment sets over the different collections, languages and tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Collections</head><p>Different test collections were used in the ad hoc task this year. The main stream used national newspaper documents from 2002 as the target collections, creating sets of new topics and making new relevance assessments. The robust task reused existing CLEF test collections and did not create any new topics or make any fresh relevance assessments.</p><p>Documents. The document collections used for the CLEF 2007 ad hoc tasks are part of the CLEF multilingual corpus of newspaper and news agency documents described in the Introduction to these Proceedings.</p><p>In the main stream monolingual and bilingual tasks, Bulgarian, Czech, Hungarian and English national newspapers for 2002 were used. Much of this data represented new additions to the CLEF multilingual comparable text corpora: Czech is a totally new language in the ad hoc track although it was introduced into the speech retrieval track last year; the Bulgarian collection was expanded with the addition of another national newspaper, and in order to have comparable data for English, we acquired a new American-English collection: Los Angeles Times 2002. Table <ref type="table" coords="3,253.78,607.05,4.98,9.96" target="#tab_0">1</ref> summarizes the collections used for each language.</p><p>The robust task used test collections containing news documents for the period 1994-1995 in three languages (English, French, and Portuguese) used in CLEF 2000 through CLEF 2006. Table <ref type="table" coords="3,314.19,643.53,4.98,9.96" target="#tab_1">2</ref> summarizes the collections used for each language.</p><p>Topics Topics in the CLEF ad hoc track are structured statements representing information needs; the systems use the topics to derive their queries. Each topic consists of three parts: a brief "title" statement; a one-sentence "description"; a more complex "narrative" specifying the relevance assessment criteria.</p><p>Sets of 50 topics were created for the CLEF 2007 ad hoc mono-and bilingual tasks. All topic sets were created by native speakers. One of the decisions taken early on in the organization of the CLEF ad hoc tracks was that the same set of topics would be used to query all collections, whatever the task. There were a number of reasons for this: it makes it easier to compare results over different collections, it means that there is a single master set that is rendered in all query languages, and a single set of relevance assessments for each language is sufficient for all tasks. In CLEF 2006 we deviated from this rule as we were using document collections from two distinct periods (1994/5 and 2002) and created partially separate (but overlapping) sets with a common set of timeindependent topics and separate sets of time-specific topics. As we had expected this really complicated our lives as we had to build more topics and had to specify very carefully which topic sets were to be used against which document collections<ref type="foot" coords="4,179.76,321.53,3.97,6.97" target="#foot_0">1</ref> . We determined not to repeat this experience this year and thus only used collections from the same time period.</p><p>We created topics in both European and non-European languages. European language topics were offered for Bulgarian, Czech, English, French, Hungarian, Italian and Spanish. The non-European languages were prepared according to demand from participants. This year we had Amharic, Chinese, Indonesian, Oromo plus the group of Indian languages: Bengali, Hindi, Marathi, Tamil and Telugu.</p><p>The provision of topics in unfamiliar scripts did lead to some problems. These were not caused by encoding issues (all CLEF data is encoded using UTF-8) but rather by errors in the topic sets which were very difficult for us to spot. Although most such problems were quickly noted and corrected, and the participants were informed so that they all used the right set, one did escape our notice: the title of Topic 430 in the Czech set was corrupted and systems using Czech thus did not do well with this topic. It should be remembered, however, that an error is one topic does not really impact significantly on the comparative results of the systems. The topic will, however, be corrected for future use. This year topics have been identified by means of a Digital Object Identifier (DOI) 2 of the experiment <ref type="bibr" coords="4,249.49,530.13,10.45,9.96" target="#b0">[1]</ref> which allows us to reference and cite them. Below we give an example of the English version of a typical CLEF 2007 topic: &lt;top lang="en"&gt; &lt;num&gt;10.2452/401-AH&lt;/num&gt; &lt;title&gt;Euro Inflation&lt;/title&gt; &lt;desc&gt;Find documents about rises in prices after the introduction of the Euro.&lt;/desc&gt; &lt;narr&gt;Any document is relevant that provides information on the rise of prices in any country that introduced the common European currency.&lt;/narr&gt; &lt;/top&gt; For the robust task, the topic sets from CLEF 2001 to 2006 in English, French and Portuguese were used. For English and French, which have been part of CLEF for more time, training topics were offered and a set of 100 topics were used for testing. For Portuguese, no training topics were possible and a set of 150 test topics was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Participation Guidelines</head><p>To carry out the retrieval tasks of the CLEF campaign, systems have to build supporting data structures. Allowable data structures include any new structures built automatically (such as inverted files, thesauri, conceptual networks, etc.) or manually (such as thesauri, synonym lists, knowledge bases, rules, etc.) from the documents. They may not, however, be modified in response to the topics, e.g. by adding topic words that are not already in the dictionaries used by their systems in order to extend coverage. Some CLEF data collections contain manually assigned, controlled or uncontrolled index terms. The use of such terms is limited to specific experiments that have to be declared as "manual" runs.</p><p>Topics can be converted into queries that a system can execute in many different ways. CLEF strongly encourages groups to determine what constitutes a base run for their experiments and to include these runs (officially or unofficially) to allow useful interpretations of the results. Unofficial runs are those not submitted to CLEF but evaluated using the trec eval package. This year we have used the new package written by Chris Buckley for the Text REtrieval Conference (TREC) (trec eval 8.0) and available from the TREC website <ref type="foot" coords="5,463.20,462.17,3.97,6.97" target="#foot_2">3</ref> .</p><p>As a consequence of limited evaluation resources, a maximum of 12 runs each for the mono-and bilingual tasks was allowed (no more than 4 runs for any one language combination -we try to encourage diversity). For bi-and monolingual robust tasks, 4 runs were allowed per language or language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessment</head><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the groups participating in the ad hoc tasks are used to form a pool of documents for each topic and language by collecting the highly ranked documents from selected runs according to a set of predefined criteria. Traditionally, the top 100 ranked documents from each of the runs selected are included in the pool; in such a case we say that the pool is of depth 100. This pool is then used for subsequent relevance judgments. After calculating the effectiveness measures, the results are analyzed and run statistics produced and distributed.</p><p>The stability of pools constructed in this way and their reliability for postcampaign experiments is discussed in <ref type="bibr" coords="6,300.96,166.29,10.45,9.96" target="#b1">[2]</ref> with respect to the CLEF 2003 pools. New pools were formed in CLEF 2007 for the runs submitted for the main stream mono-and bilingual tasks. Instead, the robust tasks used the original pools and relevance assessments from previous CLEF campaigns.</p><p>The main criteria used when constructing these pools were:</p><p>favour diversity among approaches adopted by participants, according to the descriptions of the experiments provided by the participants; -choose at least one experiment for each participant in each task, chosen among the experiments with highest priority as indicated by the participant; -add mandatory title+description experiments, even though they do not have high priority; -add manual experiments, when provided; -for bilingual tasks, ensure that each source topic language is represented.</p><p>One important limitation when forming the pools is the number of documents to be assessed. We estimate that assessors can judge from 60 to 100 documents per hour, providing binary judgments: relevant / not relevant. This is actually an optimistic estimate and shows what a time-consuming and resource expensive task human relevance assessment is. This limitation impacts strongly on the application of the criteria above -and implies that we are obliged to be flexible in the number of documents judged per selected run for individual pools.</p><p>This meant that this year, in order to create pools of more-or-less equivalent size (approx. 20,000 documents), the depth of the Bulgarian, Czech and Hungarian pools varied: 60 for Czech and 80 for Bulgarian and Hungarian, rather than the depth of 100 originally used to judge TREC ad hoc experiments <ref type="foot" coords="6,460.56,457.37,3.97,6.97" target="#foot_3">4</ref> . In his paper in these working notes, Tomlinson <ref type="bibr" coords="6,328.22,470.73,10.57,9.96" target="#b2">[3]</ref> makes some interesting observations in this respect. He claims that on average, the percentage of relevant items assessed was less than 60% for Czech, 70% for Bulgarian and 85% for Hungarian. However, as Tomlinson also points out, it has already been shown that test collections created in this way do normally provide reliable results, even if not all relevant documents are included in the pool.</p><p>When building the pool for English, in order to respect the above criteria and also to obtain a pool depth of 60, we had to include more than 25,000 documents. Even so, as can be seen from Table <ref type="table" coords="6,295.80,566.49,3.90,9.96" target="#tab_2">3</ref>, it was impossible to include very many runs -just one monolingual and one bilingual run for each set of experiments. We will certainly be performing some post-workshop stability tests on these pools.</p><p>The box plot of Figure <ref type="figure" coords="6,256.82,602.49,4.98,9.96" target="#fig_0">1</ref> compares the distributions of the relevant documents across the topics of each pool for the different ad hoc pools; the boxes are ordered by decreasing mean number of relevant documents per topic. As can be noted, Bulgarian, Czech, and Hungarian distributions appear similar, even though the Czech and Hungarian ones are slightly more asymmetric towards topics with a greater number of relevant documents. On the other hand, the English distribution presents a greater number of relevant documents per topic, with respect to the other distributions, and is quite asymmetric towards topics with a greater number of relevant documents. All the distributions show some upper outliers, i.e. topics with a great number of relevant document with respect to the behaviour of the other topics in the distribution. These outliers are probably due to the fact that CLEF topics have to be able to retrieve relevant documents in all the collections; therefore, they may be considerably broader in one collection compared with others depending on the contents of the separate datasets. Thus, typically, each pool will have a different set of outliers. Table <ref type="table" coords="7,178.68,498.69,4.98,9.96" target="#tab_2">3</ref> reports summary information on the 2007 ad hoc pools used to calculate the results for the main monolingual and bilingual experiments. In particular, for each pool, we show the number of topics, the number of runs submitted, the number of runs included in the pool, the number of documents in the pool (relevant and non-relevant), and the number of assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of Information Retrieval Systems (IRSs) can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participants and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are  <ref type="bibr" coords="9,290.18,118.29,9.91,9.96" target="#b3">[4]</ref>. For the robust task, we used different measures, see below Section 5.</p><p>The individual results for all official ad hoc experiments in CLEF 2007 are given in the Appendix at the end of these Working Notes <ref type="bibr" coords="9,388.48,154.17,10.51,9.96" target="#b4">[5,</ref><ref type="bibr" coords="9,398.99,154.17,7.01,9.96" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="9,218.64,202.41,3.90,9.96" target="#tab_3">4</ref>, a total of 22 groups from 12 different countries submitted results for one or more of the ad hoc tasks -a slight decrease on the 25 participants of last year. Table <ref type="table" coords="9,244.69,226.41,4.98,9.96" target="#tab_4">5</ref> provides a breakdown of the number of participants by country.</p><p>A total of 235 runs were submitted with a decrease of about 20% on the 296 runs of 2006. The average number of submitted runs per participant also slightly decreased: from 11.7 runs/participant of 2006 to 10.6 runs/participant of this year.</p><p>Participants were required to submit at least one title+description ("TD") run per task in order to increase comparability between experiments. The large majority of runs (138 out of 235, 58.72%) used this combination of topic fields, 50 (21.28%) used all fields, 46 (19.57%) used the title field, and only 1 (0.43%) used the description field. The majority of experiments were conducted using automatic query construction (230 out of 235, 97.87%) and only in a small fraction of the experiments (5 out 237, 2.13%) were queries been manually constructed from topics. A breakdown into the separate tasks is shown in Table <ref type="table" coords="9,432.36,381.81,4.11,9.96" target="#tab_5">6</ref>(a).</p><p>Fourteen different topic languages were used in the ad hoc experiments. As always, the most popular language for queries was English, with Hungarian second. The number of runs per topic language is shown in Table <ref type="table" coords="9,423.12,417.69,4.20,9.96" target="#tab_5">6</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Stream Monolingual Experiments</head><p>Monolingual retrieval focused on central-European languages this year, with tasks offered for Bulgarian, Czech and Hungarian. Eight groups presented results for 1 or more of these languages. We also requested participants in the bilingualto-English task to submit one English monolingual run, but only in order to provide a baseline for their bilingual experiments and in order to strengthen the English pool for relevance assessment <ref type="foot" coords="9,297.84,530.33,3.97,6.97" target="#foot_4">5</ref> .</p><p>Five of the participating groups submitted runs for all three languages. One group was unable to complete its Bulgarian experiments, submitting results for just the other two languages. The two groups from the Czech Republic only submitted runs for Czech. From the graphs and from 7, it can be seen that the best performing groups were more-or-less the same for each language and that the results did not greatly differ. It should be noted that these are all veteran participants with much experience at CLEF. As usual in the CLEF monolingual task, the main emphasis in the experiments was on stemming and morphological analysis. The group from University of Neuchatel, which had the best overall performances for all languages, focused very much on stemming strategies, testing both light and aggressive stemmers for the Slavic languages (Bulgarian and Czech). For Hungarian they worked on decompounding. This group also compared performances obtained using wordbased and 4-gram indexing strategies <ref type="bibr" coords="11,299.65,469.05,10.00,9.96" target="#b6">[7]</ref>. Another of the best performers, JHU-APL, normally uses an n-gram approach. Unfortunately, we have not received a paper yet from this group so cannot comment on their performance. The other group with very good performance for all languages was Opentext. This group also compared 4-gram results against results using stemming for all three languages. They found that while there could be large impacts on individual topics, there was little overall difference in average performance. Their experiments also confirmed past findings that indicate that blind relevance feedback can be detrimental to results, depending on the evaluation measures used <ref type="bibr" coords="11,413.64,564.69,9.91,9.96" target="#b2">[3]</ref>. The results of the statistical tests given towards the end of this paper show that the best results of these three groups did not differ significantly.</p><p>The group from Alicante also achieved good results testing query expansion techniques <ref type="bibr" coords="11,184.68,619.53,10.00,9.96" target="#b7">[8]</ref>, while the group from Kolkata compared a statistical stemmer against a rule-based stemmer for both Czech and Hungarian <ref type="bibr" coords="11,415.46,631.53,9.89,9.96" target="#b8">[9]</ref>. Czech is a morphologically complex language and the two Czech only groups both used approaches involving morphological analysis and lemmatization <ref type="bibr" coords="11,415.09,655.41,14.60,9.96" target="#b9">[10]</ref>, <ref type="bibr" coords="11,436.69,655.41,14.60,9.96" target="#b10">[11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Table <ref type="table" coords="12,161.64,467.61,4.98,9.96" target="#tab_6">7</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant. Figures 2 to 5 compare the performances of the top participants of the Monolingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Stream Bilingual Experiments</head><p>The bilingual task was structured in three tasks (X → BG, CS, or HU target collection) plus a task for non-European topic languages against an English target collection. A special sub-task testing Indian languages against the English collection was also organised in response to requests from a number of research groups working in India. For the bilingual to English task, participating groups also had to submit an English monolingual run, to be used both as baseline and also to reinforce the English pool. All groups participating in the Indian  languages sub-task also had to submit at least one run in Hindi (mandatory) plus runs in other Indian languages (optional).</p><p>We were disappointed to only receive runs from one participant for the X → BG, CS, or HU tasks. Furthermore, the results were quite poor; as this group normally achieves very good performance, we suspect that these runs were probably corrupted in some way. For this reason, we have decided to disregard them as being of little significance. Therefore, in the rest of this section, we only comment briefly on the X → EN results.</p><p>We received runs using the following topic languages: Amharic, Chinese, Indonesian and Oromo plus, for the Indian sub-task, Bengali, Hindi, Marathi and Telugu <ref type="foot" coords="15,183.72,237.29,3.97,6.97" target="#foot_5">6</ref> .</p><p>For many of these languages few processing tools or resources are available. It is thus very interesting to see what measures the participants adopted to overcome this problem. Unfortunately, there has not yet been time to read the submitted reports from each group and, here below, we give just a first cursory glance at some of the approaches and techniques adopted. We will provide a more in-depth analysis at the workshop.</p><p>The top performance in the bilingual task was obtained by an Indonesian group; they compared different translation techniques: machine translation using Internet resources, transitive translation using bilingual dictionaries and French and German as pivot languages, and lexicons derived from parallel corpus created by translating all the CLEF English documents into Indonesian using a commercial MT system. They found that they obtained best results using the MT system together with query expansion <ref type="bibr" coords="15,323.41,394.89,14.60,9.96" target="#b12">[12]</ref>.</p><p>The second placed group used Chinese for their queries and a dictionary based translation technique. The experiments of this group concentrated on developing new strategies to address two well-known CLIR problems: translation ambiguity, and coverage of the lexicon <ref type="bibr" coords="15,258.85,443.13,14.60,9.96" target="#b13">[13]</ref>. The work by <ref type="bibr" coords="15,341.18,443.13,15.49,9.96" target="#b14">[14]</ref> which used Amharic as the topic language also paid attention to the problems of sense disambiguation and out-of-vocabulary terms.</p><p>The third performing group also used Indonesian as the topic language; unfortunately we have not received a paper from them so far so cannot comment on their approach. An interesting paper, although slightly out of the task as the topic language used was Hungarian was <ref type="bibr" coords="15,313.58,515.25,14.60,9.96" target="#b15">[15]</ref>. This group used a machine readable dictionary approach but also applied Wikipedia data to eliminate unlikely translations according to the conceptual context. The group testing Oromo used linguistic and lexical resources developed at their institute; they adopted a bilingual dictionary approach and also tested the impact of a light stemmer for Afaan Oromo on their performance with positive results <ref type="bibr" coords="15,354.02,575.01,14.60,9.96" target="#b16">[16]</ref>.</p><p>The groups using Indian topic languages tested different approaches. The group from Kolkata submitted runs for Bengali, Hindi and Telugu to English using a bilingual dictionary lookup approach <ref type="bibr" coords="15,340.15,611.37,14.60,9.96" target="#b17">[17]</ref>. They had the best performance using Telugu probably because they carried out some manual tasks during indexing. A group from Bangalore tested a statistical MT system trained on 29.63% Difference parallel aligned sentences and a language modelling based retrieval algorithm for a Hindi to English system <ref type="bibr" coords="16,264.38,326.97,14.60,9.96" target="#b19">[18]</ref>. The group from Bombay had the best overall performances; they used bilingual dictionaries for both Hindi and Marathi to English and applied term-to-term cooccurrence statistics for sense disambiguation <ref type="bibr" coords="16,154.57,362.85,14.60,9.96" target="#b20">[19]</ref>. The Hyderabad group also used bilingual lexicons for query translation from Hindi and Telugu to English together with a variant of the TFIDF algorithm and a hybrid boolean formulation for the queries to improve ranking <ref type="bibr" coords="16,462.38,386.73,14.60,9.96" target="#b21">[20]</ref>. Interesting work was done by the group from Kharagpur which submitted runs for Hindi and Bengali. They attempted to overcome the lack of resources for Bengali by using phoneme-based transliterations to generate equivalent English queries from Hindi and Bengali topics <ref type="bibr" coords="16,303.39,434.49,14.60,9.96" target="#b22">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Table <ref type="table" coords="16,161.76,482.85,4.98,9.96" target="#tab_8">8</ref> shows the best results for this task. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision). Again both pooled and not pooled runs are included in the best entries for each track, with the exception of Bilingual X → EN.</p><p>Figure <ref type="figure" coords="16,181.21,530.61,4.98,9.96" target="#fig_2">6</ref> compares the performances of the top participants of the Bilingual English <ref type="foot" coords="16,167.16,541.25,3.97,6.97" target="#foot_6">7</ref> .</p><p>For bilingual retrieval evaluation, a common method to evaluate performance is to compare results against monolingual baselines. This year we can only comment on the results for the bilingual to English tasks. The best results were obtained by a system using Indonesian as a topic language. This group achieved 88.10% of the best monolingual English IR system. This is a good result considering that Indonesian is not a language for which a lot of resources and machinereadable dictionaries are available. It is very close to the best results obtained  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Indian To English Subtask Results</head><p>Table <ref type="table" coords="17,162.36,510.81,4.98,9.96" target="#tab_10">9</ref> shows the best results for the Indian sub-task. The performance difference between the best and the last (up to 6) placed group is given (in terms of average precision). The first set of rows regard experiments for the mandatory topic language: Hindi; the second set of rows report experiments where the source language is one of other Indian languages.</p><p>It is interesting to note that in both sets of experiments, the best performing participant is the same. In the second set, we can note that for three (Hindi, Marathi, and Telegu) out of the four Indian languages used the performances of the top groups are quite similar.</p><p>The best performance for the Indian sub-task is 76.12% of the best bilingual English system (achieved by veteran CLEF participants) and 67.06% of the monolingual baseline, which is quite encouraging for a new task with languages where encoding issues and linguistic resources make the task difficult. This is in fact comparable with the performances of some newly introduced European languages. For example, we can compare them to those for Bulgarian and Hungarian in CLEF 2006:</p><p>-X → BG: 52.49% of best monolingual Bulgarian IR system; -X → HU: 53.13% of best monolingual Hungarian IR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust Experiments</head><p>The robust task ran for the second time at CLEF 2007. It is an ad-hoc retrieval task based on data of previous CLEF campaigns. The evaluation approach is modified and a different perspective is taken. The robust task emphasizes the difficult topics by a non-linear integration of the results of individual topics into one result for a system <ref type="bibr" coords="18,263.88,488.13,15.49,9.96" target="#b23">[22,</ref><ref type="bibr" coords="18,279.37,488.13,11.62,9.96" target="#b24">23]</ref>. By doing this, the evaluation results are interpreted in a more user oriented manner. Failures and very low results for some topics hurt the user experience with a retrieval system. Consequently, any system should try to avoid these failures. This has turned out to be a hard task <ref type="bibr" coords="18,134.76,535.89,14.60,9.96" target="#b25">[24]</ref>. Robustness is a key issue for the transfer of research into applications. The robust task rewards systems which achieve a minimal performance level for all topics.</p><p>In order to do this, the robust task uses the geometric mean of the average precision for all topics (GMAP) instead of the mean average of all topics (MAP). This measure has also been used at a roust track at the Text Retrieval Conference (TREC) where robustness was explored for monolingual English retrieval <ref type="bibr" coords="18,462.37,607.65,14.60,9.96" target="#b24">[23]</ref>. At CLEF, robustness is evaluated for monolingual and bilingual retrieval for several European languages.</p><p>The robust task at CLEF exploits data created for previous CLEF editions. Therefore, a larger data set can be used for the evaluation. A larger number of topics allows a more reliable evaluation <ref type="bibr" coords="19,306.49,217.05,14.60,9.96" target="#b26">[25]</ref>. A secondary goal of the robust task is the definition of larger data sets for retrieval evaluation.</p><p>As described above, the CLEF2007 robust task offered three languages often used in previous CLEF campaigns: English, French and Portuguese. The data used has been developed during CLEF 2001 through 2006. Generally, the topics from CLEF 2001 until CLEF 2003 were training topics whereas the topics developed between 2004 and 2006 were the test topics on which the main evaluation measures are given.</p><p>Thus, the data used in the robust task in 2007 is different from the set defined for the roust task at CLEF 2006. The documents which need to be searched are articles from major newspapers and news providers in the three languages. Not all collections had been offered consistently for all CLEF campaigns, therefore, not all collections were integrated into the robust task. Most data from 1995 was omitted in order to provide a homogeneous collection. However, for Portuguese, for which no training data was available, only data from 1995 was used. Table <ref type="table" coords="19,134.76,396.33,10.02,9.96" target="#tab_11">10</ref> shows the data for the robust task.</p><p>The robust task attracted 63 runs submitted by 7 groups (CLEF 2006: 133 runs from 8 groups). Effectiveness scores were calculated with the version 8.0 of the program which provides the Mean Average Precision (MAP), while the Geometric Average Precision (GMAP) was calculated using DIRECT version 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Robust Monolingual Results</head><p>Table <ref type="table" coords="19,163.20,501.93,10.02,9.96" target="#tab_12">11</ref> shows the best results for this task. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision).</p><p>The results cannot be compared to the results of the CLEF 2005 and CLEF 2006 campaign in which the same topics were used because a smaller collection had to be searched.</p><p>Figures from 7 to 9 compare the performances of the top participants of the Robust Monolingual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robust Bilingual Results</head><p>Table <ref type="table" coords="19,163.20,631.53,10.02,9.96" target="#tab_13">12</ref> shows the best results for this task. The performance difference between the best and the last (up to 5) placed group is given (in terms of average precision). All the experiments where from English to French. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2007:</p><p>-X → FR: 85.05% of best monolingual French IR system; Figure <ref type="figure" coords="20,181.45,537.69,10.02,9.96" target="#fig_3">10</ref> compares the performances of the top participants of the Robust Bilingual task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Approaches Applied to Robust Retrieval</head><p>The REINA system applied different measures of robustness during the training phase in order to optimize the performance. A local query expansion technique added terms. The CoLesIR system experimented with n-gram based translation for bi-lingual retrieval which requires no languages specific components. SINAI tried to increase the robustness of the results by expanding the query with an external knowledge source. This is a typical approach in order to obtain additional  query terms and avoid zero hits in case of out of vocabulary problems. Contrary to standard query expansion techniques, the new terms form a second query and results of both initial and second query are integrated under a logistic fusion strategy. The Daedalus group submitted experiments wit the Miracle system. BM25 weighting without blind relevance feedback was applied. For descriptions of all the robust experiments, see the Robust section in these Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistical Testing</head><p>When the goal is to validate how well results can be expected to hold beyond a particular set of queries, statistical testing can help to determine what differences between runs appear to be real as opposed to differences that are due to sampling issues. We aim to identify runs with results that are significantly different from the results of other runs. "Significantly different" in this context means that the difference between the performance scores for the runs in question appears greater than what might be expected by pure chance. As with all statistical testing, conclusions will be qualified by an error probability, which was chosen to be 0.05 in the following. We have designed our analysis to follow closely the methodology used by similar analyses carried out for TREC <ref type="bibr" coords="23,400.47,345.45,14.67,9.96" target="#b27">[26]</ref>.</p><p>We used the MATLAB Statistics Toolbox, which provides the necessary functionality plus some additional functions and utilities. We use the ANalysis Of VAriance (ANOVA) test. ANOVA makes some assumptions concerning the data be checked. Hull <ref type="bibr" coords="23,209.76,393.69,15.49,9.96" target="#b27">[26]</ref> provides details of these; in particular, the scores in question should be approximately normally distributed and their variance has to be approximately the same for all runs. Two tests for goodness of fit to a normal distribution were chosen using the MATLAB statistical toolbox: the Lilliefors test <ref type="bibr" coords="23,155.15,441.45,15.61,9.96" target="#b29">[27]</ref> and the Jarque-Bera test <ref type="bibr" coords="23,291.36,441.45,14.60,9.96" target="#b30">[28]</ref>. In the case of the CLEF tasks under analysis, both tests indicate that the assumption of normality is violated for most of the data samples (in this case the runs for each participant).</p><p>In such cases, a transformation of data should be performed. The transformation for measures that range from 0 to 1 is the arcsin-root transformation: arcsin √ x which Tague-Sutcliffe <ref type="bibr" coords="23,231.36,535.17,15.49,9.96" target="#b31">[29]</ref> recommends for use with precision/recall measures. Table <ref type="table" coords="23,176.64,547.41,10.02,9.96" target="#tab_2">13</ref> shows the results of both the Lilliefors and Jarque-Bera tests before and after applying the Tague-Sutcliffe transformation. After the transformation the analysis of the normality of samples distribution improves significantly, with some exceptions. The difficulty to transform the data into normally distributed samples derives from the original distribution of run performances which tend towards zero within the interval [0,1].</p><p>In the following sections, two different graphs are presented to summarize the results of this test. All experiments, regardless of topic language or topic fields, are included. Results are therefore only valid for comparison of individual pairs of runs, and not in terms of absolute performance. Both for the ad-hoc Table <ref type="table" coords="24,163.92,115.80,9.01,8.97" target="#tab_2">13</ref>. Lilliefors (LF) and Jarque-Bera (JB) test for each Ad-Hoc track with and without Tague-Sutcliffe (TS) arcsin transformation. Each entry is the number of experiments whose performance distribution can be considered drawn from a Gaussian distribution, with respect to the total number of experiment of the track. The value of alpha for this test was set to 5%. and robust tasks, only runs where significant differences exist are shown; the remainder of the graphs can be found in the Appendices <ref type="bibr" coords="24,384.41,361.05,10.51,9.96" target="#b4">[5,</ref><ref type="bibr" coords="24,394.91,361.05,7.01,9.96" target="#b5">6]</ref>. The first graph shows participants' runs (y axis) and performance obtained (x axis). The circle indicates the average performance (in terms of Precision) while the segment shows the interval in which the difference in performance is not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Track</head><p>The second graph shows the overall results where all the runs that are included in the same group do not have a significantly different performance. All runs scoring below a certain group perform significantly worse than at least the top entry of the group. Likewise all the runs scoring above a certain group perform significantly better than at least the bottom entry in that group. To determine all runs that perform significantly worse than a certain run, determine the rightmost group that includes the run, all runs scoring below the bottom entry of that group are significantly worse. Conversely, to determine all runs that perform significantly better than a given run, determine the leftmost group that includes the run. All runs that score better than the top entry of that group perform significantly better.  We have reported the results of the ad hoc cross-language textual document retrieval track at CLEF 2007. This track is considered to be central to CLEF as for many groups it is the first track in which they participate and provides them with an opportunity to test their systems and compare performance between monolingual and cross-language runs, before perhaps moving on to more complex system development and subsequent evaluation. This year, the monolingual task focused on central European languages while the bilingual task included an activity for groups that wanted to use non-European topic languages and languages with few processing tools and resources. Each year, we also include a task aimed at examining particular aspects of cross-language text retrieval. Again this year, the focus was examining the impact of "hard" topics on performance in the "robust" task.</p><p>The paper also describes in some detail the creation of the pools used for relevance assessment this year. We still have to do stability tests on these pools; the results will be published in the CLEF 2007 post-workshop Proceedings.</p><p>Although there was quite a good participation in the monolingual Bulgarian, Czech and Hungarian tasks and the experiments report some interesting work on stemming and morphological analysis, we were very disappointed by the lack of participation in bilingual tasks for these languages. On the other hand, the interest in the task for non-European topic languages was encouraging and the results reported can be considered positively. We are currently undecided about the future of the main mono-and cross-language tasks in the ad hoc track; this will be a topic for discussion at the breakout session during the workshop.</p><p>The robust task has analyzed the performance of systems for older CLEF data under a new perspective. A larger data set which allows a more reliable comparative analysis of systems was assembled. Systems needed to avoid low performing topics. Their success was measured with the geometric mean (GMAP) which introduces a bias on poor performing topics. Results for the robust task for mono-lingual retrieval or English, French and Portuguese as well as for bilingual retrieval from English to French are reported. Robustness can also be interpreted as the fitness of a system under a variety of conditions. The definition on what robust retrieval means has to continue. All participants in CLEF 2007 are invited to engage in the discussion of the future of the robust task.</p><p>The test collections for CLEF 2000 -CLEF 2003 are now publicly available on the Evaluations and Language resources Distribution Agency (ELDA) catalog 8 . the group responsible for Czech led by Pavel Pecina 9 , and the group responsible for Hungarian led by Tamás Váradi and Gergely Bottyán. These groups worked very hard under great pressure in order to complete the heavy load of relevance assessments in time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,179.04,310.08,257.05,8.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of the relevant documents across the pools.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,250.80,720.84,113.58,8.97"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Monolingual English</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="17,257.64,407.88,99.89,8.97"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Bilingual English</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="22,240.48,709.44,134.47,8.97"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Robust Bilingual French</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="28,134.76,660.84,345.85,8.97;28,134.76,671.76,278.91,8.97"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Ad-Hoc Monolingual Hungarian. Experiments grouped according to the Tukey T Test (DOI 10.2455/TUKEY T TEST.D46DC11E6C986891646E633B0E27104C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="29,134.76,592.80,345.84,8.97;29,134.76,605.72,200.55,5.81"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Ad-Hoc Bilingual English. The figure shows the Tukey T Test (DOI 10.2455/TUKEY T TEST.4E800CB597F00B0A2CEF6D50479867EF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,185.04,115.32,245.17,72.21"><head>Table 1 .</head><label>1</label><figDesc>Test collections for the main stream Ad Hoc tasks.</figDesc><table coords="3,188.88,134.40,234.40,53.13"><row><cell>Language</cell><cell>Collections</cell></row><row><cell cols="2">Bulgarian Sega 2002, Standart 2002, Novinar 2002</cell></row><row><cell>Czech</cell><cell>Mlada fronta DNES 2002, Lidové Noviny 2002</cell></row><row><cell>English</cell><cell>LA Times 2002</cell></row><row><cell cols="2">Hungarian Magyar Hirlap 2002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,199.56,201.60,213.04,61.29"><head>Table 2 .</head><label>2</label><figDesc>Test collections for the Robust task.</figDesc><table coords="3,199.56,220.68,213.04,42.21"><row><cell>Language</cell><cell>Collections</cell></row><row><cell>English</cell><cell>LA Times 94, Glasgow Herald 95</cell></row><row><cell>French</cell><cell>ATS (SDA) 94/95, Le Monde 94</cell></row><row><cell cols="2">Portuguese Publico 94/95, Folha de Sao Paulo 94/95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,144.48,126.60,311.76,522.33"><head>Table 3 .</head><label>3</label><figDesc>Summary information about CLEF 2007 pools.</figDesc><table coords="8,144.48,147.36,311.76,135.81"><row><cell cols="2">Bulgarian Pool (DOI 10.2454/AH-BULGARIAN-CLEF2007)</cell></row><row><cell></cell><cell>19,441 pooled documents</cell></row><row><cell>Pool size</cell><cell>-18,429 not relevant documents -1,012 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>13 out of 18 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 11 out of 16 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 2 out of 2 submitted experiments</cell></row><row><cell>Assessors</cell><cell>4 assessors</cell></row><row><cell cols="2">Czech Pool</cell></row></table><note coords="8,274.08,275.69,120.04,6.97;8,144.48,308.52,41.12,8.97;8,240.96,288.00,101.55,8.97;8,247.68,303.00,133.23,8.97;8,247.68,314.04,105.03,8.97;8,240.96,329.04,35.92,8.97;8,144.60,358.32,93.71,8.97;8,240.96,345.12,143.54,8.97;8,247.68,360.24,208.56,8.97;8,247.68,371.16,184.92,8.97;8,144.60,386.40,43.56,8.97;8,240.96,387.00,42.50,8.97;8,211.56,401.04,189.52,8.97;8,144.48,435.36,41.12,8.97;8,240.96,414.72,101.55,8.97;8,247.68,429.84,133.23,8.97;8,247.68,440.76,112.23,8.97;8,240.96,455.88,35.92,8.97;8,144.60,485.04,93.71,8.97;8,240.96,471.96,148.10,8.97;8,247.68,486.96,208.56,8.97;8,247.68,498.00,194.16,8.97;8,144.60,513.24,43.56,8.97;8,240.96,513.84,42.50,8.97;8,200.52,527.76,211.72,8.97;8,144.60,639.96,43.56,8.97;9,134.76,118.29,150.64,9.96"><p>(DOI 10.2454/AH-CZECH-CLEF2007) Pool size 20,607 pooled documents -19,485 not relevant documents -762 relevant documents 50 topics Pooled Experiments 19 out of 29 submitted experiments -monolingual: 17 out of 27 submitted experiments -bilingual: 2 out of 2 submitted experiments Assessors 4 assessors English Pool (DOI 10.2454/AH-ENGLISH-CLEF2007) Pool size 24,855 pooled documents -22,608 not relevant documents -2,247 relevant documents 50 topics Pooled Experiments 20 out of 104 submitted experiments -monolingual: 10 out of 31 submitted experiments -bilingual: 10 out of 73 submitted experiments Assessors 5 assessors Hungarian Pool (DOI 10.2454/AH-HUNGARIAN-CLEF2007) Assessors calculated for CLEF are given in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,136.20,138.36,357.95,271.17"><head>Table 4 .</head><label>4</label><figDesc>CLEF 2007 ad hoc participants</figDesc><table coords="10,136.20,159.12,357.95,250.41"><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>alicante</cell><cell>U.Alicante -Languages&amp;CS</cell><cell>Spain</cell></row><row><cell>bohemia</cell><cell>U.W.Bohemia</cell><cell>Czech Republic</cell></row><row><cell cols="2">bombay-ltrc Indian Inst. Tech.</cell><cell>India</cell></row><row><cell cols="2">budapest-acad Informatics Lab</cell><cell>Hungary</cell></row><row><cell>colesun</cell><cell>COLESIR &amp; U.Sunderland</cell><cell>Spain</cell></row><row><cell>daedalus</cell><cell>Daedalus &amp; Spanish Univ. Consortium</cell><cell>Spain</cell></row><row><cell>depok</cell><cell>U.Indonesia</cell><cell>Indonesia</cell></row><row><cell>hildesheim</cell><cell>U.Hildesheim</cell><cell>Germany</cell></row><row><cell>hyderabad</cell><cell cols="2">International Institute of Information Technology (IIIT) India</cell></row><row><cell>isi</cell><cell>Indian Statistical Institute</cell><cell>India</cell></row><row><cell>jadavpur</cell><cell>Jadavpur University</cell><cell>India</cell></row><row><cell>jaen</cell><cell>U.Jaen-Intell.Systems</cell><cell>Spain</cell></row><row><cell>jhu-apl</cell><cell>Johns Hopkins University Applied Physics Lab</cell><cell>United States</cell></row><row><cell>kharagpur</cell><cell>IIT-Kharagpur-CS</cell><cell>India</cell></row><row><cell>msindia</cell><cell>Microsoft India</cell><cell>India</cell></row><row><cell>nottingham</cell><cell>U.Nottingham</cell><cell>United Kingdom</cell></row><row><cell>opentext</cell><cell>Open Text Corporation</cell><cell>Canada</cell></row><row><cell>prague</cell><cell>Charles U., Prague</cell><cell>Czech Republic</cell></row><row><cell>reina</cell><cell>U.Salamanca</cell><cell>Spain</cell></row><row><cell>stockholm</cell><cell>U. Stockholm</cell><cell>Sweden</cell></row><row><cell>unine</cell><cell>U.Neuchatel-Informatics</cell><cell>Switzerland</cell></row><row><cell>xldb</cell><cell>U.Lisbon</cell><cell>Portugal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,199.80,465.48,215.91,173.01"><head>Table 5 .</head><label>5</label><figDesc>CLEF 2007 ad hoc participants by country.</figDesc><table coords="10,236.88,486.24,138.65,152.25"><row><cell>Country</cell><cell># Participants</cell></row><row><cell>Canada</cell><cell>1</cell></row><row><cell>Cezch Republic</cell><cell>2</cell></row><row><cell>Germany</cell><cell>1</cell></row><row><cell>Hungary</cell><cell>1</cell></row><row><cell>India</cell><cell>6</cell></row><row><cell>Indonesia</cell><cell>1</cell></row><row><cell>Portugal</cell><cell>1</cell></row><row><cell>Spain</cell><cell>5</cell></row><row><cell>Sweden</cell><cell>1</cell></row><row><cell>Switzerland</cell><cell>1</cell></row><row><cell>United Kingdom</cell><cell>1</cell></row><row><cell>United States</cell><cell>1</cell></row><row><cell>Total</cell><cell>22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,134.76,115.32,342.76,236.25"><head>Table 6 .</head><label>6</label><figDesc>Breakdown of experiments into tracks and topic languages.</figDesc><table coords="11,134.76,139.80,342.76,211.77"><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) List of experiments by</cell></row><row><cell cols="3">(a) Number of experiments per track, participant.</cell><cell>topic language.</cell><cell></cell></row><row><cell>Track</cell><cell cols="2"># Part. # Runs</cell><cell cols="2">Topic Lang. # Runs</cell></row><row><cell>Monolingual-BG</cell><cell>5</cell><cell>16</cell><cell>English</cell><cell>73</cell></row><row><cell>Monolingual-CS</cell><cell>8</cell><cell>27</cell><cell>Hungarian</cell><cell>33</cell></row><row><cell>Monolingual-EN</cell><cell>10</cell><cell>31</cell><cell>Czech</cell><cell>26</cell></row><row><cell>Monolingual-HU</cell><cell>6</cell><cell>19</cell><cell>Bulgarian</cell><cell>16</cell></row><row><cell>Bilingual-X2BG</cell><cell>1</cell><cell>2</cell><cell>Indonesian</cell><cell>16</cell></row><row><cell>Bilingual-X2CS</cell><cell>1</cell><cell>2</cell><cell>French</cell><cell>14</cell></row><row><cell>Bilingual-X2EN</cell><cell>10</cell><cell>73</cell><cell>Hindi</cell><cell>13</cell></row><row><cell>Bilingual-X2HU</cell><cell>1</cell><cell>2</cell><cell>Chinese</cell><cell>12</cell></row><row><cell>Robust-Mono-EN</cell><cell>3</cell><cell>11</cell><cell>Portuguese</cell><cell>11</cell></row><row><cell>Robust-Mono-FR</cell><cell>5</cell><cell>12</cell><cell>Amharic</cell><cell>9</cell></row><row><cell>Robust-Mono-PT</cell><cell>4</cell><cell>11</cell><cell>Bengali</cell><cell>4</cell></row><row><cell>Robust-Bili-X2FR</cell><cell>3</cell><cell>9</cell><cell>Oromo</cell><cell>4</cell></row><row><cell>Robust-Training-Mono-EN</cell><cell>2</cell><cell>6</cell><cell>Marathi</cell><cell>2</cell></row><row><cell>Robust-Training-Mono-FR</cell><cell>2</cell><cell>6</cell><cell>Telugu</cell><cell>2</cell></row><row><cell>Robust-Training-Bili-X2FR</cell><cell>2</cell><cell>8</cell><cell>Total</cell><cell>235</cell></row><row><cell>Total</cell><cell></cell><cell>235</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,125.04,115.32,385.43,302.37"><head>Table 7 .</head><label>7</label><figDesc>Best entries for the monolingual track.</figDesc><table coords="12,125.04,136.08,385.43,281.61"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-MONO-BG-CLEF2007.UNINE.UNINEBG4</cell><cell>44.22%</cell></row><row><cell></cell><cell>2nd jhu-apl</cell><cell>10.2415/AH-MONO-BG-CLEF2007.JHU-APL.APLMOBGTD4</cell><cell>36.57%</cell></row><row><cell>Bulgarian</cell><cell>3rd opentext 4th alicante</cell><cell>10.2415/AH-MONO-BG-CLEF2007.OPENTEXT.OTBG07TDE 10.2415/AH-MONO-BG-CLEF2007.ALICANTE.IRNBUEXP2N</cell><cell>35.02% 29.81%</cell></row><row><cell></cell><cell>5th daedalus</cell><cell>10.2415/AH-MONO-BG-CLEF2007.DAEDALUS.BGFSBG2S</cell><cell>27.19%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>62.33%</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-MONO-CS-CLEF2007.UNINE.UNINECZ4</cell><cell>42.42%</cell></row><row><cell></cell><cell>2nd jhu-apl</cell><cell>10.2415/AH-MONO-CS-CLEF2007.JHU-APL.APLMOCSTD4</cell><cell>35.86%</cell></row><row><cell>Czech</cell><cell>3rd opentext 4th prague</cell><cell>10.2415/AH-MONO-CS-CLEF2007.OPENTEXT.OTCS07TDE 10.2415/AH-MONO-CS-CLEF2007.PRAGUE.PRAGUE01</cell><cell>34.84% 34.19%</cell></row><row><cell></cell><cell>5th daedalus</cell><cell>10.2415/AH-MONO-CS-CLEF2007.DAEDALUS.CSFSCS2S</cell><cell>32.03%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>32.44%</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-MONO-HU-CLEF2007.UNINE.UNINEHU4</cell><cell>47.73%</cell></row><row><cell></cell><cell>2nd opentext</cell><cell>10.2415/AH-MONO-HU-CLEF2007.OPENTEXT.OTHU07TDE</cell><cell>43.34%</cell></row><row><cell>Hungarian</cell><cell>3rd alicante 4th jhu-apl</cell><cell>10.2415/AH-MONO-HU-CLEF2007.ALICANTE.IRNHUEXP2N 10.2415/AH-MONO-HU-CLEF2007.JHU-APL.APLMOHUTD5</cell><cell>40.09% 39.91%</cell></row><row><cell></cell><cell>5th daedalus</cell><cell>10.2415/AH-MONO-HU-CLEF2007.DAEDALUS.HUFSHU2S</cell><cell>34.99%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>36.41%</cell></row><row><cell>English (only for Bilingual X2EN participants)</cell><cell cols="2">3rd nottingham 10.2415/AH-MONO-EN-CLEF2007.NOTTINGHAM.MONOT 4th depok 10.2415/AH-MONO-EN-CLEF2007.DEPOK.UIQTDMONO 5th hyderabad 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.ENTD OMENG07 Difference</cell><cell>43.42% 42.74% 40.57% 40.16% 9.61%</cell></row></table><note coords="12,183.36,353.88,327.11,8.97;12,181.56,364.80,51.88,8.97;12,260.04,366.68,171.02,5.81"><p>1st bombay-ltrc 10.2415/AH-MONO-EN-CLEF2007.BOMBAY-LTRC.IITB MONO TITLE DESC 44.02% 2nd jhu-apl 10.2415/AH-MONO-EN-CLEF2007.JHU-APL.APLMOENTD5</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="16,95.76,115.32,435.35,165.33"><head>Table 8 .</head><label>8</label><figDesc>Best entries for the bilingual task.</figDesc><table coords="16,95.76,136.08,435.35,144.57"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Lang.</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell>Bulgarian</cell><cell cols="2">1st jhu-apl Difference</cell><cell>en</cell><cell>10.2415/AH-BILI-X2BG-CLEF2007.JHU-APL.APLBIENBGTD4</cell><cell>7.33%</cell></row><row><cell>Czech</cell><cell cols="2">1st jhu-apl Difference</cell><cell>en</cell><cell>10.2415/AH-BILI-X2CS-CLEF2007.JHU-APL.APLBIENCSTD4</cell><cell>21.43%</cell></row><row><cell></cell><cell cols="2">1st depok</cell><cell>id</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDTOGGLEFB10D10T</cell><cell>38.78%</cell></row><row><cell></cell><cell cols="3">2nd nottingham zh</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAWOTD</cell><cell>34.56%</cell></row><row><cell>English</cell><cell cols="3">3rd jhu-apl 4th hyderabad om id</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.JHU-APL.APLBIIDENTDS 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.OMTD07</cell><cell>33.24% 29.91%</cell></row><row><cell></cell><cell cols="3">5th bombay-ltrc hi</cell><cell cols="2">10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLEDESC DICE 29.52%</cell></row><row><cell></cell><cell cols="2">Difference</cell><cell></cell><cell></cell><cell>31.37%</cell></row><row><cell>Hungarian</cell><cell cols="2">1st jhu-apl</cell><cell>en</cell><cell>10.2415/AH-BILI-X2HU-CLEF2007.JHU-APL.APLBIENHUTD5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="18,93.96,115.32,454.67,188.01"><head>Table 9 .</head><label>9</label><figDesc>Best entries for the bilingual Indian subtask.</figDesc><table coords="18,93.96,136.08,454.67,167.25"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Lang.</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell cols="3">1st bombay-ltrc hi</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLEDESC DICE</cell><cell>29.52%</cell></row><row><cell>Hindi to English</cell><cell cols="3">2nd msindia 3rd hyderabad hi hi 4th jadavpur hi 5th kharagpur hi</cell><cell cols="2">10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.2007 RBLM ALL CROSS 1000 POSSCORES 21.80% 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.HITD 15.60% 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILIHI2ENR1 10.86% 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.HINDITITLE 4.77%</cell></row><row><cell></cell><cell>6th</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Difference</cell><cell></cell><cell></cell><cell>518.87%</cell></row><row><cell>Bengali/</cell><cell cols="3">1st bombay-ltrc hi</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLEDESC DICE</cell><cell>29.52%</cell></row><row><cell>Hindi/</cell><cell cols="2">2nd msindia</cell><cell>hi</cell><cell cols="2">10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.2007 RBLM ALL CROSS 1000 POSSCORES 21.80%</cell></row><row><cell>Marathi/</cell><cell cols="3">3rd bombay-ltrc mr</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB MAR TITLE DICE</cell><cell>21.63%</cell></row><row><cell>Telugu</cell><cell cols="3">4th hyderabad te</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.TETD</cell><cell>21.55%</cell></row><row><cell>to</cell><cell cols="2">5th jadavpur</cell><cell>te</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILITE2ENR1</cell><cell>11.28%</cell></row><row><cell>English</cell><cell cols="3">6th kharagpur bn</cell><cell>10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.BENGALITITLEDESC</cell><cell>7.25%</cell></row><row><cell></cell><cell cols="2">Difference</cell><cell></cell><cell></cell><cell>307.17%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="19,101.40,115.32,401.31,76.05"><head>Table 10 .</head><label>10</label><figDesc>Data for the Robust Task 2007.</figDesc><table coords="19,101.40,134.40,401.31,56.97"><row><cell cols="2">Language Target Collection</cell><cell>Training Topic DOIs</cell><cell>Test Topic DOIs</cell></row><row><cell>English</cell><cell>LA Times 1994</cell><cell cols="2">10.2452/41-AH-10.2452/200-AH 10.2452/251-AH-10.2452/350-AH</cell></row><row><cell>French</cell><cell>Le Monde 1994 SDA 1994</cell><cell cols="2">10.2452/41-AH-10.2452/200-AH 10.2452/251-AH-10.2452/350-AH</cell></row><row><cell cols="2">Portuguese Público 1995</cell><cell>-</cell><cell>10.2452/201-AH-10.2452/350-AH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="20,92.88,115.32,457.31,234.21"><head>Table 11 .</head><label>11</label><figDesc>Best entries for the robust monolingual task.</figDesc><table coords="20,92.88,136.08,457.31,213.45"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell></cell><cell>1st reina</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2007.REINA.REINAENTDNT</cell><cell>38.97%</cell><cell>18.50%</cell></row><row><cell></cell><cell>2nd daedalus</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2007.DAEDALUS.ENFSEN22S</cell><cell>37.78%</cell><cell>17.72%</cell></row><row><cell>English</cell><cell cols="2">3rd hildesheim 10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2007.HILDESHEIM.HIMOENBRFNE 4th</cell><cell>5.88%</cell><cell>0.32%</cell></row><row><cell></cell><cell>5th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell cols="2">562.76% 5,681.25%</cell></row><row><cell></cell><cell>1st unine</cell><cell>10.2415/AH-ROBUST-MONO-FR-TEST-CLEF2007.UNINE.UNINEFR1</cell><cell>42.13%</cell><cell>14.24%</cell></row><row><cell></cell><cell>2nd reina</cell><cell>10.2415/AH-ROBUST-MONO-FR-TEST-CLEF2007.REINA.REINAFRTDET</cell><cell>38.04%</cell><cell>12.17%</cell></row><row><cell>French</cell><cell>3rd jaen 4th daedalus</cell><cell>10.2415/AH-ROBUST-MONO-FR-TEST-CLEF2007.JAEN.UJARTFR1 10.2415/AH-ROBUST-MONO-FR-TEST-CLEF2007.DAEDALUS.FRFSFR22S</cell><cell>34.76% 29.91%</cell><cell>10.69% 7.43%</cell></row><row><cell></cell><cell cols="2">5th hildesheim 10.2415/AH-ROBUST-MONO-FR-TEST-CLEF2007.HILDESHEIM.HIMOFRBRF2</cell><cell>27.31%</cell><cell>5.47%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>54.27%</cell><cell>160.33%</cell></row><row><cell></cell><cell>1st reina</cell><cell>10.2415/AH-ROBUST-MONO-PT-TEST-CLEF2007.REINA.REINAPTTDNT</cell><cell>41.40%</cell><cell>12.87%</cell></row><row><cell></cell><cell>2nd jaen</cell><cell>10.2415/AH-ROBUST-MONO-PT-TEST-CLEF2007.JAEN.UJARTPT1</cell><cell>24.74%</cell><cell>0.58%</cell></row><row><cell>Portuguese</cell><cell>3rd daedalus 4th xldb</cell><cell>10.2415/AH-ROBUST-MONO-PT-TEST-CLEF2007.DAEDALUS.PTFSPT2S 10.2415/AH-ROBUST-MONO-PT-TEST-CLEF2007.XLDB.XLDBROB16 10</cell><cell>23.75% 1.21%</cell><cell>0.50% 0.07%</cell></row><row><cell></cell><cell>5th</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell cols="2">3,321,49% 18,285.71%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="20,92.88,364.44,450.89,97.89"><head>Table 12 .</head><label>12</label><figDesc>Best entries for the robust bilingual task.</figDesc><table coords="20,92.88,385.20,450.89,77.13"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP GMAP</cell></row><row><cell></cell><cell>1st reina</cell><cell>10.2415/AH-ROBUST-BILI-X2FR-TEST-CLEF2007.REINA.REINAE2FTDNT</cell><cell>35.83% 12.28%</cell></row><row><cell></cell><cell>2nd unine</cell><cell>10.2415/AH-ROBUST-BILI-X2FR-TEST-CLEF2007.UNINE.UNINEBILFR1</cell><cell>33.50% 5.01%</cell></row><row><cell>French</cell><cell>3rd colesun 4th</cell><cell cols="2">10.2415/AH-ROBUST-BILI-X2FR-TEST-CLEF2007.COLESUN.EN2FRTST4GRINTLOGLU001 22.87% 3.57%</cell></row><row><cell></cell><cell>5th</cell><cell></cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>54.27% 243.98%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.72,634.32,199.06,8.97"><p>This is something that anyone reusing the CLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2006" xml:id="foot_1" coords="4,368.85,634.32,111.78,8.97;4,144.72,645.24,99.87,8.97;4,137.52,655.12,3.65,4.17;4,144.72,656.66,88.95,8.27"><p>ad hoc test collection needs to be very careful about. 2 http://www.doi.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.72,656.66,145.23,8.27"><p>http://trec.nist.gov/trec_eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.72,634.32,335.82,8.97;6,144.72,645.24,335.80,8.97;6,144.72,656.16,189.64,8.97"><p>Tests made on NTCIR pools in previous years have suggested that a depth of 60 in normally adequate to create stable pools, presuming that a sufficient number of runs from different systems have been included</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,144.72,634.32,335.83,8.97;9,144.72,645.24,335.88,8.97;9,144.72,656.16,224.41,8.97"><p>Ten groups submitted runs for monolingual English. We have included a graph showing the top 5 results but it must be remembered that the systems submitting these were actually focusing on the bilingual part of the task.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="15,144.72,656.16,329.54,8.97"><p>Although topics had also been requested in Tamil, in the end they were not used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="16,144.72,645.24,335.79,8.97;16,144.72,656.16,179.93,8.97"><p>Since for the other bilingual tasks only one participant submitted experiments, only the graphs for bilingual English are reported</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="36,144.72,656.66,93.63,8.27"><p>http://www.elda.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>We should like to acknowledge the enormous contribution of the groups responsible for topic creation and relevance assessment. In particulat, we thank the group responsible for the work on Bulgarian led by <rs type="person">Kiril Simov</rs> and <rs type="person">Petya Osenova</rs>,</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Experiment DOI Groups 10.2415/AH-MONO-BG-CLEF2007.UNINE.UNINEBG4 X 10.2415/AH-MONO-BG-CLEF2007.UNINE.UNINEBG1 X X 10.2415/AH-MONO-BG-CLEF2007.UNINE.UNINEBG2 X X 10.2415/AH-MONO-BG-CLEF2007.UNINE.UNINEBG3 X X 10.2415/AH-MONO-BG-CLEF2007.JHU-APL.APLMOBGTD4 X X X 10.2415/AH-MONO-BG-CLEF2007.OPENTEXT.OTBG07TDE X X X X 10.2415/AH-MONO-BG-CLEF2007.JHU-APL.APLMOBGTDN4 X X X X 10.2415/AH-MONO-BG-CLEF2007.JHU-APL.APLMOBGTD5 X X X X 10.2415/AH-MONO-BG-CLEF2007.OPENTEXT.OTBG07TD X X X X X 10.2415/AH-MONO-BG-CLEF2007.OPENTEXT.OTBG07T X X X X X 10.2415/AH-MONO-BG-CLEF2007.ALICANTE.IRNBUEXP2N X X X X X X 10.2415/AH-MONO-BG-CLEF2007.DAEDALUS.BGFSBG2S X X X X X 10.2415/AH-MONO-BG-CLEF2007.OPENTEXT.OTBG07TDNZ X X X X 10.2415/AH-MONO-BG-CLEF2007.ALICANTE.IRNBUEXP3 X X X 10.2415/AH-MONO-BG-CLEF2007.ALICANTE.IRNBUEXP2 X X 10.2415/AH-MONO-BG-CLEF2007.ALICANTE.IRNBUNEXP X  X X X 10.2415/AH-MONO-EN-CLEF2007.NOTTINGHAM.MONOT X X X 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.ENTD OMENG07 X X X X 10.2415/AH-MONO-EN-CLEF2007.DEPOK.UIQTDMONO X X X X 10.2415/AH-MONO-EN-CLEF2007.JHU-APL.APLMOENTD4 X X X X 10.2415/AH-MONO-EN-CLEF2007.MSINDIA.LM ALL MONO TD 1000 POSSCORES X X X X 10.2415/AH-MONO-EN-CLEF2007.MSINDIA.2007 LM ALL MONO 1000 POSSCORES X X X X 10.2415/AH-MONO-EN-CLEF2007.BOMBAY-LTRC.IITB MONO TITLE X X X X 10.2415/AH-MONO-EN-CLEF2007.DEPOK.UIQTMONO X X X X 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.OMENG07E X X X X 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.ENTDN OMENG07 X X X X 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.OMENG07 X X X X 10.2415/AH-MONO-EN-CLEF2007.NOTTINGHAM.MONOTD X X X X 10.2415/AH-MONO-EN-CLEF2007.MSINDIA.RBLM ALL MONO TD 1000 POSSCORES X X X X 10.2415/AH-MONO-EN-CLEF2007.HYDERABAD.ENGMONO X X X X 10.2415/AH-MONO-EN-CLEF2007.KHARAGPUR.ENGLISHTITLEDESC X X X X 10.2415/AH-MONO-EN-CLEF2007.KHARAGPUR.ENGLISHTITLEDESCNARR X X X X 10.2415/AH-MONO-EN-CLEF2007.MSINDIA.2007 RBLM ALL MONO 1000 POSSCORES X X X X 10.2415/AH-MONO-EN-CLEF2007.DEPOK.UIQDMONO X X X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN6 X X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN2 X X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN4 X X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN3 X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN5 X X X 10.2415/AH-MONO-EN-CLEF2007.BUDAPEST-ACAD.MONOEN1 X X X 10.2415/AH-MONO-EN-CLEF2007.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Experiment DOI Groups 10.2415/AH-MONO-HU-CLEF2007.UNINE.UNINEHU4 X 10.2415/AH-MONO-HU-CLEF2007.UNINE.UNINEHU2 X X 10.2415/AH-MONO-HU-CLEF2007.UNINE.UNINEHU1 X X X 10.2415/AH-MONO-HU-CLEF2007.UNINE.UNINEHU3 X X X 10.2415/AH-MONO-HU-CLEF2007.OPENTEXT.OTHU07TDE X X X X 10.2415/AH-MONO-HU-CLEF2007.JHU-APL.APLMOHUTDN4 X X X X X 10.2415/AH-MONO-HU-CLEF2007.JHU-APL.APLMOHUTD5 X X X X X 10.2415/AH-MONO-HU-CLEF2007.ALICANTE.IRNHUEXP2N X X X X X 10.2415/AH-MONO-HU-CLEF2007.OPENTEXT.OTHU07TD X X X X X 10.2415/AH-MONO-HU-CLEF2007.ALICANTE.IRNHUEXP3 X X X X 10.2415/AH-MONO-HU-CLEF2007.JHU-APL.APLMOHUTD4 X X X X 10.2415/AH-MONO-HU-CLEF2007.ALICANTE.IRNHUEXP2 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAWOTD X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDINTERSECTIONUNIONSYNFB5D10T X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDDESYNFB5D10T X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JHU-APL.APLBIIDENTDS X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDINTERSECTIONUNIONSYNFB10D10T X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTTOGGLE X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDDESYNFB10D5T X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAUWOTD X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAUWOT X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JHU-APL.APLBIIDENTD5 X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JHU-APL.APLBIIDENTD4 X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAWOT X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.OMTD07 X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.OMTDN07 X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JHU-APL.APLBIIDENTDW X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLEDESC DICE X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDINTERSECTIONUNIONSYN X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAWTD X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.COOTD X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLEDESC PMI X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.OMT07 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.ALLTRANSTD X X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAUWTD X X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAUWT X X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.GRAWT X X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.COOT X X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLE DICE X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.NOTTINGHAM.ALLTRANST X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.TETD X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.2007 LM ALL CROSS 1000 POSSCORES X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB MAR TITLE DICE X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.2007 RBLM ALL CROSS 1000 POSSCORES X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.RBLM ALL CROSS TD 1000 POSSCORES X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.NOST OMTDN07 X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLE PMI X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.LM ALL CROSS TD 1000 POSSCORES X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB MAR TITLE PMI X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI6 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI2 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI4 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI1 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI3 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI5 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.HITD X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 6 X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 4 X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 2 X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 5 X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 1 X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 3 X X X 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILITE2ENR1 X X 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILIHI2ENR1 X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN8 X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.2007 RBLM ALL CROSS 1000 POSSCORES X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.RBLM ALL CROSS TD 1000 POSSCORES X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.NOST OMTDN07 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB HINDI TITLE PMI X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.MSINDIA.LM ALL CROSS TD 1000 POSSCORES X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BOMBAY-LTRC.IITB MAR TITLE PMI X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI6 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI2 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI4 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI1 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI3 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING WIKI5 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.HYDERABAD.HITD X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 6 X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 4 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 2 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 5 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 1 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.BUDAPEST-ACAD.BILING 3 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILITE2ENR1 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILIHI2ENR1 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN8 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.JADAVPUR.AHBILIBN2ENR1 X X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN6 X X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN7 X X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN4 X X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN2 X X X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN5 X X X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.BENGALITITLEDESC X X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.BENGALITITLEDESCNARR X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN1 X X X 10.2415/AH-BILI-X2EN-CLEF2007.STOCKHOLM.DSV AMH BLNG RUN3 X X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.HINDITITLEDESCNARR X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.HINDITITLE X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.BENGALITITLE X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDENGFB5D5T10D19T X X 10.2415/AH-BILI-X2EN-CLEF2007.KHARAGPUR.HINDITITLEDESC X X 10.2415/AH-BILI-X2EN-CLEF2007.DEPOK.UIQTDENGPSEUDOTRANS20D14T X </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="37,142.88,209.76,337.68,8.97;37,151.56,220.80,314.61,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="37,214.88,209.76,124.61,8.97">The DOI Handbook -Edition 4</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paskin</surname></persName>
		</author>
		<idno type="DOI">10.1000/186[lastvisited2007</idno>
		<ptr target="http://dx.doi.org/10.1000/186[lastvisited2007" />
		<imprint>
			<date type="published" when="2006-08-30">August 30. 2006</date>
			<publisher>International DOI Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,231.72,337.61,8.97;37,151.56,242.76,328.98,8.97;37,151.56,253.68,329.03,8.97;37,151.56,264.60,329.03,8.97;37,151.56,275.52,203.67,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="37,216.10,231.72,136.34,8.97">CLEF 2003 -Overview of results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="37,276.31,242.76,204.23,8.97;37,151.56,253.68,329.03,8.97;37,151.56,264.60,148.87,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="37,307.05,264.60,168.26,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="44" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,286.56,337.69,8.97;37,151.56,297.48,329.11,8.97;37,151.56,308.52,318.92,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="37,217.17,286.56,263.40,8.97;37,151.56,297.48,24.61,8.97">Sampling Precision to Depth 10000: Evaluation Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,318.55,297.48,162.12,8.97;37,151.56,308.52,16.83,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">2007. September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,319.44,337.61,8.97;37,151.56,330.36,328.82,8.97;37,151.56,341.40,328.87,8.97;37,151.56,352.32,329.05,8.97;37,151.56,363.24,233.79,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="37,261.56,319.44,153.69,8.97">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="37,322.37,330.36,158.01,8.97;37,151.56,341.40,328.87,8.97;37,151.56,352.32,180.07,8.97">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="37,338.73,352.32,141.88,8.97;37,151.56,363.24,26.39,8.97">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,374.28,337.86,8.97;37,151.56,385.20,328.91,8.97;37,151.56,396.12,328.82,8.97;37,151.56,407.16,81.23,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="37,272.60,374.28,208.14,8.97;37,151.56,385.20,129.62,8.97">Appendix A: Results of the Core Tracks -Ad-hoc Bilingual and Monolingual Tasks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,407.51,385.20,72.96,8.97;37,151.56,396.12,106.38,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,418.08,337.86,8.97;37,151.56,429.00,329.08,8.97;37,151.56,440.04,328.90,8.97;37,151.56,450.96,134.73,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="37,272.84,418.08,207.90,8.97;37,151.56,429.00,166.46,8.97">Appendix B: Results of the Core Tracks -Ad-hoc Robust Bilingual and Monolingual Tasks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,455.73,429.00,24.91,8.97;37,151.56,440.04,162.78,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,461.88,337.79,8.97;37,151.56,472.92,328.95,8.97;37,151.56,483.84,271.28,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="37,249.07,461.88,212.94,8.97">Stemming Approaches for East European Languages</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dolamic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,264.55,472.92,184.07,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,494.76,337.56,8.97;37,151.56,505.80,329.06,8.97;37,151.56,516.72,328.82,8.97;37,151.56,527.64,81.23,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="37,250.76,494.76,229.68,8.97;37,151.56,505.80,138.03,8.97">Applying Query Expansion techniques to Ad Hoc Monolingual tasks with the IR-n system</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Noguera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,420.47,505.80,60.15,8.97;37,151.56,516.72,113.95,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.88,538.68,337.87,8.97;37,151.56,549.60,328.95,8.97;37,151.56,560.52,271.28,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="37,295.38,538.68,180.15,8.97">Hungarian and Czech Stemming using YASS</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,271.15,549.60,178.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.54,569.28,338.09,11.25;37,151.56,582.48,328.71,8.97;37,151.56,593.40,243.08,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="37,241.39,571.56,193.74,8.97">Charles University at CLEF 2007 Ad-Hoc Track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Češka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="37,235.88,582.48,184.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="37,142.54,604.44,338.11,8.97;37,151.56,615.36,329.09,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="37,248.36,604.44,232.30,8.97;37,151.56,615.36,325.25,8.97">Czech Monolingual Information Retrieval Using Off-The-Shelf Components -the University of West Bohemia at CLEF 2007 Ad-Hoc track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ircing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Müuller</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="37,151.56,626.28,328.95,8.97;37,151.56,637.32,271.28,8.97" xml:id="b11">
	<monogr>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
		<title level="m" coord="37,271.15,626.28,178.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,119.04,337.93,8.97;38,151.56,130.08,329.07,8.97;38,151.56,141.00,294.80,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="38,306.55,119.04,173.93,8.97">Evaluating Language Resources for CLEF</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hayurani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adriani</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,296.59,130.08,179.27,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">2007. September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,152.76,337.99,8.97;38,151.56,163.80,329.07,8.97;38,151.56,174.72,294.80,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="38,298.87,152.76,181.67,8.97;38,151.56,163.80,20.09,8.97">Ambiguity and Unknown Term Translation in CLIR</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Truran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brailsford</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,298.38,163.80,177.49,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,186.48,338.11,8.97;38,151.56,197.52,329.07,8.97;38,151.56,208.44,294.80,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="38,206.87,186.48,273.79,8.97;38,151.56,197.52,16.83,8.97">Amharic-English Information Retrieval with Pseudo Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Argaw</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,296.59,197.52,179.28,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,220.32,337.81,8.97;38,151.56,231.24,328.92,8.97;38,151.56,242.16,328.82,8.97;38,151.56,253.20,81.23,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="38,368.87,220.32,111.48,8.97;38,151.56,231.24,103.51,8.97">Performing Cross-Language Retrieval with Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schönhofen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benczúr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bíró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Csalogány</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,402.51,231.24,77.97,8.97;38,151.56,242.16,106.38,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,264.96,338.02,8.97;38,151.56,275.88,329.11,8.97;38,151.56,286.92,318.92,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="38,259.28,264.96,221.28,8.97;38,151.56,275.88,24.61,8.97">Oromo-English Information Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,318.55,275.88,162.12,8.97;38,151.56,286.92,16.83,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">2007. September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,298.68,338.19,8.97;38,151.56,309.60,328.83,8.97" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="38,175.30,309.60,281.07,8.97">Bengali, Hindi and Telugu to English Ad-hoc Bilingual task at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Naskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Godavarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,151.56,320.64,328.95,8.97;38,151.56,331.56,271.28,8.97" xml:id="b18">
	<monogr>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
		<title level="m" coord="38,271.15,320.64,178.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,343.32,337.92,8.97;38,151.56,354.36,328.93,8.97;38,151.56,365.28,329.14,8.97;38,151.56,376.20,35.63,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="38,284.96,343.32,195.50,8.97;38,151.56,354.36,69.86,8.97">Cross-Lingual Information Retrieval System for Indian Languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumaran</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,359.34,354.36,121.14,8.97;38,151.56,365.28,60.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,388.08,337.98,8.97;38,151.56,399.00,329.07,8.97;38,151.56,410.04,328.71,8.97;38,151.56,420.96,243.08,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="38,439.41,388.08,41.12,8.97;38,151.56,399.00,266.53,8.97">Hindi and Marathi to English Cross Language Information Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Chinnakotla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ranadive</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">P</forename><surname>Damani</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,235.88,410.04,184.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">2007. September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,432.72,337.95,8.97;38,151.56,443.76,329.11,8.97;38,151.56,454.68,318.92,8.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="38,247.88,432.72,232.62,8.97;38,151.56,443.76,40.34,8.97">IIIT Hyderabad at CLEF 2007 -Adhoc Indian Language CLIR task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,316.98,443.76,163.69,8.97;38,151.56,454.68,16.83,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,466.44,338.07,8.97;38,151.56,477.48,329.07,8.97;38,151.56,488.40,328.71,8.97;38,151.56,499.32,243.08,8.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="38,407.63,466.44,72.99,8.97;38,151.56,477.48,277.69,8.97">Bengali and Hindi to English Cross-language Text Retrieval under Limited Resources</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/[lastvisited2007" />
	</analytic>
	<monogr>
		<title level="m" coord="38,235.88,488.40,184.43,8.97">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007-09-05">September 5. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,511.20,338.06,8.97;38,151.56,522.12,329.00,8.97;38,151.56,533.04,328.86,8.97" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="38,213.70,511.20,158.97,8.97">On GMAP: and Other Transformations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="38,256.98,522.12,223.58,8.97;38,151.56,533.04,153.16,8.97">Proc. 15th International Conference on Information and Knowledge Management (CIKM 2006)</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tsotras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>15th International Conference on Information and Knowledge Management (CIKM 2006)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,544.92,337.93,8.97;38,151.56,555.84,22.85,8.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="38,223.90,544.92,144.69,8.97">The TREC Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="38,379.97,544.92,56.22,8.97">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,567.72,338.07,8.97;38,151.56,578.64,329.08,8.97;38,151.56,589.56,329.11,8.97;38,151.56,600.60,60.66,8.97" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="38,195.58,567.72,228.68,8.97">Why do Successful Search Systems Fail for Some Topics</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="38,413.51,578.64,67.13,8.97;38,151.56,589.56,196.35,8.97">Proc. 2007 ACM Symposium on Applied Computing (SAC 2007)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Wan Koo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</editor>
		<meeting>2007 ACM Symposium on Applied Computing (SAC 2007)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="872" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="38,142.54,612.36,338.12,8.97;38,151.56,623.28,328.93,8.97;38,151.56,634.32,329.10,8.97;38,151.56,645.24,329.08,8.97;38,151.56,656.16,107.47,8.97" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="38,260.12,612.36,220.54,8.97;38,151.56,623.28,90.82,8.97">Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="38,221.84,634.32,258.82,8.97;38,151.56,645.24,253.13,8.97">Proc. 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting>28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,142.54,119.04,338.10,8.97" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="39,190.42,119.04,273.27,8.97">Using Statistical Testing in the Evaluation of Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="39,151.56,130.08,329.08,8.97;39,151.56,141.00,329.06,8.97;39,151.56,151.92,240.05,8.97" xml:id="b28">
	<monogr>
		<title level="m" coord="39,347.91,130.08,132.73,8.97;39,151.56,141.00,329.06,8.97;39,151.56,151.92,52.73,8.97">Proc. 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Korfhage</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Rasmussen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<meeting>16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="39,142.54,162.96,338.07,8.97;39,151.56,173.88,92.99,8.97" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Conover</surname></persName>
		</author>
		<title level="m" coord="39,215.01,162.96,137.33,8.97">Practical Nonparametric Statistics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>1st edn</note>
</biblStruct>

<biblStruct coords="39,142.54,184.80,338.11,8.97;39,151.56,195.84,329.08,8.97;39,151.56,206.76,72.24,8.97" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lütkepohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<title level="m" coord="39,430.17,184.80,50.49,8.97;39,151.56,195.84,174.62,8.97">Introduction to the Theory and Practice of Econometrics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>2nd edn</note>
</biblStruct>

<biblStruct coords="39,142.54,217.68,337.94,8.97;39,151.56,228.72,329.06,8.97;39,151.56,239.64,328.75,8.97" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="39,236.14,217.68,244.34,8.97;39,151.56,228.72,35.87,8.97">The Pragmatics of Information Retrieval Experimentation, Revisited</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tague-Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="39,341.92,228.72,134.82,8.97">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Spack</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Willett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publisher, Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
