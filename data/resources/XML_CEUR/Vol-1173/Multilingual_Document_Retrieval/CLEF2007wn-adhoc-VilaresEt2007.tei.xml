<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.77,148.63,409.13,15.60;1,201.31,170.63,200.19,15.51">CoLesIR at CLEF 2007: from English to French via Character N -Grams</title>
				<funder ref="#_Q3vvRUS">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Xunta de Galicia</orgName>
				</funder>
				<funder ref="#_ZJcjG8p #_w93t7bz">
					<orgName type="full">Ministerio de Educación y Ciencia and FEDER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,126.04,204.56,65.41,9.02"><forename type="first">Jesús</forename><surname>Vilares</surname></persName>
							<email>jvilares@udc.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of A Coruña Campus de Elviña</orgName>
								<address>
									<postCode>15071</postCode>
									<settlement>-A Coruña (</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.87,204.56,86.50,9.02"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Oakes</surname></persName>
							<email>michael.oakes@sunderland.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Sunderland St. Peter&apos;s Campus</orgName>
								<address>
									<postCode>SR6 0DD (</postCode>
									<settlement>Sunderland</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.44,204.56,76.13,9.02"><forename type="first">Manuel</forename><surname>Vilares</surname></persName>
							<email>vilares@uvigo.es</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Vigo Campus As</orgName>
								<address>
									<addrLine>Lagoas s/n</addrLine>
									<postCode>32004</postCode>
									<settlement>Ourense</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.77,148.63,409.13,15.60;1,201.31,170.63,200.19,15.51">CoLesIR at CLEF 2007: from English to French via Character N -Grams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E48E3632132B552EDF8FF5742FADB3B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing-Indexing methods</term>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval-Query formulation</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing-Machine translation</term>
					<term>Text analysis Algorithms, Measurement, Performance, Experimentation Cross-Language Information Retrieval, character n-grams, translation algorithms, alignment algorithms, association measures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is an extension of our proposal originally presented in CLEF 2006, which, unfortunately, could not be ready on time for the workshop. We describe here a knowledge-light approach for query translation in Cross-Language Information Retrieval systems. This proposal itself can be considered as an extension of the previous work of the Johns Hopkins University Applied Physics Lab, preserving its advantages but avoiding its main drawbacks. As in their original proposal, our work is based on the direct translation of character n-grams, avoiding in this way the need for word normalization during indexing or translation, and also dealing with out-of-vocabulary words. Moreover, since such a solution does not rely on language-specific processing, it can be used with languages of very different natures even when linguistic information and resources are scarce or unavailable. Nevertheless, in contrast with the original approach, our proposal is much faster and transparent, making extensive use of freely available resources. The system has been tested in the robust ad-hoc English-to-French bilingual task, obtaining encouraging results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This work is an extension of the proposal originally presented by our group in the previous CLEF edition, a new knowledge-light approach for query translation in Cross-Language Information Retrieval (CLIR) systems based on the direct translation of character n-grams. Such a proposal itself can be considered as an extension of the previous work of the Johns Hopkins University Applied Physics Lab (JHU/APL) on the employment of overlapping character n-grams for indexing documents <ref type="bibr" coords="2,139.82,147.32,10.50,9.96" target="#b4">[7,</ref><ref type="bibr" coords="2,153.64,147.32,7.00,9.96" target="#b5">8]</ref>.</p><p>The interest in using overlapping character n-grams comes from the fact that it provides a surrogate means to normalize word forms and to allow to manage languages of very different natures without further processing. Such a knowledge-light approach does not rely on languagespecific processing, and it can be used even when linguistic information and resources are scarce or unavailable.</p><p>In the case of monolingual retrieval, the employment of n-grams is quite straightforward, since both queries and documents are just tokenized into overlapping n-grams instead of words: the word tomato, for example, is split into -tom-, -oma-, -mat-and -ato-. The resulting n-grams are then processed by the retrieval engine either for indexing or querying. Nevertheless, when extending its use to the case of CLIR, an extra translation phase is needed during querying.</p><p>Aiming to avoid some of the limitations of classic dictionary-based translation methods, such as the need for word normalization or the inability to handle out-of-vocabulary words, JHU/APL researchers developed a direct n-gram translation algorithm which allows translation not at the word level but at the n-gram level <ref type="bibr" coords="2,239.88,314.69,9.94,9.96" target="#b5">[8]</ref>. This n-gram translation algorithm takes as input a parallel corpus, aligned at the paragraph (or document) level and extracts candidate translations as follows. Firstly, for each candidate n-gram term to be translated, paragraphs containing this term in the source language are identified. Next, their corresponding paragraphs in the target language are also identified and, using an ad-hoc statistical measure, a translation score is calculated for each of the terms occurring in the target language texts. Finally, the target n-gram with the highest translation score is selected as the potential translation of the source n-gram. Nevertheless, the whole process was found to be very slow, making the testing of new developments difficult: it could take several days in the case of working with 5-grams, for example.</p><p>This paper describes a new direct n-gram alignment proposal we have developed both to speed up the process and to make the system more transparent. The article is structured as follows. Firstly, Sect. 2 describes our approach. Next, in Sect. 3, our proposal is evaluated. Finally, in Sect. 4, we present our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of the system</head><p>Taking as our model the system designed by JHU/APL, we developed our own n-gram based retrieval system, trying to preserve the advantages of the original proposal but avoiding its main drawbacks. Moreover, instead of the ad-hoc resources developed for the original system <ref type="bibr" coords="2,487.36,536.83,10.50,9.96" target="#b4">[7,</ref><ref type="bibr" coords="2,502.10,536.83,6.99,9.96" target="#b5">8]</ref>, our system has been built using freely available resources when possible in order to make it more transparent and to minimize effort. This way, we use the open-source retrieval platform Terrier [1] instead of the ad-hoc retrieval system employed by the original design, and the well-known Europarl parallel corpus<ref type="foot" coords="2,257.82,583.81,3.97,4.84" target="#foot_0">1</ref>  <ref type="bibr" coords="2,266.03,584.65,10.50,9.96" target="#b1">[4]</ref> is used as training data instead of the ad-hoc corpus employed by JHU/APL.</p><p>Nevertheless, the main difference is the n-gram alignment algorithm itself, which now consists of two phases. In the first phase, the slowest one, the input parallel corpus is aligned at the word-level using the well-known statistical tool GIZA++ <ref type="bibr" coords="2,342.21,632.47,9.94,9.96" target="#b6">[9]</ref>, obtaining as output the translation probabilities between the different source and target language words. In our case, after some initial experiments <ref type="bibr" coords="2,146.06,656.38,14.59,9.96" target="#b8">[11]</ref>, we have opted for a bidirectional alignment <ref type="bibr" coords="2,363.49,656.38,10.50,9.96" target="#b2">[5]</ref> which considers a (w EN , w SP ) English-to-Spanish word alignment only if there also exists a corresponding (w SP , w EN ) Spanishto-English alignment. This way the subsequent processing will be focused only on those words whose translation seems less ambiguous, reducing both the number of input word pairs to be processed and output n-gram pairs to be obtained by more than 60%. This reduction allows us to reduce greatly both computing and storage resources -including processing time.</p><p>Next, prior to the second phase, heuristics can be applied -if desired-for refining or modifying the word-to-word translation scores calculated by GIZA++. In our case, we have removed those least-probable word alignments from the input (those with a word translation probability less than a threshold W , with W =0.15) <ref type="bibr" coords="3,263.54,171.22,14.59,9.96" target="#b8">[11]</ref>. Such pruning leads to a considerable extra reduction of processing time and storage space: a reduction of over 90% in the number of both input word pairs processed and output n-gram pairs aligned.</p><p>Finally, in the second phase, n-gram translation scores are computed employing statistical association measures <ref type="bibr" coords="3,185.79,219.05,9.94,9.96" target="#b3">[6]</ref>, taking as input the translation probabilities previously calculated by GIZA++.</p><p>As we can see, this first step acts as an initial filter, since only those n-gram pairs corresponding to aligned words will be considered, whereas in the original JHU/APL approach all n-gram pairs corresponding to aligned paragraphs were considered. This approach increases the speed of the process by concentrating most of the complexity in the word-level alignment phase, allowing ngram alignment techniques to be easily tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word-level alignment using association measures</head><p>Our n-gram alignment algorithm is an extension of the way association measures can be used for creating bilingual word dictionaries taking as input parallel collections aligned at the paragraph level <ref type="bibr" coords="3,114.08,360.97,14.58,9.96" target="#b7">[10]</ref>. In this context, given a word pair (w s , w t ) -w s standing for the source language word, and w t for its candidate target language translation-, their cooccurrence frequency can be organized in a contingency table resulting from a cross-classification of their cooccurrences in the input aligned corpus:</p><formula xml:id="formula_0" coords="3,229.91,417.85,142.69,58.48">T = wt T = wt S = ws O 11 O 12 = R 1 S = ws O 21 O 22 = R 2 = C 1 = C 2 = N</formula><p>As shown, the first row accounts for those instances where the source language paragraph contains w s , while the first column accounts for those instances where the target language paragraph contains w t . The cell counts are called the observed frequencies: O 11 , for example, stands for the number of aligned paragraphs where the source language paragraph contains w s and the target language paragraph contains w t ; O 12 stands for the number of aligned paragraphs where the source language paragraph contains w s but the target language paragraph does not contain w t ; and so on. The total number of word pairs considered -or sample size N -is the sum of the observed frequencies. The row totals, R 1 and R 2 , and the column totals, C 1 and C 2 , are also called marginal frequencies and O 11 is called the joint frequency.</p><p>Once the contingency table has been built, different association measures can be easily calculated for each word pair. The most promising pairs, those with the highest association measures, are stored in the bilingual dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptations for n-gram-level alignment</head><p>We have described how to compute and use association measures for generating bilingual word dictionaries from parallel corpora. However, in our case we do not start with aligned paragraphs composed of words, but aligned words -previously aligned through GIZA++-composed of character n-grams. A first choice for adapting the previous word-level alignment algorithm to the case of n-grams could be just to adapt the contingency table to the new context, by considering that we are managing n-gram pairs (g s , g t ) cooccurring in aligned words instead of word pairs (w s , w t ) cooccurring in aligned paragraphs. So, contingency tables should be adapted accordingly: O 11 , for example, should be re-formulated as the number of aligned word pairs where the source language word contains n-gram g s and the target language word contains n-gram g t . However, we do not have real instances of n-gram cooccurrences at aligned words, but just probable ones, since GIZA++ uses a statistical alignment model which computes a translation probability for each cooccurring word pair <ref type="bibr" coords="4,281.82,159.27,9.94,9.96" target="#b6">[9]</ref>. So, the same word may be aligned with several translation candidates, each one with a given probability. Taking as example the case of the English words milk and milky, and the Spanish words leche (milk), lechoso (milky) and tomate (tomato), a possible output word-level alignment -with its corresponding probabilitieswould be: Our proposal consists of weighting the likelihood of a cooccurrence according to the probability of its containing word alignments. So, the resulting contingency tables corresponding to the n-gram pairs (-milk-, -lech-) and (-milk-, -toma-) are as follows: Notice that, for example, the O 11 frequency corresponding to (-milk-, -lech-) is not 2 as might be expected, but 1.90. This is because the pair appears in two word alignments -milk-leche and milky-lechoso-, but each cooccurrence in an alignment has been weighted according to its translation probability: O11 = 0.98 (for milk-leche) + 0.92 (for milky-lechoso) = 1.90 .</p><formula xml:id="formula_1" coords="4,124.94,328.75,254.80,26.80">T = -lech- T = -lech- S = -</formula><p>Once the contingency tables have been generated, the association measures corresponding to each n-gram pair can be computed. In contrast with the original JHU/APL approach <ref type="bibr" coords="4,457.81,549.66,10.50,9.96" target="#b4">[7,</ref><ref type="bibr" coords="4,472.53,549.66,6.99,9.96" target="#b5">8]</ref>, which used an ad-hoc measure, ours uses three of the most extensively used standard measures: the Dice coefficient (Dice), mutual information (MI ), and log-likelihood (logl ), which are defined by the following equations <ref type="bibr" coords="4,176.59,585.52,9.94,9.96" target="#b3">[6]</ref>:</p><formula xml:id="formula_2" coords="4,97.77,613.49,415.22,23.73">Dice(gs, gt) = 2O11 R1 + C1 . (1) M I(gs, gt) = log N O11 R1C1 . (2) logl(gs, gt) = 2 X i,j Oij log N Oij RiCj .<label>(3)</label></formula><p>If using the Dice coefficient, for example, we find that the association measure of the pair (-milk-, -lech-) -the correct one-is much higher than that of the pair (-milk-, -toma-) -the wrong one:</p><p>Dice(-milk-, -lech-) = 2 * 1.90 6.09+2.82 = 0.43 . Dice(-milk-, -toma-) = 2 * 0.15 6.09+0.15 = 0.05 .</p><p>Notice that if we consider that a real existing cooccurrence instance corresponds to a 100% probability, we can think about the original word-based algorithm described in Sect. 2.1 as a particular case of the generalized n-gram-based algorithm we have proposed here with n=∞. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Since the lack of time did not allow us to have our n-gram direct translation tool ready on time for the past CLEF 2006 workshop <ref type="bibr" coords="5,225.71,332.07,14.58,9.96" target="#b7">[10]</ref>, this year we have taken part again in the robust ad-hoc task, specifically in the English-to-French bilingual task, in order to present the current development of our work. The robust task is essentially an ad-hoc task which re-uses the topics and collections from past CLEF editions <ref type="bibr" coords="5,156.95,379.89,9.94,9.96">[2]</ref>. In this case, the French document collection is formed by 87,191 news reports (243 MB) provided by Le Monde and SDA and corresponding to the year 1994. The English topics set consists of 200 topics divided into two subsets: a training topics subset to be used for tuning purposes, formed by 100 topics (C041-C140); and a test topics subset for testing purposes, formed by the remaining 100 topics (C251-C350). Moreover, only title and description topic fields were used in the submitted queries.</p><p>With respect to the indexing process, documents were simply split into n-grams and indexed, as were the queries. We have used 4-grams as a compromise n-gram size after studying the results previously obtained by the JHU/APL group <ref type="bibr" coords="5,289.38,475.53,10.50,9.96" target="#b4">[7,</ref><ref type="bibr" coords="5,303.66,475.53,7.74,9.96" target="#b5">8]</ref> using different lengths. Before that, the text had been lowercased and punctuation marks were removed <ref type="bibr" coords="5,344.47,487.48,9.94,9.96" target="#b5">[8]</ref>, but not diacritics. The open-source Terrier platform [1] was used as retrieval engine with a InL2<ref type="foot" coords="5,365.18,498.60,3.97,4.84" target="#foot_2">2</ref> ranking model <ref type="bibr" coords="5,439.35,499.45,9.94,9.96" target="#b0">[3]</ref>. No stopword removal or query expansion were applied at this point.</p><p>For querying, the source language topic is firstly split into n-grams. Next, these n-grams are replaced by their N most probable alignments. <ref type="foot" coords="5,289.07,534.46,3.97,4.84" target="#foot_3">3</ref> The resulting translated topics are then submitted to the retrieval system. 4 Because of the lack of time, we could not tune the N value for this new set of English-to-French experiments, so we decided to take those values used in our previous English-to-Spanish experiments <ref type="bibr" coords="5,230.50,571.18,14.58,9.96" target="#b8">[11]</ref>:</p><formula xml:id="formula_3" coords="5,238.80,588.94,125.29,33.88">Dice coefficient N =1 Mutual Information N =10 Log-likelihood N =1</formula><p>Finally, Fig. <ref type="figure" coords="5,146.74,630.27,4.97,9.96">1</ref> and Fig. <ref type="figure" coords="5,195.54,630.27,4.97,9.96" target="#fig_0">2</ref> show the results obtained for each association measure: the Dice coefficient (EN2FR Dice), Mutual Information (EN2FR MI), and log-likelihood (EN2FR logl). We also show the results for two baselines: by querying the French index with the initial English topics split into 4-grams (EN) -allowing us to measure the impact of casual matches-, and by querying the index using the French topics split into 4-grams (FR) -i.e. a French monolingual run and our ideal performance goal. Notice that mean average precision (MAP) values are also given. These results show that the log-likelihood measure obtains the best results for both topic sets, although no significant difference is found with respect to Dice. <ref type="foot" coords="6,350.00,323.42,3.97,4.84" target="#foot_5">5</ref> On the other hand, both approaches perform significantly better than mutual information.</p><p>Although we still need to improve our results in order to reach our ideal performance goal, our current results are encouraging, since it must be taken into account that these are still our first experiments, so the margin for improvement is still great.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>This paper extends the proposal originally presented in CLEF 2006 for the development of a CLIR system which uses character n-grams not only as indexing units, but also as translation units. This system was inspired by the work of the Johns Hopkins University Applied Physics Lab <ref type="bibr" coords="6,470.20,450.76,10.50,9.96" target="#b4">[7,</ref><ref type="bibr" coords="6,483.93,450.76,6.99,9.96" target="#b5">8]</ref>, but tries to preserve its advantages while avoiding its main drawbacks. As in their original proposal, our work is based on the direct translation of character n-grams, avoiding in this way the need for word normalization during indexing or translation, and also dealing with out-of-vocabulary words.</p><p>Moreover, since such a solution does not rely on language-specific processing, it can be used with languages of very different natures even when linguistic information and resources are scarce or unavailable. Nevertheless, in contrast with the original approach, our proposal is much faster and transparent, making extensive use of freely available resources.</p><p>So, the n-gram alignment algorithm described consists of two phases. In the first phase, the slowest one, word-level alignment of the text is made through a statistical alignment tool. In the second phase, n-gram translation scores are computed employing statistical association measures, taking as input the translation probabilities calculated in the previous phase. This new approach speeds up the training process, concentrating most of the complexity in the word-level alignment phase, making the testing of new association measures for n-gram alignment easier.</p><p>With respect to our future work, new tests with other languages of different characteristics are being prepared in order to complete the tuning of the system, including the possibility of removing high or low-frequency n-grams, the employment of relevance feedback, or the use of pre or post-translation expansion techniques in the case of translingual runs <ref type="bibr" coords="6,407.59,654.00,9.94,9.96" target="#b5">[8]</ref>.</p><p>(PGIDIT05PXIC30501PN, PGIDIT05SIN044E, and Rede Galega de Procesamento da Linguaxe e Recuperación de Información). The authors would also like to thank John I. Tait for his support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,90.00,267.97,422.59,9.96;6,90.00,279.93,31.24,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision at top D documents graphs obtained for the training (left) and test topic sets (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,93.30,114.71,415.97,163.22"><head></head><label></label><figDesc>Figure 1: Precision vs. Recall graphs obtained for the training (left) and test topic sets (right).</figDesc><table coords="5,94.34,114.71,409.62,142.59"><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">EN (MAP=0.2567)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">EN (MAP=0.0947)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">FR (MAP=0.4270)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">FR (MAP=0.1956)</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell cols="6">EN2FR Dice (MAP=0.3219) EN2FR MI (MAP=0.2627)</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell cols="6">EN2FR Dice (MAP=0.1419) EN2FR MI (MAP=0.0985)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">EN2FR logl (MAP=0.3293)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">EN2FR logl (MAP=0.1442)</cell><cell></cell></row><row><cell>Precision (P)</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision (P)</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Recall (Re)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Recall (Re)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.25,711.31,344.74,7.97"><p>This corpus was extracted from the proceedings of the European Parliament, containing up to</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_1" coords="2,463.35,711.31,49.26,7.97;2,90.00,720.77,422.57,7.97;2,90.00,730.24,277.19,7.97"><p>million words per language. It includes versions in 11 European languages: Romance (French, Italian, Spanish, Portuguese), Germanic (English, Dutch, German, Danish, Swedish), Greek and Finnish.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="5,105.25,708.63,304.53,7.97"><p>Inverse Document Frequency model with Laplace after-effect and normalization 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="5,105.25,718.14,111.16,7.97"><p>With N ∈ {1, 2,3, 5, 10, 20, 30,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4" coords="5,217.82,718.14,55.88,7.97;5,101.09,726.78,411.52,8.84;5,90.00,737.11,422.59,7.97;5,90.00,746.57,422.59,7.97"><p>40, 50, 75, 100}.<ref type="bibr" coords="5,101.09,726.78,3.65,4.87" target="#b1">4</ref> A second selection algorithm, consisting of taking those alignments with a probability greater or equal than a threshold T , was also used in previous experiments<ref type="bibr" coords="5,292.82,737.11,12.39,7.97" target="#b8">[11]</ref>. Nevertheless, this threshold-based approach has been dismissed because of the difficulty for fixing T and because if performed not as well as this top-rank-based approach.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="6,105.25,740.34,282.62,7.97"><p>Two-tailed T-tests over MAPs with α=0.05 have been used along this work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research has been partially funded by the <rs type="funder">European Union</rs> (<rs type="grantNumber">FP6-045389</rs>), <rs type="funder">Ministerio de Educación y Ciencia and FEDER</rs> (<rs type="grantNumber">TIN2004-07246-C03</rs> and <rs type="grantNumber">HUM2007-66607-C04</rs>), and <rs type="funder">Xunta de Galicia</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Q3vvRUS">
					<idno type="grant-number">FP6-045389</idno>
				</org>
				<org type="funding" xml:id="_ZJcjG8p">
					<idno type="grant-number">TIN2004-07246-C03</idno>
				</org>
				<org type="funding" xml:id="_w93t7bz">
					<idno type="grant-number">HUM2007-66607-C04</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.46,218.02,402.18,9.96;7,110.48,229.98,402.15,9.96;7,110.48,241.94,88.99,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,284.48,218.02,228.16,9.96;7,110.48,229.98,189.01,9.96">Probabilistic models of information retrieval based on measuring divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,313.60,229.98,194.36,9.96">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,261.86,402.18,9.96;7,110.48,273.82,402.15,9.96;7,110.48,285.77,401.91,9.96;7,110.48,297.73,26.51,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,165.33,261.86,283.09,9.96">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.iccs.inf.ed.ac.uk/~pkoehn/publications/europarl/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,476.64,261.86,36.00,9.96;7,110.48,273.82,243.08,9.96">Proc. of the 10th Machine Translation Summit (MT Summit X)</title>
		<meeting>of the 10th Machine Translation Summit (MT Summit X)</meeting>
		<imprint>
			<date type="published" when="2005-08">2005. August 2007</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,317.65,402.15,9.96;7,110.47,329.61,402.16,9.96;7,110.47,341.56,268.13,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,263.67,317.65,149.85,9.96">Statistical phrase-based translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,431.72,317.65,80.89,9.96;7,110.47,329.61,402.16,9.96;7,110.47,341.56,189.50,9.96">NAACL &apos;03: Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,361.48,402.13,9.96;7,110.47,373.45,74.91,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,254.88,361.48,231.14,9.96">Foundations of statistical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,393.37,402.16,9.96;7,110.47,405.32,225.60,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,250.48,393.37,262.14,9.96;7,110.47,405.32,34.63,9.96">Character n-gram tokenization for European language text retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,153.38,405.32,92.87,9.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,425.25,402.15,9.96;7,110.47,437.20,383.31,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,243.78,425.25,268.83,9.96;7,110.47,437.20,15.91,9.96">JHU/APL experiments in tokenization and non-word translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,189.81,437.20,151.90,9.96">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,457.13,122.85,9.96;7,253.43,457.13,259.22,9.96;7,110.47,469.08,402.14,9.96;7,110.47,481.04,293.25,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,253.43,457.13,259.22,9.96;7,110.47,469.08,56.87,9.96">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://www.fjoch.com/GIZA++.html" />
	</analytic>
	<monogr>
		<title level="j" coord="7,186.86,469.08,116.14,9.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-08">2003. August 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.46,500.97,402.21,9.96;7,110.47,512.92,402.15,9.96;7,110.47,524.87,25.43,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,296.36,500.97,216.31,9.96;7,110.47,512.92,116.77,9.96">CoLesIR at CLEF 2006: rapid prototyping of a n-gram-based CLIR system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">I</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,247.83,512.92,189.81,9.96">Working Notes of the CLEF 2006 Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Available at [2</note>
</biblStruct>

<biblStruct coords="7,110.46,544.80,402.14,9.96;7,110.47,556.76,402.10,9.96;7,110.47,568.71,56.41,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,299.39,544.80,213.21,9.96;7,110.47,556.76,87.94,9.96">Character n-grams translation in cross-language information tetrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vilares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,258.94,556.76,149.69,9.96">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4592</biblScope>
			<biblScope unit="page" from="217" to="228" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
