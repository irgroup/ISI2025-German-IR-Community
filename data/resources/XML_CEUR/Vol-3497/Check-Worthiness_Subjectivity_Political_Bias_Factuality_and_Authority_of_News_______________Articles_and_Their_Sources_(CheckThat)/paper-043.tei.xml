<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,388.86,15.42;1,89.29,106.66,263.33,15.42">Accenture at CheckThat! 2023: Learning to Detect Factuality Levels of News Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,47.01,11.96"><forename type="first">Sieu</forename><surname>Tran</surname></persName>
							<email>sieu.tran@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.94,134.97,73.62,11.96"><forename type="first">Paul</forename><surname>Rodrigues</surname></persName>
							<email>paul.rodrigues@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.21,134.97,84.88,11.96"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
							<email>b.strauss@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.09,134.97,86.91,11.96"><forename type="first">Evan</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
							<email>emwillia@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,388.86,15.42;1,89.29,106.66,263.33,15.42">Accenture at CheckThat! 2023: Learning to Detect Factuality Levels of News Sources</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">808F981EF086B7C55E8E6C2EE5CE8D1C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fact detection</term>
					<term>fact checking</term>
					<term>factuality</term>
					<term>factivity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>News sources are expected to present factual evidence with accurate and unbiased reasoning to inform their readership. A news source that repeatedly presents reporting with low factuality can erode confidence and trust and lose a reader. With a low barrier to technical entry, numerous news sources have emerged, and it can be difficult to evaluate a new source for accuracy, especially if one is not familiar with their reputation. Tools such as social media and news aggregators can utilize social signals, but the social network propagating the news may not be motivated or qualified to evaluate the source for factuality. Propagation of false and misleading information through these networks can harm decision making of a group and cause spread of misinformation. This paper presents a model that learns the factuality levels of news articles and news sources, using an NLP data augmentation strategy to increase the size of the training data. The model can then be applied to unseen news sources and articles to identify the factuality level of the document. A system, like this one, could be utilized in a news aggregator or social network to flag an article with a credibility warning or to deactivate user interface elements that promote article dissemination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As news information ecosystems become increasingly fractured and divisive, and with the increase of citizen journalism causing an increase in news sources, users must constantly evaluate the veracity of the sources to which they are exposed. Automated reliability detection systems can remove some of this burden from the user and can aid search engines, news aggregators, and social media recommendation systems in leading users to accurate and reliable content. Our submission to CLEF's CheckThat! 2023 Task 4 develops an automated news source reliability detection system.</p><p>Detecting reliability based on a subset of articles for each domain is an inherently challenging task as the lines between "high", "mixed", and "low" reliability are often blurry. It can be unclear where cut-offs between each category should lie. Authors of articles can insinuate false conspiracies or explanations, recount stories in misleading and biased ways, or ask leading questions all without ever stating a technical falsehood. Even on highly-unreliable sites, some articles often contain true or mostly-true information. For example, In the task 4 data provided by organizers, "dcclothesline", an unreliable news domain, has an article that refers to the COVID-19 vaccine as a "genocide", but also has an article that accurately, albeit with biased language, recounts a story about Rashida Tlaib falsely attributing the death of a Palestinian child to "Israeli Settlers" on Twitter <ref type="bibr" coords="2,248.53,154.71,11.43,10.91" target="#b0">[1]</ref>.</p><p>Several works have used web graph data and search engine optimization data to explore statistical connections between unreliable news domains as well as the approaches the domains take to manipulate search engines <ref type="bibr" coords="2,250.48,195.36,11.48,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,265.47,195.36,7.52,10.91" target="#b2">3,</ref><ref type="bibr" coords="2,276.50,195.36,7.65,10.91" target="#b3">4]</ref>. While reliability detection remains rare in the literature, other misinformation detection tasks often leverage domain-level data. <ref type="bibr" coords="2,460.71,208.91,12.99,10.91" target="#b4">[5]</ref> jointly predict news media reliability as well as bias using Copula Ordinal Regression (COR) models. Many researchers use site reliability labels within a larger detection system to detect false or misleading news (e.g. <ref type="bibr" coords="2,187.55,249.56,11.36,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,201.63,249.56,8.14,10.91" target="#b6">7]</ref>)</p><p>In this work, we describe the data augmentation and fine tuning approach employed by the Accenture Team for CLEF CheckThat! Lab's Task 4. Teams were tasked with classifying websites sources as "low", "mixed", or "high" factuality from their articles and evaluated using mean absolute error (MAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exploratory Analysis</head><p>Table <ref type="table" coords="2,115.20,362.38,4.97,10.91" target="#tab_0">1</ref> shows the number of samples and unique word counts for each of the datasets provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Label Balance</head><p>The training dataset had label bias which skewed towards sources that were "high" labeled on factuality: 61% high, 26% mixed, and 13% low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">WordPiece Analysis</head><p>Transformer models utilize WordPiece tokenization schemes that are dependant on the model being evaluated. At the time of pre-training, the WordPiece algorithm determines which pieces of words will be retained, and which will be discarded. We present our analysis in Table <ref type="table" coords="2,500.04,618.52,3.81,10.91" target="#tab_1">2</ref>.</p><p>Unexpectedly, the RoBERTa tokenizers we used did not return UNK tokens on any dataset provided by the CLEF CheckThat! organizers.  <ref type="bibr" coords="3,110.75,315.96,16.17,10.91" target="#b10">[11]</ref>. For this work, we fine-tune roberta-large <ref type="bibr" coords="3,315.79,315.96,16.17,10.91" target="#b11">[12]</ref>. The English RoBERTa model contains 50,265 WordPieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Augmentation</head><p>The organizers provided a training and a development set for each language. We use the provided training set and development set to create internal training and validation sets for experimentation. We use the test set provided by organizers as a hold-out test set.</p><p>For each article, augmentation and training were done with via back-translation using AWS translation. We appended back-translated low and mixed factuality articles to the training set. In our 2021 experiment <ref type="bibr" coords="3,195.16,476.85,16.21,10.91" target="#b12">[13]</ref>, we found that this form of augmentation resulted in a significant increase in recall and F1-score for check-worthy tweets. Due to significant sample imbalance in the training sets for both task, we augmented the 0 (Low)-and 1 (Mixed)-class until the samples are roughly balanced. Table <ref type="table" coords="3,251.85,517.49,5.01,10.91" target="#tab_2">3</ref> shows the BLEU score for each back-translation scheme performed. BLEU score is historically used to compute the quality of machine translation by comparing machine translated text to a human translated reference. When using BLEU score for data augmentation the lower the BLEU score, the more divergent the translation to the original text, providing more diverse training data.</p><p>The number of new tokens added to the text corpus by back translation can be found in 4. Adding a large number of useful new tokens aids in diversifying the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification</head><p>For the RoBERTa model, we added an additional mean-pooling layer and dropout layer on top of the model prior to the final three binary classification layers, each of which corresponding to a class (i.e., 0 (Low), 1 (Mixed), or 2 (High)). The highest class probability determines the article final classification. Adding these additional layers has been shown to help prevent over-fitting while fine-tuning. We used an Adam optimizer with a learning rate of 2ùëí -5 and an epsilon of 1.5ùëí -8. We use a binary cross-entropy loss function, 4 epochs, and a batch size of 32. The majority class of all articles released by a given news source determines the level at which the news source is factual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Table <ref type="table" coords="4,115.43,471.08,5.00,10.91" target="#tab_3">5</ref> shows model performance on the test set provided by the organizers. The classification task reached a weighted average F1-score of 0.592. The accuracy of the classifier was 0.590. This method received a Mean Absolute Error (MAE) of 0.467 in the official evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduced the methods and results from the Accenture Team for the 2023 CLEF CheckThat! Lab's Task 4, identifying the factuality of news sources. Teams were tasked with classifying websites sources as "Low", "Mixed", or "High" factuality from their article content and the tasks were evaluated using mean absolute error (MAE). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,391.39,423.52,81.83"><head>Table 1 Dataset descriptions Task Modeling Group # of Source # of Articles Unique Words</head><label>1</label><figDesc></figDesc><table coords="2,95.27,440.44,377.57,32.78"><row><cell>Factuality News Media Source Train</cell><cell>947</cell><cell>7,948</cell><cell>98,725</cell></row><row><cell>Factuality News Media Source Test</cell><cell>122</cell><cell>1,054</cell><cell>36,766</cell></row><row><cell>Factuality News Media Source Validation</cell><cell>120</cell><cell>1,049</cell><cell>36,157</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.96,90.49,417.23,236.38"><head>Table 2</head><label>2</label><figDesc>Token distribution in dataIn this work, we utilize RoBERTa models. The Bidirectional Encoder Representation Transformer (BERT) is a transformer-based architecture that was introduced in 2018<ref type="bibr" coords="3,413.77,234.66,11.46,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,427.96,234.66,7.65,10.91" target="#b8">9]</ref>. BERT has had a substantial impact on the field of NLP, and achieved state of the art results on 11 NLP benchmarks at the time of its release. RoBERTa, introduced by[10], modified various parts of BERTs training process. These modifications include more training data, more pre-training steps with bigger batches over more data, removing BERT's Next Sentence Prediction, training on longer sequences, and dynamically changing the masking pattern applied to the training data</figDesc><table coords="3,89.29,119.83,351.14,88.52"><row><cell cols="3">Tokenizer Type Modeling Set WordPiece</cell></row><row><cell>RoBERTa-based</cell><cell>Train</cell><cell>8,645,632</cell></row><row><cell></cell><cell>Test</cell><cell>1,103,855</cell></row><row><cell></cell><cell>Validation</cell><cell>1,057,250</cell></row><row><cell cols="3">3. Transformer Architectures and Pre-Trained Models</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.98,90.49,388.23,227.64"><head>Table 3</head><label>3</label><figDesc>Average Sentence BLEU Score for Each Back-translation Scheme</figDesc><table coords="4,88.99,122.05,388.22,196.07"><row><cell>Label</cell><cell>Back-translation</cell><cell cols="3">Avg Sentence BLEU Score</cell></row><row><cell>0 (Low)</cell><cell>EN &gt; ES &gt; EN</cell><cell>0.454</cell><cell></cell><cell></cell></row><row><cell>0 (Low)</cell><cell>EN &gt; EN &gt; EN &gt; FR &gt; EN</cell><cell>0.383</cell><cell></cell><cell></cell></row><row><cell>0 (Low)</cell><cell cols="2">EN &gt; EN &gt; EN &gt; FR &gt; EN &gt; DE &gt; EN 0.335</cell><cell></cell><cell></cell></row><row><cell cols="2">1 (Mixed) EN &gt; EN &gt; EN</cell><cell>0.481</cell><cell></cell><cell></cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">New Tokens in Machine Translated Text</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Unique</cell><cell>Unique</cell><cell></cell></row><row><cell></cell><cell></cell><cell>tokens</cell><cell>tokens</cell><cell>New Tokens</cell></row><row><cell>Label</cell><cell>Back-translation</cell><cell>in source</cell><cell>in MT</cell><cell>in MT</cell></row><row><cell>0 (Low)</cell><cell>EN &gt; SP &gt; EN</cell><cell>58770</cell><cell>58156</cell><cell>18626</cell></row><row><cell>0 (Low)</cell><cell>EN &gt; SP &gt; EN &gt; FR &gt; EN</cell><cell>58770</cell><cell>57322</cell><cell>20356</cell></row><row><cell>0 (Low)</cell><cell cols="2">EN &gt; SP &gt; EN &gt; FR &gt; EN &gt; DE &gt; EN 58770</cell><cell>55945</cell><cell>21906</cell></row><row><cell cols="2">1 (Mixed) EN &gt; SP &gt; EN</cell><cell>102970</cell><cell>93502</cell><cell>33677</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.98,90.49,307.74,154.75"><head>Table 5</head><label>5</label><figDesc>Accenture results from CheckThat! 2023 Task 4</figDesc><table coords="5,198.55,119.83,198.18,125.40"><row><cell>Class</cell><cell cols="3">Precision Recall F1-score</cell></row><row><cell>0 (Low)</cell><cell>0.500</cell><cell>0.263</cell><cell>0.345</cell></row><row><cell>1 (Mixed)</cell><cell>0.364</cell><cell>0.516</cell><cell>0.427</cell></row><row><cell>2 (High)</cell><cell>0.750</cell><cell>0.708</cell><cell>0.729</cell></row><row><cell>macro avg</cell><cell>0.538</cell><cell>0.496</cell><cell>0.500</cell></row><row><cell cols="2">weighted avg 0.614</cell><cell>0.590</cell><cell>0.592</cell></row><row><cell></cell><cell cols="2">Accuracy MAE</cell><cell></cell></row><row><cell></cell><cell>0.590</cell><cell>0.467</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,112.66,656.03,393.60,10.91;4,112.33,669.58,29.19,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Frantzman</surname></persName>
		</author>
		<title level="m" coord="4,189.06,656.03,317.21,10.91">Rashida Tlaib retweets unverified claim Israelis killed Palestinian boy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,271.94,393.58,10.91;5,112.66,285.49,252.80,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,232.70,271.94,273.54,10.91;5,112.66,285.49,78.42,10.91">Investigating fake and reliable news sources using complex networks analysis</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mazzeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rapisarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,199.26,285.49,88.24,10.91">Frontiers in Physics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">886544</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,299.04,395.16,10.91;5,112.66,312.59,284.51,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,246.62,299.04,261.20,10.91;5,112.66,312.59,24.38,10.91">Search engine manipulation to spread pro-kremlin propaganda</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Harvard Kennedy School Misinformation Review</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,326.14,393.61,10.91;5,112.66,339.69,191.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,171.98,326.14,334.29,10.91;5,112.66,339.69,20.41,10.91">Disinformation optimised: gaming search engine algorithms to amplify junk news</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bradshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,142.10,339.69,98.68,10.91">Internet policy review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,353.24,393.61,10.91;5,112.66,366.79,393.59,10.91;5,112.66,380.34,146.44,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00542</idno>
		<title level="m" coord="5,333.01,353.24,173.26,10.91;5,112.66,366.79,359.05,10.91">Multi-task ordinal regression for jointly predicting the trustworthiness and the leading political ideology of news media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,393.89,393.33,10.91;5,112.66,407.44,324.36,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,315.70,393.89,190.29,10.91;5,112.66,407.44,142.00,10.91">Fakeflow: Fake news detection by modeling the flow of affective information</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09810</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,420.99,393.33,10.91;5,112.66,434.53,393.33,10.91;5,112.66,448.08,393.33,10.91;5,112.28,461.63,395.39,10.91;5,112.41,475.18,38.81,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,207.31,420.99,298.68,10.91;5,112.66,434.53,99.43,10.91">Ecol: E arly det ec tion of co vid l ies using content, prior knowledge and source information</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Baris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Boukhers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,234.34,434.53,271.64,10.91;5,112.66,448.08,393.33,10.91;5,112.28,461.63,110.64,10.91">Combating Online Hostile Posts in Regional Languages during Emergency Situation: First International Workshop, CONSTRAINT 2021, Collocated with AAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-02-08">February 8, 2021. 2021</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers 1</note>
</biblStruct>

<biblStruct coords="5,112.66,488.73,393.33,10.91;5,112.66,502.28,363.59,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,323.15,488.73,182.83,10.91;5,112.66,502.28,181.08,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,112.66,515.83,393.33,10.91;5,112.66,529.38,321.60,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m" coord="5,321.65,515.83,184.33,10.91;5,112.66,529.38,191.23,10.91">Well-read students learn better: On the importance of pre-training compact models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,542.93,395.17,10.91;5,112.66,556.48,393.33,10.91;5,112.33,570.03,296.49,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,140.00,556.48,263.30,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,583.58,395.17,10.91;5,112.66,597.12,394.53,10.91;5,112.66,610.67,393.33,10.91;5,112.66,624.22,393.33,10.91;5,112.66,637.77,395.00,10.91;5,112.66,651.32,17.97,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,287.06,583.58,220.77,10.91;5,112.66,597.12,259.19,10.91">Accenture at CheckThat! 2020: If you say so: Posthoc fact-checking of claims using transformer-based models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novak</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_226.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,235.82,610.67,270.17,10.91;5,112.66,624.22,78.85,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="5,480.05,625.24,25.94,9.72;5,112.66,637.77,122.95,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,664.87,395.17,10.91;6,112.66,86.97,393.33,10.91;6,112.33,100.52,296.49,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,140.00,86.97,263.30,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoy-Anov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,114.06,393.33,10.91;6,112.66,127.61,394.53,10.91;6,112.66,141.16,122.77,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05684</idno>
		<title level="m" coord="6,276.86,114.06,229.12,10.91;6,112.66,127.61,389.62,10.91">Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
