<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,411.79,15.42;1,89.29,106.66,356.38,15.42;1,89.29,128.58,412.58,15.43;1,89.29,150.49,54.21,15.43;1,89.29,172.83,231.64,11.96">OpenFact at CheckThat! 2023: Head-to-Head GPT vs. BERT -A Comparative Study of Transformers Language Models for the Detection of Check-worthy Claims Notebook for the CheckThat! Lab at CLEF 2023</title>
				<funder ref="#_vRszMX4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,198.73,81.61,11.96"><forename type="first">Marcin</forename><surname>Sawiński</surname></persName>
							<email>marcin.sawinski@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.05,198.73,78.89,11.96"><forename type="first">Krzysztof</forename><surname>Węcel</surname></persName>
							<email>krzysztof.wecel@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.10,198.73,89.22,11.96"><forename type="first">Ewelina</forename><surname>Księżniak</surname></persName>
							<email>ewelina.ksiezniak@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.97,198.73,79.92,11.96"><forename type="first">Milena</forename><surname>Stróżyna</surname></persName>
							<email>milena.strozyna@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.72,212.68,129.89,11.96"><forename type="first">Włodzimierz</forename><surname>Lewoniewski</surname></persName>
							<email>wlodzimierz.lewoniewski@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.25,212.68,69.64,11.96"><forename type="first">Piotr</forename><surname>Stolarski</surname></persName>
							<email>piotr.stolarski@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.89,212.68,98.46,11.96"><forename type="first">Witold</forename><surname>Abramowicz</surname></persName>
							<email>witold.abramowicz@ue.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Poznań University of Economics and Business</orgName>
								<address>
									<addrLine>Al. Niepodległości 10</addrLine>
									<postCode>61-875</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,411.79,15.42;1,89.29,106.66,356.38,15.42;1,89.29,128.58,412.58,15.43;1,89.29,150.49,54.21,15.43;1,89.29,172.83,231.64,11.96">OpenFact at CheckThat! 2023: Head-to-Head GPT vs. BERT -A Comparative Study of Transformers Language Models for the Detection of Check-worthy Claims Notebook for the CheckThat! Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">50C34F37B9704693D4570B9660D1F74D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>check-worthiness</term>
					<term>fact-checking</term>
					<term>fake news detection</term>
					<term>language models</term>
					<term>GPT</term>
					<term>BERT</term>
					<term>LLM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the research findings resulting from experiments conducted as part of the Check-That! Lab Task 1B-English submission at CLEF 2023. The aim of the research was to evaluate the check-worthiness of short texts in English. Various methodologies were employed, including zero-shot, few-shot, and fine-tuning techniques, and different GPT and BERT models were assessed. Given the significant increase in the use of GPT models in recent times, we posed a research question to investigate whether GPT models exhibit notable superiority over BERT models in detecting check-worthy claims.</p><p>Our findings indicate that fine-tuned BERT models can perform comparably to large language models such as GPT-3 in identifying check-worthy claims for this particular task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In today's fast-paced and interconnected world, the need for fact-checking has become more critical than ever before. With the proliferation of social media platforms and the ease of sharing information, it has become increasingly challenging to discern between what is true and what is not. Rapid spreading of misinformation and disinformation for political, ideological, or personal gains may lead to significant consequences on public opinion, decision-making processes, and even societal harmony <ref type="bibr" coords="2,258.90,100.52,11.41,10.91" target="#b0">[1]</ref>. This is why fact-checking plays a vital role, serving as a crucial tool to verify the accuracy and credibility of information. Initially, the primary focus of fact-checking was to confirm the accuracy of information presented in news articles prior to their publication. This responsibility lies at the heart of the journalistic profession. In present times, fact-checking refers also to the analyses of claims after a certain information is published and concerns particularly information that is shared on the Internet <ref type="bibr" coords="2,457.07,168.26,11.59,10.91" target="#b1">[2]</ref>. This is carried out by people (fact-checkers), not related to the author of information being verified, who critically examine and verify it and thus help to combat the spread of misinformation. Usually, the fact-checking process consists of several steps, starting with selecting a claim to check, through contextualizing and analyzing, consulting data and domain experts, writing up the results along with deciding on the rating, and finally disseminating the report <ref type="bibr" coords="2,446.32,236.01,11.27,10.91" target="#b2">[3]</ref>. The main challenge is that the majority of the fact-checker's job is still done manually. Therefore, there is a pressing need to develop various technologies that would facilitate, speed up, and improve fact-checking work and detection of fake news.</p><p>The first step of the fact-checking process primarily involves the identification of checkworthy claims. The aim is to identify, prioritize, filter, and select claims that are worth to fact-check, considering their factual coherence and potential impact. The process of selecting claims to fact-check entails identifying statements from diverse sources like posts, news articles, interviews, etc., assessing their check-worthiness (i.e., if they are factual claims that can be verified), and assessing their relevance and appeal to the target audience in terms of significance, usefulness, and engagement. In the research presented in this paper, our focus was specifically on the check-worthiness aspect. As outlined by <ref type="bibr" coords="2,306.92,385.05,11.57,10.91" target="#b3">[4]</ref>, a claim may be deemed check-worthy if the information it carries is: 1) harmful -it attacks a person, organization, country, group, race, community, etc., or 2) urgent or breaking news -news-like statements about prominent people, organizations, countries and events, or 3) up-to-date -referring to recent official document with facts, definitions and figures. The automatic identification of such check-worthy claims is a challenging task and the main focus of this study.</p><p>The study presented in this paper is based on experiments performed as part of the CheckThat! Lab, Task 1B-English at CLEF 2023 <ref type="bibr" coords="2,247.55,479.89,11.49,10.91" target="#b4">[5]</ref>. The study focuses on a single task of assessing checkworthiness in unimodal (text-only) contents in English language. The task is defined as a binary classification problem, where the goal is to classify a given claim as check-worthy or not. The task is evaluated using the F1 score metric over positive class. The aim of this paper is to present various approaches that were applied in the task of assessing check-worthiness of unimodal content in English language and discuss the obtained results. It also shows the progress made beyond the state of the art.</p><p>The paper is organized as follows. Section 2 is an overview of state of the art in detecting check-worthy claims. Section 3 describes details of the conducted experiments, stating with dataset characteristic and how it was used for training. Methods based on GPT, BERT and boosting models follow. In Section 4 the results of experiments are discussed. The paper concludes with indications of directions for future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Transformer-based models, such as BERT and GPT-3, have been already used by teams that participated in CheckThat! Lab 2022 Task 1 that was held in the framework of CLEF 2022 <ref type="bibr" coords="3,89.29,138.38,11.58,10.91" target="#b5">[6]</ref>. This task concerned detection of relevant claims in tweets, taking into account various criteria, such as check-worthiness, verifiability, harmfulness, and attention-worthiness. The subtask, that concerned the check-worthiness of tweets, was the most popular one and covered several languages: Arabic, Bulgarian, Dutch, English, Spanish, and Turkish. However, English was the most popular target language. The datasets in all languages tackled tweets related to COVID-19 and politics. The evaluation metric for this task was the F1-measure with respect to the positive class. In total, there were 13 solutions submitted on the check-worthiness task for English language last year. The top-ranked system was built with RoBERTa large, after a data augmentation process based on back-translation <ref type="bibr" coords="3,303.02,246.77,11.28,10.91" target="#b6">[7]</ref>, achieving F1-measure of 0.698. The tweets texts were translated to French, then back translated to English and combined to the training dataset. Moreover, all links from tweets were replaced with "@link". The second-best system was based on an ensemble approach that combined fine-tuned BERT and RoBERTa models (F1 of 0.667) <ref type="bibr" coords="3,146.13,300.97,11.51,10.91" target="#b7">[8]</ref>. In total, it was ensemble of ten models, pre-trained on tweets about COVID-19. Moreover, they applied various pre-processing techniques, like removing URLs, hashtags, numbers and other symbols. The third best solution, with F1 value of 0.626, used a fine-tuned GPT-3 model that is originally trained on English <ref type="bibr" coords="3,312.80,341.62,11.48,10.91" target="#b8">[9]</ref>. Other approaches, that were submitted or considered in internal experiments, were based either on a single transformer-based model, like BERT, DistilBERT, Electra, XML RoBERTa, mT5-XL, or XLNet, or ensemble of several models, e.g., various versions of BERT and RoBERTa models. There were also tested solutions with classifiers, like SVM and Random Forest. Moreover, most of the teams applied various additional techniques, starting from data augmentation to increase the size of training dataset (e.g., machine translating labeled datasets in other languages to the corresponding language, or back-translation), feature extraction for tweets, which were further used in addition to the textual data, using ELMo embeddings combined with linguistic features (LIWC), or including additional unlabeled training data. There were also some experiments with quantum natural language processing (QNLP), however the technique posed some problems, as reported by <ref type="bibr" coords="3,487.36,477.11,16.12,10.91" target="#b9">[10]</ref>.</p><p>It is worth-mentioning that some solutions covered multiple languages, by application of different strategies, such as MT-based data augmentation (application of translation and backtranslation to increase the training dataset in different languages) <ref type="bibr" coords="3,400.81,517.76,16.42,10.91" target="#b10">[11]</ref>, mT5 multilingual transformer (a single model that might be applied to multiple languages) <ref type="bibr" coords="3,426.60,531.30,16.42,10.91" target="#b11">[12]</ref>, or zero-shot strategy (a fine-tuned GPT-3 model fed with only instances in English and applied to other languages during testing) <ref type="bibr" coords="3,205.36,558.40,11.43,10.91" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The main objective of the experiments was to verify the hypothesis that the large GPT models are able to significantly outperform BERT models in detecting check-worthy claims. In order to test the hypothesis, multiple experiments were carried-out using various GPT and BERT models, as described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The dataset offered for training consisted of 23,533 statements extracted from U.S. general election presidential debates, annotated by human coders and originally published in January 2015, known as ClaimBuster dataset <ref type="bibr" coords="4,262.43,134.63,16.42,10.91" target="#b12">[13]</ref>. This is not completely inline with the subtask description that texts in dataset are multigenre.</p><p>The dataset was split into train, dev and dev_test with 16,876, 5,625 and 1,032 examples in each split respectively. The comparison with the original ClaimBuster dataset revealed that train and dev splits were generated from examples with crowd-sourced labels. The dev_test split was identical to the ClaimBuster dataset, called ground-truth.</p><p>The ground-truth dataset was labeled by 3 experts and was used to screen spammers and low-quality participants of the crowd-sourced part of the dataset. The difference between groundtruth and crowd-sourced surfaced during evaluation of the results. The models trained on train dataset achieved, on average, F1 score 0.1 higher when tested on dev_test (i.e., ground-truth) then on dev (i.e., crowd-sourced). The difference could be attributed to the composition of split (e.g., less borderline examples in dev_test) or the quality of labels (e.g., a higher consistency in dev_test), with the latter being more probable as it correlates with the dataset creation process (i.e., experts vs. crowd-sourced labels).</p><p>This observation led to the conclusion that reshuffling the splits and filtering of the dataset could be a way to avoid overfitting to the crowd-sourced labels and to improve the model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methods based on GPT models</head><p>Transformers have revolutionized NLP field and became a dominant architecture for state-ofthe-art solutions, including the top-ranked solutions submitted to CheckThat! Lab in recent years. In 2022 most teams used BERT models <ref type="bibr" coords="4,300.34,428.25,12.99,10.91" target="#b5">[6]</ref> and only one team <ref type="bibr" coords="4,404.98,428.25,18.07,10.91" target="#b13">[14]</ref> used GPT models in their solution and won 1st place in the subtask 1B -'Verifiable Factual Claims Detection' and 3rd place in the subtask 1A -'Check-Worthiness Estimation' in English language. Given the spectacular development of GPT models over the recent year, we decided to verify the potential of large GPT models in detecting check-worthy claims. The study explored three main approaches to using GPT models: zero-shot learning, few-shot learning, and fine-tuning.</p><p>GPT models can be trained in two distinct ways: for text completion and for following instructions while maintaining conversation (chat). At the time of conducting the experiments, many pre-trained GPT models were available from many authors, e.g., OpenAI (GPT-3<ref type="foot" coords="4,475.44,534.89,3.71,7.97" target="#foot_0">1</ref> , GPT-3.5<ref type="foot" coords="4,101.60,548.43,3.71,7.97" target="#foot_1">2</ref> , GPT-4<ref type="foot" coords="4,138.36,548.43,3.71,7.97" target="#foot_2">3</ref> ), Anthropic (Claude<ref type="foot" coords="4,231.89,548.43,3.71,7.97" target="#foot_3">4</ref> ), Stanford(Alpaca) <ref type="bibr" coords="4,315.78,550.19,17.85,10.91" target="#b14">[15]</ref>, Meta(LLaMA) <ref type="bibr" coords="4,396.53,550.19,20.25,10.91" target="#b15">[16]</ref>. The models varied in size, number of parameters, training data, and training objectives.</p><p>OpenAI models were chosen for experiments for two reasons: they showed the advantage in multiple performance tests and they were cost-effective in terms of fine-tuning and inference (e.g., the cost of fine-tuning a GPT-3 based curie model with 7000 examples via API was around 2 USD, while a creation of a dedicated Virtual Machine with NVidia GPU A100 80GB to host a model similar in size would cost more by a few orders of magnitude). However, we should keep in mind that the cloud-based models available for prompting and fine-tuning via API can change over time and impact reproducibility of experiments.</p><p>The GPT language models created by OpenAI in 2018 use transformers architecture together with generative pre-training on a large corpus of unlabelled text, followed by discriminative fine-tuning on specific tasks. The subsequent updates to the GPT model formed a series of "GPT-n" models. The original GPT-1 model, released in 2018, had 117 million parameters and was trained on BooksCorpus dataset (7 000 unique unpublished books from a variety of genres, 4.5 GB) <ref type="bibr" coords="5,124.18,208.91,16.25,10.91" target="#b16">[17]</ref>.</p><p>The GPT-2 model, released in 2019, introduced modified initialization, pre-normalization, and reversible tokenization. It featured 1.5 billion parameters and was trained on WebText dataset (40 GB) <ref type="bibr" coords="5,124.95,249.56,16.25,10.91" target="#b17">[18]</ref>.</p><p>The GPT-3 model, released in 2020, introduced alternating dense and locally banded sparse attention patterns in the layers of the transformer <ref type="bibr" coords="5,314.77,276.66,16.28,10.91" target="#b18">[19]</ref>. It featured 175 billion parameters and was trained on a filtered and deduplicated Common Crawl dataset (570GB) and other highquality reference corpora (WebText2, Books and Wikipedia). Additionally, a set of 8 differently sized models was created to test dependence of model performance on its size. Four models were released for general use and named: ada, babbage, curie, and davinci with 350 million, 3, 13 and 175 billion parameters, respectively. At the time of conducting the experiments presented in the paper, the four GPT-3 models were the most advanced language models from OpenAI that were publicly available for fine-tuning. The datasets used to train these models might have changed since the publication of the original paper. At the moment of writing Microsoft reports, the curie model was trained using 800GB of text data and the davinci model was trained using 45TB of text data <ref type="foot" coords="5,165.15,410.39,3.71,7.97" target="#foot_4">5</ref> .</p><p>Further on, fine-tuning using a combination of supervised training and reinforcement learning from human feedback allowed for the creation of instruction-following models (InstructGPT) that could be further fine-tuned for conversational interaction (ChatGPT). The GPT-3.5 model, released in 2022, is optimized for dialogue and forms the basis for ChatGPT. The size and performance of the model is comparable to Instruct Davinci (i.e., the biggest GPT-3 model finetuned for instruction-following; however, the technical details of the model were not disclosed by OpenAI).</p><p>The GPT-4 model, released in 2023, is also fine-tuned for instruction-following and for dialogue. It brought a significant improvement over GPT-3.5 in numerous benchmarks, but the technical details of the model were not disclosed by OpenAI <ref type="bibr" coords="5,359.52,547.64,16.25,10.91" target="#b19">[20]</ref>.</p><p>In our experiments, the GPT-3 model was used for fine-tuning and the GPT-3.5 and GPT-4 models were used for zero-shot and few-shot learning. The text completion approach is best used with GPT-3 models for fine-tuning, while the chat approach can leverage the GPT-3.5 and GPT-4 models for using zero-shot learning and few-shot learning. The GPT models do not require text preprocessing and were used with raw text from the dataset.</p><p>Data augmentation techniques, that often improve the performance of the model by enhancing the dataset, were not applied. According to the OpenAI fine-tuning guide <ref type="foot" coords="6,421.21,85.21,3.71,7.97" target="#foot_5">6</ref> , each doubling of the dataset size leads to a linear increase in model quality. However, in our experiments, we have not tested this assumption and decided to go in the opposite direction. Our experiments were inspired by intuition derived from Kaplan et al. <ref type="bibr" coords="6,320.69,127.61,17.91,10.91" target="#b20">[21]</ref> that show: bigger models do not scale linearly with the size of the training data. The conclusion that a large enough model can be trained on a small dataset and still achieve good results led to an idea to explore the potential of limiting the size of the dataset in order to improve its quality. The reasoning is illustrated in Figure <ref type="figure" coords="6,120.36,181.81,3.74,10.91" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Zero-shot learning using GPT-4</head><p>Zero-shot learning is a technique used in machine learning, particularly in natural language processing (NLP), where models are able to perform tasks without having prior examples of the specific task in their training data. GPT-4 is designed to have a good understanding of language and context, enabling it to perform various tasks with no task-specific fine-tuning.</p><p>To use this approach, the input prompt is designed to give clear and direct instructions related to the task <ref type="bibr" coords="6,138.88,519.23,16.24,10.91" target="#b21">[22]</ref>. These instructions help the GPT model understand the desired output format and perform the task using its pre-trained knowledge. The more specific and contextually relevant the instructions are, the better the model performs.</p><p>The input prompt for zero-shot learning with GPT-4 is shown in Listing 1. The variable {claims} in the prompt template is substituted during the run-time with the numbered list of claims to be classified (see <ref type="bibr" coords="6,222.07,586.98,41.31,10.91">Listing 2)</ref>. The output of the model is a numbered list of checkworthiness ratings for each claim in the input list (see Listing 3).</p><p>Listing 1: Zero-shot learning: the prompt template for GPT-4</p><p>You are a factchecker assistant with task to identify sentences that are check -worthy . Sentence is check -worthy only if it contains a verifiable factual claim and that claim can be harmful .</p><p>Classify the check -worthiness of these sentences outputting only yes or no : { claims } Check -worthiness ratings :</p><p>Listing 2: Zero-shot learning: an example of input for the {claims} variable 1. He ' s been a professor for a long time at a great school . 2. There ' s no way they would give it up . 3. They ' re able to charge women more for the same exact procedure a man gets . 4. As far as a say is concerned , the people already had their say . 5. I am the Democratic Party right now . The prompt template was designed to leverage the knowledge accumulated in the model weights during pre-training. To avoid the ambiguity of the term check-worthiness, the prompt instructs the model to classify the sentences as check-worthy or not based on specific criteria. The check-worthiness definition is briefly explained (the claim must be factual, verifiable, and potentially harmful). Please note that the terms factual, verifiable, and harmful are not further explained to the model and the prompt relies on the pre-trained representations of these concepts. To limit the token count of the prompt, the model is instructed to output only values Yes or No and takes multiple sentences as input. We have observed that changing of the wording of the prompt has a visible impact on individual responses; however, the overall classification accuracy remains at a similar level. The same applies to minor spelling or grammatical errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Few-shot learning using GPT-4</head><p>Few-shot learning refers to the ability of a machine learning model to learn and adapt to new tasks or predict outputs based on a small amount of training data. While traditional deep learning techniques generally rely on extensive data for successful task performance, the fewshot learning method in the GPT model utilizes the pre-trained weights to reduce the number of training examples required. It is important to note that the model is not trained (fine-tuned) on the new task, i.e., the model weights are not updated, but the expected behavior is formed only during the inference time <ref type="bibr" coords="7,228.05,656.03,16.25,10.91" target="#b18">[19]</ref>.</p><p>Few-shot learning experiment used three prompt templates:</p><p>• A system prompt that guides the model regarding the expected response by giving a context and an instruction to initiate the model's output. The system prompt is the same for all claims in the input list (see Listing 4).</p><p>• A list of assistant prompts that are used to convey the examples of the expected output.</p><p>The assistant prompts can be different for each claim in the input list to reflect a specific context (see Listing 5).</p><p>• A user prompt that is used to initiate the model's output. It contains the claim to be classified and the expected output format (see <ref type="bibr" coords="8,323.11,199.74,40.62,10.91">Listing 6)</ref>.</p><p>In order to find the best examples to be included in the few-shot learning experiment, the semantic search approach was applied. Firstly, all sentences from the train dataset were converted into sentence embeddings (768 dimensional dense vectors) using all-mpnet-base-v2 model from the HuggingFace Transformers library 7 . Secondly, the embeddings were used to find the most similar examples to the claim to be classified. Eight most similar examples (the top four with the label Yes and the top four with the label No) were then used as the assistant prompts. The similarity was calculated using the cosine similarity measure.</p><p>Listing 4: Few-shot learning: the system prompt template You are a fact -checker assistant with task to identify check -worthy sentences . You need to evaluate 2 conditions : condition 1 -sentence contains a verifiable factual claim , condition 2 -the claim could be potentially harmfull . A sentence is check -worthy if both conditions are met -in this case expected output is 'yes '. Otherwise output 'no '.</p><p>Listing 5: Few-shot learning: an example of the assistant prompt User : I want it to be clearer . Assistant : No User : Iran went from zero centrifuges to develop nuclear weapons to 4 ,000. Assistant : Yes User : Uh -Also , so that we can get the uh -opportunity for the questioners to question me , it will be before the next television debate . Assistant : No User : Marijuana use is up 141 percent .</p><p>Question : Classify if the sentence is check -worthy : That ' s one way one could do it .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Fine-tuning GPT-3</head><p>As mentioned earlier, at the time of conducting the experiments, the fine-tuning was available for the following base models from OpenAI<ref type="foot" coords="10,289.15,167.62,3.71,7.97" target="#foot_6">8</ref> : davinci, curie, babbage, and ada based on the GPT-3 architecture. These models do not have any instruction-following training. Instead, in the fine-tuning process, the model is trained to generate text that is similar to the provided examples. The dataset received from CheckThat! 2023 organizers was split into train, dev and dev_test with the major difference observed in dev_test (see Section 3.1).</p><p>Since the larger models tend to learn from fewer examples <ref type="bibr" coords="10,366.90,237.12,16.39,10.91" target="#b20">[21]</ref>, the dataset was truncated to exclude labels of the lowest quality. The authors of the ClaimBuster dataset introduced screening criteria to exclude low quality labels and published the same dataset filtered using stricter criteria with respect to the labels assigned (the file called 2xNCS.json <ref type="foot" coords="10,434.52,276.01,3.71,7.97" target="#foot_7">9</ref> ). This reduced dataset was used for fine-tuning GPT-3 models, considering the authors' experience that they produce better models. In order to capture the effect of the train data quality on the fine-tuned model predictions, two distinct datasets were prepared for fine-tuning with a total of 8706 examples each. The first model (called curated) was fine-tuned on the dataset with the highest quality labels (the examples also mentioned in ClaimBuster 2xNCS.json file) and the second model (called raw) was fine-tuned on the dataset with randomly selected examples. The dataset split was 1000 examples for validation and 7706 for training. After removing duplicated entries, the final train dataset contained 7694 and 7685 examples respectively. It is important to note that no new examples or features were added to the dataset and the whole training and validation process was executed using the dataset provided by the CheckThat! 2023 organizers. The only information derived from ClaimBuster was the list of examples ids with expected higher quality of labels that were used to filter the dataset further utilized for fine-tuning curated version on the model.</p><p>The fine-tuning was performed using davinci and curie models from OpenAI. The same hyperparameter values were used for all fine-tuning experiments (see Table <ref type="table" coords="10,430.19,481.00,3.59,10.91" target="#tab_0">1</ref>). The format of the data prepared for fine-tuning is shown in Listing 8. Data used for fine-tuning contained only a binary label (Yes or No) without any indication of the purpose of the classification. This is important because the fine-tuning process is not aware of the task that the model is supposed to solve, yet it is still able to learn to classify the sentences correctly.</p><p>Listing 8: Fine-tuning data format example {" prompt ":" We might have to do it slowly . -&gt;" ," completion ":" no \ n "} {" prompt ":" It was $200 billion deficit instead . -&gt;" ," completion ":" yes \ n "} </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Methods based on BERT models</head><p>In this section, we describe the methodology of fine-tuning various BERT-based models for the task of classifying claims as check-worthy or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Basic models</head><p>Datasets: We used datasets as specified in the CheckThat! Lab, Task 1B-English, without reshuffling. train.tsv with 16,876 rows was used for training, dev.tsv with 5625 rows was used as evaluation during training, and dev_test.tsv with 1032 rows to compare the performance of various trained models.</p><p>Hardware: we fine-tuned our models on a local machine with four NVIDIA GeForce RTX 2080 Ti GPU cards, each with 11 GB of memory. The available machine limited the number of potential models that might be used for fine-tuning. Our experience shows that larger models usually caused 'out of memory' error (OOM).</p><p>The following models were subject to the fine-tuning: • DistilBERT <ref type="bibr" coords="11,410.03,414.77,16.41,10.91" target="#b23">[24]</ref>,</p><p>• DeBERTa <ref type="bibr" coords="11,486.67,414.77,16.41,10.91" target="#b24">[25]</ref>,</p><p>• RoBERTa <ref type="bibr" coords="11,141.20,428.32,16.40,10.91" target="#b25">[26]</ref>, • XLM-RoBERTa <ref type="bibr" coords="11,242.63,428.32,16.40,10.91" target="#b26">[27]</ref> , • ALBERT <ref type="bibr" coords="11,314.69,428.32,16.40,10.91" target="#b27">[28]</ref> , • RemBERT <ref type="bibr" coords="11,393.42,428.32,16.40,10.91" target="#b28">[29]</ref> , • CamemBERT <ref type="bibr" coords="11,486.68,428.32,16.40,10.91" target="#b29">[30]</ref>,</p><p>• ELECTRA <ref type="bibr" coords="11,144.62,441.87,16.35,10.91" target="#b30">[31]</ref>, • GPT neo 125M <ref type="bibr" coords="11,243.92,441.87,16.35,10.91" target="#b31">[32]</ref>, • YOSO <ref type="bibr" coords="11,303.21,441.87,16.35,10.91" target="#b32">[33]</ref>. Although the CamemBERT model is for French, we wanted to check, how it will behave during the fine-tuning. YOSO was the most exotic one among the tested models, and an efficient sampling scheme for short claims did not yield any satisfactory results (during the evaluation, some of the answers even appeared to have swapped "Yes" and "No" responses).</p><p>The default optimization method for training was AdamW (the Pythorch version adamw_pytorch). When the model was larger and caused OOM, we first reduced the batch size. Typically, we decreased from 16 to 8, or even to 4 in extreme cases. In the case of small batches, we also turned on a variant based on the gradient accumulation with 8 or 16 steps. It increased the training time without a significant impact on the accuracy. Concerning the float precision, we tested two settings for the models: FP16 and FP32. However, this did not result in an observable reduction in memory usage. Training could be faster, though. When the above approaches to reduce memory usage were not sufficient, we also switched the optimizer to Adafactor, but this only applied to the RemBERT model. Adafactor reduces memory usage while retaining the empirical benefits of adaptivity. Using Adafactor allowed us to obtain results, although the training time was much longer, reaching 3 hours, whereas typical fine-tuning with AdamW was 20-30 minutes.</p><p>Various learning rates were also tested. The default learning rate (lr) was 2e-5, and alternatives values of 1e-5 and 3e-5 were also explored. The typical fine-tuning duration was scheduled for 5 epochs and took 20-30 minutes. If it was observed that the evaluation loss was still decreasing, the training was extended to 10 epochs while monitoring for signs of overfitting. The following models were trained for 10 epochs: RoBERTa, DistilBERT, YOSO, GPT-neo, and RemBERT with Adafactor. Aditionally, more sophisticated methods for the fine-tuning were applied, such as layer-wise learning rate decay <ref type="bibr" coords="12,309.58,168.26,16.41,10.91" target="#b33">[34]</ref>. This method involves using different learning rates for different layers. The rationale behind this approach is that the lower layers (closer to the input) capture general language information, while the upper layers (closer to the output) encode task-specific information, such as classification. In general, the upper layers are trained with a higher learning rate, which gradually decreases when moving to the lower layers. The decay rate assumed in our method is 0.9. One of the RoBERTa models was trained using this custom optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">F1 as training objective</head><p>Optimization of a classification model typically focuses on accuracy. However, tn the defined Task 1B, the criteria for model selection was a higher F1 for the positive class. Models were fine-tuned with this objective in mind, but it did not always result in the best results when evaluated on the test dataset.</p><p>In Figure <ref type="figure" coords="12,143.58,353.08,3.78,10.91" target="#fig_2">2</ref>, two confusion matrices are shown: one is of the model trained to maximize F1 macro average (on the left), and the other is of the model trained specifically to maximize F1 of the positive class 'Yes' (on the right). The test dataset used for this and the following evaluations we was dev_test.tsv, which contains 1032 rows.</p><p>The DeBERTa model depicted on the left of Figure <ref type="figure" coords="12,324.60,407.28,5.02,10.91" target="#fig_2">2</ref> was also the best among locally trained BERT-like models in terms of the final evaluation. It was only 0.004 worse in terms of F1 measure compared to the overall winning model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Layer-wise learning rate decay</head><p>Here, we present the results of applying the layer-wise learning rate decay method. In Figure <ref type="figure" coords="12,499.85,648.85,3.67,10.91" target="#fig_1">3</ref>, two confusion matrices are shown. The left matrix presents the RoBERTa model trained with a fixed learning rate, i.e., it was the same for each layer. On the right, the matrix presents the RoBERTa model trained with varying learning rate, where the learning rate decreases as we move closer to the bottom layers. As expected, the more sophisticated optimizer provided a better model, at least on the dev_test datasets. The number of false positives was reduced by 3, and the number of false negatives was reduced by 6. The final results on the test dataset were comparable -the difference was only 0.002 in the F1 score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">The best model according to dev_test dataset</head><p>The most promising results were provided by the ELECTRA model. It had the highest value of F1 score for the positive class. The evaluation metrics and confusion matrix on dev_test dataset are presented in Figure <ref type="figure" coords="13,194.72,396.06,3.74,10.91" target="#fig_4">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">LightGBM ensemble model</head><p>Analysis of submissions to previous editions of the CheckThat! Lab <ref type="bibr" coords="13,410.00,652.86,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="13,425.23,652.86,12.59,10.91" target="#b34">35,</ref><ref type="bibr" coords="13,441.58,652.86,9.03,10.91" target="#b5">6]</ref> and similar publications suggested exploring ensemble methods to improve prediction accuracy. As a result, we decided to investigate the suitability of the LightGBM model, a gradient boosting framework that utilizes tree-based learning algorithms. The approach for this model was to combine the outputs from the previous fine-tuned models, along with predictions from other available models (data enrichment). This data included both the predicted labels and their corresponding probabilities.</p><p>Our LightGBM mode was trained on the following features:</p><p>• predictions (0 or 1) and probabilities (between 0.0 and 1.0) from the following finetuned models: The last point requires special attention. The ELECTRA model is trained with the objective of detecting replaced tokens. By utilizing the discriminator, it is possible to calculate the probability of each token (typically a word, depending on tokenization) in the sentence. If an unexpected token occurs, it may serve as a signal to verify the sentence. The assumption is that anomalous sentences are more lie=kely to be check-worthy. From the ELECTRA discriminator, we obtain logits for each token in the input sentence. We then create the following variables: the logit of the first token, the logit of the last token, the minimum logit value, the mean logit value, the maximum logit value, the number of odd tokens (when logit is bigger than zero), and the percentage of odd tokens.</p><p>We have explored various settings for hyperparameters in numerous attempts. However, despite our efforts, the F1 score never exceeded 0.79 on the evaluation set (dev_test.tsv, N=1032). Many training sessions exhibited similar patterns to the one depicted in Figure <ref type="figure" coords="14,478.52,547.57,3.67,10.91" target="#fig_5">5</ref>. The occurrence of overfitting varied depending on the learning rate, with some cases experiencing it sooner while others later in the training process.</p><p>Training of the best model was conducted with the following hyperparameters: objective: binary, boosting: dart, learning_rate: 0.05, bagging_fraction: 1, max_depth': -1, num_leaves: 255, min_gain_to_split: 0, min_data_in_leaf: 20, min_sum_hessian_in_leaf: 0.001, max_bin: 255, num_leaves: 255. Concerning the importance of variables, the following were the most important: de-berta_probY=5226 and xlm-roberta_probY=4676. So, the best models also contributed the most to the ensemble model. Our additional models, beyond the fine-tuned ones, also contributed to the overall result. For example, BERTemo_love=3971 was on the fourth place; the ninth place was taken by ELECTRA_logit_last=3363, and RoBERTasent_neutral=3276 was on the tenth place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary of experiments results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics calculated on test set</head><p>The highest F1 score of 0.898 was obtained by the GPT-3 curie model, which was fine-tuned with approximately 7690 examples selected based on stricter label quality criteria (GPT-3 curie fine-tuned curated). The model demonstrated top performance during internal validation and was subsequently submitted for the CheckThat! 2023 evaluation, where it achieved the highest score.</p><p>Interestingly, when the same model (GPT-3 curie) was fine-tuned with identical hyperparameters and the same number of examples (approximately 7690) randomly sampled from the entire dataset, it obtained F1 score of only 0.826 (GPT-3 curie fine-tuned random). The difference between these two models is significant (0.072) and shows that the quality of fine-tuning data is crucial for the final performance of the model.</p><p>Surprisingly, the larger GPT-3 davinci model, trained with the same setup and dataset as the winning method, performed marginally worse across all metrics, obtaining the F1 score of 0.876 (GPT-3 davinci fine-tuned curated), despite having 13 times more parameters.</p><p>The second best model was DeBERTa v3 with F1 score of 0.894 (DeBERTa v3 base fine-tuned). With only 86M backbone parameters, 98M parameters in the Embedding layer, and trained on 160GB data, this model displayed unparalleled parameter-efficiency, when compared to much larger curie and davinci models. Despite having the same accuracy as the winning model, the F1 score was only 0.004 lower due to a slightly less favorable precision-recall trade-off.</p><p>Other BERT models showed lower performance, but still surpassed the scores of zero-shot learning and few-shot learning approaches achieved by GPT-4.</p><p>The complete results are presented in Table <ref type="table" coords="16,303.12,114.06,3.81,10.91" target="#tab_4">2</ref>, and further can be found in the confusion matrices of the test results generated for each model (Figure <ref type="figure" coords="16,359.49,127.61,3.57,10.91" target="#fig_6">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the results</head><p>It is challenging to determine a clear winner between the BERT and GPT models. The leading models, namely GPT-3 curie and DeBERTa v3, performed comparably throughout the competition, consistently yielding similar results during validation and testing. The GPT-3 curie model was selected for the final submission due to its slightly better performance on the validation dataset, although the difference was not significant. However, in real-world scenarios, the De-BERTa v3 model may be preferred due to its smaller size and faster inference time. Additionally, DeBERTa v3 exhibited an advantage over the CheckThat! Lab 2022 Task 1 winning model, RoBERTa, proving the ongoing advancements in BERT architectures that are worth further exploration.</p><p>On the other hand, the GPT models demonstrated exceptional capabilities in leveraging limited data through few-shot and zero-shot learning. Although the achieved performance in this competition, with F1 scores of 0.788 and 0.778 respectively, may not appear remarkable, these results could be considered satisfactory when applied to real-world scenarios involving unknown data and the absence of an extensive training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Perspectives for future work</head><p>The experiments conducted for the CheckThat! 2023 competition have demonstrated the potential of GPT and BERT models in the task of check-worthiness. However, there are several avenues for future research that could further improve the performance of these models. Some of the most promising directions are discussed below.</p><p>Further dataset optimizations could be performed to improve the quality of the training data. It is not unlikely that DeBERTa v3 could benefit from reducing the train dataset by removing the most ambiguous examples. Data enrichment techniques, such as incorporating named entity recognition models, can also play its role.</p><p>Additionally, as mentioned in the section about the BERT models, we were limited by GPU resources -more specifically the memory limitation of 11GB. We could not fine-tune larger language models that could provide more accurate predictions. Various methods to reduce memory usage were attempted, but they either resulted in lower performance in terms of F1 score or were unacceptably slow. There is a decent group of larger models derived from Pythia<ref type="foot" coords="18,496.92,152.96,7.41,7.97" target="#foot_11">13</ref> , which have not been examined yet. Looking at their performance in comparison to much larger models, like GPT-4, they hold the potential for significant enhancements.</p><p>Moreover, the few-shot learning and zero-shot learning approaches could be further explored by experimenting with different models and prompts. Specifically Chain-of-Thought and Treeof-Thoughts <ref type="bibr" coords="18,148.80,222.46,18.07,10.91" target="#b35">[36]</ref> approaches could lead to solutions that generalize better in unknown or quickly evolving domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,376.46,382.49,9.15;6,471.78,374.88,3.97,6.12;6,479.73,376.46,21.98,9.14;6,501.71,374.88,3.97,6.12;6,89.29,388.68,375.41,8.87;6,99.21,202.77,396.84,165.41"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:A series of language model training runs, with models ranging in size from 10 3 to 10 9 parameters (excluding embeddings). Source: Scaling Laws for Neural Language Models<ref type="bibr" coords="6,448.24,388.68,16.46,8.87" target="#b20">[21]</ref> </figDesc><graphic coords="6,99.21,202.77,396.84,165.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,149.64,315.81,296.00,10.91;7,116.20,333.01,37.72,8.76;7,116.20,346.56,31.28,8.76;7,116.20,360.10,37.72,8.76;7,116.20,373.65,31.28,8.76;7,116.20,387.20,37.72,8.76"><head>Listing 3 :</head><label>3</label><figDesc>Zero-shot learning: an example of output for the prompt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,89.29,571.47,418.22,8.93;12,89.29,583.48,197.66,8.87;12,273.90,465.46,112.51,95.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Metrics and confusion matrices for DeBERTa based models trained with various objectives. Left: F1 macro avg. Right: F1 positive optimized.</figDesc><graphic coords="12,273.90,465.46,112.51,95.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,89.29,291.58,417.79,8.93;13,89.29,303.58,232.27,8.87;13,171.38,178.94,125.01,106.05"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Confusion matrices for the RoBERTa based models trained with various learning rates. Left: fixed learning rate. Right: layer-wise learning rate decay.</figDesc><graphic coords="13,171.38,178.94,125.01,106.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,89.29,559.94,260.23,8.93;13,345.28,427.14,145.84,121.78"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Metrics and confusion matrix of the ELECTRA model</figDesc><graphic coords="13,345.28,427.14,145.84,121.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,89.29,238.79,222.60,8.93;15,89.29,84.19,416.70,142.04"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training of Light GBM models: all 4 metrics</figDesc><graphic coords="15,89.29,84.19,416.70,142.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="17,89.29,523.30,180.64,8.93;17,89.29,84.18,416.71,426.55"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Confusion matrices of the models</figDesc><graphic coords="17,89.29,84.18,416.71,426.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,88.99,90.49,290.56,105.74"><head>Table 1</head><label>1</label><figDesc>Hyperparameters used for fine-tuning GPT-3 models</figDesc><table coords="11,215.72,122.05,163.83,74.17"><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Batch size</cell><cell>8</cell></row><row><cell>Learning rate multiplier</cell><cell>0.1</cell></row><row><cell>Epochs</cell><cell>4</cell></row><row><cell>Prompt loss weight</cell><cell>0.01</cell></row><row><cell>Compute classification metrics</cell><cell>True</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,88.99,156.62,372.99,237.24"><head>Table 2</head><label>2</label><figDesc>The results obtained by the GPT and BERT models</figDesc><table coords="16,133.29,188.24,328.69,205.63"><row><cell>Model</cell><cell cols="3">F1 precision recall accuracy</cell></row><row><cell>GPT-3 curie fine-tuned curated</cell><cell>0.898</cell><cell>0.948 0.852</cell><cell>0.934</cell></row><row><cell>DeBERTa v3 base fine-tuned</cell><cell>0.894</cell><cell>0.978 0.824</cell><cell>0.934</cell></row><row><cell>GPT-3 davinci fine-tuned curated</cell><cell>0.876</cell><cell>0.946 0.815</cell><cell>0.921</cell></row><row><cell>RoBERTa base fine-tuned</cell><cell>0.862</cell><cell>0.966 0.778</cell><cell>0.915</cell></row><row><cell>RoBERTa base fine-tuned with custom optimizer layer-wise learning rate decay</cell><cell>0.860</cell><cell>0.976 0.769</cell><cell>0.915</cell></row><row><cell>LightGBM ensemble of all BERT-based models and additional embeddings</cell><cell>0.854</cell><cell>0.976 0.759</cell><cell>0.912</cell></row><row><cell>ELECTRA fine-tuned</cell><cell>0.851</cell><cell>0.954 0.769</cell><cell>0.909</cell></row><row><cell>AlBERT large v2 fine-tuned</cell><cell>0.848</cell><cell>0.976 0.750</cell><cell>0.909</cell></row><row><cell>DistilBERT base uncased fine-tuned</cell><cell>0.827</cell><cell>0.952 0.731</cell><cell>0.896</cell></row><row><cell>GPT-3 curie fine-tuned random</cell><cell>0.826</cell><cell>1.000 0.704</cell><cell>0.899</cell></row><row><cell>GPT neo 125M fine-tuned</cell><cell>0.800</cell><cell>0.961 0.685</cell><cell>0.884</cell></row><row><cell>GPT-4 few-shot learning</cell><cell>0.788</cell><cell>0.867 0.722</cell><cell>0.868</cell></row><row><cell>GPT-4 zero-shot learning</cell><cell>0.778</cell><cell>0.710 0.861</cell><cell>0.833</cell></row><row><cell>GPT-4 Chain-of-Thought</cell><cell>0.722</cell><cell>0.574 0.972</cell><cell>0.745</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,638.14,173.12,8.97"><p>https://platform.openai.com/docs/models/gpt-3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,649.10,180.82,8.97"><p>https://platform.openai.com/docs/models/gpt-3-5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.57,660.05,173.12,8.97"><p>https://platform.openai.com/docs/models/gpt-4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,92.57,671.01,197.06,8.97"><p>https://www.anthropic.com/index/introducing-claude</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,92.57,671.00,306.73,8.97"><p>https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,92.57,671.04,192.74,8.97"><p>https://platform.openai.com/docs/guides/fine-tuning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="10,92.57,660.05,173.12,8.97"><p>https://platform.openai.com/docs/models/gpt-3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="10,92.57,671.00,125.55,8.97"><p>https://zenodo.org/record/3836810</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="14,95.35,649.12,249.27,8.97"><p>https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="14,95.35,660.08,260.62,8.97"><p>https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="14,95.35,671.04,211.19,8.97"><p>https://huggingface.co/google/electra-large-discriminator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="18,95.35,670.99,136.31,8.97"><p>https://github.com/EleutherAI/pythia</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The research is supported by the project "<rs type="projectName">OpenFact -artificial intelligence tools for verification of veracity of information sources and fake news detection</rs>" (<rs type="grantNumber">INFOSTRATEG-I/0035/2021-00</rs>), granted within the <rs type="programName">INFOSTRATEG I program</rs> of the <rs type="institution">National Center for Research and Development</rs>, under the topic: Verifying information sources and detecting fake news.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vRszMX4">
					<idno type="grant-number">INFOSTRATEG-I/0035/2021-00</idno>
					<orgName type="project" subtype="full">OpenFact -artificial intelligence tools for verification of veracity of information sources and fake news detection</orgName>
					<orgName type="program" subtype="full">INFOSTRATEG I program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assistant : Yes</head><p>User : Well , Alan ( ph ) , thank you very much for the question . Assistant : No User : When they tried to reduce taxes , he voted against that 127 times . Assistant : Yes Listing 6: Few-shot learning: an example of the user prompt User : When they tried to reduce taxes , he voted against that 127 times . Assistant :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Few-shot learning with Chain-of-Thought using GPT-4</head><p>The popular method to improve the quality of responses generated by sufficiently large language models is to use the Chain-of-Thought approach <ref type="bibr" coords="9,312.37,336.34,16.41,10.91" target="#b22">[23]</ref>. This approach builds on the few-shot learning method and extends it by decomposing complex, multi-step problems into intermediate steps, which can form a predefined protocol for solving a task. The Chain-of-Thought experiment was conducted using only four examples (two with the label Yes and two with the label No). The examples were selected manually and used as the assistant prompts (see <ref type="bibr" coords="9,432.57,390.54,40.62,10.91">Listing 7)</ref>. So the final answer is : Yes .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,393.91,393.33,10.91;18,112.66,407.46,393.33,10.91;18,112.66,421.01,49.87,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,287.78,393.91,218.21,10.91;18,112.66,407.46,331.29,10.91">Fact checkers facing fake news and disinformation in the digital age: A comparative analysis between spain and united kingdom</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>López-Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vicente-Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,452.45,407.46,53.54,10.91">Publications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,434.55,394.62,10.91;18,112.66,448.10,393.57,10.91;18,112.33,461.65,78.48,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,376.57,434.55,130.71,10.91;18,112.66,448.10,156.40,10.91">Computational fact checking: a content management perspective</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,277.67,448.10,215.46,10.91">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2110" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,475.20,393.33,10.91;18,112.66,488.75,393.33,10.91;18,112.33,502.30,53.12,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,310.85,475.20,195.14,10.91;18,112.66,488.75,115.18,10.91">True or false: Studying the work practices of professional fact-checkers</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Armacost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,236.20,488.75,261.94,10.91">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,515.85,394.53,10.91;18,112.66,529.40,394.62,10.91;18,112.28,542.95,393.71,10.91;18,112.66,556.50,393.33,10.91;18,112.66,570.05,393.33,10.91;18,112.41,583.60,138.60,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,371.19,529.40,136.09,10.91;18,112.28,542.95,296.33,10.91">Overview of checkthat! 2020: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,432.44,542.95,73.54,10.91;18,112.66,556.50,393.33,10.91;18,112.66,570.05,128.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25, 2020. 2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="215" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,597.15,394.53,10.91;18,112.66,610.69,393.64,10.91;18,112.66,624.24,335.58,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hakimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<title level="m" coord="18,332.76,610.69,173.54,10.91;18,112.66,624.24,308.16,10.91">Overview of the CLEF-2023 CheckThat! lab task 1 on check-worthiness in multimodal and multigenre content</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,86.97,394.53,10.91;19,112.66,100.52,393.33,10.91;19,112.66,114.06,313.16,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<title level="m" coord="19,272.86,100.52,233.13,10.91;19,112.66,114.06,217.04,10.91">Overview of the clef-2022 checkthat! lab task 1 on identifying relevant claims in tweets</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>CLEF &apos;2022</note>
</biblStruct>

<biblStruct coords="19,112.66,127.61,395.17,10.91;19,112.66,141.16,394.53,10.91;19,112.66,154.71,144.36,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="19,168.73,127.61,339.10,10.91;19,112.66,141.16,25.91,10.91">Ai rational at checkthat! 2022: using transformer models for tweet classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,161.60,141.16,345.59,10.91;19,112.66,154.71,48.25,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,168.26,393.32,10.91;19,112.66,181.81,394.53,10.91;19,112.66,195.36,144.36,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,181.82,168.26,324.16,10.91;19,112.66,181.81,37.71,10.91">Zorros at checkthat! 2022: ensemble model for identifying relevant claims in tweets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Buliga</forename><surname>Nicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,172.86,181.81,334.33,10.91;19,112.66,195.36,48.25,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,208.91,393.33,10.91;19,112.66,222.46,393.33,10.91;19,112.66,236.01,229.43,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,303.66,208.91,202.33,10.91;19,112.66,222.46,113.34,10.91">Polimi-flatearthers at checkthat! 2022: Gpt-3 applied to claim detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agrestia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hashemianb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carmanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,248.92,222.46,257.06,10.91;19,112.66,236.01,133.32,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,249.56,393.33,10.91;19,112.66,263.11,393.58,10.91;19,112.66,276.66,380.22,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,274.06,249.56,231.93,10.91;19,112.66,263.11,261.38,10.91">Fraunhofer sit at checkthat! 2022: semi-supervised ensemble classification for detecting check-worthy tweets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">Nunes</forename><surname>Grieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,398.04,263.11,108.20,10.91;19,112.66,276.66,284.11,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,290.20,393.33,10.91;19,112.66,303.75,393.33,10.91;19,112.66,317.30,394.53,10.91;19,112.66,330.85,22.69,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,332.12,290.20,173.86,10.91;19,112.66,303.75,286.50,10.91">Tobb etu at checkthat! 2022: detecting attention-worthy and harmful tweets and check-worthy claims</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sonmezer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,424.71,303.75,81.27,10.91;19,112.66,317.30,321.29,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,344.40,393.32,10.91;19,112.66,357.95,393.32,10.91;19,112.66,371.50,229.43,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,254.21,344.40,251.77,10.91;19,112.66,357.95,115.80,10.91">Nus-ids at checkthat! 2022: identifying check-worthiness of tweets using checkthat5</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Mingzhe</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D</forename><surname>Gollapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,251.48,357.95,254.50,10.91;19,112.66,371.50,133.32,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,385.05,393.33,10.91;19,112.66,398.60,388.54,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,297.40,385.05,208.58,10.91;19,112.66,398.60,28.41,10.91">A Benchmark Dataset of Check-worthy Factual Claims</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,164.36,398.60,275.31,10.91">14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,412.15,394.62,10.91;19,112.66,425.70,393.33,10.91;19,112.66,439.25,257.96,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,323.90,412.15,183.38,10.91;19,112.66,425.70,142.11,10.91">PoliMi-FlatEarthers at CheckThat! 2022: GPT-3 applied to claim detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agrestia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Hashemianb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Carmanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,277.47,425.70,228.51,10.91;19,112.66,439.25,161.84,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,452.79,394.53,10.91;19,112.28,466.34,393.70,10.91;19,112.66,479.89,363.75,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,112.28,466.34,244.29,10.91">Alpaca: A strong, replicable instruction-following model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target=")7" />
	</analytic>
	<monogr>
		<title level="j" coord="19,365.34,466.34,140.64,10.91;19,112.66,479.89,83.25,10.91">Stanford Center for Research on Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,493.44,394.53,10.91;19,112.66,506.99,393.32,10.91;19,112.66,520.54,304.30,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coord="19,449.30,506.99,56.68,10.91;19,112.66,520.54,181.60,10.91">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,534.09,350.35,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="19,152.79,534.09,280.41,10.91">Improving language understanding with unsupervised learning</title>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,547.64,393.33,10.91;19,112.66,561.19,170.84,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="19,407.43,547.64,98.55,10.91;19,112.66,561.19,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,574.74,394.53,10.91;19,112.66,588.29,394.53,10.91;19,112.66,601.84,394.53,10.91;19,112.66,615.39,394.53,10.91;19,112.66,628.93,294.89,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m" coord="19,112.66,628.93,172.82,10.91">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,642.48,259.08,10.91" xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Gpt-4 technical report</note>
</biblStruct>

<biblStruct coords="19,112.66,656.03,394.53,10.91;19,112.48,669.58,382.17,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m" coord="19,194.87,669.58,177.09,10.91">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,86.97,394.53,10.91;20,112.66,100.52,394.53,10.91;20,112.66,114.06,393.33,10.91;20,112.66,127.61,193.55,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="20,260.31,114.06,245.68,10.91;20,112.66,127.61,71.08,10.91">Training language models to follow instructions with human feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,141.16,395.17,10.91;20,112.66,154.71,394.92,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m" coord="20,465.66,141.16,42.17,10.91;20,112.66,154.71,272.23,10.91">Chain-ofthought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,168.26,394.53,10.91;20,112.66,181.81,236.19,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="20,294.89,168.26,212.30,10.91;20,112.66,181.81,114.38,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,195.36,393.33,10.91;20,112.28,208.91,163.30,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m" coord="20,250.96,195.36,255.03,10.91;20,112.28,208.91,40.93,10.91">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,222.46,395.17,10.91;20,112.66,236.01,395.00,10.91;20,112.66,250.88,89.67,8.76" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="20,203.56,236.01,273.34,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,263.11,394.53,10.91;20,112.66,276.66,393.33,10.91;20,112.66,290.20,154.08,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<title level="m" coord="20,272.40,276.66,233.58,10.91;20,112.66,290.20,32.08,10.91">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,303.75,393.53,10.91;20,112.66,317.30,362.58,10.91" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="20,392.55,303.75,113.64,10.91;20,112.66,317.30,240.16,10.91">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,330.85,393.32,10.91;20,112.66,344.40,247.77,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12821</idno>
		<title level="m" coord="20,350.87,330.85,155.11,10.91;20,112.66,344.40,125.07,10.91">Rethinking embedding coupling in pre-trained language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,357.95,394.52,10.91;20,112.66,371.50,393.33,10.91;20,112.66,385.05,393.33,10.91;20,112.66,398.60,359.83,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,152.54,371.50,196.66,10.91">CamemBERT: a Tasty French Language Model</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.645</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,370.91,371.50,135.08,10.91;20,112.66,385.05,393.33,10.91;20,112.66,398.60,46.58,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,412.15,393.33,10.91;20,112.66,425.70,297.03,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="20,323.19,412.15,182.80,10.91;20,112.66,425.70,174.50,10.91">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,439.25,395.17,10.91;20,112.66,452.79,395.01,10.91;20,112.66,466.34,138.95,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="20,327.48,439.25,180.35,10.91;20,112.66,452.79,163.58,10.91">GPT-Neo: Large scale autoregressive language modeling with meshtensorflow</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5551208</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5551208.doi:10.5281/zenodo.5551208" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,479.89,393.98,10.91;20,112.66,493.44,372.40,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09714</idno>
		<title level="m" coord="20,386.03,479.89,120.61,10.91;20,112.66,493.44,249.68,10.91">You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,506.99,395.16,10.91;20,112.66,520.54,149.69,10.91" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<title level="m" coord="20,362.78,506.99,145.05,10.91;20,112.66,520.54,27.32,10.91">Revisiting few-sample bert finetuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,534.09,394.53,10.91;20,112.66,547.64,393.33,10.91;20,112.66,561.19,394.62,10.91;20,112.66,574.74,393.33,10.91;20,112.66,588.29,394.53,10.91;20,112.66,601.84,195.45,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="20,318.27,547.64,187.72,10.91;20,112.66,561.19,371.63,10.91">Overview of the clef-2021 checkthat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,112.66,574.74,393.33,10.91;20,112.66,588.29,215.25,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 12th International Conference of the CLEF Association, CLEF 2021</title>
		<meeting><address><addrLine>Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 21-24, 2021. 2021</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="264" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,615.39,394.61,10.91;20,112.66,628.93,366.68,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10601</idno>
		<title level="m" coord="20,429.88,615.39,77.39,10.91;20,112.66,628.93,243.98,10.91">Tree of thoughts: Deliberate problem solving with large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
