<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,309.08,15.42;1,89.29,106.66,411.72,15.42;1,89.29,128.58,127.08,15.43;1,89.29,150.91,227.70,11.96">Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles Notebook for the CheckThat Lab at CLEF 2023</title>
				<funder ref="#_vSCmkMH">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_v3XEZre">
					<orgName type="full">of the Republic of Bulgaria</orgName>
				</funder>
				<funder>
					<orgName type="full">ESI Funds</orgName>
				</funder>
				<funder ref="#_bjw2raz">
					<orgName type="full">European Union-NextGenerationEU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,71.66,11.96"><forename type="first">Georgi</forename><surname>Pachov</surname></persName>
							<email>georgi.patchov@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.58,176.82,85.30,11.96"><forename type="first">Dimitar</forename><surname>Dimitrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.53,176.82,66.56,11.96"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
							<email>koychev@fmi.uni-sofia.bg</email>
							<affiliation key="aff0">
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.08,176.82,70.20,11.96"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<email>preslav.nakov@mbzuai.ac.ae</email>
							<affiliation key="aff1">
								<orgName type="department">Mohamed</orgName>
								<orgName type="institution">Zayed University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,309.08,15.42;1,89.29,106.66,411.72,15.42;1,89.29,128.58,127.08,15.43;1,89.29,150.91,227.70,11.96">Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles Notebook for the CheckThat Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">25187329AEEF278E539DC873574C3FF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Subjectivity detection</term>
					<term>Sentence Embeddings</term>
					<term>Few-shot learning</term>
					<term>Transformer</term>
					<term>Ensemble</term>
					<term>Natural Language Processing</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The wide-spread use of social networks has given rise to subjective, misleading, and even false information on the Internet. Thus, subjectivity detection can play an important role in ensuring the objectiveness and the quality of a piece of information. This paper presents the solution built by the Gpachov team for the CLEF-2023 CheckThat! lab Task 2 on subjectivity detection. Three different research directions are explored. The first one is based on fine-tuning a sentence embeddings encoder model and dimensionality reduction. The second one explores a sample-efficient few-shot learning model. The third one evaluates fine-tuning a multilingual transformer on an altered dataset, using data from multiple languages. Finally, the three approaches are combined in a simple majority voting ensemble, resulting in 0.77 macro F1 on the test set and achieving 2nd place on the English subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Subjectivity is a feature of language and a form of bias, in which whenever a person is sharing information, it comes out skewed by the speaker's own personal preferences, beliefs and views. In today's interconnected world, where opinions and biases travel fast and far, subjectivity detection can be a very important piece in order to ensure information reporting is done in a clear, objective and unbiased fashion.</p><p>Specifically, subjectivity in news and media articles can be nuanced, subtle and difficult to identify. Detection of subjectivity in such texts can play an important role in identifying potentially misleading or malicious texts and in detecting fake news online.</p><p>In Task 2 of CheckThat! Lab at CLEF 2023 <ref type="bibr" coords="1,282.43,564.69,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,295.85,564.69,7.49,10.91" target="#b1">2]</ref>, systems are required to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic instead. This is a binary classification task in which systems have to identify whether a text (a sentence or a paragraph) is subjective or objective.</p><p>This paper explores the effects of fine-tuning a large pre-trained language model on the subjectivity task. Additionally, we have examined the angles of few-shot-learning and finetuning a sentence embedding model. Finally, an ensemble method is proposed to unify all three into one solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While sentiment analysis can be regarded as a classic NLP task with lots of research already available on the subject, subjectivity classification is deemed to be a slightly less popular research topic. <ref type="bibr" coords="2,120.76,240.44,13.00,10.91" target="#b2">[3]</ref> looks at the subjectivity detection task as a way to improve sentiment analysis classifiers by excluding neutral (objective) sentences. They offer a broad survey on published subjectivity detection methods, categorizing them into syntactic (keyword-spotting, lexical affinity, statistical methods), semantic (parse trees, convolutional neural networks, extreme learning machines) and multi-modal (BiLSTM, multiple-kernel learning).</p><p>In <ref type="bibr" coords="2,112.21,308.18,11.54,10.91" target="#b3">[4]</ref>, authors explore multi-task learning with hard parameter sharing via Neural Tensor Network. They demonstrate that using a single network with shared layers while learning on two semantically related datasets can improve performance on both datasets.</p><p>In <ref type="bibr" coords="2,111.62,348.83,11.28,10.91" target="#b4">[5]</ref>, authors compare Word2Vec and BERT embedding models in the context of subjectivity detection. With additional classification models to process the embedding outputs, authors demonstrate the superiority of BERT embeddings in high-resource settings, while showing Word2Vec embeddings can be more efficient in low-resource settings. In the current challenge, pairing an embedding encoder with various classification models is also explored.</p><p>In <ref type="bibr" coords="2,112.61,416.58,11.58,10.91" target="#b5">[6]</ref>, authors compare the performance of pure transformer models against a variety of more specialized methods for short text classification. Their results show superior performance of the transformer models and part of the research performed in this paper is influenced by their findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data and Baseline Solution</head><p>For the subjectivity detection task, datasets in 6 different languages were provided -Arabic, Dutch, English, German, Italian and Turkish. A total of 7 datasets were available -one for each language, and one for the multilingual version of the task. The English dataset contained a total of 1019 examples. 800 of the provided examples were labeled as training, the other 219 as validation. A baseline solution <ref type="foot" coords="2,241.14,568.29,3.71,7.97" target="#foot_0">1</ref> is provided by competition organizers, which consists of a sentence encoder model, producing sentence embeddings, which are then classified with Logistic Regression.</p><p>An interesting insight was that most of the sequences were relatively short. For the English dataset, the average number of words in a sequence was 23, while 90% of sequences consisted of 40 words or less.</p><p>The English training set is imbalanced, with 64% of samples labeled objective and 36%subjective. This imbalance is not present in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Evaluation</head><p>Three research directions were explored, each of them resulting in a separate solution. The final program used for submission is a simple majority voting ensemble of the three solutions. The first research direction explores what is achievable using sentence embeddings. The second one looks at a few-shot-learning model and dual-stage fine-tuning. The third is based on fine-tuning a pre-trained transformer model, also utilizing training data from the other languages available for the task.</p><p>All evaluations of experiments are done on the English validation set, provided by the organizers. All research directions will be described in more detail in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sentence Embeddings</head><p>Multiple experiments with sentence embeddings were conducted. All of them were based on using a pre-trained sentence embedding encoder model <ref type="bibr" coords="3,333.51,317.26,12.68,10.91" target="#b6">[7]</ref> . The following ideas were explored:</p><p>‚Ä¢ Using more powerful classifiers on top of sentence embeddings output ‚Ä¢ Using dimensionality reduction ‚Ä¢ Fine-tuning the sentence embeddings encoder in the context of subjectivity detection Initially, the baseline solution provided by organizers used sentence embeddings and a simple Logistic Regression on top to produce classification outputs. More complex classifiers were tested. Multiple different classifiers yielded improvements, measured on the validation set. The most performant was LogisticRegression (from sklearn), using ElasticNet penalty, balanced class weights, 'saga' solver and 0.5 as regularization constant.</p><p>A potential place for improvement was related to sentence embeddings dimensionality. Due to only having 800 training examples, embeddings of dimensionality 384 could prove challenging for a classifier. Using dimensionality reduction, information from embedding vectors can be further compressed in a way that could make it easier for classifiers to find a proper decision boundary.</p><p>Best performance was achieved using PCA with 110 remaining components (out of 384), which explained 92.5% of total variance. Experiments with different classifiers and dimensionality reduction are summarized in the first column of Table <ref type="table" coords="3,332.05,560.67,3.74,10.91" target="#tab_0">1</ref>.</p><p>While dimensionality reduction can help classifiers, the sentence embeddings themselves were generally created for a very broad category of NLP tasks. Fine-tuning the embeddings encoder for subjectivity detection proved to be helpful for all of the tested classifiers.</p><p>Embeddings were fine-tuned using cosine similarity loss in a contrastive learning manner. The new similarity label of a pair of sentences consisted of two equally weighted componentstheir original similarity and their label-based similarity. Label-based similarity is defined as 1 if the two sentences are from the same class and 0 otherwise. This can be summarized with the following equation: Training pairs were generated -each sentence was paired with every other sentence and a similarity label was generated using Formula 1. In total, 2N*(2N-1) training pairs were generated.</p><p>All of the tested classifiers performed better using the fine-tuned embeddings (with N=100) instead of original embeddings. All experiments with classifiers, dimensionality reduction and fine-tuned sentence embeddings are summarized in Table <ref type="table" coords="4,358.39,458.32,3.81,10.91" target="#tab_0">1</ref>. For both the original and the fine-tuned embeddings, best performance was achieved through PCA and Linear Regression with elastic net penalty. All macro F1 scores are measured on the (English) validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Few-Shot Learning</head><p>The second research direction explored what can be achieved with few-shot learning. Experiments are based on the SetFit model from HuggingFace <ref type="bibr" coords="4,338.79,548.70,11.43,10.91" target="#b7">[8]</ref>. While conceptually similar to the idea of fine-tuning sentence embeddings, the SetFit model has numerous advantages, including faster training, better sample efficiency and a dual-stage fine-tuning mechanism. In the first stage, the classification head is frozen and embeddings are fine-tuned. Vice versa in the second stage.</p><p>Experiments and results for this approach are outlined in Table <ref type="table" coords="4,381.44,616.44,3.70,10.91">2</ref>. Results are similar to finetuning sentence embeddings encoder, but achieved while using a lot less information (samples) from the task-specific dataset, which showed promise of low variance and good generalization capabilities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transformer Fine-Tuning</head><p>The third research direction was based on the idea of fine-tuning a transformer model to the dataset provided. Lots of transformer models are available in the HuggingFace hub <ref type="bibr" coords="5,462.31,385.05,11.47,10.91" target="#b8">[9]</ref>. BERT <ref type="bibr" coords="5,89.29,398.59,16.32,10.91" target="#b9">[10]</ref>, RoBERTa <ref type="bibr" coords="5,157.22,398.59,17.98,10.91" target="#b10">[11]</ref> and DeBERTa <ref type="bibr" coords="5,242.63,398.59,17.98,10.91" target="#b11">[12]</ref> were used throughout the experiments. Limitations in hardware capabilities restricted the experiments to only 'base' and 'large' variants of the models. Xlarge and xxlarge models were not tested. Initially, experiments were performed using the English dataset only. Later on, additional experiments were performed including data from other languages. Table <ref type="table" coords="5,126.54,466.34,4.97,10.91" target="#tab_1">3</ref> summarizes the experiments and results when using the English dataset. Fine-tuning a DeBERTa-v2-large achieved the best performance on the validation set.</p><p>Additional experiments were performed using data from other languages. Multiple training datasets were created. The first one consisted of all the available data for all languages. The second one consisted of English, Arabic and Turkish -the languages who had published baseline solutions with higher F1 than English. The third one contained training samples from English and German translated to English using a neural model <ref type="bibr" coords="5,328.30,547.64,16.09,10.91" target="#b12">[13]</ref>. While creating the datasets, English validation samples were always manually excluded as a final step to prevent data leakage.</p><p>For all three datasets, multilingual transformer models were used -bert-base-multilingual <ref type="bibr" coords="5,89.29,588.28,16.25,10.91" target="#b9">[10]</ref>, mDeBERTa <ref type="bibr" coords="5,166.06,588.28,17.91,10.91" target="#b13">[14]</ref> and xlm-roberta-base <ref type="bibr" coords="5,284.61,588.28,16.25,10.91" target="#b14">[15]</ref>. Table <ref type="table" coords="5,126.26,601.83,4.97,10.91">4</ref> summarizes the results. An xlm-roberta-base model was fine-tuned using all available data, achieving a macro F1 score of 0.83, which showed slight improvement over using Englishonly dataset. The best results were obtained by using English and German translated to English, fitted to xlm-roberta-base model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ensemble</head><p>Best performing solutions from all three research directions were chosen and formed into an ensemble (Figure <ref type="figure" coords="6,184.60,414.77,3.65,10.91" target="#fig_0">1</ref>). The final solution was a simple majority voting ensemble from the following solutions:</p><p>‚Ä¢ A fine-tuned sentence embeddings encoder, producing improved sentence embeddings (in the context of this task). The encoder output passes through dimensionality reduction (384 dimensions reduced to 110). The reduced embeddings are then classified through LogisticRegression with equally weighted L1 and L2 penalties, 'saga' solver, balanced class weights and 0.5 regularization constant. ‚Ä¢ A few-shot learning SetFit model trained using dual-stage fine-tuning procedure with N=20 sentence pairs. ‚Ä¢ An xlm-roberta-base model, fine-tuned on English and German translated to English.</p><p>The final submitted ensemble was able to achieve 0.85 macro F1 on English validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The final solution achieved macro F1 of 0.85 on validation set and 0.77 on test set. This indicates generalization issues, likely stemming from overfitting the solution to the validation set.</p><p>Each of the three separate methods was analyzed on the test set, to identify possible causes for the significantly lower performance on the test set. All components of the solution perform worse on the test set than on the validation set, indicating a more difficult test set. The first method however incurs a bigger loss of performance than the others. This is likely caused by an issue in the fine-tuning procedure, resulting in the encoder model producing highly specialized embeddings, which lose a bigger part of their pretrained semantics than optimal and thus generalize poorly. Less examples used for contrastive learning, smaller learning rate, or using a holdout set to check for generalization issues could have mitigated this problem.</p><p>Results also indicate that the best performer is an xlm-roberta-base model trained on English and translated German. All test and validation F1 scores are summarized in Table <ref type="table" coords="7,453.81,395.73,3.74,10.91" target="#tab_2">5</ref>.</p><p>Table <ref type="table" coords="7,127.84,409.28,5.17,10.91">6</ref> indicates that the solution is biased to expect more frequent 'objective' examples, hence the higher recall but lowered precision. This is likely stemming from the initial class imbalance in the English training set, where 64% of examples are labeled objective. While this imbalance doesn't seem to have a major impact on final F1 scores, a more balanced solution could have been achieved by using any of the well-known class-imbalance techniques, e.g. sample weighing or choosing a different threshold.</p><p>Overall, xlm-roberta model trained on English and translated German seem to be the best performer. The few-shot-learning approach yielded decent test results. The fine-tuned SBERT encoder method seems to not be general enough and is reducing the performance of the full solution. Submitting predictions from only the transformer model would have resulted in 0.79 macro F1, which could have won the English subtask challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>As indicated by Table <ref type="table" coords="7,190.40,603.40,3.81,10.91" target="#tab_2">5</ref>, the transformer-based solution proved to be the most effective and robust out of the ones used in the ensemble. Further experiments can be conducted with newer and more promising versions of existing transformer models. For example, DeBERTa-V3 <ref type="bibr" coords="7,488.00,630.49,17.99,10.91" target="#b13">[14]</ref> is reported to improve performance of the original DeBERTa model with 1.37% on the GLUE benchmark. This can prove relevant to the task of subjectivity classification as well.</p><p>Additionally, due to resource constraints, transformer models with 'xlarge' and 'xxlarge' architectures were not used for this research. For the same reason, very little hyperparameter tuning was performed. Using a larger model and exploring bigger hyperparameter space can potentially improve the results.</p><p>Finally, the transformed-based approach used for subtask 2A (English) can also be attempted and used for the other 5 languages in the task -Arabic, Dutch, German, Italian and Turkish.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,344.56,141.82,8.93;6,89.29,84.19,416.68,247.81"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schema of the ensemble</figDesc><graphic coords="6,89.29,84.19,416.68,247.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,418.67,311.00"><head>Table 1</head><label>1</label><figDesc>Classifiers and dimensionality reduction with original vs fine-tuned embeddings encoder (Macro F1)</figDesc><table coords="4,88.99,122.10,396.60,190.16"><row><cell cols="3">Classifier‚àñEmbedding Original Sentence Embeddings Fine-Tuned Embeddings (N=100)</cell></row><row><cell>Baseline (SBERT + LR)</cell><cell>0.74</cell><cell>0.76</cell></row><row><cell>SVM</cell><cell>0.76</cell><cell>0.78</cell></row><row><cell>ElasticNet</cell><cell>0.77</cell><cell>0.8</cell></row><row><cell>PCA + ElasticNet</cell><cell>0.78</cell><cell>0.81</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row><row><cell>Few-shot learning experiments with SetFit</cell><cell></cell><cell></cell></row><row><cell cols="3">Fine-tuning regime Number of samples Macro F1</cell></row><row><cell>Single stage</cell><cell>10</cell><cell>0.79</cell></row><row><cell>Single stage</cell><cell>20</cell><cell>0.8</cell></row><row><cell>Dual stage</cell><cell>20</cell><cell>0.81</cell></row><row><cell>Dual stage</cell><cell>64</cell><cell>0.8</cell></row><row><cell>Dual stage</cell><cell>100</cell><cell>0.79</cell></row></table><note coords="4,89.80,364.21,415.68,9.57;4,495.06,377.03,11.57,10.91;4,100.20,390.58,407.46,10.91"><p><p>ùëÅ ùëíùë§_ùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶_ùêøùëéùëèùëíùëô(ùê¥, ùêµ) = 0.5 * ùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶(ùê¥, ùêµ) + 0.5 * (ùëêùëôùëéùë†ùë†(ùê¥) == ùëêùëôùëéùë†ùë†(ùêµ))</p>(1) To generate training samples, N objective and N subjective sentences were randomly selected.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,383.82,233.74"><head>Table 3</head><label>3</label><figDesc>Transformer experiments with English dataset</figDesc><table coords="5,88.99,122.10,383.82,202.12"><row><cell></cell><cell cols="3">Transformer Model Architecture Macro F1</cell></row><row><cell></cell><cell>BERT</cell><cell>base</cell><cell>0.77</cell></row><row><cell></cell><cell>BERT</cell><cell>large</cell><cell>0.78</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>base</cell><cell>0.81</cell></row><row><cell></cell><cell>DeBERTa-v2</cell><cell>large</cell><cell>0.82</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Transformer experiments with multilingual datasets</cell><cell></cell></row><row><cell>Model</cell><cell>Architecture</cell><cell cols="2">Training Set</cell><cell>F1 macro</cell></row><row><cell>bert-multilingual</cell><cell>base</cell><cell>All data</cell><cell></cell><cell>0.81</cell></row><row><cell>xlm-roberta</cell><cell>large</cell><cell>All data</cell><cell></cell><cell>0.81</cell></row><row><cell>mdeberta-v3</cell><cell>base</cell><cell>All data</cell><cell></cell><cell>0.82</cell></row><row><cell>xlm-roberta</cell><cell>base</cell><cell>All data</cell><cell></cell><cell>0.83</cell></row><row><cell>xlm-roberta</cell><cell>base</cell><cell cols="2">English, Arabic and Turkish</cell><cell>0.82</cell></row><row><cell>xlm-roberta</cell><cell>base</cell><cell cols="2">English, German translated to English</cell><cell>0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,345.05,183.69"><head>Table 5</head><label>5</label><figDesc>Test scores of individual solutions and ensemble</figDesc><table coords="7,88.99,119.88,345.05,154.29"><row><cell>Solution</cell><cell></cell><cell cols="2">Val F1 score Test F1 score</cell></row><row><cell cols="2">Fine-tuned S-BERT, PCA, ElasticNet</cell><cell>0.81</cell><cell>0.7</cell></row><row><cell>SetFit model</cell><cell></cell><cell>0.81</cell><cell>0.76</cell></row><row><cell>xlm-roberta-base</cell><cell></cell><cell>0.84</cell><cell>0.79</cell></row><row><cell cols="2">Majority voting ensemble</cell><cell>0.85</cell><cell>0.77</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Per-class precision, recall and f1-scores of the final solution</cell><cell></cell></row><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>subjective</cell><cell>0.83</cell><cell>0.71</cell><cell>0.77</cell></row><row><cell>objective</cell><cell>0.73</cell><cell>0.84</cell><cell>0.78</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.01,304.10,8.97"><p>https://gitlab.com/checkthat_lab/clef2023-checkthat-lab/-/tree/main/task2/baseline</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is partially funded by Project <rs type="grantNumber">UNITe BG05M2OP001-1.001-0004</rs> funded by the <rs type="programName">OP "Science and Education for Smart Growth</rs>", co-funded by the <rs type="funder">EU</rs> through the <rs type="funder">ESI Funds</rs>, and partially financed by the <rs type="funder">European Union-NextGenerationEU</rs>, through the <rs type="programName">National Recovery and Resilience Plan</rs> <rs type="funder">of the Republic of Bulgaria</rs>, project No <rs type="grantNumber">BG-RRP-2.004-0008</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vSCmkMH">
					<idno type="grant-number">UNITe BG05M2OP001-1.001-0004</idno>
					<orgName type="program" subtype="full">OP &quot;Science and Education for Smart Growth</orgName>
				</org>
				<org type="funding" xml:id="_bjw2raz">
					<orgName type="program" subtype="full">National Recovery and Resilience Plan</orgName>
				</org>
				<org type="funding" xml:id="_v3XEZre">
					<idno type="grant-number">BG-RRP-2.004-0008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,312.61,394.53,10.91;8,112.33,326.16,394.85,10.91;8,112.14,339.71,395.05,10.91;8,112.66,353.26,394.53,10.91;8,112.66,366.81,394.53,10.91;8,112.66,380.36,395.17,10.91;8,112.66,393.91,393.33,10.91;8,112.33,407.46,81.51,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,179.18,339.71,328.01,10.91;8,112.66,353.26,307.13,10.91">Overview of the CLEF-2023 CheckThat! Lab checkworthiness, subjectivity, political bias, factuality, and authority of news articles and their source</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,233.97,380.36,273.86,10.91;8,112.66,393.91,393.33,10.91;8,112.33,407.46,27.43,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,112.66,421.01,395.17,10.91;8,112.66,434.55,394.53,10.91;8,112.66,448.10,395.17,10.91;8,112.66,461.65,394.52,10.91;8,112.14,475.20,395.05,10.91;8,112.33,488.75,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,239.52,448.10,268.31,10.91;8,112.66,461.65,95.64,10.91">Overview of the CLEF-2023 CheckThat! lab task 2 on subjectivity in news articles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Antici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K√∂hler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Korre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Leistra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Muti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mehmet Deniz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.14,475.20,390.50,10.91">Working Notes of CLEF 2023-Conference and Labs of the Evaluation Forum, CLEF 2023</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,502.30,393.33,10.91;8,112.66,515.85,393.98,10.91;8,112.66,529.40,311.47,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,352.77,502.30,153.21,10.91;8,112.66,515.85,251.28,10.91">Distinguishing between facts and opinions for sentiment analysis: Survey and challenges</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2017.12.006</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2017.12.006" />
	</analytic>
	<monogr>
		<title level="j" coord="8,374.38,515.85,88.11,10.91">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,542.95,393.61,10.91;8,112.66,556.50,256.70,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,282.94,542.95,223.33,10.91;8,112.66,556.50,126.03,10.91">Polarity and subjectivity detection with multitask learning and bert embedding</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Satapathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pardeshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05363</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,570.05,395.17,10.91;8,112.66,583.60,394.51,10.91;8,112.66,599.59,123.08,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,398.70,570.05,109.14,10.91;8,112.66,583.60,161.01,10.91">Objectivity and Subjectivity Classification with BERT for</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T V</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Soo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cher</surname></persName>
		</author>
		<idno type="DOI">10.2991/978-94-6463-094-7_20</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,277.57,583.60,67.25,10.91">Bahasa Melayu</title>
		<imprint>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,610.69,393.33,10.91;8,112.66,624.24,349.17,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Scherp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16878</idno>
		<title level="m" coord="8,194.91,610.69,311.07,10.91;8,112.66,624.24,219.43,10.91">Transformers are short text classifiers: A study of inductive short text classifiers on benchmarks and real-world datasets</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,637.79,394.53,10.91;8,112.66,651.34,122.77,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="8,219.42,637.79,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,284.48,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,468.26,86.97,37.72,10.91;9,112.66,100.52,154.04,10.91">Efficient few-shot learning without prompts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">E S</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Korat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pereg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11055</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,394.53,10.91;9,112.66,127.61,394.53,10.91;9,112.66,141.16,393.33,10.91;9,112.66,154.71,393.33,10.91;9,112.66,168.26,394.53,10.91;9,112.66,181.81,305.86,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,296.48,141.16,209.50,10.91;9,112.66,154.71,46.35,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,186.08,154.71,319.91,10.91;9,112.66,168.26,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.66,208.91,311.37,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="9,326.58,195.36,179.40,10.91;9,112.66,208.91,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,222.46,395.17,10.91;9,112.66,236.01,395.01,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="9,137.85,236.01,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,249.56,393.33,10.91;9,112.66,263.11,168.38,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m" coord="9,261.90,249.56,244.09,10.91;9,112.66,263.11,38.60,10.91">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,395.17,10.91;9,112.66,290.20,395.01,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00401</idno>
		<title level="m" coord="9,425.61,276.66,82.21,10.91;9,112.66,290.20,266.10,10.91">Multilingual translation with extensible multilingual pretraining and finetuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,393.33,10.91;9,112.66,317.30,313.71,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09543</idno>
		<title level="m" coord="9,213.83,303.75,292.16,10.91;9,112.66,317.30,183.73,10.91">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,330.85,394.53,10.91;9,112.66,344.40,393.33,10.91;9,112.66,357.95,393.32,10.91;9,112.66,371.50,397.48,10.91;9,112.36,387.49,157.20,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,271.58,344.40,234.41,10.91;9,112.66,357.95,20.10,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzm√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,155.39,357.95,350.59,10.91;9,112.66,371.50,233.00,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
