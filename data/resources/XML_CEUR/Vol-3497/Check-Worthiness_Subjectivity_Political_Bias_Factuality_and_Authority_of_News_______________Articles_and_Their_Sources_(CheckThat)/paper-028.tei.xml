<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,385.37,15.42;1,89.29,106.66,318.86,15.42;1,89.29,128.58,401.03,15.43;1,89.29,150.49,71.19,15.43;1,89.29,172.83,231.64,11.96">Fraunhofer SIT at CheckThat! 2023: Can LLMs Be Used for Data Augmentation &amp; Few-Shot Classification? Detecting Subjectivity in Text Using ChatGPT Notebook for the CheckThat! Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">Lernlabor Cybersicherheit&quot; (LLCS)</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder>
					<orgName type="full">ATHENE -CRISIS</orgName>
				</funder>
				<funder>
					<orgName type="full">Hessian Ministry of Higher Education, Research, Science and the Arts</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,198.73,115.10,11.96"><forename type="first">Raphael</forename><forename type="middle">Antonius</forename><surname>Frick</surname></persName>
							<email>raphael.frick@sit.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<orgName type="institution" key="instit2">ATHENE -National Research Center for Applied Cybersecurity</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,385.37,15.42;1,89.29,106.66,318.86,15.42;1,89.29,128.58,401.03,15.43;1,89.29,150.49,71.19,15.43;1,89.29,172.83,231.64,11.96">Fraunhofer SIT at CheckThat! 2023: Can LLMs Be Used for Data Augmentation &amp; Few-Shot Classification? Detecting Subjectivity in Text Using ChatGPT Notebook for the CheckThat! Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D44047647CF789D70B35B9E6E876AE5D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Subjectivity Detection</term>
					<term>Large Language Models</term>
					<term>Few-Shot Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fight against the spread of misinformation and rumors on the Internet has become a difficult issue lately. In some cases, it is difficult to tell whether a news article published on the Internet contains opinions or was written objectively. This year's CheckThat! 2023 Task 2 dealt with the recognition of such texts. Due to the recent rise of large language models, this work analyzed the extent to which large language models such as ChatGPT can be used to augment unbalanced data sets and whether they can serve as a reliable few-shot classifier. The proposed approaches were trained and evaluated on the English and German subtasks of the challenge. While the models trained with the augmented data were unable to outperform the BERT models trained without the additional data, the few-shot classification scheme was able to outperform across different data set splits, most notably with the English test set. On the private test sets, the proposed ChatGPT-based few-shot classifiers achieved an 洧냧1 value of 0.73 on the English data and an 洧냧1 value of 0.68 on the German data. However, they have not been shown to achieve stable performance over multiple data set splits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media has introduced new ways how information can be disseminated, allowing individuals to share news easily and opinions with a global audience. However, this unprecedented accessibility has also paved the way for the rapid spread of fake news. The viral nature of social media platforms amplifies the reach and impact of false information, often leading to widespread misinformation and confusion.</p><p>A particular challenge associated to this is the ability to distinguish between news articles shared on the internet that are written subjectively or objectively. Subjectively written texts require special attention because they do not express facts in a value-free way, but instead contain feelings and opinions of the author.</p><p>As part of this year's CheckThat! 2023 competition <ref type="bibr" coords="2,337.14,114.06,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,351.62,114.06,7.65,10.91" target="#b1">2]</ref>, Task 2 <ref type="bibr" coords="2,398.54,114.06,13.00,10.91" target="#b2">[3]</ref> was about assessing whether a sentence in a news article was written in an objective or subjective tone. The task was offered to several languages, including Arabic, Dutch, English, German, Italian, and Turkish. In this paper, we describe the approach we used in the competition to classify news articles written in English and German. The approach takes advantage of ChatGPT, a large language model (LLM) based on GPT3.5. LLMs are deep learning models trained on large amounts of textual data so that they can produce coherent and contextually relevant responses to natural language input. They have demonstrated their remarkable capabilities on a variety of NLP tasks, including speech translation, sentiment analysis, text generation, and question answering. Despite their success, LLMs still face several challenges that warrant further investigation. One such challenge is the generation of biased content, which stems from the models' training data reflecting the biases present in the real world. In this paper, we investigate whether LLMs can be used to enrich imbalanced datasets and whether they are useful for distinguishing between objectively and subjectively written text by using them as few-shot classifiers. By using ChatGPT as a few-shot classifier, an 洧냧 1 score of 0.73 was achieved on the English private test data set, whereas an 洧냧 1 score of 0.68 was achieved on the German test set.</p><p>The remainder of the paper is structured as follows. In Section 2, an introduction to large language models and solutions to detecting objectivity in text is given. Section 3 gives a description over the data set provided by and used throughout the competition. The analyzed methods and their results on each data set are showcased in Section 4. The paper then concludes with a discussion on the achieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Large Language Models</head><p>Recently, large language models such as ChatGPT<ref type="foot" coords="2,311.51,462.89,3.71,7.97" target="#foot_0">1</ref> , LLama <ref type="bibr" coords="2,353.66,464.64,11.43,10.91" target="#b3">[4]</ref>, and Bard<ref type="foot" coords="2,411.65,462.89,3.71,7.97" target="#foot_1">2</ref> have gained a lot of popularity. These models were trained on large datasets comprising billions of websites and documents, and can thus recognize patterns that enable them to produce conditioned text.</p><p>Even though they are capable of synthesizing text even for complex topics, there are still some major challenges that require solving. Since the models are trained on data collected within a certain time period, the generated texts cannot refer to events happening thereafter. Based on the collected data, they try to estimate which token is most likely to follow next for a given sequence of tokens. However, this has the consequence that the texts produced are tainted with biases, e.g., in relation to gender and politics. Because the models do not include a control loop that determines whether the statements made in the synthesized texts are true or not, some texts contain fictitious statements. Most models are unimodal and consider only textual data. Recently, however, the focus has shifted from purely text-based models to models that support multiple modalities, such as visual data combined with textual data <ref type="bibr" coords="2,448.42,627.23,11.43,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Zero-&amp; Few-Shot Learning</head><p>Machine learning algorithms, and deep learning classifiers in particular, require a large amount of labeled data when training. However, in some cases it is not possible to provide a large set of examples that could be used to train such a model.</p><p>Here, zero-shot and few-shot learning can be used to solve this problem. Few-shot learning is a type of machine learning in which the model is trained using a small number of examples. The goal is to make predictions for an NLP task without having seen a single labeled example (in zero-shot learning <ref type="bibr" coords="3,172.15,188.83,11.93,10.91" target="#b5">[6]</ref>) or only a few examples (in few-shot learning <ref type="bibr" coords="3,389.91,188.83,11.93,10.91" target="#b6">[7]</ref>) that are specific to the task at hand. While large language models have proven useful in text generation, they can also be used as zero and few-shot classifiers, as shown in the work of <ref type="bibr" coords="3,374.12,215.93,11.24,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,388.08,215.93,8.88,10.91" target="#b8">9]</ref> and <ref type="bibr" coords="3,418.50,215.93,16.10,10.91" target="#b9">[10]</ref>. Therefore, this work investigated whether large language models are also suitable for discriminating between subjectively and objectively written texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Set Description</head><p>The goal of Task 2 of this year's CheckThat! Lab Challenge was to predict whether a text snippet was written in a subjective or objective tone. The task covered the languages Arabic, Dutch, English, German, Italian, and Turkish. However, we only participated in the English and German language variant of the task.</p><p>The data sets of both languages consisted of text snippets gathered from news articles. Each of these were either written subjectively or objectively, resulting in a binary classification problem. Labels were created by human annotators and provided along with the challenge data set. Examples from the data set are shown in Table <ref type="table" coords="3,318.28,396.50,3.74,10.91" target="#tab_0">1</ref>.</p><p>The class distributions from each of the provided the data sets can be viewed in Table <ref type="table" coords="3,483.61,410.05,3.74,10.91" target="#tab_1">2</ref>. As can be seen, the data sets are slightly imbalanced, with most samples belonging to the objectivity class. This is to be expected, as most news articles are more likely to be written in a value-free manner and do not contain opinions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods and Results</head><p>Large language models such as ChatGPT can be used to generate text based on a specific instruction. It can be used for multiple use cases, such as code generation, but also to solve specific tasks, including text analysis. In this work, ChatGPT was used to analyze how well it performs in enriching data sets and whether it can solve the task of subjectivity and objectivity itself by using it as a few-shot classifier. For this, ChatGPT (GPT-3.5-turbo) was accessed using its official API<ref type="foot" coords="4,152.16,389.82,3.71,7.97" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Using Large Language Models for Dataset Augmentation</head><p>Since the data sets provided for each language had a slight class imbalance, the goal was to use ChatGPT to synthesize additional samples to mitigate potential negative side effects. For this purpose, each sample was taken from the data set and ChatGPT was instructed to transform it into a sentence representing the opposing class. Here, the following instruction was used:</p><p>User: Rewrite a text that was written in a subjective tone or in an objective tone so that it represents the other class. Subjectivity: "{sentence}" Objectivity:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT: {synthesis}</head><p>The Table <ref type="table" coords="4,140.39,603.89,5.17,10.91" target="#tab_3">4</ref> shows the synthesis results of some examples. While the synthesized sentences correctly express the target class, the texts written in a subjective tone were often written from a personal perspective. As a result, these sentences often begin with "I wonder... ", "In my opinion... ", or "To me... ". The subjectively written sentences in the training set provided It is not entirely clear how BA.4.6 has emerged, but it's possible it could be a recombinant variant. OBJ GPT In my opinion, it seems that the origin of BA.4.6 is not fully understood, but it is plausible that it could be a recombinant strain. SUBJ 3.</p><p>Akzeptieren Sie keine Erpressung. SUBJ GPT Eine Erpressung sollte nicht akzeptiert werden. OBJ 4.</p><p>Der andere Angeklagte bekundete, er k칬nne sich an den ganzen Vorgang nicht erinnern. OBJ GPT "Ich habe den Eindruck gewonnen, dass der andere Angeklagte sich nicht an den Vorgang erinnern konnte. " SUBJ as part of the competition did not follow this style of writing. Consequently, the synthesized sentences do not represent the data from the provided data set well. The same also applies for sentences synthesized from the German data set.</p><p>For the classification, BERT-based models <ref type="bibr" coords="5,277.39,255.79,19.54,10.91" target="#b10">[11]</ref> were fine-tuned on both, the provided sentences of the train set and the synthesized sentences by ChatGPT. The English model was based on bert-base-cased and the German model on bert-base-german-cased. During training, the Adam algorithm <ref type="bibr" coords="5,136.38,296.43,18.03,10.91" target="#b11">[12]</ref> was used as an optimizer because it has an adaptive learning rate mechanism. As the initial learning rate, a value of 0.0004 was set. The model was fine-tuned over five epochs, using a batch size of 24. To ensure optimal performance on the private test split of the competition data set, only the model with the highest performance on the development split was retained.</p><p>As it can be seen in Table <ref type="table" coords="5,216.35,364.18,3.74,10.91" target="#tab_2">3</ref>, the models fine-tuned without any additional data provided by ChatGPT performed better on all data sets. One reason for this could be the difference between the writing style of the artificially generated examples and the writing style in the provided data set. Therefore, in the case of subjectivity detection, it is not advisable to supplement the data with additional data from ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Using Large Language Models as Few-Shot Classifiers</head><p>In the second approach, ChatGPT was used exclusively to automatically classify the provided samples without using a separate classifier. Here, one can distinguish between zero-and few-shot classification. In contrast to the few-shot classification, zero-shot classification takes only a description of the task as input, without resorting to examples for reference. As the data set splits already contain labels, a few-shot classification scheme was chosen. It exploits the ability to self-define the output of the GPT model via its API. In this way, a fake chat history was first built that mimicked the classification responses ChatGPT would have returned based on sample text snippets. This chat history was then used to perform analysis on any sentences from the development and test sets. For the classification, the following instructions were used: User: Classify, whether a text was written in a subjective tone or in an objective tone. Text: "First by habit one thinks of those for which we have traditional images: The machine, the boss, the pork barrel, the spoils system, the politician everywhere in his popular character, acquiring merit and power by spending public money; doing things for his people with the money of other people, taking care at the same time to do enough for himself with everybody's money. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class:</head><p>Simulated System Response: subjectively User: Classify, whether a text was written in a subjective tone or in an objective tone. Text: "Garina, who was there, recalls that Belgrade "looked like a war zone". " Class:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulated System Response: objectively</head><p>User: Classify, whether a text was written in a subjective tone or in an objective tone. Text: "{sentence}" Class:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT: {prediction}</head><p>Table <ref type="table" coords="6,116.66,303.75,5.17,10.91" target="#tab_2">3</ref> showcases the results of the few-shot classifier on the test set. While it showed the lowest performance on the English development data set, it outperformed all models on the test data set. However, the opposite was true when evaluating the German data set. Here, it achieved the best 洧냧 1 values on the development data set, but performed significantly worse on the test data set than the fine-tuned BERT model or the model trained on additional artificially generated training data. Since the data in the private test data sets may differ from the data in the development data set in terms of certain characteristics, such as general writing style, the fine-tuned models may generalize less well on unseen data. However, since the ChatGPT-based few-shot classifier does not require any further training process, its performance is less stable across multiple data sets in contrast to fine-tuned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>ChatGPT and other large language models such as LLama and BARD have attracted a lot of attention recently. After being trained on large corpora such as documents, web pages, and more, they have been shown to perform well on text generation and some analysis tasks. In this work, we investigated whether they can help identify news articles that report on a topic influenced by their opinions as part of the CheckThat! Lab 2023 competition. While ChatGPT was able to synthesize new data that reflected the target class very well, it also introduced several stylistic patterns that may affect the model that uses this data to fine-tune it. Therefore, the models using additional training data were unable to outperform the fine-tuned BERT models adapted to the downstream task. When ChatGPT was used as a few-shot classifier, performance varied dramatically depending on the split of the data set and the language present in the data. It performed best for the private test set of the English data set, but worse for the development set. For the German data set, it performed best on the development set but worst on the private test set. This indicates that using ChatGPT as a few-shot classifier bears the risk of achieving less stable performances across different data sets than, for example, fine-tuned models. Overall, an 洧냧 1 value of 0.73 was achieved on the English test set and a value of 0.68 on the German test set. Thus, the few-shot classifier was still able to outperform the competition's baseline models which featured a macro-洧냧 1 score of 0.72 on the English test set and a score of 0.64 on the German test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,479.70,411.27,82.67"><head>Table 1</head><label>1</label><figDesc>Instances of subjectively (SUBJ) and objectively (OBJ) written sentences for Task 2 Instance Class 1. Marko Mihailovi캖, the 29-year-old figurehead of Belgrade Pride, led the city's winning bid. OBJ 2. This is the strongest case for stakeholder capitalism. SUBJ 3. Als Erg칛nzung zur Spritze bringt Pfizer eine Corona-Tablette auf den Markt. OBJ 4. &gt; Erlange die legale Steuerfreiheit &amp; entziehe den Satanisten die Macht! SUBJ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,367.16,110.00"><head>Table 2</head><label>2</label><figDesc>Class distribution of the CheckThat! Lab 2023 Task 2 English (E) and German (G) data set</figDesc><table coords="4,208.32,118.58,178.63,81.91"><row><cell></cell><cell cols="3">Total Objectivity Subjectivity</cell></row><row><cell cols="2">Train 洧냦 830</cell><cell>532</cell><cell>298</cell></row><row><cell>Dev 洧냦</cell><cell>219</cell><cell>113</cell><cell>106</cell></row><row><cell>Test 洧냦</cell><cell>243</cell><cell>116</cell><cell>127</cell></row><row><cell cols="2">Train 洧냨 800</cell><cell>492</cell><cell>308</cell></row><row><cell>Dev 洧냨</cell><cell>200</cell><cell>123</cell><cell>77</cell></row><row><cell>Test 洧냨</cell><cell>291</cell><cell>194</cell><cell>97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,219.88,344.51,133.80"><head>Table 3</head><label>3</label><figDesc>Classification results provided by each method for Task 2</figDesc><table coords="4,161.77,247.96,271.73,105.71"><row><cell></cell><cell></cell><cell>English</cell><cell></cell><cell cols="2">German</cell></row><row><cell></cell><cell></cell><cell cols="4">macro F1 SUBJ F1 macro F1 SUBJ F1</cell></row><row><cell></cell><cell>BERT</cell><cell>0.76</cell><cell>0.75</cell><cell>0.77</cell><cell>0.69</cell></row><row><cell>Dev</cell><cell>BERT + GPT</cell><cell>0.70</cell><cell>0.67</cell><cell>0.76</cell><cell>0.67</cell></row><row><cell></cell><cell>GPT</cell><cell>0.71</cell><cell>0.74</cell><cell>0.78</cell><cell>0.74</cell></row><row><cell></cell><cell>BERT</cell><cell>0.69</cell><cell>0.65</cell><cell>0.73</cell><cell>0.63</cell></row><row><cell>Test</cell><cell>BERT + GPT</cell><cell>0.64</cell><cell>0.57</cell><cell>0.73</cell><cell>0.63</cell></row><row><cell></cell><cell>GPT</cell><cell>0.73</cell><cell>0.77</cell><cell>0.68</cell><cell>0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,90.49,412.83,58.86"><head>Table 4</head><label>4</label><figDesc>Instances of synthesized subjectively (SUBJ) and objectively (OBJ) written sentences for Task 2</figDesc><table coords="5,114.40,117.90,24.10,6.18"><row><cell>Instance</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,660.06,117.74,8.97"><p>https://openai.com/blog/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,671.02,86.61,8.97"><p>https://bard.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.57,671.03,129.02,8.97"><p>https://openai.com/blog/openai-api</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> and the <rs type="funder">Hessian Ministry of Higher Education, Research, Science and the Arts</rs> within their joint support of "<rs type="funder">ATHENE -CRISIS</rs>" and "<rs type="funder">Lernlabor Cybersicherheit" (LLCS)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,271.96,394.53,10.91;7,112.66,285.51,393.33,10.91;7,112.66,299.06,394.61,10.91;7,112.48,312.61,394.70,10.91;7,112.28,326.16,394.91,10.91;7,112.66,339.71,80.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,436.46,285.51,69.52,10.91;7,112.66,299.06,375.16,10.91">The CLEF-2023 CheckThat! Lab: Checkworthiness, subjectivity, political bias, factuality, and authority</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr칩n-Cede침o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,189.54,326.16,150.64,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="506" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,353.26,394.53,10.91;7,112.33,366.81,394.85,10.91;7,112.14,380.36,395.05,10.91;7,112.66,393.91,394.53,10.91;7,112.66,407.46,394.53,10.91;7,112.66,421.01,395.17,10.91;7,112.66,434.55,393.33,10.91;7,112.33,448.10,81.51,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,179.18,380.36,328.01,10.91;7,112.66,393.91,307.13,10.91">Overview of the CLEF-2023 CheckThat! Lab checkworthiness, subjectivity, political bias, factuality, and authority of news articles and their source</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr칩n-Cede침o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,233.97,421.01,273.86,10.91;7,112.66,434.55,393.33,10.91;7,112.33,448.10,27.43,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,112.66,461.65,395.17,10.91;7,112.66,475.20,394.53,10.91;7,112.66,488.75,395.17,10.91;7,112.66,502.30,394.52,10.91;7,112.14,515.85,395.05,10.91;7,112.33,529.40,120.27,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,239.52,488.75,268.31,10.91;7,112.66,502.30,95.64,10.91">Overview of the CLEF-2023 CheckThat! lab task 2 on subjectivity in news articles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr칩n-Cede침o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Antici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K칬hler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Korre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Leistra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Muti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mehmet Deniz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.14,515.85,390.50,10.91">Working Notes of CLEF 2023-Conference and Labs of the Evaluation Forum, CLEF 2023</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,542.95,394.53,10.91;7,112.66,556.50,393.32,10.91;7,112.66,570.05,311.98,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rozi칟re</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coord="7,449.30,556.50,56.68,10.91;7,112.66,570.05,181.60,10.91">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,583.60,394.53,10.91;7,112.66,597.15,393.33,10.91;7,112.66,610.69,397.48,10.91;7,112.36,626.69,32.07,7.90" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,421.79,597.15,84.20,10.91;7,112.66,610.69,230.08,10.91">Language is not all you need: Aligning perception with language models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.14045</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,637.79,394.53,10.91;7,112.66,651.34,395.17,10.91;8,112.66,86.97,393.33,10.91;8,112.66,100.52,214.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,112.66,651.34,395.17,10.91;8,112.66,86.97,104.28,10.91">I2mvformer: Large language model generated multi-view document supervision for zero-shot image classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G Z A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,238.04,86.97,267.95,10.91;8,112.66,100.52,105.89,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15169" to="15179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,394.53,10.91;8,112.66,127.61,342.68,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,261.03,114.06,241.42,10.91">Few-shot named entity recognition via meta-learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,127.61,248.60,10.91">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4245" to="4256" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,141.16,394.53,10.91;8,112.66,154.71,394.53,10.91;8,112.66,168.26,394.53,10.91;8,112.66,181.81,394.53,10.91;8,112.66,195.36,393.86,10.91;8,112.66,208.91,299.14,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,112.66,195.36,171.12,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,393.33,10.91;8,112.66,236.01,383.48,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,343.05,222.46,162.94,10.91;8,112.66,236.01,40.58,10.91">Large language models are zero-shot reasoners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,161.67,236.01,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,249.56,393.33,10.91;8,112.66,263.11,324.10,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,388.24,249.56,117.75,10.91;8,112.66,263.11,193.88,10.91">Large language models are zero-shot rankers for recommender systems</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08845</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,276.66,393.33,10.91;8,112.66,290.20,311.37,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="8,326.58,276.66,179.40,10.91;8,112.66,290.20,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,303.75,395.01,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="8,227.57,303.75,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
