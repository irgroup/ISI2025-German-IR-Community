<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,80.81,396.40,19.63;1,89.29,102.73,318.69,19.63;1,89.29,124.65,336.85,19.63;1,89.29,146.57,252.31,19.63;1,89.29,171.20,232.13,13.63">Fraunhofer SIT at CheckThat! 2023: Enhancing the Detection of Multimodal and Multigenre Check-Worthiness Using Optical Character Recognition and Model Souping Notebook for the CheckThat! Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">Lernlabor Cybersicherheit&quot; (LLCS)</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder>
					<orgName type="full">ATHENE -CRISIS</orgName>
				</funder>
				<funder>
					<orgName type="full">Hessian Ministry of Higher Education, Research, Science and the Arts</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,197.10,115.10,13.63"><forename type="first">Raphael</forename><forename type="middle">Antonius</forename><surname>Frick</surname></persName>
							<email>raphael.frick@sit.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<orgName type="institution" key="instit2">ATHENE -National Research Center for Applied Cybersecurity</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.18,197.10,52.32,13.63"><forename type="first">Inna</forename><surname>Vogel</surname></persName>
							<email>inna.vogel@sit.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<orgName type="institution" key="instit2">ATHENE -National Research Center for Applied Cybersecurity</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.92,197.10,77.70,13.63"><forename type="first">Jeong-Eun</forename><surname>Choi</surname></persName>
							<email>jeong-eun.choi@sit.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<orgName type="institution" key="instit2">ATHENE -National Research Center for Applied Cybersecurity</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,80.81,396.40,19.63;1,89.29,102.73,318.69,19.63;1,89.29,124.65,336.85,19.63;1,89.29,146.57,252.31,19.63;1,89.29,171.20,232.13,13.63">Fraunhofer SIT at CheckThat! 2023: Enhancing the Detection of Multimodal and Multigenre Check-Worthiness Using Optical Character Recognition and Model Souping Notebook for the CheckThat! Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">357A72D050F6A247ED9A9291C8BCF32E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Check-Worthiness Detection</term>
					<term>Multimodality</term>
					<term>Optical Character Recognition</term>
					<term>Model Souping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab challenge for check-worthiness detection in multimodal and unimodal content. Check-worthiness detection aims to facilitate manual fact-checking efforts by prioritizing the statements that fact-checkers should consider first. It can also be seen as the first step of a fact-checking system. Our approach was ranked first in Task 1A and second in Task 1B. The goal of Task 1A is to determine whether a claim in a tweet that contains both a snippet of text and an image is worth fact-checking. For this task, we propose a novel way to detect check-worthiness. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in Task 1A with an ğ¹ 1 score of 0.7297 achieved on the private test set. The aim of Task 1B is to determine whether a text snippet from a political debate it should be assessed for check-worthiness. Our bestperforming method takes advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall ğ¹ 1 score of 0.878 and was ranked as the second-best model in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In today's digitally connected world, social media platforms have become leading channels for the dissemination of information and play a crucial role in shaping public opinion. However, the proliferation of fake news and false information poses a major challenge to the reliability and trustworthiness of content disseminated on these platforms. To combat such false or misleading information, several manual fact-checking initiatives have been launched, such as: FactCheck.org <ref type="foot" coords="2,153.22,83.25,4.06,9.95" target="#foot_0">1</ref> , PolitiFact<ref type="foot" coords="2,204.81,83.25,4.06,9.95" target="#foot_1">2</ref> or Snopes <ref type="foot" coords="2,255.63,83.25,4.06,9.95" target="#foot_2">3</ref> . With billions of data shared on social media platforms every second, it is even for computers infeasible to review all of the data. Therefore, automatic identification of most worthy and prioritized claims for fact-checking can be very useful for human experts. The check-worthiness task can be considered as the first of three steps in the fact-checking pipeline, which traditionally consists of <ref type="bibr" coords="2,330.83,139.59,11.56,12.44" target="#b0">[1]</ref>:</p><p>1. Detect check-worthy statements in a text. 2. Retrieve claims that could be useful to fact-check and that have been verified in the past 3. Automated veracity estimation.</p><p>While much attention has been paid to the detection of review-worthy tweets and political debates in text form <ref type="bibr" coords="2,183.76,229.57,11.37,12.44" target="#b1">[2,</ref><ref type="bibr" coords="2,198.55,229.57,7.57,12.44" target="#b2">3]</ref>, detecting check-worthiness in content that includes both images and text is still a relatively unexplored area with only a few publications tackling this issue <ref type="bibr" coords="2,492.42,243.12,11.44,12.44" target="#b3">[4]</ref>. Not only is multimedia content frequently shared with text on social media these days, but it can also assist in the spread of disinformation. They can serve to attract the reader's attention, but also contain false information. For example, images and videos can be taken out of context and used in a new context, or be manipulated using AI-assisted tools or manual retouching. In some cases in the past, images consisting only of text were posted without a descriptive text to circumvent the automatic reporting mechanisms of social media platforms such as Facebook. This demonstrates the need to extend the check-worthiness estimation from text-only data to multimodal data.</p><p>The CheckThat! Lab has been tackling this scientific problem for the past several years. This year, CheckThat! Lab <ref type="bibr" coords="2,213.33,378.62,11.36,12.44" target="#b4">[5,</ref><ref type="bibr" coords="2,227.99,378.62,8.96,12.44" target="#b5">6]</ref> offered two kinds of data for the check-worthiness subtask <ref type="bibr" coords="2,89.29,392.17,11.43,12.44" target="#b6">[7]</ref>. For Subtask 1A (multimodal), a text snippet (tweet) plus an image had to be assessed for check-worthiness. The aim of Task 1B (Multigenre) was to identify check-worthy statements from a tweet or a political debate/ speech transcription. Fraunhofer SIT participated in Task 1A and 1B of the CLEF 2023 CheckThat! Lab Challenge for the English language. We achieved first place in Task 1A and second place in Task 1B. This paper describes both approaches for identifying relevant claims in English multimodal tweets and political debates.</p><p>Our proposed methodology for Subtask 1A involves a multimodal approach that combines textual cues in the provided images and descriptive texts, and that uses a pair of BERT-based transformation models to extract meaningful features from them. The classifier for Subtask 1B is based on an ensemble learning scheme to improve upon the uncertainty of single classifiers. Since traditional stacking-based ensemble classifiers cause high computational overhead leading to long inference times, they are not always suitable for analyzing large data sets, especially data from social media. Therefore, in this paper, we present an approach for detecting check-worthiness in texts that uses Model Souping to benefit from ensemble classification while consuming fewer resources and having low inference times.</p><p>The paper is structured as follows: Section 2 summarizes the related work and some winning approaches from the last iterations of the challenge. In Section 3, we describe our solution for detecting check-worthiness in multimodal tweets, whereas Section 4 contains a description of our approaches to estimate the check-worthiness in written text along with their results on the respective data sets. The last section concludes our work with a brief discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The initial check-worthiness detection methods were based on extracting meaningful features. Given U.S. presidential election transcripts, ClaimBuster <ref type="bibr" coords="3,373.08,171.12,12.85,12.44" target="#b0">[1]</ref> predicts check-worthiness by extracting a set of 6,615 features in total (sentiment, word count, tf-idf weighted bagof-words, Part-of Speech tags, entity type), and used an SVM classifier for the prediction. Gencheva et al. <ref type="bibr" coords="3,158.54,211.77,12.84,12.44" target="#b7">[8]</ref> extended the features used by ClaimBuster by including contextual features such as the sentence's position, the size of a segment belonging to a speaker, topics, or word embeddings. Using all features in combination with a neural network (FNN) outperformed the ClaimBuster version achieving a MAP of 0.427.</p><p>In the CheckThat! 2018 competition on check-worthiness detection Hansen et al. <ref type="bibr" coords="3,457.16,265.96,12.85,12.44" target="#b8">[9]</ref> showed that an RNN with multiple word representations (word embeddings, POS tagging, and syntactic dependencies) could obtain state-of-the-art results for check-worthiness prediction. The authors later <ref type="bibr" coords="3,149.04,306.61,17.91,12.44" target="#b9">[10]</ref> extended their work by applying weak supervision using a collection of unlabeled political speeches and showed significant improvements.</p><p>The objective of the CheckThat! challenge in 2021 was to determine which tweets within a set of COVID-19 related tweets are worth checking. The authors of the best performing model <ref type="bibr" coords="3,119.96,360.81,12.85,12.44" target="#b1">[2]</ref> fine-tuned several pretrained transformer models. BERTweet achieved the best results (MAP 0.849 on the development set), a model that was trained on 850 million English tweets and 23 million COVID-19 related English tweets using RoBERTa.</p><p>Savchev <ref type="bibr" coords="3,140.65,401.46,12.85,12.44" target="#b2">[3]</ref> experimented in the CheckThat! 2022 competition with three different pretrained transformer models: BERT, DistilBERT and RoBERTa. Back translation (English tweets were translated to French and back to English) was applied to increase the training set. The best results (ğ¹ 1 0.90, Accuracy 0.85), and thus the first place in the competitions, were achieved by combining data augmentation and the RoBERTa model.</p><p>Gao et al. <ref type="bibr" coords="3,145.95,469.20,12.84,12.44" target="#b3">[4]</ref> participated in the AAAI 2022 Multimodal Fact Verification Factify Challenge by implementing two baseline solutions including an ensemble model and an end-to-end multimodal entailment model. The ensemble model outperformed the end-to-end model. They combined two uni-modal models and a multimodal attention network using a 3-way textual entailment classifier, visual similarity with a pre-trained CNN model, and heuristics learned from the dataset. They additionally explored the multimodal fusion technique to model the interaction between different modalities in claim-document pairs and combine information from them. Their best performing model was ranked first by obtaining a weighted average ğ¹ 1 score of 0.77 on both the validation and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 1A: Mixing Unimodal Classifiers to Estimate the Check-Worthiness of Multimodal Tweets</head><p>In the following, the data set, the examined approaches and their results for detecting checkworthiness in multimodal tweets will be described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Set Description</head><p>The CheckThat! Lab Subtask 1A covered the Arabic and English language; we only participated in the subtask dealing with English data. The data set consisted of social media posts collected by Twitter through its API. Each entry in the dataset contained the text of the tweets, an image, and a text determined by an optical character recognition on the associated image. Examples from the data set are shown in Figure <ref type="figure" coords="4,258.94,160.16,3.74,12.44" target="#fig_0">1</ref>. The aim of Task 1A was to predict whether a given multimodal Tweet requires the need of undergoing a manual review by a human expert. Along with the contest, a data set was provided that was divided into four splits: a train split, a dev split, a dev-test split, and a test split. While labels for the train set, dev set, and dev-test set were provided upon release, the gold labels for the test split were not provided until after the competition was completed. In addition to the labeled data set, a set of unlabeled data was provided. The label distributions of each individual data set split are displayed in Table <ref type="table" coords="4,335.42,438.10,3.74,12.44" target="#tab_0">1</ref>. As can be seen, the data set suffers from class imbalance. Within each split, there were almost twice as many tweets not worthy of verification as tweets worthy of verification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methods and Results</head><p>Detecting check-worthiness in multimodal tweets presents its own challenges. For one, the length of the texts are limited by a character count restriction. Moreover, both the text and the accompanying image contribute equally to the level of check-worthiness. During the Corona pandemic, text messages were embedded into images, as well as diagrams and charts misinterpreted deliberately. Here, an analysis of both, textual and imagery data is required to assess their check-worthiness fully. In this paper, we present a classification scheme that takes advantage of two classifiers that provide an initial prediction for each modality and then merge their predictions to make a final decision. By including a step that processes the tweets before training and inference, and by using fine-tuning, the classifiers are adapted to the particular writing style typically found in tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pre-Processing</head><p>Unlike text data in documents, books, and web pages, tweets often contain hashtags, emojis, and URLs. The package pysentimiento <ref type="bibr" coords="5,267.90,243.12,17.91,12.44" target="#b10">[11]</ref> provides methods for resolving emojis and converting hashtags and URLs to generic tokens. As URLs usually do not contribute much to the check-worthiness of a tweet, their analysis can be omitted. By resolving emojis into their descriptive meanings, they can be more easily processed by classifiers previously trained on generic text. An example of the pre-processing can be viewed in Table <ref type="table" coords="5,405.29,297.31,3.74,12.44">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Example of a pre-processed (PP) tweet.</p><p>Instance @MMDA @gmanews Smoke belching Bus..Dapat eto tinatanggal sa road. This contributes increase of Smog!Global Warming! :cold_sweat: https://t.co/AX39EMqC5W PP @USER @USER Smoke belching Bus..Dapat eto tinatanggal sa road. This contributes increase of Smog!Global Warming! emoji anxious face with sweat emoji</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Classifying the Textual Data</head><p>To analyze the textual data, a BERT-based <ref type="bibr" coords="5,281.88,424.24,20.42,12.44" target="#b11">[12]</ref> model was fine-tuned on the pre-processed tweets. Throughout the training process, an optimizer based on the Adam algorithm <ref type="bibr" coords="5,467.98,437.79,17.91,12.44" target="#b12">[13]</ref> was employed to take advantage of its adaptive learning rate mechanism. Initially, a learning rate of 0.0004 was selected. The model underwent fine-tuning over five epochs, utilizing a batch size of 24. To ensure optimal performance on the competition data set's development split, only the model checkpoint with the highest performance was retained.</p><p>The performance of a BERT model trained with and without pre-processing is displayed in Table <ref type="table" coords="5,116.71,519.08,3.74,12.44" target="#tab_1">3</ref>. The classifier trained with pre-processed tweets has higher ğ¹ 1 scores than the one trained without them. In the specific case of classifying the test set, the ğ¹ 1 score increased from 0.5377 to 0.7172. Thus, it is advisable to take advantage of pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Classifying the Visual Data</head><p>For the visual data of the data set, a separate classifier was trained. Two types of classifiers were tried for the challenge, which differed in the type of input data they process: raw image data and textual data extracted from an optical character recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using Vision Transformer</head><p>To classify raw image data, a ViT-based Vision Transformer model was fine-tuned <ref type="bibr" coords="5,190.08,664.92,16.25,12.44" target="#b13">[14]</ref>. In particular, the google/vit-base-patch16-224-in21k from the hug- gingface repository was fine-tuned on the provided image train data using a batch size of 16 within 4 epochs. Similar to the fine-tuned BERT models, Adam was used as the optimizer with a learning rate of 0.0002 and model checkpoints were utilized.</p><p>As shown in Table <ref type="table" coords="6,188.60,386.99,3.74,12.44" target="#tab_1">3</ref>, the Vision Transformer was unable to learn meaningful patterns as indicated by the ğ¹ 1 -scores of 0.0000. In particular, the model learned that predicting the majority class ("No") maximizes the validation loss. There could be several reasons for this. Here, the classifier may have tended to classify the majority class due to the class imbalance within the data set, and the images found in each class may not be sufficiently different for providing good class separation. For further investigation a CNN classifier based on the EfficientNet architecture <ref type="bibr" coords="6,146.13,468.29,17.92,12.44" target="#b14">[15]</ref> was trained and evaluated. However, the results did not differ significantly from those of the Vision Transformer. Therefore, the visual model was not included in the final classifier.</p><p>Using Optical Character Recognition Since the data-driven imaging models did not perform well in this task, another method for evaluating the information found in the shared images was investigated.</p><p>Many times images contain text that can provide additional information for detecting checkworthy content. To evaluate these, the easyOCR package was used. It is based on the work of Shi et al. <ref type="bibr" coords="6,149.59,591.89,17.92,12.44" target="#b15">[16]</ref> and supports the extraction of text in different languages. The extracted characters were combined into a single string (Figure <ref type="figure" coords="6,330.01,605.44,3.57,12.44" target="#fig_1">2</ref>), which then served as input to a finetuned BERT model. The BERT model was fine-tuned similar to the classifier predicting the check-worthiness of texts, except a batch size of 8 was utilized.</p><p>Compared to the model that predicts text data, the performance of the classifier that estimates check-worthiness based on text within images provides less good results. While the accuracy is 69% on average, the ğ¹ 1 scores obtained for each split are much lower. Since the data suffers from class imbalance, the ğ¹ 1 score is preferred over the accuracy score. One reason for the lower scores is that not all images contain text and, on the other hand, some images in the data set were not written in English. Therefore, a multilingual model like XLM <ref type="bibr" coords="7,456.56,331.61,22.88,12.44" target="#b16">[17]</ref> could provide better performance.</p><p>Combining BERT with an OCR-Analysis For the final solution, the classifier that predicts check-worthiness based on text and the classifier that uses optical character recognition were combined. Models that perform better than others should have a greater impact on the final prediction than models that perform less well. Here, we first estimated the validation losses on the dev set for each classifier. Then, the loss values of the opposing models were used to weight the logits predicted by each classification model. By this, the text-based model that was able to produce better results on the dev set, had a greater impact on the final decision than the OCR-based visual model.</p><p>Combining the two classifiers resulted in a slight improvement in overall performance. The ğ¹ 1 values (see Table <ref type="table" coords="7,179.34,495.86,4.16,12.44" target="#tab_1">3</ref>) were improved across the classification of all data sets. With a ğ¹ 1 score of 0.7297 it placed first in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 1B: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification</head><p>The following describes our solution to detecting check-worthiness in textual data using efficient ensemble learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Set Description</head><p>The ChackThat! Lab data sets for Subtask 1B cover the languages Arabic, English, and Spanish. While we only participated in the English language variant of the task, the described approach can also be adapted for other languages.</p><p>For the English task the data set consisted of political debates collected from the US presidential general election debates. Examples from the data set are shown in Table <ref type="table" coords="8,446.49,112.49,3.74,12.44" target="#tab_2">4</ref>. The aim of Task 1B was to predict whether a text snippet from a political debate has to be assessed manually by an expert by estimating its check-worthiness. The data set was annotated by human labelers. The label distributions and data set split were provided by the organizers and are shown in Table <ref type="table" coords="8,197.02,270.68,3.74,12.44" target="#tab_3">5</ref>. The "train" corpus consists of 16,876 entries. Each entry is labeled either "Yes" or "No" on whether it is worth fact-checking (YES) or not (No). The organizers have also provided a development set "dev" (5,625 entries), a development test set "dev test" (1,032 entries), and a test set with 318 statements. As it can be seen, the data set is highly imbalanced with about a quarter of the sentences being check-worthy. This is also due to the fact that attention-worthy sentences occur less frequently in the text than non-check-worthy sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Methodology and Results</head><p>Text data from social media and messenger applications such as Twitter and Telegram, news and blogging websites, and transcribed political debates may contain incorrect information that needs to be subjected to manual review by an expert. Hereby, texts of interest are those that contain asserted facts that can be proven or disproven. To identify if a text is check-worthy, three approaches have been tested as part of the CheckThat! 2023 competition: an estimation using named entities, a method combining the named entity recognition with BERT and the final solution consisting of an ensemble classifier based on Model Souping. In the following, the three approaches will be described and their performance discussed.</p><p>Estimation Using Named Entity Analysis Facts can often be expressed using named entities, such as names (person / corporation / location / event / objects) or numbers (cardinals / ordinals / quantities) and dates. A thorough examination of the train partitioning of the data set revealed that samples classified as check-worthy had a higher use of named entities than those classified as not worthy of reviewing. Using Flair <ref type="bibr" coords="9,334.15,372.64,16.26,12.44" target="#b17">[18]</ref>, a named entity recognition model pre-trained on the OntoNotes data set <ref type="bibr" coords="9,256.85,386.19,16.45,12.44" target="#b18">[19]</ref>, the named entities within each of the provided text snippets were extracted and categorized. Hereby, check-worthy texts contained on average 1.679 named entities, whereby non-attention-worthy texts featured only 0.662 named entities on average. Further analysis showed that in addition to the number of named entities featured, the types of entities also varied between the two classes. Figure <ref type="figure" coords="9,378.86,440.39,5.07,12.44" target="#fig_2">3</ref> showcases the distribution of named entity types pre-grouped by similarity. The parent type NUM consists of ordinal numbers, cardinal numbers, quantities, percentages, and money, while DATE consists of time and dates. GPE consists of nationalities, countries, and states, and LOC consists of places and events. PER and ORG remained self-contained. The distribution shows that texts worth examining often contain numbers and counts, while nationalities, countries, and states are found less frequently.</p><p>The resulting information was then used to train a classifier, namely a logistic regression model, using the number of a given parent type as input. As indicated in Table <ref type="table" coords="9,433.08,548.78,3.74,12.44" target="#tab_4">6</ref>, the model was able to achieve medium to high accuracies, especially when classifying dev and dev-test split of the data set. In comparison, however, the ğ¹ 1 -values are very low, making the model unsuitable for real-world applications. One reason for this lies in the imbalanced class distribution within the data set. Another one is that analyzing the occurrence of named entities alone does not provide enough information for a precise estimation. A text mentioning numerous named entities but which is written in a subjective tone and expresses an opinion is not worthy of review. Thus, to mitigate this problem, contextual information must be analyzed as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining the Analysis of Named Entities with Language Models</head><p>To include additional information about the context, the second attempt combined the named entity recognition with a language model. Here, BERT <ref type="bibr" coords="10,280.05,382.03,21.66,12.44" target="#b11">[12]</ref> was fine-tuned on data, in which the named entities found in the previous step were exchanged with special tokens reflecting their respective named entity type (see Table <ref type="table" coords="10,243.55,409.13,3.57,12.44" target="#tab_5">7</ref>). For this, the tokenizer was modified to contain the six additional tokens &lt;NUM&gt;, &lt;DATE&gt;, &lt;LOC&gt;, &lt;GPE&gt;, &lt;PER&gt;, and &lt;ORG&gt;. During training, an optimizer based on Adam <ref type="bibr" coords="10,296.85,532.31,22.25,12.44" target="#b12">[13]</ref> was utilized to leverage from the adaptive learning rate mechanism. A learning rate of 0.0004 was chosen as the initial learning rate. The model was fine-tuned in 5 epochs with a batch size of 24. Model checkpoints were used to keep only the model checkpoint, that performed best on the dev split of the competition data set.</p><p>Table <ref type="table" coords="10,127.43,586.51,5.07,12.44" target="#tab_1">3</ref> shows, that by combining a named entity recognition with a language model such as BERT, the performance can be further increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Final Solution Using Ensemble Learning Based on Model Souping</head><p>To compare the hybrid method with a fully data-driven approach, fine-tuning was performed using solely the raw text data. Again, a BERT model was chosen and fine-tuned using the configuration described in the previous section.</p><p>Training the model several times with different seeds showed large differences in performance (see BERT A, BERT B, and BERT C in Table <ref type="table" coords="11,314.26,112.49,3.57,12.44" target="#tab_4">6</ref>). This is because the initial weights of the model are initialized differently depending on the set seed. The same applies to the way the training split is shuffled after each epoch. As a result, individual models converge differently and can find different local minima, resulting in a sometimes good or less good performance. Unable to determine which seed maximizes performance on the validation, and test sets, it is common to take advantage of ensemble learning.</p><p>There exist several approaches to perform ensemble classification, such as bagging, boosting, and stacking <ref type="bibr" coords="11,142.90,207.34,17.43,12.44" target="#b19">[20]</ref>. In each of the methods, individually trained classifiers called weak classifiers are combined to improve the classification uncertainty. The main disadvantage of ensemble learners, however, lies within their computational efficiency during inference. In particular, stacking-based ensemble classifiers, which consist of a combination of ğ‘ models providing an initial prediction and a meta classifier taking these to form a final decision output, require the inference of ğ‘ + 1 models. As such, ensemble classification may not be applicable in real-world applications, in which large amounts of data need to be assessed in a timely manner using as less computational resources as possible.</p><p>To compensate for these problems, Model Souping as proposed by Wortsman et al. can be applied <ref type="bibr" coords="11,124.57,329.28,16.26,12.44" target="#b20">[21]</ref>. Model Souping removes the requirement of having multiple weak classifiers and a meta-classifier by providing a single master-model that is used during inference. Mastermodels can be built by taking the trained weights of each individual classifier and combining them by averaging, weighted averaging, or using a feedback loop. Initial tests with image and text classification tasks showed improved performance while maintaining resource efficiency. It should be noted, however, that Model Souping can only be applied with models sharing the same architecture.</p><p>In this paper, we took advantage of Model Soups that adaptively adjust the influence of each individual model in the master model based on the performance on the dev split of the data set. Here, the fully data-driven models were used in favor of the hybrid models BERT with a named entity recognition due to their performance on the dev, and dev-test split. By evaluating each of the three trained models on the dev set, their test loss values were retrieved. Based on them, their influence-score ğ¼ was calculated using the following formula:</p><formula xml:id="formula_0" coords="11,266.14,516.38,240.49,14.19">ğ¼ = ğ¿ ğ‘¡ğ‘’ğ‘ ğ‘¡ /ğ¿ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™<label>(1)</label></formula><p>Low-performing models should have a lower impact within the master model, whereas betterperforming ones, should have a higher influence on the weights of the master model. The influence value ğ¼ was then used to weight the trained weights of each model. While the ensemble classifier was unable to outperform the best individual classifier (BERT C; ğ¹ 1 = 0.8952) on the test data set, it helped with balancing out results from models (BERT A; ğ¹ 1 = 0.7784) suffering from low performance. It should be noted, however, that if all weak classifiers perform equally well on a particular data set, the performance gain will be negligible.</p><p>The approach based on Model Souping was used to classify the private test set of this year's CheckThat! competition. It was able to place second best. Although it performed best among the three methods described, its capabilities in terms of explainability and transparency are limited due to the fact that it is a fully data-driven approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The detection of check-worthy texts can be seen as a first step towards identifying false information spread on the Internet. When used as a pre-filter, it can dramatically reduce the amount of data that needs to be manually reviewed by human experts. In this paper, we have described our approach to check-worthiness detection in multimodal and unimodal content.</p><p>Multimodal data in social media, such as Twitter, pose new challenges for check-worthiness detection. We presented a new method for detecting review-worthy tweets that contain an image in addition to the descriptive text of the tweet (Task 1A). It combines two classifiers trained separately for each modality. The experiments showed that when analyzing visual data, an OCR analysis outperformed a classifier trained on raw image data. Combining the BERT model trained on the tweet text with the BERT model trained on the extracted strings from an optical character recognition slightly improved the performance. The combined approach performed best in the competition with a ğ¹ 1 score of 0.7297.</p><p>For Task 1B, we presented an ensemble classification scheme based on Model Souping. Experiments on the validation split and the private test set revealed that the proposed approach can be used to tackle the issue of classification uncertainty while reducing the computational overhead often associated with ensemble learning. The model was able to place second best in the competition with a ğ¹ 1 score of 0.878. Future work may consider applying weight adjustments using a feedback loop to better compensate for the misclassification of edge cases as well as introducing other means to achieve explainability and transparency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,187.67,367.42,11.36;4,141.38,205.92,312.52,138.66"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Instances of check-worthy (Yes) and non-check-worthy (No) tweets for Task 1A</figDesc><graphic coords="4,141.38,205.92,312.52,138.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,88.28,416.69,11.36;7,89.29,100.24,235.36,11.36;7,193.47,118.49,208.35,151.34"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Text string extracted by easyOCR: "Trending For you News Sports Fun Coronavirus Health 3 hours ago India reports its first confirmed coronavirus case"</figDesc><graphic coords="7,193.47,118.49,208.35,151.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,88.21,233.46,11.36"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Normalized distribution of named entity types</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,89.29,492.54,338.64,112.35"><head>Table 1</head><label>1</label><figDesc>Class distribution of the CheckThat! Lab 2023 task 1B English data set</figDesc><table coords="4,164.85,520.60,263.08,84.28"><row><cell></cell><cell>Total</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Train</cell><cell>2,356 / 100.00 %</cell><cell>820 / 34.80%</cell><cell>1,536 / 65.20%</cell></row><row><cell>Dev</cell><cell>271 / 100.00 %</cell><cell>87 / 32.10%</cell><cell>184 / 67.90%</cell></row><row><cell>Dev Test</cell><cell>548 / 100.00 %</cell><cell>174 / 31.75%</cell><cell>374 / 68.25%</cell></row><row><cell>Test</cell><cell>736 / 100.00 %</cell><cell>277 / 37.64%</cell><cell>459 / 62.36%</cell></row><row><cell>Sum</cell><cell>3,911 / 100.00 %</cell><cell cols="2">1,358 / 34.72% 2,553 / 65.28%</cell></row><row><cell cols="3">Unlabeled 110,173 / 100.00 % ?</cell><cell>?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,89.29,88.21,418.36,236.82"><head>Table 3</head><label>3</label><figDesc>Scores achieved by each model on each data set. ğ‘ƒğ‘ƒ refers to models that took advantage of the preprocessed data. Regarding the metrics, ğ´ refers to the accuracy score, ğ‘ƒ to the precision score, ğ‘… to the recall score and ğ¹ 1 to the ğ¹ 1 score. The character ğ‘‘ denotes the dev data split, ğ‘‘ğ‘¡ the dev-test data split and ğ‘¡ the test split of the data set.</figDesc><table coords="6,169.10,152.12,254.58,172.91"><row><cell>BERT</cell><cell>BERT + PP</cell><cell>Vision Transformer</cell><cell>OCR</cell><cell>BERT + PP + OCR</cell></row><row><cell cols="2">0.8155 0.7565 0.7937 0.5827 0.5747 0.8506 0.6667 0.6916 0.8321 0.7865 0.8361 0.6245 0.5862 0.8218 0.7097 ğ¹ 1 ğ‘‘ğ‘¡ 0.6892 ğ´ ğ‘‘ ğ‘ƒ ğ‘‘ ğ‘… ğ‘‘ ğ¹ 1 ğ‘‘ ğ´ ğ‘‘ğ‘¡ ğ‘ƒ ğ‘‘ğ‘¡ ğ‘… ğ‘‘ğ‘¡ 0.7500 0.7772 ğ´ ğ‘¡ 0.8843 0.6865 ğ‘ƒ ğ‘¡ 0.3863 0.7509 ğ‘… ğ‘¡ ğ¹ 1 ğ‘¡ 0.5377 0.7172</cell><cell>0.6790 0.0000 0.0000 0.0000 0.6825 0.0000 0.0000 0.0000 0.6236 0.0000 0.0000 0.0000</cell><cell>0.7048 0.5946 0.2529 0.3548 0.7190 0.6563 0.2414 0.3529 0.6685 0.6701 0.2347 0.3476</cell><cell>0.7970 0.6379 0.8506 0.7291 0.8248 0.6893 0.8161 0.7474 0.8057 0.7659 0.6968 0.7297</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,89.29,140.72,411.99,75.05"><head>Table 4</head><label>4</label><figDesc>Instances of check-worthy (Yes) and non-check-worthy (No) sentences for Task 1B</figDesc><table coords="8,108.81,168.81,27.26,8.95"><row><cell>Instance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,89.29,377.71,335.45,100.00"><head>Table 5</head><label>5</label><figDesc>Class distribution of the CheckThat! Lab 2023 Task 1B English data set</figDesc><table coords="8,170.53,405.78,254.21,71.93"><row><cell></cell><cell>Total</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Train</cell><cell cols="3">16,876 / 100.00% 4,058 / 24.05% 12,818 / 75.95%</cell></row><row><cell>Dev</cell><cell>5,625 / 100.00%</cell><cell cols="2">1,355 / 24.09% 4,270 / 75.91%</cell></row><row><cell cols="2">Dev Test 1,032 / 100.00%</cell><cell>238 / 23.06%</cell><cell>794 / 76.94%</cell></row><row><cell>Test</cell><cell>318 / 100.00%</cell><cell>108 / 33.96%</cell><cell>210 / 66.04%</cell></row><row><cell>Sum</cell><cell cols="3">23,851 / 100.00% 5,759 / 24.15% 18,092 / 75.85%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,89.29,88.21,417.80,245.39"><head>Table 6</head><label>6</label><figDesc>Scores achieved by each model on each data set. ğ´ refers to the accuracy score, ğ‘ƒ to the precision score, ğ‘… to the recall score, and ğ¹ 1 to the ğ¹ 1 score. The character ğ‘‘ denotes the dev data split, ğ‘‘ğ‘¡ the dev-test data split, and ğ‘¡ the test split of the data set.</figDesc><table coords="10,99.21,140.15,396.87,193.45"><row><cell></cell><cell>Logistic Regression + NER</cell><cell>BERT + NER</cell><cell cols="3">BERT A BERT B BERT C</cell><cell>Model Souping + BERT</cell></row><row><cell>ğ´ ğ‘‘ ğ‘ƒ ğ‘‘ ğ‘… ğ‘‘ ğ¹ 1 ğ‘‘ ğ´ ğ‘‘ğ‘¡ ğ‘ƒ ğ‘‘ğ‘¡ ğ‘… ğ‘‘ğ‘¡ ğ¹ 1 ğ‘‘ğ‘¡ ğ´ ğ‘¡ ğ‘ƒ ğ‘¡ ğ‘… ğ‘¡ ğ¹ 1 ğ‘¡</cell><cell>0.7909 0.6751 0.2546 0.3697 0.8430 0.8333 0.3992 0.5398 0.6981 0.7727 0.1574 0.2615</cell><cell>0.8796 0.7834 0.6915 0.7346 0.9554 0.9444 0.8571 0.8987 0.8711 0.9855 0.6296 0.7684</cell><cell>0.8728 0.7524 0.7041 0.7274 0.9690 0.9558 0.9076 0.9310 0.8710 0.9351 0.6667 0.7784</cell><cell>0.8764 0.7248 0.7852 0.7538 0.9729 0.9303 0.9538 0.9419 0.9308 0.9674 0.8241 0.8900</cell><cell>0.8565 0.6608 0.8310 0.7362 0.9680 0.8958 0.9748 0.9336 0.9308 0.9216 0.8704 0.8952</cell><cell>0.8670 0.6849 0.8295 0.7503 0.9709 0.9094 0.9706 0.9390 0.9214 0.9278 0.8333 0.8780</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,89.29,449.87,412.77,67.14"><head>Table 7</head><label>7</label><figDesc>Examples of a named entity extraction in check-worthy (Yes), and non-check-worthy (No) sentences</figDesc><table coords="10,105.51,477.98,22.65,7.44"><row><cell>Instance</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,647.82,93.88,10.22"><p>http://www.factcheck.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,658.78,95.12,10.22"><p>http://www.politifact.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,92.57,669.74,134.40,10.22"><p>https://www.snopes.com/fact-check/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> and the <rs type="funder">Hessian Ministry of Higher Education, Research, Science and the Arts</rs> within their joint support of "<rs type="funder">ATHENE -CRISIS</rs>" and "<rs type="funder">Lernlabor Cybersicherheit" (LLCS)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,545.81,393.32,12.44;12,112.66,559.36,395.16,12.44;12,112.66,572.90,327.35,12.44" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,307.02,545.81,198.96,12.44;12,112.66,559.36,193.76,12.44">Toward automated fact-checking: Detecting check-worthy factual claims by claimbuster</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,316.90,559.36,190.92,12.44;12,112.66,572.90,295.43,12.44">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,586.45,395.16,12.44;12,112.26,600.00,394.92,12.44;12,112.66,613.55,393.58,12.44;12,112.66,627.10,393.52,12.44;12,112.66,640.65,394.52,12.44;12,112.66,654.20,274.21,12.44" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,326.38,586.45,181.44,12.44;12,112.26,600.00,321.63,12.44">Nlp&amp;ir@uned at checkthat! 2021: Checkworthiness estimation and fake news detection using transformer models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Martinez-Rico</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>MartÃ­nez-Romo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2936/paper-44.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,311.76,613.55,194.48,12.44;12,112.66,627.10,240.16,12.44">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="12,284.16,640.65,153.23,12.44">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="545" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,85.40,395.16,12.44;13,112.66,98.94,393.32,12.44;13,112.14,112.49,395.04,12.44;13,112.66,126.04,395.16,12.44;13,112.14,139.59,337.83,12.44" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,171.26,85.40,336.56,12.44;13,112.66,98.94,31.98,12.44">AI rational at checkthat!-2022: Using transformer models for tweet classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savchev</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-52.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,421.07,98.94,84.91,12.44;13,112.14,112.49,347.21,12.44">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,347.20,126.04,154.54,12.44">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="656" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,153.14,393.60,12.44;13,112.66,166.69,394.52,12.44;13,112.66,180.24,395.16,12.44;13,112.66,193.79,395.16,12.44;13,112.66,207.34,393.32,12.44;13,112.66,220.89,394.03,12.44;13,112.66,234.44,49.68,12.44" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,421.43,153.14,84.83,12.44;13,112.66,166.69,145.69,12.44">Logically at factify 2022: Multimodal fact verfication</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oikonomou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiskovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bandhakavi</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3199/paper6.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,180.24,395.16,12.44;13,112.66,193.79,395.16,12.44;13,112.66,207.34,86.24,12.44">Proceedings of the Workshop on Multi-Modal Fake News and Hate-Speech Detection (DE-FACTIFY 2022) co-located with the Thirty-Sixth AAAI Conference on Artificial Intelligence ( AAAI 2022)</title>
		<title level="s" coord="13,124.56,220.89,152.09,12.44">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</editor>
		<meeting>the Workshop on Multi-Modal Fake News and Hate-Speech Detection (DE-FACTIFY 2022) co-located with the Thirty-Sixth AAAI Conference on Artificial Intelligence ( AAAI 2022)<address><addrLine>Virtual Event, Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-02-27">February 27, 2022. 2022</date>
			<biblScope unit="volume">3199</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,247.99,395.16,12.44;13,112.66,261.53,395.00,12.44;13,112.66,275.08,394.53,12.44;13,112.66,288.63,394.60,12.44;13,112.28,302.18,394.90,12.44;13,112.66,315.73,395.16,12.44;13,112.66,329.28,393.32,12.44;13,112.66,342.83,162.98,12.44" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,218.19,275.08,288.99,12.44;13,112.66,288.63,373.64,12.44">Overview of the CLEF-2023 CheckThat! Lab checkworthiness, subjectivity, political bias, factuality, and authority of news articles and their source</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>StruÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,298.92,315.73,208.90,12.44;13,112.66,329.28,393.32,12.44;13,112.66,342.83,108.91,12.44">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,112.66,356.38,394.52,12.44;13,112.66,369.93,393.32,12.44;13,112.66,383.48,393.32,12.44;13,112.66,397.03,395.16,12.44;13,112.66,410.58,393.33,12.44;13,112.66,424.13,170.04,12.44" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,488.72,369.93,17.26,12.44;13,112.66,383.48,393.32,12.44;13,112.66,397.03,39.09,12.44">The CLEF-2023 CheckThat! Lab: Checkworthiness, subjectivity, political bias, factuality, and authority</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>StruÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,276.03,410.58,151.45,12.44">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="506" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,437.67,394.52,12.44;13,112.66,451.22,393.63,12.44;13,112.66,464.77,394.52,12.44;13,112.66,478.32,393.33,12.44;13,112.66,491.87,328.44,12.44" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,331.69,451.22,174.60,12.44;13,112.66,464.77,304.16,12.44">Overview of the CLEF-2023 CheckThat! lab task 1 on check-worthiness in multimodal and multigenre content</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hakimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MÃ­guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,319.39,478.32,186.60,12.44;13,112.66,491.87,200.90,12.44">Working Notes of CLEF 2023-Conference and Labs of the Evaluation Forum, CLEF 2023</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,505.42,393.33,12.44;13,112.66,518.97,393.32,12.44;13,112.66,532.52,393.32,12.44;13,112.66,546.07,394.03,12.44;13,112.66,559.62,310.80,12.44" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,430.45,505.42,75.53,12.44;13,112.66,518.97,296.99,12.44">A context-aware approach for detecting worth-checking claims in political debates</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>MÃ rquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-049-6_037</idno>
		<ptr target="https://doi.org/10.26615/978-954-452-049-6_037.doi:10.26615/978-954-452-049-6_037" />
	</analytic>
	<monogr>
		<title level="m" coord="13,439.22,518.97,66.75,12.44;13,112.66,532.52,393.32,12.44;13,112.66,546.07,18.15,12.44">Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing, RANLP 2017<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,573.17,393.32,12.44;13,112.66,586.72,393.32,12.44;13,112.66,600.26,394.52,12.44;13,112.48,613.81,395.33,12.44;13,112.14,627.36,395.04,12.44;13,112.66,640.91,276.74,12.44" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,318.92,573.17,187.06,12.44;13,112.66,586.72,393.32,12.44;13,112.66,600.26,256.51,12.44">The copenhagen team participation in the check-worthiness task of the competition of automatic identification and verification of claims in political debates of the clef-2018 checkthat! lab</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,216.93,613.81,284.81,12.44;13,196.57,627.36,305.33,12.44">Working Notes of CLEF Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
	<note>CLEF 2018 Working Notes, CEUR Workshop Proceedings, CEUR</note>
</biblStruct>

<biblStruct coords="13,112.66,654.46,395.16,12.44;13,112.26,668.01,394.93,12.44;14,112.66,85.40,393.32,12.44;14,112.66,98.94,393.32,12.44;14,112.66,112.49,394.03,12.44;14,112.66,126.04,60.05,12.44" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,338.44,654.46,169.37,12.44;13,112.26,668.01,303.19,12.44">Neural weakly supervised fact checkworthiness detection with contrastive sampling-based ranking loss</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2380/paper_56.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="14,293.97,85.40,212.01,12.44;14,112.66,98.94,130.32,12.44">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="14,124.56,112.49,152.09,12.44">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,139.59,393.32,12.44;14,112.66,153.14,204.25,12.44" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Giudici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Luque</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09462</idno>
		<title level="m" coord="14,262.70,139.59,243.27,12.44;14,112.66,153.14,82.41,12.44">pysentimiento: A python toolkit for sentiment analysis and socialnlp tasks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,166.69,393.32,12.44;14,112.66,180.24,303.55,12.44" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="14,327.04,166.69,178.93,12.44;14,112.66,180.24,181.07,12.44">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,193.79,395.01,12.44" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="14,230.64,193.79,161.38,12.44">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,207.34,395.16,12.44;14,112.66,220.89,393.33,12.44;14,112.41,234.44,376.89,12.44" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="14,420.94,220.89,85.05,12.44;14,112.41,234.44,255.23,12.44">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,247.99,394.53,12.44;14,112.66,261.53,114.95,12.44" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="14,184.31,247.99,317.98,12.44">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,275.08,393.32,12.44;14,112.66,288.63,368.89,12.44" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="14,209.06,275.08,296.91,12.44;14,112.66,288.63,246.75,12.44">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05717</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,302.18,395.00,12.44;14,112.66,317.03,89.53,10.78" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m" coord="14,258.55,302.18,208.87,12.44">Cross-lingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,329.28,395.16,12.44;14,112.66,342.83,393.32,12.44;14,112.66,356.38,395.16,12.44;14,112.66,369.93,100.87,12.44" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,421.45,329.28,86.37,12.44;14,112.66,342.83,170.11,12.44">FLAIR: An easy-touse framework for state-of-the-art NLP</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,306.30,342.83,199.68,12.44;14,112.66,356.38,395.16,12.44;14,112.66,369.93,23.76,12.44">NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,383.48,394.53,12.44;14,112.66,397.03,395.16,12.44;14,112.66,410.58,394.60,12.44;14,112.66,424.13,323.25,12.44" xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lance</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nianwen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>El-Bachouti</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Houston</surname></persName>
		</author>
		<idno type="DOI">10.35111/XMHB-2B84</idno>
		<ptr target="https://catalog.ldc.upenn.edu/LDC2013T19.doi:10.35111/XMHB-2B84" />
	</analytic>
	<monogr>
		<title level="j" coord="14,356.91,410.58,80.59,12.44">Ontonotes release</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,437.67,395.16,12.44;14,112.66,451.22,279.89,12.44" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,207.25,437.67,300.57,12.44;14,112.66,451.22,121.90,12.44">An empirical comparison of voting classification algorithms : Bagging, boosting, and variants</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,242.76,451.22,81.08,12.44">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,464.77,394.53,12.44;14,112.66,478.32,393.33,12.44;14,112.26,491.87,393.73,12.44;14,112.66,505.42,139.87,12.44" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05482</idno>
		<title level="m" coord="14,398.49,478.32,107.50,12.44;14,112.26,491.87,393.73,12.44;14,112.66,505.42,17.75,12.44">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
