<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.32,451.03,17.04;1,72.02,96.08,201.14,17.04;1,72.02,126.22,234.24,10.80">FakeDTML at CheckThat! 2023: Identifying Check-Worthiness of Tweets and Debate Snippets Notebook for the CheckThat! Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,152.62,134.29,10.80"><forename type="first">Abdullah</forename><surname>Al Mamun Sardar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jahangirnagar University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.41,152.62,81.58,10.80"><forename type="first">Md</forename><forename type="middle">Ziaul</forename><surname>Karim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,310.15,152.62,61.79,10.80"><forename type="first">Krishno</forename><surname>Dey</surname></persName>
							<email>krishno.cse@diu.edu.bd</email>
							<affiliation key="aff1">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.15,152.62,77.55,10.80"><forename type="first">Md</forename><forename type="middle">Arid</forename><surname>Hasan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Daffodil International University</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.32,451.03,17.04;1,72.02,96.08,201.14,17.04;1,72.02,126.22,234.24,10.80">FakeDTML at CheckThat! 2023: Identifying Check-Worthiness of Tweets and Debate Snippets Notebook for the CheckThat! Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D43D06269A629C3F359B9C54F0F47484</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Check-worthiness</term>
					<term>Fact-Checking</term>
					<term>Check-worthy claim detection</term>
					<term>XLM-RoBERTa</term>
					<term>PassiveAggressive Classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a wealth of knowledge available online. Some are trustworthy, while others are deceptive and phony. The need to identify such false information arises from the danger it poses to society at a mass. Nowadays, there is a significant need for information that requires fact-checking. As a result, we need a layer preceding fact-checking, where it can be determined whether a claim is check-worthy. This will streamline the automated fact-checking process by filtering out a lot of unnecessary data that is nonetheless necessary. We carried out such a study as part of CLEF 2023 CheckThat! Lab (CTL) task 1B, where we were provided with a dataset of tweets and debate snippets and were asked to conduct an experiment to verify whether a particular news tweet/debate snippet is check worthy. The dataset contains 3 languages (English, Arabic, Spanish). We used several machine learning and deep learning algorithms in our experiments. Among them, XLM-RoBERTa which outperformed other algorithms for English and Arabic but for Spanish we found that Logistic Regression can outperform other models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the digital age, where information is easily accessible and rapidly disseminated, the proliferation of misinformation poses a significant challenge to society. Now we read more news online than any time before. As a consequence of this phenomenon, misleading claims, false narratives, and fabricated facts can easily spread through online platforms, leading to widespread confusion and a distortion of public understanding. To combat this issue, NLP researchers developed different strategies such as satire detection <ref type="bibr" coords="1,145.30,553.29,16.88,9.94" target="#b19">[20]</ref>, automatic fact checking <ref type="bibr" coords="1,284.01,553.29,17.51,9.94" target="#b17">[18,</ref><ref type="bibr" coords="1,306.07,553.29,13.04,9.94" target="#b18">19]</ref>, clickbait detection <ref type="bibr" coords="1,415.31,553.29,16.79,9.94" target="#b26">[27]</ref>, harmful and toxic comment detection <ref type="bibr" coords="1,158.42,565.89,18.30,9.94" target="#b11">[12]</ref>  <ref type="bibr" coords="1,179.33,565.89,18.30,9.94" target="#b27">[28]</ref> and check-worthy claim detection <ref type="bibr" coords="1,351.55,565.89,12.78,9.94" target="#b2">[3]</ref>  <ref type="bibr" coords="1,366.91,565.89,11.70,9.94" target="#b4">[5]</ref>. Among these variant strategies, fact-checking has emerged as a crucial mechanism for verifying the validity of claims and promoting the dissemination of reliable information. Recently a lot of investigation has been done on automatic fact-checking <ref type="bibr" coords="1,136.34,603.84,18.44,9.94" target="#b17">[18]</ref> [19] <ref type="bibr" coords="1,182.24,603.84,16.88,9.94" target="#b20">[21]</ref>, where researchers tried to come up with machine learning and deep learning based language modeling to identify whether a claim is fake or real. However, the amount of information which needs to be processed is huge. To filter out unnecessary claims which are not checkworthy, researchers set a prior step in the fact-checking process, where a claim should be checked first to identify whether it is check-worthy or not. Wright, D., &amp; Augenstein, I <ref type="bibr" coords="2,390.55,74.42,12.91,9.94" target="#b2">[3]</ref> have visualized two claims on three different domains each, one which is check-worthy and the other which is not. They have shown that the second claim (for each domain) is not worthy of verification particularly when it comes to fact-checking. In this study we have investigated the check-worthiness of claims under the CTL 2023 subtask 1B <ref type="bibr" coords="2,123.14,125.08,16.88,9.94" target="#b30">[31]</ref>. We were given a dataset of 3 languages and then we performed a binary classification task to find whether a claim is check-worthy or not.</p><p>The rest of the paper is structured as follows: In section 2, we review the related work covering factchecking and check-worthiness studies. Our adopted approach to conduct our experiments is presented in detail in Section 3. And then in section 4, we will compare the results of different algorithms along with the analysis of further experiments which we have conducted after the task submission. The rest two sections are conclusion and reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Assessing the check worthiness of tweets, or the need for fact-checking, is crucial in filtering out misinformation. Recently, the field of Natural Language Processing (NLP) and its researchers have witnessed a surge in the popularity of automated fact-checking systems <ref type="bibr" coords="2,392.30,296.59,11.77,9.94" target="#b5">[6]</ref>. These systems have been developed using pre-trained language models specific to a particular language <ref type="bibr" coords="2,442.76,309.31,11.77,9.94" target="#b1">[2]</ref>, incorporating datasets from local fact-checking organizations. An alternative approach suggested a lookup method that makes decisions based on the labeling of a given sample <ref type="bibr" coords="2,357.84,334.63,11.77,9.94" target="#b2">[3]</ref>. This approach further integrates transfer learning from Wikipedia's citation needed detection by employing a unified approach along with BERT <ref type="bibr" coords="2,126.01,359.83,18.30,9.94" target="#b28">[29]</ref> and PUC (Positive Unlabelled Conversion). In certain cases, to assess the credibility of information, it becomes necessary to comprehend the context at both word and sentence levels <ref type="bibr" coords="2,507.18,372.55,11.69,9.94" target="#b3">[4]</ref>. To address this, a layered approach combining RNN Encoders (LSTM/biLSTM units), a custom checkworthiness classifier, and handcrafted claim rank features was utilized. A different study using a combination of transformer-based and traditional models was explored to enhance computational efficiency and overall performance <ref type="bibr" coords="2,236.79,423.21,11.77,9.94" target="#b4">[5]</ref>. In the context of fact-checking on social media and lowresource languages, CheckthaT5, a sequence-to-sequence model based on mT5, was introduced as a solution <ref type="bibr" coords="2,109.80,448.41,11.68,9.94" target="#b5">[6]</ref>. The current state-of-the-art fact-checking models have limitations in terms of low-resource languages such as Spanish <ref type="bibr" coords="2,189.58,461.13,11.69,9.94" target="#b0">[1]</ref>. In the table below we have given a very brief overview of some previous works on check-worthiness detection of claims. Bertbase-bgcased, RobBERT <ref type="bibr" coords="3,95.30,74.42,18.43,9.94" target="#b10">[11]</ref> 2022</p><p>Used XLNet embedding techniques in the proposed language model for its autoregressive and autoencoding properties and SVM classifier for tweet classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-CWT-22</head><p>XLNet, SVM <ref type="bibr" coords="3,95.30,146.44,18.43,9.94" target="#b11">[12]</ref> 2022</p><p>The approach outperforms the official baseline by 8%. To improve the model performance and counteract class imbalance they set up class weights that correspond to a manual rescaling weight assigned to each class CT-CWT-22 GCN, ELECTRA <ref type="bibr" coords="3,95.30,227.44,18.43,9.94" target="#b12">[13]</ref> 2022 Applies augmentation techniques like back translation to increase the number of data. Uses large pretrained models and finetunes a few of them to get satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-CWT-22</head><p>DistilBE RT, BERT and RoBERTa <ref type="bibr" coords="3,95.30,295.39,18.43,9.94" target="#b13">[14]</ref> 2022 Demonstrated the performance of gated recurrent units for each of the subtasks AraBERT and BERT base Arabic were trained and fine-tuned. Despite the small sized annotated data, the model achieved satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-CWT-22</head><p>AraBERT , ARBERT, MARBERT, Arabic base BERT <ref type="bibr" coords="3,95.30,376.39,18.43,9.94" target="#b14">[15]</ref> 2021 Presents machine learning classifiers for news claim and topic classification, achieving F1 scores of 38.92% and 78.96% respectively, and discusses the dataset augmentation findings regarding the ineffectiveness of alternative word insertion for fake news classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-CWT-21</head><p>LR, MLP, SVM, RF <ref type="bibr" coords="3,95.30,471.33,18.43,9.94" target="#b15">[16]</ref> 2019 Proposes a multi-task deep-learning approach for estimating the checkworthiness of claims in political debates, demonstrating the benefits of learning from multiple fact-checking sources and achieving state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CW-USPD-2016</head><p>Neural Multi-task Learning Model (novel) <ref type="bibr" coords="3,95.30,555.57,18.43,9.94" target="#b16">[17]</ref> 2021</p><p>The research paper proposes an approach for check-worthiness estimation using RoBERTa, fine-tuning a pre-trained language representation model on the classification task with annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-CWT-21</head><p>RoBERT a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>In this study, we used the dataset released by CLEF CheckThat! organizers. The dataset consists of a total of 3 languages (English, Spanish &amp; Arabic). The dataset contains a unique id, a text snippet from a tweet or a debate/speech transcription which will be classified and a class label column where two class labels (Yes &amp; No) tells whether any claim is check-worthy or not. Distribution of the dataset for 3 different languages is presented in Table <ref type="table" coords="4,261.80,87.04,4.28,9.94">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>For data cleaning and preprocessing we have used traditional NLP techniques. First, we perform URLs and unnecessary character removal steps by following the approach discussed in <ref type="bibr" coords="4,460.53,314.35,17.00,9.94" target="#b31">[32]</ref>. Then we performed the removal of punctuations and null valued rows. Finally, along with the removal of stopwords, we removed hashtag signs and usernames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Traditional Machine Learning Algorithms</head><p>In our experiment, we used five traditional ML algorithms: (i) MultinomialNB (MNB) <ref type="bibr" coords="4,485.62,444.45,19.52,9.94" target="#b21">[22]</ref> (ii) Support Vector Machine (SVM) <ref type="bibr" coords="4,216.53,457.05,19.52,9.94" target="#b22">[23]</ref> (iii) LogisticRegression (LR) <ref type="bibr" coords="4,370.15,457.05,19.52,9.94" target="#b23">[24]</ref> (iv) RandomForest Classifier (RF) <ref type="bibr" coords="4,100.46,469.65,18.05,9.94" target="#b24">[25]</ref>. As this task is a kind of online learning, we used another algorithm named (v)PassiveAggresive Classifier <ref type="bibr" coords="4,211.68,482.37,17.95,9.94" target="#b25">[26]</ref>, which is popular for online learning tasks. We have trained our dataset with uni, bi and tri grams. For word embedding, we used both TF-IDF and CountVectorizer techniques. We used linear kernel for Support Vector Machine. To tune hyperparameter, we used grid search for all traditional ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Transformer Based Algorithm</head><p>In 2018, Google released BERT <ref type="bibr" coords="4,228.82,578.01,16.88,9.94" target="#b28">[29]</ref>, a pre-trained language model. Since then BERT based models are widely used for language based tasks. In our experiment we used a variant of the BERT based language model (RoBERTa) which was introduced by Facebook back in 2019 <ref type="bibr" coords="4,424.93,603.36,16.89,9.94" target="#b29">[30]</ref>. In particular, we used XLM-RoBERTa <ref type="bibr" coords="4,177.29,615.96,12.91,9.94" target="#b4">[5]</ref> in our experiment. We used a learning rate of 2e-5 to fine tune the hyperparameter. A full list of hyperparameters that we used in our experiment and their corresponding values are given in Table <ref type="table" coords="4,185.38,641.28,4.14,9.94" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Methodology</head><p>In our experiment, we used both traditional machine learning algorithms and a transformer based deep learning algorithm. We merged the train and dev-test set to train the model. We used different ngrams for traditional machine learning algorithms. We have experimented with both TF-IDF and CountVectorizer techniques. Below in the diagram, we provided our proposed approach for the experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Result Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance of Transformer and ML models</head><p>For traditional ML algorithms, we have experimented with different n-grams and vectorization techniques which described in in section 3.3.1 and we took the one which provided the best performance. Among 5 traditional ML algorithms, MultinomialNB outperformed the other four for English with bi-gram and CountVectorizer by achieving an F1 score of 80.36%, Logistic Regression outperformed the other four for Spanish with uni-gram and CountVectorizer, and for Arabic, PassiveAggressive Classifier provides the best result with uni-gram and TF-IDF vectorizer by achieving an F1 score of 57.87%. For transformer based algorithms, we used XLM-RoBERTa. We have provided a list of hyperparameters in Table <ref type="table" coords="5,225.39,501.93,4.14,9.94" target="#tab_2">3</ref>. We used those same corresponding values to train the model for each language. Our experiment shows that the transformer based model can outperform all the traditional ML algorithms for English and Arabic but it gave poor results for Spanish. We have already seen in <ref type="bibr" coords="5,105.50,539.97,12.91,9.94" target="#b0">[1]</ref> and <ref type="bibr" coords="5,139.56,539.97,12.78,9.94" target="#b4">[5]</ref> that transformer based models did not give better results for low resource languages (e.g. Spanish). Our experiments show that Logistic Regression achieves 89.76% f1 score for Spanish while XLM-RoBERTa achieved only 51.26%. A performance comparison table for Transformer and traditional ML based models are given in Table <ref type="table" coords="5,284.16,577.89,4.17,9.94" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Leaderboard Result</head><p>For the leaderboard submission we used XLM-RoBERTa for English language, for Spanish and Arabic, we used MultinomialNB. Our task on English language ranked 8 th but due to late submission our task on Spanish and Arabic language did not get position number but it came right after 6 th (for both Arabic and Spanish). In the following subsection we described in detail how with further experiments we have gained better results. The performance of our models for each language on the leaderboard is presented in Table <ref type="table" coords="6,156.09,341.59,4.16,9.94" target="#tab_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Experiment Analysis</head><p>Du, Mingzhe, et al. <ref type="bibr" coords="6,172.78,511.17,12.80,9.94" target="#b5">[6]</ref> performed an error analysis for their model submitted for CTL which did not perform well for the English language. They presumed that their model most probably became language agnostic as they tried several languages with the same model and that might be the reason why they got poor performance on English language. For our task at CTL, after the publication of the results on leaderboard, we saw our model performed poorly on Spanish and Arabic, gave an F1-score of 44% and 53% for Spanish and Arabic respectively. Then we performed some further investigation into why the performance was not so good when other teams came up with better results. In our later experiment, we have applied whitespace tokenization for all languages, Snowball stemmer for Spanish, Arabic Stemmer for Arabic and Porter Stemmer for English. With this set up, we then experimented with some other ML algorithms (LR, RF, SVM, PassiveAggressive Classifier). Our further experiment shows that LR outperformed all the other models (including XLM-RoBERTa) for Spanish language. For Arabic language, using PassiveAggressive Classifier we were able to increase the performance by 4.87% than the leaderboard result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present our participation in CLEF CTL 2023 subtask 1B to detect check-worthy claims. Our experiment shows that both uni-gram and bi-gram models can perform better with TF-IDF and CountVectorizer techniques. Although our initial models did not perform well on the leaderboard for Arabic and Spanish, we conducted some further investigations. We experimented with several</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.02,323.45,150.96,11.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.02,498.67,442.08,229.95"><head>Table 1</head><label>1</label><figDesc>Summary of some published works on check-worthiness detection</figDesc><table coords="2,92.18,528.21,421.92,200.41"><row><cell>Ref</cell><cell>Year</cell><cell>Contribution</cell><cell></cell><cell></cell><cell>Dataset</cell><cell>Models</cell></row><row><cell>[8]</cell><cell>2021</cell><cell>Improved</cell><cell>performance</cell><cell>through</cell><cell cols="2">CT-CWT-21 BERT,</cell></row><row><cell></cell><cell></cell><cell cols="3">contextual embedding augmentation on</cell><cell></cell><cell>RoBERTa-</cell></row><row><cell></cell><cell></cell><cell>training dataset.</cell><cell></cell><cell></cell><cell></cell><cell>based</cell></row><row><cell>[9]</cell><cell>2022</cell><cell cols="3">Utilizing a specialized ensemble</cell><cell>CT-CWT-22</cell><cell>Twitter-</cell></row><row><cell></cell><cell></cell><cell cols="3">architecture, it combines the strengths of</cell><cell></cell><cell>domain</cell></row><row><cell></cell><cell></cell><cell cols="3">ten diverse transformer-based models.</cell><cell></cell><cell>adapted</cell></row><row><cell></cell><cell></cell><cell cols="3">These models have been pre-trained on</cell><cell></cell><cell>version</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell cols="3">Twitter data, enabling them to generate</cell><cell></cell><cell>RoBERTa,</cell></row><row><cell></cell><cell></cell><cell cols="3">raw predictions with precision and</cell><cell></cell><cell>TweetEva</cell></row><row><cell></cell><cell></cell><cell>accuracy.</cell><cell></cell><cell></cell><cell></cell><cell>, BERTweet</cell></row><row><cell>[10]</cell><cell>2022</cell><cell cols="3">Fine Tuning Various Transformer</cell><cell>CT-CWT-22</cell><cell>AraBERT</cell></row><row><cell></cell><cell></cell><cell cols="3">Models. Increasing Training Data via</cell><cell></cell><cell>v0.2-Twitter,</cell></row><row><cell></cell><cell></cell><cell cols="3">Machine Translation. Language Specific</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BERT with Manifold Mixup.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.02,87.04,370.28,151.90"><head></head><label></label><figDesc>:</figDesc><table coords="4,72.02,99.38,370.28,139.56"><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CLEF dataset distribution for English, Spanish and Arabic</cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell>Label</cell><cell>Train + Dev</cell><cell>Test</cell></row><row><cell>English</cell><cell>YES</cell><cell>5,651</cell><cell>108</cell></row><row><cell></cell><cell>NO</cell><cell>17,882</cell><cell>210</cell></row><row><cell>Spanish</cell><cell>YES</cell><cell>3,211</cell><cell>509</cell></row><row><cell></cell><cell>NO</cell><cell>11,737</cell><cell>4,491</cell></row><row><cell>Arabic</cell><cell>YES</cell><cell>2,654</cell><cell>377</cell></row><row><cell></cell><cell>NO</cell><cell>5,772</cell><cell>123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.02,667.06,350.49,84.95"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="4,72.02,680.38,350.49,71.63"><row><cell>Hyperparameter description for XLM-RoBERTa</cell><cell></cell></row><row><cell>Parameter Name</cell><cell>Corresponding Values</cell></row><row><cell>Maximum sequence length</cell><cell>128</cell></row><row><cell>Batch size</cell><cell>16</cell></row><row><cell>Learning rate</cell><cell>2e-5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.02,223.12,415.53,539.60"><head>Table 4</head><label>4</label><figDesc>Performance comparison for Transformer and ML models</figDesc><table coords="5,101.54,223.12,386.01,539.60"><row><cell>CLEF Data</cell><cell cols="2">Data Cleaning</cell><cell>Feature</cell><cell>ML/DL</cell></row><row><cell cols="3">&amp; Preprocessing</cell><cell>Extraction</cell><cell>algorithms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Result</cell></row><row><cell cols="2">Language</cell><cell>Model</cell><cell></cell><cell>F1 Score</cell></row><row><cell></cell><cell></cell><cell>MultinomialNB</cell><cell></cell><cell>80.36</cell></row><row><cell></cell><cell></cell><cell>SVC</cell><cell></cell><cell>72.81</cell></row><row><cell>English</cell><cell></cell><cell>LogisticRegression</cell><cell></cell><cell>79.11</cell></row><row><cell></cell><cell></cell><cell cols="2">PassiveAggressive Classifier</cell><cell>75.93</cell></row><row><cell></cell><cell></cell><cell cols="2">RandomForestClassifier</cell><cell>67.92</cell></row><row><cell></cell><cell></cell><cell>XLM-RoBERTa</cell><cell></cell><cell>83.30</cell></row><row><cell></cell><cell></cell><cell>MultinomialNB</cell><cell></cell><cell>88.87</cell></row><row><cell></cell><cell></cell><cell>SVC</cell><cell></cell><cell>89.11</cell></row><row><cell>Spanish</cell><cell></cell><cell>LogisticRegression</cell><cell></cell><cell>89.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.02,366.53,336.27,81.13"><head>Table 5</head><label>5</label><figDesc></figDesc><table coords="6,72.02,379.97,336.27,67.69"><row><cell>Leaderboard Results</cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell>Model</cell><cell>F1 Score</cell></row><row><cell>English</cell><cell>XLM-RoBERTa</cell><cell>0.833</cell></row><row><cell>Spanish</cell><cell>MultinomialNB</cell><cell>0.440</cell></row><row><cell>Arabic</cell><cell>MultinomialNB</cell><cell>0.530</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>machine learning and deep learning algorithms and found that the transformer based deep learning model (XLM-RoBERTa) can outperform traditional machine learning models for English and Arabic but for Spanish language, one traditional ML model (Logistic Regression) did better than other models.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,108.02,157.48,415.20,9.94;7,108.02,170.08,414.93,9.94;7,108.02,182.80,73.83,9.94" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,196.88,170.08,326.07,9.94;7,108.02,182.80,69.40,9.94">Overview of the CLEF-2022 CheckThat! lab task 1 on identifying relevant claims in tweets</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,195.40,415.12,9.94;7,108.02,208.12,415.12,9.94;7,108.02,220.72,187.11,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,356.23,195.40,166.90,9.94;7,108.02,208.12,320.16,9.94">Automated Claim Detection for Factchecking: A Case Study using Norwegian Pre-trained Language Models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sheikhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Touileb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,448.54,208.12,74.60,9.94;7,108.02,220.72,182.85,9.94">The 24rd Nordic Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2023-03">2023, March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,233.32,414.97,9.94;7,108.02,246.07,190.85,9.94" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02736</idno>
		<title level="m" coord="7,273.75,233.32,249.24,9.94;7,108.02,246.07,34.53,9.94">Claim check-worthiness detection as positive unlabelled learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,108.02,258.67,414.99,9.94;7,108.02,271.39,109.71,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,203.69,258.67,144.06,9.94">Detecting check-worthy claims</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">T</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Doctoral dissertation</note>
</biblStruct>

<biblStruct coords="7,108.02,283.99,415.10,9.94;7,108.02,296.59,393.32,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,401.47,283.99,121.65,9.94">Z-Index at CheckThat! Lab</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tarannum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R H</forename><surname>Noori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07308</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,135.97,296.59,208.78,9.94">Check-Worthiness Identification on Tweet Text</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,108.02,309.31,415.12,9.94;7,108.02,321.91,288.65,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,306.45,309.31,216.68,9.94;7,108.02,321.91,171.16,9.94">NUS-IDS at CheckThat! 2022: identifying checkworthiness of tweets using CheckthaT5</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D</forename><surname>Gollapalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Notes of CLEF</note>
</biblStruct>

<biblStruct coords="7,108.02,334.63,415.24,9.94;7,108.02,347.23,414.82,9.94;7,108.02,359.83,132.75,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,384.02,334.63,139.24,9.94;7,108.02,347.23,92.39,9.94">A benchmark dataset of checkworthy factual claims</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,219.05,347.23,303.79,9.94;7,108.02,359.83,28.19,9.94">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020-05">2020. May</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="821" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,372.55,414.74,9.94;7,108.02,385.15,414.92,9.94;7,108.02,397.87,214.73,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,329.59,372.55,193.17,9.94;7,108.02,385.15,414.92,9.94;7,108.02,397.87,57.91,9.94">Accenture at CheckThat! 2021: interesting claim identification and ranking with contextually sensitive lexical training data augmentation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05684</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,108.02,410.47,414.94,9.94;7,108.02,423.21,121.71,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,258.83,410.47,264.13,9.94;7,108.02,423.21,116.76,9.94">Zorros at CheckThat! 2022: Ensemble Model for Identifying Relevant Claims in Tweets</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Buliga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raschip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,435.81,378.95,9.94;7,108.02,448.41,415.26,9.94;7,108.02,461.13,143.67,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,456.71,435.81,66.26,9.94;7,108.02,448.41,415.26,9.94;7,108.02,461.13,26.91,9.94">TOBB ETU at CheckThat! 2022: detecting attention-worthy and harmful tweets and check-worthy claims</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sonmezer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Notes of CLEF</note>
</biblStruct>

<biblStruct coords="7,144.02,473.73,378.96,9.94;7,108.02,486.45,306.56,9.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,361.39,473.73,161.59,9.94;7,108.02,486.45,301.70,9.94">VTU_BGM at CheckThat! 2022: An Autoregressive Encoding Model for Detecting Check-worthy Claims</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kavatagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rachh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mulimani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,499.05,378.80,9.94;7,108.02,511.65,411.44,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,391.07,499.05,131.75,9.94;7,108.02,511.65,284.04,9.94">Courage at checkthat! 2022: Harmful tweet detection using graph neural networks and electra</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Donabauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,399.07,511.65,103.41,9.94">Working Notes of CLEF</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,524.37,378.97,9.94;7,108.02,536.97,199.73,9.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,238.39,524.37,284.60,9.94;7,108.02,536.97,83.36,9.94">AI Rational at CheckThat! 2022: using transformer models for tweet classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savchev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Working Notes of CLEF</note>
</biblStruct>

<biblStruct coords="7,144.02,549.69,378.74,9.94;7,108.02,562.29,314.96,9.94" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,384.54,549.69,138.22,9.94;7,108.02,562.29,310.81,9.94">iCompass at CheckThat! 2022: ARBERT and AraBERT for Arabic Checkworthy Tweet Identification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Taboubi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A B</forename><surname>Nessir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haddad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,574.89,378.90,9.94;7,108.02,587.64,414.81,9.94;7,108.02,600.24,180.63,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,491.30,574.89,31.62,9.94;7,108.02,587.64,409.76,9.94">CIC at CheckThat! 2021: Fake News detection Using Machine Learning And Data Augmentation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,119.78,600.24,102.85,9.94">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021-09">2021, September</date>
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,612.96,378.70,9.94;7,108.02,625.56,415.14,9.94;7,108.02,638.16,122.07,9.94" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="7,516.18,612.96,6.55,9.94;7,108.02,625.56,383.64,9.94">It takes nine to smell a rat: Neural multi-task learning for check-worthiness prediction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vasileva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07912</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,144.02,650.88,379.01,9.94;7,108.02,663.48,350.84,9.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,235.98,650.88,287.05,9.94;7,108.02,663.48,162.63,9.94">NLytics at CheckThat! 2021: Check-Worthiness Estimation as a Regression Problem on Transformers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pritzkau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,290.09,663.48,102.79,9.94">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="592" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,676.20,378.82,9.94;7,108.02,688.80,414.77,9.94;7,108.02,701.52,225.89,9.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,108.02,688.80,414.77,9.94;7,108.02,701.52,37.74,9.94">Team Sigmoid at CheckThat! 2021 Task 3a: Multiclass fake news detection with Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A M</forename><surname>Sardar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Salma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bhuiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,165.02,701.52,102.86,9.94">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,144.02,714.12,378.80,9.94;7,108.02,726.72,415.00,9.94;7,108.02,739.44,216.88,9.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,371.76,714.12,151.06,9.94;7,108.02,726.72,196.97,9.94">Fakedetector: Effective fake news detection with deep diffusive neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,350.33,726.72,172.69,9.94;7,108.02,739.44,110.69,9.94">IEEE 36th international conference on data engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04">2020, April. 2020</date>
			<biblScope unit="page" from="1826" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,74.42,378.88,9.94;8,108.02,87.04,381.80,9.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,108.02,87.04,234.09,9.94">Context-Driven Satire Detection With Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Razali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Halin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Norowi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Doraisamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,349.75,87.04,54.72,9.94">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="78780" to="78787" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,99.76,378.87,9.94;8,108.02,112.36,415.10,9.94;8,108.02,125.08,217.13,9.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,475.02,99.76,47.87,9.94;8,108.02,112.36,280.24,9.94">Fake news detection using deep learning models: A novel approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Upreti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Akbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,396.07,112.36,127.05,9.94;8,108.02,125.08,150.70,9.94">Transactions on Emerging Telecommunications Technologies</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3767</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,137.68,378.85,9.94;8,108.02,150.28,415.05,9.94;8,108.02,163.00,167.91,9.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,296.47,137.68,226.40,9.94;8,108.02,150.28,56.86,9.94">Bayesian multinomial Naïve Bayes classifier to text classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,184.25,150.28,338.82,9.94">Advanced Multimedia and Ubiquitous Engineering: MUE/FutureTech 2017</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,175.60,378.76,9.94;8,108.02,188.32,220.85,9.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,307.33,175.60,215.45,9.94;8,108.02,188.32,56.86,9.94">Support vector machine for functional data classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Villa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,171.62,188.32,71.71,9.94">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="730" to="742" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,200.81,378.78,10.04;8,108.02,213.52,407.24,9.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="8,470.99,200.81,51.81,10.04;8,108.02,213.52,278.47,9.94">Interpreting parameters in the logistic regression model with random effects</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Budtz-Jørgensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Endahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,393.43,213.52,45.85,9.94">Biometrics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="909" to="914" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,226.24,379.19,9.94;8,108.02,238.84,79.71,9.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="8,298.69,226.24,209.75,9.94">Classification and regression by randomForest</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,516.46,226.24,6.75,9.94;8,108.02,238.84,19.87,9.94">R news</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,251.59,379.14,9.94;8,108.02,264.19,414.82,9.94;8,108.02,276.91,178.95,9.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="8,276.44,251.59,242.85,9.94">Fake news detection using passive-aggressive classifier</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Meel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,119.78,264.19,403.06,9.94;8,108.02,276.91,22.08,9.94">Inventive Communication and Computational Technologies: Proceedings of ICICCT 2020</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,289.51,379.15,9.94;8,108.02,302.11,415.03,9.94;8,108.02,314.83,26.52,9.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="8,282.73,289.51,176.76,9.94">Clickbait detection using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,480.22,289.51,42.94,9.94;8,108.02,302.11,347.83,9.94">2016 2nd international conference on next generation computing technologies (NGCT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-10">2016, October</date>
			<biblScope unit="page" from="268" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,327.32,378.78,10.04;8,108.02,340.15,345.32,9.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="8,243.42,327.32,279.38,10.04;8,108.02,340.15,77.27,9.94">Machine learning methods for toxic comment classification: a systematic review</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Andročec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,192.77,340.15,184.57,9.94">Acta Universitatis Sapientiae, Informatica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,352.75,379.09,9.94;8,108.02,365.35,391.76,9.94" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="8,410.43,352.75,112.68,9.94;8,108.02,365.35,235.12,9.94">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,144.02,378.07,378.84,9.94;8,108.02,390.67,402.92,9.94" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="8,108.02,390.67,246.10,9.94">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,144.02,403.39,378.92,9.94;8,108.02,416.01,415.24,9.94;8,108.02,428.61,414.92,9.94;8,108.02,441.33,366.08,9.94" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="8,226.62,416.01,296.63,9.94;8,108.02,428.61,226.20,9.94">Overview of the CLEF-2023 CheckThat! Lab Task 1 on Check-Worthiness in Multimodal and Multigenre Content</title>
		<author>
			<persName coords=""><forename type="first">Barrón-Cedeño</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hakimov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nakovet</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,355.11,428.61,167.83,9.94;8,108.02,441.33,262.19,9.94">Proceedings of the Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<meeting>the Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,453.93,378.74,9.94;8,108.02,466.65,415.09,9.94;8,108.02,479.25,375.44,9.94" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="8,398.01,453.93,124.75,9.94;8,108.02,466.65,329.89,9.94">CrisisBench: Benchmarking crisis-related social media datasets for humanitarian information processing</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,456.94,466.65,66.17,9.94;8,108.02,479.25,270.71,9.94">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2021-05">2021, May</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
