<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,388.86,15.42;1,89.29,106.66,331.91,15.42">Accenture at CheckThat! 2023: Learning to Detect Political Bias of News Articles and Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,47.01,11.96"><forename type="first">Sieu</forename><surname>Tran</surname></persName>
							<email>sieu.tran@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.94,134.97,73.62,11.96"><forename type="first">Paul</forename><surname>Rodrigues</surname></persName>
							<email>paul.rodrigues@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.21,134.97,84.88,11.96"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
							<email>b.strauss@accenturefederal.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Accenture</orgName>
								<address>
									<addrLine>1201 New York Ave NW</addrLine>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.09,134.97,86.91,11.96"><forename type="first">Evan</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
							<email>emwillia@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,388.86,15.42;1,89.29,106.66,331.91,15.42">Accenture at CheckThat! 2023: Learning to Detect Political Bias of News Articles and Sources</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">223E3BAF44BC79C096E42341FC803364</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>bias detection</term>
					<term>political bias</term>
					<term>news analysis</term>
					<term>data-driven journalism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the methodology of Team Accenture for the CLEF CheckThat! shared task on identifying political biases in news articles and news sources. We utilize machine back-translation to augment the minority classes in datasets labeling article and news source bias in three categories-Left, Center, and Right, and used this augmented data to fine-tune RoBERTa transformer models. This was the highest ranking strategy in the shared task for detecting both political bias of a news article (at 0.473 Mean Average Precision) as well as for detecting political bias of a news source (at 0.549 mean average precision).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Political bias in news articles can jeopardize an article's reliability. Biased articles can employ loaded language, lack proper context, can frame a story selectively, or can include outright falsehoods <ref type="bibr" coords="1,139.52,408.22,11.40,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,153.65,408.22,7.60,10.91" target="#b1">2]</ref>. Consumption of partisan political outlets has been linked to numerous realworld behavioral differences. Responses to COVID-19 were found by numerous studies to be strongly partisan. In 2020, <ref type="bibr" coords="1,212.49,435.32,72.88,10.91">Gollwitzer et al.</ref> found that consumption of the US conservative media outlet, Fox News, was associated with reduced physical distancing and increased vaccine hesitancy <ref type="bibr" coords="1,134.61,462.42,11.36,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,148.69,462.42,7.57,10.91" target="#b3">4]</ref>.</p><p>Consequently, political bias detection has become an important task. <ref type="bibr" coords="1,407.15,475.97,12.80,10.91" target="#b0">[1]</ref> propose a headline attention network to detect political bias in Telugu newspapers. <ref type="bibr" coords="1,371.91,489.52,12.71,10.91" target="#b4">[5]</ref> use Copula Ordinal Regression (COR) models to jointly predict news media reliability and bias. <ref type="bibr" coords="1,399.21,503.06,12.91,10.91" target="#b5">[6]</ref> provides an analysis of linguistic features of biased domains. <ref type="bibr" coords="1,270.84,516.61,12.93,10.91" target="#b6">[7]</ref> demonstrate that biased news domains form link communities. Other researchers have employed political bias detection methods on non-news datasets, including the political bias of congressional speeches <ref type="bibr" coords="1,368.85,543.71,11.44,10.91" target="#b7">[8]</ref>, of cable news channels <ref type="bibr" coords="1,491.93,543.71,11.44,10.91" target="#b8">[9]</ref>, and of YouTube videos <ref type="bibr" coords="1,193.13,557.26,16.25,10.91" target="#b9">[10]</ref>.</p><p>CheckThat! 2023 Task 3A provides bias-labeled articles that teams must classify as 'Left', 'Center', or 'Right'. Task 3B provides bias-labeled news-sites and a collection of articles scraped from each news site. Given the news source and the articles, the goal is to classify URLs of news outlets as 'Left', 'Center', or 'Right'. Teams are evaluated using mean absolute error (MAE) <ref type="bibr" coords="2,89.29,114.06,17.96,10.91" target="#b10">[11]</ref> where lower MAE is better. The setup of the task is similar to the article-level fake-news domain detection setup used in <ref type="bibr" coords="2,230.40,127.61,16.20,10.91" target="#b11">[12]</ref>. However, Subtask 3B uses domain bias labels rather than domain reliability labels.</p><p>In this work, we describe that data augmentation and fine-tuning approach employed by Team Accenture for CheckThat! lab subtasks 3A and 3B. Of the four teams that submitted, the Accenture team achieved the best overall MAE for 3A (0.473). Of the two teams that submitted, the Accenture team achieved the best overall MAE for 3B (0.549).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exploratory Analysis</head><p>Table <ref type="table" coords="2,115.20,253.99,4.97,10.91" target="#tab_0">1</ref> shows the number of samples and unique word counts for each of the datasets provided. We see that while the training set for news article bias consists of a much larger training sample than the news media source bias, it has significantly less number of unique words. We would hypothesize that a larger quantity of unique words would yield models of higher performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Label Balance</head><p>As shown in Figure <ref type="figure" coords="2,178.13,496.27,3.73,10.91" target="#fig_0">1</ref>, all of the datasets provided by the CheckThat! organizers had label bias which skewed each dataset towards articles that were labeled class 2 (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">WordPiece Analysis</head><p>Transformer models utilize WordPiece tokenization schemes that are dependant on the model being evaluated. At the time of pre-training, the WordPiece algorithm determines which pieces of words will be retained, and which will be discarded. We present our analysis in Table <ref type="table" coords="2,500.04,586.64,3.81,10.91" target="#tab_1">2</ref>.</p><p>Unexpectedly, the RoBERTa tokenizers we used did not return UNK tokens on any dataset provided by the CLEF CheckThat! organizers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformer Architectures and Pre-Trained Models</head><p>In this work, we utilize RoBERTa models. The Bidirectional Encoder Representation Transformer (BERT) is a transformer-based architecture that was introduced in 2018 <ref type="bibr" coords="3,420.06,458.57,16.55,10.91" target="#b12">[13,</ref><ref type="bibr" coords="3,439.91,458.57,12.42,10.91" target="#b13">14]</ref>. BERT has had a substantial impact on the field of NLP, and achieved state of the art results on 11 NLP benchmarks at the time of its release. RoBERTa, introduced by <ref type="bibr" coords="3,370.83,485.67,16.28,10.91" target="#b14">[15]</ref>, modified various parts of BERTs training process. These modifications include more training data, more pre-training steps with bigger batches over more data, removing BERT's Next Sentence Prediction, training on longer sequences, and dynamically changing the masking pattern applied to the training data <ref type="bibr" coords="3,110.75,539.87,16.17,10.91" target="#b15">[16]</ref>. For this work, we fine-tune roberta-large <ref type="bibr" coords="3,315.79,539.87,16.17,10.91" target="#b16">[17]</ref>. The English RoBERTa model contains 50,265 WordPieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Augmentation</head><p>The organizers provided a training and a development set for each language. We use the provided training set and development set to create internal training and validation sets for experimentation. We use the test set provided by organizers as a hold-out test set. For each article, training data was augmented using back-translation provided by AWS Translate. We appended back-translated left-and center-labeled articles to the training set. In our 2021 experiment <ref type="bibr" coords="4,186.08,382.09,16.42,10.91" target="#b17">[18]</ref>, we found that this form of augmentation resulted in a significant increase in recall and F1 score for check-worthy tweets. For both article and news source classification, we used Spanish as the pivot language. Due to significant sample imbalance in the training sets for both tasks, we augmented the 0 (Left)-and 1 (Center)-class until the samples are balanced. Specifically, for classification of article bias, we augmented 5,000 samples of the 0-class and 2,000 of the 1 (Center)-class. For classification of news source bias, we augmented 700 samples of the 0 (Left)-class and 300 of the 1 (Left)-class. Table <ref type="table" coords="4,385.07,463.38,5.01,10.91" target="#tab_2">3</ref> shows the BLEU score for each back-translation scheme. The lower the score, the more divergent the translation to the original text. In a machine translation workflow we would wish to maximize the BLEU score for the best translation. In a data augmentation workflow, we wish to introduce variation to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification</head><p>For the Article Bias Classification RoBERTa model, we added an additional mean-pooling layer and dropout layer on top of the model prior to the final three binary classification layers, each of which corresponding to a class (i.e., 0 (Left), 1 (Center), or 2 (Right)). The highest class probability determines the article's final classification. This approach is sometimes referred to as the one-against-all approach for multi-class problem <ref type="bibr" coords="4,332.54,621.51,16.09,10.91" target="#b18">[19]</ref>. Adding these additional layers has been shown to help prevent over-fitting while fine-tuning. We used an Adam optimizer with a learning rate of 2ùëí -5 and an epsilon of 1.5ùëí -8. We use a binary cross-entropy loss function, 4 epochs, and a batch size of 32. For the News Source Bias Classification, the same model architecture above is used to finetune a RoBERTa model and classify all article from each news source. We used the majority class label of all articles of a given news source as the class label to establish source bias.</p><p>For comparison, we have fine-tuned an additional RoBERTa model for each task above with a single multi-class classifier instead of three binary classifiers. We kept all parameters above the same with a few exceptions: we used the Softmax activation and the sparse categorical cross-entropy loss function instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Table <ref type="table" coords="6,114.70,111.28,4.97,10.91" target="#tab_3">5</ref> contains model performance on the test set provided by the organizers. Our One-Against-All News Article Bias had an accuracy of 0.633 and a weighted average F1-score of 0.632. Our One-Against-All News Media Source Bias classifier had an accuracy of 0.627 and a weighted average F1-score of 0.625. The official evaluation numbers are shown in Table <ref type="table" coords="6,449.29,151.93,3.81,10.91">6</ref>, where the One-Against-All News Article Bias received an MAE of 0.473 and the One-Against-All News Media Source Bias classifier received an MAE of 0.549.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper demonstrates models for the classification of political bias in news articles and news sources. In both models, we utilize a pre-trained RoBERTa Large model fine-tuned to the task. We utilized back translation as a strategy to augment the training data, addressing label bias which is found in many machine learning problems. Of the four teams that participated in this shared task, Team Accenture achieved the best overall MAE for 3A (0.473). Of the two teams that submitted, Team Accenture achieved the best overall MAE for 3B (0.549). In addition, for classification of Bias News Article, the one-against-all and the multi-class classifier did not differ significantly in performance. However, for classification of Bias News Sources, the multi-class classifier outperform the one-against-all approach in accuracy but under-perform in MAE. Overall, Team Accenture's choice of implementing three binary classification layers instead of a single multi-class classifier is the optimal one in both performance and fine-tuning time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,255.94,196.17,8.93;3,145.43,84.19,304.41,159.19"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Label distribution across training sets</figDesc><graphic coords="3,145.43,84.19,304.41,159.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,323.64,393.58,117.69"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="2,89.29,335.65,393.28,105.69"><row><cell>Dataset Descriptions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="4">Modeling Group # of Source # of Articles Unique Words</cell></row><row><cell>News Article Bias</cell><cell>Train</cell><cell></cell><cell>45,066</cell><cell>47,054</cell></row><row><cell>News Article Bias</cell><cell>Test</cell><cell></cell><cell>5,198</cell><cell>21,503</cell></row><row><cell>News Article Bias</cell><cell>Validation</cell><cell></cell><cell>5,008</cell><cell>19,752</cell></row><row><cell>News Media Source Bias</cell><cell>Train</cell><cell>817</cell><cell>6,994</cell><cell>89,007</cell></row><row><cell>News Media Source Bias</cell><cell>Test</cell><cell>102</cell><cell>896</cell><cell>34,372</cell></row><row><cell>News Media Source Bias</cell><cell>Validation</cell><cell>104</cell><cell>878</cell><cell>37,484</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,289.09,359.02,107.26"><head>Table 2</head><label>2</label><figDesc>Token Distribution in Data for Each Task.</figDesc><table coords="3,147.26,314.96,300.75,81.39"><row><cell>Task</cell><cell cols="3">Tokenizer Type Modeling Set WordPiece</cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>4,336,612</cell></row><row><cell>News Article Bias</cell><cell>RoBERTa-based</cell><cell>Test</cell><cell>614,064</cell></row><row><cell></cell><cell></cell><cell>Validation</cell><cell>482,337</cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>7,324,674</cell></row><row><cell cols="2">News Media Source Bias RoBERTa-based</cell><cell>Test</cell><cell>985,660</cell></row><row><cell></cell><cell></cell><cell>Validation</cell><cell>1,128,531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.98,90.49,394.30,237.80"><head>Table 3</head><label>3</label><figDesc>Average Sentence BLEU Score for Each Back-translation Scheme</figDesc><table coords="4,88.99,120.27,394.29,208.01"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average Sentence</cell></row><row><cell>Task</cell><cell cols="5">Label Class Back-translation</cell><cell>BLEU Score</cell></row><row><cell>News Article Bias</cell><cell></cell><cell>0 (Left)</cell><cell></cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>0.504</cell></row><row><cell>News Article Bias</cell><cell cols="3">1 (Center)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>0.481</cell></row><row><cell cols="2">News Media Source Bias</cell><cell>0 (Left)</cell><cell></cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>0.491</cell></row><row><cell cols="4">News Media Source Bias 1 (Center)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>0.517</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">New Tokens in Machine Translated Text</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unique</cell><cell>Unique</cell><cell>New</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tokens</cell><cell>tokens</cell><cell>Tokens</cell></row><row><cell>Task</cell><cell cols="4">Label Class Back-translation</cell><cell cols="2">in source</cell><cell>in MT</cell><cell>in MT</cell></row><row><cell>News Article Bias</cell><cell cols="2">0 (Left)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>61771</cell><cell>55982</cell><cell>19045</cell></row><row><cell>News Article Bias</cell><cell cols="2">1 (Center)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>27017</cell><cell>25498</cell><cell>6916</cell></row><row><cell>News Media Source Bias</cell><cell cols="2">0 (Left)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>36221</cell><cell>31591</cell><cell>11451</cell></row><row><cell cols="3">News Media Source Bias 1 (Center)</cell><cell cols="2">EN &gt; ES &gt; EN</cell><cell>22062</cell><cell>19561</cell><cell>6545</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.98,90.49,397.51,402.27"><head>Table 5</head><label>5</label><figDesc>Accenture results from 2023 CheckThat! Lab Task 3</figDesc><table coords="5,88.98,119.88,397.51,372.87"><row><cell>Task</cell><cell cols="2">Classifer Type</cell><cell>Class</cell><cell cols="3">Precision Recall F1-score</cell></row><row><cell>News Article Bias</cell><cell cols="2">One-Against-All</cell><cell>0 (Left)</cell><cell>0.822</cell><cell>0.480</cell><cell>0.606</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 (Center)</cell><cell>0.622</cell><cell>0.791</cell><cell>0.696</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2 (Right)</cell><cell>0.418</cell><cell>0.769</cell><cell>0.542</cell></row><row><cell></cell><cell></cell><cell></cell><cell>macro avg</cell><cell>0.621</cell><cell>0.680</cell><cell>0.615</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">weighted avg 0.696</cell><cell>0.633</cell><cell>0.632</cell></row><row><cell>News Article Bias</cell><cell>Multi-class</cell><cell></cell><cell>0 (Left)</cell><cell>0.865</cell><cell>0.409</cell><cell>0.555</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 (Center)</cell><cell>0.599</cell><cell>0.841</cell><cell>0.700</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2 (Right)</cell><cell>0.416</cell><cell>0.783</cell><cell>0.543</cell></row><row><cell></cell><cell></cell><cell></cell><cell>macro avg</cell><cell>0.627</cell><cell>0.678</cell><cell>0.599</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">weighted avg 0.709</cell><cell>0.619</cell><cell>0.608</cell></row><row><cell cols="3">News Media Source Bias One-Against-All</cell><cell>0 (Left)</cell><cell>0.600</cell><cell>0.720</cell><cell>0.655</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 (Center)</cell><cell>0.645</cell><cell>0.690</cell><cell>0.667</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2 (Right)</cell><cell>0.634</cell><cell>0.542</cell><cell>0.584</cell></row><row><cell></cell><cell></cell><cell></cell><cell>macro avg</cell><cell>0.626</cell><cell>0.650</cell><cell>0.635</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">weighted avg 0.629</cell><cell>0.627</cell><cell>0.625</cell></row><row><cell>News Media Source Bias</cell><cell>Multi-class</cell><cell></cell><cell>0 (Left)</cell><cell>0.710</cell><cell>0.880</cell><cell>0.786</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 (Center)</cell><cell>0.585</cell><cell>0.828</cell><cell>0.686</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2 (Right)</cell><cell>0.867</cell><cell>0.542</cell><cell>0.667</cell></row><row><cell></cell><cell></cell><cell></cell><cell>macro avg</cell><cell>0.721</cell><cell>0.750</cell><cell>0.713</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">weighted avg 0.748</cell><cell>0.706</cell><cell>0.701</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Accenture results from 2023 CheckThat! Lab Task 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Task</cell><cell cols="2">Classifer Type</cell><cell cols="2">Accuracy MAE</cell></row><row><cell cols="2">News Article Bias</cell><cell cols="3">One-Against-All 0.633</cell><cell>0.473</cell></row><row><cell cols="2">News Article Bias</cell><cell cols="2">Multi-class</cell><cell>0.619</cell><cell>0.491</cell></row><row><cell cols="5">News Media Source Bias One-Against-All 0.627</cell><cell>0.549</cell></row><row><cell cols="2">News Media Source Bias</cell><cell cols="2">Multi-class</cell><cell>0.706</cell><cell>0.373</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,445.32,393.33,10.91;6,112.66,458.87,393.32,10.91;6,112.66,472.42,261.31,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,312.79,445.32,193.20,10.91;6,112.66,458.87,80.10,10.91">Detecting political bias in news articles using headline attention</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R R</forename><surname>Gangula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Duggenpudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mamidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,215.77,458.87,290.21,10.91;6,112.66,472.42,182.79,10.91">Proceedings of the 2019 ACL workshop BlackboxNLP: analyzing and interpreting neural networks for NLP</title>
		<meeting>the 2019 ACL workshop BlackboxNLP: analyzing and interpreting neural networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,485.97,393.33,10.91;6,112.66,499.52,393.58,10.91;6,112.33,513.06,29.19,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02247</idno>
		<title level="m" coord="6,309.61,485.97,196.38,10.91;6,112.66,499.52,243.83,10.91">Disentangling structure and style: Political bias detection in news by inducing document hierarchy</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,526.61,395.01,10.91;6,112.30,540.16,393.68,10.91;6,112.66,553.71,308.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,161.36,540.16,344.62,10.91;6,112.66,553.71,98.00,10.91">Partisan differences in physical distancing are linked to health outcomes during the covid-19 pandemic</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gollwitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>P√§rnamets</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,219.66,553.71,112.28,10.91">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1186" to="1197" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,567.26,393.73,10.91;6,112.66,580.81,360.71,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,210.47,567.26,295.92,10.91;6,112.66,580.81,185.69,10.91">The effects of partisan media in the face of global pandemic: How news shaped COVID-19 vaccine hesitancy</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stecula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,306.71,580.81,110.81,10.91">Political Communication</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,594.36,393.61,10.91;6,112.66,607.91,393.59,10.91;6,112.66,621.46,146.44,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00542</idno>
		<title level="m" coord="6,333.01,594.36,173.26,10.91;6,112.66,607.91,359.05,10.91">Multi-task ordinal regression for jointly predicting the trustworthiness and the leading political ideology of news media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,635.01,393.32,10.91;6,112.66,648.56,393.08,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10652</idno>
		<title level="m" coord="6,335.81,635.01,170.17,10.91;6,112.66,648.56,211.03,10.91">Analyzing political bias and unfairness in news articles at different levels of granularity</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,662.11,393.33,10.91;7,112.66,86.97,393.32,10.91;7,112.66,100.52,136.61,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,356.88,662.11,149.11,10.91;7,112.66,86.97,124.81,10.91">A link-based approach to detect media bias in news websites</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patricia Aires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">G</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,260.69,86.97,245.29,10.91;7,112.66,100.52,48.47,10.91">Companion Proceedings of The 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="742" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,114.06,393.32,10.91;7,112.66,127.61,393.33,10.91;7,112.66,141.16,313.58,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,324.93,114.06,181.05,10.91;7,112.66,127.61,179.45,10.91">A machine learning pipeline to examine political bias with congressional speeches</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hajare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagavathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,314.77,127.61,191.22,10.91;7,112.66,141.16,198.49,10.91">2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="239" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,154.71,393.33,10.91;7,112.28,168.26,201.77,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,242.87,154.71,134.17,10.91">Measuring dynamic media bias</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lelkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mccrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,384.80,154.71,121.19,10.91;7,112.28,168.26,93.58,10.91">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">e2202197119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,181.81,393.33,10.91;7,112.66,195.36,393.33,10.91;7,112.66,208.91,107.17,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dinkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08948</idno>
		<title level="m" coord="7,306.77,181.81,199.21,10.91;7,112.66,195.36,313.77,10.91">Predicting the leading political ideology of youtube channels using acoustic, textual, and metadata information</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,222.46,393.33,10.91;7,112.66,236.01,394.61,10.91;7,112.14,249.56,395.05,10.91;7,112.33,263.11,120.27,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,450.85,222.46,55.13,10.91;7,112.66,236.01,374.81,10.91">Overview of the CLEF-2023 CheckThat! lab task 3 on political bias of news articles and news media</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azizov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.14,249.56,390.80,10.91">Working Notes of CLEF 2023-Conference and Labs of the Evaluation Forum, CLEF &apos;2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,276.66,395.17,10.91;7,112.66,290.20,393.33,10.91;7,112.66,303.75,236.38,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,471.14,276.66,36.69,10.91;7,112.66,290.20,227.54,10.91">A topicagnostic approach for identifying fake news pages</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elghafari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.34,290.20,140.65,10.91;7,112.66,303.75,148.45,10.91">Companion proceedings of the 2019 World Wide Web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="975" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,317.30,393.33,10.91;7,112.66,330.85,363.59,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,323.15,317.30,182.83,10.91;7,112.66,330.85,181.08,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,344.40,393.33,10.91;7,112.66,357.95,321.60,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m" coord="7,321.65,344.40,184.33,10.91;7,112.66,357.95,191.23,10.91">Well-read students learn better: On the importance of pre-training compact models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,371.50,395.17,10.91;7,112.66,385.05,393.33,10.91;7,112.33,398.60,296.49,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,140.00,385.05,263.30,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,412.15,395.17,10.91;7,112.66,425.70,394.53,10.91;7,112.66,439.25,393.33,10.91;7,112.66,452.79,393.33,10.91;7,112.66,466.34,395.00,10.91;7,112.66,479.89,17.97,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,287.06,412.15,220.77,10.91;7,112.66,425.70,259.19,10.91">Accenture at CheckThat! 2020: If you say so: Posthoc fact-checking of claims using transformer-based models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Novak</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_226.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,235.82,439.25,270.17,10.91;7,112.66,452.79,78.85,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="7,480.05,453.81,25.94,9.72;7,112.66,466.34,122.95,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,493.44,395.17,10.91;7,112.66,506.99,393.33,10.91;7,112.33,520.54,296.49,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="7,140.00,506.99,263.30,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,534.09,393.33,10.91;7,112.66,547.64,394.53,10.91;7,112.66,561.19,122.77,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05684</idno>
		<title level="m" coord="7,276.86,534.09,229.12,10.91;7,112.66,547.64,389.62,10.91">Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,574.74,394.53,10.91;7,112.66,588.29,394.53,10.91;7,112.39,601.84,153.10,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,195.43,574.74,306.96,10.91">One-against-all multi-class svm classification using reliability measures</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,128.76,588.29,344.13,10.91">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="854" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
