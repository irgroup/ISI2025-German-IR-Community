<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.43,451.02,16.72;1,72.02,98.19,153.18,16.72;1,72.00,126.70,162.24,11.63">Encoded Classifier Using Knowledge Distillation for Multi-Author Writing Style Analysis Notebook for PAN at CLEF 2023</title>
				<funder ref="#_W7rbrup">
					<orgName type="full">Natural Science Foundation of Guangdong Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,153.22,70.14,11.63"><forename type="first">Mingjie</forename><surname>Huang</surname></persName>
							<email>mingjiehuang007@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.96,153.22,74.10,11.63"><forename type="first">Zhaohao</forename><surname>Huang</surname></persName>
							<email>zhaohaohuang6@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.88,153.22,60.60,11.63"><forename type="first">Leilei</forename><surname>Kong</surname></persName>
							<email>kongleilei@fosu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.43,451.02,16.72;1,72.02,98.19,153.18,16.72;1,72.00,126.70,162.24,11.63">Encoded Classifier Using Knowledge Distillation for Multi-Author Writing Style Analysis Notebook for PAN at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDE90E782E9072068C8450601BC8DAB1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-author writing style analysis</term>
					<term>knowledge distillation</term>
					<term>pre-trained language model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report on our writing style change detection system which is used for the PAN task of Multi-Author Writing Style Analysis. To detect the writing style change within texts, a method based on an encoded classifier using knowledge distillation is proposed. The method proposed in this paper consists of two parts: A neural classifier based on an encoder of a pretrained language model is used to extract the features of texts and make the writing style change detection. And the knowledge distillation method based on the teacher-student architecture is used for model compression. We evaluate our methods on three tasks with different difficulties on the metrics for F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of the style change detection (SCD) task is to identify text positions within a given multiauthor document at which the author switches <ref type="bibr" coords="1,274.92,420.25,11.70,10.70" target="#b0">[1]</ref>. The SCD task in PAN@CLEF 2023 is defined as for a given text, find all positions of writing style change on the paragraph level <ref type="bibr" coords="1,424.80,432.97,11.70,10.70" target="#b0">[1]</ref>. The simultaneous change of authorship and topic is carefully controlled and participants are provided with datasets of three difficulty levels <ref type="bibr" coords="1,171.72,458.16,11.70,10.70" target="#b0">[1]</ref>. Multi-level data sets can not only reflect the ability of the same model in different scenarios but also inspire participants to try to use diverse methods to solve SCD problems <ref type="bibr" coords="1,72.00,483.47,12.92,10.70" target="#b0">[1]</ref> .</p><p>In recent years, PAN@CLEF has held many international competitions on SCD tasks, and the participants also provided a variety of inspiring methods. There are three main categories of participant approaches. The first category is the method of artificially selecting traditional features in the text for similarity discrimination <ref type="bibr" coords="1,187.92,534.10,11.70,10.70" target="#b2">[3]</ref>. The second type uses the neural network model to extract the text representation and then calculates the similarity of the text representations <ref type="bibr" coords="1,418.43,546.70,11.70,10.70" target="#b3">[4]</ref>. The third category leverages a more complex pre-trained language model, and some participants build a Siamese model or multi-model fusion on this basis <ref type="bibr" coords="1,218.53,572.02,12.00,10.70" target="#b4">[5,</ref><ref type="bibr" coords="1,233.65,572.02,7.92,10.70" target="#b5">6]</ref>. According to the performance rankings of various methods in recent years, it can be observed that models with larger parameters and more complex structures tend to have better performance on SCD tasks.</p><p>In this paper, we propose a method of encoded classifier using knowledge distillation <ref type="bibr" coords="1,465.02,609.93,12.80,10.70" target="#b6">[7]</ref> for multiauthor writing style analysis. The teacher model with huge parameters and complex structure will be trained on the task-specific dataset and in-domain dataset. To allow the uploaded model to run on the evaluation machine, the teacher model will be compressed through the technology of knowledge distillation, where the trained teacher model and student model will be finetuned on task-specific datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Figure <ref type="figure" coords="2,102.84,303.71,4.34,10.83">1</ref>: The architecture of the encoded classifier using knowledge distillation.</p><p>Our proposed method is shown in Figure <ref type="figure" coords="2,274.55,330.01,4.14,10.70">1</ref>, Firstly, the teacher model is trained on task-specific datasets and external data. Then the teacher model will be compressed by using knowledge distillation <ref type="bibr" coords="2,71.99,355.33,12.92,10.70" target="#b6">[7]</ref> to a smaller student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data processing</head><p>Given a document 𝐷𝐷, it will be divided into fragments according to natural paragraphs, denoted as {𝑝𝑝 1 , 𝑝𝑝 2 , … , 𝑝𝑝 𝑛𝑛 }. Then each fragment will be recombined with the following fragment to form a new text pair, forming a total of n-1 pairs, denoted as {(𝑝𝑝 1 , 𝑝𝑝 2 ), (𝑝𝑝 2 , 𝑝𝑝 3 ) … , (𝑝𝑝 𝑛𝑛-1 , 𝑝𝑝 𝑛𝑛 )}. After preprocessing, we converted the multi-label classification problem into a writing style similarity discrimination problem between text pairs. For paragraph pairs that are longer than the maximum sequence size of our models, we first count the token sequence length distribution of paragraphs in the data set, and then select the length that can cover 90% of the sequence as the threshold length for text cutting, and discard the part that exceeds the threshold length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.</head><p>Teacher model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge distillation 2.3.1. Model training</head><p>The student model is a model of the same series as the teacher model while the student model has smaller parameters. The student model is not as powerful as the teacher model in reasoning and classification while the student model has the advantage of smaller computing resource requirements and faster inference speed than the teacher model.</p><p>Before knowledge distillation, the teacher model must first be fine-tuned on the task-specific data set again to ensure that the teacher model fits the data adequately. After that, the knowledge of the teacher model will be transferred to the student model through knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Loss function</head><p>To allow our final trained model to run on limited computing resources, Hinton's classic knowledge distillation <ref type="bibr" coords="3,121.69,282.01,12.92,10.70" target="#b6">[7]</ref> method is used in our model. The loss function consists of two parts, the distillation loss between the teacher model and the student model and the loss between the student predictions and ground truth labels.</p><p>After the task-specific data is fed into the teacher model and the student model, the two models each output a probability distribution, also known as logits. When calculating the loss between the teacher model and the student model, the logits output by the two models will be divided by a temperature coefficient 𝑇𝑇, then normalized by the softmax function into a probability distribution and finally the KL divergence loss between the two probability distributions can be calculated, which is called distillation loss <ref type="bibr" coords="3,91.93,383.63,11.70,10.70" target="#b6">[7]</ref>, denoted as 𝐿𝐿 𝑘𝑘𝑘𝑘 .</p><formula xml:id="formula_0" coords="3,200.52,394.96,311.86,32.66">𝐿𝐿 𝑘𝑘𝑘𝑘 = 𝐾𝐾𝐿𝐿( exp(𝑧𝑧 𝑖𝑖 𝑠𝑠 ) ∑ exp�𝑧𝑧 𝑗𝑗 𝑠𝑠 � 𝑗𝑗 , exp�𝑧𝑧 𝑖𝑖 𝑡𝑡 � ∑ exp�𝑧𝑧 𝑗𝑗 𝑡𝑡 � 𝑗𝑗 )<label>(1)</label></formula><p>where 𝑧𝑧 𝑠𝑠 is the logits of student model, 𝑧𝑧 𝑡𝑡 is the logits of teacher model, 𝐾𝐾𝐿𝐿() is KL divergence function.</p><p>In addition to the distillation loss, another part of the total loss is the cross-entropy loss between the probability predicted by the student model and the ground truth label, denoted as 𝐿𝐿 𝑐𝑐𝑐𝑐 . Combining the two losses, with different weights, we get the final total loss function.</p><formula xml:id="formula_1" coords="3,217.56,493.16,294.83,12.46">𝐿𝐿 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 = 𝛼𝛼𝐿𝐿 𝑘𝑘𝑘𝑘 + (1 -𝛼𝛼)𝐿𝐿 𝑐𝑐𝑐𝑐 (2)</formula><p>where 𝛼𝛼 is the weight of ground truth loss, and 𝐿𝐿 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 is the total loss of the whole knowledge distillation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>The data on writing style change detection provided by PAN@CLEF consists of three difficulty levels.</p><p>1. Easy: The paragraphs of a document cover a variety of topics, allowing approaches to make use of topic information to detect authorship changes. 2. Medium: The topical variety in a document is small (though still present) forcing the approaches to focus more on style to effectively solve the detection task. 3. Hard: All paragraphs in a document are on the same topic.</p><p>In the data set given by PAN, the label information available to the participants includes the number of authors in the document and the labels of whether the style of writing changes between paragraphs. We split the documents in the dataset by natural segments and labels and re-counted the number of datasets. The statistical results are shown in the table below. We choose the training set of the PAN 2020 authorship verification task, a total of 52601 pieces of data, as external data for training. The authorship verification dataset is composed of text pairs and labels used to judge whether they are written by the same author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Experiments setup</head><p>In this paper, we choose mT0-xl <ref type="bibr" coords="4,230.40,287.41,12.80,10.70" target="#b7">[8]</ref> as the teacher model's encoder with 24-layer, 2048-hidden, 24heads, and 1.8B parameters. And we choose mT0-large <ref type="bibr" coords="4,325.92,300.01,12.80,10.70" target="#b7">[8]</ref> as the student model's encoder with 24layer, 1024-hidden, 24-heads, and 0.6B parameters. The vocab size is 250,112.</p><p>When we finetune the teacher model on all three datasets and external datasets, the training batch size is set to 16, and the maximum length of the encoder is set to 256, which means the total length of text pairs is 512, since most of the text pairs have less than 512 tokens. We use Adafactor optimizer, learning rate set to 1e-6, training epochs set to 10, and dropout layer, the rate set to 0.1, to avoid overfitting during fine-tuning.</p><p>When distilling the model, the temperature coefficient 𝑇𝑇 is set to 4, and weight 𝛼𝛼 is set to 0.7. Other hyperparameters are set as shown in Table <ref type="table" coords="4,261.83,401.39,4.14,10.70" target="#tab_1">2</ref>. We train and distill the model on an A800 80GB GPU, and test it on a virtual machine with 4 CPUs and 40GB of memory. The deep learning framework we use is Pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The models are tested on TIRA <ref type="bibr" coords="4,223.21,596.41,12.92,10.70" target="#b8">[9]</ref> and evaluated on F1-score for three tasks respectively. The results on the three tasks of validation dataset and test dataset respectively are shown in the table below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The final scores of our model on the three tasks of validation dataset and test dataset respectively Dataset task1 task2 task3 validation dataset 0.9691 0.8003 0.7867 test dataset 0.9678 0.8057 0.7900</p><p>From the results in the table 3, we can infer that our methods have a certain degree of generalization, since the model that performs well on the validation dataset can also have similar performance on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation experiments</head><p>To demonstrate the validity of a larger model, we trained a Bert-base model with 110M parameters as our baseline. Table <ref type="table" coords="5,173.89,118.57,5.52,10.70">3</ref> shows the performance of a BERT-base <ref type="bibr" coords="5,366.00,118.57,18.32,10.70" target="#b9">[10]</ref> model distilled from a BERTlarge model and an mT0-large model distilled from an mT0-xl model. For this part, we separate 20% of data from training data to monitor the training progress and test these two models on validation data. The results prove that the model with more parameters can indeed perform better than the small model, although the gap is not large, it may also be because the large model takes longer to fit, and in the experiment, we only fine-tuned 20 epochs.</p><p>Then we put 20% of the training data back into the training set to train the model and re-compare the performance. The results are shown in Table <ref type="table" coords="5,287.39,305.64,4.14,10.70" target="#tab_3">5</ref>. From the results in the table, we can observe that when a small amount of training data is added, the performance of the model does not necessarily improve. But when we add a large amount of training data to the model, the model can gain some knowledge from it to improve performance on certain tasks. So we can guess that enhancing the data or introducing more in-domain data can add more positive effects to the training of the model.</p><p>In addition, since the mT0-xl model is relatively large compared to popular pre-trained models such as BERT-base, which only has 110M parameters, we tried many hyperparameters before selecting the final parameter settings. We mainly test hyperparameters on the data set of task 1. The performance corresponding to hyperparameters is shown in the following table. For the mT0-xl with great amounts of parameters, choosing a conventional learning rate of 3e-4 may make it difficult to find a better global optimal point, and choosing a smaller learning rate to fit more for more epochs can expect to get better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,72.00,103.09,451.00,197.20"><head></head><label></label><figDesc></figDesc><graphic coords="2,72.00,103.09,451.00,197.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,75.46,444.98,81.82"><head>Table 1</head><label>1</label><figDesc>Dataset size of three tasks.</figDesc><table coords="4,85.92,104.50,431.06,52.78"><row><cell>task</cell><cell>train</cell><cell>easy validation</cell><cell cols="2">medium train validation</cell><cell>train</cell><cell>hard validation</cell></row><row><cell>num of documents</cell><cell>4,200</cell><cell>900</cell><cell>4,200</cell><cell>900</cell><cell>4,200</cell><cell>900</cell></row><row><cell>num of text pairs</cell><cell>12,904</cell><cell>2,828</cell><cell>28,216</cell><cell>7,042</cell><cell>19,113</cell><cell>4,112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,71.97,427.28,437.66,67.93"><head>Table 2</head><label>2</label><figDesc>Hyperparameters of model training.</figDesc><table coords="4,95.04,456.46,414.60,38.74"><row><cell>procedure</cell><cell>learning rate</cell><cell>dropout rate</cell><cell>batch size</cell><cell>epochs</cell></row><row><cell>teacher finetuning</cell><cell>1e-6</cell><cell>0.1</cell><cell>16</cell><cell>20</cell></row><row><cell>distillation</cell><cell>3e-4</cell><cell>0.1</cell><cell>16</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,71.97,169.77,410.15,68.32"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="5,71.97,183.21,410.15,54.89"><row><cell cols="2">F1 score comparison on validation data.</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>task1</cell><cell>task2</cell><cell>task3</cell></row><row><cell>BERT-base distilled</cell><cell>0.9645</cell><cell>0.7957</cell><cell>0.7805</cell></row><row><cell>mT0-large distilled</cell><cell>0.9649</cell><cell>0.8029</cell><cell>0.7696</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,71.97,331.41,451.17,95.21"><head>Table 5</head><label>5</label><figDesc>F1 score comparison for mT0-large distilled models with an 80% training set, with the complete training set or with the both complete train data and external data.</figDesc><table coords="5,87.60,374.02,394.52,52.59"><row><cell>model</cell><cell>task1</cell><cell>task2</cell><cell>task3</cell></row><row><cell>80% train data</cell><cell>0.9649</cell><cell>0.8029</cell><cell>0.7696</cell></row><row><cell>all train data</cell><cell>0.9650</cell><cell>0.7939</cell><cell>0.7675</cell></row><row><cell>with external data</cell><cell>0.9691</cell><cell>0.8003</cell><cell>0.7867</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,71.97,570.55,451.15,108.66"><head>Table 6</head><label>6</label><figDesc>F1 score of student model with different hyperparameter sets for both teacher model and student model.</figDesc><table coords="5,94.92,613.18,401.59,66.03"><row><cell>batch size</cell><cell>dropout rate</cell><cell>epochs</cell><cell>teacher lr</cell><cell>task1 F1</cell></row><row><cell>8</cell><cell>0.1</cell><cell>10</cell><cell>2e-4</cell><cell>0.9592</cell></row><row><cell>16</cell><cell>0.2</cell><cell>10</cell><cell>2e-4</cell><cell>0.9641</cell></row><row><cell>16</cell><cell>0.1</cell><cell>10</cell><cell>3e-4</cell><cell>0.9688</cell></row><row><cell>16</cell><cell>0.1</cell><cell>20</cell><cell>1e-6</cell><cell>0.9691</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="2,86.16,585.37,436.87,10.70;2,72.00,598.09,451.24,10.70;2,72.00,610.69,451.09,10.70;2,72.00,623.40,330.72,10.70;2,86.16,636.00,436.92,10.70;2,72.00,648.60,451.06,10.70;2,72.00,661.32,451.12,10.70;2,72.00,673.91,451.10,10.70;2,72.00,686.63,344.40,10.70;2,86.16,699.23,436.78,10.70;2,72.00,711.95,451.06,10.70;2,72.00,724.54,253.55,10.70"><p>The teacher model consists of an encoder based on the Transformer architecture and an MLP, where the encoder converts text into a high-dimensional text representation, and the MLP performs binary classification based on the text representation. The encoder part uses the encoder of the pre-trained language model, so it only needs to be fine-tuned on the downstream tasks.We train the teacher model using conventional fine-tuning methods on pre-trained language models. In addition to training the teacher model with the dataset specific to style change detection, we also use an additional in-domain dataset as a supplement, expecting the teacher model to learn more relevant writing style information. But this external data set is only used for the training of the teacher model, not for that of the student model, to avoid interference with the student model. Specifically, we unify the formats of different datasets into the form of text pairs and labels. During training, the dataset is randomly shuffled to ensure an even distribution of different data, and the parameters of both the encoder and MLP will be updated.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions</head><p>In this paper, we proposed a method based on an encoded classifier using knowledge distillation for writing style change detection. We first train a large model as the teacher model on datasets of all three tasks and external datasets, then distill the teacher model into a smaller student model. With this approach, we obtain a student model that can run with fewer resources and possesses capabilities close to that of the teacher model. In the follow-up wortion tunning skills to leverage the emergence ability of LLM for SCD tasks.</p></div>
<div><head n="6.">Acknowledgments</head><p>This work is supported by the <rs type="funder">Natural Science Foundation of Guangdong Province, China</rs> (No.<rs type="grantNumber">2022A1515011544</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_W7rbrup">
					<idno type="grant-number">2022A1515011544</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,108.00,322.69,415.15,10.70;6,107.96,335.29,415.07,10.70;6,107.96,347.88,415.19,10.70;6,107.96,360.60,233.76,10.70" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,364.40,322.69,158.74,10.70;6,107.96,335.29,415.07,10.70;6,107.96,347.88,77.53,10.70">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,426.56,347.88,96.59,10.70;6,107.96,360.60,47.74,10.70">CLEF 2023 Labs and Workshops</title>
		<title level="s" coord="6,163.64,360.60,104.20,10.70">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.96,373.20,415.04,10.70;6,107.94,385.92,414.98,10.70;6,107.96,398.51,276.60,10.70" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,322.38,373.20,200.62,10.70;6,107.94,385.92,96.89,10.70">Overview of the Multi-Author Writing Style Analysis Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,471.68,385.92,51.24,10.70;6,107.96,398.51,90.47,10.70">CLEF 2023 Labs and Workshops</title>
		<title level="s" coord="6,206.48,398.51,104.16,10.70">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.95,411.23,415.10,10.70;6,107.95,423.83,415.04,10.70;6,107.95,436.43,415.08,10.70;6,107.95,449.14,48.68,10.70" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,361.04,411.23,162.01,10.70;6,107.95,423.83,241.19,10.70">Mixed Style Feature Representation and B-maximal Clustering for Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Castro-Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Rodríguez-Lozada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,213.68,436.43,149.76,10.70">CLEF 2020 Labs and Workshops</title>
		<title level="s" coord="6,372.82,436.43,74.85,10.70">Notebook Papers</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.94,461.74,415.08,10.70;6,107.94,474.46,415.09,10.70;6,107.94,487.06,301.44,10.70" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,214.49,461.74,308.53,10.70;6,107.94,474.46,96.41,10.70">Style Change Detection on Real-World Data using LSTM-powered Attribution Algorithm</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Löfflad</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,496.13,474.46,26.90,10.70;6,107.94,487.06,115.19,10.70">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="6,231.06,487.06,104.36,10.70">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.93,499.65,415.07,10.70;6,107.93,512.37,415.01,10.70;6,107.93,524.97,415.08,10.70;6,107.93,537.68,24.84,10.70" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,466.26,499.65,56.74,10.70;6,107.93,512.37,233.26,10.70">Using Single BERT For Three Tasks Of Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,218.32,524.97,143.98,10.70">CLEF 2021 Labs andWorkshops</title>
		<title level="s" coord="6,371.07,524.97,105.52,10.70">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.92,550.28,415.08,10.70;6,107.92,562.88,415.04,10.70;6,107.92,575.60,64.32,10.70" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,226.72,550.28,296.28,10.70;6,107.92,562.88,101.49,10.70">Style Change Detection: Method Based On Pre-trained Model And Similarity Recognition</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,236.68,562.88,155.98,10.70">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="6,403.48,562.88,112.89,10.70">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.91,588.19,415.07,10.70;6,107.90,600.91,156.00,10.70" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,287.06,588.19,190.53,10.70">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531.2015</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,484.47,588.19,38.50,10.70;6,107.90,600.91,37.87,10.70">Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.90,613.51,415.03,10.70;6,107.90,626.10,313.21,10.70" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01786.2022</idno>
		<title level="m" coord="6,467.29,613.51,55.63,10.70;6,107.90,626.10,195.34,10.70">Crosslingual Generalization through Multitask Finetuning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.90,638.82,415.12,10.70;6,107.91,651.42,415.06,10.70;6,107.90,664.14,415.07,10.70;6,107.88,676.73,24.84,10.70" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,309.16,638.82,213.85,10.70;6,107.91,651.42,72.42,10.70">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frobe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,411.25,651.42,111.72,10.70;6,107.90,664.14,245.53,10.70">Advances in Information Retrieval. 45th European Conference on {IR} Research</title>
		<title level="s" coord="6,361.45,664.14,156.90,10.70">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,143.88,689.33,378.99,10.70;6,107.85,702.05,415.11,10.70;6,107.85,714.64,415.04,10.70;6,107.85,727.36,151.33,10.70" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,366.46,689.33,156.41,10.70;6,107.85,702.05,181.61,10.70">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,298.87,702.05,224.09,10.70;6,107.85,714.64,415.04,10.70;6,107.85,727.36,57.38,10.70">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
