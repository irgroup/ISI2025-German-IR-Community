<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.47,430.94,16.66;1,72.00,98.24,249.92,16.66;1,72.00,116.51,148.83,10.64;3,362.69,335.11,63.41,8.43;3,360.91,426.63,66.93,8.43">A Writing Style Embedding Based on Contrastive Learning for Multi-Author Writing Style Analysis Notebook for PAN at CLEF 2023</title>
				<funder ref="#_c4ryn4b">
					<orgName type="full">National Social Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,142.06,68.98,11.63"><forename type="first">Haoyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,149.64,142.06,77.64,11.63"><forename type="first">Zhongyuan</forename><surname>Han</surname></persName>
							<email>hanzhongyuan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.30,142.06,54.12,11.63"><forename type="first">Zengyao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.98,142.06,49.62,11.63"><forename type="first">Yong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.47,430.94,16.66;1,72.00,98.24,249.92,16.66;1,72.00,116.51,148.83,10.64;3,362.69,335.11,63.41,8.43;3,360.91,426.63,66.93,8.43">A Writing Style Embedding Based on Contrastive Learning for Multi-Author Writing Style Analysis Notebook for PAN at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0448343C80E3F2BC462CDD0F6F5BEA9B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Style Change Detection</term>
					<term>Contrastive Learning</term>
					<term>Sentence Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Writing style change detection aims to identify the text position where the author switches in multi-author documents for further author identification. This paper introduces our experiment on the shared task of PAN 23. We apply the method of comparative learning in the analysis of writing style and optimize the sentence segment embedding output by the encoder of the pretraining model so that the encoder can obtain more similar vectors in space when processing sentences with similar styles, and expand the distance between the embedding representation of paragraphs with different styles. We use the optimized encoder to generate sentence embeddings by analyzing the tag data combined with paragraph sample pairs and classifying them through a full connection layer. Through experiments, we obtained F1-scores of 0.9145, 0.8203, and 0.6755 on Task 1, Task 2, and Task 3 of the official test set, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The writing style analysis task aims to identify the position where the author's identity changes in a given multi-author document. By analyzing the author's writing style, it can help identify authorship, verify whether the document has been tampered with, whether the article is suspected of plagiarism, etc. In recent years, PAN has organized a series of tasks to detect writing style changes in the text, and conducted writing style analysis in such sub-areas as the number of authors, paragraphs with style changes, and sentence level style change detection. In PAN 2023 <ref type="bibr" coords="1,366.01,485.52,11.92,10.64" target="#b0">[1,</ref><ref type="bibr" coords="1,381.19,485.52,7.95,10.64" target="#b1">2]</ref>, special attention was paid to the analysis of writing style under the condition of limited topic diversity, which made more attention to the article's writing style rather than the article's topic information as a signal of style change.</p><p>Since Google proposed the BERT <ref type="bibr" coords="1,247.51,523.44,12.83,10.64" target="#b2">[3]</ref> model in 2018, as a feature extraction tool from text to embedding, BERT has become increasingly popular in the NLP field. In a previous style change detection task, Zhang et al. <ref type="bibr" coords="1,192.41,548.76,12.83,10.64" target="#b3">[4]</ref> used pre-trained BERT, and Lin et al. <ref type="bibr" coords="1,374.92,548.76,12.83,10.64" target="#b4">[5]</ref> obtained the best results in the last year based on three BERT-like models using ensemble learning. However, the sentence representation directly derived from BERT is often constrained in a small area, showing high similarity, which is called "Collapse" <ref type="bibr" coords="1,188.84,586.69,11.68,10.64" target="#b5">[6]</ref>, so it is difficult to be directly used for text semantic matching. Therefore, the concept of Contrastive Learning (CL) is proposed. By using the method of comparison in loss calculation, the distance between similar sentences in the vector space is closer, and different sentences are alienated. The goal is to learn a better semantic representation space from the samples. The critical point in CL is the construction of samples. Gao et al. <ref type="bibr" coords="1,310.17,637.33,12.83,10.64" target="#b6">[7]</ref> proposed SimCSE, an unsupervised solution to generate similar samples using dropout quickly, and took other sentences in the same batch as counterexamples. For supervised learning, they use the dataset (𝑥𝑥 𝑖𝑖 , 𝑥𝑥 𝑖𝑖 + , 𝑥𝑥 𝑖𝑖 -) (where 𝑥𝑥 𝑖𝑖 is the premise, 𝑥𝑥 𝑖𝑖 + and 𝑥𝑥 𝑖𝑖 -are entailment and contradiction hypotheses) to build the corresponding model, and achieved good results. However, it is challenging to make triplet data on some tasks, so Su <ref type="bibr" coords="2,467.53,74.99,12.77,10.64" target="#b7">[8]</ref> proposed CoSENT (Cosine Sentence). This simpler but more powerful supervised CL solution enables the encoder to learn better sentence embedding representation by comparing the cosine distance between sample pairs. In this paper, CoSENT will be used to finetune an encoder to output a closer embedding in the same author's article and calculate the final label through an MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Datasets</head><p>PAN 23 provides tasks with three difficulty levels. It is necessary to find the position of all writing style changes at the paragraph level on a given text (i.e., for two consecutive paragraphs, evaluate whether there are style changes). The difficult difference of tasks lies in the diversity of document topics:</p><p> Easy: The paragraphs of the document cover a variety of topics, allowing the use of topic information to detect whether the author's identity has changed.  Medium: The theme in the document changes little (but still exists), forcing more attention to the style to effectively solve the detection task.  Hard: All paragraphs in the document are on the same topic. The dataset of this task provided by PAN 23 comes from Reddit and provides the user posts and their replies of each sub-section. Each dataset is divided into three parts: training set and verification set including ground truth data, and test set without ground truth data for evaluation. Table <ref type="table" coords="2,476.83,311.40,5.49,10.64" target="#tab_0">1</ref> provides statistics on the number of original datasets. All documents are provided in English and may contain any number of style changes. However, style changes may only occur between paragraphs (i.e., a single paragraph is always authored by a single author and does not contain style changes). Each input problem is referenced by an ID (i.e., the document that detects style changes), which is then used to identify the solution submitted for the input problem. The ground truth data includes the number of authors and the binary labels of each pair of consecutive paragraphs (1 for style changes, otherwise 0), but does not provide specific paragraph author information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this paper, we use CoSENT as a comparative learning method to train an encoder, to make the sentence embedding encoded by the paragraphs of the same author closer in space, while the sentence embedding of the paragraphs written by different authors is farther. After the encoder training, we connect a full connection layer classifier to generate the corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Pre-processing</head><p>First, the dataset needs to be converted into positive or negative instances of paragraph pairs (𝑃𝑃 𝑖𝑖 , 𝑃𝑃 𝑗𝑗 , 𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙) to continue the subsequent comparative learning training. In the official dataset, only the binary labels and the number of authors of each pair of consecutive paragraphs are provided, which means that we cannot directly determine the author information of each paragraph. Therefore, if we directly convert the samples based on the official dataset, we will get a training set with fewer samples. If all the two passages with unchanged authorship are considered as positive instances and the other passages as negative instances, the following situation may occur: Passages that are far away but by the same author, are incorrectly marked as negative instances, which will confuse the model. (In particular, when the number of authors is equal to the number of style changes in the document + 1, the author information of each paragraph can be uniquely determined, which can be proved by the pigeonhole principle in combinatorics, and we call it author information transparency.)</p><p>In this case, to avoid the above-mentioned problems and to make better use of the available data, we propose a method to generate positive and negative instances:</p><p> First, divide the paragraphs whose style has not changed into the same group, based on the labels.  If the number of paragraphs in a group is greater than one, combine each of them in two to obtain a positive instance.  Two-by-two combinations of negative instances between two adjacent but different groups.</p><p>In this way, we can obtain a large number of high-quality paragraph pairs and ensure that the resulting positive and negative instances are always correct, although some possible negative instances may be lost. We only apply the above strategy to the training set. Only positive and negative instances pairs transformed from the original labels are used for the validation set. Table <ref type="table" coords="3,114.00,506.76,5.49,10.64" target="#tab_1">2</ref> shows the number of paragraph pairs after dataset processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder Training</head><p>In encoder training, the positive and negative instances pairs in a batch will be sent to the encoder for encoding, and the similarity between the instances pairs will be calculated by cosine distance. We hope that the similarity of positive instances pairs is greater than that of negative instances, that is, for any positive instances pair (𝑖𝑖, 𝑗𝑗) ∈ Ω 𝑝𝑝𝑝𝑝𝑝𝑝 and negative instances pair (𝑘𝑘, 𝑙𝑙) ∈ Ω 𝑛𝑛𝑛𝑛𝑛𝑛 , there are:</p><formula xml:id="formula_0" coords="3,224.11,736.86,288.25,12.94">cos�𝑢𝑢 𝑖𝑖 , 𝑢𝑢 𝑗𝑗 � &gt; cos(𝑢𝑢 𝑘𝑘 , 𝑢𝑢 𝑙𝑙 )<label>(1)</label></formula><p>Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Maecenas por�tor congue massa. Fusce posuere, magna sed pulvinar ultricies.</p><p>Purus lectus malesuada libero, sit amet commodo magna eros quis urna. Fusce nibh magna, venena�s id lacus ac, pulvinar sagi�s eros.</p><p>Ut ac velit non tortor auctor �ncidunt in gravida erat. Duis in massa eget nisl cursus pulvinar. Morbi tempus augue quis turpis fringilla mollis.</p><p>Proin vitae elit dignissim, posuere augue eget, blandit nulla. Nullam non commodo ipsum, in laoreet justo.</p><p>Ves�bulum pulvinar vulputate purus, ac posuere leo pulvinar sit amet. E�am blandit maximus nulla, vel volutpat nisl. Sed eu feugiat tortor.</p><p>Maecenas rhoncus nisi nibh, non ultrices ex pre�um vel. Quisque in por�tor sem. Aliquam a posuere risus, et ma�s massa. Fusce accumsan dui ut erat suscipit sagi�s. Where 𝑢𝑢 𝑥𝑥 represents the embedding representation of the paragraph 𝑥𝑥. The work of Su et al. <ref type="bibr" coords="4,498.96,75.17,11.92,10.64" target="#b7">[8,</ref><ref type="bibr" coords="4,513.90,75.17,9.11,10.64" target="#b8">9]</ref> and Sun et al. <ref type="bibr" coords="4,134.63,87.89,18.36,10.64" target="#b9">[10]</ref> suggests an effective solution to such problems, so we get the equation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Para1</head><formula xml:id="formula_1" coords="4,153.18,118.82,359.19,27.87">ℒ = log �1 + � 𝑙𝑙 𝜆𝜆�cos(𝑢𝑢 𝑘𝑘 ,𝑢𝑢 𝑙𝑙 )-cos�𝑢𝑢 𝑖𝑖 ,𝑢𝑢 𝑗𝑗 �� (𝑖𝑖,𝑗𝑗)∈Ω 𝑝𝑝𝑝𝑝𝑝𝑝 ,(𝑘𝑘,𝑙𝑙)∈Ω 𝑛𝑛𝑛𝑛𝑛𝑛 � (2)</formula><p>Where 𝜆𝜆 &gt; 0 is a hyperparameter, which is taken as 20 in this experiment. The above equation is used to optimize the encoder, and the cosine distance of the encoder output instances is evaluated for correlation with the labels using the spearman metric, which assesses how well the relationship between two variables can be described using a monotonic function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classifier Training</head><p>After completing the encoder training, we freeze the parameters of the encoder, then encode the instances of the paragraph pairs to be predicted, take out the last layer of the model's [CLS] vector as the embedding representation of the paragraph (𝑢𝑢, 𝑣𝑣), subtract the two matrices and take the absolute value, and splice them with the original one into a feature matrix (𝑢𝑢, 𝑣𝑣, |𝑢𝑢 -𝑣𝑣|), and feed it into a tanhactivated linear layer for classification. The prediction results are optimized using cross-entropy loss and evaluated by F1-scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the actual experiment 1 , we choose the DeBERTa BASE <ref type="bibr" coords="4,342.06,655.73,18.29,10.64" target="#b10">[11]</ref> as our pretrain encoder model. Our hyperparameters are set as follows: For the encoder, the batch size is set to 24, the maximum sequence length is 512, and the excess will be truncated. The initial learning rate is set to 1e-5, and trained in 20 epochs; For classifiers, the batch size is set to 64, the initial learning rate is set to 5e-5, and trained in 10 epochs. AdamW was used to optimize each training, and a warmup rate of 0.1 was set.</p><p>The spearman score of the encoder model and the metrics finally obtained by the classifier can be found in Table <ref type="table" coords="4,139.48,731.64,4.13,10.64" target="#tab_2">3</ref>. 1 Our source code is available at https://github.com/icyray/CL-MAWSA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We finally submitted the model to TIRA <ref type="bibr" coords="5,266.40,234.47,18.36,10.64" target="#b11">[12]</ref> to run and obtain the final metrics of the model. Table <ref type="table" coords="5,72.01,247.13,5.49,10.64" target="#tab_2">3</ref> provides the scores obtained by our model in the official test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper briefly describes the results of our team's work on the PAN 2023 shared task. We used a CoSENT-based contrastive learning method to finetune the encoder and a linear classifier to obtain the final results. The experimental results show that contrastive learning has promising applications in this kind of tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,467.01,450.98,10.77;3,72.00,480.99,49.23,10.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dataset processing. Positive and negative instances are generated based on the division of the group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.00,450.20,450.95,10.77;4,72.01,463.64,450.90,10.37"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Model description. The figure on the left describes the encoder training conducted by CoSENT base on contrastive learning, and the figure on the right shows the model's overall structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.02,349.95,442.54,83.01"><head>Table 1</head><label>1</label><figDesc>Statistics of the original dataset</figDesc><table coords="2,82.92,379.04,431.64,53.93"><row><cell>Datasets</cell><cell>Dataset 1 #documents</cell><cell>#para.</cell><cell>Dataset 2 #documents</cell><cell>#para.</cell><cell>Dataset 3 #documents</cell><cell>#para.</cell></row><row><cell>Training set</cell><cell>4200</cell><cell>17104</cell><cell>4200</cell><cell>32416</cell><cell>4200</cell><cell>23313</cell></row><row><cell>Validation set</cell><cell>900</cell><cell>3730</cell><cell>900</cell><cell>7942</cell><cell>900</cell><cell>5012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.00,532.65,443.20,82.96"><head>Table 2</head><label>2</label><figDesc>Statistics of the processed dataset</figDesc><table coords="3,91.02,561.74,424.18,53.87"><row><cell>Datasets</cell><cell>Dataset 1 #pos.</cell><cell>#neg.</cell><cell>Dataset 2 #pos.</cell><cell>#neg.</cell><cell>Dataset 3 #pos.</cell><cell>#neg.</cell></row><row><cell>Training set</cell><cell>2459</cell><cell>13304</cell><cell>46029</cell><cell>39125</cell><cell>15712</cell><cell>24703</cell></row><row><cell>Validation set</cell><cell>377</cell><cell>2451</cell><cell>4013</cell><cell>3029</cell><cell>2159</cell><cell>1953</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,75.56,397.00,96.40"><head>Table 3</head><label>3</label><figDesc>Metrics on the validation set</figDesc><table coords="5,125.52,104.66,343.48,67.31"><row><cell>Datasets</cell><cell>Encoder spearman</cell><cell>Classifier F1-scores</cell></row><row><cell>Dataset 1</cell><cell>0.5648</cell><cell>0.9054</cell></row><row><cell>Dataset 2</cell><cell>0.6941</cell><cell>0.8177</cell></row><row><cell>Dataset 3</cell><cell>0.4715</cell><cell>0.7038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.01,272.96,414.63,54.41"><head>Table 3</head><label>3</label><figDesc>Metric on the test set</figDesc><table coords="5,95.64,302.06,391.00,25.31"><row><cell>Tasks</cell><cell>Task 1</cell><cell>Task 2</cell><cell>Task 3</cell></row><row><cell>F1-scores on Test set</cell><cell>0.9145</cell><cell>0.8203</cell><cell>0.6755</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>This work is supported by the <rs type="funder">National Social Science Foundation of China</rs> (No. <rs type="grantNumber">22BTQ101</rs>).</p></div>
<div><head n="8.">References</head></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_c4ryn4b">
					<idno type="grant-number">22BTQ101</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,93.30,544.07,429.64,10.64;5,93.30,556.73,429.68,10.64;5,93.30,569.34,429.79,10.64;5,93.30,582.00,429.70,10.64;5,93.30,594.66,230.70,10.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,359.40,544.07,163.54,10.64;5,93.30,556.73,429.68,10.64;5,93.30,569.34,78.06,10.64">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Ríos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,195.74,569.34,323.15,10.64;5,93.30,582.00,397.79,10.64">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="5,93.30,594.66,155.11,10.64">Lecture Notes in Computer Science</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multi-linguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="5,93.30,607.32,429.67,10.64;5,93.30,619.98,429.67,10.64;5,93.30,632.64,114.60,10.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,285.84,607.32,237.13,10.64;5,93.30,619.98,58.45,10.64">Overview of the Multi-Author Writing Style Analysis Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,198.98,619.98,323.99,10.64;5,93.30,632.64,26.73,10.64">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.31,645.24,429.67,10.64;5,93.32,657.90,340.68,10.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,273.38,645.24,249.59,10.64;5,93.32,657.90,102.94,10.64">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,203.68,657.90,153.84,10.64">Proceedings of NAACL-HLT 2019</title>
		<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.33,670.56,429.67,10.64;5,93.35,683.22,429.65,10.64;5,93.35,695.88,330.84,10.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,248.67,670.56,274.33,10.64;5,93.35,683.22,148.19,10.64">Style Change Detection Based On Writing Style Similarity-Notebook for PAN at CLEF 2021</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,93.35,695.88,144.72,10.64">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="5,246.11,695.88,104.10,10.64">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.36,708.49,429.75,10.64;5,93.36,721.15,429.65,10.64;5,93.36,733.81,24.76,10.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,284.57,708.49,238.53,10.64;5,93.36,721.15,103.04,10.64">Ensemble Pre-trained Transformer Models for Writing Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">T.-M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-W</forename><surname>Tzeng</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,218.49,721.15,145.46,10.64">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="5,372.19,721.15,104.45,10.64">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.36,746.47,429.69,10.64;5,93.36,759.13,429.79,10.64;6,93.30,74.99,429.71,10.64;6,93.30,87.65,262.42,10.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,239.58,746.47,283.47,10.64;5,93.36,759.13,100.18,10.64">Consert: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,220.44,759.13,302.71,10.64;6,93.30,74.99,429.71,10.64;6,93.30,87.65,47.62,10.64">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="6,93.30,100.26,429.69,10.64;6,93.30,112.92,429.63,10.64;6,93.30,125.58,264.65,10.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,211.05,100.26,267.02,10.64">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,93.30,112.92,429.63,10.64;6,93.30,125.58,162.06,10.64">Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.30,138.24,429.71,10.64;6,93.30,150.90,177.55,10.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,122.88,138.24,365.90,10.64">CoSENT (I): A more efficient sentence embedding scheme than Sentence-BERT</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<ptr target="https://spaces.ac.cn/archives/8847" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.30,163.56,429.73,10.64;6,93.30,177.00,110.64,10.64" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Murtadha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.02955</idno>
		<title level="m" coord="6,247.56,163.56,206.02,10.64">Zlpr: A novel loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,93.30,190.44,432.52,10.64;6,93.30,203.09,429.67,10.64;6,93.30,215.75,179.68,10.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,245.58,190.44,275.74,10.64">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,135.98,203.09,353.14,10.64">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="6397" to="6406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.29,228.41,429.77,10.64;6,93.28,241.02,269.03,10.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,227.98,228.41,275.30,10.64">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,93.28,241.02,236.93,10.64">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.28,253.68,429.64,10.64;6,93.27,266.34,429.73,10.64;6,93.27,279.00,429.67,10.64;6,93.27,291.66,113.39,10.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,304.16,253.68,218.75,10.64;6,93.27,266.34,88.50,10.64">Continuous Integration for Reproducible Shared Tasks with TIRA.io</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,206.91,266.34,316.08,10.64;6,93.27,279.00,101.03,10.64">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="6,203.20,279.00,160.69,10.64">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
