<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,342.96,15.42;1,89.29,106.66,378.60,15.42;1,89.29,129.00,157.29,11.96">UZH at PAN-2023: Profiling Cryptocurrency Influencers using Ensemble of Language Models Notebook for PAN at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,154.90,75.12,11.96"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
							<email>abhinav.kumar2@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.05,154.90,47.32,11.96"><forename type="first">Le</forename><surname>Hoang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.36,154.90,56.67,11.96"><forename type="first">Minh</forename><surname>Trinh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.47,154.90,98.89,11.96"><forename type="first">Afshan</forename><forename type="middle">Anam</forename><surname>Saeed</surname></persName>
							<email>afshananam.saeed@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,342.96,15.42;1,89.29,106.66,378.60,15.42;1,89.29,129.00,157.29,11.96">UZH at PAN-2023: Profiling Cryptocurrency Influencers using Ensemble of Language Models Notebook for PAN at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">965DA89E69BF29CF74E87474627E3634</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Few Shot Learning</term>
					<term>Transformers</term>
					<term>Fine Tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the era of social media, the impact of social content can be massive, especially when cryptocurrency investors rely on peer advises. The crypto market has been shown to be volatile, and certain social media users have a stronger influence in the market through their social media posts than others. In this paper, we aim to classify the social media based cryptocurrency influencers into categories depicting their influence in the crypto market based on their English tweets. The task is performed under low resource setting due to limited data availability, and is done using fine tuning approaches for few shot learning. We fine tune various large language models including Bert, Roberta and Electra. We additionally fine tune a combination of models to create ensemble models to take advantages of multiple pre-trained models for our classification task. We find that the ensemble models provide better test accuracy than the single models for few shot learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cryptocurrencies, characterized by their decentralized and digital nature, have garnered immense attention from investors seeking lucrative opportunities. However, the volatile and often chaotic nature of cryptocurrency markets poses challenges for investors looking to make informed investment decisions. Unfortunately, traditional forecasting approaches that rely solely on numerical historical data are often insufficient in capturing the complex dynamics and sentiment surrounding cryptocurrencies <ref type="bibr" coords="1,291.27,500.13,11.43,10.91" target="#b0">[1]</ref>.</p><p>In recent years, social media platforms have emerged as influential spaces where discussions and opinions about cryptocurrencies flourish. Tweets and online conversations about various cryptocurrencies have the power to shape public perception and drive market sentiment. This phenomenon is particularly evident during significant events, such as major price fluctuations or the introduction of new cryptocurrencies. As a result, conventional forecasting methods that overlook the impact of social media sentiment fail to capture crucial information <ref type="bibr" coords="1,451.10,581.42,11.43,10.91" target="#b1">[2]</ref>.</p><p>Behavioral finance theories suggest that individuals are prone to making decisions based on biases and herd behavior when faced with uncertainty. In the context of cryptocurrencies, the abundance of tweets and social media discussions creates an environment where investors are susceptible to the influence of others. The so-called "hype" surrounding certain cryptocurrencies, often fueled by social media activity, can trigger herd behavior and impact investment decisions <ref type="bibr" coords="2,89.29,141.16,11.33,10.91" target="#b0">[1]</ref>. Studies have shown that cryptocurrencies valued and endorsed by experts tend to be more successful, underscoring the role of social media in shaping investor sentiment.</p><p>In volatile environments, where rapid price fluctuations and market uncertainty prevail, social media platforms become even more important for investors. Users seek advice, opinions, and insights from other social media users to gain a better understanding of market trends and potential investment opportunities. The real-time nature of social media, coupled with the ability to connect with a large community of investors, makes it an attractive source of information and a platform for sharing experiences. However, caution must be exercised, as the reliability and expertise of social media users can vary significantly.</p><p>In this work, we aim to categorize users into an influencer profile based on their English tweets using Transformer Encoder models. Since availability of social media tweets specific to Cryptocurrency influencers is limited, the goal of this project is to use this limited amount of data to create a model that is able to profile users based on their influence in the crypto market. We will begin by introducing Few-Shot Learning, Ensemble Modeling techniques, and the dataset. We would then discuss the methodology used, which includes details about the hugging face pre-trained models taken under consideration. We conclude by describing the results of our single body and two body models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">NLP in Finance</head><p>The field of finance has undergone a significant transformation in recent years, with traditional forecasting techniques being complemented by innovative approaches that leverage the power of natural language processing (NLP). By incorporating NLP into financial analysis, professionals can gain valuable insights into market trends, investor sentiment, and the forecasting of future trends. NLP techniques, such as sentiment analysis, event detection, trend analysis, and risk management, enhance decision-making processes and enable more accurate forecasting. In the present context, we will be using fine-tuning approaches of NLP in understanding the coherence within user tweets of a specific class, so as to identify the users significantly impacting the cryptocurrency market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Few Shot Learning</head><p>Obtaining large amounts of data for training large models is a challenging and expensive affair. Within the realm of natural language processing (NLP) tasks, the focus is on efficiently solving new tasks using only a small number of labeled examples. Recent advancements in self-supervised pre-training of transformer models, which employ language modeling objectives, have yielded remarkable achievements in learning general-purpose parameters applicable to diverse downstream NLP tasks. Nevertheless, despite the benefits of pre-training, these models are not optimized for fine-tuning with limited supervision. Consequently, they still require substantial amounts of task-specific data to achieve satisfactory performance.</p><p>Few-shot learning tackles the challenge of training classifiers with minimal amounts of training data. This includes the extreme case of zero-shot learning, where no labeled data is available for training purposes, and few-shot classification, where classifiers are trained with only a few labeled examples per class. Few-shot learning typically relies on representing task labels in a textual format, such as their names or concise descriptions. Its applications span various domains and are widely used in image classification, sentiment classification from short text, and object recognition <ref type="bibr" coords="3,214.20,195.36,11.43,10.91" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Ensemble Models</head><p>The instances in Few Shot Learning can depend on Fine-tuning of a single pre-trained transformer model. To leverage the collective intelligence of multiple transformer models, Ensemble Deep Learning methods exist that combine the predictions of multiple individual models to obtain enhanced and more accurate results. This method allows for more robust and reliable predictions and the diversity in the models can help compensate for the weaknesses or biases of individual models, resulting in better overall performance. Due to its multiple model strength, this method is good for handling complex tasks and is a valuable technique for tasks where accurate predictions are essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>Two dataset files are provided by the shared task organisers, namely, train_text and train_truth. The train_text dataset provided contains the following features -'Twitter User Id', 'Text' (English tweets), and 'Tweet Id'. For every Twitter user ID, the labels are mentioned in the train_truth dataset. There are five classes present in the train_truth dataset, namely, 'no influencer', 'nano', 'micro', 'macro', and 'mega'. These labels are grouped in the order of the User Id's influence on their followers, with 'no influencer' being the least influential and 'mega' being the most influential. Consequently, it depicts how much of an influence the user has in the crypto market. There are multiple tweets given for each user, ranging from 1-10 tweets per user, and 32 users given for every class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data pre-processing</head><p>In this section, we pre-process the tweets which we got for training our model. We joined the train_text and train_truth dataset into a single data frame on the 'User Ids' and removed the 'Tweet Ids' from the resulting data frame, as the identifier of each tweet was not needed for our approach.</p><p>Twitter dataset underwent the following process:</p><p>• Noise data removal • Short tweet removal 1) Noise data removal: Here noise means words or any text that doesn't add any relevance to the classification task. Based on this reasoning, we removed URLs present in the tweets as they are mostly short links or dead links. User mentions starting with '@' are also removed from the tweets.</p><p>2) Short tweet removal: Short tweets often lack sufficient information, serving as noise in datasets. Removing them enhances data quality by preserving more relevant information for processing. Some short tweets are merely retweets or emotional responses, lacking substantial content. The limited information in these tweets poses a challenge for models to discern meaningful patterns. We remove any tweet that is shorter than five words.</p><p>These features are input into various models and outputs are used to validate the models. In our experiment, we found two body ensemble model more accurate than other and in rest of the article we will focus on that. First we will talk about pre-trained models that we used and then we discuss how we combined the output of these models to get the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Problem Modeling</head><p>Our initial proposal for the given Few-Shot classification problem is to make use of standard fine-tuning pre-trained language models individually. Thus, we use pre-trained Large Language Models (LLMs) on tweets or for general text classification. Most of these models have more than 350M parameters and the evaluation was done on a subset of the dataset, which we took to be the test dataset. The models utilised are: Twitter RoBERTa Large <ref type="bibr" coords="4,402.27,353.48,11.37,10.91" target="#b3">[4]</ref>, RoBERTa Large <ref type="bibr" coords="4,492.02,353.48,11.38,10.91" target="#b4">[5]</ref>, Electra Large <ref type="bibr" coords="4,151.75,367.03,11.53,10.91" target="#b5">[6]</ref>, BERTweet Large <ref type="bibr" coords="4,247.67,367.03,12.95,10.91" target="#b6">[7]</ref> finetuned on TweetNER7 dataset <ref type="bibr" coords="4,414.16,367.03,11.44,10.91" target="#b7">[8,</ref><ref type="bibr" coords="4,428.34,367.03,7.63,10.91" target="#b8">9]</ref>. For simplicity, this last model will be referred to as 'BERTweet Large' throughout the rest of this paper.</p><p>To evaluate the fine-tuning results on the test data, we used two strategies-referred to as the Concatenation Method and the Single Tweet Classification Method-each gauged on accuracy and F1 score metrics. Additionally, to prevent overfitting, early stopping was employed.</p><p>We used two approaches for evaluating our fine-tuning results on the test data. Both these approaches were evaluated based on the accuracy and F1 score. Early stopping was added to avoid over-fitting:</p><p>Concatenation Method: For a given user, we concatenate their tweets up to a maximum token size of 512. Each model then classifies these concatenated tweets into their corresponding labels. We ensure the tweets are separated by a model-specific separation token (e.g., '[SEP]' for BERT and '&lt;/s&gt;' for RoBERTa).</p><p>Single Tweet Classification Method: This strategy involves each model classifying tweets individually, grouping them based on user id. To determine a user's label, we use majority voting among the classified tweets. In case of a tie, we extract the final label by averaging the probabilities.</p><p>From our preliminary analysis, it became apparent that the performance of an individually fine-tuned model left room for improvement. In addition, we observed that the Concatenation Method consistently outperformed the Single Tweet Classification Method. With these findings in mind, we sought to further enhance our performance and thus, we formulated a novel approach, which we refer to as the "Two Body Model".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pretrained Models Used</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TwHIN-BERT</head><p>TwHIN-BERT <ref type="bibr" coords="5,155.27,128.10,18.07,10.91" target="#b9">[10]</ref> stands for Twitter Heterogeneous Information Network BERT. It's a pretrained language model built on BERT's architecture, trained on 7 billion multilingual tweets. Enhanced with social media-specific features, it is highly effective in capturing the nuances of tweets. The dataset used for training is of high quality, having been thoroughly preprocessed to remove noisy or irrelevant tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twitter RoBERTa Large</head><p>The Twitter RoBERTa Large model <ref type="bibr" coords="5,248.72,231.63,11.51,10.91" target="#b3">[4]</ref>, is a RoBERTa based language model, specifically finetuned for Twitter data. Initially trained on a corpus of 90 million tweets, the model has since been continuously updated on a rapidly growing Twitter dataset, reaching a total of 154 million tweets by the time of our usage. This model's specialization and ongoing adaptation make it particularly effective at understanding the dynamic nature of discourse on Twitter. Its ability to stay current is especially valuable in fast-evolving fields such as cryptocurrencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DistilBERT</head><p>DistilBERT, a streamlined variant of BERT by Hugging Face <ref type="bibr" coords="5,363.72,348.70,16.41,10.91" target="#b10">[11]</ref>, has 40% fewer parameters, thus offering speed and efficiency benefits. Despite its smaller size, it retains 95% of BERT's performance on certain benchmarks. Due to its balance between performance and computational needs, DistilBERT serves as a reliable baseline for assessing larger language models, particularly in resource-constrained environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELECTRA-Small</head><p>ELECTRA-Small is a compact version of the ELECTRA model <ref type="bibr" coords="5,364.46,452.23,11.39,10.91" target="#b5">[6]</ref>. Its novel two-step approach enables efficient data use during training while its smaller size leads to less computational requirement. Despite its compactness, ELECTRA-Small maintains competitive performance, making it an excellent baseline for evaluating the efficiency of larger transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Two Body Model</head><p>In our methodology, we adopt a novel approach named the "Two Body Model". This approach involves extracting the embeddings of the CLS tokens from the last hidden layer of a pair of Transformer Encoder Models. We experimented with two sets of pairs: a pair of smaller models (Electra Small + DistilBERT) for computational efficiency, and a pair of larger models (TwHIN-BERT + Twitter RoBERTa Large), which bear the advantage of being pre-trained on Twitter data. The larger models, besides being pre-trained, also possess a more significant number of parameters. This feature allows them to capture more complex and subtle signals, leading to potentially richer and more insightful embeddings.</p><p>Once the embeddings are extracted, they are concatenated and fed into a standard Multi-Layer Perceptron that employs the Gaussian Error Linear Unit (GELU) <ref type="bibr" coords="5,401.26,664.55,17.81,10.91" target="#b11">[12]</ref> activation function.</p><p>The motivation behind GELU is to bridge stochastic regularizers, such as dropout, with nonlinearities, i.e., activation functions. The output of this model provides the classification of the level of influence of the tweet's author based on a concatenation of their tweets (using the Concatenation Method).</p><p>The primary design of this configuration is to harness the unique strengths of each component model within the pairs. The ultimate goal is to optimize performance for our specific task, which is Twitter data classification. Through this methodology, we capitalize on the specialized capabilities of the selected models, thereby ensuring that our approach is well-equipped to handle the intricacies and subtleties of Twitter discourse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Head First Fine-tuning (HeFit)</head><p>Within our experiments, we adapted a recent alternative two stage fine-tuning procedure by Head-First Fine-Tuning (HeFiT) <ref type="bibr" coords="6,232.35,258.64,17.88,10.91" target="#b12">[13]</ref> which demonstrated increase ability of Language Models to adapt to the Twitter Domain during fine-tuning.</p><p>The HeFiT approach is a two-stage process designed to gradually adapt the model to the specific task at hand. In the first stage, only the parameters of the new classification head are updated, keeping the pre-trained transformer encoder parameters frozen. This allows the new classification head to learn to make predictions using the existing representations produced by the pre-trained model. After this stage, which in our case lasted for 3 epochs, all model parameters are unfrozen and updated during training. This allows for a more nuanced adaptation of the model to the specific characteristics of the task and the data, as the encoders bodies can now adjust its representations to better suit the classification problem. An additional benefit we found with this approach was selected is signs of more stabile training and consistent performance improvements when the labeled data is scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Experimental Setup</head><p>We incorporated a gradient accumulation technique to deal with the constraints of GPU memory. This technique allows us to effectively increase the batch size without exceeding the memory capacity of our GPU. Specifically, we set the mini-batch size to 1 and used an accumulation step of 16, effectively simulating a batch size of 16. This was done to ensure that our model, which has a significant number of parameters, could be efficiently trained on a T4 GPU which has only 16 GB of memory. For updating network weights, gradient descent is used with Adam optimisation <ref type="bibr" coords="6,148.49,538.70,16.29,10.91" target="#b13">[14]</ref>. We have initialized learning rate with value of 0.000003 which get updated while training.</p><p>The entire fine-tuning process was carried out for a total of 20 epochs. As a result of this procedure, our model was not only able to leverage the power of two pre-trained language models but also effectively adapt to the specific characteristics of Twitter text data, demonstrating the efficacy of our Two Body Model approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In conclusion, we find from the accuracy and F1 scores that the fine-tuned two-body models are better models than single fine-tuned models. Amongst the single models, we find that Electra gave us significant performance as compared to the other models when using the majority voting approach to classify users based on their tweets. Amongst the two-body models, we obtained the best accuracy with the TwHIN BERT and Twitter RoBERTa large combination. While the model showed robust performance for certain categories, the performance was lower for others. These observations suggest possible avenues for further refinement of the model. Specifically, efforts should be made to improve the recall for the 'no influencer' and 'macro' categories and the precision for the 'micro' and 'mega' categories. Addressing these issues will likely lead to an improvement in the overall accuracy as well as individual precision and recall metrics for each class in future iterations of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>For the future work, we suggest using prompting methods on the fine tuned single and two body models to check for increase in accuracy scores. Manual Prompting methods did not give us good results on the pre-trained models and thus it would be interesting to see the improvements. Additionally, higher parameter models like t5 can be worked with individually or in ensembles to make a better classification model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.99,90.49,352.44,97.14"><head>Table 5</head><label>5</label><figDesc>Confusion Matrix for the two body model (TwHIN-BERT and Twitter RoBERTa Large)</figDesc><table coords="9,151.36,118.53,290.07,69.10"><row><cell></cell><cell cols="5">No Influencer Nano Micro Macro Mega</cell></row><row><cell cols="2">No Influencer 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Nano</cell><cell>0</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Micro</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Macro</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell></row><row><cell>Mega</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Our deepest appreciation goes to the <rs type="institution">Department of Computational Linguistics at the University of Zurich</rs>, whose provision of essential technical infrastructure was instrumental in our completion of the task. We are equally grateful to <rs type="person">Simon Clematide</rs> and <rs type="person">Andrianos Michail</rs> for their insightful advice and technical recommendations, which significantly enriched our work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Standard Fine-tuning</head><p>Our evaluation results from the two fine-tuning strategies-Concatenation Method and Single Tweet Classification Method-are presented in Tables <ref type="table" coords="7,328.38,145.80,5.04,10.91">1</ref> and<ref type="table" coords="7,355.22,145.80,3.71,10.91">2</ref>, respectively. From these results, it's evident that the Concatenation Method yields higher accuracy and F1 scores for the BERT and RoBERTa Models. Conversely, the Electra model demonstrates superior performance when using the Single Tweet Classification Method, achieving the highest accuracy score of 0.60 amongst all models. However, the overall accuracy and F1 scores gleaned from fine-tuning these pre-trained models are less than optimal. This may be attributed to the limited size of our labeled dataset, as well as the relative low number of parameters in these models, which can impede their capability to accurately classify labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Two-body Models</head><p>We used the Two-body Models method to accomplish the classification task to take advantage of multiple models at the same time. In our project, we worked on two combinations of models. Model 1 includes a combination of DistilBERT and ELECTRA small. Model 2 includes Twitter RoBERTa Large and TwHIN-BERT. The ensemble models underwent training for a total of 20 epochs. Evaluation of the two models on test dataset is reported in Table <ref type="table" coords="7,415.54,588.37,3.74,10.91">3</ref>.</p><p>We see that Model 2 performed better than Model 1. Upon the completion of the final epoch, Model 2 yielded a training loss of 0.242 and achieved a training accuracy of 91.7%. Further validation of the model on a separate validation set resulted in a validation loss of 1.18 and a validation accuracy of 66.7%. A detailed performance evaluation of the TwHIN BERT and Twitter RoBERTa Large was carried out by examining precision, recall, and f1-score metrics across different classes, as seen in Table <ref type="table" coords="8,127.71,203.21,3.74,10.91">4</ref>. The 'no influencer' category saw a perfect precision of 1.00, indicating that when the model predicted a 'no influencer', it was always correct. However, the model only correctly identified 33% of the actual 'no influencer' instances, suggesting room for improvement in recall. Consequently, the f1-score, which balances both precision and recall, stood at 0.50 for this category. The model demonstrated a balanced performance for the 'nano' category, achieving both precision and recall of 0.67. This resulted in an f1-score of 0.67. For the 'micro' category, the model had a lower precision of 0.60, implying that 40% of the 'micro' predictions were incorrect. Nevertheless, it showed a perfect recall of 1.00, correctly identifying all actual 'micro' instances. This resulted in a relatively high f1-score of 0.75. The 'macro' category exhibited the same pattern as the 'no influencer' class, with a precision of 1.00, but a lower recall of 0.33, leading to an f1-score of 0.50. Finally, the 'mega' category mirrored the 'micro' class performance with a precision of 0.60, a perfect recall of 1.00, and consequently, an f1-score of 0.75.</p><p>The confusion matrix (Table <ref type="table" coords="8,232.57,508.34,4.25,10.91">5</ref>) provides an in-depth look into the model's predictions. It shows that all instances of 'mega' and 'micro' categories were predicted correctly. On the contrary, in the 'macro' and 'no influencer' categories, the model correctly classified only one out of three instances, misclassifying the remaining ones. The 'nano' category saw two correct predictions, with one instance misclassified.</p><p>In the shared-task official result, Model 2 reported a Macro F1 score of 50.21. Our model was able to achieve a commendable performance, demonstrating its ability to effectively leverage the unique strengths of the TwHIN-BERT and Twitter RoBERTa Large models in a synergistic manner. This result further underscores the effectiveness of our novel approach to ensemble modeling and the potential of the HeFiT method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,642.50,394.53,10.91;9,112.33,656.05,393.65,10.91;10,112.66,86.97,395.17,10.91;10,112.66,100.52,394.62,10.91;10,112.66,114.06,221.15,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,112.33,656.05,328.60,10.91">Tweet based reach aware temporal attention network for NFT valuation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neerkaje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guhathakurta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chava</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-emnlp.471" />
	</analytic>
	<monogr>
		<title level="m" coord="9,466.96,656.05,39.02,10.91;10,112.66,86.97,395.17,10.91;10,112.66,100.52,98.10,10.91">Findings of the Association for Computational Linguistics: EMNLP 2022, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6321" to="6332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.53,10.91;10,112.66,168.26,394.04,10.91;10,112.66,181.81,286.99,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,321.03,127.61,184.96,10.91;10,112.66,141.16,140.57,10.91">FAST: Financial news and tweet based time aware network for stock trading</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.185</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.185.doi:10.18653/v1/2021.eacl-main.185" />
	</analytic>
	<monogr>
		<title level="m" coord="10,275.92,141.16,230.07,10.91;10,112.66,154.71,393.53,10.91;10,112.66,168.26,118.30,10.91">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2164" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.32,10.91;10,112.66,208.91,315.11,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D</forename><surname>La Peña Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10543</idno>
		<title level="m" coord="10,484.96,195.36,21.02,10.91;10,112.66,208.91,185.53,10.91">Zero and few-shot learning for author profiling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.32,10.91;10,112.66,236.01,259.35,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03829</idno>
		<title level="m" coord="10,412.02,222.46,93.96,10.91;10,112.66,236.01,129.84,10.91">Timelms: Diachronic language models from twitter</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,394.53,10.91;10,112.30,263.11,393.68,10.91;10,112.66,276.66,107.17,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="10,173.53,263.11,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.66,303.75,295.16,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="10,334.34,290.20,171.65,10.91;10,112.66,303.75,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,393.32,10.91;10,112.33,330.85,393.65,10.91;10,112.66,344.40,230.27,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,270.60,317.30,235.39,10.91;10,112.33,330.85,28.53,10.91">BERTweet: A pre-trained language model for English Tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.88,330.85,343.10,10.91;10,112.66,344.40,157.27,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,393.33,10.91;10,112.33,371.50,393.65,10.91;10,112.66,385.05,393.33,10.91;10,112.66,398.60,394.62,10.91;10,112.66,412.15,395.01,10.91;10,112.66,425.70,216.79,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,383.04,357.95,122.94,10.91;10,112.33,371.50,270.09,10.91">Named entity recognition in Twitter: A dataset and analysis on short-term temporal shifts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.aacl-main.25" />
	</analytic>
	<monogr>
		<title level="m" coord="10,404.73,371.50,101.25,10.91;10,112.66,385.05,393.33,10.91;10,112.66,398.60,343.97,10.91">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<title level="s" coord="10,176.24,412.15,186.51,10.91">Association for Computational Linguistics</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing<address><addrLine>Online only</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.33,10.91;10,112.66,452.79,393.53,10.91;10,112.66,466.34,393.53,10.91;10,112.66,479.89,395.01,10.91;10,112.66,493.44,254.51,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,286.18,439.25,219.80,10.91;10,112.66,452.79,108.05,10.91">An all-round python library for transformer-based named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T-Ner</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-demos.7</idno>
		<ptr target="https://aclanthology.org/2021.eacl-demos.7.doi:10.18653/v1/2021.eacl-demos.7" />
	</analytic>
	<monogr>
		<title level="m" coord="10,242.65,452.79,263.54,10.91;10,112.66,466.34,393.53,10.91;10,112.66,479.89,117.86,10.91">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,394.62,10.91;10,112.28,520.54,394.91,10.91;10,112.66,534.09,122.77,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Florez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Kishky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07562</idno>
		<title level="m" coord="10,454.07,506.99,53.21,10.91;10,112.28,520.54,390.36,10.91">Twhin-bert: A socially-enriched pre-trained language model for multilingual tweet representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,394.53,10.91;10,112.66,561.19,243.23,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="10,302.07,547.64,205.12,10.91;10,112.66,561.19,113.82,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,377.26,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<title level="m" coord="10,229.72,574.74,150.04,10.91">Gaussian error linear units (gelus)</title>
		<imprint>
			<publisher>Learning</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,395.17,10.91;10,112.66,601.84,394.53,10.91;10,112.66,615.39,173.79,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Konstantinou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01194</idno>
		<title level="m" coord="10,295.76,588.29,212.06,10.91;10,112.66,601.84,390.26,10.91">Uzh_clyp at semeval-2023 task 9: Head-first finetuning and chatgpt data generation for cross-lingual learning in tweet intimacy prediction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,628.93,395.01,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="10,227.57,628.93,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
