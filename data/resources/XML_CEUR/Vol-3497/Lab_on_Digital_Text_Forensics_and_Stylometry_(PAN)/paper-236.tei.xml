<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,387.62,15.42;1,89.29,106.66,412.19,15.42;1,89.29,128.58,57.09,15.43">Few Shot Profiling of Cryptocurrency Influencers using Natural Language Inference &amp; Large Language Models</title>
				<funder ref="#_RVVwzfr #_qbHsqjN #_AhvWDer">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_FjgkQyh #_XyyAg7J">
					<orgName type="full">Villa-Cueva</orgName>
				</funder>
				<funder>
					<orgName type="full">CONACYT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,92.34,11.96"><forename type="first">Emilio</forename><surname>Villa-Cueva</surname></persName>
							<email>emilio.villa@cimat.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">Centro de Investigación en Matemáticas AC</orgName>
								<address>
									<addrLine>Jalisco S/N, Col. Valenciana</addrLine>
									<postCode>36023</postCode>
									<settlement>Guanajuato</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.01,156.89,122.22,11.96"><forename type="first">Jorge</forename><forename type="middle">Miguel</forename><surname>Valles-Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">Centro de Investigación en Matemáticas AC</orgName>
								<address>
									<addrLine>Jalisco S/N, Col. Valenciana</addrLine>
									<postCode>36023</postCode>
									<settlement>Guanajuato</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.60,156.89,142.73,11.96"><forename type="first">Adrián</forename><surname>Pastor López-Monroy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">Centro de Investigación en Matemáticas AC</orgName>
								<address>
									<addrLine>Jalisco S/N, Col. Valenciana</addrLine>
									<postCode>36023</postCode>
									<settlement>Guanajuato</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,170.84,115.86,11.96"><forename type="first">Fernando</forename><surname>Sanchez-Vega</surname></persName>
							<email>fernando.sanchez@cimat.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Ciencias de la Computación</orgName>
								<orgName type="department" key="dep2">Centro de Investigación en Matemáticas AC</orgName>
								<address>
									<addrLine>Jalisco S/N, Col. Valenciana</addrLine>
									<postCode>36023</postCode>
									<settlement>Guanajuato</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Consejo Nacional de Ciencia y Tecnología (CONACYT)</orgName>
								<address>
									<addrLine>Av. de los Insurgentes Sur 1582</addrLine>
									<postCode>03940</postCode>
									<settlement>CDMX</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.43,170.84,117.73,11.96"><forename type="first">Roberto</forename><surname>Lopez-Santillan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facultad de Ingeniería</orgName>
								<orgName type="institution">Universidad Autónoma de Chihuahua</orgName>
								<address>
									<addrLine>Circuito Universitario Campus II</addrLine>
									<postCode>31125</postCode>
									<settlement>Chihuahua</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,387.62,15.42;1,89.29,106.66,412.19,15.42;1,89.29,128.58,57.09,15.43">Few Shot Profiling of Cryptocurrency Influencers using Natural Language Inference &amp; Large Language Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">91E2A57A1C2D30768490E3600F00ABAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pretrained language models</term>
					<term>transformers</term>
					<term>fine tuning</term>
					<term>natural language inference</term>
					<term>text classification</term>
					<term>data augmentation</term>
					<term>ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the system proposed by team NLP-CIMAT for the PAN 2023 shared task, "Profiling Cryptocurrency Influencers with Few-shot Learning." The shared task involves three classification subtasks, each featuring low-resource datasets with a limited number of examples per label. The first subtask focuses on predicting the magnitude of an influencer. The second subtask involves classifying the interest of the influencer. Lastly, the third subtask focuses on predicting the intent of the tweet, with the aim of identifying its underlying purpose or motivation. Our system exploits pre-trained language models by adapting two distinct training frameworks: traditional fine-tuning and entailment-based fine-tuning. The traditional fine-tuning approach trains a transformer encoder to predict the class of each tweet. In contrast, the entailment-based approach utilizes a model pre-trained for the NLI task and further trains it using the task data by reframing the classification problem as an entailment problem. Although the former is suitable for ample labeled data, the entailment-based approach is more effective in low-resource scenarios. We found that, in the tasks' data, entailment-based and traditional fine-tuning schemes showed outstanding performance, we propose an ensembling technique that combines the strengths of both strategies through a soft-voting approach over the traditional fine-tuning predictions and the entailment-probabilities of the entailment approach. Furthermore, we also employ a Data Augmentation strategy by prompting ChatGPT to generate another synthetic tweet for each of the tweets in the dataset. Our submitted system ranked first for the second subtask, and obtained highly competitive results in the other two. Overall, our team obtained the first place in the shared task, demonstrating the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, Natural Language Processing (NLP) has witnessed significant advancements, and the domain of social media analysis and author profiling is not the exception. This paper addresses the PAN 2023 <ref type="bibr" coords="2,198.11,138.38,12.88,10.91" target="#b0">[1]</ref> shared task, "Profiling Cryptocurrency Influencers with Few-shot Learning. " <ref type="bibr" coords="2,136.06,151.93,11.33,10.91" target="#b1">[2]</ref>, which consists of classifying cryptocurrency influencers of twitter by their level of influence, interest, and intent, based on few tweets.</p><p>Our proposed methodology builds upon the great performance of transformer based language models <ref type="bibr" coords="2,123.02,192.57,11.23,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,136.89,192.57,7.42,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,146.95,192.57,7.43,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,157.01,192.57,7.43,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,167.07,192.57,8.88,10.91" target="#b6">7]</ref> adapted to perform transfer learning, data augmentation and fine tuning of Natural Language Inference (NLI) <ref type="bibr" coords="2,241.44,206.12,11.33,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,255.50,206.12,8.94,10.91" target="#b8">9]</ref> models. The intuitive idea is that by fine-tuning a pretrained transformer-based language model on cryptocurrency-related data, we aim to capture domain-specific nuances and improve the model's performance on influencer classification. The data augmentation step helps to artificially expand the available training data to enhance model generalization and improve classification performance. We incorporate the insights from recent advancements in NLI models. Works such as <ref type="bibr" coords="2,347.34,273.87,17.97,10.91" target="#b9">[10]</ref> have showcased the efficacy of transformer-based models in classifying entailment and contradiction relationships between two sentences, a premise and an hypothesis. By leveraging this capability, we follow <ref type="bibr" coords="2,468.85,300.97,17.93,10.91" target="#b10">[11]</ref> and <ref type="bibr" coords="2,89.29,314.52,17.91,10.91" target="#b11">[12]</ref> in order to make text classification using NLI models.</p><p>To further improve the robustness and diversity of our training data, in a second stage we adopt data augmentation. Data augmentation techniques play a crucial role in enhancing the performance of machine learning models, particularly in scenarios where labeled training data is limited, which is our case. In this paper, we propose a novel approach for data augmentation using ChatGPT, a powerful language model developed by OpenAI. Unlike traditional data augmentation methods that rely on rule-based or linguistic transformations, our approach leverages the generative capabilities of ChatGPT to generate synthetic labeled authors by using prompts. We observed that using data augmentation significantly improves results for interest and intent classification.</p><p>In a final stage, we propose an ensemble approach that combines fine-tuned language models and NLI models, employing a soft voting mechanism. The intuition of this is that both methods address the same task but employ distinct approaches, thereby enabling the extraction of complementary strengths from each via an ensemble. Experimental results demonstrate that the ensemble approach outperforms individual models, and data augmentation using ChatGPT yields superior results in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-tuning BERT on few examples</head><p>Pretrained language models based on Transformers have significantly advanced the field of natural language processing (NLP) in recent years. These models leverage large-scale pretraining on vast amounts of unlabeled text data to learn powerful representations of words, sentences, and documents. One of the pioneering works in this domain is the BERT (Bidirectional Encoder Representations from Transformers) model introduced by <ref type="bibr" coords="2,359.67,651.55,11.58,10.91" target="#b2">[3]</ref>, which attained state-of-theart performance on various NLP tasks, including text classification and extractive question answering. Building upon this foundation, subsequent research has introduced variations and improvements to pretrained language models. For instance, RoBERTa <ref type="bibr" coords="3,410.14,100.52,12.99,10.91" target="#b3">[4]</ref> demonstrated the effectiveness of training larger models with more data and longer training durations. DeBERTa <ref type="bibr" coords="3,89.29,127.61,12.72,10.91" target="#b5">[6]</ref> introduced enhanced modeling techniques for better representation learning. These models highlight the continuous evolution and exploration of pretrained language models, showcasing their success in various NLP tasks and their potential for further advancements.</p><p>The prevailing method for utilizing pretrained language models in text classification involves replacing the original output layer with a task-specific layer and fine-tuning the entire model. This modifies the new layer's weights and gradually all original weights. For instance, in the case of text classification, an additional classification head maps the [CLS] last hidden state, or a pool of all last hidden states, to an unnormalized probability distribution across output classes. DocBERT <ref type="bibr" coords="3,171.16,236.01,17.92,10.91" target="#b12">[13]</ref> achieve state of the art results in four datasets by fine tuning BERT for document classification. The process of fine tuning for text classification introduces two sources of variability: the initialization of weights in the classification head and the order of data in the stochastic fine-tuning optimization. Furthermore, <ref type="bibr" coords="3,328.78,276.66,17.89,10.91" target="#b13">[14]</ref> identified several factors that cause instability when fine tuning BERT for tasks with few training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Modeling Few-Shot Classification as an Entailment problem</head><p>In a Few-Shot setting, training language models for classification (such as BERT) becomes challenging due to the large number of parameters to be updated and the few instances. Although in <ref type="bibr" coords="3,100.24,367.03,17.76,10.91" target="#b14">[15]</ref> the authors found that large language models can be suitable zero-shot predictors, or fewshot predictors with prompted examples in the context, these models are typically prohibitively expensive and complex to train and deploy. Some alternatives include reformulating the masked token prediction task in language models to predict the classes as the most probable class for the masked token <ref type="bibr" coords="3,152.81,421.23,16.30,10.91" target="#b15">[16,</ref><ref type="bibr" coords="3,171.84,421.23,13.95,10.91" target="#b16">17]</ref> or fine-tuning in advance for a similar task. However, <ref type="bibr" coords="3,425.63,421.23,17.76,10.91" target="#b10">[11]</ref> demonstrated that it is possible to take advantage of language models previously trained for the NLI task for Few-Shot classification by reformulating the classification problem as an entailment problem. This is done by formulating the text input as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;CLS&gt; Text input &lt;SEP&gt; Hypothesis for a given class &lt;EOS&gt;</head><p>The idea is that the inputs with correct hypotheses should be labeled as "entailment" and the incorrect hypothesis as "contradiction". Then, the models are fine-tuned for NLI using the few samples available. The authors in <ref type="bibr" coords="3,263.07,534.00,18.07,10.91" target="#b10">[11]</ref> found that this technique outperformed previous methods for Few-Shot classification. A drawback of the entailment approach is that the final trained models are not task-agnostic <ref type="bibr" coords="3,256.71,561.10,16.38,10.91" target="#b11">[12]</ref>, thus, models trained for a given few-shot problem may not be as effective for another. Furthermore, using these models in a production scenario may not be as efficient since -for each prediction-C * inputs need to pass through the model. To relieve this, <ref type="bibr" coords="3,157.77,601.75,17.99,10.91" target="#b11">[12]</ref> proposes a label-tuning approach, which uses a sentence encoder to map the input and the hypothesis to a vector space. In this scenario, only the hypothesis encoder has to be trained such that the encoded hypothesis of the correct class is more similar to the encoded text than the other classes. The official baseline of the shared task consists of a T5-large encoder <ref type="bibr" coords="4,128.63,86.97,18.07,10.91" target="#b17">[18]</ref> with label tuning. Nevertheless -although efficient-the label-tuning approach tends to underperform compared to the traditional cross-attention approach described above, therefore, we employ cross-attention models for the subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Crypto-influencer Dataset</head><p>The shared task "Profiling Cryptocurrency Influencers with Few-shot Learning" comprises tweets authored by various cryptocurrency influencers. The task encompasses three distinct subtasks, each focused on a specific profiling aspect and associated with its own set of tweets in English:</p><p>1. Subtask 1: Magnitude Profiling This subtask involves profiling the magnitude of an influencer, determined by the number of followers. The task's dataset consists of up to 10 tweets per user and 32 users per label. There are five possible labels for this subtask: null, nano, micro, macro, and mega. The total number of tweets counting each user adds up to a total of 929 tweets, which makes the dataset of subtask 1 the largest of the three. 2. Subtask 2: Interest Identification This subtask aims to identify the specific interest of a user based on the content of their tweet. Five labels describe different interests: technical information, price update, trading matters, gaming, and other. Therefore, this dataset is composed of a total of 320 tweets. 3. Subtask 3: Intent Classification In this subtask, the objective is to determine the intent behind a tweet written by a user. The four possible intent labels include subjective opinion, financial information, advertising, and announcement. In total this dataset is composed of 256 tweets, making it the smallest of the three subtasks.</p><p>It can be inferred that the tweets were collected from various sources related to known cryptocurrency influencers and their interactions within the community. The labeling process likely involved manual annotation or expert judgments to assign the appropriate labels for each subtask based on the content and context of the tweets. The exact methodology and criteria used for retrieval and labeling, however, was undisclosed in the information provided by the time we write this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text pre-processing</head><p>Following <ref type="bibr" coords="4,135.19,563.54,11.27,10.91" target="#b4">[5]</ref>, for all our experiments we used a normalization strategy for tweets by converting word tokens of user mentions and web/url links into special tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corresponding strings. Here's and example of an original tweet and how it would appear after the normalization strategy:</p><p>• Original: "RT @momomeatmaker: Fresh drop ♥ \n-3 Men Please -\nthe more the merrier-\n\n30 editions 0.5 $XTZ\n\ncollect via\nhttps://t.co/FVt1WR27TX". • Normalized: "RT @USER: Fresh drop :heart: \n-3 Men Please -\nthe more the merrier-\n\n30 editions 0.5 $XTZ\n\ncollect via HTTPURL".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Prompt formats for data augmentation using ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|X| Subtask 1:</head><p>Suppose there is a twitter user who is a cryptocurrency influencer and their class of influence is {label} influencer. They wrote the next tweets:\n {listed_tweets}\n Based on these tweets, invent a new user who also is a {label} cryptocurrency influencer and write a new tweet of that user here:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 2:</head><p>Suppose there is a twitter user who is a cryptocurrency influencer with interest in {label}. They wrote the next tweets:\n {listed_tweets}\n Based on these tweets, invent a new user who also is a cryptocurrency influencer with interest in {label} and write a new tweet of that user here:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 3:</head><p>Suppose there is a twitter user who is a cryptocurrency influencer with {label} intent. They wrote the next tweets:\n {listed_tweets}\n Based on these tweets, invent a new user who also is a cryptocurrency influencer with {label} intent a new tweet of that user here:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>An example of a tweet of subtask 3 and the resulting synthetic creation of ChatGPT using it as context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|X| Original tweet:</head><p>What are we saying about $FXS? I'll ignore the coming snapshot for the airdrop and I will wait for price to reach lower before I spot buy some Frax. IMO this snapshot is only the driver for the price to have a short-term rally before falling brutally. Expect a hard selloff. https:t.coAkradbARhu</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic tweet:</head><p>What are we saying about $FXS? I think the airdrop is a great way to get more people involved in the project and I will definitely be buying some before the snapshot. I believe the price will continue to rise after the snapshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ChatGPT data augmentation</head><p>To perform data augmentation with ChatGPT, we follow a two-step process. First, for each author 𝐴 in the training dataset we create a prompt using their tweets as shown in Figure <ref type="figure" coords="5,500.04,510.71,3.81,10.91" target="#fig_0">1</ref>.</p><p>Second, we use ChatGPT to genearate 𝑛 𝐴 different tweets, where 𝑛 𝐴 is the total number of tweets of author 𝐴. These generated tweets are going to be the tweets of a new synthetic author of the same class as 𝐴. Our procedure ensures that the total number of tweets and authors per class of the generated data are equal to the original data. The augmented dataset is the combination of both the original and the generated data, and is twice the size of the original training dataset.</p><p>ChatGPT allows the creation of more natural and contextually relevant synthetic data, which aids in training models that can better handle real-world scenarios. Additionally, ChatGPT's vast knowledge base enables it to generate text across a wide range of topics and styles, producing more diverse augmented data than traditional machine learning methods for data augmentation. See Figure <ref type="figure" coords="5,138.21,659.75,3.74,10.91">2</ref>. We used the Open AI's package * for python to generate all the synthetic data with GPT-3.5 <ref type="bibr" coords="6,487.55,86.97,16.09,10.91" target="#b18">[19]</ref>. The model version was text-davinci-002, although there are several model checkpoints for GPT-3.5, with 128 maximum tokens and temperature equals to 1.0. We did not explore any other configuration of parameters, which could be future work. We share the dataset and script with the community * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-tuned models</head><p>Starting from a pre-trained transformer-based language model, we added a classification head on top of the model, such that each input tweet produces an output vector of size corresponding to the number of possible classes. By applying the softmax function to the output vector, a probability distribution is obtained where each entry represents the probability of the tweet belonging to the corresponding class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Training</head><p>For the training phase, each tweet was used as a training instance, with its class determined by the author's class. In all three subtasks, the classes are balanced in terms of the number of authors. For subtasks 2 and 3, each author has only one tweet. For subtask 1, each author can have between 1 and 10 tweets, resulting in imbalanced classes in terms of the number of tweet instances. In this case, we decided to introduce sample weighting in the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Inference</head><p>During the inference phase, all tweets from the test split are processed to obtain their probability distributions in the class space. The author's class is determined by performing a soft voting mechanism on the distributions of their tweets. The class with the highest cumulative probability across their tweets is selected as their final class assignment. In other words, for an author 𝐴, its predicted class 𝑐 𝐴 is computed as:</p><formula xml:id="formula_0" coords="6,255.02,457.70,251.62,26.56">𝑐 𝐴 = arg max 𝑐∈𝒞 ∑︁ 𝑡∈𝑇 𝐴 𝑝(𝑐|𝑡)<label>(1)</label></formula><p>where 𝒞 is the class set and 𝑇 𝐴 is tweets set of author 𝐴. Specifically, for subtasks 2 and 3, the author's class is just the class with highest probability of their unique tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">NLI-based models</head><p>This section provides an overview of the training process for the entailment-based models used in the shared tasks. Additionally, it outlines the inference process employed once the models have been trained.</p><p>• Training.</p><p>We follow the same approach as <ref type="bibr" coords="6,262.11,624.89,17.89,10.91" target="#b10">[11]</ref> for training models for training the entailed-based approaches for the three subtasks. We begin by fine-tuning a pretrained language model, such as BERT, for the Natural Language Inference (NLI) task. To accomplish this, we utilize well-known datasets such as SNLI (Stanford Natural Language Inference) <ref type="bibr" coords="7,491.74,100.52,11.58,10.91" target="#b7">[8]</ref>, MNLI (Multi-Genre Natural Language Inference) <ref type="bibr" coords="7,340.62,114.06,11.58,10.91" target="#b8">[9]</ref>, and ANLI (Adversarial NLI) <ref type="bibr" coords="7,487.15,114.06,16.41,10.91" target="#b9">[10]</ref>.</p><p>Following the fine-tuning of the model for the NLI task, we proceed to fine-tune it further for classification tasks by incorporating text-hypothesis pairs. For every text sample in the training dataset, we construct two distinct text inputs intended for the entertainment models. One is assigned an entailment label (representing the correct class), while the other is assigned a contradiction label (randomly selected from the pool of incorrect classes). We additionally take into consideration two sets of label hypotheses: one devised by our team and the other generated by ChatGPT by providing a detailed description of the class along with our own hypothesis serving as an illustrative example. Furthermore, we train two sets of models: one exclusively using the original training data and the other incorporating the augmented data generated by ChatGPT. • Inference.</p><p>After completing the training phase for entailment-based classification models, we make predictions for a given sample text by calculating the entailment probabilities for each potential class within the dataset. For instance, in the case of subtask-2, which involves interest identification and encompasses five distinct classes, we would compute the entailment probabilities for all five text-hypothesis pairs. Subsequently, we select the class with the highest probabilities as the predicted label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ensembling entailment and full fine-tuning approaches</head><p>Considering that the subtask datasets for our problem are relatively larger compared to other few-shot problems * , the disparity in performance between traditional fine-tuned models and entailment-based approaches may not be as pronounced. Consequently, we anticipate that combining the predictions from both strategies through an ensemble approach can lead to enhanced prediction performance. By leveraging the strengths of each approach and mitigating their respective limitations, ensembling the predictions is expected to yield improved overall performance, taking advantage of the complementary aspects of the two strategies.</p><p>The main idea behind the ensemble system is to aggregate the predictions from various models. Initially, we plan to employ a soft-voting mechanism by summing the output logits of each model and subsequently selecting the class with the highest cumulative probability as the final prediction. However, a significant challenge arises in this approach: the predicted logits from the entailment-based models correspond to NLI predictions, encompassing entailment, contradiction, and neutral classes. Consequently, we must extract only the entailment class outputs to aggregate them into fine-tuned outputs, ensuring compatibility and coherence within the ensemble framework.</p><p>The process for performing the soft-voting between the two different approaches is shown in Figure <ref type="figure" coords="7,120.36,625.82,3.74,10.91" target="#fig_0">1</ref>, and explained in the following lines: • Given that we have 𝑁 classes, in the entailment-based approach we would have 𝑁 predictions per sample, corresponding to each class hypothesis. • From these NLI predictions * , we can simulate an output similar to the fine-tuning approach (green box) by focusing only on the entailment logits for each text-hypothesis pair (brownbox). • To sum the predicted class probability distributions for the soft-voting mechanism, we must transform the entailment outputs for each class by applying a softmax function with a temperature parameter, which ensures that the resulting distribution exhibits a similar entropy to that of the fine-tuned outputs (yellow-box). • Finally, we sum the predicted class probability distributions from the different models (red box). Then, we select the class with the highest probability as the final prediction</p><p>This approach allows us to incorporate entailment-based predictions into the ensemble framework while improving compatibility of the combined models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation Results</head><p>Due to the few-shot nature of the task, creating a development partition from the data could lead to biased estimations of the models' performance. To address this, we employ a 4-fold validation approach, ensuring equal class representation and robust performance estimation. Evaluation is based on the F1 macro metric, aligned with the official task metric, providing comprehensive assessment across the folds. First, we evaluate the performance of individual models. Each model was trained following one of the two methods described in Section 4: the fine-tuned (FT) approach, and the Entailment-Based (EB) approach. For the latter, we evalaute the set of hypothesis crafted by us (EB-Ho), and the set crafted by ChatGPT (EB-Hg). In order to identify the most promising models for further evaluation, we conducted a preliminary assessment of different models for each subtask. In this section we report the most promising architectures we evaluated for each subtask.</p><p>For subtask 1, we evaluated the performance of BERT, CryptoBERT, and FinBERT models using the fine-tuning method. For the entailment-based approach, we assessed the performance of RoBERTa-Tweets (pretrained specifically on Tweets) and FinBERT models. The evaluation results are presented in Table <ref type="table" coords="9,228.07,168.26,3.81,10.91" target="#tab_0">3</ref>. Notably, the traditional fine-tuning approach, particularly utilizing the BERT model, obtained a superior performance compared to the entailment-based approach. This outcome can be attributed to the fact that the first subtask encompassed a substantial amount of data, with over 900 tweets available for training. Consequently, the entailment-based approach may not be the most suitable method for this task, emphasizing the advantage of the traditional fine-tuning approach in scenarios with a larger data volume.</p><p>In the case of subtask 2, BERTweet, CryptoBERT, and DeBERTa were selected for the finetuning method and Roberta-Tweets and DeBERTa for entalment approaches. According to the results presented in Table <ref type="table" coords="9,231.83,276.66,3.81,10.91" target="#tab_1">4</ref>, the entailment-based models demonstrate slightly better performance, although the performance gap between the two approaches is relatively small. The DeBERTa model using the entailment method achieves the best performance, which we consider to be moderately better than the fine-tuning approach. This observation aligns with the fact that subtasks 2 and 3 have considerably less data compared to subtask 1. Consequently, the entailment-based methods showcase superiority in these subtasks where data scarcity is more prominent.</p><p>Moving on to subtask 3, we conducted evaluations using the same architectures as in subtask 2 for both the fine-tuning and entailment-based approaches. The results are presented in Table <ref type="table" coords="9,89.29,398.60,3.81,10.91" target="#tab_2">5</ref>. As observed, the entailment-based method yields better performance, particularly for the DeBERTa model. This can be attributed to the fact that subtask 3 has the smallest number of tweets, making the entailment-based approach more effective for this task.</p><p>Finally, we select the best performing individual models an build an ensemble following the procedure described in Section 4.5. The intuitive idea behind this ensemble is to leverage the capabilities of both approaches through a soft-voting mechanism. The obtained results are shown in Table <ref type="table" coords="9,127.32,479.89,3.69,10.91" target="#tab_3">6</ref>, we can observe that, for subtasks 2 and 3, there is a significant performance gain by ensembling both architectures. For subtask 1, we were not able to improve the performance of the single BERT model, mainly because of the larger performance gap with the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results in test partition</head><p>The best-performing systems in the 4-fold validation experiments were submitted for each subtask to be evaluated in the test partition. The official evaluation metrics obtained are shown in Table <ref type="table" coords="9,126.13,583.82,3.66,10.91" target="#tab_4">7</ref>. For the first subtask, we found that a single BERT <ref type="bibr" coords="9,351.89,583.82,12.69,10.91" target="#b2">[3]</ref> model fine-tuned on the subtask data achieved the best performance, surpassing training with ChatGPT data augmentation or models pretrained in a language domain closer to finance and cryptocurrency. For the second subtask, we submitted and ensemble that contained both fine-tuned and entailment-based models. For fine-tuned models, we employed CryptoBERT and BERTweet. In the case of entaiment-based models, we used DeBERTa and RoBERTa-Tweets. We trained most of the models -excepting DeBERTa-with ChatGPT synthetic data. In the case of the third subtask, the submitted system was also an ensemble of entailment-based models and fine-tuned models. This ensemble consisted on three fine-tuned models (BERTweet, FinBERT, and DeBERTA), and three entailment-based models (three DeBERTa models trained with different configuration for hypotheses and training data (See Table <ref type="table" coords="10,269.47,568.90,3.57,10.91" target="#tab_4">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>As previously highlighted, it is worth noting that the amount of data provided for the various subtasks, particularly subtask 1, is relatively larger than other few-shot classification problems. Consequently, the performance gap between traditional fine-tuning and the entailment-based approach is not as pronounced in the 4-fold evaluations we conducted. In subtask 1, the fine-tuning approaches exhibit superior performance compared to the entailment-based models. However, for subtasks 2 and 3, we do observe slightly better performance from the entailment-based models. In particular, assembling both approaches in subtasks 2 and 3 leads to a significant performance increase, highlighting the potential complementarity between the two techniques.</p><p>Finally, we observed promising results in incorporating a Large Language Model (LLM), specifically ChatGPT, into our system. It mostly allowed us to effectively double the available data by generating synthetic samples for each tweet. This augmentation of the dataset enhanced the model's ability to learn and generalize from a larger pool of examples. Also, ChatGPT assisted in crafting a comprehensive set of hypotheses for each subtask in the entailment- based approaches. Leveraging the LLM's language generation capabilities, we generated highly informative hypotheses, helping the entailment-based models to better discern among different labels.</p><p>Our proposed approaches demonstrated superior performance compared to the baseline models that relied on label tuning. Furthermore, our team achieved notable rankings, securing the first position in subtask 2, the third position in subtask 1, and the fifth position in subtask 3. As a result, our team obtained the highest average scores, ultimately obtaining the first place overall in the shared task. These results affirm the competitiveness and effectiveness of our proposed system, underscoring its capability to excel in this particular low-resource task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ethical Concerns</head><p>Models trained with low-resource data can be susceptible to biases present in the limited examples used for training. As a result, the predictions made by these models may be influenced by such biases, potentially leading to unfair or discriminatory outcomes in real-world scenarios. Caution should be exercised when relying on the predictions of these models, particularly in decision-making processes.</p><p>Furthermore, systems designed to profile individuals based on their social media posts should be deployed with care to avoid exacerbating existing biases and inequalities within the influencer ecosystem. Additionally, there is a risk of privacy invasion if the neural network inadvertently extracts sensitive information from the tweets, further emphasizing the need for responsible and ethical usage of such systems. Proper attention should be given to ensuring the protection of privacy rights and minimizing any potential harm or discrimination that may arise from the use of these models and their outputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,216.10,418.23,8.93;8,89.02,228.10,416.96,8.87;8,89.29,240.06,416.69,8.87;8,89.29,252.01,416.70,8.87;8,89.29,263.97,416.70,8.87;8,89.29,275.93,90.99,8.87;8,110.13,84.19,375.02,124.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ensemble pipeline for combining the predictions from both approaches using soft-voting. The inputs are represented in blue color, while the brown-colored boxes illustrate the outputs from the model trained for Natural Language Inference (NLI). Subsequently, only the entailment classes are selected, resulting in the yellow-colored box. The green boxes represent the output from the fine-tuned models. Finally, the red-colored box represents the predicted class probabilities after the soft-voting mechanism is applied.</figDesc><graphic coords="8,110.13,84.19,375.02,124.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,90.49,438.90,200.38"><head>Table 3 4</head><label>3</label><figDesc>-fold validation results for subtask 1. The tags are used later for describing ensemble configurations. The results demonstrate a clear superiority of the traditional fine-tuning approach for this subtask. This observation aligns with our hypothesis that the larger amount of available data contributes to the enhanced performance.</figDesc><table coords="10,95.27,157.97,432.63,132.90"><row><cell>Subtask 1</cell><cell></cell><cell></cell><cell>Only original data</cell><cell></cell><cell>Using Augmented Data</cell><cell></cell></row><row><cell>Tag</cell><cell>Model</cell><cell cols="2">Strategy F1 macro</cell><cell></cell><cell>F1 macro</cell><cell></cell></row><row><cell>B</cell><cell>BERT</cell><cell>FT</cell><cell>58.870</cell><cell cols="2">8.390 54.650</cell><cell>7.120</cell></row><row><cell>CB</cell><cell>CryptoBERT</cell><cell>FT</cell><cell>51.010</cell><cell cols="2">6.790 46.130</cell><cell>6.400</cell></row><row><cell>FB</cell><cell>FinBERT</cell><cell>FT</cell><cell>55.640</cell><cell cols="2">8.440 48.400</cell><cell>9.760</cell></row><row><cell>FB</cell><cell>FinBERT</cell><cell cols="2">EB -Hg 51.958</cell><cell cols="2">5.119 52.618</cell><cell>7.667</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Hg 46.550</cell><cell cols="2">6.920 50.204</cell><cell>5.220</cell></row><row><cell>FB</cell><cell>FinBERT</cell><cell cols="2">EB -Ho 49.553</cell><cell cols="2">7.415 51.287</cell><cell>5.665</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Ho 48.097</cell><cell cols="2">7.157 48.697</cell><cell>12.717</cell></row><row><cell>Baseline</cell><cell>T5-encoder</cell><cell>LT</cell><cell>43.25</cell><cell>4.22</cell><cell>42.17</cell><cell>8.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,313.12,434.27,188.42"><head>Table 4 4</head><label>4</label><figDesc>-fold validation results for subtask 2. The results show that the entailment-based approach outperforms the traditional fine-tuning approach in this subtask. Particularly, the DeBERTa model demonstrates the best overall performance among the evaluated models.</figDesc><table coords="10,95.27,368.65,427.99,132.89"><row><cell>Subtask 2</cell><cell></cell><cell></cell><cell>Only original data</cell><cell>Using Augmented Data</cell><cell></cell></row><row><cell>Tag</cell><cell>Model</cell><cell cols="2">Strategy F1 macro</cell><cell>F1 macro</cell><cell></cell></row><row><cell>BT</cell><cell>Bertweet</cell><cell>FT</cell><cell>61.480</cell><cell>3.040 62.290</cell><cell>3.370</cell></row><row><cell>CB</cell><cell>CryptoBert</cell><cell>FT</cell><cell>58.190</cell><cell>5.880 62.990</cell><cell>6.080</cell></row><row><cell>BT</cell><cell>Bert</cell><cell>FT</cell><cell>55.240</cell><cell>3.710 56.060</cell><cell>2.570</cell></row><row><cell>D</cell><cell>DeBERTa</cell><cell cols="2">EB -Hg 63.318</cell><cell>5.026 61.457</cell><cell>1.468</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Hg 60.106</cell><cell>3.404 62.694</cell><cell>0.254</cell></row><row><cell>D</cell><cell>DeBERTa</cell><cell cols="2">EB -Ho 62.120</cell><cell>2.853 61.429</cell><cell>3.182</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Ho 58.514</cell><cell>1.303 59.576</cell><cell>3.783</cell></row><row><cell>Baseline</cell><cell>T5-encoder</cell><cell>LT</cell><cell>60.200</cell><cell>4.150 60.490</cell><cell>3.710</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,90.49,434.27,188.42"><head>Table 5 4</head><label>5</label><figDesc>-fold validation results for subtask 3. The results indicate a slightly better performance when employing an entailment-based approach for this subtask. Notably, when incorporating ChatGPT Data Augmentation, we observe significantly improved results in this approach.</figDesc><table coords="11,95.27,146.01,427.99,132.89"><row><cell>Subtask 3</cell><cell></cell><cell></cell><cell>Only original data</cell><cell>Using Augmented Data</cell><cell></cell></row><row><cell>Tag</cell><cell>Model</cell><cell cols="2">Strategy F1 macro</cell><cell>F1 macro</cell><cell></cell></row><row><cell>BT</cell><cell>Bertweet</cell><cell>FT</cell><cell>68.600</cell><cell>5.370 67.520</cell><cell>5.460</cell></row><row><cell>CB</cell><cell>CryptoBert</cell><cell>FT</cell><cell>63.270</cell><cell>2.660 64.650</cell><cell>4.050</cell></row><row><cell>D</cell><cell>DeBERTa</cell><cell>FT</cell><cell>63.820</cell><cell>3.680 67.940</cell><cell>2.260</cell></row><row><cell>D</cell><cell>DeBERTa</cell><cell cols="2">EB -Hg 68.447</cell><cell>4.646 74.365</cell><cell>4.527</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Hg 65.141</cell><cell>4.639 64.336</cell><cell>2.899</cell></row><row><cell>D</cell><cell>DeBERTa</cell><cell cols="2">EB -Ho 67.517</cell><cell>4.511 72.828</cell><cell>5.294</cell></row><row><cell>RT</cell><cell cols="3">RoBERTa-Tweets EB -Ho 60.458</cell><cell>3.091 66.339</cell><cell>6.207</cell></row><row><cell>Baseline</cell><cell>T5-encoder</cell><cell>LT</cell><cell>67.030</cell><cell>4.560 62.150</cell><cell>5.760</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.98,301.17,417.00,219.04"><head>Table 6</head><label>6</label><figDesc>Figure 4: 4-fold validation results on Ensembles. The results demonstrate a significant improvement in subtasks 2 and 3 when employing ensembles. The nomenclature used for the ensembles corresponds to the Tags in Tables 3, 4, and 5: The letters indicate the employed model, the superscript denotes the approach (entailment-based or traditional fine-tuning), and the subscript indicates the usage of Data Augmentation.</figDesc><table coords="11,120.92,380.60,353.44,139.61"><row><cell cols="2">Models in ensemble</cell><cell></cell><cell>F1 macro</cell></row><row><cell>FB 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell>+FB 𝐸𝐵-𝐻𝑜 𝐷𝐴</cell><cell></cell><cell>54.398</cell><cell>8.287</cell></row><row><cell>Subtask 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">B 𝐹 𝑇 +FB 𝐹 𝑇 +CB 𝐹 𝑇</cell><cell></cell><cell>56.433</cell><cell>10.531</cell></row><row><cell cols="2">CB 𝐹 𝑇 𝐷𝐴 +D 𝐸𝐵-𝐻𝑔</cell><cell></cell><cell>65.936</cell><cell>1.387</cell></row><row><cell>Subtask 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CB 𝐹 𝑇 𝐷𝐴 +BT 𝐹 𝑇 𝐷𝐴 +D 𝐸𝐵-𝐻𝑔 +RT 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell>68.458</cell><cell>2.667</cell></row><row><cell cols="2">BT 𝐹 𝑇 +D 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell></cell><cell>74.735</cell><cell>3.964</cell></row><row><cell>Subtask 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BT 𝐹 𝑇 + FB 𝐹 𝑇 𝐷𝐴 + D 𝐹 𝑇 𝐷𝐴 +D 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell>+D 𝐸𝐵-𝐻𝑜 𝐷𝐴</cell><cell>+D 𝐸𝐵-𝐻𝑔 75.819</cell><cell>4.926</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.99,90.49,416.99,106.84"><head>Table 7</head><label>7</label><figDesc>F1 macro scores obtained in the test set for each subtask. We follow the same nomenclature as in Table6.</figDesc><table coords="12,123.02,131.84,349.24,65.49"><row><cell>Submitted system</cell><cell></cell><cell cols="2">F1 macro Rank</cell></row><row><cell>Subtask 1 B 𝐹 𝑇</cell><cell></cell><cell>58.44</cell><cell>3rd</cell></row><row><cell cols="2">Subtask 2 CB 𝐹 𝑇 𝐷𝐴 +BT 𝐹 𝑇 𝐷𝐴 +D 𝐸𝐵-𝐻𝑔 +RT 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell>67.12</cell><cell>1st</cell></row><row><cell>Subtask 3 BT 𝐹 𝑇 + FB 𝐹 𝑇 𝐷𝐴 + D 𝐹 𝑇 𝐷𝐴 +D 𝐸𝐵-𝐻𝑔 𝐷𝐴</cell><cell>+D 𝐸𝐵-𝐻𝑜 𝐷𝐴</cell><cell>+D 𝐸𝐵-𝐻𝑔 64.46</cell><cell>5th</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="8,89.29,669.86,3.82,5.24;8,93.34,671.03,217.62,8.97"><p>* The probabilities for entailment, neutral, and contradiction.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors thank <rs type="institution">Consejo Nacional de Ciencia, Humanidades y Tecnología (CONAHCYT)</rs>, <rs type="institution">Centro de Investigación en Matemáticas (CIMAT)</rs> and <rs type="institution">Instituto Nacional de Astrofísica, Óptica y Electrónica (INAOE)</rs> for the computer resources provided through the <rs type="institution">INAOE Supercomputing Laboratory's Deep Learning Platform for Language Technologies (Laboratorio de Supercómputo: Plataforma de Aprendizaje Profundo)</rs> with the project "<rs type="projectName">Identification of Aggressive and Offensive text through specialized BERT's ensembles</rs>" and <rs type="projectName">CIMAT Bajio Supercomputing Laboratory</rs> (#<rs type="grantNumber">300832</rs>). Sanchez-Vega would like to thank <rs type="institution">CONACYT</rs> for its support through the <rs type="programName">Program "Investigadoras e Investigadores por México</rs>" by the project "<rs type="projectName">Desarrollo de Inteligencia Artificial aplicada a la prevención de violencia y salud mental</rs>. " (ID. <rs type="grantNumber">11989</rs>, No. 1311) and the <rs type="programName">COLMEX Interdisciplinary Data Science Program</rs> (<rs type="grantName">Open Society Grant</rs>). <rs type="person">Valles-Silva</rs> (<rs type="grantNumber">CVU 1069625</rs>) and <rs type="funder">Villa-Cueva</rs> (<rs type="grantNumber">CVU 1019520</rs>) thank <rs type="funder">CONACYT</rs> for the support through the master's degree scholarship at <rs type="institution">CIMAT</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RVVwzfr">
					<orgName type="project" subtype="full">Identification of Aggressive and Offensive text through specialized BERT&apos;s ensembles</orgName>
				</org>
				<org type="funded-project" xml:id="_qbHsqjN">
					<idno type="grant-number">300832</idno>
					<orgName type="project" subtype="full">CIMAT Bajio Supercomputing Laboratory</orgName>
					<orgName type="program" subtype="full">Program &quot;Investigadoras e Investigadores por México</orgName>
				</org>
				<org type="funded-project" xml:id="_AhvWDer">
					<idno type="grant-number">11989</idno>
					<orgName type="grant-name">Open Society Grant</orgName>
					<orgName type="project" subtype="full">Desarrollo de Inteligencia Artificial aplicada a la prevención de violencia y salud mental</orgName>
					<orgName type="program" subtype="full">COLMEX Interdisciplinary Data Science Program</orgName>
				</org>
				<org type="funding" xml:id="_FjgkQyh">
					<idno type="grant-number">CVU 1069625</idno>
				</org>
				<org type="funding" xml:id="_XyyAg7J">
					<idno type="grant-number">CVU 1019520</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Such as the ones evaluated by <ref type="bibr" coords="7,205.18,670.99,14.72,8.97" target="#b10">[11]</ref> by utilizing only eight samples per label.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,199.79,394.52,10.91;13,112.66,213.34,394.53,10.91;13,112.66,226.89,394.52,10.91;13,112.66,240.44,393.53,10.91;13,112.66,253.99,394.53,10.91;13,112.66,267.54,395.17,10.91;13,112.66,281.08,393.33,10.91;13,112.66,294.63,394.53,10.91;13,112.66,308.18,65.44,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,294.21,226.89,212.97,10.91;13,112.66,240.44,393.53,10.91;13,112.66,253.99,42.05,10.91">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Ríos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pęzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,371.28,267.54,136.55,10.91;13,112.66,281.08,393.33,10.91;13,112.66,294.63,197.14,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,342.79,294.63,159.86,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,112.66,321.73,393.33,10.91;13,112.66,335.28,393.33,10.91;13,112.14,348.83,159.26,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,466.53,321.73,39.46,10.91;13,112.66,335.28,261.87,10.91">Profiling Cryptocurrency Influencers with Few shot Learning at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,417.97,335.28,88.02,10.91;13,112.14,348.83,47.32,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="13,167.44,348.83,73.93,10.91">Notebook Papers</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,362.38,393.33,10.91;13,112.66,375.93,393.33,10.91;13,112.66,389.48,393.32,10.91;13,112.66,403.03,393.33,10.91;13,112.66,416.58,394.03,10.91;13,112.66,430.13,185.51,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,323.15,362.38,182.83,10.91;13,112.66,375.93,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="13,327.87,375.93,178.11,10.91;13,112.66,389.48,393.32,10.91;13,112.66,403.03,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="13,112.66,443.67,395.17,10.91;13,112.66,457.22,395.01,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="13,137.85,457.22,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,470.77,393.53,10.91;13,112.66,484.32,393.33,10.91;13,112.66,497.87,394.53,10.91;13,112.66,511.42,394.51,10.91;13,112.36,527.41,127.52,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,296.79,470.77,209.39,10.91;13,112.66,484.32,62.84,10.91">BERTweet: A pre-trained language model for English tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.2</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.2.doi:10.18653/v1/2020.emnlp-demos.2" />
	</analytic>
	<monogr>
		<title level="m" coord="13,198.47,484.32,307.51,10.91;13,112.66,497.87,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,538.52,393.33,10.91;13,112.66,552.07,168.38,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m" coord="13,261.90,538.52,244.09,10.91;13,112.66,552.07,38.60,10.91">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,565.62,395.01,10.91;13,112.66,581.61,97.35,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Araci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10063</idno>
		<title level="m" coord="13,155.39,565.62,321.35,10.91">Finbert: Financial sentiment analysis with pre-trained language models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,592.72,393.32,10.91;13,112.66,606.27,247.08,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m" coord="13,337.32,592.72,168.66,10.91;13,112.66,606.27,117.21,10.91">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,619.81,393.32,10.91;13,112.66,633.36,275.18,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m" coord="13,289.01,619.81,216.97,10.91;13,112.66,633.36,145.31,10.91">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,646.91,393.73,10.91;13,112.66,660.46,338.81,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14599</idno>
		<title level="m" coord="13,399.85,646.91,106.53,10.91;13,112.66,660.46,208.53,10.91">Adversarial nli: A new benchmark for natural language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,393.33,10.91;14,112.66,100.52,369.67,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,329.99,86.97,139.38,10.91">Entailment as few-shot learner</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2104.14690" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,393.32,10.91;14,112.66,127.61,315.11,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D</forename><surname>La Peña Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10543</idno>
		<title level="m" coord="14,484.96,114.06,21.02,10.91;14,112.66,127.61,185.53,10.91">Zero and few-shot learning for author profiling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,141.16,395.01,10.91;14,112.66,157.15,97.35,7.90" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08398</idno>
		<title level="m" coord="14,286.52,141.16,190.00,10.91">Docbert: Bert for document classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,168.26,394.54,10.91;14,112.66,181.81,394.62,10.91;14,112.31,195.36,184.53,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,377.76,168.26,129.44,10.91;14,112.66,181.81,48.14,10.91">Revisiting few-sample {bert} fine-tuning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=cO1IH43yUF" />
	</analytic>
	<monogr>
		<title level="m" coord="14,183.76,181.81,240.74,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,394.53,10.91;14,112.66,222.46,394.53,10.91;14,112.66,236.01,394.53,10.91;14,112.66,249.56,394.53,10.91;14,112.66,263.11,395.01,10.91;14,112.66,276.66,187.21,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="14,112.66,263.11,174.33,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,290.20,393.33,10.91;14,112.66,303.75,395.01,10.91;14,112.66,319.74,97.35,7.90" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="14,207.64,290.20,298.35,10.91;14,112.66,303.75,83.85,10.91">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2001.07676" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,330.85,394.53,10.91;14,112.66,344.40,395.01,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,230.29,330.85,272.63,10.91">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2012.15723" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,394.53,10.91;14,112.66,371.50,393.33,10.91;14,112.66,385.05,365.41,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="14,112.66,371.50,357.82,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,398.60,394.53,10.91;14,112.66,412.15,394.53,10.91;14,112.66,425.70,394.53,10.91;14,112.66,439.25,394.53,10.91;14,112.66,452.79,394.53,10.91;14,112.66,466.34,393.33,10.91;14,112.28,479.89,394.42,10.91;14,112.66,493.44,277.49,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,112.66,452.79,166.47,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="14,175.26,466.34,237.32,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
