<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.32,416.57,16.17">Overview of the Trigger Detection Task at PAN 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,114.87,80.70,10.37"><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
							<email>matti.wiegmann@uni-weimar.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.44,114.87,91.67,10.37"><forename type="first">Magdalena</forename><surname>Wolska</surname></persName>
							<email>magdalena.wolska@uni-weimar.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.56,114.87,74.06,10.37"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<email>martin.potthast@uni-leipzig.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ScaDS.AI</orgName>
								<address>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.31,114.87,58.78,10.37"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<email>benno.stein@uni-weimar.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<settlement>Weimar</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.32,416.57,16.17">Overview of the Trigger Detection Task at PAN 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">04F9DDF878C549966C5700262285A641</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Trigger warnings are document labels that warn the reader about content that might cause discomfort or distress. These labels are often asked for by online communities, especially by vulnerable groups. Here, we present trigger detection at PAN 2023 as a multi-label document classification task: Given a fan fiction document, assign all appropriate trigger warnings from the given label set. We derive a set of 32 trigger warnings based on two widely referenced institutional guidelines on sensitive content. We compile a 341,000 document evaluation resource, fan fiction documents from Archive of our Own (AO3), which we fully annotated with the 32 trigger warnings. Six participants submitted solutions to the task. The submissions cover several different methods; the most effective submissions use hierarchical deep learning with RoBERTa-based encodings. The top approach achieves a macro F1 of 0.35 and a micro F1 of 0.75.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A trigger in psychology is a stimulus that elicits negative emotions or feelings of distress. In general, triggers include a broad range of stimuli-such as smells, tastes, sounds, textures, or sights-which may relate to or evoke memories of possibly distressing acts or events. To proactively apprise the audience that a piece of media (writing, audio, video, etc.) contains potentially distressing material, the use of "trigger warnings"-labels indicating the type of potentially triggering content present-has become common in institutionalized education but also in online communities <ref type="bibr" coords="1,206.34,467.21,11.43,9.46" target="#b0">[1]</ref>, making it possible for sensitive audience to prepare themselves for the content and better manage their reactions. Particularly online communities have expanded the trauma-related concept of trigger warnings used in psychology to many more types of potentially distressing content like eating disorders, discrimination, suicide, abuse, or pornography.</p><p>Fiction in particular can make its readers susceptible to triggers, as it often serves "escaping" reality for a while by identifying oneself with the characters in a story and experiencing their fate with particular intensity. This may partly explain why the community of the fan fiction site Archive of our Own (AO3) is one of the few where trigger warnings are used proactively and as a matter of course: About 50% of the 7.8 million AO3 works have author-provided warnings.</p><p>In this pilot edition of the Trigger Detection task at PAN 2023, we establish the computational problem of identifying whether or not a given document contains triggering content. In particular, we formalize trigger detection as a multi-label document classification (MLC) task as follows:</p><p>Given a fan fiction document, assign all appropriate trigger warnings from the given label set.</p><p>We created a new evaluation resource, PAN23-trigger-detection, containing ca. 340,000 fan fiction works from Archive of our Own (AO3) annotated with a 32-label trigger warning set. We rely on user-generated labels (authors assigned warning-like labels) and follow the authors' understanding of triggers and which documents require a warning. The warnings are assigned via AO3's freeform content descriptors ("tags"), a custom, high-dimensional label system. Since tags include also non-warning descriptors, we developed a distant-supervision strategy to detect if a freeform tag corresponds to one of the 32 predefined warnings compiled from institutional content warning guidelines. The task is primarily evaluated with the standard measures for multi-label classification, micro and macro F 1 . In total, 6 participants submitted software to Trigger Detection 2023.</p><p>This overview paper first details the creation of the evaluation resource (Section 2), in particular the distillation of the warning label set from two institutional content guidelines, the scraping of AO3, the distant-supervision labeling, and the curation of the works. Furthermore, the evaluation procedure (metrics and baselines) are described in Section 3, the 6 participant submissions are described in Section 4, and the results are discussed in Section 5.<ref type="foot" coords="2,372.59,329.89,3.65,6.91" target="#foot_0">1</ref>,<ref type="foot" coords="2,379.89,329.89,3.65,6.91" target="#foot_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>For the trigger detection task, we created a new evaluation resource, the PAN23-trigger-detection corpus, consisting of 341,246 fan fiction works downloaded from Archive of our Own and annotated in a multi-label setting with a set of 32 warning labels. An extended version of our annotation method and the evaluation resource is presented by Wiegmann et al. <ref type="bibr" coords="2,437.48,432.53,11.59,9.46" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Curating a Set of Warning Labels</head><p>Since there is no authoritative (closed-set) set of trigger warning labels, we derived these labels for use in our dataset from two guideline documents for labeling sensitive content: the University of Reading list of "themes that require trigger warnings" <ref type="bibr" coords="2,348.41,510.45,12.87,9.46" target="#b2">[3]</ref> and the University of Michigan list of content warnings <ref type="bibr" coords="2,196.12,524.00,11.71,9.46" target="#b3">[4]</ref>. The two largely overlapping lists comprise, each, 21 categories of triggering concepts, including health-related (eating disorders, mental illness), sexually-oriented (sexual assault, pornography) as well as verbal (hate speech, racial slurs), and physical abuse (animal cruelty, blood, suicide). The lists were pre-processed to unfold compound categories into individual elements (e.g. "Animal cruelty or animal death" → "animal cruelty", "animal death") and lower-cased. Table <ref type="table" coords="2,196.35,591.74,5.56,9.46">9</ref> (see Appendix) shows the aligned source labels and the merged set. This merged set of warnings comprises 35 categories; we removed the rarest three labels since there were too few annotated documents with those labels in the final dataset. From them, we derived the 32 label trigger warning set for the PAN 2023 Trigger Detection task (see Table <ref type="table" coords="2,490.04,632.39,3.94,9.46" target="#tab_0">1</ref>). Three major observations can be made of the merged university label set (Table <ref type="table" coords="3,466.74,609.89,4.12,9.46">9</ref>): First, the granularity of triggers is not uniform (e.g., both abuse and the more specific child abuse are included). Second, the set comprises subsets of related concepts which lend themselves to semantic abstraction (e.g., sexism, classism and other -isms and -phobias can be considered types of prejudice). Third, the prescribed list is not exhaustive, as is also pointed out on both websites.  To obtain labels that abstract over the inconsistent granularity (Table <ref type="table" coords="4,398.84,356.58,3.90,9.46" target="#tab_0">1</ref>), that are orthogonal in terms of semantics, and to better inform later annotation decisions, we grouped the original labels into semantically related subsets. The grouping was done by identifying semantic fields, trigger domains, with which the triggering concepts can be associated via some semantic relation, for instance, is-a or results-in; since the label set is sufficiently small, grouping was done manually. Technically speaking, for warnings formulated as complex nouns, we first identified the semantic content-bearing lexeme and used that as the basis for grouping. For most complex nouns, the head noun was used; the label "pornographic content" is an example of an exception in the case of which the content-bearing adjective was used to identify its semantic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Acquiring the Source Documents</head><p>Table <ref type="table" coords="4,115.25,515.79,5.37,9.46" target="#tab_1">2</ref> shows the descriptive statistics of our source data: ca. 8 million works of fan fiction from Archive of our Own. We initially downloaded all works released between August 13, 2008 (the platform launch) and August 09, 2021, from archiveofourown.org and extracted the document text and metadata (i.e., the freeform tags) from the scraped HTML. To download the HTML page of each work, we scraped the output of the search function to get the work ID and then constructed a direct URL to that work's page. Since the search function was limited to 10,000 works per page, we constructed queries to search for all works released on one particular day, for each day in the release window.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Assigning Trigger Warnings to Source Documents</head><p>We labeled all works in the source data via distant supervision based on the freeform tags assigned to the works by their author(s). The results and evaluation are shown in Table <ref type="table" coords="5,425.22,285.27,4.03,9.46" target="#tab_2">3</ref>. We first identify the freeform tags that also indicate a warning from our 32-label set and, second, we assign this warning to all works labeled with the indicative freeform tag. The underlying mapping table, which maps from freeform tag to trigger warning, was created by (i) manually annotating the 2,000 most common tags, (ii) efficiently identifying sub-structures of the tag graph that indicate a trigger warning, annotating each node in the structure with that warning, and (iii) merging both results, giving priority to the manual annotations. We manually annotated two sets of freeform tags: first, the 2,000 most frequent tags (0-2k), which cover just over 50% of tag occurrences, and second, the 10,000-11,000 most frequent tags (10-11k) as an evaluation dataset. All tags were annotated by two annotators; diverging annotations were merged by critical discussion. Then, the sub-structures of the tag graph that indicate the same trigger warning across all nodes were identified (cf. Figure <ref type="figure" coords="5,420.20,434.31,4.45,9.46" target="#fig_0">1</ref>) by extracting and manually annotating rooted sub-graphs from the tag graph in a 5-stage process:</p><p>1. Grouping of all tags via the synonym relation and identification of the canonical tag. One tag per synonym set is marked as canonical by wranglers, all other synonyms are direct successors of the canonical tag and have no other arcs. 2. Identification of meta-sources: canonical tags that are source nodes in the meta-sub graph.</p><p>Meta-sub relations indicate a directed lexical entailment between canonical tags and have a typical depth of 2 to 4. 3. Identification of candidate sources of trigger graphs: meta-sources that are also direct successors of the No Fandom node in the parent-child graph. Sinks in this graph are the canonical tags and all predecessors are either a Fandom, media type, or No Fandom. The latter is added as a parent to tags that apply to many Fandoms, including content warnings but also, for example, holidays and languages. This yields ca. 5,000 tags. 4. Identification of trigger graph sources: manual annotation of all candidate sources, discarding the nodes without a trigger warning. 5. Identification of all trigger graphs: manual depth-first traversal of the tag graph along the meta-sub relation, starting from a trigger graph source. If a successor does not agree with the trigger warning assigned to its predecessor, the arc between them is removed, and the successor added as new trigger graph source to be annotated with a new trigger warning. Table <ref type="table" coords="6,127.19,231.81,5.52,9.46" target="#tab_2">3</ref> shows that we can annotate 52% of all freeform tag occurrences manually with high reliability. With our method, we can completely annotate more than half of all works in the corpus. The other half of the works are only partially annotated since our method only annotates 15% of the unique tags. Tags are only wrangled (i.e., added to the tag graph) if they occur thrice. Since 89.9% of unique freeform tags occur only once, our method misses them. We evaluate the effectiveness of our annotation approach by comparing the inferred annotations with the two manually annotated tag sets 0-2k and 10-11k across the four different trigger warning sets. As shown in Table <ref type="table" coords="6,158.14,326.66,4.09,9.46" target="#tab_2">3</ref>, our approach achieves both 0.95 accuracy and F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Sampling the Evaluation Dataset</head><p>From the resulting collection of annotated fan fiction works, we sampled PAN23-trigger-detection by discarding all works that had no warning assigned, were originally published pre-2009 (as opposed to posted after that since AO3 also archives works from older fan fiction sites), had freeform tags that could not clearly mapped, was not in English (ca. 8% of the works), had less than 50 or more than 6,000 words (outliers; ease of computation), less than 2 or more than 66 freeform tags (confidence threshold), less than 1,000 hits (views), or, less than 10 kudos (likes; popularity threshold). We also removed all (near) duplicates. The resulting dataset contains 341,246 fan fiction works, which was split with stratified sampling into 90:5:5 training, validation, and test sets; i.e., we kept the label distribution equal across the three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Properties of the Evaluation Dataset</head><p>Table <ref type="table" coords="6,115.24,536.68,5.37,9.46" target="#tab_5">4</ref> shows the descriptive statistics of the dataset splits. The training dataset with ca. 300,000 works is large enough to train deep neural classifiers. The datasets contain ca. 5% very short documents (&lt;512 words) that can be used by a BERT-based system without truncation and ca. 85% medium-sized documents (&lt;4,096 words) that can be used by a sparse-attention model. Figure <ref type="figure" coords="6,121.59,590.88,5.56,9.46" target="#fig_1">2</ref> shows the distribution of the labels over the test dataset. The most frequent label is pornography and occurs in ca. 77% of the documents. Most labels are less common, between ca. 10% for sexual-assault and 6e-4% for animal-cruelty. Documents have 1-13 labels per document, ca. 71% with a single label, 20% with two, and 6% with three.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation and Baselines</head><p>We evaluate the submissions primarily through the established multi-label classification metrics F 1 and Accuracy (primary metrics). In addition, we also evaluate the effectiveness of individual labels/label groups (extended metrics) and the effectiveness in relation to document metadata. Lastly, we construct and evaluate voting-based ensembles from the submissions. As primary metrics, we compare precision, recall, and F 1 at both micro-and macro average, and subset accuracy, which measures accuracy on a per-sample basis (i.e., if all labels of one example are set correctly). In our assessments, we favor the macro over the micro F 1 scores due to the label imbalance. We also favor recall over precision, since we consider trigger warning assignment a high-recall task where false negatives cause more harm than false positives. However, we opted not to modify the metrics or their parameters to reflect this preference.</p><p>As extended metrics, we compare precision and recall of pornography (due to its frequency), the average effectiveness of the 15 next-most common labels (sexual-assault-dissection), and the average effectiveness of the 16 least common labels. We also compute the number of classes with either zero or a very low (&lt;0.1) precision and recall to check for high-frequency label bias.</p><p>As metadata-based metrics, we compare micro and macro F 1 for the document subsets that fall within certain metadata thresholds. First, we compare short (&lt;500), medium, and long (&gt;4,000) documents. We assume that short works are easier to classify since models can capitalize more directly on BERT (which has a short input size). Second, we compare works with few (&lt;5), medium, and many (&gt;20) freeform tags. We assume that works with many freeform tags are easier to classify because many tags suggest that authors took greater care with annotating their works and the resulting higher label quality leads to better effectiveness. Third, we compare works with low (&lt;50 comments, &lt;60 bookmarks, &gt;450 kudos, &gt;8,500 hits), medium, and high (&gt;280 comments, &gt;330 bookmarks, &gt;1,850 kudos, &gt;35,000 hits) popularity. We assume that works with high popularity are also easier to classify because authors are more diligent when tagging works that gain much attention. Fourth, we compare works with an archive warning (Graphic Depictions Of Violence, Major Character Death, Rape/Non-Con, Underage), without warning (No Archive Warnings Apply), and works that do not specify the warnings (Choose Not To Use Archive Warnings). We assume that works with a warning are easier to classify and works without specified warning are the hardest, since authors hide warning tags within spoilers and might therefore less diligently annotate freeform warnings. Fifth, we compare works with an Explicit or Mature rating to works with neither. We assume that explicit or mature works contain more markers and are thus easier to classify. Finally, we construct four ensembles from the submitted results, where the assignment of a true label is decided by voting to surpass a threshold 𝜏 . The Top-3 ensemble uses the three best submissions with 𝜏 = 2, the other ensembles use all submissions with 𝜏 = {3, 5, 7}.</p><p>As a baseline, we trained an XGBoost <ref type="bibr" coords="8,266.76,394.01,12.61,9.46" target="#b4">[5]</ref> classifier based on word-1-3-gram features encoded as TF•IDF document vectors with a minimum document frequency of 5. We used only the top 10,000 features according to a 𝜒 2 feature selection. The dataset was undersampled uniformly at random to 1,000 samples per label. As parameters, we used a max depth or 3, a learning rate of 0.25, and 300 estimators with 10-round early stopping. The features word-1, 2, and 3-grams and character-3 and 5-grams were evaluated, as well as feature selection (with or without), model parameters, and the thresholds for over-and undersampling via grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions</head><p>The 6 submissions to the PAN 2023 Trigger Detection task employed a broad set of techniques, from hierarchical transformer structures to strategic feature engineering. Table <ref type="table" coords="8,454.42,548.72,5.56,9.46" target="#tab_7">5</ref> shows an overview of the different strategies used by the participants. All participants used a form of a neural network as a model, where RoBERTa was most common and most successful as a classifier or pre-trained model to produce a strong input encoding. Most submissions also focused on improving the long document aspect of the task (most documents are longer than the input size of the state-of-the-art classification models) by using hierarchical classifiers (chunks are encoded, and prediction is based on a combination of encodings), or voting-based approaches (chunks are labeled individually, document labels are aggregated over chunk labels). The submissions cope with the label imbalance (the most common label (pornography) is an order of magnitude more common than the other labels) through over-and undersampling or by changing class-weights in the loss function, so that misclassifying a rare class increases the error more than a common label. Sahin et al. <ref type="bibr" coords="9,154.78,88.05,12.73,9.46" target="#b5">[6]</ref> submitted a hierarchical transformer architecture that achieved the top macro F 1 score (by a slim margin of 0.002) and came in second in micro F 1 and accuracy, while having a relatively high recall within the top approaches. The approach first segments the document into chunks (200 words with 50 words overlap) and then pre-trains a RoBERTa transformer on the chunks to learn the genre. The architecture then embeds all chunks of a document using the pre-trained transformer, followed by an LSTM for each label (in a one-vs-all setting), predicting the class from a sequence of chunk-embeddings (RoBERTa's [CLS] token). To cope with label imbalance, the approach assigns positive weights in the loss function to the rare half of the labels.</p><p>Su et al. <ref type="bibr" coords="9,141.57,196.44,12.86,9.46" target="#b6">[7]</ref> submitted a siamese transformer that achieves the second-best macro F 1 score (by a slim margin of 0.002) and the top scores in micro F 1 and accuracy, while notably favoring precision over recall. The approach segments the documents into 505-word chunks, encodes the first and last chunk using RoBERTa, mean-pools the contextual embeddings (ignoring the [CLS] token), and classifies based on the pooled embeddings using a 1D convolutional neural network.</p><p>Cao et al. <ref type="bibr" coords="9,149.74,264.19,12.87,9.46" target="#b7">[8]</ref> submitted a voting-based transformer that favors recall over precision. The approach segments the training documents into chunks, assigns each chunk the labels from its source document, and trains a single RoBERTa-based classifier on each chunk. To make predictions, the documents are again chunked, the labels for each chunk are predicted, and a label is assigned to the document if it is assigned to more than half of the chunks. The training data was dynamically over-and undersampled: pornography was undersampled to 5,000 examples and other labels to 2,000 examples. Examples with rare labels were replicated 8-10 times.</p><p>Cao et al. <ref type="bibr" coords="9,147.31,359.03,12.80,9.46" target="#b8">[9]</ref> also submitted a voting-based transformer that achieved very balanced results, neither favoring macro over micro scores nor precision over recall. The approach chunks and votes similarly to Cao et al. <ref type="bibr" coords="9,211.63,386.13,12.72,9.46" target="#b7">[8]</ref> but builds two different models to overcome the data imbalance, one for pornography and one for the other 31 classes. The pornography model was trained on a random selection of 40,000 works with and 40,000 works without the pornography warning. The model for the other labels removes works with only the pornography warning, undersamples frequent classes to 3,000 examples, and oversamples rare labels by replicating works 4-6 times.</p><p>Felser et al. <ref type="bibr" coords="9,156.16,453.87,18.16,9.46" target="#b9">[10]</ref> submitted a 1-vs-rest multi-layer perceptron based on two features: fasttextbased document embeddings and superclass probabilities. This approach achieved the top micro and macro recall, at the cost of precision on the test dataset. Document embeddings were created by training a fasttext model from the training data, generating the embeddings for each unique word in a document, scaling them by term frequency, and adding and normalizing the scaled word vectors over the document. The superclass probabilities were determined by grouping the 32 labels semantically into 6 superclasses, bootstrapping a seeded LDA with the 50 most relevant bi-grams of each group (determined through a TF•IDF-like approach for n-gram weighting, which downgrades pornographic terms), and training a classifier to predict the superclass based on the topic model outputs, using class probabilities as features. Label imbalance was addressed via class penalties in the loss function, where the MLP-2 variant has a higher penalty.</p><p>Lastly, Lakshmaiah et al. <ref type="bibr" coords="9,218.75,602.92,18.20,9.46" target="#b10">[11]</ref> present an LSTM-based approach using GloVE-embeddings, which is third in micro F 1 with very high precision but rather weak in macro average scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Table <ref type="table" coords="10,115.22,562.85,5.36,9.46" target="#tab_8">6</ref> shows the evaluation results for the primary metrics as discussed in Section 3, ordered by macro F 1 . Here, the hierarchical classifiers are the most effective by a large margin, followed by the XGBoost baseline. The most effective approach by macro F 1 is the one by Sahin et al. with 0.352, a small margin before that of Su et al. with 0.350. The best approach by micro F 1 and subset accuracy is the one by Su et al.. The XGBoost baseline is only beaten by these two top approaches. The models score very differently in precision and recall, depending on the architecture. Four models score generally higher in recall, the other 4 in precision. There is no obvious relationship between effectiveness and preference for precision or recall. The ensembles (top 3 and 𝜏 = 3) beat the submissions but by a very small margin of ca. 0.02.</p><p>Table <ref type="table" coords="11,126.73,481.09,5.42,9.46" target="#tab_9">7</ref> shows the evaluation results for the extended metrics. Unsurprisingly, all submissions score very high on pornography and notably lower on all rare labels, which explains the difference between macro and micro F 1 . There is a clear decrease in efficiency with decreasing label frequency. It also becomes more obvious that models tend to be good in either precision or recall with large differences between them. Combining the strength of the high-recall and high-precision approaches is a potential way forward, albeit our basic ensemble exploits that only marginally.</p><p>Table <ref type="table" coords="11,126.34,562.38,5.35,9.46">8</ref> shows the evaluation results based on document subsets with common metadata values. Regarding the document length, the macro F 1 scores are mixed: Models that use the complete work as single examples during training (Sahin et al. <ref type="bibr" coords="11,321.07,589.48,11.58,9.46" target="#b5">[6]</ref>, the baseline, and Felser et al. <ref type="bibr" coords="11,468.15,589.48,17.44,9.46" target="#b9">[10]</ref>) are slightly (0.05-0.1) less effective on short texts; models that use only a section of the document (Su et al. <ref type="bibr" coords="11,130.75,616.58,11.59,9.46" target="#b6">[7]</ref>, Cao, G. et al. <ref type="bibr" coords="11,209.23,616.58,12.26,9.46" target="#b8">[9]</ref>) are slightly (0.05-1.0) less effective on long texts. On micro F 1 , all models tend to perform worse on shorter texts. This contradicts our assumption (and prior evidence <ref type="bibr" coords="11,131.62,643.68,12.43,9.46" target="#b1">[2]</ref>) that models will be generally better on short texts which can fully capitalize on BERTs strength on short inputs. An alternative hypothesis is that shorter documents are simply less clear and have fewer of the markers that the classifier expects to make a positive prediction. Regarding the tag count, the top models are slightly (0-0.1) less effective when there are many freeform tags. There is no difference between the less effective models. This also contradicts our assumption that models with many tags are easier to classify due to higher label reliability. Regarding popularity, there is no notable difference in micro F 1 . On macro F 1 , models are slightly (0.04-0.14) more efficient on high popularity works than on low popularity works. This agrees with our assumption that labels of popular works are more reliable. Regarding the archive warnings, there is no notable difference between works with or without warnings. However, the most effective models are slightly (ca. 0.05 macro, ca. 0.15 micro) less effective on works with undeclared warnings than on others. This agrees with our assumption that these works are less diligently tagged by their authors (potentially as a spoiler tag). Lastly, regarding the rating, models are more (ca. 0.2-0.3 micro F 1 ) effective on explicit works, which is likely an artifact from the very effective classification of the pornography label. On macro F 1 , contrary to the micro score, the submissions are slightly (0-0.1) less effective on explicit works. This also contradicts our assumptions that explicit or mature works are easier to classify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>We present the first task on trigger detection at PAN 2023, for which we created a 341,000 document evaluation resource of fan fiction works annotated with up to 32 labels in a multi-label classification setting. We extensively evaluate the results of six participant submissions. The most effective submissions score 0.35 on macro F 1 and 0.75 on micro 𝐹 1 .</p><p>We find several factors that impact the effectiveness of the submissions. First, we find that encoding and training on the full documents is important for good scores on long documents and hierarchical models appear to be best in this regard. We assume that it is central to find triggering passages that only appear in some parts of the document and that inform the classification decision, instead of finding the topic or style that is also present in the beginning. Surprisingly, short documents appear to be much harder to classify, so models with a strong encoding for short texts (BERT) are important and document vectors are less effective as features. None of the top models manage to be great at both, short and long-document effectiveness, leaving potential for improvement. The effect sizes on all metadata comparisons are small (ca. 0.05-0.15).</p><p>Second, we find that all submissions are much less effective on rare labels and very effective on very common labels. We assume that the triggering concept goes beyond what can be observed from the passages in the training data, hence the models can not connect the triggers in the test data to the learned concept.</p><p>Third, we find that the submissions are more effective on popular works and less effective on works with an Choose Not To Use Archive Warnings declaration. We assume that authors' diligence in annotating freeform tags varies a lot, so some works are under-tagged (i.e. authors want to avoid spoilers) and authors are more diligent in assigning warnings for popular works. However, we also find that the submissions are less effective on works with many freeform tags, so the reverse assumption (over-tagging decreases label reliability) also has some merit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,201.85,416.86,9.35;5,89.29,213.90,308.46,9.22"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic display of the properties of the tag graph and how they were used to infer trigger labels from the additional tags with minimal manual annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,246.57,339.24,9.35"><head>p o r n o g r a p h y v i o l e n c e s e x u a l -a s s a u l t a b u s e b l o o d d e a t h i n c e s t u n d e r a g e s u i c i d e p r e g n a n c y c h i l d -a b u s e d y i n g h o m o p he x i s m m i s c a r r i a g e s t r a n s p h o b i a a b o r t i oFigure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of the 32 classes in the PAN23-trigger-detection dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.98,90.03,418.39,491.94"><head>Table 1</head><label>1</label><figDesc>The complete trigger warning set used in PAN23-trigger-detection grouped into 7 more general, semantically coherent groups. The italic part of the definitions are examples of AO3's freeform tags that match the respective warning label.</figDesc><table coords="3,89.30,143.95,416.67,438.02"><row><cell>Trigger domain</cell><cell>Definition and Example Tags</cell></row><row><cell cols="2">Discrimination/Prejudice-related</cell></row><row><cell>ableism</cell><cell>Discrimination against disabled persons, Ableist Language</cell></row><row><cell>classism</cell><cell>Discrimination based on social class, Rich/Poor Divide, Class Oppression</cell></row><row><cell>homophobia</cell><cell>Discrimination against homosexuals, Homophobic Language, Gay Panic</cell></row><row><cell>misogyny</cell><cell>Discrimination/hate against women, Misogynistic Language</cell></row><row><cell>racism</cell><cell>Discrimination based on race (including fantasy races), Fantastic Prejudice</cell></row><row><cell>sexism</cell><cell>Discrimination based on gender stereotypes, Misgendering, Deadnaming</cell></row><row><cell>transphobia</cell><cell>Discrimination against transgender persons</cell></row><row><cell cols="2">Hostile Acts/Aggression-related</cell></row><row><cell>violence</cell><cell>Physical violence, Manhandling, Torture, Murder</cell></row><row><cell>animal-cruelty</cell><cell>Violence/Harm against animals, Animal Abuse, Animal Mistreatment</cell></row><row><cell>sexual-assault</cell><cell>Physical, sexual Violence, Rape, Sexual Abuse, Non-consensual Actions</cell></row><row><cell>abuse</cell><cell>Domestic Violence, Verbal Abuse, Psychological Abuse, Bullying</cell></row><row><cell>child-abuse</cell><cell>Like abuse, but explicitly directed against children</cell></row><row><cell>abduction</cell><cell>Abduction by deception, often non-violent, Stockholm Syndrom</cell></row><row><cell>kidnapping</cell><cell>Violent kidnapping, hostage situations, Captivity</cell></row><row><cell>Pregnancy-related</cell><cell></cell></row><row><cell>pregnancy</cell><cell>(Issues of) being pregnant, Male Pregnancy, Fertility Issues</cell></row><row><cell>miscarriages</cell><cell>(Aftermath of) Miscarriages, Child Loss</cell></row><row><cell>childbirth</cell><cell>Act of Giving birth</cell></row><row><cell>abortion</cell><cell>Termination of pregnancy, including non-voluntary</cell></row><row><cell>Anatomy-related</cell><cell></cell></row><row><cell>dissection</cell><cell>Dissection of body parts, Mutiliation, Body Horror, Surgery, Loosing Limbs</cell></row><row><cell>blood</cell><cell>Blood, Gore, Wounds</cell></row><row><cell>Death-related</cell><cell></cell></row><row><cell>dying</cell><cell>The process of dying from the subject's perspective, Drowning, Euthanasia</cell></row><row><cell>death</cell><cell>Death of others Character death, Killing, Corpses, Coping with Loss or Grief</cell></row><row><cell>animal-death</cell><cell>Death of animals</cell></row><row><cell>Mental Health-related</cell><cell></cell></row><row><cell>mental-illness</cell><cell>Severe mental illness, Hallucinations, Dissociative Identity Disorder, Insanity</cell></row><row><cell>suicide</cell><cell>Suicide attempt, ideation, conduct, and aftermath</cell></row><row><cell>eating-disorders</cell><cell>Anorexia, Bulimia, Self-starvation, Binge Eating</cell></row><row><cell>fat-phobia</cell><cell>Obesity, Fat-Shaming, Weight (Loss) Issues</cell></row><row><cell>body-hatred</cell><cell>Body and gender dysmorphia</cell></row><row><cell>self-harm</cell><cell>Self-destructive acts or behavior, Self-mutiliation</cell></row><row><cell>Sexuality-related</cell><cell></cell></row><row><cell>incest</cell><cell>Sex between family members, Sibling Incest, Twincest</cell></row><row><cell>underage</cell><cell>Sex with a minor, consensual and non-consensual, Pedophilia</cell></row><row><cell>pornographic-content</cell><cell>Graphic display of sex, plays, toys, technique descriptions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.98,90.03,418.93,85.42"><head>Table 2</head><label>2</label><figDesc>Sizes of the works in the corpus of works with various properties relevant for sampling datasets.</figDesc><table coords="4,89.29,119.74,416.69,55.71"><row><cell cols="2">Corpus Size and properties</cell><cell cols="2">Corpus Size and properties</cell><cell>Corpus Size and properties</cell><cell></cell></row><row><cell>Words Total works with warnings</cell><cell>58B 7.9M 2.6M</cell><cell>Release Date pre-2009 More than 1 chapter More than 6k words</cell><cell>246K 1.9M 1.8M</cell><cell>Less than 3 tags More than 66 tags Up to 90% annotated works Less than 10 kudos</cell><cell>2.3M 8K 3.7M 1.3M</cell></row><row><cell>Non-English Language</cell><cell>751K</cell><cell>Duplicates</cell><cell>8K</cell><cell>Less than 1000 hits</cell><cell>4.5M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.98,197.16,418.94,61.48"><head>Table 3</head><label>3</label><figDesc>Number of AO3 freeform tags that can be annotated with a trigger warning by different methods. 0-2k and 10-11k contain manually annotated tags, tag graph contains tags annotated via distant supervision, and combined contains 0-2k and tag graph.</figDesc><table coords="4,90.58,251.15,90.17,7.48"><row><cell>Set</cell><cell>No.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,90.58,251.15,414.13,77.52"><head>tags in set (% of all) TWs (% of set) Mapping evaluation</head><label></label><figDesc></figDesc><table coords="4,90.58,265.01,414.13,63.66"><row><cell></cell><cell>Tag occurrence</cell><cell>Unique tags</cell><cell></cell><cell>Prec</cell><cell>Rec</cell><cell>F 1</cell><cell>Acc</cell></row><row><cell>0-2k</cell><cell>27.7M ( 52.14)</cell><cell>2K ( 0.02)</cell><cell cols="2">463 (22.92) 0.95</cell><cell>0.94</cell><cell>0.94</cell><cell>0.94</cell></row><row><cell>10-11k</cell><cell>0.3M ( 0.57)</cell><cell>1K ( 0.01)</cell><cell cols="2">85 ( 8.47) 0.98</cell><cell>0.97</cell><cell>0.97</cell><cell>0.97</cell></row><row><cell>tag graph</cell><cell>42.2M ( 79.51)</cell><cell>1.4M ( 14.55)</cell><cell>116K ( 8.19)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>combined</cell><cell>44.3M ( 83.35)</cell><cell>1.4M ( 14.55)</cell><cell>116K ( 8.19)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>All tags</cell><cell>53.1M (100.00)</cell><cell>9.7M (100.00)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,88.98,90.03,417.00,113.63"><head>Table 4</head><label>4</label><figDesc>Descriptive statistics of the training, validation, and test split of the dataset.</figDesc><table coords="6,89.29,120.85,416.69,82.82"><row><cell>Training Dataset</cell><cell></cell><cell>Validation Dataset</cell><cell></cell><cell>Test Dataset</cell><cell></cell></row><row><cell>Total Works</cell><cell>307,102</cell><cell>Total Works</cell><cell>17,104</cell><cell>Total Works</cell><cell>17,040</cell></row><row><cell>&lt; 512 words</cell><cell>15,233</cell><cell>&lt; 512 words</cell><cell>861</cell><cell>&lt; 512 words</cell><cell>813</cell></row><row><cell>&lt; 4,096 words</cell><cell>261,156</cell><cell>&lt; 4,096 words</cell><cell>14,571</cell><cell>&lt; 4,096 words</cell><cell>14,555</cell></row><row><cell>Mean no. words</cell><cell>2,400</cell><cell>Mean no. words</cell><cell>2,386</cell><cell>Mean no. words</cell><cell>2,388</cell></row><row><cell>Median no. words</cell><cell>2,126</cell><cell>Median no. words</cell><cell>2,115</cell><cell>Median no. words</cell><cell>2,101</cell></row><row><cell>90pct no. words</cell><cell>4,579</cell><cell>90pct no. words</cell><cell>4,550</cell><cell>90pct no. words</cell><cell>4,558</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,88.98,90.03,417.00,168.06"><head>Table 5</head><label>5</label><figDesc>Overview of the submitted methods. Listed is the (dominant) model architecture, the feature representation, the method used to handle long documents, and the sampling strategy used to handle the skewed label distribution.</figDesc><table coords="8,89.62,142.30,416.03,115.79"><row><cell>Participant</cell><cell>Model</cell><cell>Features</cell><cell>Length</cell><cell>Imbalance</cell></row><row><cell>Sahin et al. [6]</cell><cell cols="2">RoBERTa + LSTM CLS embedding</cell><cell>Hierarchical cls.</cell><cell>Weighted loss</cell></row><row><cell>Su et al. [7]</cell><cell>RoBERTa + CNN</cell><cell>Context embeddings</cell><cell>Hierarchical cls.</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>with 1D convolution</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>and mean-pooling</cell><cell></cell><cell></cell></row><row><cell>XGBoost baseline</cell><cell>XGBoost</cell><cell>TF•IDF</cell><cell cols="2">Document features Undersampling</cell></row><row><cell>Cao et al. [8]</cell><cell>RoBERTa</cell><cell>CLS token</cell><cell>Voting</cell><cell>Over-and under-sampling</cell></row><row><cell>Cao et al. [9]</cell><cell>RoBERTa</cell><cell>CLS token</cell><cell>Voting</cell><cell>Over-and under-sampling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ separate classifiers</cell></row><row><cell>Felser et al. [10]</cell><cell>MLP</cell><cell cols="3">Aggregate word emb. Document features Weighted loss</cell></row><row><cell></cell><cell></cell><cell>+ topic model</cell><cell></cell><cell></cell></row><row><cell cols="2">Lakshmaiah et al. [11] LSTM</cell><cell>GloVE</cell><cell>(LSTM)</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,88.98,90.03,417.00,188.42"><head>Table 6</head><label>6</label><figDesc>Participant scores of the trigger detection task at PAN 2023. Shown are the core metrics, sorted by macro F 1 . Bold indicates the leading approach for each metric. Scores of the voting-based ensembles are bold when they are better than the leading submission.</figDesc><table coords="10,166.41,144.40,262.46,134.05"><row><cell>Participant</cell><cell>Macro</cell><cell></cell><cell>Micro</cell><cell>Acc</cell></row><row><cell></cell><cell>Prec Rec</cell><cell>F 1</cell><cell>Prec Rec</cell><cell>F 1</cell></row><row><cell>Sahin et al. [6]</cell><cell cols="4">0.37 0.42 0.352 0.73 0.74 0.74 0.59</cell></row><row><cell>Su et al. [7]</cell><cell cols="4">0.54 0.30 0.350 0.80 0.71 0.75 0.62</cell></row><row><cell>XGBoost baseline</cell><cell cols="4">0.52 0.25 0.301 0.88 0.57 0.69 0.53</cell></row><row><cell>Cao H. et al. [8]</cell><cell cols="4">0.24 0.29 0.228 0.43 0.79 0.56 0.18</cell></row><row><cell>Cao G. et al. [9]</cell><cell cols="4">0.28 0.22 0.225 0.58 0.66 0.62 0.32</cell></row><row><cell>Felser et al. [10]</cell><cell cols="4">0.11 0.63 0.161 0.27 0.82 0.40 0.27</cell></row><row><cell cols="5">Lakshmaiah et al. [11] 0.10 0.04 0.048 0.82 0.50 0.63 0.52</cell></row><row><cell>Ensemble (Top 3)</cell><cell>0.56 0.30</cell><cell>0.36</cell><cell cols="2">0.88 0.68 0.77 0.63</cell></row><row><cell>Ensemble (𝜏 = 3)</cell><cell>0.38 0.42</cell><cell>0.37</cell><cell cols="2">0.65 0.80 0.72 0.52</cell></row><row><cell>Ensemble (𝜏 = 5)</cell><cell>0.55 0.20</cell><cell>0.26</cell><cell cols="2">0.88 0.65 0.75 0.60</cell></row><row><cell>Ensemble (𝜏 = 7)</cell><cell>0.39 0.07</cell><cell>0.10</cell><cell cols="2">0.97 0.50 0.66 0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,88.96,300.37,418.40,209.72"><head>Table 7</head><label>7</label><figDesc>Participant scores of the trigger detection task at PAN 2023. Shown are the extended metrics: precision and recall for Pornography, the more common half of labels excluding pornography (Mid) and the rare half (Bot) as well as the number of classes where precision and recall is either zero or below 0.1. Participants are sorted by total macro F 1 (cf. Table6).</figDesc><table coords="10,126.80,366.58,341.68,143.51"><row><cell>Participant</cell><cell>Porn.</cell><cell>Mid</cell><cell>Bot</cell><cell cols="2">Zero P/R</cell><cell>&lt;0.1 P/R</cell></row><row><cell></cell><cell cols="6">Prec Rec Prec Rec Prec Rec Prec Rec Prec Rec</cell></row><row><cell>Sahin et al. [6]</cell><cell cols="3">0.95 0.96 0.62 0.48 0.12 0.51</cell><cell>3</cell><cell>3</cell><cell>10</cell><cell>6</cell></row><row><cell>Su et al. [7]</cell><cell cols="3">0.90 0.97 0.61 0.43 0.57 0.19</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>9</cell></row><row><cell>XGBoost baseline</cell><cell cols="3">0.98 0.87 0.62 0.21 0.38 0.24</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>9</cell></row><row><cell>Cao H. et al. [8]</cell><cell cols="3">0.86 0.98 0.22 0.61 0.16 0.12</cell><cell>5</cell><cell>5</cell><cell>7</cell><cell>13</cell></row><row><cell>Cao G. et al. [9]</cell><cell cols="3">0.97 0.88 0.29 0.42 0.24 0.09</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>15</cell></row><row><cell cols="4">Felser et al. [10] (MLP1) 0.97 0.91 0.18 0.72 0.03 0.64</cell><cell>7</cell><cell>5</cell><cell>24</cell><cell>5</cell></row><row><cell cols="4">Felser et al. [10] (MLP2) 0.97 0.91 0.26 0.45 0.03 0.31</cell><cell>13</cell><cell>13</cell><cell>22</cell><cell>13</cell></row><row><cell>Lakshmaiah et al. [11]</cell><cell cols="3">0.93 0.91 0.18 0.04 0.00 0.00</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>30</cell></row><row><cell>Ensemble (Top 3)</cell><cell cols="3">0.96 0.96 0.72 0.38 0.50 0.25</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>7</cell></row><row><cell>Ensemble (𝜏 = 3)</cell><cell cols="3">0.94 0.97 0.43 0.61 0.28 0.36</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>6</cell></row><row><cell>Ensemble (𝜏 = 5)</cell><cell cols="3">0.97 0.93 0.68 0.33 0.76 0.10</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>16</cell></row><row><cell>Ensemble (𝜏 = 7)</cell><cell cols="3">0.98 0.87 0.82 0.08 0.91 0.02</cell><cell>18</cell><cell>20</cell><cell>18</cell><cell>25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.50,660.76,364.35,7.77"><p>The baseline and evaluation code used for this task is available at github.com/pan-webis-de/pan-code.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.50,671.91,209.59,7.77"><p>The data used are available at zenodo.org/record/7612628.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Participant scores of the trigger detection task at PAN 2023. Shown are scores of examples with certain properties based on different document lengths, number of freeform tags (tag confidence), popularity confidence (hits, kudos, comments, bookmarks), works with, without, and with unspecified AO3 archive warning, and works with or without and explicit or mature rating. Participants are sorted by total macro F 1 (cf.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tables and Figures</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.92,112.89,364.49,9.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,154.74,112.89,189.22,9.46">Trigger Warnings: History, Theory, Context</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Knox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Rowman &amp; Littlefield</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,126.44,393.25,9.46;13,112.41,139.99,393.57,9.46;13,112.92,153.53,394.58,9.46;13,112.92,167.08,357.43,9.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,473.18,126.44,32.99,9.46;13,112.41,139.99,316.14,9.46">Trigger Warning Assignment as a Multi-Label Document Classification Problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,452.39,139.99,53.59,9.46;13,112.92,153.53,342.51,9.46">Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="13,112.92,180.63,393.07,9.46;13,112.92,194.18,394.97,9.46;13,112.92,207.73,393.06,9.46;13,112.92,221.28,108.21,9.46" xml:id="b2">
	<monogr>
		<ptr target="https://www.reading.ac.uk/cqsd/-/media/project/functions/cqsd/documents/qap/trigger-warnings.pdf" />
		<title level="m" coord="13,213.52,180.63,292.47,9.46;13,112.92,194.18,269.91,9.46">Guide to policy and procedures for teaching and learning; Guidance on content warnings on course content (&apos;trigger&apos; warnings)</title>
		<imprint>
			<date type="published" when="2023-05-10">2023. May 10, 2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Reading</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,234.83,394.97,9.46;13,112.92,248.38,393.67,9.46;13,112.92,261.93,394.87,9.46;13,112.92,275.48,98.52,9.46" xml:id="b3">
	<monogr>
		<ptr target="https://sites.lsa.umich.edu/inclusive-teaching-sandbox/wp-content/uploads/sites/853/2021/02/An-Introduction-to-Content-Warnings-and-Trigger-Warnings-Draft.pdf" />
		<title level="m" coord="13,219.50,234.83,256.44,9.46">An Introduction to Content Warnings and Trigger Warnings</title>
		<imprint>
			<date type="published" when="2023-05-10">2023. May 10, 2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,289.03,393.06,9.46;13,112.92,302.58,394.42,9.46;13,112.92,316.12,394.97,9.46;13,112.10,329.67,254.55,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,211.65,289.03,187.44,9.46">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="13,423.19,289.03,82.79,9.46;13,112.92,302.58,394.42,9.46;13,112.92,316.12,40.49,9.46">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,343.22,393.06,9.46;13,112.92,356.77,393.06,9.46;13,112.56,370.32,394.78,9.46;13,112.92,383.87,96.16,9.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,291.56,343.22,214.42,9.46;13,112.92,356.77,152.55,9.46">ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">E</forename><surname>Kucukkaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,145.94,370.32,356.02,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,397.42,394.43,9.46;13,112.92,410.97,393.07,9.46;13,112.92,424.52,232.08,9.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,212.42,397.42,198.13,9.46">Siamese Networks in Trigger Detection task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,296.02,410.97,209.97,9.46;13,112.92,424.52,127.91,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,438.07,393.06,9.46;13,112.53,451.62,394.82,9.46;13,112.92,465.17,393.07,9.46;13,112.92,478.71,232.08,9.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,389.69,438.07,116.29,9.46;13,112.53,451.62,297.03,9.46">Trigger Warning Labeling with RoBERTa and Resampling for Distressing Content Detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,296.02,465.17,209.97,9.46;13,112.92,478.71,127.91,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,492.26,393.06,9.46;13,112.92,505.81,394.43,9.46;13,112.92,519.36,393.07,9.46;13,112.92,532.91,130.55,9.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,383.20,492.26,122.78,9.46;13,112.92,505.81,209.45,9.46">A dual-model classification method based on RoBERTa for Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.58,519.36,308.40,9.46;13,112.92,532.91,26.38,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,546.46,393.06,9.46;13,112.53,560.01,393.46,9.46;13,112.56,573.56,394.78,9.46;13,112.92,587.11,96.16,9.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,336.24,546.46,169.74,9.46;13,112.53,560.01,147.63,9.46">FoSIL at PAN?23: Trigger Detection with a Two Stage Topic Classifier</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Felser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Demus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Labudde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Spranger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,145.94,573.56,356.02,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.92,600.66,394.58,9.46;13,112.92,614.21,394.87,9.46;13,112.92,627.76,303.57,9.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,315.50,600.66,171.09,9.46">Trigger Detection in Social Media Text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lakshmaiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Balouchzahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,368.77,614.21,139.02,9.46;13,112.92,627.76,199.40,9.46">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
