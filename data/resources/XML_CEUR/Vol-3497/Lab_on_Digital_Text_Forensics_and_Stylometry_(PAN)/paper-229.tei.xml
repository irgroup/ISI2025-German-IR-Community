<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,360.49,15.42;1,89.29,106.66,276.91,15.42">ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,71.83,11.96"><forename type="first">Umitcan</forename><surname>Sahin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<postCode>06378</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.76,134.97,109.34,11.96"><forename type="first">Izzet</forename><forename type="middle">Emre</forename><surname>Kucukkaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<postCode>06378</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.10,134.97,73.55,11.96"><forename type="first">Cagri</forename><surname>Toraman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<postCode>06378</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,360.49,15.42;1,89.29,106.66,276.91,15.42">ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B45DF76D9C00B32CDB55D892EC322844</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Trigger detection</term>
					<term>Fanfiction</term>
					<term>Transformer-based language models</term>
					<term>Long text classification</term>
					<term>Multi-label classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the validation set, which are higher than the baseline results shared at PAN CLEF 2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fanfiction has been incredibly popular in recent years, especially among online communities. It is a lively and imaginative form of literary expression. It entails the development of fresh storylines, characters, and situations based on pre-existing fictional worlds, giving fans the chance to develop their favorite stories and pursue original, inventive concepts. As the Fanfiction community expands, there is an increasing need to address crucial issues related to participant safety and well-being.</p><p>The existence of triggering content is a major issue in the world of Fanfiction. Content that can cause extreme negative emotional reactions or the traumatization of people is referred to as triggering content. It might touch on issues like abuse, violence, mental health, or other upsetting topics. It is essential to create systems for properly identifying and managing triggering content given the diverse and frequently fragile character of Fanfiction readers and writers.</p><p>In this study, as Aselsan Research Center -Natural Language Processing team (ARC-NLP), we propose a method for trigger detection in long text documents using natural language CLEF 2023 -Conference and Labs of the Evaluation Forum, September 18-21, 2023, Thessaloniki, Greece ucsahin@aselsan.com.tr (U. Sahin); ekucukkaya@aselsan.com.tr (I. E. Kucukkaya); ctoraman@aselsan.com.tr (C. Toraman) 0000-0001-9594-3148 (U. Sahin); 0009-0006-2877-8713 (I. E. Kucukkaya); 0000-0001-6976-3258 (C. Toraman) processing (NLP) and machine learning techniques. We seek to train a classification algorithm capable of precisely recognizing multiple triggering contents by using the concept of hierarchical recurrence over Transformer-based language models <ref type="bibr" coords="2,322.95,114.06,11.24,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,336.93,114.06,7.43,10.91" target="#b1">2,</ref><ref type="bibr" coords="2,347.09,114.06,7.49,10.91" target="#b2">3]</ref>. In our method, we first split long Fanfiction documents into smaller sized segments with an overlap between each consecutive segment. We use these segments to fine-tune a Transformer-based language model. Then, we extract each segment's feature embedding from the fine-tuned Transformer model, which are used in the training of multiple LSTM models. Finally, we combine the predictions of the trained LSTM models to generate trigger labels for multi-class and multi-label classification. To the best of our knowledge, we are first to use the techniques and methods described in this paper in the context of trigger detection in Fanfiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>In the context of trigger detection at PAN CLEF 2023 <ref type="bibr" coords="2,327.14,267.54,11.37,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,341.24,267.54,7.57,10.91" target="#b4">5]</ref>, our objective is to assign warning labels to Fanfiction documents that may contain content capable of causing discomfort or distress (known as triggering content) <ref type="bibr" coords="2,268.17,294.63,11.48,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,283.04,294.63,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,293.95,294.63,7.65,10.91" target="#b6">7]</ref>. Specifically, trigger detection is posed as a multi-label document classification task, aiming to assign the appropriate trigger warnings to each document without exceeding the necessary labels. It is important to note that all trigger warnings are determined from the perspective of the document's author, meaning that the author decides which specific trigger(s) the document contains. There are 32 distinct trigger labels including pornographic-content, violence, death and sexual-assault etc<ref type="foot" coords="2,426.65,360.62,3.71,7.97" target="#foot_0">1</ref> . Each document can contain more than one trigger label, which leads to the need to adopt a multi-class and multi-label approach.   The dataset for trigger detection at PAN CLEF 2023 comprises of fanfiction pieces sourced from archiveofourown.org (AO3) <ref type="bibr" coords="2,211.27,589.68,11.23,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,224.72,589.68,7.49,10.91" target="#b6">7]</ref>. Each document falls within the range of 50 to 6,000 words and is accompanied by one or more trigger labels. Table <ref type="table" coords="2,317.11,603.23,4.97,10.91">1</ref> shows the number of documents, average number of words, and presence of labels for training, validation, and test sets. As mentioned before, the label set encompasses 32 distinct trigger warnings, exhibiting a distribution where certain labels are frequently encountered while the majority of labels are relatively uncommon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Table <ref type="table" coords="3,115.89,86.97,5.09,10.91" target="#tab_3">2</ref> shows the class distribution ratios with respect to 32 trigger labels in the training and validation sets. As seen, the classes are greatly imbalanced. Furthermore, we also note that the class distributions between the training and validation sets are very similar to each other, which suggests that they come from the same distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method: Hierarchical Recurrence over Transformer-based Language Model</head><p>In this section, we describe our method of hierarchical recurrence over Transformer-based model for long text classification of multi-label trigger detection. The diagram of the model is shown in Figure <ref type="figure" coords="4,176.56,156.31,3.81,10.91" target="#fig_0">1</ref>. As shown in the figure, we divide our methodology into four parts: 1) Segmentation, 2) Tokenization, 3) Feature extraction, and 4) Model training, which are explained in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segmentation</head><p>According to the information provided in Table <ref type="table" coords="4,305.86,232.99,3.79,10.91">1</ref>, the average word count in each document within the training set exceeds 2000. Traditional Transformer-based language models such as BERT <ref type="bibr" coords="4,117.57,260.09,12.89,10.91" target="#b7">[8]</ref> employ tokenizers with a maximum length of 512 tokens and typically truncate the remaining text. However, this approach is inadequate for accurately classifying long documents since crucial information may be omitted through truncation, resulting in subpar classification performance. Therefore, we follow a similar approach to <ref type="bibr" coords="4,337.16,300.73,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="4,350.85,300.73,7.42,10.91" target="#b1">2,</ref><ref type="bibr" coords="4,360.73,300.73,7.42,10.91" target="#b2">3,</ref><ref type="bibr" coords="4,370.61,300.73,8.88,10.91" target="#b8">9]</ref> in our segmentation method.</p><p>We first apply text processing to the documents by</p><p>‚Ä¢ removing HTML tags,</p><p>‚Ä¢ removing URLs,</p><p>‚Ä¢ removing English stop words <ref type="bibr" coords="4,249.68,365.13,16.25,10.91" target="#b9">[10]</ref>, and ‚Ä¢ lower-casing all text.</p><p>Then, we split the processed document into segments (i.e., text chunks) of 200 words (i.e., ùë§ 0 , ..., ùë§ 200 ) with an overlap of 50 words between each consecutive segment as shown in Figure <ref type="figure" coords="4,89.04,428.59,3.78,10.91" target="#fig_0">1</ref>. We assign the original document label to each segment. In other words, we represent each document in the training set with a variable-length sequence of 200-word segments where each segment is assigned the original document label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tokenization</head><p>For tokenization, we fine-tune a Transformer-based RoBERTa model <ref type="bibr" coords="4,390.84,505.27,17.75,10.91" target="#b10">[11]</ref> using the sequence of segments obtained after our segmentation method. We use the base-version of the model at HuggingFace<ref type="foot" coords="4,147.34,530.61,3.71,7.97" target="#foot_1">2</ref> with a learning rate of 1ùëí -5, epoch number 3, and training batch size of 8. We also use the corresponding RoBERTa model's tokenizer with a maximum length of 256 tokens.</p><p>After the tokenization step, we represent each 200-word segment by the corresponding tokens with size 256 (i.e., ùë° 0 , ..., ùë° 255 in Figure <ref type="figure" coords="4,261.46,573.01,3.57,10.91" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Extraction</head><p>For this method, we feed forward the segment tokens obtained after our tokenization method to the fine-tuned RoBERTa model and construct segment embeddings. We extract the embedding of the CLS token (i.e., a special classification token used for classification tasks) from the last  hidden layer of the fine-tuned RoBERTa model for each segment and use them as our feature vectors, which are input to the LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Training</head><p>After feature extraction, we train a single layer LSTM network with a hidden unit size of 100 and a batch size of 8. We use Pytorch's implementation of the Stochastic Gradient Descent (SGD) optimizer with learning rate of 0.01 <ref type="bibr" coords="6,287.59,176.99,16.41,10.91" target="#b11">[12]</ref>. In the classification step, we use two fully connected linear layers with sizes 100 and ReLU <ref type="bibr" coords="6,306.29,190.54,17.87,10.91" target="#b12">[13]</ref> as the activation function between them. Furthermore, we broke down the multi-label trigger detection problem into a multiple binary classification problem. Therefore, we train the aforementioned LSTM model for each trigger class (32 in total) in a one-vs-all classification setting. Finally, we use Binary Cross Entropy (BCE) as our loss function.</p><p>As seen in Table <ref type="table" coords="6,174.33,258.29,3.66,10.91" target="#tab_3">2</ref>, the trigger classes are greatly imbalanced. For instance, while the 77.52% of the training data includes pornographic content, only the 0.05% of it consists of animal cruelty. There are many methods such as oversampling the underrepresented classes, that are proposed to solve the class imbalance problem for deep neural networks <ref type="bibr" coords="6,423.25,298.94,16.41,10.91" target="#b13">[14]</ref>. In this study, we solve this problem by changing the weights of the underrepresented classes during the back-propagation updates. We do this by incorporating a weight of positive examples for the loss function. For example, if a dataset contains 20 positive and 1000 negative examples of a single class, then weight for the positive class is assigned as 1000/20 = 50 in the loss function. In this way, the loss would act as if the dataset contains 20 √ó 50 = 1000 positive examples. In our final model, we assign positive class weights to the loss function for the trigger classes from 15 (i.e., kidnapping) to 32 (i.e., animal-cruelty). For the first 14 trigger classes, we do not assign any positive class weights to BCE loss function.</p><p>Finally, for each trigger class, we train the LSTM model up to 10 epochs and save the best performing model with respect to the highest F1 scores achieved on the validation set. We then combine the predictions of the 32 trained LSTM models to produce our final multi-label trigger labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the baseline methods used for multi-label trigger detection in Fanfiction, and share the classification performance of our model on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head><p>‚Ä¢ BERT: We use HuggingFace's implementation of the Transformer-based BERT model <ref type="foot" coords="6,501.78,580.61,3.71,7.97" target="#foot_2">3</ref>and fine-tune this model with the training set for trigger detection. We set learning rate to 1ùëí -5, number of training epochs to 5, and batch size to 8. We use multi-label classification layer with 32 classes on top of the BERT model. We use the corresponding BERT tokenizer with maximum length of 512 tokens, truncating the rest of the document in the training set. ‚Ä¢ RoBERTa-Segment: We use the same RoBERTa model that is fine-tuned with the segmented tokens described in Section 4. ‚Ä¢ TFIDF+XGBoost: This baseline is shared by the organizers at PAN CLEF 2023 for trigger detection <ref type="bibr" coords="7,161.93,262.58,13.00,10.91" target="#b6">[7]</ref> <ref type="foot" coords="7,174.93,260.83,3.71,7.97" target="#foot_3">4</ref> . It uses Gradient Boosted Trees <ref type="bibr" coords="7,330.33,262.58,18.07,10.91" target="#b14">[15]</ref> based on a TF-IDF <ref type="bibr" coords="7,439.35,262.58,18.07,10.91" target="#b15">[16]</ref> document vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Validation Results</head><p>Multi-label F1-macro and F1-micro scores serve as the primary performance metrics for trigger detection at PAN CLEF 2023<ref type="foot" coords="7,217.02,337.65,3.71,7.97" target="#foot_4">5</ref> . To assess the effectiveness of our model and the baselines, we compute these scores on the validation set, and the results are presented in Table <ref type="table" coords="7,458.48,352.96,3.81,10.91" target="#tab_5">3</ref>. Notably, our model outperforms all others in terms of multi-label F1-macro and F1-micro scores.</p><p>The limitations of the BERT model become evident as it struggles due to the constrained token size of 512. Truncating long documents reduces its ability to capture triggering content effectively. While our RoBERTa-Segment baseline improves upon BERT, it still falls short in performance since it lacks the crucial hierarchical recurrence concept integrated into its core architecture.</p><p>Among the baselines, the TFIDF+XGBoost approach achieves the highest F1-macro and F1-micro scores. Despite the absence of contextual information, XGBoost compensates by leveraging TF-IDF vector representations, enabling comprehensive coverage of tokens throughout the entire document.</p><p>These results underscore the existence of ample room for improvement in the non-trivial task of multi-label trigger detection in Fanfiction. It is evident that further improvements are necessary to enhance the effectiveness of trigger detection methods and address the challenges associated with this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Class-based Validation Results</head><p>Table <ref type="table" coords="7,115.04,592.37,4.97,10.91">4</ref> shows the binary classification performances of the trained models for each trigger class on the validation set. It is worth noting that by incorporating a positive class weight into the loss function for the classes from 15 to 32, where positive instances are particularly scarce, the model's capability to predict positive class instances is significantly enhanced. Consequently, this leads to an anticipated improvement in the overall performance of multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Various binary classification scores of the proposed method (32 hierarchical LSTM models over Transformer-based RoBERTa) on the validation set for trigger detection. Pos. Ratio indicates the true positive class ratio in the validation set. Pos. Pred. Ratio indicates the positive class ratio predicted by the proposed method. The classification performance computed in terms of macro and micro F1, Precision (P), and Recall (R) scores. Overall multi-label classification performances are given at the bottom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Pos. Ratio F1-macro P-macro R-macro F1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Test Results</head><p>We submitted our model as a dockerized image to the TIRA system <ref type="bibr" coords="8,380.94,621.05,16.08,10.91" target="#b16">[17]</ref>. The test was conducted on a hardware configuration consisting of a single CPU Core, 10GB of RAM, and a single Nvidia GTX 1080 with 8GB. The test completion time was approximately 150 minutes. The final test results of all the participants for PAN CLEF 2023 Trigger Detection are presented in Table <ref type="table" coords="8,500.04,661.70,3.81,10.91">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The leaderboard in terms of the F1-macro and F1-micro test scores of all participants at PAN CLEF 2023 Trigger Detection. Our team's name is pan23-transformers. trigger-detection-baseline is the TF-IDF+XGBoost model explained in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This study presents an approach for detecting triggers in Fanfiction by employing natural language processing (NLP) and machine learning techniques. Our objective is to train a classification algorithm capable of accurately identifying multiple instances of triggering content.</p><p>In our method, we initially break down lengthy Fanfiction documents into smaller segments, ensuring an overlap between consecutive segments. These segments are then used to fine-tune a Transformer-based language model. From this fine-tuned model, we extract feature embeddings for each segment, which serve as inputs for training multiple LSTM models. Subsequently, the predictions of these trained LSTM models are combined to generate trigger labels for multi-class and multi-label classification. We show that our method that is based on hierarchical recurrence over Transform-based model achieves better classification performance than the baselines used for multi-label trigger detection in Fanfiction. Our model ranks first in terms of the multi-label F1-macro score and second in terms of the multi-label F1-micro score on the test set for PAN CLEF 2023 Trigger Detection. Furthermore, Our experimental findings strongly indicate that conventional NLP techniques, such as TF-IDF document vectorization and Transformer-based models with standard tokenization limits (typically set at a maximum length of 512 tokens), exhibit limited performance in the context of multi-class and multi-label classification tasks, particularly when dealing with lengthy documents. These techniques often struggle to effectively handle the complexities associated with the simultaneous prediction of multiple trigger labels in scenarios where extensive text is involved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,612.70,416.69,8.93;5,89.29,624.43,418.22,9.14;5,89.29,636.66,416.69,8.87;5,89.29,648.61,416.69,8.87;5,89.29,660.57,416.70,8.87;5,89.29,672.52,416.70,8.87;5,89.29,684.48,394.81,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hierarchical recurrence over Transformer-based model diagram for trigger detection. Each document is split into 200-word segments with a 50-word overlap between the consecutive segments.ROBERTa<ref type="bibr" coords="5,132.74,636.66,16.54,8.87" target="#b10">[11]</ref> along with its tokenizer (256 tokens) is used as the Transformer model and fine-tuned using all the segmented documents in the training set. Transformer model and its tokenizer are shared throughout the network. CLS embeddings are extracted for each segment of a given document, which are then used as input feature vectors for the training of the LSTM model. For the classifier, two fully-connected linear layers with ReLU activation and binary cross-entropy (BCE) loss are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,461.27,24.58,8.93"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,88.93,473.28,417.05,81.81"><head></head><label></label><figDesc>Trigger detection dataset shared at PAN CLEF 2023. The number of documents, average number of words in each document and presence of labels for training, validation, and test sets are given.</figDesc><table coords="2,196.18,504.88,202.92,50.21"><row><cell></cell><cell cols="3">#Document #Avg. Words Labels</cell></row><row><cell>Train</cell><cell>307,102</cell><cell>2,350</cell><cell>‚úì</cell></row><row><cell>Validation</cell><cell>17,104</cell><cell>2,336</cell><cell>‚úì</cell></row><row><cell>Test</cell><cell>17,040</cell><cell>2,338</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,88.99,156.62,373.24,428.57"><head>Table 2</head><label>2</label><figDesc>Class distribution ratios with respect to 32 trigger labels in the training and validation sets.</figDesc><table coords="3,210.12,188.24,175.04,396.95"><row><cell>Class</cell><cell cols="2">Train Validation</cell></row><row><cell>1 pornographic</cell><cell>77.52%</cell><cell>77.33%</cell></row><row><cell>2 violence</cell><cell>9.48%</cell><cell>9.46%</cell></row><row><cell>3 death</cell><cell>6.77%</cell><cell>6.75%</cell></row><row><cell>4 sexual-assault</cell><cell>10.20%</cell><cell>10.18%</cell></row><row><cell>5 abuse</cell><cell>7.22%</cell><cell>7.21%</cell></row><row><cell>6 blood</cell><cell>4.92%</cell><cell>4.90%</cell></row><row><cell>7 suicide</cell><cell>2.67%</cell><cell>2.67%</cell></row><row><cell>8 pregnancy</cell><cell>4.44%</cell><cell>4.44%</cell></row><row><cell>9 child-abuse</cell><cell>2.34%</cell><cell>2.34%</cell></row><row><cell>10 incest</cell><cell>4.39%</cell><cell>4.38%</cell></row><row><cell>11 underage</cell><cell>2.90%</cell><cell>2.89%</cell></row><row><cell>12 homophobia</cell><cell>1.61%</cell><cell>1.61%</cell></row><row><cell>13 self-harm</cell><cell>1.71%</cell><cell>1.71%</cell></row><row><cell>14 dying</cell><cell>2.44%</cell><cell>2.44%</cell></row><row><cell>15 kidnapping</cell><cell>1.46%</cell><cell>1.45%</cell></row><row><cell>16 mental-illness</cell><cell>1.36%</cell><cell>1.36%</cell></row><row><cell>17 dissection</cell><cell>0.56%</cell><cell>0.55%</cell></row><row><cell>18 eating-disorder</cell><cell>0.39%</cell><cell>0.40%</cell></row><row><cell>19 abduction</cell><cell>0.35%</cell><cell>0.34%</cell></row><row><cell>20 body-hatred</cell><cell>0.44%</cell><cell>0.44%</cell></row><row><cell>21 childbirth</cell><cell>0.28%</cell><cell>0.28%</cell></row><row><cell>22 racism</cell><cell>0.13%</cell><cell>0.13%</cell></row><row><cell>23 sexism</cell><cell>0.17%</cell><cell>0.17%</cell></row><row><cell>24 miscarriage</cell><cell>0.16%</cell><cell>0.17%</cell></row><row><cell>25 transphobia</cell><cell>0.12%</cell><cell>0.12%</cell></row><row><cell>26 abortion</cell><cell>0.11%</cell><cell>0.12%</cell></row><row><cell>27 fat-phobia</cell><cell>0.24%</cell><cell>0.24%</cell></row><row><cell>28 animal-death</cell><cell>0.06%</cell><cell>0.07%</cell></row><row><cell>29 ableism</cell><cell>0.09%</cell><cell>0.09%</cell></row><row><cell>30 classism</cell><cell>0.06%</cell><cell>0.06%</cell></row><row><cell>31 misogyny</cell><cell>0.07%</cell><cell>0.08%</cell></row><row><cell>32 animal-cruelty</cell><cell>0.05%</cell><cell>0.05%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.99,90.49,417.00,105.75"><head>Table 3</head><label>3</label><figDesc>F1-macro and F1-micro scores of our model and baseline methods. The scores are computed on the validation set provided by PAN CLEF 2023.</figDesc><table coords="7,172.59,134.06,250.10,62.17"><row><cell>Model</cell><cell cols="2">F1-macro F1-micro</cell></row><row><cell>BERT</cell><cell>0.0471</cell><cell>0.4607</cell></row><row><cell>RoBERTa-Segment</cell><cell>0.1869</cell><cell>0.6958</cell></row><row><cell>TF-IDF + XGBoost (task baseline)</cell><cell>0.2575</cell><cell>0.7274</cell></row><row><cell>Hierarchical LSTM + RoBERTa (ours)</cell><cell>0.3720</cell><cell>0.7360</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,94.42,217.87,404.33,350.85"><head></head><label></label><figDesc>-micro P-micro R-micro Pos. Pred. Ratio</figDesc><table coords="8,94.42,232.77,404.33,335.94"><row><cell>pornographic</cell><cell>0.773</cell><cell>0.901</cell><cell>0.906</cell><cell>0.897</cell><cell>0.932</cell><cell>0.932</cell><cell>0.932</cell><cell>0.781</cell></row><row><cell>violence</cell><cell>0.095</cell><cell>0.715</cell><cell>0.744</cell><cell>0.694</cell><cell>0.912</cell><cell>0.912</cell><cell>0.912</cell><cell>0.074</cell></row><row><cell>death</cell><cell>0.068</cell><cell>0.781</cell><cell>0.802</cell><cell>0.763</cell><cell>0.948</cell><cell>0.948</cell><cell>0.948</cell><cell>0.058</cell></row><row><cell>sexual-assault</cell><cell>0.102</cell><cell>0.748</cell><cell>0.784</cell><cell>0.722</cell><cell>0.918</cell><cell>0.918</cell><cell>0.918</cell><cell>0.078</cell></row><row><cell>abuse</cell><cell>0.072</cell><cell>0.727</cell><cell>0.768</cell><cell>0.699</cell><cell>0.936</cell><cell>0.936</cell><cell>0.936</cell><cell>0.052</cell></row><row><cell>blood</cell><cell>0.049</cell><cell>0.758</cell><cell>0.791</cell><cell>0.733</cell><cell>0.959</cell><cell>0.959</cell><cell>0.959</cell><cell>0.039</cell></row><row><cell>suicide</cell><cell>0.027</cell><cell>0.797</cell><cell>0.841</cell><cell>0.764</cell><cell>0.981</cell><cell>0.981</cell><cell>0.981</cell><cell>0.021</cell></row><row><cell>pregnancy</cell><cell>0.044</cell><cell>0.882</cell><cell>0.883</cell><cell>0.881</cell><cell>0.980</cell><cell>0.980</cell><cell>0.980</cell><cell>0.044</cell></row><row><cell>child-abuse</cell><cell>0.023</cell><cell>0.726</cell><cell>0.751</cell><cell>0.705</cell><cell>0.977</cell><cell>0.977</cell><cell>0.977</cell><cell>0.019</cell></row><row><cell>incest</cell><cell>0.044</cell><cell>0.837</cell><cell>0.835</cell><cell>0.839</cell><cell>0.973</cell><cell>0.973</cell><cell>0.973</cell><cell>0.044</cell></row><row><cell>underage</cell><cell>0.029</cell><cell>0.681</cell><cell>0.744</cell><cell>0.645</cell><cell>0.971</cell><cell>0.971</cell><cell>0.971</cell><cell>0.017</cell></row><row><cell>homophobia</cell><cell>0.016</cell><cell>0.711</cell><cell>0.750</cell><cell>0.682</cell><cell>0.984</cell><cell>0.984</cell><cell>0.984</cell><cell>0.012</cell></row><row><cell>self-harm</cell><cell>0.017</cell><cell>0.795</cell><cell>0.871</cell><cell>0.745</cell><cell>0.989</cell><cell>0.989</cell><cell>0.989</cell><cell>0.011</cell></row><row><cell>dying</cell><cell>0.024</cell><cell>0.678</cell><cell>0.735</cell><cell>0.644</cell><cell>0.975</cell><cell>0.975</cell><cell>0.975</cell><cell>0.015</cell></row><row><cell>kidnapping</cell><cell>0.015</cell><cell>0.618</cell><cell>0.791</cell><cell>0.576</cell><cell>0.986</cell><cell>0.986</cell><cell>0.986</cell><cell>0.004</cell></row><row><cell>mental-illness</cell><cell>0.014</cell><cell>0.598</cell><cell>0.566</cell><cell>0.787</cell><cell>0.942</cell><cell>0.942</cell><cell>0.942</cell><cell>0.062</cell></row><row><cell>dissection</cell><cell>0.006</cell><cell>0.583</cell><cell>0.552</cell><cell>0.771</cell><cell>0.971</cell><cell>0.971</cell><cell>0.971</cell><cell>0.029</cell></row><row><cell>eating-disorder</cell><cell>0.004</cell><cell>0.756</cell><cell>0.700</cell><cell>0.858</cell><cell>0.995</cell><cell>0.995</cell><cell>0.995</cell><cell>0.007</cell></row><row><cell>abduction</cell><cell>0.003</cell><cell>0.576</cell><cell>0.548</cell><cell>0.697</cell><cell>0.985</cell><cell>0.985</cell><cell>0.985</cell><cell>0.014</cell></row><row><cell>body-hatred</cell><cell>0.004</cell><cell>0.639</cell><cell>0.595</cell><cell>0.765</cell><cell>0.988</cell><cell>0.988</cell><cell>0.988</cell><cell>0.012</cell></row><row><cell>childbirth</cell><cell>0.003</cell><cell>0.683</cell><cell>0.621</cell><cell>0.882</cell><cell>0.993</cell><cell>0.993</cell><cell>0.993</cell><cell>0.009</cell></row><row><cell>racism</cell><cell>0.001</cell><cell>0.605</cell><cell>0.633</cell><cell>0.587</cell><cell>0.998</cell><cell>0.998</cell><cell>0.998</cell><cell>0.001</cell></row><row><cell>sexism</cell><cell>0.002</cell><cell>0.577</cell><cell>0.563</cell><cell>0.599</cell><cell>0.996</cell><cell>0.996</cell><cell>0.996</cell><cell>0.003</cell></row><row><cell>miscarriage</cell><cell>0.002</cell><cell>0.694</cell><cell>0.662</cell><cell>0.741</cell><cell>0.997</cell><cell>0.997</cell><cell>0.997</cell><cell>0.003</cell></row><row><cell>transphobia</cell><cell>0.001</cell><cell>0.722</cell><cell>0.682</cell><cell>0.785</cell><cell>0.998</cell><cell>0.998</cell><cell>0.998</cell><cell>0.002</cell></row><row><cell>abortion</cell><cell>0.001</cell><cell>0.531</cell><cell>0.523</cell><cell>0.549</cell><cell>0.997</cell><cell>0.997</cell><cell>0.997</cell><cell>0.002</cell></row><row><cell>fat-phobia</cell><cell>0.002</cell><cell>0.780</cell><cell>0.780</cell><cell>0.780</cell><cell>0.998</cell><cell>0.998</cell><cell>0.998</cell><cell>0.002</cell></row><row><cell>animal-death</cell><cell>0.001</cell><cell>0.558</cell><cell>0.545</cell><cell>0.583</cell><cell>0.998</cell><cell>0.998</cell><cell>0.998</cell><cell>0.001</cell></row><row><cell>ableism</cell><cell>0.001</cell><cell>0.535</cell><cell>0.538</cell><cell>0.533</cell><cell>0.999</cell><cell>0.999</cell><cell>0.999</cell><cell>0.001</cell></row><row><cell>classism</cell><cell>0.001</cell><cell>0.500</cell><cell>0.500</cell><cell>0.500</cell><cell>0.999</cell><cell>0.999</cell><cell>0.999</cell><cell>0.000</cell></row><row><cell>misogyny</cell><cell>0.001</cell><cell>0.501</cell><cell>0.503</cell><cell>0.604</cell><cell>0.976</cell><cell>0.976</cell><cell>0.976</cell><cell>0.024</cell></row><row><cell>animal-cruelty</cell><cell>0.001</cell><cell>0.505</cell><cell>0.505</cell><cell>0.658</cell><cell>0.982</cell><cell>0.982</cell><cell>0.982</cell><cell>0.018</cell></row><row><cell>Multi-label</cell><cell>-</cell><cell>0.3720</cell><cell>0.3920</cell><cell>0.4330</cell><cell>0.7360</cell><cell>0.7330</cell><cell>0.7400</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,89.29,146.01,417.89,187.49"><head></head><label></label><figDesc>-baseline is the TFIDF+XGBoost model explained in Section 5. Furthermore, only the submission with the highest F1-macro score was included for teams with multiple submissions. At the end, our team, named pan23-transformers, achieved first place in terms of the multi-label F1-macro score and second place in terms of the multi-label F1-micro score in the leaderboard with our hierarchical recurrence over Transformer-based language model.</figDesc><table coords="9,89.29,146.01,326.92,133.12"><row><cell></cell><cell cols="2">F1-macro F1-micro</cell></row><row><cell>pan23-transformers (Ours)</cell><cell>0.352</cell><cell>0.737</cell></row><row><cell>pan23-supergirl</cell><cell>0.350</cell><cell>0.753</cell></row><row><cell>trigger-detection-baseline</cell><cell>0.301</cell><cell>0.689</cell></row><row><cell>pan23-jojo-no-kimyou-na-bouken</cell><cell>0.228</cell><cell>0.557</cell></row><row><cell>pan23-marvel-cinematic-universe</cell><cell>0.225</cell><cell>0.616</cell></row><row><cell>pan23-sherlock</cell><cell>0.161</cell><cell>0.402</cell></row><row><cell>pan23-game-of-thrones</cell><cell>0.048</cell><cell>0.625</cell></row><row><cell>trigger-detection</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.03,224.64,8.97"><p>https://pan.webis.de/clef23/pan23-web/trigger-detection.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,671.04,130.35,8.97"><p>https://huggingface.co/roberta-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,92.57,671.04,151.16,8.97"><p>https://huggingface.co/bert-base-uncased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,92.57,660.04,326.03,8.97"><p>https://github.com/pan-webis-de/pan-code/tree/master/clef23/trigger-detection/baselines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,92.57,671.00,224.64,8.97"><p>https://pan.webis.de/clef23/pan23-web/trigger-detection.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,111.28,393.33,10.91;10,112.66,124.83,393.33,10.91;10,112.14,138.38,193.68,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,358.43,111.28,147.56,10.91;10,112.66,124.83,101.83,10.91">Hierarchical transformers for long document classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.91,124.83,269.07,10.91;10,112.14,138.38,78.76,10.91">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,151.93,393.32,10.91;10,112.66,165.48,394.62,10.91;10,112.66,179.03,394.53,10.91;10,112.66,192.57,90.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,301.52,151.93,204.47,10.91;10,112.66,165.48,105.24,10.91">Revisiting transformer-based models for long document classification</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.41,165.48,265.86,10.91;10,112.66,179.03,241.06,10.91">Findings of the Association for Computational Linguistics: EMNLP 2022, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7212" to="7230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,206.12,394.61,10.91;10,112.66,219.67,393.32,10.91;10,112.33,233.22,394.86,10.91;10,112.66,246.77,249.43,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,226.21,206.12,261.71,10.91">Efficient classification of long documents using transformers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.79</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,219.67,393.32,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,260.32,394.52,10.91;10,112.66,273.87,394.53,10.91;10,112.66,287.42,394.52,10.91;10,112.66,300.97,393.53,10.91;10,112.66,314.52,394.53,10.91;10,112.66,328.07,395.17,10.91;10,112.66,341.62,393.33,10.91;10,112.66,355.17,394.53,10.91;10,112.66,368.71,65.44,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,294.21,287.42,212.97,10.91;10,112.66,300.97,393.53,10.91;10,112.66,314.52,42.05,10.91">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-R√≠os</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pƒôzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,371.28,328.07,136.55,10.91;10,112.66,341.62,393.33,10.91;10,112.66,355.17,197.14,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,342.79,355.17,159.86,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,112.66,382.26,393.60,10.91;10,112.66,395.81,393.33,10.91;10,112.66,409.36,339.30,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,329.21,382.26,177.06,10.91;10,112.66,395.81,30.91,10.91">Overview of the Trigger Detection Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,428.73,395.81,77.26,10.91;10,112.66,409.36,257.93,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,422.91,395.16,10.91;10,112.66,436.46,348.10,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schr√∂der</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.04409</idno>
		<title level="m" coord="10,378.24,422.91,129.58,10.91;10,112.66,436.46,166.26,10.91">Trigger warnings: Bootstrapping a violence detector for fanfiction</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,450.01,393.53,10.91;10,112.14,463.56,393.85,10.91;10,112.66,477.11,394.62,10.91;10,112.66,490.66,358.82,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,472.37,450.01,33.82,10.91;10,112.14,463.56,316.65,10.91">Trigger Warning Assignment as a Multi-Label Document Classification Problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schr√∂der</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,452.19,463.56,53.80,10.91;10,112.66,477.11,344.15,10.91">Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,504.21,393.33,10.91;10,112.66,517.76,363.59,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,504.21,152.55,10.91;10,112.66,517.76,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,531.30,393.60,10.91;10,112.26,544.85,395.41,10.91;10,112.66,558.40,262.27,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,230.95,531.30,275.31,10.91;10,112.26,544.85,125.80,10.91">Named entity recognition in Turkish: A comparative study with detailed error analysis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozcelik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.103065</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.103065" />
	</analytic>
	<monogr>
		<title level="j" coord="10,250.67,544.85,174.29,10.91">Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">103065</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,571.95,393.33,10.91;10,112.66,585.50,260.12,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,226.29,571.95,279.70,10.91;10,112.66,585.50,129.27,10.91">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,599.05,395.17,10.91;10,112.66,612.60,393.32,10.91;10,112.33,626.15,129.27,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,140.43,612.60,261.00,10.91">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,639.70,394.53,10.91;10,112.66,653.25,333.69,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,200.95,653.25,159.77,10.91">Automatic differentiation in pytorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NIPS-W</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,666.80,395.01,10.91;11,112.66,89.41,97.35,7.90" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,172.91,666.80,203.72,10.91">Deep learning using rectified linear units (relu)</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,393.32,10.91;11,112.66,114.06,318.49,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,282.56,100.52,223.42,10.91;11,112.66,114.06,146.02,10.91">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,267.50,114.06,74.64,10.91">Neural networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,394.52,10.91;11,112.66,154.71,80.57,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,212.44,127.61,184.59,10.91">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.52,127.61,84.47,10.91;11,112.66,141.16,389.61,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.61,10.91;11,112.66,181.81,70.52,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,208.87,168.26,204.74,10.91">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>McGraw-Hill Book Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,394.52,10.91;11,112.66,208.91,394.62,10.91;11,112.48,222.46,394.70,10.91;11,112.28,236.01,393.71,10.91;11,112.66,249.56,393.33,10.91;11,112.66,263.11,129.64,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,174.00,208.91,296.53,10.91">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,192.86,236.01,313.12,10.91;11,112.66,249.56,95.19,10.91">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="11,215.26,249.56,158.83,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
