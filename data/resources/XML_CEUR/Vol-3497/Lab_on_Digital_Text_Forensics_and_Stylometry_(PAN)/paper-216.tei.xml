<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,319.38,15.42;1,89.29,106.66,181.07,15.42;1,89.29,129.00,157.29,11.96">Enhancing Authorship Verification using Sentence-Transformers Notebook for PAN at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,78.78,11.96"><forename type="first">Momen</forename><surname>Ibrahim</surname></persName>
							<email>momen.ibrahim2019@alexu.edu.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.72,154.90,72.44,11.96"><forename type="first">Ahmed</forename><surname>Akram</surname></persName>
							<email>ahmed.akram2018@alexu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.80,154.90,100.71,11.96"><forename type="first">Mohammed</forename><surname>Radwan</surname></persName>
							<email>mohamed.radwan2000@alexu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.16,154.90,62.49,11.96"><forename type="first">Rana</forename><surname>Ayman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,121.25,11.96"><forename type="first">Mustafa</forename><surname>Abd-El-Hameed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.18,168.85,84.93,11.96"><forename type="first">Nagwa</forename><surname>El-Makky</surname></persName>
							<email>nagwamakky@alexu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.11,168.85,70.20,11.96"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
							<email>mtorki@alexu.edu.eg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Systems Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Alexandria University</orgName>
								<address>
									<settlement>Alexandria</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,319.38,15.42;1,89.29,106.66,181.07,15.42;1,89.29,129.00,157.29,11.96">Enhancing Authorship Verification using Sentence-Transformers Notebook for PAN at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8EAAE8C8B6F7B19C5BE2FAD865A5AC17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Authorship Verification</term>
					<term>pre-trained language models</term>
					<term>Sentence-Transformers</term>
					<term>SBERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship verification is a growing field of research that aims to determine whether two texts were written by the same author or different authors. In this paper, we describe our system for the PAN@CLEF 2023 Authorship Verification challenge <ref type="bibr" coords="1,275.94,256.97,10.57,8.97" target="#b0">[1]</ref> which requires solving the task on a cross-Discourse Types and open-set collection of essays (written discourse), emails (written discourse), Interviews (spoken discourse), and Speech transcriptions (spoken discourse). We use Sentence-Transformers which is a popular framework for generating sentence embedding based on the idea of using pre-trained language models, such as BERT (Bidirectional Encoder Representations from Transformers) to capture the semantic features of different authors' documents. As a result of studying multiple sentence-transformers, we selected all-MiniLM-L12-v2 model and achieved the first rank in PAN 23 with an overall score of 62.3% on the testing set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Authorship Verification (AV) is a specialized area in the field of digital texts that focuses on analyzing and comparing the unique stylistic and linguistic characteristics present in multiple texts. The primary objective of AV is to ascertain whether these texts were authored by the same individual. Its significance has been magnified in recent times due to the abundance of digital content accessible through digital libraries, online journalism platforms, and social media networks.</p><p>AV serves a crucial purpose in various domains where automatic verification of document authorship is essential. With the exponential growth of digital texts available online, the need to accurately determine the authorship of documents has become increasingly prominent. This is particularly relevant in contexts where the evaluation of researchers relies on the impact and quantity of their publications. Similarly, public figures often face scrutiny based on their social media posts. In such scenarios, AV offers a valuable tool to establish the authenticity and authorship of documents.</p><p>The digital landscape, encompassing digital libraries, online journalism, and social media platforms, has witnessed a surge in textual data. However, along with this surge, there has also been an upswing in online crimes. AV plays a pivotal role in addressing these challenges by identifying instances of fraudulent activities, such as phishing emails, and detecting cases of plagiarism. By analyzing stylistic features embedded in texts, including writing style, genre, temperament, sentiment, native language, and gender, AV provides insights into the unique attributes and traits of authors.</p><p>In summary, AV is a specialized field that utilizes advanced techniques to analyze and compare the distinctive patterns and features present in multiple texts, with the aim of determining whether they share a common author. The increasing availability of digital texts and the need for accurate document authorship verification in various domains underscore the significance of AV in today's digital landscape.</p><p>In the authorship verification task at PAN@CLEF-2023 <ref type="bibr" coords="2,342.37,357.95,12.69,10.91" target="#b1">[2]</ref> they introduced more challenging scenarios where each author verification case considers texts from different discourse types. This edition focuses on cross-discourse type authorship verification, including both written (essays and emails) and spoken language (interviews and speech transcriptions). The corpus consists of texts from around 100 individuals, all native English speakers aged 18-22. The topics of the texts are unrestricted, and the level of formality may vary within each discourse type.</p><p>Most of the documents in the training set of PAN23 are emails, interviews and speechtranscriptions which are relatively short documents. This inspired us to use Sentence-Transformers specially SBERT <ref type="bibr" coords="2,162.21,466.34,12.68,10.91" target="#b2">[3]</ref> due to their power in semantic similarity tasks in addition to their efficiency. We evaluated different pre-trained models for their quality to embedded sentences.</p><p>As, the used models are trained for short sentences, this directed us to chunk the documents in the pairs to fit in the max-sequence length of the used pre-trained models. We tried different chuncking techniques. These chunking techniques will be explained in section 4.2. This paper is structured as follows: Section 2 provides a background and related work on authorship verification. Section 3 describes the dataset used in the PAN@CLEF 2023 challenge, including the different discourse types and the number of texts in each category. Section 4 explains the preprocesing step on the dataset and the chunking technique for handling long texts. Section 5.1 presents the methodology used in our system, including the use of Sentence-Transformers for generating sentence embeddings. Section 6 discusses experiments on other models, including a model that uses only stylometric features. Section 7 discusses the results and performance of these models on the validation set. Finally, Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related work</head><p>Reimers and Gurevych (2019) <ref type="bibr" coords="3,223.30,111.28,12.85,10.91" target="#b2">[3]</ref> propose a new method for generating sentence embeddings, which are vector representations of sentences that capture their semantic meaning. The authors use a Siamese neural network architecture, based on the BERT model, to generate embeddings that are optimized for use in tasks such as semantic similarity and paraphrase detection. The model is trained on a large corpus of text data, and the resulting embeddings are shown to outperform previous state-of-the-art methods on several benchmark datasets. The authors also introduce a new technique for fine-tuning the model on smaller datasets, which further improves its performance on specific tasks. In PAN@CLEF 2021, <ref type="bibr" coords="3,376.80,206.12,75.69,10.91" target="#b3">Peng et al. (2021)</ref>  <ref type="bibr" coords="3,455.22,206.12,12.79,10.91" target="#b3">[4]</ref> propose a method that utilizes a pre-trained model to encode text information to solve the authorship verification in the PAN@CLEF 2021. To resolve the problem of long text encoding, the method proposed is to split long texts into short texts that a pre-trained model, BERT, can encode. The classification model achieved the highest c@1 and F1-score on the small dataset of PAN Authorship Verification datasets. Accordingly, the approach described can encode long text information efficiently in long text pairs. Stylometric feature extraction is another approach proposed by, Weerasinghe et al. (2021) <ref type="bibr" coords="3,493.31,300.97,12.68,10.91" target="#b4">[5]</ref> The authors demonstrate the impact of different feature representations on the performance of the proposed method of computing the absolute difference between the feature vectors as input to the logistic regression classifier for each document pair, they evaluate several feature sets, including word and character n-grams, punctuation usage, and syntactic features.</p><p>BERT-like transformers for authorship verification also perform well in this task. <ref type="bibr" coords="3,457.98,368.71,48.00,10.91;3,89.29,382.26,51.66,10.91" target="#b5">Manolache et al., (2021)</ref>  <ref type="bibr" coords="3,143.32,382.26,12.69,10.91" target="#b5">[6]</ref> explore the use of them for authorship verification, The authors evaluate several transformers on the PAN-2020 dataset and find that they achieve high performance, but also they show through Integrated Gradients XAI technique that, they rely on topical clues rather than stylistic features. To address this issue, the authors propose new splits for the dataset that ensure topic and author diversity and show that they improve the models' domain generalization ability. The authors also introduce a new dataset, DarkReddit, that contains texts from different domains and genres, and use it to test the models in a low-data regime.</p><p>Galicia et al.,(2022) <ref type="bibr" coords="3,183.08,477.11,11.85,10.91" target="#b6">[7]</ref> proposed a Graph-based Siamese Network approach for the authorship verification task at PAN 2022 <ref type="bibr" coords="3,220.27,490.66,11.40,10.91" target="#b7">[8]</ref>. They also introduced a way to split PAN22 dataset to ensure that the training and validation sets are both balanced and author-disjoint. This is important to avoid misleading results that may arise from data leakage or overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Description</head><p>The evolution of authorship verification tasks in previous editions of PAN@CLEF competitions is as follows. Initially, the task focused on comparing writing styles in different languages and genres. Later, cross-domain authorship verification using fan-fiction texts was explored and found to be feasible across different fandom. In the 2022 edition, more challenging scenarios were introduced, involving cross-DT authorship verification, where texts belonged to different discourse types (DTs), all within the realm of written language. The current edition focuses on cross-discourse type authorship verification, where both written language (essays and emails) and spoken language (interviews and speech transcriptions) are included as discourse types.</p><p>The dataset provided by PAN is a collection of English texts from different Discourse Types (DT). The goal of the task is to determine if two texts from different DTs are written by the same author. The dataset has a tag for each pair of texts indicating the authorship, the author, and the discourse type of each text. Since we augment the training set of PAN 23 by the training set of PAN 22. The next subsections describe both of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Authorship Verification Dataset at PAN 2022</head><p>In that dataset the organizers provide cross-DT authorship verification cases using DTs corresponding to written language. The discourse types and their corresponding number of texts in the train dataset are: The corpus consists of texts from approximately 100 individuals who are native English speakers and share a similar age range of 18-22. The text samples cover a wide range of topics, without any restrictions, and within each discourse type, there can be variations in the level of formality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Authorship Verification Dataset at PAN 2023</head><p>In that dataset the organizers provide cross-DT authorship verification cases using DTs not only corresponding to written language but also to spoken language.</p><p>The discourse types and their corresponding number of texts in the train dataset are:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>Based on the strong similarity between the two tasks of PAN 22 and PAN 23, and because the number of pairs in the new task of PAN 23 is too small, we decided to combine both train datasets into one train dataset. And the new number of combinations in that combined dataset are shown in Table <ref type="table" coords="5,460.92,629.40,3.74,10.91" target="#tab_5">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Preprocessing</head><p>This section describes the pre-processing operations we did on the combined dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Splitting</head><p>We used an author-disjoint method similar to the one introduced in <ref type="bibr" coords="6,389.11,368.70,12.74,10.91" target="#b6">[7]</ref> to split the dataset into training and validation sets for our model evaluation. This means that no text from one set has the same author as any text from another set. Since the dataset had 21,100 problems and only 112 authors, this method resulted in unbalanced sets, with more positive problems than negative ones. To address this issue, we generated new negative instances by pairing texts from different authors. We followed these steps: Let A and B be the subsets of texts from each set grouped by author. We obtained positive and negative pairs by computing the Cartesian products P = A × A and N = A × B respectively. Then, we removed pairs that had the same DT, and finally randomly sampled positive and negative pairs from P and N sets to balance the training and validation sets. The final dataset had an equal number of true and false problems. The total number of problems and the number of problems in the positive class, the number of texts and authors on each partition are shown in Table <ref type="table" coords="6,335.32,517.74,3.74,10.91" target="#tab_6">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Texts Chunking</head><p>The sentence transformer is based on a Siamese Network that takes two texts (documents) as input. The length of document 1 and/or document 2 can be larger than the max-sequencelength,K, allowed by the sentence -transformer model. So, for training, we chunk each document into a set of chunks , each of size K. We combine the first 2 chunks of the 2 documents, the second 2 chunks, etc. This creates for each original pair, M pairs, where M is the number of chucks of the larger document. The chunking at evaluation time is described in section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Architecture</head><p>We chose all-MiniLM-L12-v2 as our model. It is a Siamese network, based on BERT-like pretrained model. It belongs to the family of sentence transformers. It uses a contrastive learning objective and has a max sequence length of 256 tokens. It is a lightweight model with a size of 120 MB. However, it provides high-quality semantic similarity in addition to its efficiency.</p><p>Figure <ref type="figure" coords="7,120.36,309.07,5.07,10.91" target="#fig_0">1</ref> shows the model architecture at training and at inference times. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Embedding Layer</head><p>It is the first layer of the model, responsible for converting input tokens into dense vector representations (embeddings). BERT-like models typically use a combination of token embeddings, segment embeddings, and position embeddings. The token embeddings are learned representations specific to each token in the model's vocabulary. Segment embeddings are used to distinguish different sentences, and position embeddings give the model a sense of the order of words in a sentence since transformers don't have a built-in understanding of sequence order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Transformer Encoder Layers</head><p>These are the core of the model. Each transformer encoder layer consists of two sub-layers:</p><p>• Self-Attention Mechanism (Multi-Head Attention): The purpose of the self-attention mechanism is to compute a representation of each word that takes into account the influence of other words in the sentence.</p><p>In each attention head, for every word, the model calculates a score (attention score) that signifies how much focus should be placed on other words. These scores determine how much each word will contribute to the final representation of the current word. The mechanism performs the following steps:</p><p>-Query, Key, and Value Vectors: Each input word is transformed into Query, Key, and Value vectors using learned linear transformations. -Attention Score: An attention score is computed for each pair of words. This score is calculated as the dot product of the Query vector of the current word and the Key vector of the other word, followed by a scaling operation (dividing by the square root of the dimension of the key vector). -Softmax Normalization: The attention scores are then passed through a softmax function to obtain the weights, which ensures they are positive and sum to 1. -Weighted Sum: Finally, a weighted sum of the Value vectors of all words is computed, where the weights are the softmax-normalized attention scores.</p><p>This process is done in parallel in each attention head. Each head may potentially learn to pay attention to different types of connections between words. The output from each head is concatenated and linearly transformed to produce the final output of the multi-head attention layer. • Feed-Forward Neural Network: The output from the self-attention mechanism for each position passes through this layer. It's a simple feed-forward neural network that consists of two fully connected layers with a ReLU activation function in between. This network doesn't have recurrent or convolutional connections, and it operates independently on each position, treating each position identically. This is where most of the model's parameters reside.</p><p>Apart from these, there are also residual connections around both the self-attention and FFNN components, followed by layer normalization. The residual connections help in preventing the vanishing gradient problem and enable the model to be deeper. Layer normalization is used to stabilize the network's learning process and reduce training time.</p><p>These components work together to create a powerful model that can capture complex patterns in the input data. The stacking of multiple layers allows the model to learn more abstract and high-level features as the information propagates up the layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Pooler Layer</head><p>Once the transformer layers have processed the input, the final step in the model architecture is to convert the output into a fixed-size sentence embedding. This is achieved by applying a pooling operation to the output of the transformer layers. The output is a single vector that represents the entire input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>As can be seen from the model architecture (Figure <ref type="figure" coords="9,330.18,253.54,4.25,10.91" target="#fig_0">1</ref>) the output at evaluation time is the cosine similarity of two chunks from the two documents. At the evaluation, the first chunk of K tokens of each document is compared, the second grouping of K tokens from each document is compared, etc. Finally, we take the average of the M scores as the final cosine similarity. Since the cosine similarity range is from -1 to 1, we re-scale the score to be from 0 to 1 using the approach proposed in the baseline model of PAN 22. During training, we optimize two threshold values p1 and p2. All evaluation scores lower than p1 correspond to negative answers, and all evaluation scores greater than p2 are scaled to positive answers. The remaining scores (between p1 and p2) are set to 0.5 and are left unanswered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hyper-parameter Tuning</head><p>Hyperparameter tuning plays a crucial role in optimizing the performance of the model. In this section, we outline the hyperparameters used in our experiments and describe the process of tuning them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Model Configuration</head><p>We employed the "all-MiniLM-L12-v2" model architecture (with 256 tokens as max-seq-length) for our experiments. This model has shown promising results in various natural language processing tasks and is pre-trained on a large corpus of text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Hyper-parameters</head><p>The list below includes the hyper-parameters considered, together with the values selected after fine tuning, using the validation set.</p><p>• Chunk Length: We set the chunk length to 256, which determines the maximum length of text chunks used during training and evaluation. • Batch Size: The batch size was set to 32, which determines the number of training samples processed in parallel during each iteration. • Distance Metric: We used the cosine distance metric to measure the similarity between text embeddings.</p><p>• Loss Function: We utilized the ContrastiveLoss function as the loss function for training the model. • Epochs: We trained the model for 189 epochs, representing the number of times the entire dataset was iterated during training. • Scheduler: The learning rate scheduler was set to 'warmuplinear', which gradually increases the learning rate during the warm-up phase and then linearly decays it. • Warmup Steps: The warm-up steps were set to 0, indicating that no warm-up phase was employed. • Learning Rate: We set the initial learning rate to 2e-05, which determines the step size during gradient descent optimization. • eps: The epsilon value was set to 1e-06, which prevents division by zero in certain calculations. • Thresholds P1&amp;P2: The thresholds values that were used to leave difficult cases unanswered as explained in section 5.2 as, they were set to 0.45 and 0.54 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Hyper-parameter Tuning Process</head><p>The process of hyper-parameter tuning involved a combination of manual exploration and iterative experimentation. We started with initial values based on previous studies and gradually refined them through empirical evaluation.</p><p>We performed a grid search on the validation set to explore different combinations of hyperparameters. For each combination, we trained the model and evaluated its performance using the evaluation metrics in section 5.2, such as area under the curve (AUC), C@1 score, F1 score, Brier score, and overall performance.</p><p>Based on the evaluation results, we iteratively adjusted the hyper-parameters to improve the model's performance. This process involved experimenting with different values for specific hyper-parameters while keeping others constant. We repeated this iterative process until we achieved satisfactory performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Performance Comparison</head><p>To assess the impact of hyper-parameter tuning, we compared the performance of the model before and after tuning. The evaluation metrics mentioned in section 5.2 were used to measure and compare the performance across different hyper-parameter configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments on Other Models</head><p>We performed experiments on two other models. The first is nli-distilroberta-base-v2 , which is a sentence-transformer model based on distilroberta. It has a max-sequence-length of 512 tokens. The second model is the Stylometric model proposed in <ref type="bibr" coords="10,368.78,618.21,11.27,10.91" target="#b4">[5]</ref>. The results of these models on PAN 23 validation set are given in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Table <ref type="table" coords="11,117.38,111.28,5.17,10.91" target="#tab_7">6</ref> shows our results on the Authorship verification PAN 23 validation dataset using all-MiniLM-L12-v2, nli-distilroberta-base-v2 and the Stylometric model. So, the best one is the all-MiniLM-L12-v2 model, which was selected for the submissions. The performance of our three submitted runs on the PAN23 dataset is presented in Table <ref type="table" coords="11,499.56,267.97,3.81,10.91" target="#tab_8">7</ref>, according to the TIRA <ref type="bibr" coords="11,190.71,281.52,12.84,10.91" target="#b8">[9]</ref> website. We submitted three runs on TIRA website: 'resolving-globe','reduced-graph', and 'goldenottoman'. All of them use same version of the trained model (all-MiniLM-L12-v2). The first two runs are the same but, the second run has additional option to take p1 and p2 [described in section 5.2] as input so that we can modify them. And both of them have the same default values and they use the chunking technique described in section 4.2. The third run was to see the effect of soft-chunking on the input data so that there is no sentence split between two chunks unless it has a number of tokens larger than the max sequence length of the model [256 in our case].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we presented the effectiveness of sentence transformers in the authorship verification problem. Based on the result we found that the all-MiniLM-L12-v2 model achieved the best performance on the validation dataset compared to other models like nli-distilroberta-base-v2 and Stylometric model. These results highlight the effectiveness of sentence transformers in tackling authorship verification, showcasing the potential of the all-MiniLM-L12-v2 model for improving the accuracy of such tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,89.29,662.81,122.00,8.93;7,211.78,331.78,171.72,318.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model Architecture</figDesc><graphic coords="7,211.78,331.78,171.72,318.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,388.36,317.52,117.69"><head>Table 1</head><label>1</label><figDesc>Combinations of Different Discourse Types for Each Pair in PAN 22</figDesc><table coords="4,188.76,419.98,217.75,86.08"><row><cell cols="2">Pair Discourse Type Combination Number of Pairs</cell></row><row><cell>('Email', 'Text message')</cell><cell>7484</cell></row><row><cell>('Essay', 'Email')</cell><cell>1618</cell></row><row><cell cols="2">('Business memo', 'Text message') 780</cell></row><row><cell>('Essay', 'Text message')</cell><cell>1182</cell></row><row><cell>('Email', 'Business memo')</cell><cell>1014</cell></row><row><cell>('Essay', 'Business memo')</cell><cell>186</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,107.28,613.18,272.72,55.62"><head></head><label></label><figDesc>And also, the corpus consists of texts from approximately 100 individuals who are native English speakers and share a similar age range of 18-22. The text samples cover a wide range of topics, without any restrictions, and within each discourse type, there can be variations in the level of formality.</figDesc><table coords="4,107.28,613.18,272.72,55.62"><row><cell>• Essays (written discourse): 2594</cell></row><row><cell>• Emails (written discourse): 7054</cell></row><row><cell>• Interviews (spoken discourse): 6090</cell></row><row><cell>• Speech transcriptions (spoken discourse): 1934</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,156.56,320.37,117.69"><head>Table 2</head><label>2</label><figDesc>Combinations of Different Discourse Types for Each Pair in PAN 23</figDesc><table coords="5,185.91,188.18,223.45,86.07"><row><cell>Pair Discourse Type Combination</cell><cell>Number of Pairs</cell></row><row><cell>('Speech transcription', 'Email')</cell><cell>1036</cell></row><row><cell>('Email', 'Interview')</cell><cell>4564</cell></row><row><cell>('Essay', 'Email')</cell><cell>1454</cell></row><row><cell>('Essay', 'Interview')</cell><cell>884</cell></row><row><cell cols="2">('Speech transcription', 'Interview') 642</cell></row><row><cell>('Essay', 'Speech transcription')</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,88.99,382.90,482.05,234.89"><head>Table 3</head><label>3</label><figDesc>Number of problems, texts, authors and topics in the PAN 22 and PAN 23 train datasets.</figDesc><table coords="5,95.27,414.52,475.77,203.27"><row><cell></cell><cell cols="4">Problems Unique Texts Authors Discourse Types</cell></row><row><cell>PAN 22 Train Dataset</cell><cell>12264</cell><cell>1046</cell><cell>56</cell><cell>Essays, Emails, Text messages and Business memos</cell></row><row><cell>PAN 23 Train Dataset</cell><cell>8836</cell><cell>886</cell><cell>56</cell><cell>Essays, Emails, Interviews and Speech transcriptions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Essays, Emails,</cell></row><row><cell>Combined Dataset</cell><cell>21100</cell><cell>1932</cell><cell>112</cell><cell>Text messages, Business memos,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Interviews and Speech transcriptions</cell></row><row><cell cols="5">That new dataset has the following number of texts for each discourse type:</cell></row><row><cell cols="3">• Essays (written discourse): 5580</cell><cell></cell><cell></cell></row><row><cell cols="3">• Emails (written discourse): 17170</cell><cell></cell><cell></cell></row><row><cell cols="4">• Text messages (written discourse): 9446</cell><cell></cell></row><row><cell cols="4">• Business memos (written discourse): 1980</cell><cell></cell></row><row><cell cols="3">• Interviews (spoken discourse): 6090</cell><cell></cell><cell></cell></row><row><cell cols="5">• Speech transcriptions (spoken discourse): 1934</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,88.99,90.49,338.16,177.47"><head>Table 4</head><label>4</label><figDesc>Combinations of Different Discourse Types for Each Pair in the Combined Dataset</figDesc><table coords="6,185.91,122.10,223.45,145.85"><row><cell>Pair Discourse Type Combination</cell><cell>Number of Pairs</cell></row><row><cell>('Email', 'Text message')</cell><cell>7484</cell></row><row><cell>('Essay', 'Email')</cell><cell>3072</cell></row><row><cell cols="2">('Business memo', 'Text message') 780</cell></row><row><cell>('Essay', 'Text message')</cell><cell>1182</cell></row><row><cell>('Email', 'Business memo')</cell><cell>1014</cell></row><row><cell>('Essay', 'Business memo')</cell><cell>186</cell></row><row><cell>('Speech transcription', 'Email')</cell><cell>1036</cell></row><row><cell>('Email', 'Interview')</cell><cell>4564</cell></row><row><cell>('Essay', 'Interview')</cell><cell>884</cell></row><row><cell cols="2">('Speech transcription', 'Interview') 642</cell></row><row><cell>('Essay', 'Speech transcription')</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,88.99,546.75,416.99,81.83"><head>Table 5</head><label>5</label><figDesc>Total number of problems, number of problems in the positive class, number of texts and number of authors on our splits.</figDesc><table coords="6,192.33,590.32,210.62,38.25"><row><cell cols="5">Split Total Positive Unique Texts Authors</cell></row><row><cell cols="2">Train 18968</cell><cell>9484</cell><cell>1740</cell><cell>101</cell></row><row><cell>Val</cell><cell>2132</cell><cell>1066</cell><cell>192</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,88.99,153.78,367.53,81.83"><head>Table 6</head><label>6</label><figDesc>Results of Authorship verification PAN23 Validation dataset</figDesc><table coords="11,138.76,185.39,317.76,50.21"><row><cell>Model</cell><cell cols="3">AUC C@1 F0.5u F1-score Brier Overall</cell></row><row><cell>all-MiniLM-L12-v2</cell><cell>0.684 0.632 0.615</cell><cell>0.711</cell><cell>0.761 0.681</cell></row><row><cell cols="2">nli-distilroberta-base-v2 0.572 0.509 0.553</cell><cell>0.667</cell><cell>0.692 0.598</cell></row><row><cell>Stylometric Model</cell><cell>0.505 0.512 0.433</cell><cell>0.343</cell><cell>0.521 0.463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,88.99,310.52,398.90,81.83"><head>Table 7</head><label>7</label><figDesc>Results of the performance of our three submissions on the PAN23 testing dataset (from TIRA)</figDesc><table coords="11,107.39,342.14,380.50,50.21"><row><cell>Submission name</cell><cell>Model</cell><cell cols="3">AUC C@1 F0.5u F1-score Brier Overall</cell></row><row><cell>resolving-globe</cell><cell cols="2">all-MiniLM-L12-v2 0.616 0.572 0.562</cell><cell>0.617</cell><cell>0.746 0.623</cell></row><row><cell>reduced-graph</cell><cell cols="2">all-MiniLM-L12-v2 0.616 0.572 0.562</cell><cell>0.617</cell><cell>0.746 0.623</cell></row><row><cell>golden-ottoman</cell><cell cols="2">all-MiniLM-L12-v2 0.598 0.546 0.55</cell><cell>0.622</cell><cell>0.744 0.612</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,107.59,111.28,398.80,10.91;12,107.59,124.83,399.60,10.91;12,107.59,138.38,172.05,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,463.47,111.28,42.92,10.91;12,107.59,124.83,199.44,10.91">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="12,356.78,124.83,145.04,10.91">CLEF 2023 Labs and Workshops</title>
		<title level="s" coord="12,107.59,138.38,103.05,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,107.59,151.93,399.60,10.91;12,107.59,165.48,399.60,10.91;12,107.59,179.03,399.59,10.91;12,107.59,192.57,398.60,10.91;12,107.59,206.12,399.60,10.91;12,107.59,219.67,400.25,10.91;12,107.59,233.22,398.40,10.91;12,107.59,246.77,396.49,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,291.40,179.03,215.78,10.91;12,107.59,192.57,398.60,10.91;12,107.59,206.12,42.05,10.91">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Ríos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pęzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,348.70,219.67,159.13,10.91;12,107.59,233.22,398.40,10.91;12,107.59,246.77,137.43,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,276.42,246.77,155.05,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,107.59,260.32,399.59,10.91;12,107.59,273.87,345.26,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<ptr target="https://arxiv.org/abs/1908.10084" />
		<title level="m" coord="12,217.19,260.32,285.40,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,107.59,287.42,398.40,10.91;12,107.59,300.97,337.59,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,296.99,287.42,209.00,10.91;12,107.59,300.97,114.45,10.91">Encoding text information by pre-trained model for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.53,300.97,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2103" to="2107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,107.59,314.52,400.25,10.91;12,107.59,328.07,345.27,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,283.75,314.52,224.08,10.91;12,107.59,328.07,122.19,10.91">Feature vector difference based authorship verification for open-world settings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Greenstadt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,256.21,328.07,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2201" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,107.59,341.62,400.25,10.91;12,107.59,355.17,398.40,10.91;12,107.26,368.71,29.19,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Manolache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Burceanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05125</idno>
		<title level="m" coord="12,428.89,341.62,78.94,10.91;12,107.59,355.17,248.83,10.91">Transferring bertlike transformers&apos; knowledge for authorship verification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,107.59,382.26,400.24,10.91;12,107.59,395.81,255.04,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Martinez-Galicia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Embarcadero-Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ríos-Orduña</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<title level="m" coord="12,475.59,382.26,32.24,10.91;12,107.59,395.81,223.12,10.91">Graphbased siamese network for authorship verification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,107.59,409.36,400.25,10.91;12,107.59,422.91,399.60,10.91;12,107.59,436.46,400.24,10.91;12,107.59,450.01,398.39,10.91;12,107.59,463.56,399.60,10.91;12,107.59,477.11,55.16,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,295.62,422.91,211.56,10.91;12,107.59,436.46,400.24,10.91;12,107.59,450.01,28.98,10.91">Overview of pan 2023: Authorship verification, multi-author writing style analysis, profiling cryptocurrency influencers, and trigger detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Ríos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pęzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,159.21,450.01,346.77,10.91;12,107.59,463.56,88.22,10.91">Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">April 2-6, 2023. 2023</date>
			<biblScope unit="page" from="518" to="526" />
		</imprint>
	</monogr>
	<note>Part III</note>
</biblStruct>

<biblStruct coords="12,107.59,490.66,399.60,10.91;12,107.59,504.21,399.68,10.91;12,107.41,517.76,399.77,10.91;12,107.20,531.30,398.78,10.91;12,107.59,544.85,398.80,10.91;12,107.27,558.40,106.34,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,170.50,504.21,298.77,10.91">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.26,531.30,316.73,10.91;12,107.59,544.85,91.10,10.91">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="12,205.60,544.85,151.87,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
