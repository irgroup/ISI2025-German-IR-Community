<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,443.62,17.04;1,72.02,96.20,57.90,17.04">Supervised Contrastive Learning for Multi-Author Writing Style Analysis</title>
				<funder ref="#_xR7eUZ8">
					<orgName type="full">National Social Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,129.10,62.88,10.80"><forename type="first">Zhanhong</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName coords="1,143.54,129.10,74.14,10.80"><forename type="first">Changle</forename><surname>Zhong</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,233.45,129.10,56.91,10.80"><forename type="first">Haoliang</forename><surname>Qi</surname></persName>
							<email>qihaoliang@fosu.edu.cn</email>
						</author>
						<author>
							<persName coords="1,298.36,129.10,49.67,10.80"><forename type="first">Yong</forename><surname>Han</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Foshan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,443.62,17.04;1,72.02,96.20,57.90,17.04">Supervised Contrastive Learning for Multi-Author Writing Style Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7AA82C437668E17514DAF1BE410D2FDE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Supervised Contrastive Learning</term>
					<term>Multi-Author Writing Style Analysis</term>
					<term>Soft Prompt</term>
					<term>and Pre-trained Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes supervised contrastive learning with a p-tuning method to solve the Multi-Author Writing Style Analysis task. The motivation is to capture more detail of the variation between the two paragraphs and exploit the potential performance of the pretrained models. Therefore, we combine two methods, the Rdrop method and the supervised contrastive learning (SupCon) method, with p-tuning technology. Then trained and evaluated the three challenging Multi-Author Writing Style Analysis datasets (easy, medium, hard), which the PAN gave, and we achieved a score of 98.280, 83.035, 82.081 on each of the three difficulty tests set on the f1 metric. In addition, we conducted ablation experiments, which proved that supervised contrastive learning was beneficial in capturing more detailed changes in text and stimulating the potential of pre-trained models performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-author style identification is a task to discern whether two authors' writing styles are consistent. In detail, the style change detection task aims to identify text positions within a given multiauthor document at which the author switches. If multiple authors have written a text, the task is to find evidence that we can detect variations in the writing style. Multi-Author Writing Style is widely used in plagiarism detection and author identification. In addition, style change detection can help to uncover gift authorships, verify a claimed authorship, or to develop new technology for writing support.</p><p>Supervised contrastive learning(SupCon) <ref type="bibr" coords="1,277.95,514.77,14.49,9.94" target="#b0">[1]</ref> method, which purpose of introducing label information into contrastive learning is to be able to use label information to use the same label as a positive sample and vice versa as a negative sample, and it has been applied in image classification and compared with the traditional unsupervised contrastive learning method at the time reached SOTA in the computer vision field. P-tuning <ref type="bibr" coords="1,222.91,565.32,13.40,9.94" target="#b1">[2]</ref>, a soft-hard template method, is proposed, hoping that the model can learn how to represent the embedding of some words in the template through downstream tasks.</p><p>Recently study <ref type="bibr" coords="1,137.78,590.64,13.75,9.94" target="#b4">[5]</ref> combines discrete prompts <ref type="bibr" coords="1,273.30,590.64,14.70,9.94" target="#b3">[4]</ref> with contrastive learning, using discrete prompts to help construct positive and negative examples in supervised contrastive learning. However, due to discrete prompts, the paper <ref type="bibr" coords="1,193.63,615.96,13.97,9.94" target="#b1">[2]</ref> has proved that in the context of manually setting prompts, different prompts will lead to different performances. Hence, there are better solutions than manually setting prompts. Therefore, based on the above, we combine p-tuning with supervised comparative learning to improve performance in completing the current task---the multi-author writing style analysis dataset released by PAN <ref type="bibr" coords="1,140.77,666.60,16.12,9.94" target="#b5">[6]</ref>.</p><p>In this paper, we introduce a method that combines the P-tuning, Rdrop <ref type="bibr" coords="2,396.12,87.28,14.37,9.94" target="#b2">[3]</ref>, and contrastive learning technology to aim for as simple an improvement as possible but deliver an excellent performance. In detail, the model has three-part. The first part is an lstm <ref type="bibr" coords="2,312.30,112.60,13.63,9.94" target="#b6">[7]</ref> model employed to learn the optimal prompt for the current downstream task. The second part is the deberta-v3 <ref type="bibr" coords="2,361.03,125.32,13.87,9.94" target="#b7">[8]</ref> model, which handles the current task. Finally, the third part is classifier with contrastive learning loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">P-tuning with supervised contrastive learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network Architecture</head><p>First, we regard the current task as a binary classification task. Given the model input, the goal is to use deberta-v3 to implement text classification tasks. Then the core part of the model introduces the ptuning method to learn a soft-hard template according to the current downstream task and introduces supervised contrastive learning to use label information to make better feature representation.</p><p>According to the model shown in figure1, it consists of encoding, classification, contrastive learning, and p-tuning tasks. The first is the encoding part. We use the deberta-v3 <ref type="bibr" coords="2,385.79,329.35,13.82,9.94" target="#b7">[8]</ref> model to encode the model, which is the transformer block and dropout layer shown in the figure. Noticed that the green and red spots indicate that the same model input was encoded by the pre-trained model, but the encoded result is different because of the existence of the dropout layer. There will be a difference between the output of two identical inputs because the dropout layer will randomly drop something, making the output of the two inputs have some subtle differences. Next comes the classification part, where we use linear layers and classifiers to classify the encoded content, making it possible to complete the current downstream task. Then is the comparative learning part, which is the part that encloses â„’ ğ‘œğ‘¢ğ‘¡ ğ‘ ğ‘¢ğ‘ in the frame, as shown in the figure. After obtaining the hidden state, we perform supervised contrastive learning calculations. In addition, we also use the rdrop method on the obtained hidden state to calculate the situation when two identical samples are used as a positive pair or a negative sample pair (this situation is not calculated in the SupCon method). That is the part including â„’ ğ‘…ğ· , as described in Figure <ref type="figure" coords="2,72.02,483.09,4.14,9.94">1</ref>. See section 2.2 for details. Finally is the p-tuning method, as shown in Figure <ref type="figure" coords="2,430.20,483.09,4.08,9.94">1</ref>. The part including p-tuning is the method of using p-tuning. Details are in section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Model Architecture</head><p>Overall the primary correlate loss function can be defined as follow.</p><formula xml:id="formula_0" coords="3,203.81,96.60,281.06,16.80">â„’ = â„’ ğ‘œğ‘¢ğ‘¡ ğ‘ ğ‘¢ğ‘ + â„’ ğ‘…ğ·<label>(1)</label></formula><p>The loss â„’ ğ‘œğ‘¢ğ‘¡ ğ‘ ğ‘¢ğ‘ means contrastive learning loss and the loss â„’ ğ‘…ğ· represents Rdrop method loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.Supervised contrastive learning</head><p>Firstly, the PAN has given three complex data sets for Multi-Author Writing Style Analysis. Then, we directly form a paragraph pair of two paragraphs and combine them with the soft template(section 2.3) as input to the model. Then we counted the token length of each paragraph pair which did not include the soft template in the dataset for all difficulties, including the training set and development set. The statistical results show that the length of most of the data is less than 512.</p><p>Give a batch name as â„¬. The contents of â„¬ can define as {(ğ‘† 1 , ğ‘¦ 1 ), (ğ‘† 2 , ğ‘¦ 2 ) â€¦ (ğ‘† ğ‘˜ , ğ‘¦ ğ‘˜ )) âˆˆ B, where ğ‘† ğ‘˜ means the paragraph pair, and ğ‘¦ ğ‘˜ is the correlate label. Then we extend the batch by copying each paragraph in the batch, naming it as â„¬ â€² â„¬ â€² can define as {(ğ‘† 1 , ğ‘¦ 1 ), (ğ‘† 1 , ğ‘¦ 1 ), (ğ‘† 2 , ğ‘¦ 2 ), (ğ‘† 2 , ğ‘¦ 2 ) â€¦ (ğ‘† ğ‘˜ , ğ‘¦ ğ‘˜ )} âˆˆ â„¬â€².Then we combine the expanded batch with the soft-hard template to get B Ì‚, B Ì‚ can be defined as {(ğ‘† 1 + ğ‘Ÿ 1 , ğ‘¦ 1 ), (ğ‘† 1 + ğ‘Ÿ 1 , ğ‘¦ 1 ), (ğ‘† 2 + ğ‘Ÿ 2 , ğ‘¦ 2 ), (ğ‘† 2 + ğ‘Ÿ 2 , ğ‘¦ 2 ) â€¦ (ğ‘† ğ‘˜ + ğ‘Ÿ k , ğ‘¦ ğ‘˜ )} âˆˆ â„¬â€², where ğ‘Ÿ k is a softhard template.Then we feed the B Ì‚ into the pre-training model, which is composed of the transformer <ref type="bibr" coords="3,509.29,342.55,13.74,9.94" target="#b7">[8]</ref> block and dropout layer in the figure for encoding to get the corresponding hidden state â„‹ . â„‹ can be defined as {â„ 1 , â„ 1 â€² , â„ 2 , â„ 2 â€² â€¦ â„ ğ‘˜ , â„ ğ‘˜ â€² },â„ ğ‘˜ ğ‘ğ‘›ğ‘‘ â„ ğ‘˜ â€² mean the hidden state of paragraph ğ‘† ğ‘˜ and copied sentence ğ‘† ğ‘˜ . Once â„‹ is obtained, we calculate â„’ ğ‘œğ‘¢ğ‘¡ ğ‘ ğ‘¢ğ‘ and â„’ ğ‘…ğ· .</p><p>After getting hidden state â„‹, we calculate the supervised contrastive learning loss(equation 2). First, we define the same label as a positive sample and vice versa as a negative example. However, the difference is that for ğ‘† ğ‘˜ and its corresponding copied ğ‘† ğ‘˜ , although their labels are the same, they do not participate in the calculation of supervised contrastive learning in this case. Instead, we use the Rdrop <ref type="bibr" coords="3,507.98,434.01,15.15,9.94" target="#b2">[3]</ref> method to calculate the loss (equation 3). After defining the positive and negative samples, we use formula 2 to calculate the supervised contrastive learning loss.</p><formula xml:id="formula_1" coords="3,184.97,482.57,338.18,51.65">â„’ ğ‘œğ‘¢ğ‘¡ ğ‘ ğ‘¢ğ‘ = âˆ‘ -1 ğ¾ âˆ‘ ğ‘™ğ‘œğ‘” ğ‘’ğ‘¥ğ‘ (â„ ğ‘– â‹… â„ ğ‘— / ğœ) âˆ‘ ğ‘’ğ‘¥ğ‘ (â„ ğ‘– â‹… â„ ğ‘ / ğœ) ğ‘âˆˆğ´(ğ‘–) ğ‘—âˆˆğ½(ğ‘–) ğ‘˜ ğ‘–=1<label>(2)</label></formula><p>The index ğ‘— âˆˆ ğ½(ğ‘–) means the paragraph corresponding to index j should have the same label as the paragraph corresponding to the current index i and index i â‰  j. This is done to form a sample of positive examples. Similarly, the part pâˆˆA(i) indicates that the paragraph pair corresponds to the index of p â‰  i. This is done to form a sample of negative examples. The numerator part of the fraction calculates the distance of each positive example with an exponential function, and the denominator part is the sum of the distance with the exponential function of each negative example in the batch.</p><p>Next is the rdrop formula,</p><formula xml:id="formula_2" coords="3,97.70,638.02,413.84,30.12">â„’ ğ‘…ğ· = -log ğ‘ 1 (ğ‘¦ ğ‘˜ Ì‡|(ğ‘† ğ‘˜ + p â€² k )) -log ğ‘ 2 (ğ‘¦ ğ‘˜ |(ğ‘† ğ‘˜ + p â€² k )) + ğ›¼ 2 [ğ· ğ‘˜ğ‘™ (ğ‘ 1 (ğ‘¦ ğ‘˜ Ì‡|(ğ‘† ğ‘˜ + p â€² k )) || ğ‘ 2 (ğ‘¦ ğ‘˜ |(ğ‘† ğ‘˜ + p â€² k ))] + ğ· ğ‘˜ğ‘™ (ğ‘ 2 (ğ‘¦ ğ‘˜ |(ğ‘† ğ‘˜ + p â€² k )) || ğ‘ 1 (ğ‘¦ ğ‘˜ Ì‡|(ğ‘† ğ‘˜ + p â€² k )) (<label>3</label></formula><formula xml:id="formula_3" coords="3,487.12,655.18,4.11,11.04">)</formula><p>where Î± is a hyperparameter, kl means Kullback-Leibler divergence <ref type="bibr" coords="3,381.94,682.22,18.50,9.94" target="#b9">[10]</ref>. Then ğ‘ 1 (ğ‘¦ ğ‘˜ Ì‡|(ğ‘† ğ‘˜ + p â€² k )) and</p><formula xml:id="formula_4" coords="3,72.02,695.04,70.04,12.96">ğ‘ 2 (ğ‘¦ ğ‘˜ |(ğ‘† ğ‘˜ + p â€² k ))</formula><p>represent the probability distribution of the i-th paragraph and the probability distribution after copying the i-th paragraph. Among them, p â€² k represents the soft-hard template, and the plus sign in ğ‘† ğ‘˜ + p â€² k represents the original input of the model combined with the soft-hard prompt template. For the combination method, see section 2.3, and the part -log ğ‘ 1 (ğ‘¦ ğ‘˜ Ì‡|(ğ‘† ğ‘˜ + p â€² k )) is used to calculate the loss on the current task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.P-tuning</head><p>We directly use the method described by p-tuning to build the soft-hard template. Given a manual prompt, we define it as ğ‘ . Then ğ‘ consists of the following words {ğ‘¤ 1 ğ‘¤ 2 â€¦ ğ‘¤ k }, ğ‘¤ k means one of the words in the prompt. Then we manually replace the words in ğ‘ with learnable tokens, and the replaced result is p â€² , which consists of the following words {ğ‘Ÿ 1 ğ‘¤ 2 â€¦ ğ‘Ÿ i }, ğ‘Ÿ i means we manually replace the token at position i with a learnable token ğ‘Ÿ i . After getting p â€² , we combine paragraph ğ‘† ğ‘˜ and p â€² , and the combination method can be a cloze form or prompt as a prefix for model input. We use a cloze-style approach to combine prompt and model inputs, and we mark this paradigm as ğ‘† ğ‘˜ + p â€² k . Then we get the final input of the model and then feed this input to the transformer block to get the corresponding hidden state. Finally, we map the hidden state onto a vocabulary space through the linear layer and get the probabilities of 'yes' and 'no'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data statistics</head><p>The PAN provides all data. The data is available in three difficulty levels: easy, medium, and hard. Each difficulty data set is divided into a training set, a development set, and a test set. The distribution of each dataset is 70%, 15%, and respectively. Organizing the data according to the method mentioned in section 2.2 will result in the following number of paragraph pairs, as Table <ref type="table" coords="4,464.68,440.49,5.52,9.94" target="#tab_0">1</ref> shows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experience setting</head><p>In this work, the deberta-v3-base model is selected for use in the p-tuning technique. It concludes with 12 transformers <ref type="bibr" coords="4,164.95,646.44,13.70,9.94" target="#b8">[9]</ref> layers and 12 attention heads and its hidden size is 768. Table <ref type="table" coords="4,469.28,646.44,5.52,9.94" target="#tab_1">2</ref> shows the detail of the hyperparameter. We set the early stopping to 10, setting the learning rate to 2e-5 and the Rdrop alpha coefficient to 4, and the supervised contrastive learning temperature coefficient to 70.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>We will present two experiments, which are the main experiment and the ablation experiment. The main experiment is the best result achieved so far, and the ablation experiment is to investigate how different settings affect the performance of the model. We use the method of section 2.2 to construct the model input for our proposal, but the difference is that for fine-tune Bert, our input does not use the template. In this experiment, we report an ablation experiment on our model. The following three experiments show the performance of our proposed method with the SupCon method removed. The performance obtained by removing the SupCon and Rdrop methods, i.e., the performance of the plain p-tuning method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have accomplished the tasks mentioned by PAN <ref type="bibr" coords="5,363.96,552.00,41.13,9.94">[11][12]</ref>, and we propose a method that combines three types of technology to solve the multi-Author Writing Style Analysis task. To solve this task, we propose a method that combines Rdrop, supervised contrastive learning, and p-tuning technology. The proposed method obtains 98.280, 83.035, 82.081 on three datasets of varying difficulty. This validates the ability of our proposed method to accomplish the Multi-Author Writing Style Analysis task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,123.95,531.01,347.05,195.70"><head></head><label></label><figDesc></figDesc><graphic coords="2,123.95,531.01,347.05,195.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.02,477.91,404.56,67.71"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,72.02,491.85,404.56,53.77"><row><cell>the statistical result</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>dataset1</cell><cell>dataset2</cell><cell>dataset3</cell></row><row><cell>Train-set</cell><cell>12904</cell><cell>28216</cell><cell>19113</cell></row><row><cell>Dev-set</cell><cell>2828</cell><cell>7042</cell><cell>4112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.02,150.02,432.52,67.80"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,72.02,163.96,432.52,53.86"><row><cell cols="2">the best score in different difficulty development set</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">dataset1@ F1-SCORE dataset2@ F1-SCORE dataset3@ F1-SCORE</cell></row><row><cell>Our method</cell><cell>99.088</cell><cell>83.034</cell><cell>82.0</cell></row><row><cell>Fine-tune bert</cell><cell>96.446</cell><cell>79.574</cell><cell>78.051</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.02,310.97,444.44,178.21"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,72.02,324.91,444.44,164.28"><row><cell cols="2">the score in the different difficulty test set</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">dataset1@ F1-SCORE dataset2@ F1-SCORE dataset3@ F1-SCORE</cell></row><row><cell>Test set</cell><cell>98.280</cell><cell>83.035</cell><cell>82.081</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ablation experiment in different difficulty development sets</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">dataset1 @ F1-SCORE dataset2 @ F1-SCORE dataset3 @ F1-SCORE</cell></row><row><cell>Our method</cell><cell>99.078</cell><cell>83.034</cell><cell>82.0</cell></row><row><cell>Without SupCon</cell><cell>96.902</cell><cell>81.016</cell><cell>78.623</cell></row><row><cell>Without SupCon &amp;</cell><cell>96.849</cell><cell>81.407</cell><cell>73.448</cell></row><row><cell>Rdrop</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgments</head><p>This work is supported by the <rs type="funder">National Social Science Foundation of China</rs> (No. <rs type="grantNumber">22BTQ101</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xR7eUZ8">
					<idno type="grant-number">22BTQ101</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,89.90,732.26,425.86,9.94;5,89.90,744.86,384.99,9.94;5,89.90,758.54,100.91,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,89.90,744.86,137.22,9.94">Supervised contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,234.21,744.86,226.88,9.94">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.90,74.68,396.93,9.94;6,89.90,87.28,152.79,9.94" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m" coord="6,364.97,74.68,89.74,9.94">Gpt understands, too</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,89.90,100.00,384.85,9.94;6,89.90,112.60,377.71,9.94;6,89.90,126.28,154.23,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,440.55,100.00,34.21,9.94;6,89.90,112.60,176.90,9.94">R-drop: Regularized dropout for neural networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,274.35,112.60,193.26,9.94;6,89.90,126.28,36.80,9.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10890" to="10905" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.90,139.60,432.81,9.94;6,89.90,152.20,269.40,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,199.74,139.60,322.98,9.94;6,89.90,152.20,81.86,9.94">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,89.90,164.80,433.26,9.94;6,89.90,177.52,180.54,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,226.95,164.80,292.05,9.94">Contrastive learning for prompt-based few-shot language learners</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01308</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,89.90,190.12,402.59,9.94;6,89.90,202.84,396.56,9.94;6,89.90,215.44,429.80,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,296.46,190.12,196.03,9.94;6,89.90,202.84,120.65,9.94">Overview of the Multi-Author Writing Style Analysis Task at PAN 2023</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,89.90,215.44,344.62,9.94">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.90,229.03,436.03,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,226.19,229.03,107.33,9.94">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,341.42,229.03,87.33,9.94">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,89.90,242.35,401.59,9.94;6,89.90,255.07,368.16,9.94" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09543</idno>
		<title level="m" coord="6,198.13,242.35,293.36,9.94;6,89.90,255.07,180.58,9.94">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,89.90,266.86,433.24,11.04;6,89.90,280.63,389.41,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,89.90,280.63,106.25,9.94">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,203.62,280.63,227.09,9.94">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.42,293.23,409.66,9.94;6,89.90,306.79,134.18,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,148.08,293.23,121.25,9.94">Kullback-leibler divergence</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,291.12,293.23,206.57,9.94">International encyclopedia of statistical science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="720" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.42,320.11,413.41,9.94;6,89.90,332.72,402.33,10.05;6,89.90,345.43,404.53,9.94;6,89.90,358.03,388.59,9.94;6,89.90,370.77,388.27,9.94;6,89.90,383.37,431.37,9.94;6,89.90,396.09,385.57,9.94;6,89.90,408.69,407.49,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,275.72,345.43,218.71,9.94;6,89.90,358.03,388.59,9.94;6,89.90,370.77,41.01,9.94">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-RÃ­ Os</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>FrÃ¶be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>PÄ™zik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,346.62,383.37,174.65,9.94;6,89.90,396.09,385.57,9.94;6,89.90,408.69,143.00,9.94">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,266.94,408.69,155.04,9.94">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,92.42,421.29,430.75,9.94;6,89.90,434.01,361.23,9.94;6,89.90,446.61,410.43,9.94;6,89.90,459.33,383.76,9.94;6,89.90,471.93,414.31,9.94;6,89.90,485.49,119.07,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,130.93,434.01,286.00,9.94">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>FrÃ¶be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,170.02,459.33,303.64,9.94;6,89.90,471.93,98.28,9.94">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="6,195.70,471.93,155.11,9.94">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
