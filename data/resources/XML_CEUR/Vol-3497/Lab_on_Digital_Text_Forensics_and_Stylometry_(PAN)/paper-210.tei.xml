<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,405.85,15.42;1,88.78,106.66,105.55,15.43">Profiling Cryptocurrency Influencers with Sentence Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,63.84,11.96"><forename type="first">Kavya</forename><surname>Girish</surname></persName>
						</author>
						<author>
							<persName coords="1,165.78,134.97,59.15,11.96"><forename type="first">Asha</forename><surname>Hegde</surname></persName>
							<email>hegdekasha@gmail.com</email>
						</author>
						<author>
							<persName coords="1,237.57,134.97,136.80,11.96"><forename type="first">Fazlourrahman</forename><surname>Balouchzahi</surname></persName>
							<email>fbalouchzahi2021@cic.ipn.mx</email>
						</author>
						<author>
							<persName coords="1,89.29,148.92,108.35,11.96"><forename type="first">Hosahalli</forename><surname>Lakshmaiah</surname></persName>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,405.85,15.42;1,88.78,106.66,105.55,15.43">Profiling Cryptocurrency Influencers with Sentence Transformers</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C3ACCE68E6B0B10F3E841573F652576D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Cryptocurrency</term>
					<term>Few Shot Learning</term>
					<term>Word2Vec</term>
					<term>Sentence Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few Shot Learning (FSL) is a supervised Machine Learning (ML) problem which deals with learning with few labeled samples. To address the challenges of FSL in terms of low-resource perspective, in this paper, we describe the models submitted to "Profiling Cryptocurrency Influencers with Few-shot Learning" -a shared task in PAN@CLEF 2023. The task has a focus on English Twitter posts for three subtasks: i) Subtask1 -Low-resource Influencer Profiling, ii) Subtask2 -Low-resource Influencer Interest Identification and iii) Subtask3 -Low-resource Influencer Intent Identification, with a very small set of labeled data. Two models: i) FSL-Word2Vec -Linear Support Vector Classifier (LinearSVC) trained with word embeddings extracted from Google's pre-trained Word2Vec and ii) FSL-ST -LinearSVC trained with sentence embeddings obtained from stsb-bert-base, are submitted to the shared task. FSL-Word2Vec models obtained macro F1 scores of 46.66 and 50.42 for Subtasks 2 and 3 respectively and FSL-ST model obtained a macro F1 score of 37.92 for Subtask1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cryptocurrency is a digital currency created as an alternative form of currency using encryption algorithms. The transactions in cryptocurrency are verified and maintained by a decentralized system using cryptography, rather than by a centralized authority. Today, there are more than 22,789 different cryptocurrencies, with an estimated total value of 1 trillion dollars 1 . Some of the top cryptocurrencies with billions of dollars in the market include Bitcoin, Ethereum, Tether, Binance Coin, and Dogecoin.</p><p>Social media platforms are playing a prominent role in expanding the cryptocurrency communities to reach the general public. The rising ubiquity of speculative trading of cryptocurrencies over social media has led to sentiment driven "bubbles" <ref type="bibr" coords="1,348.12,523.84,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,362.58,523.84,7.65,10.91" target="#b1">2]</ref>. Further, comments/ posts in social media from highly influential personalities often cause a growing chain reaction leading to a short squeeze and the creation of a bubble. These personalities known as Cryptocurrency Influencers, act as the link between the cryptocurrency industry and general public. Vitalik Buterin, Elon Musk, Andreas M, and S Anthony Pompliano are some of the most popular and leading Crypto Influencers. These Influencers can help the public and new crypto investors about the trending cryptocurrencies, comment on crypto news, and provide marketing services to crypto startups<ref type="foot" coords="2,171.35,125.86,3.71,7.97" target="#foot_0">2</ref> . Cryptocurrency Influencers can be categorized based on: i) number of followers (Nano influencers -1K-10K followers, Micro influencers -10K-100K followers, Macro influencers -100K-1M followers and Mega or celebrity influencers -1M+ followers), ii) interest (technical information, price update, trading matters, gaming) and iii) intent (subjective opinion, financial information, advertising, announcement). People, especially the ones who want to know more information about cryptocurrencies or the ones who are interested in investing in cryptocurrencies can search for these Influencers to understand the nuances of crypto market. Hence, instead of searching for the Influencers, it will be very helpful if such Influencers' profile is automatically available to the users. This requirement leads to profiling Cryptocurrency Influencers automatically and can be considered as a special case of Author Profiling which is the need of the day to expand cryptocurrency market <ref type="bibr" coords="2,330.07,263.11,11.43,10.91" target="#b2">[3]</ref>.</p><p>Crypto Influencers have built a sizable followers on social media sites by expressing their thoughts and observations on cryptocurrencies. However, profiling Cryptocurrency Influencers in social media is very challenging due the very informal kind of data that is available on social media platforms. Given the social media data about the impact the cryptocurrency Influencers have created and the category to which these Influencers belong, profiling the Influencers can be modeled as a text classification problem. However, in a real environment, data collection is a major challenge and real-time profiling needs to be done in a few milliseconds, which implies to process as little data as possible. This demands for the tools which can make predictions accepting very less data.</p><p>Conventionally, large amount of annotated data is required to train the ML models. However, collecting, annotating, and validating large data is very expensive. Further, there are many cases where it is just next to impossible to collect large datasets or the available large datasets may not be accessible for public. For example, rare diseases would not have a large number of radiological images or it would be frustrating if smartphones need to have thousands of pictures of users to recognize them and get unlocked. In such scenarios, ML models trained on a small number of samples results in low performance on the Test set. A solution to such scenarios is FSL which aims to build accurate ML models with less training data <ref type="bibr" coords="2,404.90,493.44,12.88,10.91" target="#b3">[4]</ref>  <ref type="bibr" coords="2,420.51,493.44,11.47,10.91" target="#b4">[5]</ref>. FSL which was originally developed for Computer Vision models to work with relatively few training samples is now being used for Natural Language Processing (NLP) applications and the results are encouraging. However, FSL for NLP applications is a young area that needs more research and refinement.</p><p>"Profiling Cryptocurrency Influencers with Few-shot Learning" is a shared task<ref type="foot" coords="2,465.10,559.43,3.71,7.97" target="#foot_1">3</ref> in PAN @CLEF 2023 which aims to profile Cryptocurrency Influencers in social media, from a lowresource perspective <ref type="bibr" coords="2,188.42,588.29,11.58,10.91" target="#b5">[6]</ref>. Focusing on English Twitter posts, this shared task includes three different subtasks and the details of the subtasks are shown in Table <ref type="table" coords="2,385.07,601.84,3.66,10.91" target="#tab_0">1</ref>. To address the challenges of text classification from a low-resource perspective, in this paper, we describe the FSL models submitted to the above mentioned shared task to profile Cryptocurrency Influencers with very less data provided by the organizers of the shared task. The proposed models makes use of Google's pre-trained Word2Vec<ref type="foot" coords="3,225.69,377.76,3.71,7.97" target="#foot_2">4</ref> and stsb-bert-base <ref type="foot" coords="3,311.81,377.76,3.71,7.97" target="#foot_3">5</ref> -a Sentence Transformer (ST), to represent text which in turn are used to train LinearSVC models to predict the probabilities of the samples belonging to the classes in the predefined set of classes of a subtask and assign the label of the class having highest probability to the Test sample.</p><p>The rest of the paper is organized as follows: Section 2 gives a brief description of the related work followed by the methodology in Section 3. The experiments and results are discussed in Section 4 and the paper concludes with future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Profiling Cryptocurrency Influencers and FSL are relatively the new fields for NLP researchers <ref type="bibr" coords="3,492.63,519.44,11.28,10.91" target="#b4">[5]</ref>. However, researchers have explored many techniques for Author Profiling. Hence, considering profiling Cryptocurrency Influencers as a special case of Author Pofiling, a brief description of few of the relevant works are given below:</p><p>Wang et al. <ref type="bibr" coords="3,153.38,573.63,12.84,10.91" target="#b3">[4]</ref> conducted a comprehensive and systematic review of FSL starting from the formal definition of FSL, the relatedness and differences of FSL with relevant learning problems such as weakly supervised learning, imbalanced learning, Transfer Learning (TL) and metalearning. They focused on learning experiences for small samples and categorised FSL from the perspective of data, model and algorithm. To discuss the pros and cons of each category, they explored various learning models including multitask learning, embedding learning, learning with external memory, and generative modelling by refining existing parameters, meta-learned parameters, and optimizer on image data with data augmentation, such as flipping, scaling, rotation, and reflection. Patel et al. <ref type="bibr" coords="4,247.68,127.61,12.84,10.91" target="#b6">[7]</ref> presented Sequential Autoregressive Prompting (SAP) -a technique that enables the prompting of bidirectional models. Considering the machine translation task as a case study, they prompt the bidirectional mT5<ref type="foot" coords="4,387.79,152.96,3.71,7.97" target="#foot_4">6</ref> model and demonstrated its few-shot and zero-shot translations. This model outperformed the few-shot translations of unidirectional models like GPT-3 <ref type="foot" coords="4,253.93,180.06,3.71,7.97" target="#foot_5">7</ref> and XGLM <ref type="foot" coords="4,311.97,180.06,3.71,7.97" target="#foot_6">8</ref> . Further, they also showed that SAP is a better choice in question answering and text summarization tasks. Their results demonstrate prompt based few-shot and zero-shot learning are emergent techniques in building broader class of language models compared to unidirectional models. Joo and Hwang <ref type="bibr" coords="4,430.73,222.46,12.84,10.91" target="#b7">[8]</ref> presented the model submitted to PAN@CLEF 2019 shared task on Author Profiling to determine whether a tweet's author is a bot or human and in case of human, to perform gender identification. Their Gradient Boosted Decision Tree (GBDT) classifier trained by stacking: character count, psycholinguistic features, Term Frequency -Inverse Document Frequency (TF-IDF), Doc2vec and BERT embeddings, obtained accuracies of 0.9333 and 0.8352 for bot identification and gender identification tasks respectively.</p><p>Parnami and Lee <ref type="bibr" coords="4,179.06,317.30,12.84,10.91" target="#b4">[5]</ref> conducted a detailed survey of the recently proposed FSL algorithms. The agenda of the survey includes learning dynamics, characteristics of FSL, approaches to deal with FSL problems from the perspectives of meta-learning, TL, and hybrid approaches.</p><p>Further, with open problems and challenges in FSL followed by the discussion about FSL issues, such as training the same way as testing, learning constrained to a single distribution of tasks, performing joint classification from seen and unseen classes, and FSL for domains other than images, they conclude their survey. Tunstall et al. <ref type="bibr" coords="4,310.59,398.60,12.84,10.91" target="#b8">[9]</ref> proposed SETFIT (Sentence Transformer Finetuning) -an efficient and prompt-free framework for few-shot fine-tuning of ST. This model first finetunes a pretrained ST on a small number of text pairs to generate rich text embeddings, which are used to train a classification head. This simple framework without prompts or verbalizers have achieved high accuracy with orders of magnitude less parameters than existing techniques. They have also demonstrated that SETFIT can be applied in multilingual settings by simply switching the ST body and evaluated SETFIT with three variants of transformer based models (SETFIT ROBERTA , SETFIT MPNET , and SETFIT MINILM ) on various text classification tasks (sentiment analysis, spam detection, and topic classification) considering the available bench-marked datasets (SST-5, AmazonCF, Emotion, EnronSpam, and AGNews). Their proposed SETFIT ROBERTA model out performed GPT-3 by exhibiting a maximum average score of 71.3.</p><p>Many researchers have worked on Author Profiling considering a substantial amount of data. But, very few works are explored for Author Profiling with few labeled samples. Hence, exploring FSL for profiling Cryptocurrency Influencers with few labeled samples has enough scope and opens up new avenues of research in this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Two models, FSL-Word2Vec and FSL-ST are proposed to address the challenges of the shared task and these models differ primarily in pre-processing and the way of representing texts. The dataset shared by the organizers of the shared task is collected from Twitter <ref type="bibr" coords="5,424.34,138.38,11.25,10.91" target="#b5">[6,</ref><ref type="bibr" coords="5,438.31,138.38,13.96,10.91" target="#b9">10]</ref> and Twitter data will usually be noisy/dirty with lot of URL's, hashtags, user mentions, emojis and numeric information, which are not significant for classification. Hence, text data needs to be cleaned and prepared for text representation. The first step in this direction is to tokenize the input into sentences and applying the pre-processing steps on the sentences depending on the model used for text representation. The framework of the proposed methodology is visualized in Figure <ref type="figure" coords="5,417.56,382.15,5.17,10.91" target="#fig_0">1</ref> and the proposed models are described below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FSL-Word2Vec</head><p>• Pre-processing -emojis are converted to the corresponding text and the entire text is lower cased. Further, the noisy content, punctuation, and the stop words are removed from the text. English stopword list <ref type="foot" coords="5,271.48,470.77,3.71,7.97" target="#foot_7">9</ref> available at NLTK library is used to remove the stop words and Porter Stemmer<ref type="foot" coords="5,236.99,484.32,7.41,7.97" target="#foot_8">10</ref> is used to strip the affixes from the words. The remaining content in the pre-processed sentences are given as input to the text representation module. • Text Representation -deals with how efficiently text documents are represented. The introduction of Word2Vec by Mikolov et al. <ref type="bibr" coords="5,303.73,541.62,17.91,10.91" target="#b10">[11]</ref> gave rise to the representation of words by a fixed dimension dense vector of small size like 50, 100, 200 and 300. These vectors which are called as pre-trained vectors/embeddings will be trained on a very large corpus. With this representation, any text can be represented as an aggregation of the representation of words. Leveraging pre-trained embeddings for word representation will help to capture the semantic knowledge, even when the task-specific dataset is limited. Google's pretrained Word2Vec 11 is one such implementation trained on roughly 100 billion words from a Google News dataset. Word2Vec captures the general semantic relationships, patterns, and domains and contains vectors of size 300 for 3 million words and phrases. Using Google's pre-trained Word2Vec model, the text is represented as follows:</p><p>for each word in a sentence, a vector of 300 dimension is extracted (if the word is not in the vocabulary, a zero vector of length 300 is used) -for each sentence, the mean of the vectors of all the words in that sentence is obtained for each text, the mean of the vectors of all the sentences in that text is obtained With this arrangement, each text/sample in the dataset will be represented by a dense vector of size 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">FSL-ST</head><p>• Pre-processing -as ST represent the sentences, very limited pre-processing is carried out on the sentences to maintain the sentence structure. This includes converting emojis to the corresponding text and removing the noisy content from each sentence in the given text. Stopword removal, Stemming and converting text to lowercase, are not performed to retain the sentence structure. The pre-processed sentences are given as input to the text representation module. • Text Representation -ST is a Python framework for generating contextualized embeddings for sentences. By embedding sentences into a vector space, ST enables the proximity of similar sentences, allowing for various applications such as semantic search, clustering, and retrieval. With its user-friendly methods, ST simplifies the process of generating embeddings, exhibits state-of-the-arts performance, provides multilingual support, and is reliable as it belongs to open source community. Few-shot and zero-shot approaches have received a great deal of interest in the research community due to the availability of ST and the untapped capacity to use them in resource-constrained domains <ref type="bibr" coords="6,116.56,460.68,11.28,10.91" target="#b8">[9]</ref>. stsb-bert-base is a ST model that maps sentences and paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. Using stsb-bert-base 12 , the text is represented as follows:</p><p>-Each sentence in a text is represented by a vector of 768 dimension for each text, the mean of the vectors of all the sentences in that text is obtained With this arrangement, each sample in the dataset will be represented by a dense vector of size 768. Hyperparameters and their values of stsb-bert-base are shown in Table <ref type="table" coords="6,428.39,559.15,3.74,10.91">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Building</head><p>The motivation for using FSL is to learn with few available samples. As the number of training samples for each subtask is very less, the vector representations obtained for the text samples of each subtask are grouped according to the classes/categories and the average of these representations is obtained. This arrangement reduces the number of training samples in the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,352.50,212.18,8.93;5,106.51,228.84,382.25,111.10"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Framework of the proposed methodology</figDesc><graphic coords="5,106.51,228.84,382.25,111.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,407.97,251.12"><head>Table 1</head><label>1</label><figDesc>Details of the subtasks</figDesc><table coords="3,98.31,116.30,398.65,225.30"><row><cell>Subtask</cell><cell>Name of the Subtask</cell><cell>Classes</cell><cell>Description of Label Distribution</cell><cell>Train Set</cell><cell>Test Set</cell></row><row><cell></cell><cell></cell><cell>Null /</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subtask1</cell><cell>Low-resource Influencer Profiling</cell><cell>No influencer Nano Micro Macro</cell><cell>32 users per label with a maximum of 10 English tweets each</cell><cell>160</cell><cell>220</cell></row><row><cell></cell><cell></cell><cell>Mega</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Price update</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subtask2</cell><cell>Low-resource Influencer Interest Identification</cell><cell>Technical information Trading matters Gaming</cell><cell>64 users per label with 1 English tweets</cell><cell>320</cell><cell>402</cell></row><row><cell></cell><cell></cell><cell>Other</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Subjective</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Low-resource</cell><cell>opinion</cell><cell>64 users per label</cell><cell></cell><cell></cell></row><row><cell>Subtask3</cell><cell>Influencer Intent</cell><cell>Financial</cell><cell>with 1 English tweets</cell><cell>256</cell><cell>292</cell></row><row><cell></cell><cell>Identification</cell><cell>information</cell><cell>each</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Advertising</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Announcement</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,92.57,660.06,245.98,8.97"><p>https://dailycoin.com/crypto-influencers-you-need-to-know-about/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,92.57,671.02,221.77,8.97"><p>https://pan.webis.de/clef23/pan23-web/author-profiling.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,92.57,660.06,198.36,8.97"><p>https://huggingface.co/fse/word2vec-google-news-300</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,92.57,671.02,220.77,8.97"><p>https://huggingface.co/sentence-transformers/stsb-bert-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,92.57,649.04,211.03,8.97"><p>https://huggingface.co/docs/transformers/model_doc/mt5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,92.57,660.00,117.13,8.97"><p>https://github.com/openai/gpt-3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,92.57,670.96,215.27,8.97"><p>https://huggingface.co/docs/transformers/model_doc/xglm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="5,92.57,649.11,173.41,8.97"><p>https://www.nltk.org/search.html?q=stopwords</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="5,95.35,660.07,171.61,8.97"><p>https://www.nltk.org/api/nltk.stem.porter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="5,95.35,671.03,198.36,8.97"><p>https://huggingface.co/fse/word2vec-google-news-300</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>subtask to the number of classes in that subtask. These new training samples which are the representative of each class are used to train the LinearSVC model.</p><p>To evaluate the model on the Test set, the samples in the Test set will be pre-processed and represented as discussed in Sections 3.1 and 3.2. These vector representations are fed to the LinearSVC model to predict the probabilities of the samples belonging to the classes in the predefined set of classes in the subtask and assign the label of the class having highest probability to the Test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experiments and Results</head><p>The statistics of the dataset for the subtasks shown in Table <ref type="table" coords="7,356.07,479.76,5.05,10.91">1</ref> indicates that the given datasets are very small in size. If ML classifiers are used in a conventional way, this small size dataset will pose challenges for achieving good performance and generalization on unseen data. This justifies the need for using FSL for profiling Cryptocurrency Influencers.</p><p>As the Development set is not provided by the organizers, several experiments were conducted initially by splitting the given Train set into Train and Test set and considering different word representations, ST and classifiers. The combination of word representation -Google's pretrained Word2Vec, ST -stsb-bert-base and classifier -LinearSVC, which exhibited the best performance in the initial experiments are considered for the actual experiments to obtain the probabilites and predictions on the Test set which in turn are submitted to the organizers for evaluation. The predictions are evaluated based on the macro F1 score as it considers the average F1 score across all classes, providing a comprehensive measure of model effectiveness. Table <ref type="table" coords="7,116.05,642.35,5.13,10.91">3</ref> gives the performances of the proposed models for all the three subtasks. Among the proposed models, FSL-Word2Vec model obtained better macro F1 scores of 46.66 and 50.42 for Subtask2 and Subtask3 respectively whereas FSL-ST model obtained macro F1 score of 37.92 for Subtask1. The very low macro F1 score for Subtask1 may be because the number of training samples is only 160 which is very small to train any ML model. The performances of both the models for Subtask3 are better compared to that of the other two subtasks. Further, there are no much differences in the performances of the two models in all the subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper describes the FSL models proposed for "Profiling Cryptocurrency Influencers with Few-shot Learning" shared task in PAN@CLEF 2023. The proposed methodology consists of two models: i) FSL-Word2Vec -LinearSVC model trained with embeddings extracted from Google's pre-trained Word2Vec and ii) FSL-ST -LinearSVC model trained with sentence embeddings obtained using stsb-bert-base. The performances of both the models for Subtask3 are better compared to that of the other two subtasks. Further, there are no much differences in the performances of the two models in all the subtasks. Efficient FSL techniques will be explored further to handle labeled data with few samples.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,339.71,393.33,10.91;8,112.66,353.26,393.33,10.91;8,112.28,366.81,329.63,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,401.32,339.71,104.67,10.91;8,112.66,353.26,339.72,10.91">Cryptocurrency bubble detection: A new stock market dataset, financial task &amp; hyperbolic models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,478.96,353.26,27.03,10.91;8,112.28,366.81,299.98,10.91">North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,380.36,393.33,10.91;8,112.66,393.91,95.51,10.91" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">W</forename><surname>Chohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,182.14,380.36,258.82,10.91">Counter-hegemonic finance: The gamestop short squeeze</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,407.46,393.33,10.91;8,112.66,421.01,393.33,10.91;8,112.66,434.55,182.13,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,485.20,407.46,20.79,10.91;8,112.66,421.01,185.52,10.91">Zero and few-shot learning for author profiling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L D</forename><surname>Pena Sarrac'en</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,320.80,421.01,185.18,10.91;8,112.66,434.55,152.18,10.91">International Conference on Applications of Natural Language to Data Bases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,448.10,393.61,10.91;8,112.66,461.65,394.62,10.91;8,112.66,475.20,252.17,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,297.37,448.10,208.89,10.91;8,112.66,461.65,94.21,10.91">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386252</idno>
		<ptr target="https://doi.org/10.1145/3386252.doi:10.1145/3386252" />
	</analytic>
	<monogr>
		<title level="j" coord="8,234.51,461.65,91.23,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,488.75,393.32,10.91;8,112.66,502.30,219.44,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,203.62,488.75,302.36,10.91;8,112.66,502.30,35.05,10.91">Learning from few examples: A summary of approaches to few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parnami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<idno>ArXiv, volume abs/2203.04291</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,515.85,394.52,10.91;8,112.66,529.40,394.53,10.91;8,112.66,542.95,394.52,10.91;8,112.66,556.50,393.53,10.91;8,112.66,570.05,394.53,10.91;8,112.66,583.60,395.17,10.91;8,112.66,597.15,393.33,10.91;8,112.66,610.69,394.53,10.91;8,112.66,624.24,65.44,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,294.21,542.95,212.97,10.91;8,112.66,556.50,393.53,10.91;8,112.66,570.05,42.05,10.91">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Ríos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pęzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,371.28,583.60,136.55,10.91;8,112.66,597.15,393.33,10.91;8,112.66,610.69,197.14,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,342.79,610.69,159.86,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,112.66,637.79,393.33,10.91;8,112.66,651.34,374.54,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,447.72,637.79,58.26,10.91;8,112.66,651.34,190.29,10.91">Bidirectional language models are also few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Rasooli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno>ArXiv, volume abs/2209.14500</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.32,10.91;9,112.66,100.52,196.88,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,191.45,86.97,314.53,10.91;9,112.66,100.52,144.84,10.91">Profiling on social media : An ensemble learning model using various features notebook for pan at clef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.33,10.91;9,112.66,127.61,352.25,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,468.26,114.06,37.72,10.91;9,112.66,127.61,159.04,10.91">Efficient Few-shot Learning without Prompts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">E S</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Korat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pereg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11055</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,393.33,10.91;9,112.14,168.26,159.26,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,466.53,141.16,39.46,10.91;9,112.66,154.71,261.87,10.91">Profiling Cryptocurrency Influencers with Few shot Learning at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.97,154.71,88.02,10.91;9,112.14,168.26,47.32,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="9,167.44,168.26,73.93,10.91">Notebook Papers</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,393.32,10.91;9,112.66,195.36,358.10,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,309.61,181.81,196.37,10.91;9,112.66,195.36,64.56,10.91">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,200.24,195.36,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
