<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,390.32,15.42;1,89.29,106.66,345.87,15.42;1,89.29,129.00,157.29,11.96">Leveraging Large Language Models with Multiple Loss Learners for Few-Shot Author Profiling Notebook for PAN at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,106.46,11.96"><forename type="first">Hamed</forename><forename type="middle">Babaei</forename><surname>Giglou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TIB Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Leibniz Universit√§t Hannover</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.68,154.90,85.54,11.96"><forename type="first">Mostafa</forename><surname>Rahgouy</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Auburn University</orgName>
								<address>
									<settlement>Alabama</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.86,154.90,82.17,11.96"><forename type="first">Jennifer</forename><surname>D'souza</surname></persName>
							<email>jennifer.dsouza@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="department">TIB Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.68,154.90,83.96,11.96;1,89.29,168.85,36.30,11.96"><forename type="first">Milad</forename><forename type="middle">Molazadeh</forename><surname>Oskuee</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,138.24,168.85,67.71,11.96"><forename type="first">Hadi</forename><surname>Bayrami</surname></persName>
							<email>hadibayrami@student.uma.ac.ir</email>
						</author>
						<author>
							<persName coords="1,208.94,168.85,64.30,11.96"><forename type="first">Asl</forename><surname>Tekanlou</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Mohaghegh Ardabili</orgName>
								<address>
									<settlement>Ardabil</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.23,168.85,72.22,11.96"><forename type="first">Cheryl</forename><forename type="middle">D</forename><surname>Seals</surname></persName>
							<email>sealscd@auburn.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Auburn University</orgName>
								<address>
									<settlement>Alabama</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,390.32,15.42;1,89.29,106.66,345.87,15.42;1,89.29,129.00,157.29,11.96">Leveraging Large Language Models with Multiple Loss Learners for Few-Shot Author Profiling Notebook for PAN at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6CC5ABE99A36CE401BA8D7D99B223CE3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-shot Learning</term>
					<term>Large Language Models</term>
					<term>Low-resource Text Classification</term>
					<term>Cryptocurrency</term>
					<term>Author Profiling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of author profiling (AP) is to study the characteristics of authors through the analysis of how language is exchanged among people. Studying these attributes sometimes is challenging due to the lack of annotated data. This indicates the significance of studying AP from a low-resource perspective. This year at AP@PAN 2023 the major interest raised in profiling cryptocurrency influencers with a few-shot learning technique to analyze the effectiveness of advanced approaches in dealing with new tasks from a low-resource perspective. The AP-2023 task consists of 3 subtasks including cryptocurrency influencer analysis, interest identification, and intent identification. In this work, we studied the integration of Bi-Encoders with Large Language Models (LLMs), to enhance the semantic representation of authors by enabling the models to transfer knowledge across domains and adapt to new tasks with a small number of data. We incorporated multi-losses to enforce LLMs to learn the representations of different categories and authors to facilitate similarity-based comparisons among authors and categories. Finally, our approach achieved impressive F1 Macro scores of 52.31 for crypto influencer profiling, 61.21 for crypto influencer interest identification, and 65.83 for crypto influencer intent identification using limited supervised learning data. Overall, the obtained and experimental analysis shows the effectiveness of the integration of multiple-loss learners with LLMs in profiling cryptocurrency influencers using limited resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cryptocurrencies have gained high popularity in recent years, capturing the attention of many. Factors such as independency from central authorities, the potential offered by different cryptocurrency projects, and the influence of social media influencers have contributed to their trendy status. However, in a real environment where, for instance, traders may want to leverage social media signals to forecast the market, data collection is a challenge and real-time profiling needs to be done in a few milliseconds, which implies processing as little data as possible. However, due to the economic and temporal cost, the psychological and linguistic expertise needed by the annotator, and the congenital subjectivity involved in the data preparation for analysis of social media for cryptocurrency topics, studying this field from a low-resource perspective become an important matter.</p><p>In recent years, there has been significant interest in exploring the capabilities of Large Language Models (LLMs) to perform new tasks through inference alone. This emerging approach, known as in-context learning or prompting technique, relies on utilizing zero-shot or a limited number of input-label pairs, referred to as demonstrations, to enable the model to predict new inputs without explicit training on the specific task. To improve the in-context learning technique, we have three ways; 1) Training an LLM on multiple tasks, thereby offering the LLMs a training exposure to diverse tasks, with the hypothesis that it facilitates its adaptation to novel tasks during testing, 2) Choosing labeled examples for the demonstrations more effectively, and 3) Exploring variants of in-context learning, such as learning to follow instructions or incorporating external knowledge sources <ref type="bibr" coords="2,283.03,368.71,11.58,10.91" target="#b0">[1]</ref>. Moreover, LLMs demonstrate their capability as few-shot learners, achieving impressive performance on novel tasks with minimal training examples. By scaling up LLMs, their effectiveness in a task-agnostic, few-shot environment can be significantly enhanced, often surpassing the performance of previous state-of-the-art fine-tuning methods <ref type="bibr" coords="2,186.43,422.91,11.58,10.91" target="#b1">[2]</ref>. Task profiling cryptocurrencies influencers can benefit from this since analyzing users' behavior on social media often is challenging due to the low resource perspective of the task.</p><p>In this paper, we addressed a PAN-2023 shared task <ref type="bibr" coords="2,324.30,463.56,12.69,10.91" target="#b2">[3]</ref> on Profiling Cryptocurrency Influencers <ref type="bibr" coords="2,89.29,477.11,13.00,10.91" target="#b4">[4]</ref> to profile cryptocurrency influencers in social media, from a low-resource perspective in three subtasks:</p><p>‚Ä¢ Subtask 1: Low-resource influencer profiling ‚Ä¢ Subtask 2: Low-resource influencer interest identification ‚Ä¢ Subtask 3: Low-resource influencer intent identification To identify and analyze cryptocurrency influencers on Twitter, we utilized LLMs and a semantic textual similarity approach with multiple loss learners to leverage LLMs for learning different profiling tasks in a few-shot scenario in the English language. The code for the proposed method has been published in a repository on GitHub<ref type="foot" coords="2,372.80,594.76,3.71,7.97" target="#foot_0">1</ref> for the research community.</p><p>The rest of the paper is organized as follows. Section 2 presents related work. Section 3 describes the proposed methodology in detail. Section 4 describes the experimental setup including dataset, metrics, and training settings. Next, in section 5 we discuss the results. Finally, section 6 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer Learning (TL) has been successfully applied to many machine learning applications, including text sentiment classification, image classification, human activity classification, software defect classification, and multi-language text classification. TL transfers knowledge from the source domain/task, where training data is abundant, to the target domain/task, where training data is scarce <ref type="bibr" coords="3,189.90,165.48,11.47,10.91" target="#b5">[5]</ref>. TL methods are popularly used in Few-Shot Learning (FSL), where the prior knowledge is transferred from the source task to the few-shot task. Where FSL is a type of machine learning problem, that contains only a limited number of examples with supervised information <ref type="bibr" coords="3,195.92,206.12,11.43,10.91" target="#b6">[6]</ref>. Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="3,89.29,219.67,12.69,10.91" target="#b7">[7]</ref> is a powerful transformer-based model pretrained on a large corpus of unlabeled text data. It has been successfully applied in many FSL tasks, where the pretrained BERT model is fine-tuned on a few-shot dataset for tasks like text classification, named entity recognition, and question answering. By leveraging the pretraining knowledge, BERT can effectively generalize and adapt to new few-shot tasks. Similarly, T5: Text-To-Text Transfer Transformer <ref type="bibr" coords="3,447.60,273.87,13.00,10.91" target="#b8">[8]</ref> is trained on a large-scale text-to-text dataset, where it learns to map input text to output text. T5 can be fine-tuned on a few-shot task by formulating the task as a text-to-text problem. It has been applied to various FSL tasks, including text classification, summarization, and machine translation. Moreover, Flan-T5 <ref type="bibr" coords="3,223.98,328.07,12.69,10.91" target="#b9">[9]</ref> combines the benefits of FSL and T5 to enable effective knowledge transfer and adaptation to new few-shot tasks. It highlights the significant performance boost achieved by incorporating chain-of-thought <ref type="bibr" coords="3,311.91,355.17,17.82,10.91" target="#b10">[10]</ref> prompting data for reasoning tasks and providing a comprehensive evaluation of instruction-finetuned models across various setups and benchmarks.</p><p>The FASL <ref type="bibr" coords="3,147.49,395.81,18.07,10.91" target="#b11">[11]</ref> is a platform that integrates FSL and active learning to facilitate rapid and effective training of text classification models. The authors examine various active learning methods to determine their effectiveness in a few-shot setup and develop a model that predicts the optimal point to stop annotating data. In <ref type="bibr" coords="3,292.50,436.46,18.01,10.91" target="#b12">[12]</ref> explores the construction of text classifiers with minimal or zero training data by employing Siamese Networks, which embed both texts and labels to enable model adaptation in few-shot scenarios by solely modifying the label embeddings. The SimSCE <ref type="bibr" coords="3,208.55,477.11,18.07,10.91" target="#b13">[13]</ref> is a simple contrastive learning (CL) framework that uses the standard dropout operation to generate high-quality training pairs. It's scalable and better in the regular data augmentation used in NLP tasks. CL <ref type="bibr" coords="3,331.43,504.21,18.04,10.91" target="#b14">[14]</ref> is a kind of unsupervised learning that learns how to represent data by comparing pairs that are alike and different. The model is trained to tell apart positive pairs and negative pairs in the input data. The aim is to learn a representation space where similar samples are close to each other, while different samples are far from each other. CL has been shown to be effective for few-shot learning. It has also been used in combination with other techniques such as self-supervised learning and transfer learning to improve performance on downstream tasks with limited labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we describe the details of our proposed model. Our proposed approach aims to predict whether the user is keen to be a cryptocurrency influencer's profile, interest, and intent on Twitter. With recent advancements in LLMs, we studied leveraging LLMs for cryptocurrency influencer profiling, we utilized the Flan-T5 model and employed it in the form of text generation, incorporating multiple instructions per user tweet. The primary objective was to train the Flan-T5-Encoder using the whole model architecture. Next, we constructed a bi-encoder model using the finetuned encoder component of Flan-T5. We employed contrastive learning (CL) and Multiple Negative Ranking (MNR) <ref type="bibr" coords="4,270.63,154.71,18.06,10.91" target="#b15">[15]</ref> losses during the training. For CL, we created negative samples which allowed us to create a comparative framework for the model to learn from, however, the MNR uses positive pairs and automatically generates negative pairs in low dimensional space. Both losses enhance the ability of Flan-T5 to distinguish between different types of author profiles. In the testing phase, we employed cosine similarity to compare different users and categories them. Figure <ref type="figure" coords="4,241.67,222.46,5.07,10.91">1</ref> illustrates our framework.</p><p>First, we concatenate all user tweets, next, we conducted some minor preprocessing, i.e.; Removing URLs, @ and # symbols, punctuations, special characters, additional lines, free spaces, and converting text to lowercase. The processed user texts are used for training and testing models. In the following, we will describe the training and testing components of the proposed method separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prompt Templates</head><p>Prompt templates <ref type="bibr" coords="4,169.46,339.93,17.76,10.91" target="#b16">[16]</ref> are predefined structures or guidelines that assist in generating effective prompts for LMs. They serve as a framework for constructing inputs in a standardized format that the model can understand and respond to appropriately. Prompt templates typically include placeholders or keywords that users can customize with their desired information. By using prompt templates, we can easily obtain accurate and relevant responses from the LM. The standardized format provided by prompt templates helps ensure clarity and consistency in interactions, enabling efficient and effective communication with the model. For these reasons, we manually designed 10 prompt templates per subtask to convert original user tweets to form questions with contexts (tweets) as inputs of an LM. The prompt templates are illustrated as ùëá ùëñ (ùëà ), where ùëñ is the ùëñ-th template and ùëà is the combined user tweets. For example, ùëá 2 (ùëà ) is the second template for querying LMs for user interest and it is defined as follows:</p><p>Analyze the given tweets to identify if the user has a particular purpose in cryptocurrency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweets: {tweets}</head><p>Where {tweets} is a placeholder for a user's tweets. All the templates per task are listed in table 3 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Finetuning Flan-T5-Encoder</head><p>In this study, we conducted fine-tuning of the Flan-T5 LLM for a multi-class classification task using designed templates (section A. Flan-T5 in figure <ref type="figure" coords="4,333.02,634.84,3.61,10.91">1</ref>). Flan-T5, a state-of-the-art language model based on the Transformer architecture, was initially pre-trained on a vast amount of text data to capture the intricacies of language semantics. By fine-tuning Flan-T5 on our specific task, we aimed to enhance its ability to accurately classify text inputs into multiple predefined classes where later only the encoder component will be used. To accomplish this, we followed a systematic approach that involved filling ùëá ùëñ (ùëà ùëó ) (where ùëó is user ùëó-th in the training dataset) prompt templates for all the users and feeding the Flan-T5 as input with the respective class as output. By conditioning the Flan-T5 model on these templates, we generated synthetic training examples for each class, augmenting the original labeled data. This approach not only helped address the issue of the low-resource nature of the problem but also enhanced the model's generalization capability by exposing it to diverse and informative examples, which resulted in generalized encoder architecture for the bi-encoder component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bi-Encoder Model with Multiple Losses</head><p>LLMs hallucination is one of the weaknesses of LLMs which sometimes makes them hard for classification tasks due to the limited available classes automatic classification with LLMs without interfering is tricky, especially when we are performing an FSL, but nevertheless, they contain a large amount of information about different topics which make them very good few-shot learners <ref type="bibr" coords="6,169.73,190.73,16.25,10.91" target="#b17">[17]</ref>.</p><p>The Bi-Encoder model is a type of encoder architecture where commonly used in tasks such as semantic similarity. It is designed to encode pairs of text inputs and measure their semantic similarity. In our case, considering the ùëá (ùë¢ ùëñ ) as a set of filled prompt templates per user as a premise, we created the ùê∂ ùëñ hypothesis where ùê∂ ùëñ represents the hypotheses template for the user ùëñ-th (hypotheses templates per subtask listed in the table 4 at appendix). So per a user, we can create a ‚à™ 10 ùë°=1 (ùëá ùë° (ùë¢ ùëñ ), ùê∂ ùëñ ) premise-hypothesis set as positive samples for training a Bi-Encoder model to correlate users with similar classes into a high-dimensional space, where it will learn premise representations in semantic space that have a similar hypothesis. To further optimize the performance of the Flan-T5, we have explored Bi-Encoder models with the use of multiple losses during training. Incorporating multiple loss functions allows the model to capture different aspects of the data and learn more robust and accurate representations. We applied the following loss during training a Flan-T5-Encoder with a Bi-Encoder strategy.</p><p>‚Ä¢ Contrastive Learning Loss (CL): It encourages similar premise-hypothesis pairs to have higher similarity scores, while dissimilar pairs (premise with incorrect hypothesis) have lower scores. This loss effectively trains the model to distinguish between relevant and irrelevant pairs. This results in identifying the premise (users) with the relevant hypothesis (label) as a similar premise-hypothesis pair and the incorrect hypothesis as a dissimilar pair. ‚Ä¢ Multiple Negative Ranking Loss (MNR): It is crucial to explicitly optimize the model for ranking relevant user-label (premise-hypothesis) pairs with higher than irrelevant ones. Ranking loss is used to directly optimize the ranking order of user-label (premisehypothesis) pairs. By incorporating a ranking loss, the Bi-Encoder model can better capture the relative importance of different users for a given label candidate.</p><p>The MNR uses positive premise-hypothesis (user-label) pairs for the training and it automatically will rank the respective hypothesis at a higher rank than the other unrelated hypotheses. However, CL except for the positive premise-hypothesis pairs requires negative pairs as well which means a user with an incorrect label (premise with incorrect hypothesis). For this, we created automatically negative samples by corrupting the hypothesis with premises during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Testing</head><p>The BiEncoder model consists of two encoders, referred to as the "premise encoder" and the "hypothesis encoder". These encoders independently transform the premise and hypothesis texts into fixed-dimensional representations. As a result of these, we will obtain ùëÉ 10*1024 and ùêª ùëõ*1024 matrixes as a fixed-dimensional representation of premises and hypothesis, respectively.</p><p>Where ùëõ is the number of label candidates and for subtask 1 and subtask 2 it is set to ùëõ = 5 and for subtask 3 it is set to ùëõ = 4. Next, we calculated the cosine-similarity matrix between premises ùëâ and hypothesis ùëà matrixes as following:</p><formula xml:id="formula_0" coords="7,235.31,288.68,124.65,33.58">ùëÜ = 10 ‚àëÔ∏Å ùë°=1 ùëõ ‚àëÔ∏Å ùëê=1 ùëêùëúùë†ùëñùëõùëí(ùëÉ ùë° , ùêª ùëê )</formula><p>Where ùëÜ 10*ùëõ is the obtained similarity matrix, next for premise-based hypothesis identification we chose the maximum probable hypothesis per all 10 premises as follows:</p><formula xml:id="formula_1" coords="7,234.45,369.08,126.38,12.06">ùêª ‚Ä≤ = ùëéùëüùëîùëöùëéùë•(ùëÜ, ùëéùë•ùëñùë† = 1)</formula><p>Where ùêª ‚Ä≤ is a predicted hypothesis for all 10 premises and the most appeared hypothesis in ùêª ‚Ä≤ will be chosen as a final hypothesis prediction as follows:</p><formula xml:id="formula_2" coords="7,262.65,429.75,69.98,12.06">ùê∂ = ùëöùëúùëëùëí(ùêª ‚Ä≤ )</formula><p>Where ùê∂ is the final candidate hypothesis for the input premise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Dataset: Table <ref type="table" coords="7,158.87,510.17,5.07,10.91" target="#tab_1">1</ref> presents the statistics of the dataset <ref type="bibr" coords="7,327.55,510.17,16.24,10.91" target="#b18">[18]</ref>, which consists of 380 users in total for subtask 1 with a maximum of 10 tweets per user in 5 categories, 722 users for subtask 2 with a single tweet per in 5 categories, and 548 users for subtask 1 with a single tweet per user in 4 categories. The dataset is balanced in the training where for subtask 1 per each class only 32 users are provided and for subtask 1 and 2 only 64 users per class. We split the train set into the 50/50 proportion for experimental analysis where the stats are presented in the table <ref type="table" coords="7,468.34,577.91,3.74,10.91" target="#tab_1">1</ref>.</p><p>Metrics: According to the multi-class nature of the subtasks we have used Macro F1 as an evaluation metric.</p><p>Training Setups: We considered the Flan-T5-Large<ref type="foot" coords="7,317.65,628.14,3.71,7.97" target="#foot_1">2</ref> variant for training models. We utilized a consistent training strategy for all subtasks where Flan-T5 trained using AdamW optimizer with a learning rate of 1ùëí -5 For 10 epoch and batch size of 4. Also, the Bi-Encoder model is trained to minimize the combined loss, which is a weighted sum of the individual losses. The model's parameters are updated using AdamW optimizer with and learning rate of 1ùëí -5 and batch size of 4. The CL requires a margin to put negative samples at least a margin further apart from the anchor than the positive, where we set this metric into 0.5 during the training with cosine distant metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We experimented using a train-test set from the train set as an evaluation dataset to analyze the proposed model. Table <ref type="table" coords="8,211.25,411.07,5.13,10.91" target="#tab_2">2</ref> shows the experimentally designed models and their results per subtask. In the following, we discussed the experimental models and our analysis for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Models</head><p>In our experimental analysis of the models, we evaluated five different models based on their F1 Macro scores. To compare the proposed methods, we implemented two baseline models, Ran-domBaseline and Flan-T5 + Zero-Shot, provided initial performance benchmarks. Subsequently, we designed and tested three additional models:</p><p>1. RandomBaseline: A random prediction model that predicts based on the random selection of a class from candidate label sets. 2. Flan-T5 + Zero-Shot: The Flan-T5 has been trained on NLI task so to use Flan-T5 for inferencing in a zero-shot manner, we used "xnli: premise: {premise} hypothesis: {hypothesis}" prompt template to query Flan-T5. Where {premise}, and {hypothesis} are placeholder for premise and hypothesis. 3. Flan-T5 + Few-Shot: A text-to-text with a few-shot scenario and instruction tuning for text classification. 4. Flan-T5 + Bi-Encoder+CL+MNR: A Bi-Encoder model that uses CL and MNR losses with pretrained Flan-T5 encoder component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Flan-T5+Few-Shot+Bi-Encoder+CL+MNR:</head><p>A Bi-Encoder model that uses CL and MNR losses with finetuned Flan-T5-Encoder model from Flan-T5+Few-Shot model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Findings</head><p>Baseline models: The RandomBaseline model exhibited limited performance across all three subtasks, with an average F1 Macro score of 23.92. The Flan-T5+Zero-Shot model also demonstrated below average level performance, with an average F1 Macro score of 16.45 on all three tasks. These results indicate that both baseline models struggled to effectively classify the different classes in the given tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot learning:</head><p>The Flan-T5+Few-Shot model showed substantial improvements in performance regarding the baseline model, with an average F1 Macro score of 57.42. This model successfully leveraged FSL techniques to adapt to the classification tasks, resulting in improved all subtasks by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-Encoder with multiple loss learners:</head><p>The Flan-T5 + Bi-Encoder + CL + MNR model demonstrated competitive performance, with an average F1 Macro score of 56.15. The inclusion of bi-encoders and multiple-loss learners contributed to enhanced classification accuracy.</p><p>Few-Shot Flan-T5 with Bi-Encoder and multiple loss learners: The Flan-T5 + Few-Shot +Bi-Encoder+CL+MNR model achieved the highest overall performance among all the models tested, with an average F1 Macro score of 59.05. This model combined few-shot learning techniques on Flan-T5 with bi-encoders, contrastive learning, and multiple negative ranging losses resulting in the most accurate classification across the evaluated subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Best performers:</head><p>The Flan-T5 model with few-shot scenario performed well on subtask 2 by Macro F1 score of 63.11 by 2.35 % better than Flan-T5 + Few-Shot +Bi-Encoder+CL+MNR, however, Flan-T5 + Few-Shot +Bi-Encoder+CL+MNR performed very well on both subtask 1 and subtask 2.</p><p>Our experimental analysis highlights the effectiveness of incorporating advanced techniques such as few-shot learning, bi-encoders, and multiple-loss learners. The Flan-T5+Few-Shot+Bi-Encoder+CL+MNR model emerged as the top-performing model, followed closely by the Flan-T5+Few-Shot and Flan-T5+Bi-Encoder+CL+MNR models. So for the final submission, we submitted Flan-T5+Few-Shot+Bi-Encoder+CL+MNR and Flan-T5+Few-Shot models where according to the best performer results among them we obtained the best F1 Macro of 43.06 for subtask 1, 63.11 for subtask 2, 73.34 for subtask 3, and average F1 Macro of 59.84 for final results in experimental setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative Analysis</head><p>Bottleneck in terms of performance: All the experiments reveal that models are suffering from better performance in subtask 1 due to the high input text (10 tweets per user) that overlaps semantically with other classes. It shows that the author's style analysis <ref type="bibr" coords="10,410.39,471.00,17.84,10.91" target="#b19">[19]</ref> in finding author profiles is still important in getting good performance in subtask 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of multiple losses:</head><p>By combining multiple losses, the Bi-Encoder model can leverage the strengths of each loss function and achieve improved performance. The CL loss encourages clear separation between relevant and irrelevant pairs, and the MNR ranking loss emphasizes the correct ordering of hypothesis (or label candidates). This combination allows the model to learn more comprehensive representations and enhance its ability to accurately measure semantic similarity and determine the correct hypothesis for premises.</p><p>Visualization: Given author embedding learned by different models, the t-SNE visualization in figure <ref type="figure" coords="10,118.38,604.28,5.11,10.91">2</ref> shows that Flan-T5+Few-Shot+Bi-Encoder+CL+MNR learn more discriminative author embeddings that are more distinguishable. Generally, the visualizations show that Bi-Encoder models produce linearly separable features that make them good models than Flan-T5-Few-shot, where subtask 1, 2, and 3 clearly shows that similar users scattered all over the semantic space which makes Flan-T5-Few-shot not well generalized for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Final Results</head><p>In a final submission, our team symbol obtained averaged F1 Macro scores of 52.31 for subtask 1, 61.21 for subtask 2, and 65.83 for subtask 3 over an unseen test set. For task 1, the symbol achieved 11th out of 27 participants, for task 2, the symbol acquired 6th place among 20 submissions, and for task 3, the symbol gained 3rd place among 21 teams. In all tasks, our proposed model defeats all the respective baseline models, it again confirms the quality proposed model in profiling cryptocurrency influencers with few-shot learning task. Overall, we obtained 2nd place according to the official ranking 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In conclusion, our experimental analysis focused on three multi-class classification subtasks within the context of cryptocurrency influencer profiling. The results demonstrated the effectiveness of integrating advanced techniques, including few-shot learning, bi-encoders, and multiple-loss learners, to leverage LLMs. Our approach yielded promising outcomes in development by achieving F1 Macro scores of 43.36 for subtask 1, 63.11 for subtask 2, and 73.34 for subtask 3. As a result, our team symbol achieved F1 Macro scores of 52.31 for subtask 1, 61.21 for subtask 2, and 65.83 for subtask 3. The symbol team achieves 2nd rank among competitors where the results highlight the capability of our approach in accurately determining influencers' profiles, intentions, and interests, as confirmed through manual evaluation. While our findings provide compelling evidence of the effectiveness of the applied techniques, there is still room for further exploration and improvement. Continued research and development can uncover additional avenues to enhance the performance and capabilities of our approach, opening up new possibilities in the field of author profiling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,89.29,460.38,416.69,8.93;9,89.29,472.34,369.36,8.96"><head>Subtask 3 :Figure 2 :</head><label>32</label><figDesc>Figure 2: The t-SNE visualization of the author embeddings learned from the different models for all three subtasks. The models trained on Train-Train set and visualized in all the train data's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,417.17,93.82"><head>Table 1</head><label>1</label><figDesc>Statistics of the experimental dataset. We split the train set into train-train and train-test datasets for profiling cryptocurrency influencers with few-shot learning.</figDesc><table coords="7,120.99,134.01,353.29,50.30"><row><cell>Task</cell><cell cols="5">No. of Classes Train-Train Train-Test Total-Train Final-Test</cell></row><row><cell>Subtask 1</cell><cell>5</cell><cell>80</cell><cell>80</cell><cell>160</cell><cell>220</cell></row><row><cell>Subtask 2</cell><cell>5</cell><cell>160</cell><cell>160</cell><cell>320</cell><cell>402</cell></row><row><cell>Subtask 3</cell><cell>4</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>292</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.93,90.49,420.41,153.99"><head>Table 2</head><label>2</label><figDesc>Reported F1-Macro Results for Subtask 1, Subtask 2, and Subtask 3 in Baseline and Experimental Models with Train-Test Set, and Final Results with Test Set for team symbol in the AP@2023.</figDesc><table coords="8,95.27,134.01,414.07,110.47"><row><cell>Model</cell><cell cols="4">Subtask 1 Subtask 2 Subtask 3 Average</cell></row><row><cell>Experimental results on Train-Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RandomBaseline</cell><cell>23.07</cell><cell>17.52</cell><cell>31.17</cell><cell>23.92</cell></row><row><cell>Flan-T5+Zero-Shot</cell><cell>16.61</cell><cell>6.66</cell><cell>26.07</cell><cell>16.45</cell></row><row><cell>Flan-T5+Few-Shot</cell><cell>36.47</cell><cell>63.11</cell><cell>72.68</cell><cell>57.42</cell></row><row><cell>Flan-T5+Bi-Encoder+CL+MNR</cell><cell>43.36</cell><cell>60.35</cell><cell>64.75</cell><cell>56.15</cell></row><row><cell>Flan-T5+Few-Shot+Bi-Encoder+CL+MNR</cell><cell>43.06</cell><cell>60.76</cell><cell>73.34</cell><cell>59.05</cell></row><row><cell>The symbol team submission results on Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flan-T5+Few-Shot+Bi-Encoder+CL+MNR</cell><cell>52.31</cell><cell>61.21</cell><cell>65.83</cell><cell>59.78</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.04,216.30,8.97"><p>https://github.com/HamedBabaei/author-profiling-pan2023</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,92.57,671.03,157.22,8.97"><p>https://huggingface.co/google/flan-t5-large</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>Subtask 1</head><p>This user profile in cryptocurrency is a no influencer. This user profile in cryptocurrency is a nano. This user profile in cryptocurrency is a micro. This user profile in cryptocurrency is a macro. This user profile in cryptocurrency is a mega.</p></div>
<div><head>Subtask 2</head><p>This influencer interest is a technical information. This influencer interest is a price update. This influencer interest is a trading matters. This influencer interest is a gaming. This influencer interest is a other.</p></div>
<div><head>Subtask 3</head><p>This influencer intent is a subjective opinion. This influencer intent is a financial information. This influencer intent is a advertising. This influencer intent is a announcement.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Apendix Table 3</head><p>Designed prompt templates for Subtask 1, Subtask 2, and Subtask 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask Prompt Templates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 1</head><p>Identify cryptocurrency influencers profiles from given tweets: \n\n Tweets: {tweets} User tweets: "{tweets}" \n\n Question: What is the profile of this user on twitter? {tweets} \n\n What profile is appropriate for this user from a cryptocurrency perspective? {tweets} \n\n Is this a cryptocurrency influencers? Given a collection of tweets from a user: "{tweets}" \n\n What is the user profile as a cryptocurrency influencers? What is the user related aspect of the influencer using the following tweets?? \n\n Tweets:{tweets} Given the following user tweets, determine the profile of this user as a cryptocurrency influencer: \n\n User tweets: {tweets} Consider the tweets provided: "{tweets}" \n\n What would be an appropriate profile for this user from a cryptocurrency perspective? A user has posted the following collection of tweets: "{tweets}" \n\n What is the user's profile as a cryptocurrency influencer? Evaluate the given tweets to identify cryptocurrency influencers: \n\n Tweets: {tweets}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 2</head><p>Identify the user interest in cryptocurrency from the given tweets: \n\n Tweets: {tweets} Analyze the given tweets to identify if the user has a particular interest in cryptocurrency. \n\n Tweets: {tweets} User tweets: "{tweets}" \n\n Question: What is the user interest in cryptocurrency? Given collection of tweets from a user: "{tweets}" \n\n What is the user interest in cryptocurrency influencers? {tweets} \n\n Examine the tweets and determine if the user exhibits an interest in cryptocurrency. {tweets} \n\n From the provided tweets, ascertain whether the user shows interest in following or engaging with cryptocurrency? Evaluate the given tweets to identify the user's interest in cryptocurrency:\n\n Tweets: {tweets} Given the following user tweets, determine the user interest: \n\n User tweets: {tweets} Consider the tweets provided: "{tweets}" \n\n Identify the user interest? A user has posted the following collection of tweets: "{tweets}" \n\n What is the user's preference in the cryptocurrency?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 3</head><p>Identify the user intent in cryptocurrency from the given tweets: \n\n Tweets: {tweets} Analyze the given tweets to identify if the user has a particular purpose in cryptocurrency. \n\n Tweets:{tweets} User tweets: "{tweets}" \n\n Question: What is the user intent in cryptocurrency? Given collection of tweets from a user: "{tweets}" \n\n What is the user purpose of cryptocurrency influencers? {tweets} \n\n Examine the tweets and determine if the user exhibits an intent in cryptocurrency. {tweets} \n\n From the provided tweets, ascertain whether the user shows purpose in following or engaging with cryptocurrency? Evaluate the given tweets to identify the user's intent in cryptocurrency: \n\n Tweets: {tweets} Given the following user tweets, determine the user aim in cryptocurrency: \n\n User tweets: {tweets} Consider the tweets provided: "{tweets}" \n\n Identify the user intent? A user has posted the following collection of tweets: "{tweets}" \n\n What is the user's goal in the cryptocurrency?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Hypotheses templates for labels in Subtask 1, Subtask 2, and Subtask 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask Hypotheses Templates</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,468.67,393.33,10.91;11,112.66,482.22,393.33,10.91;11,112.66,495.77,107.17,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12837</idno>
		<title level="m" coord="11,457.60,468.67,48.39,10.91;11,112.66,482.22,312.60,10.91">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,509.32,394.53,10.91;11,112.66,522.87,394.53,10.91;11,112.66,536.42,394.53,10.91;11,112.66,549.97,394.53,10.91;11,112.66,563.52,393.33,10.91;11,112.66,577.07,276.38,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,112.66,563.52,244.56,10.91">Language models are few-shot learners -special version</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,365.24,563.52,140.75,10.91;11,112.66,577.07,144.38,10.91">Advances in Neural Information Processing Systems 2020-Decem</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,590.62,394.52,10.91;11,112.66,604.17,394.53,10.91;11,112.66,617.71,394.52,10.91;11,112.66,631.26,393.53,10.91;11,112.66,644.81,394.53,10.91;11,89.29,669.46,2.78,5.98;11,92.57,671.03,221.77,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,294.21,617.71,212.97,10.91;11,112.66,631.26,393.53,10.91;11,112.66,644.81,42.05,10.91">Overview of PAN 2023: Authorship Verification, Multi-Author Writing Style Analysis, Profiling Cryptocurrency Influencers, and Trigger Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-R√≠os</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pƒôzik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<ptr target="https://pan.webis.de/clef23/pan23-web/author-profiling.html" />
		<editor>. G. Stefanos Vrochidis</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,86.97,395.17,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,394.53,10.91;12,112.66,127.61,65.44,10.91" xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="12,371.28,86.97,136.55,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,197.14,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,342.79,114.06,159.86,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="12,112.66,141.16,393.33,10.91;12,112.66,154.71,393.33,10.91;12,112.14,168.26,159.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,466.53,141.16,39.46,10.91;12,112.66,154.71,261.87,10.91">Profiling Cryptocurrency Influencers with Few shot Learning at PAN</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borrego-Obrador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,417.97,154.71,88.02,10.91;12,112.14,168.26,47.32,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="12,167.44,168.26,73.93,10.91">Notebook Papers</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,181.81,393.53,10.91;12,112.66,195.36,307.55,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,294.71,181.81,123.12,10.91">A survey of transfer learning</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-016-0043-6</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,393.33,10.91;12,112.66,222.46,395.01,10.91;12,112.66,238.45,97.35,7.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,284.78,208.91,221.21,10.91;12,112.66,222.46,83.43,10.91">Generalizing from a Few Examples: A Survey on Few-shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386252</idno>
		<idno type="arXiv">arXiv:1904.05046</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,206.82,222.46,117.78,10.91">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,393.33,10.91;12,112.66,263.11,393.33,10.91;12,112.66,276.66,393.32,10.91;12,112.66,290.20,393.33,10.91;12,112.66,303.75,394.03,10.91;12,112.66,317.30,185.51,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,323.15,249.56,182.83,10.91;12,112.66,263.11,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="12,327.87,263.11,178.11,10.91;12,112.66,276.66,393.32,10.91;12,112.66,290.20,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="12,112.66,330.85,394.53,10.91;12,112.66,344.40,393.33,10.91;12,112.66,357.95,394.04,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,112.66,344.40,341.66,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j" coord="12,462.63,344.40,43.36,10.91;12,112.66,357.95,123.70,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,371.50,394.53,10.91;12,112.66,385.05,394.53,10.91;12,112.66,398.60,394.53,10.91;12,112.66,412.15,393.32,10.91;12,112.66,425.70,395.01,10.91;12,112.66,441.69,97.35,7.90" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2210.11416" />
		<title level="m" coord="12,474.16,412.15,31.83,10.91;12,112.66,425.70,175.55,10.91">Scaling Instruction-Finetuned Language Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,452.79,393.33,10.91;12,112.66,466.34,393.33,10.91;12,112.33,479.89,29.19,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,427.04,452.79,78.95,10.91;12,112.66,466.34,239.16,10.91">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,493.44,395.17,10.91;12,112.26,506.99,393.94,10.91;12,112.66,520.54,395.01,10.91;12,112.66,536.53,97.35,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,332.46,493.44,175.38,10.91;12,112.26,506.99,110.79,10.91">Few-Shot Learning with Siamese Networks and Label Tuning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>P√©rez-Torr√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.584</idno>
		<idno type="arXiv">arXiv:2203.14655</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.16,506.99,270.04,10.91;12,112.66,520.54,115.19,10.91">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8532" to="8545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,547.64,393.33,10.91;12,112.26,561.19,393.73,10.91;12,112.28,574.74,395.39,10.91;12,112.66,588.29,281.25,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,384.93,547.64,121.05,10.91;12,112.26,561.19,45.20,10.91">Active Few-Shot Learning with FASL</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>P√©rez-Torr√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08473-7_9</idno>
		<idno type="arXiv">arXiv:2204.09347</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,413.35,574.74,26.24,10.91">LNCS</title>
		<imprint>
			<biblScope unit="volume">13286</biblScope>
			<biblScope unit="page" from="98" to="110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,601.84,394.62,10.91;12,112.66,615.39,394.53,10.91;12,112.66,628.93,90.72,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,222.23,601.84,264.64,10.91">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,615.39,390.05,10.91">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,642.48,393.32,10.91;12,112.66,656.03,251.22,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01403</idno>
		<title level="m" coord="12,297.74,642.48,208.24,10.91;12,112.66,656.03,121.36,10.91">Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,669.58,395.16,10.91;13,112.66,86.97,395.01,10.91;13,112.66,102.96,97.35,7.90" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,191.33,86.97,285.28,10.91">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lukacs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mik-Los</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,114.06,393.71,10.91;13,112.66,127.61,395.01,10.91;13,112.66,141.16,346.85,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,358.17,114.06,148.20,10.91;13,112.66,127.61,321.57,10.91">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
		<ptr target="https://doi.org/10.1145/3560815.doi:10.1145/3560815" />
	</analytic>
	<monogr>
		<title level="j" coord="13,442.71,127.61,64.96,10.91;13,112.66,141.16,22.48,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,394.53,10.91;13,112.66,168.26,394.53,10.91;13,112.66,181.81,394.53,10.91;13,112.66,195.36,394.53,10.91;13,112.66,208.91,302.57,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m" coord="13,112.66,208.91,172.82,10.91">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,222.46,393.61,10.91;13,112.66,236.01,395.00,10.91;13,112.66,249.56,149.51,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="13,362.34,222.46,143.93,10.91;13,112.66,236.01,158.70,10.91">PAN23 Profiling Cryptocurrency Influencers with Few-shot Learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chinea-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7701748</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7701748.doi:10.5281/zenodo.7701748" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,263.11,393.33,10.91;13,112.66,276.66,393.33,10.91;13,112.66,290.20,393.33,10.91;13,112.14,303.75,394.56,10.91;13,112.66,317.30,65.09,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,417.29,263.11,88.70,10.91;13,112.66,276.66,393.33,10.91;13,112.66,290.20,37.09,10.91">Profiling Irony and Stereotype Spreaders with Encoding Dependency Information using Graph Convolutional Network</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">B</forename><surname>Giglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahgouy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rahgooy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Seals</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/paper-190.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,414.70,290.20,91.29,10.91;13,112.14,303.75,48.29,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="13,169.42,303.75,106.62,10.91">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
