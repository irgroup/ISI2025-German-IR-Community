<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,397.02,15.42;1,89.29,106.66,194.28,15.42;1,89.29,129.00,220.08,11.96">SEUPD@CLEF: Team DARDS -IR System for Short and Long Term Retrieval Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,76.54,11.96"><forename type="first">Daniel</forename><surname>Carlesso</surname></persName>
							<email>daniel.carlesso@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.47,154.90,77.52,11.96"><forename type="first">Riccardo</forename><surname>Gobbo</surname></persName>
							<email>riccardo.gobbo.2@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.63,154.90,68.32,11.96"><forename type="first">Simone</forename><surname>Merlo</surname></persName>
							<email>simone.merlo@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.60,154.90,75.50,11.96"><forename type="first">Angela</forename><surname>Pomaro</surname></persName>
							<email>pomaro@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Rome La Sapienza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,69.83,11.96"><forename type="first">Diego</forename><surname>Spinosa</surname></persName>
							<email>diego.spinosa@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.12,168.85,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,397.02,15.42;1,89.29,106.66,194.28,15.42;1,89.29,129.00,220.08,11.96">SEUPD@CLEF: Team DARDS -IR System for Short and Long Term Retrieval Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">387E707F3ED936C3B79B20D5DCEB6E24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF 2023</term>
					<term>LongEval</term>
					<term>Spam detection</term>
					<term>Translation</term>
					<term>Synonyms</term>
					<term>Reranking</term>
					<term>Boosting</term>
					<term>Query Expansion</term>
					<term>Document Expansion</term>
					<term>ANOVA Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz 16</term>
					<term>0 GB (DDR4) Seagate Mobile HDD ST1000LM035 -Machine Intel(R) Core(TM) i5-2400S CPU * RAM: 8</term>
					<term>0 GB (DDR3) * GPU: NVIDIA GeForce GTX 750Ti AMD A8-7410 APU Baititon 480GB SSD</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search Engines represents an important application of Information Retrieval. In particular, a major branch of Search Engines is devoted to web search. In this document we summarize our work to produce a submission for the CLEF LongEval initiative [1], primarily concerning web search. The described activity first focuses onto the development of an indexing and searching IR system with the best possible performance based on the provided training data then evaluates its performance on test data coming from different scenarios. We first introduce the task and related problems. Subsequently we present the retrieval systems that we have used for the program submission. Afterwards, we discuss the results obtained with the various systems and compare them in the training scope to explain why some systems perform better than others. Finally, metrics analysis is extended to the additional scenarios LongEval focuses on, along with statistical considerations over the systems' output.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Search Engines (SE) are used daily by every person in the world. In particular, the main application of SE is the web search consisting in the retrieval of documents (web pages) in the web. Generally, people expect to provide a sentence as input (a query, representing their interest or information need) and to get back a list of results that are highly related (relevant) to that piece of text.</p><p>This document summarizes the information retrieval systems developed by the team DARDS as part of the Search Engines 2022/2023 course, which is held at the University of Padua, in order to address the task 1 "Retrieval" of the "LongEval" lab proposed by CLEF 2023. The goal of the lab, hence of the systems, is to retrieve the most relevant documents given a query (a short sentence representing what the user would use to look for its information need). Furthermore, the lab aims at understand the time persistence and reliability of the developed systems by testing them using some data sampled at different times.</p><p>The paper is organized as follows: Section 2 describes our approach; Section 3 explains our experimental setup; Section 4 discusses our main findings with relative failure analysis; eventually, Section 5 draws some conclusions and outlooks for future work. bridge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>The first goal we intended to achieve was to reach a stable (i.e. without Runtime Exceptions and usable with the given collections) and simple version of a basic indexing and searching system, to be used as a baseline to experiment additional features on. Afterwards, the workflow split into several lines, each applying a different improvement strategy, such as different text analyzers, different filter configurations or different document preprocessing operations. Once a better system was found, it would eventually become the new baseline to run further experiments on. In order to operate in this way, each line would evolve on a different working branch of our git repository, with its performance measured mainly through trec_eval's nDCG and/or MAP metric. Some tools were also developed to help us analyze what kind of errors the run contained, to perform some pre-processing to the collection documents and to solve some translation problems.</p><p>The main programming language that we used to develop our systems is Java, while some tools have been developed using Python. In particular, the core of our work was implemented by exploiting the Lucene java library <ref type="bibr" coords="2,256.10,443.56,11.43,10.91">[2]</ref>.</p><p>We can split our systems in four main components:</p><p>• Parser • Analyzer • Indexer • Searcher</p><p>In this section we describe the general workflow of our systems (Section 2.1), the developed tools ( Sections 2.2 and 2.3), the general components (Sections 2.4, 2.5, 2.6 and 2.7) and the complete systems (Section 2.8).  3. Analyze the documents' fields to convert them into a stream of tokens and index them to perform the search (single index folder). 4. Parse and analyze the queries to convert them into a stream of tokens. 5. Perform the search on the index (one query at the time). 6. Rerank the documents retrieved with the search (optional stage). 7. Print the results of the search into a dedicated text file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pre-processing tools</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">SPAM parser</head><p>By analyzing the results produced with the French-based system (the simplest one) on the French collection, we realized that relevant documents for a specific query were not included in the first one thousand documents retrieved by the system for that query. Before increasing the system's complexity through new features, while inspecting the retrieved documents, it turned out that several of the non-relevant documents were actually ranked as relevant documents due to the high number of words repetitions inside them. More precisely, excluding documents that were actually not relevant, we noticed that in the analyzed documents the words were not different and the text contained a sensible number of repetitions. This new information drove us to develop term frequency analyses to avoid the retrieval of documents considered "spam" during the search task or, even better, the addition to the Lucene index during the indexing phase, reducing its overall size.</p><p>With the term "spam" we denote the type of documents that "hack" the retrieval system through the use of a set of words contained in the user query, that brings the document to the top of the document-ranked list.</p><p>The following documents are some examples of "spam". We can notice that the length of the texts is very short compared to the one of an average document: The methodology that we applied to fulfil this goal takes into account two main factors:</p><formula xml:id="formula_0" coords="4,117.44,272.92,5.36,9.72">"</formula><p>1. The ratio between the most frequent word and the total length of the document, excluding articles and stopwords: Max freq Doc length .</p><p>The value obtained is as low as the term occurrences are spread all over the document. 2. The ratio between the number of words in the document and the number of different words used in it: Doc length Number different words .</p><p>(2)</p><p>This ratio reaches higher values (closer to Document Length) on documents formed by a limited set of distinct words. Spam pages often contain a small set of repeated keywords, hence an high value of this ratio may be an indicator for spam.</p><p>In order to combine the two approaches, we can obtain a third ratio by multiplying the previous ones and comparing the result with a threshold:</p><formula xml:id="formula_2" coords="4,124.01,591.86,382.63,26.01">Max freq Doc length • Doc length Number different words = Max freq Number different words &gt; threshold<label>(3)</label></formula><p>During the process, one more threshold is checked: the length of the analyzed word. Since the implemented parser splits the sentences using blank spaces, it may happen that sequence of characters (e.g. "⋆ ⋆ ⋆", "NVO",..) or symbols (e.g. €) cause an erroneous document exclusion. By applying the proposed threshold these errors are avoided. To choose the best one, a first run (without threshold constraint) was performed to analyze the reasons for document exclusions, highlighting issues for example with symbols, acronyms and English articles. In the next steps, the threshold was increased resulting in a value of 4 since the program could ignore useless words while considering words containing more characters (with a higher probability that the word was an actual word and not anything else). A better approach to this problem would be an ML-oriented algorithm which would choose the best threshold based on the final score obtained by the overall system. Both thresholds (on ratios and word length) used by the script were experimentally obtained by manually analyzing some spam documents from the training corpus (as a reference, valid threshold values could be 4 for minimum word size and 0.8 for the ratio).</p><p>The entire pre-processing task was implemented in a separate Python file which automatically reads all the files in the French collection and for each file it saves a new one that doesn't contain the documents marked as spam. The overall flow is reported in Figure2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Synonyms</head><p>In order to try to increase the system's performance, a query expansion method was implemented through the use of synonyms. The files containing French synonyms available on the internet were not sufficient and covered only a small part of the entire French dictionary. For this reason, a new collection of words that could be easily used as synonyms of words found in the queries during the search task was needed. To address this need, another Python tool that automatically performs a request to a specialized site (dictionary.reverso.net/french-synonyms/) and retrieves all the related synonyms was created. All the new words found are placed in the same line as the searched word so that the Searcher (2.7) can easily extract the information needed. The overall flow is shown in Figure <ref type="figure" coords="5,229.58,639.55,3.74,10.91" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Translation tool</head><p>Since we found the translations of the documents and the queries provided by the LongEval organizers to be imprecise, since the meaning of the original documents and queries some times turned out to be completely different from the translated ones, we developed a tool that allows to translate a piece of text from French to English. The tool has been implemented as a script in Google's "Google App Script" platform and then deployed as a web application. This tool must be used as a REST resource.</p><p>The code used to implement this tool can be found HERE.<ref type="foot" coords="6,360.29,469.85,3.71,7.97" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Parser</head><p>The parser that we implemented for all of our systems is a basic parser. This component simply reads all the files with a given extension (.txt) present in a directory tree (specified as input) and converts all the Trec formatted documents into an instance of the Lucene's class Document that has two fields: the identifier of the document (ID) and the body of the document (BODY). Some systems in addition to that also read another file (whose path is given as input) that contains pairs "document identifier"-"document url" and add an additional url field (URL) to the instances of the documents based on the identifier. In some systems, we decided to add this field for two reasons: the URL could help decreasing the rank position of the document containing "spam" (see section 2.2) and it could help us in the queries that contain a website url.</p><p>Furthermore, we tried also to add another field to the document instances that was meant to contain the document keywords (retrieved exploiting an algorithm called RAKE <ref type="bibr" coords="7,462.19,100.52,11.49,10.91" target="#b1">[3]</ref>, Rapid Automatic Keyword Extraction), but it required too much computation time so we didn't implement this option.</p><p>Eventually, the queries parser uses the tsv file format and its parsing action is done in the searcher module (see Section 2.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Analyzer</head><p>In our systems we implemented customized analyzers instead of using the standard ones proposed by Lucene. This choice was motivated by the fact that this way we could customize the filters pipeline and choose a tokenizer. Tokenizers process the input stream (the document text) and using some predefined rules they split it into tokens: these are the atomic units composing a document. Unlike words, tokens do not have to be grammatically correct or meaningful, but are suitable for processing and comparison operations. We tried two different tokenizers:</p><p>• StandardTokenizer: a grammar-based tokenizer constructed with JFlex. It implements the Word Break rules from the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29. • WhiteSpaceTokenizer: it breaks text into terms whenever it encounters a whitespace character.</p><p>We started out using StandardTokenizer and we noticed that expressions separated with hyphens were divided into different tokens. To avoid this, we tried a combination of WhiteSpaceTokenizer and PatternReplaceFilter to remove all punctuations/symbols except for hyphens. But then performance decreased and we backtracked.</p><p>In addition to the StandardTokenizer we used different filters, some of them shared by both English-based and French-based systems: French</p><p>• ElisionFilter: it targets elisions, so removes articles, prepositions and conjunctions placed either in the initial or final part of the token, usually connected by an apostrophe or hyphen. This connection makes it impossible for a generic stopword filter alone to detect and delete those particles. • ASCIIFoldingFilter: it converts alphabetic, numeric, and symbolic Unicode characters which are not in the Basic Latin Unicode block (the first 127 ASCII characters) to their ASCII equivalents, if one exists. This filter was used to remove accents and diacritical marks from French words. • FrenchLightStemFilter: this stemmer implements the "UniNE" algorithm <ref type="bibr" coords="7,453.12,594.36,11.43,10.91" target="#b2">[4]</ref>.</p><p>Note that for the French case other stemmers (FrenchMinimalStemmer, org.taurus.snowball's FrenchStemmer) and chain of stemmers (multiple stemmers in a single analyzer) have been tested and evaluated but the performance of the FrenchLigthStemmer turned out to be the best (by looking at the map and ndcg metrics). English</p><p>• NGramFilter: it generates character n-gram tokens of sizes in the given range. Tokens are ordered by position and then by gram size. • ShingleFilter: it constructs word n-grams, which are token n-grams, from the token stream. • PorterStemFilter: it applies the Porter Stemming Algorithm for English. The results are similar to using the Snowball Porter Stemmer with the language="English" argument. It is coded directly in Java and it is four times faster than the English Snowball stemmer. • KStemFilter: it is an alternative to the Porter Stem Filter for developers looking for a less aggressive stemmer. KStem was written by Bob Krovetz, ported to Lucene by Sergio Guzman-Lara (UMASS Amherst). This stemmer is only appropriate for English language text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared</head><p>• LowerCaseFilter: it converts any uppercase letter in a token to the equivalent lowercase token. All other characters are left unchanged. • StopFilter: it discards, or stops analysis of, tokens that are on the given stop words list.</p><p>• SynonymGraphFilter: it maps single-or multi-token synonyms, producing a fully correct graph output. If used for indexing it must be followed by a FlattenGraphFilter to squash tokens on top of one another, because the indexer can't directly consume a graph. • HyphenatedWordsFilter: it reconstructs hyphenated words that have been tokenized as two tokens because of a line break or other intervening whitespace in the field test.</p><p>If a token ends with a hyphen, it is joined with the following token and the hyphen is discarded. • RemoveDuplicatesTokenFilter: it removes duplicate tokens in the stream. It filters out Tokens at the same position (with position we mean the position given by lucene to the token in the token stream. There could be more tokens in the same position since some operations, like the synonym expansion, add new tokens having the same position of the original token and offset zero) and Term text as the previous token in the stream. • NumberFilter: it removes every token in the token stream that happens to be a number (this filter has been developed by team DARDS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Indexer</head><p>The indexer's job is to store the tokens, generated by processing every document with a given analyzer, into an inverted index (data structure which maps terms to documents). We implemented two types of indexer, DirectoryIndexer and ReRankDirectoryIndexer, the first one takes all the documents in a given directory and stores them into the index, the second one does the same but it can also store a list of documents passed to it. Lucene captures some statistics at indexing time which can then be used to support scoring at query time. To do this it makes use of similarity functions, which set a per-document value for every field in the document. Hence, the similarity determines how Lucene weights terms.</p><p>In our systems we tested different similarities:</p><p>• BM25Similarity </p><formula xml:id="formula_3" coords="9,107.28,284.78,144.93,125.74">• LMDirichletSimilarity • LMJelinekMercerSimilarity • DFRSimilarity • ClassicSimilarity • BooleanSimilarity • AxiomaticF2LOG • AxiomaticF2EXP • IndriDirichletSimilarity • MultiSimilarity</formula><p>In the large majority of cases the BM25Similarity turned out to be the best option to use (see Table <ref type="table" coords="9,115.34,436.70,3.67,10.91" target="#tab_2">1</ref>. Note that the values of the following table refer to an initial system that performed the search on the English corpus provided by CLEF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Searcher</head><p>The searcher is the other fundamental part of any IR system besides the Indexer. This component takes care of doing the matching between a document and a topic, giving as a result a ranking, which marks how relevant a document should be for a certain topic. In our application, topics are real user queries.</p><p>In our systems, the searcher have been customized to first parse the queries from a provided, .tsv formatted file. Then, one query at the time, this component turns the query into a stream of tokens (exploiting a query parser and an analyzer which must be the same used for indexing), converts it into a Lucene Query and searches the index for it. After performing the actual search this tool optionally executes a rerank (see Section 2.7.2) and then prints the results of the overall process into a file.</p><p>Note that in some systems the query is not directly turned into a Lucene Query but in the process the terms are boosted (see Section 2.7.1). Furthermore, in the majority of cases the query is performed by considering the BODY field of the document but in some systems the same query is applied also to the URL field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1.">Boosting</head><p>In some of our systems we decided to not to use a plain query but to boost the term according to some statistics. In particular, we computed a boost factor based on a measure that is quite similar to the the TF-IDF weight. The equations used to compute the boost factor for the term i are the following:</p><formula xml:id="formula_4" coords="10,203.20,201.94,303.44,35.42">𝑤 𝑖 = (︃ 𝑡𝑡𝑓 𝑖 ∑︀ 𝑇 𝑗 𝑡𝑡𝑓 𝑗 )︃ 0.2 • (︂ 1 + log 𝑁 + 1 𝑛 𝑖 + 1 )︂ ,<label>(4)</label></formula><p>where with 𝑡𝑡𝑓 𝑖 we indicate the "total term frequency" (number of occurrences of the term in the entire corpus), with 𝑁 the number of documents in the corpus, with 𝑇 the total number of unique terms in the collection and with 𝑛 𝑖 we denote the number of documents containing the term.</p><p>In order to boost the query terms we:</p><p>1. Compute from the index an hash map that contains as keys the terms and as values the related weight. 2. Get (exploiting the analyzer), from the original text of the query, the list of terms that will compose our Lucene Query. 3. Create a TermQuery for each of the term. 4. Wrap the TermQuery in a BoostQuery, setting the boost based on the hash map (the boost is set to zero if the term doesn't appear in the index). 5. Merge all the BoostQuery(ies) in a BooleanQuery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.2.">Reranking</head><p>After the searcher performed the task, some systems (such as the BM25FRENCHRERANK100 system 2.8.4) execute a rerank. To implement the reranking we simply consider the document retrieved by the search (hits), we create a new index based only on those documents (or only on the top-N document retrieved) and eventually we repeat the same or a slightly modified query (depending on the system) on the new index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Systems</head><p>In this subsection we describe the systems that we implemented. The overall system sequence diagram is reported in Figure <ref type="figure" coords="10,221.99,585.89,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.1.">BM25FRENCHBASE system</head><p>As a baseline for all our work, this IR system for French uses:</p><p>This configuration has been achieved by trying all possible filter configurations while retaining only the better performing ones. The stoplist used by the StopFilter is a custom stoplist created by merging different French stoplist found online and by adding some terms based on the index created (that has been analyzed using Luke). The article's list used by the ElisionFilter is a custom list of terms that we created on the basis of French dictionaries.</p><p>The ASCIIFoldingFilter was used to avoid problems related to French diacritical marks and accents since not in all queries and/or documents were used properly. This system has overall good performance on the test data, especially considering its moderate complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.2.">BM25FRENCHBOOSTURL system</head><p>The BM25FRENCHBOOSTURL system considers a document with three fields: ID,BODY,URL (see Section 2.4). Furthermore this system uses: In this system the query is performed on both the BODY field and the URL field of the document but in a slightly different way. In particular, the main boolean query is composed of two subqueries: one for the BODY field where each term is connected by means of an OR clause, the other for the URL field where term is connected by means of an AND clause. Eventually, the two sub-queries are connected by means of an OR clause to compose the final query. Furthermore, boosting is performed, as described in Section 2.7.1, in the same way for both the URL and BODY field sub-queries (we tried different combination of boosting, for example to give to the URL sub-query terms double the weight, but this combination turned out to be the best).</p><p>The stoplist used by the StopFilter is a custom stoplist created by merging different French stoplist found online and by adding some terms based on the index created (that has been analyzed using Luke).</p><p>The article's list used by the ElisionFilter is a custom list of terms that we created on the basis of French dictionaries.</p><p>The ASCIIFoldingFilter was used to avoid problems related to French diacritical marks and accents since not in all queries and/or documents were used properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.3.">BM25TRANSLATEDQUERIES system</head><p>The BM25TRANSLATEDQUERIES system has been developed as an attempt of running an English system with a better query quality: looking at the supplied English queries, multiple translation mistakes and imprecisions were noticeable (see Table <ref type="table" coords="12,407.51,362.32,5.17,10.91" target="#tab_5">2</ref> for some examples). Therefore, a custom translator was implemented, as defined in Section 2.3.</p><p>The translation of the queries is performed in the searcher right after their parsing and before turning them into a token stream.</p><p>This system uses: Furthermore, to increase the performance, we tried to translate also the documents by using our tool (see Section 2.3) but, since the tool is used as a REST resource and because of the number and length of the documents, this approach resulted to be too demanding in terms of computational time, so we abandoned this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.4.">BM25FRENCHRERANK100 system</head><p>This system has been developed to evaluate the benefits of introducing reranking mechanics (as defined in Section 2.7.2). In order to achieve a better performance, query boosting (see Section 2.7.1) is also included in the Searcher. In comparison to 2.8.2, this system does not take into account the document's URL but it considers only the BODY and ID fields.</p><p>Here follows a more detailed component overview: The rerank is performed as described in Section 2.7.2 and, in this case, the query that is used to repeat the search is the same used for the main search. The query created is a BooleanQuery obtained by connecting the BoostedQuery(ies) (each representing a boosted token of the topic ) by means of an OR clause.</p><p>Note that this system performs the rerank only for the first (most relevant) 100 document retrieved by the first search (for the CLEF LongEval task the maximum number of retrieved documents is 1000). We have chosen to rerank only 100 documents because by looking at the performance through the Trec_Eval tool we noticed that the recall at the document cut-off 100 was sufficiently high. Nonetheless, we also tried to perform the rerank considering all the retrieved document but the performance resulted to be lower.</p><p>Finally, we tried to boost the query performed in the second search in a different way (by recomputing the TF-IDF-like weights considering only the secondary index created in during the reranking step) but the performance decreased (see Table <ref type="table" coords="13,363.72,457.42,3.57,10.91" target="#tab_8">3</ref>).</p><p>The stoplist used by the StopFilter is a custom stoplist created by merging different French stoplist found online and by adding some terms based on the index created (that has been analyzed using Luke). The article's list used by the ElisionFilter is a custom list of terms that we created on the basis The ASCIIFoldingFilter was used to avoid problems related to French diacritical marks and accents since not in all queries and/or documents were used properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.5.">BM25FRENCHSPAM system</head><p>The system BM25FRENCHSPAM implements entirely the BM25FRENCHBASE (2.8.1) applying documents preprocessing. All documents used as input are first parsed by a "spam" parser implemented in Python, as described in Section 2.2.1, and executed before the BM25FRENCHSPAM analyses. Since the pre-process activity uses a threshold above which a document is excluded, it was necessary to understand the average value (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max freq</head><p>Number different words ) of the collection, assuming that most of the documents were valid (i.e. contain meaningful sentences). Using a slightly modified version (not reported) of the already existing parser (Figure <ref type="figure" coords="14,400.62,359.60,4.19,10.91" target="#fig_1">2</ref>) it turned out that the average ratio of the test collection corresponded to 6.5%.</p><p>In order to assign as correct a value as possible, some test has been done using different thresholds and checking the corresponding performance. More specifically, the considered tested values are 7%, 9%, 11% and 15% and the performance are reported in Table <ref type="table" coords="14,450.34,413.80,3.74,10.91">4</ref>.</p><p>With this kind of methodology it was expected that, for thresholds below the average value reported above, the performance would have dropped down since the parsing was acting aggressively removing the relevant documents. On the other hand, for bigger thresholds, it was also expected bad performance since the action of the filter would have been not significant. An intermediate value, instead, would have ensured the right balance between the two cases. The heuristic results show an opposite behaviour of the performance from the one described above since the increase in performance as the threshold increases. This behaviour suggests other implementations (as reported in Section 5) of the frequencies analyses possibly weighting the two ratios used during the comparisons with thresholds. With the consideration of the data discussed above, the threshold used during analyses with test data corresponds to 15%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.6.">BM25FRENCHNOENG system</head><p>After the first spam removal attempt inspecting the documents we noticed how the majority of spam documents were written in English despite being contained in the French collection. So in this second attempt the DirectoryIndexer class was modified in order to execute a preliminary filtering of the documents to index, discarding the ones written in English. This was accomplished by scanning a copy of the document for English stopwords and another copy for French stopwords. A document is inferred to be containing English content if the amount of English stopwords it contains is at least 1.5 times the amount of French stopwords it contains. Since English words are ubiquitous in all languages, a threshold higher than 1 had to be set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query language</head><p>French Document language French Analyzer Same as Baseline Preprocessing Custom DirectoryIndexer to exclude English docs</p><p>As an example, by looking at a batch of 10 discarded documents, we can what is reported in Table <ref type="table" coords="15,115.79,224.24,3.74,10.91">5</ref>.</p><p>Over the whole French corpus the pre-processing excludes 10,81% of the documents. The table above shows how the majority of removed documents are not relevant. However, as reported in Table <ref type="table" coords="15,169.23,264.89,3.77,10.91">7</ref>, no significant effect (nor improvement, nor worsening) over the baseline system has been recorded. This may be traced back to at least two reasons:</p><p>• English documents are seldom picked as relevant for French queries since their tokens do not match. This causes the removal to cause small to none precision improvement. • Some queries such as the ones in the table above are directly specified in English. This makes English documents effectively relevant for them. This causes the removal to slightly worsen recall.</p><p>As already said, this strategy is ineffective on its own but may bring some improvement if paired with other techniques (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.7.">BaseSystem system</head><p>This system is actually the same as 2.8.3 but doesn't implement the query translation so, as a consequence, it uses the English queries provided by CLEF instead of the French queries. This system is used only as a benchmark for comparison with the system described in Section 2.8.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.8.">BM25FRENCHDOCEXPANSION system</head><p>In this system we implemented the idea to expand the document vocabulary to avoid vocabulary mismatch between queries and documents. To do this we use two different analyzers, one for the documents (index time) and one for the queries (search time). This system is similar to BM25FRENCHBASE (Section 2.8.1), but we added SynonymGraphFilter, FlattenGraphFilter and RemoveDuplicatesFilter to the documents' analyzer. By doing this we can store the documents with their words and the synonyms too, lightening up the search time avoiding query expansion technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.9.">BM25FRENCHQUERYEXPANSION system</head><p>This system is similar to the previous one but instead of expanding the documents we expand the queries. It exploits the same idea of having two analyzers, one for the documents and one for the queries. This time we added SynonymGraphFilter to the queries' analyzer because sometimes users do not formulate queries using the best terms, therefore using synonyms can improve the quality of search results. This is less cost-effective than document expansion because it makes the search heavier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head><p>Overall, our experimental setup was composed by:</p><p>• Used collections: In our experiments we both used the French and English corpora provided by the CLEF LongEval organizers, which include 1570734 documents and 672 queries for each language. However, the French collection was favoured because of the translation errors the English corpus contains (see section 4 for a closer look). • Evaluation measures: The general metric we used for evaluating the various systems was the nDCG. However, more specific recall and precision metrics had also to be taken into account to guide the improvement efforts. The tool used to evaluate the performance once the run files were created was Trec_Eval (version 9.0.7). The ground-truth considered for the evaluation was the one provided directly by CLEF organizer along with the collection. • Url to git repository: https://bitbucket.org/upd-dei-stud-prj/seupd2223-dards/src/master/ • Organization of git repository: The organization of the git repository is accurately given in the README.md file. However, the main directories are:</p><p>code: which contains some sub-directories each corresponding to one of the systems and some additional run files. -runs: which contains the runs submitted to CLEF for the evaluation (properly organized in zipped directories having a name that corresponds to the related system).</p><p>• Hardware used for experiments: All the indexing, searching and eventual pre-processing work has mostly been carried out on different commercial, mid-end machines. Overall, extensive testing (especially employing advanced NLP and POS-tagging techniques) has been limited by the low computing power at our disposal that made certain processes unsustainably long to complete. We report some of the used hardware:</p><p>• Scripts and tools employed: A tool was developed to help in the error analysis process. tellme is a small C program that automatically analyzes the run file, comparing it with the qrels and producing a list of all the retrieval errors. Thresholds for both precision and recall errors can be set in order to show only the most prominent errors while a verbose mode will also print to screen the content of the related query and document, aiding error inspection. You can find the tool's C source here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section we first discuss the performance on training data and how these were used to select the system to submit to the LongEval lab. After that we analyze the performance on test data (with some discussions) and report the hypotesis testing and failure analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance on Training Data and Discussion</head><p>As aforementioned, it can be noted how work on the French corpus has been largely favoured.</p><p>Due to the fact that the English document and query set were obtained by translation of the French ones, the former batch contains a lot of translation-induced noise which strongly interferes with the goal of setting up an effective IR system. This was already seen from Table <ref type="table" coords="18,500.35,504.67,3.66,10.91" target="#tab_5">2</ref>. Another related issue is document and queries not being translated homogeneously. As an example, in Table <ref type="table" coords="18,170.84,531.77,5.14,10.91">6</ref> we report a couple consisting of a query and a highly relevant document and their translations. Such a translation, while semantically correct, makes it harder for the IR system to correctly match the document to the query.</p><p>Moreover, by comparing our base French and base English systems we observed consistently worse performance on the latter (see Table <ref type="table" coords="18,280.15,585.96,3.54,10.91">7</ref>), while employing for the two systems essentially the same techniques (adapted to the respective languages).</p><p>Overall, the best system for all metrics is the BM25FRENCHBOOSTURL. This particularly good performance can be due to the URL tokenizing system helping with queries expressing ad hoc search and known item search. In this scope, keywords contained inside the URL are good indicators of page content: most of the time, while looking for a particular service or a website, the best match contains its name in the page URL, since those are more likely to be official websites and sources. Furthermore, considering the fact that some documents were rich of repetitions and useless symbols, considering the URLs in the search phase allowed us to distinguish more between real relevant documents and outliers. Another thing to observe is the BM25TRANSLATEDQUERIES performance in comparison to BaseSystem. When compared to base French systems, the results highlight how impactful query translation is, and the different results between the two denote how using a different translator may substantially influence output. This difference in results confirms how translation adds significant noise that interferes with the system IR-wise improvement process.</p><p>The BM25FRENCHRERANK100 system instead doesn't work as expected. In particular we were expecting to increase the nDCG by reranking the first 100 documents but this metric actually decreases. This can be due to the fact that by recreating the index based only on the top-100 documents the terms statistic changes and the new statistics favors the documents that are not considered relevant in the ground-truth.</p><p>To implement the reranking we thought about using some machine learning and/or deep learning techniques (Learn To Rank, LTR) but we were worried about the fact that since we had only a small number of relevance feedback for the training topics, then the resulting system would overfit on the training data.</p><p>We chose the systems to be submitted to CLEF based on the previous consideration and by evaluating also the trade-off between performance and novelty we brought (in the last column of Table <ref type="table" coords="19,127.72,643.79,5.07,10.91">7</ref> we marked which systems were submitted).</p><p>Figure <ref type="figure" coords="19,130.95,657.34,5.02,10.91" target="#fig_3">4</ref> shows the interpolated precision-recall curve for the two best performing systems.  The results in Table <ref type="table" coords="20,193.24,296.10,5.17,10.91" target="#tab_13">8</ref> highlight how the time-wise evolution of corpora (and queries) did not affect the system's effectiveness in any relevant way. This can be traced back to two main reasons:</p><p>• The systems rely on language-specific components only for a minimal part. Moreover, such components are not largely affected by language change over time: unless way longer timespans are observed, stopwords are unlikely to change and filters working on syntax and morphology are expected to keep their behavior. • At design time, during error analysis, changes were implemented targeting frequent problems that affected documents of various nature and content. Problems affecting specific categories of documents with specific content were not considered of primary interest as a means to avoid overfitting when given potentially different content to index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hypothesis Testing &amp; Failure Analysis</head><p>To compare the submitted systems for each of the three case studies (withing time, short term and long term) we first plotted the box plots to have a visual representation of the systems, their mean and their variance. After that, we computed the two-way ANalysis Of VAriance (ANOVA) considering as hypothesis the null hypothesis 𝐻 0 : 𝜇 𝑥 = 𝜇 𝑦 (where 𝑥 and 𝑦 represent two generic systems to be compared). The reason why we considered two-way ANOVA and not one-way ANOVA is that two-way ANOVA considers both the topics and the systems effect while the one-way ANOVA considers only the systems effect. The threshold that we used to consider the systems different was 𝛼 = 0.05 (5%). The results of the various ANOVA tests have been reported in Table <ref type="table" coords="20,190.43,600.04,3.66,10.91" target="#tab_15">9</ref>, Table <ref type="table" coords="20,226.72,600.04,9.94,10.91" target="#tab_2">10</ref> and Table <ref type="table" coords="20,284.40,600.04,8.20,10.91" target="#tab_2">11</ref>, where with SS we mean "Sum of Squares", with df we mean "degrees of freedom", with MS we mean "Mean Squares", with F we indicate the F-Test value and with prob&gt;F we indicate the P-Value. Eventually, we computed the Tukey HSD Test to understand which of the systems were actually different (in Figure <ref type="figure" coords="20,213.50,654.23,3.73,10.91" target="#fig_5">6</ref>, Figure <ref type="figure" coords="20,254.67,654.23,5.06,10.91">8</ref> and Figure <ref type="figure" coords="20,312.54,654.23,10.12,10.91" target="#fig_0">10</ref> the system highlighted in blue is the one considered for the comparison, the systems in red are the systems that are considered different,  while the systems in grey are the systems that are considered similar). In all cases, the metric selected for the comparison has been the nDCG.</p><formula xml:id="formula_5" coords="21,137.78,266.66,274.09,68.51">B M 2 5 F R E N C H B A S E B M 2 5 F R E N C H B O O S T U R L B M 2 5 F R E N C H R E R A N K 1 0 0 B M 2 5 F R E N C H S P A M B M 2 5 T R A N S L A T E D Q U E R I E S<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Within Time</head><p>From the Box plots reported in Figure <ref type="figure" coords="21,260.02,565.27,5.07,10.91" target="#fig_4">5</ref> we can see that the systems, in this case, appear to be quite similar. This is not confirmed by the output of ANOVA reported in Table <ref type="table" coords="21,425.66,578.82,4.97,10.91" target="#tab_15">9</ref> since the P-Value result to be lower than the considered threshold. Nonetheless, the results can be considered reliable since the system contribution is much higher than the error contribution. The ANOVA results highlight significant differences between the systems we have analyzed. This difference may be mainly due to the presence of an English-based system (BM25TRANSLATEDQUERIES) along French-based ones. Moreover, by looking at the Tukey HSD results (reported in Figure <ref type="figure" coords="21,402.74,660.12,4.21,10.91" target="#fig_5">6</ref>) the only system that      appears different from the others is the BM25TRANSLATEDQUERIES system. This could be due to the fact that that system is the only one using the English corpus and the performance results to be quite different from the others.</p><formula xml:id="formula_6" coords="22,89.12,123.03,163.08,147.20">'B M 2 5 T R A N S L A T E D Q U E R IE S ' 'B M 2 5 F R E N C H S P A M ' 'B M 2 5 F R E N C H R E R A N K 1 0 0 ' 'B M 2 5 F R E N C H B A S E ' 'B M 2 5 F R E N C H B O O S T U R L ' (a) BM25FRENCHBOOSTURL</formula><formula xml:id="formula_7" coords="22,92.04,123.03,360.21,319.27">'B M 2 5 T R A N S L A T E D Q U E R IE S ' 'B M 2 5 F R E N C H S P A M ' 'B M 2 5 F R E N C H R E R A N K 1 0 0 ' 'B M 2 5 F R E N C H B A S E ' 'B M 2 5 F R E N C H B O O S T U R L ' (b) BM25FRENCHBASE 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 nDCG 'B M 2 5 T R A N S L A T E D Q U E R IE S ' 'B M 2 5 F R E N C H S P A M ' 'B M 2 5 F R E N C H R E R A N K 1 0 0 ' 'B M 2 5 F R E N C H B A S E ' 'B M 2 5 F R E N C H B O O S T U R L ' (c) BM25FRENCHRERANK100</formula><formula xml:id="formula_8" coords="22,298.71,295.10,151.41,147.20">'B M 2 5 T R A N S L A T E D Q U E R IE S ' 'B M 2 5 F R E N C H S P A M ' 'B M 2 5 F R E N C H R E R A N K 1 0 0 ' 'B M 2 5 F R E N C H B A S E ' 'B M 2 5 F R E N C H B O O S T U R L ' (d) BM25FRENCHSPAM</formula><formula xml:id="formula_9" coords="22,195.38,467.17,169.64,147.20">'B M 2 5 T R A N S L A T E D Q U E R IE S ' 'B M 2 5 F R E N C H S P A M ' 'B M 2 5 F R E N C H R E R A N K 1 0 0 ' 'B M 2 5 F R E N C H B A S E ' 'B M 2 5 F R E N C H B O O S T U R L ' (e) BM25TRANSLATEDQUERIES</formula><p>We can conclude that the systems are quite similar but the BM25TRANSLATEDQUERIES system turns out to be the only one showing relevant differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Short Term</head><p>By looking at the box plot reported in Figure <ref type="figure" coords="23,293.06,501.51,5.12,10.91" target="#fig_6">7</ref> we see that the systems have pretty much the same performance, only BM25TRANSLATEDQUERIES differs a bit from the others. Furthermore we can see that the extension of the whiskers in almost all the systems goes from 0 to 1, this is due to the fact that those systems performs very well for some queries (and reach the maximum value of nDCG) but very poorly for some other queries. A deeper analysis of the results revealed that the queries performing well were very specific while the ones performing badly were either or generic queries or containing terms that can be considered spam (see Section 2.2.1 for more details).</p><p>The two-way ANOVA (whose results are reported in Table <ref type="table" coords="23,369.63,609.90,9.11,10.91" target="#tab_2">10</ref>) does reveal relevant differences among the systems since the P-Value result to be lower than the considered threshold. Furthermore, the results of ANOVA can be considered reliable since the system contribution is much higher than the error contribution.</p><p>By looking at the Tukey HSD results (reported in Figure <ref type="figure" coords="23,339.79,664.10,4.08,10.91">8</ref>) we noticed that there is a clear differ- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>We have been able to reach good results in terms of nDCG and in particular of recall, in particular with the system BM25FRENCHBOOSTURL. We have developed a variety of system that use multiple different techniques to retrieve the documents always by paying attention to not create systems that are too overfitted to our training data (especially because the goal of the CLEF LongEval task was to evaluate the stability of the systems over time).</p><p>The English translations of the original French documents and queries were done by machine translators and their quality was not particularly good. Our translation tool demonstrated that machine translation could have a not negligible impact on IR.</p><p>For what concerns spam detection, after these experiments we can assert that the models we have experimented with are indeed too simple to adequately distinguish between spam and non-spam documents. For instance, a more complex algorithm that takes into account document length, word frequency distribution, word variance, sentence shape and language simultaneously may be able to target spam way more effectively. Moreover, with such an implementation, machine learning techniques may help in finding the optimal thresholds and weights for each document feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Future work</head><p>To improve our systems we will work on these aspects:</p><p>• Document translation: try to improve the documents translations by improving our translation tool (and turning it in something that allow us to execute the translation of the documents BODY field in a computationally feasible time). • System combination: develop further the systems that use the English language, trying to increase the performance. After that combine the English and French systems results trying to do a multilingual search. • Learn To Rank (LTR): try to implement some LTR (and feature extraction) technique considering more relevance feedback to avoid overfitting. • Machine learning for spam detection: look for correlation between various document features to infer with limited uncertainty whether a document contains spam.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,451.19,139.30,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General system scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,281.64,231.79,8.93;5,172.63,84.19,250.01,184.88"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Flow of the documents parser. (docs_filter.py).</figDesc><graphic coords="5,172.63,84.19,250.01,184.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,332.14,267.83,8.93;6,172.64,84.19,250.01,235.38"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Flow of the synonyms searcher. (search_synonyms.py).</figDesc><graphic coords="6,172.64,84.19,250.01,235.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="19,89.29,348.75,182.64,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interpolated precision-recall curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="21,89.29,348.75,135.29,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Box Plots Within Time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="22,89.29,622.89,416.69,8.93;22,89.29,634.90,416.70,8.87;22,89.29,646.85,219.75,8.87"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Tukey HSD Test Within Time. The blue line is the system we are taking into account to do the test. The red lines are the systems significantly different to the blue system. The grey lines are the systems not significantly different to the blue system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="23,89.29,360.38,129.14,8.93;23,99.08,84.19,397.13,263.63"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Box Plots Short Term</figDesc><graphic coords="23,99.08,84.19,397.13,263.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="27,89.29,505.81,127.18,8.93"><head>B M 2 5 FFigure 10 :</head><label>510</label><figDesc>Figure 9: Box Plots Long Term</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,118.38,272.92,359.46,75.67"><head></head><label></label><figDesc>Tableau de conversion pouce / inch en Cm mm | Tableau de conversion, Tableau</figDesc><table /><note coords="4,186.48,286.47,221.80,9.72;4,122.62,311.77,350.05,9.72;4,118.38,325.32,358.51,9.72;4,238.08,338.86,118.75,9.72"><p>de conversion de mesure, Scrapbooking à imprimer" "[...] Quiz : Cinéma Quiz : Histoire de France Quiz : Géographie Quiz : Histoire Quiz : Littérature Quiz : Espace Quiz : Bien-être Quiz : Littérature Quiz : Gamer Quiz : Formule 1 Quiz"[...]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.99,90.49,315.12,171.25"><head>Table 1</head><label>1</label><figDesc>Comparison of rerank performance.</figDesc><table coords="9,191.17,118.53,212.94,143.21"><row><cell>Similarity</cell><cell>MAP</cell></row><row><cell>BM25Similarity</cell><cell>0.1424</cell></row><row><cell>LMJelinekMercerSimilarity(0.1F)</cell><cell>0.1249</cell></row><row><cell>LMJelinekMercerSimilarity(0.5F)</cell><cell>0.1255</cell></row><row><cell>LMJelinekMercerSimilarity(0.8F)</cell><cell>0.1242</cell></row><row><cell cols="2">DFRSimilarity (most of the configurations) 0.1423</cell></row><row><cell>LMDirichletSimilarity</cell><cell>0.1204</cell></row><row><cell>IndriDirichletSimilarity</cell><cell>0.0137</cell></row><row><cell>ClassicSimilarity</cell><cell>0.0740</cell></row><row><cell>BooleanSimilarity</cell><cell>0.0121</cell></row><row><cell>AxiomaticF2LOG</cell><cell>0.1358</cell></row><row><cell>AxiomaticF2EXP</cell><cell>0.1346</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,90.49,414.43,141.60"><head>Table 2</head><label>2</label><figDesc>Query translation error examples Query ID French English q062213307 cuisson gigot agneau leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg leg</figDesc><table coords="12,95.67,211.27,242.15,20.82"><row><cell></cell><cell></cell><cell>leg leg leg leg leg leg</cell></row><row><cell>q062228</cell><cell>aeroport bordeaux</cell><cell>airport</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,89.02,441.00,435.89,160.17"><head></head><label></label><figDesc>By doing several experiments we noticed that with English the best performing stemmer was the Krovetz stemmer (KStemFilter).The StopFilter does not use a customized stoplist but it uses the standard English stoplist provided by the Lucene class EnglishAnalyzer through the static variable ENGLISH_STOP_WORDS_SET.</figDesc><table coords="12,95.27,441.00,258.81,92.55"><row><cell>Query language</cell><cell>French, translated in English</cell></row><row><cell cols="2">Document language English</cell></row><row><cell>Preprocessing</cell><cell>None</cell></row><row><cell>Similarity</cell><cell>BM25Similarity</cell></row><row><cell>Analyzer</cell><cell>Custom, composed as below</cell></row><row><cell>Tokenizer</cell><cell>StandardTokenizer</cell></row><row><cell>Filters</cell><cell>LowerCaseFilter, StopFilter, KStemFilter</cell></row><row><cell>Searcher</cell><cell>Standard searcher + query translation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,88.99,586.00,277.06,87.57"><head>Table 3</head><label>3</label><figDesc>Comparison of rerank performance.</figDesc><table coords="13,229.22,614.09,136.83,59.48"><row><cell></cell><cell cols="2">rerank100 rerank1000</cell></row><row><cell>MAP</cell><cell>0.1960</cell><cell>0.1702</cell></row><row><cell>nDCG</cell><cell>0.3657</cell><cell>0.3379</cell></row><row><cell>recall</cell><cell>0.8437</cell><cell>0.8437</cell></row><row><cell>p@5</cell><cell>0.1533</cell><cell>0.1336</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="20,88.99,90.49,442.82,155.39"><head>Table 8</head><label>8</label><figDesc>Test-time systems performance overview (nDCG)</figDesc><table coords="20,89.29,122.10,442.52,123.77"><row><cell>System name</cell><cell cols="6">Within Time nDCG Recall@1000 nDCG Recall@1000 nDCG Recall@1000 Short Term Long Term</cell></row><row><cell cols="3">BM25FRENCHBOOSTURL 0.3859 0.8708</cell><cell cols="2">0.3866 0.8323</cell><cell cols="2">0.3945 0.8556</cell></row><row><cell>BM25FRENCHBASE</cell><cell>0.3843</cell><cell>0.8684</cell><cell>0.3924</cell><cell>0.8375</cell><cell>0.3916</cell><cell>0.8531</cell></row><row><cell>BM25FRENCHRERANK100</cell><cell>0.3755</cell><cell>0.8644</cell><cell>0.3756</cell><cell>0.8365</cell><cell>0.3758</cell><cell>0.8531</cell></row><row><cell>BM25FRENCHSPAM</cell><cell>0.3605</cell><cell>0.7874</cell><cell>0.368</cell><cell>0.7686</cell><cell>0.3643</cell><cell>0.7773</cell></row><row><cell cols="2">BM25TRANSLATEDQUERIES 0.3072</cell><cell>0.7317</cell><cell>0.3051</cell><cell>0.7225</cell><cell>0.3189</cell><cell>0.7482</cell></row><row><cell cols="2">4.2. Performance on Test Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="20,88.96,255.46,417.02,38.01"><head></head><label></label><figDesc>Table 8 reports the results computed for our CLEF-submitted systems over the LongEval test datasets. Each metric group represents a different indexing scenario, whether on same-time (of training data), short term evolution or long term evolution.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="21,88.99,381.91,349.42,93.78"><head>Table 9</head><label>9</label><figDesc>Two-Way ANOVA Within Time (based on nDCG)</figDesc><table coords="21,156.86,413.52,281.55,62.16"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>prob&gt;F</cell></row><row><cell cols="2">Columns (systems) 0.4170</cell><cell>4</cell><cell cols="2">0.1042 8.2753</cell><cell>2.0570e-06</cell></row><row><cell>Rows (topics)</cell><cell cols="2">21.6586 97</cell><cell cols="3">0.2233 17.7255 1.1527e-97</cell></row><row><cell>Error</cell><cell>4.8876</cell><cell cols="2">388 0.0126</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">26.9631 489</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,89.79,660.06,420.46,8.97;6,89.50,671.02,161.55,8.97"><p>https://bitbucket.org/upd-dei-stud-prj/seupd2223-dards/src/master/code/BM25TRANSLATEDQUERIES/src/main/java /it/unipd/dei/dards/utils/GoogleScriptApi.gs</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ence between BM25TRANSLATEDQUERIES and the other systems, this is due to the fact that this system is based on the English collection. Taken the best system's test (BM25FRENCHBASE's), we see that the BM25FRENCHBASE system is significantly different from all systems other than the BM25FRENCHBOOSTURL system, this happens because they are basically the same except that the BM25FRENCHBOOSTURL system uses (in the indexing and searching phases) an additional field: the URL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Long Term</head><p>From the Box plots reported in Figure <ref type="figure" coords="24,260.57,444.09,5.09,10.91">9</ref> we can see that the systems, even in this case, appear to be quite similar. Furthermore we can see that the extension of the whiskers in almost all the systems goes from 0 to 1, this is due to the fact that those systems performs very well for some queries (and reach the maximum value of nDCG) but very poorly for some other queries.</p><p>A deeper analysis of the results revealed that the queries performing well were very specific while the ones performing badly were either or generic queries or containing terms that can be considered spam (see Section 2.2.1 for more details). The two-way ANOVA (whose results are reported in Table <ref type="table" coords="24,369.63,538.93,9.11,10.91">11</ref>) does reveal relevant differences among the systems since the P-Value result to be lower than the considered threshold. Furthermore, the results of ANOVA can be considered reliable since the system contribution is much higher than the error contribution.</p><p>By looking at the Tukey HSD results (reported in Figure <ref type="figure" coords="24,359.48,593.13,9.11,10.91">10</ref>) we noticed that the two best performing systems (BM25FRENCHBOOSTURL and BM25FRENCHBASE) happen to be quite similar between each other but different from the other systems; this happens because the BM25FRENCHBOOSTURL system is actually the same as the BM25FRENCHBASE system except that it uses one additional field: the URL. Furthermore, also the BM25FRENCHRERANK100 system and the BM25FRENCHSPAM turn out to be similar while the BM25TRANSLATEDQUERIES system still looks different and (as in the previous case studies) this is due to the fact that this last system is based on the English collection. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="29,107.59,557.84,399.60,10.91;29,107.59,571.39,400.24,10.91;29,107.59,584.94,399.60,10.91;29,107.59,598.49,399.69,10.91;29,107.59,612.04,398.39,10.91;29,107.59,625.59,398.40,10.91;29,107.59,639.14,295.68,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="29,107.59,598.49,380.14,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="29,107.59,612.04,398.39,10.91;29,107.59,625.59,304.82,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="29,443.89,625.59,62.09,10.91;29,107.59,639.14,120.03,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,107.59,666.24,398.40,10.91;30,107.59,86.97,296.77,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cowley</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470689646.ch1</idno>
		<title level="m" coord="29,292.63,666.24,213.36,10.91;30,107.59,86.97,47.87,10.91">Automatic Keyword Extraction from Individual Documents</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,107.59,100.52,398.40,10.91;30,107.59,114.06,398.40,10.91;30,106.71,127.61,400.55,10.91;30,107.59,141.16,337.56,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="30,149.62,100.52,356.36,10.91;30,107.59,114.06,43.14,10.91">Light stemming approaches for the french, portuguese, german and hungarian languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<idno type="DOI">10.1145/1141277.1141523</idno>
		<ptr target="https://doi.org/10.1145/1141277.1141523.doi:10.1145/1141277.1141523" />
	</analytic>
	<monogr>
		<title level="m" coord="30,177.82,114.06,328.17,10.91;30,106.71,127.61,11.71,10.91">Proceedings of the 2006 ACM Symposium on Applied Computing, SAC &apos;06</title>
		<meeting>the 2006 ACM Symposium on Applied Computing, SAC &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1031" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
