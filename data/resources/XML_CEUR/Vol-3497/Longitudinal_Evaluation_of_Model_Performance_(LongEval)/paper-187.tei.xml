<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,411.10,15.42;1,89.29,106.66,175.55,15.42;1,89.29,129.00,220.08,11.96">SEUPD@CLEF: RAFJAM on Longitudinal Evaluation of Model Performance Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,154.90,84.55,11.96"><forename type="first">Alvise</forename><surname>Bolzonella</surname></persName>
							<email>alvise.bolzonella@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.06,154.90,81.79,11.96"><forename type="first">Riccardo</forename><surname>Broetto</surname></persName>
							<email>riccardo.broetto@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.49,154.90,81.37,11.96"><forename type="first">Marco</forename><surname>Gasparini</surname></persName>
							<email>marco.gasparini.10@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.51,154.90,63.20,11.96"><forename type="first">Farhad</forename><surname>Sadat</surname></persName>
							<email>farhad.sadat@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,411.10,15.42;1,89.29,106.66,175.55,15.42;1,89.29,129.00,220.08,11.96">SEUPD@CLEF: RAFJAM on Longitudinal Evaluation of Model Performance Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">831ECA441D3DBF5F7342CA9D4A02A43F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>Lucene</term>
					<term>query expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is intended to be a report of the work we have done for the CLEF 2023 LongEval Lab, whose main goal is to evaluate and improve performances of IR models along time. We have implemented a basic retrieval system and then modified and extended it, focusing on different query expansion techniques, involving the use of synonyms and pseudo-relevance feedback. We will provide a description of our ideas, code and other development details, along with statistical analysis of the runs of our systems on different test collections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The main reason behind LongEval @ CLEF task <ref type="bibr" coords="1,303.46,391.54,12.79,10.91">[1]</ref> about temporal persistence of information retrieval systems is that recent research shows that their performances get worse as test data becomes distant in terms of time from train data.</p><p>Therefore, participants should develop a model producing satisfying results on the different test sets provided by the organizers, which are collected in three different moments of summer 2022. Every collection consists of a set of documents coming from Qwant's index queries, and of a set of queries related to one or more topics; the choice of these topics is based on Web trends and it is diverse enough. Our group has decided to analyse collections in French, which is the original language of the data sets.</p><p>The paper is organized as follows: Section 3 describes our approach; Section 4 explains our experimental setup; Section 5 discusses our main findings; finally, Section 6 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Project setup and basic retrieval system</head><p>We have decided to implement our retrieval system starting from the basic structure of the projects contained in the Search Engines course repository 1 , used during our studies for learning the fundamentals about Information Retrieval. The basic code for the Indexer, Searcher and Analyzer of the project is taken from the hellotipster example. We have then improved our Analyzer focusing on the hello-analyzer example, in particular we have adopted the StopFilter class from there. We have also looked into French-Analyzer class of Lucene 2 , which has suggested us to add an ElisionFilter using its default list of French articles, and to stem words with FrenchLightStemFilter 3 . Finally, as similarity score, after trying different alternatives, we have decided to use Best Matching 25 (BM25) because it provides the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pseudo-relevance feedback</head><p>The general idea for this technique comes from the lectures of the Search Engines course and from some further readings <ref type="bibr" coords="2,211.72,317.47,11.28,10.91" target="#b5">[2]</ref>. The different term weighting schemas we have tried to use (see Section 3.2.1) and Zipf's law were presented during the lectures, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Synonym-substitution query expansion</head><p>The main idea for this method comes from many papers that discuss on the lexical query expansion <ref type="bibr" coords="2,137.39,394.29,11.43,10.91" target="#b6">[3]</ref>. In order to find the most relevant words to substitute with their synonyms, we asked to an artificial intelligence, chatGPT 4 , to find the most frequent words in the query list and to provide a list of three synonyms for all this words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The procedure to obtain the final runs results consists basically of two main steps, which are the indexing of the documents in the collection, involving parsing operations and using a customised analyser, and the proper searching phase, preceded by parsing and analysis of the queries. The following sections describe our proposed solutions and implementations; they are based on Apache Lucene 5 , which is a very popular API for IR. It consists of a lot of packages for various tasks, including analysis of texts in different languages, including French 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Processing and indexing the documents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Parsing</head><p>Before being stored into the index, documents have to be parsed to correctly obtain their fields. We have inspected the structure of the .txt files with the documents: they contain IDs and bodies, marked by different tags, which have suggested us to parse them using Java regular expressions, which easily allow to detect these tags. In our project, class LEParser takes care of processing original documents in this way, storing their content in a fresh instance of the ParsedDocument class. LEParser extends DocumentParser, which is an abstract class providing a general structure for parsers and which is able to iterate over ParsedDocument instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Analyzing</head><p>This step consists of tokenizing the content of the documents and modifying and filtering the obtained tokens. This procedure is handled in class LEAnalyzer, which has allowed us to analyse the content of the documents in a customized way. It extends the abstract class Analyzer by overriding initReader and createComponents methods. By trying many different combinations and computing related evaluation measures, we have decided the sequence of operations it is supposed to do:</p><p>• tokenization with StandardTokenizer, provided by Lucene; • getting all token letters to lowercase;</p><p>• deletion of all tokens representing stopwords and French articles (we have tried different lists and chosen one of roughly 500 terms, mostly from French but also from English language); • deletion of single letters words, using a length filter; • stemming, using Lucene FrenchLightStemFilter; we have observed that more aggressive stemming algorithms, even if simpler, provide worse performances; • deletion of numeric tokens, except for integers with more than 3 digits, which often represent years; this filtering step is managed by our NumberFilter.</p><p>Some little variations to this schema ("standard analyzing") for the different systems will be briefly described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Indexing</head><p>This is the phase where the documents are actually stored by Lucene IndexWriter. In class DirectoryIndexer, the tree of directories containing the files with the documents is scanned and each document is parsed as described before. Then, its fields are wrapped in a ParsedDocument instance and passed to the writer, which is configured in order to perform our customized analysis. DirectoryIndexer contains a main method, to make it runnable, in order to create the index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Searching</head><p>This is the part where we spent most of our time, implementing different strategies: the main idea was to improve the quality of the retrieval system modifying the queries.</p><p>We started by inspecting the files with the queries, then decided to create a simple parser for the .tsv one to get ID and content (namely the "title" field) of each query, storing them in a Lucene QualityQuery instance. When a new Searcher instance is created, this file reader is invoked and, in order to ensure that query contents are by default setting analyzed like the body of the documents, our customized LEAnalyzer is passed as a parameter to the constructor.</p><p>At searching time, boolean queries are created for each QualityQuery instance. Then, each boolean query is searched using BM25 similarity <ref type="bibr" coords="4,311.78,243.03,11.54,10.91">[4]</ref>. Resulting scores for top documents are stored, and a textual run file is produced. This is the standard procedure to search a set of queries in a document index; our corrections are basically Query Expansion (QE) techniques which act on the text of queries before building the BooleanQuery.</p><p>Our three main ideas for query expansions are detailed below, along with a description of their implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pseudo-relevance feedback</head><p>Pseudo-relevance feedback technique <ref type="bibr" coords="4,257.96,387.20,12.80,10.91">[5]</ref> is a heuristic way to expand a query with significant words in order to improve search results. The main assumption behind it is that top results coming from the original query can be assumed to be relevant, and most frequent words in them to be therefore significant for the topic.</p><p>We have tried different implementations for this technique. As a common programming strategy, they use Lucene term vectors to iterate through the terms of each document, in order to weight them according to their frequency. We have observed that taking into account only occurrences of words in the top documents, and not all the retrieved ones, leads to better performance.</p><p>The main variations were about the weighting schema: we have tried with raw total term frequencies, average relative frequencies in the top ranked documents, and with tfidf weighting.</p><p>We have then looked into evaluation measures for the results, in particular map, and seen that more complicated weighting systems resulted in similar or slightly lower scores. Therefore, Occam principle suggested us to keep the simplest system, which is considering absolute frequencies of terms across the best matching documents retrieved. Fixing a number of words to add to the original query seemed too arbitrary, so we thought to take into account frequency drops of words. By drop we mean a considerable delta from the predicted frequency of a word according to Zipf's law <ref type="bibr" coords="4,196.07,631.09,11.48,10.91">[6]</ref>, which is an empirical law stating that the number of occurrences of terms and their rank when sorting them by decreasing frequency are roughly inversely proportional.</p><p>According to this idea, in our implementation words are added to the original query starting from the most frequent one until a fixed maximum number is reached, or a significant frequency drop is met. This drop happens at k-th-ranked word iff:</p><formula xml:id="formula_0" coords="5,250.26,139.30,93.58,10.77">𝑘 • 𝑡𝑓 𝑘 &lt; 𝑡𝑓 1 • 𝑘𝑍𝑖𝑝𝑓</formula><p>We have also tried some tuning on these two parameters (kZipf and the max number of words to be added), but unfortunately we observed that not expanding the query at all provided the best scores. Some additional considerations are left to sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Query expansion with synonym substitution</head><p>Synonym substitution query expansion <ref type="bibr" coords="5,270.88,239.51,13.00,10.91" target="#b10">[7]</ref> is a technique used to enhance the accuracy and relevance of search results. It involves expanding the user's original search query by replacing some of the query terms with their synonyms or related terms. This helps to capture a wider range of results that may not have been included in the original query and can improve the precision of the search results.</p><p>In our implementation of this technique we have used, as we explained in section 2.3, an artificial intelligence to create a list of three synonyms for the most frequent words in the query list. Then we create, starting from the original queries, three expanded queries obtained by replacing the words founded in the synonym list with their first, second and third synonym.</p><p>Afterwards we compute the average score for all the ScoreDoc arrays generated by the four queries and we return to the search method the ScoreDoc array of the query that maximize this value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Pseudo-relevance feedback on synonym-expanded queries</head><p>We have also tried to merge the two previously described strategies.</p><p>Pseudo-relevance query expansion is applied to the original query and to the three queries obtaining by synonyms replacement, and the results which are kept are the ones coming from the best-performing one, computing scores as described for standard query expansion with synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Summary of the runs produced by the systems</head><p>• Basic: standard analyzing, without NumberFilter;</p><p>• SynQE: standard analyzing and query expansion with synonyms;</p><p>• PseudoRelQE: standard analyzing and query expansion with pseudo-relevance feedback;</p><p>• AllQE: standard analyzing without NUmberFilter and query expansion with synonyms combined with pseudo-relevance feedback;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Documents</head><p>We used three French collections provided by Conference and Labs of the Evaluation Forum (CLEF) for The LongEval challenge:</p><p>Train collection -Consisted of almost 9 GBs of files, for a total of 1570734 documents. Short term collection -Consisted of almost of 8 GBs of files, for a total of 1593376 documents.</p><p>Long term collection -Consisted of almost of 6 GBs of files, for a total of 1081334 documents.</p><p>Train collection contains the documents, the queries and the qrels we used to train the system on, and also some fresh queries to perform in-time testing on the same documents. The others are test collections, containing only documents and queries.</p><p>The collections can be found here, along with a brief description: https://lindat.mff.cuni.cz/repository/xmlui/longeval-train-v2 <ref type="bibr" coords="6,360.12,294.84,12.84,10.91" target="#b11">[8]</ref> https://lindat.mff.cuni.cz/repository/xmlui/longeval-test-collection [9]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Topics</head><p>The topics were also provided by CLEF for the LongEval challenge and were contained in a .tsv file that is stored in the /queries folder of the collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relevance Judgements and Measures</head><p>In order to test the performances of the different analyzers, we used trec_eval to compare our results obtained on the train documents with the qrels given by CLEF. We focused our attention mainly on the Mean Average Precision (MAP) since it is described as the most relevant parameter to evaluate a run using a single number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Tools</head><p>We developed and tested our Systems with the following experimental setups:</p><p>• Java Version: Open JDK 19.0.2;</p><p>• IDE &amp; tools: Intellij IDEA, Apache Lucene, Apache Maven;</p><p>• Computer used:</p><p>-CPU: 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz -RAM: 16,0 GB LPDDR4x -SSD: 512 GB SSD NVMe PCle</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Bitbucket Repository</head><p>Link: https://bitbucket.org/upd-dei-stud-prj/seupd2223-rafjam/src/master/. We provided four solutions in the "runs" folder, which are the ones described in detail in Section 3 and in particular in 3.2.4. This section is intended to provide some numerical results and discussion about the experiments we have conducted on training data while developing our retrieval systems. One possible comparison offered by Table <ref type="table" coords="7,261.11,297.41,5.17,10.91" target="#tab_0">1</ref> is the one between synonyms and pseudo-relevance query expansions. While the SynQE System provides a slightly better result in terms of average precision, the PseudoRelQE results in a higher recall. This seems reasonable, as pseudo-relevance starts from a set of documents which have been well-ranked by a previous run, which makes them probably relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Training Data</head><p>As Figure <ref type="figure" coords="7,146.55,378.71,5.17,10.91" target="#fig_0">1</ref> displays, the Basic System offers overall better performances compared to the other systems. The PseudoRelQE and SynQE we developed offer similar precision, especially at low recall value. We also tried to combine the two, in the AllQE System, but that resulted in a slight drop in the performances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on Test Data</head><p>In this section we provide the results obtained by running our algorithms on each of the three available test collections (heldout, short term, long term). The first one consists of fresh queries to be paired with the same documents of the training collection, while the other ones are made of different topics and documents. Tables 2, 3 and 4 sum up the performances of the four systems. By inspecting them we can observe again that Basic system offers superior performances overall in terms of Normalized Discounted Cumulated Gain (nDCG) and that, as expected considering QE techniques, PseudoRelQE performs better than the others in terms of recall, as it starts from a set of documents which have already received an high rank on a previous run, making them probably relevant. The measure that appears to better succeed in differentiating the systems is nDCG, while for example MAP does not show clear patterns.  The boxplots in Figure <ref type="figure" coords="9,203.07,472.65,5.05,10.91" target="#fig_2">2</ref> confirm that the Basic system offers superior performances overall in terms of nDCG, while we need some more refined tools to make statistically significant observations about the other 3 systems. They also show that the heldout collection presents a bigger variation in results and that they are unbalanced, with a bigger concentration at low performance values. Comparing these with the results obtained on the other two collections, we can assume that the quality of queries and relevance judgements in the heldout collection is worse.</p><p>Our following statistical analysis is based on nDCG scores, computed on our runs using the relevance judgements released by CLEF for each test collection.  In order to better study the difference between runs with different systems and on different queries, we have conducted a two-way ANalysis Of VAriance (ANOVA) test on nDCG performances of our runs, considering a significance of 0.05. We want therefore to reject two null hypotheses, for each collection:</p><p>• the one stating that each system is expected to provide the same mean nDCG:</p><formula xml:id="formula_1" coords="10,199.27,513.81,223.07,10.77">𝐻 0 : 𝜇 𝐵𝑎𝑠𝑖𝑐 = 𝜇 𝑃 𝑠𝑒𝑢𝑑𝑜𝑅𝑒𝑙𝑄𝐸 = 𝜇 𝑆𝑦𝑛𝑄𝐸 = 𝜇 𝐴𝑙𝑙𝑄𝐸</formula><p>• the one stating that each topic is expected to get the same mean nDCG across the systems:</p><formula xml:id="formula_2" coords="10,246.62,554.04,129.31,10.63">𝐻 0 : 𝜇 𝑖 = 𝑐𝑜𝑛𝑠𝑡, ∀𝑖 ∈ 𝑡𝑜𝑝𝑖𝑐𝑠</formula><p>The three tables in Figure <ref type="figure" coords="10,206.01,574.76,5.07,10.91" target="#fig_4">3</ref> represents the results of the analysis. The F statistic is computed as:</p><formula xml:id="formula_3" coords="10,256.39,607.45,33.56,10.63">𝐹 𝑠𝑡𝑎𝑡 =</formula><p>𝑀 𝑆 𝑠𝑜𝑢𝑟𝑐𝑒 𝑀 𝑆 𝑒𝑟𝑟𝑜𝑟 where 𝑠𝑜𝑢𝑟𝑐𝑒 can be either Systems or Topics. As the p-values are all much lower than 5%, both null hypotheses are rejected for every collection. This means that the performances of the four systems are significantly different from a statistical point of view, and that there are also consistent differences between performances on different topics.  Finally, we have plotted the graphs representing the results of Tukey's Honestly Significant Difference (HSD) multiple comparison tests on each collection. They allow to visualize pairwise comparisons between systems, and therefore to make more precise observations. One important point is that we can confirm that the Basic system has better performances than the others under nDCG: the test shows that it's significantly comparable only with PseudoRelQE in the heldout collection, but the fact that it has few queries makes confidence intervals considerably larger than the ones in the other test collections, and so more easily overlapping. Another possible consideration is that PseudoRelQE seems to be always the second best option, although the test shows statistical evidence only in the long term collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>The first consideration to do is about both query expansion methods we have implemented: we have unfortunately observed that this heuristic operations don't lead to an improvement in terms of precision. Therefore, we haven't performed an organic refinement of parameters (in particular the maximum number of words and kZipf) because the best option was to not add words.</p><p>One more satisfying observation is about pseudo-relevance query expansion: counting word occurrences only in few top-ranked documents produces better results. That's what we expected, because documents ranked after the first ones in the original searching results, which are probably the most relevant, are less likely to contain words which are significant for the topic. In our implementation, using only the top-ranked document leads to the best performance in terms of mean average precision.</p><p>One potential improvement could be the parameter tuning which was mentioned before, and could be performed after finding a more effective term weighting model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,89.29,654.43,253.97,8.93;7,183.05,468.22,229.16,173.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Interpolated Precision at Recall of the four systems.</figDesc><graphic coords="7,183.05,468.22,229.16,173.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,149.15,244.19,90.00,9.96;9,350.09,244.19,101.48,9.96;9,247.92,419.38,99.14,9.96"><head></head><label></label><figDesc>(a) Heldout collection. (b) Short term collection. (c) Long term collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,443.00,210.07,8.93;9,193.47,256.25,208.34,156.26"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Boxplots for nDCG, for the four systems.</figDesc><graphic coords="9,193.47,256.25,208.34,156.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,252.49,173.66,90.00,9.96;10,246.75,275.18,101.48,9.96;10,247.92,376.70,99.14,9.96"><head></head><label></label><figDesc>(a) Heldout collection. (b) Short term collection. (c) Long term collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,400.33,146.74,8.93;10,89.29,287.24,416.69,82.59"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ANOVA tables for nDCG.</figDesc><graphic coords="10,89.29,287.24,416.69,82.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,252.49,278.57,90.00,9.96;11,246.75,485.00,101.48,9.96;11,247.92,691.44,99.14,9.96"><head></head><label></label><figDesc>(a) Heldout collection. (b) Short term collection. (c) Long term collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,89.29,715.06,355.35,8.93;11,172.63,497.06,250.01,187.50"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multiple comparisons of the nDCG scores of the 4 systems with Tukey HSD.</figDesc><graphic coords="11,172.63,497.06,250.01,187.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,146.99,349.24,105.74"><head>Table 1</head><label>1</label><figDesc>Scores of different systems on training collection.</figDesc><table coords="7,154.55,178.56,283.68,74.17"><row><cell cols="5">Evaluation Measure Basic SynQE PseudoRelQE AllQE</cell></row><row><cell>MAP</cell><cell>0.2026</cell><cell>0.1667</cell><cell>0.1621</cell><cell>0.1548</cell></row><row><cell>nDCG</cell><cell>0.3636</cell><cell>0.3103</cell><cell>0.3111</cell><cell>0.2893</cell></row><row><cell>nDCG@5</cell><cell>0.1893</cell><cell>0.1564</cell><cell>0.1639</cell><cell>0.1430</cell></row><row><cell>P@10</cell><cell>0.1275</cell><cell>0.1051</cell><cell>0.0967</cell><cell>0.0990</cell></row><row><cell>Recall</cell><cell>0.8332</cell><cell>0.7319</cell><cell>0.7776</cell><cell>0.7003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,176.05,349.24,105.74"><head>Table 2</head><label>2</label><figDesc>Scores of the four systems on heldout test collection.</figDesc><table coords="8,154.55,207.62,283.68,74.17"><row><cell cols="5">Evaluation Measure Basic SynQE PseudoRelQE AllQE</cell></row><row><cell>MAP</cell><cell>0.2018</cell><cell>0.1614</cell><cell>0.1971</cell><cell>0.1652</cell></row><row><cell>nDCG</cell><cell>0.3740</cell><cell>0.3193</cell><cell>0.3516</cell><cell>0.3209</cell></row><row><cell>nDCG@5</cell><cell>0.1811</cell><cell>0.1461</cell><cell>0.1842</cell><cell>0.1437</cell></row><row><cell>P@10</cell><cell>0.1337</cell><cell>0.1082</cell><cell>0.1194</cell><cell>0.1071</cell></row><row><cell>Recall</cell><cell>0.8473</cell><cell>0.7658</cell><cell>0.7713</cell><cell>0.7506</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,313.72,349.24,105.74"><head>Table 3</head><label>3</label><figDesc>Scores of the four systems on short term test collection.</figDesc><table coords="8,154.55,345.29,283.68,74.17"><row><cell cols="5">Evaluation Measure Basic SynQE PseudoRelQE AllQE</cell></row><row><cell>MAP</cell><cell>0.2207</cell><cell>0.1876</cell><cell>0.1843</cell><cell>0.1785</cell></row><row><cell>nDCG</cell><cell>0.3804</cell><cell>0.3295</cell><cell>0.3355</cell><cell>0.3172</cell></row><row><cell>nDCG@5</cell><cell>0.2089</cell><cell>0.1774</cell><cell>0.1732</cell><cell>0.1685</cell></row><row><cell>P@10</cell><cell>0.1342</cell><cell>0.1151</cell><cell>0.1166</cell><cell>0.1101</cell></row><row><cell>Recall</cell><cell>0.8224</cell><cell>0.7297</cell><cell>0.7811</cell><cell>0.7208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,451.39,349.24,105.74"><head>Table 4</head><label>4</label><figDesc>Scores of the four systems on long term test collection.</figDesc><table coords="8,154.55,482.95,283.68,74.17"><row><cell cols="5">Evaluation Measure Basic SynQE PseudoRelQE AllQE</cell></row><row><cell>MAP</cell><cell>0.2123</cell><cell>0.1719</cell><cell>0.1872</cell><cell>0.1676</cell></row><row><cell>nDCG</cell><cell>0.3807</cell><cell>0.3231</cell><cell>0.3490</cell><cell>0.3138</cell></row><row><cell>nDCG@5</cell><cell>0.1973</cell><cell>0.1607</cell><cell>0.1754</cell><cell>0.1562</cell></row><row><cell>P@10</cell><cell>0.1418</cell><cell>0.1187</cell><cell>0.1244</cell><cell>0.1131</cell></row><row><cell>Recall</cell><cell>0.8391</cell><cell>0.7481</cell><cell>0.8026</cell><cell>0.7353</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="2,92.57,616.14,309.93,8.97" xml:id="b0">
	<monogr>
		<ptr target="https://bitbucket.org/frrncl/se-unipd/src/master/" />
		<title level="m" coord="2,92.57,616.14,120.94,8.97">UniPD Search Engines repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="2,92.57,649.02,343.68,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="2,129.70,649.02,162.86,8.97">ChatGPT&quot;, knowledge cutoff: september 2021</title>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="2,92.30,659.98,224.02,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="2,166.29,659.98,51.94,8.97">Apache lucene</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="org.apache.lucene" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="2,92.57,670.94,169.60,8.97;13,89.29,85.67,71.57,12.85" xml:id="b3">
	<monogr>
		<ptr target="org.apache.lucene.analysis" />
		<title level="m" coord="2,92.57,670.94,52.91,8.97">French package</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.59,111.28,400.24,10.91;13,107.59,124.83,399.59,10.91;13,107.59,138.38,282.49,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,461.23,111.28,46.59,10.91;13,107.59,124.83,395.08,10.91">LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03229</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,154.10,138.38,122.18,10.91">Information Retrieval (cs.IR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>arXiv.org</note>
</biblStruct>

<biblStruct coords="13,107.59,151.93,399.60,10.91;13,107.59,165.48,152.05,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="13,300.72,151.93,167.93,10.91">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct coords="13,107.59,179.03,399.60,10.91;13,107.59,192.57,282.49,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,213.71,179.03,288.67,10.91">Query Expansion Techniques for Information Retrieval: a Survey</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deepak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00247</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,154.10,192.57,126.08,10.91">Information Retrieval (cs.IR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>arXiv.org</note>
</biblStruct>

<biblStruct coords="13,107.59,206.12,399.60,10.91;13,107.59,219.67,333.61,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,240.00,206.12,262.01,10.91">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,107.59,219.67,254.75,10.91">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>FnTIR)</note>
</biblStruct>

<biblStruct coords="13,107.59,233.22,398.40,10.91;13,107.59,246.77,399.59,10.91;13,107.59,260.32,399.60,10.91;13,107.59,273.87,398.40,10.91;13,107.59,287.42,399.59,10.91;13,107.59,300.97,176.25,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,258.12,233.22,247.87,10.91;13,107.59,246.77,182.09,10.91">Query Expansion for Sentence Retrieval Using Pseudo Relevance Feedback and Word Embedding</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,337.01,260.32,170.17,10.91;13,107.59,273.87,398.40,10.91;13,107.59,287.42,107.24,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Eighth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,245.78,287.42,183.98,10.91">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10456</biblScope>
			<biblScope unit="page" from="97" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.59,314.52,398.40,10.91;13,107.26,328.07,76.13,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<title level="m" coord="13,155.82,314.52,218.29,10.91">Human Behavior and the Principle of Least Effort</title>
		<meeting><address><addrLine>Cambridge (MA), USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.59,341.62,399.60,10.91;13,107.41,355.17,177.57,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,203.82,341.62,141.63,10.91">Thesaurus and query expansion</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>New Delhi, India</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Jamia Hamdard</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.59,368.71,399.60,10.91;13,107.59,382.26,399.60,10.91;13,107.59,395.81,398.40,10.91;13,107.26,409.36,286.84,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Devaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-5010,LINDAT/CLARIAH" />
		<title level="m" coord="13,160.25,382.26,119.26,10.91">LongEval train collection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,107.59,422.91,399.60,10.91;13,107.59,436.46,400.25,10.91;13,107.59,450.01,398.40,10.91;13,107.59,463.56,203.00,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Devaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-5139,LINDAT/CLARIAH" />
		<title level="m" coord="13,107.59,436.46,102.21,10.91">LongEval test collection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
