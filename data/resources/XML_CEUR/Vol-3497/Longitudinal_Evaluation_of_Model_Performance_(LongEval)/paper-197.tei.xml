<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,392.82,15.42;1,89.29,106.66,231.68,15.42;1,89.29,129.00,220.08,11.96">The Temporal Persistence of Generative Language Models in Sentiment Analysis Notebook for the LongEval Lab at CLEF 2023</title>
				<funder ref="#_EQF6T6C">
					<orgName type="full">UKRI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,93.00,11.96"><forename type="first">Pablo</forename><surname>Medina-Alias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bath</orgName>
								<address>
									<addrLine>Claverton Down</addrLine>
									<settlement>Bath</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.52,154.90,67.83,11.96"><forename type="first">Özgür</forename><surname>Şimşek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bath</orgName>
								<address>
									<addrLine>Claverton Down</addrLine>
									<settlement>Bath</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,392.82,15.42;1,89.29,106.66,231.68,15.42;1,89.29,129.00,220.08,11.96">The Temporal Persistence of Generative Language Models in Sentiment Analysis Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BE8A6976A261E1F15669CF5566B232CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text classification</term>
					<term>temporal robustness</term>
					<term>pre-trained language representations</term>
					<term>text generation</term>
					<term>social media analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained transformer-based language models (PLMs) have revolutionised text classification tasks, but their performance tends to deteriorate on data distant in time from the training dataset. Continual supervised re-training help address this issue but it is limited by the availability of newly labelled samples. This paper explores the longitudinal generalisation abilities of large generative PLMs, such as GPT-3 and T5, and smaller encoder-only alternatives for sentiment analysis in social media. We investigate the impact of time-related variations in data, model size, and fine-tuning on the classifiers' performance. Through competitive evaluation in the CLEF-2023 LongEval Task 2, we compare results from fine-tuning, few-shot learning, and zero-shot learning. Our analysis reveals the superior performance of large generative models over the benchmark RoBERTa and highlights the benefits of limited exposure to training data in achieving robust predictions on temporally distant test sets. The findings contribute to understanding how to build more temporally robust transformer-based text classifiers, reducing the need for continuous re-training with annotated data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pre-trained transformer-based language models (PLMs) such as BERT <ref type="bibr" coords="1,399.42,454.25,12.77,10.91" target="#b0">[1]</ref> have been extremely successful on text classification tasks. However, their performance tends to deteriorate when they are tested on data that is distant in time from the initial training dataset. This lack of "temporal persistence" is often addressed by continuously updating the models with newly annotated data <ref type="bibr" coords="1,159.06,508.45,11.43,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,173.22,508.45,7.62,10.91" target="#b2">3]</ref>. Simultaneously, recent progress in NLP has seen the rise of large-scale, generative PLMs such as GPT-3 <ref type="bibr" coords="1,236.79,522.00,13.00,10.91" target="#b3">[4]</ref> and T5 <ref type="bibr" coords="1,287.49,522.00,11.58,10.91" target="#b4">[5]</ref>, which have redefined the state-of-the-art in a wide range of NLP tasks, even when only learning from a few examples. The enhanced performance of these models comes at the expense of larger model sizes, making it increasingly costly to update them with incoming samples. As such, understanding how to build more temporally robust transformer-based text classifiers is essential to help alleviate the need for continuous re-training with annotated data.</p><p>With the aim of encouraging new research on temporally robust classifier systems, CLEF-2023 LongEval Task 2 <ref type="bibr" coords="2,166.63,100.52,12.99,10.91" target="#b5">[6]</ref> proposed to competitively evaluate the short and long-term persistence in performance of models for sentiment analysis in social media. In this paper, we narrow the research scope of the task to focus on the comparison of the longitudinal generalisation abilities between that of large pre-trained generative models and those of smaller encoder-only alternatives. Moreover, we strive to explore methods for training more temporally robust models. We hypothesise that extensive fine-tuning over a short-term optimisation objective might result in their decreased performance on new, evolving, datasets. Therefore, we address the following research questions:</p><p>1. How well do generative PLMs adapt to variations in the time-related characteristics of the test data? 2. How does the longitudinal drop in classification performance of encoder-only models compare to that of decoder-only and encoder-decoder models? 3. Does model size contribute to better longitudinal generalisation in PLMs? 4. How does the amount of fine-tuning affect the effectiveness of PLMs in preventing performance deterioration on temporally evolving datasets?</p><p>In an endeavour to address these questions while achieving competitive results in the task, we focused on GPT-3 and T5. We present the results obtained by these models in the evaluation phase of the competition with a comparative view of fine-tuning, few-shot, and zero-shot learning. Furthermore, we provide a post-evaluation analysis of the impact of model size and exposure to training data on the short and long-term classification scores. Our results show the superiority of large transformer-based generative models over the task benchmark, RoBERTa <ref type="bibr" coords="2,89.29,402.66,11.36,10.91" target="#b6">[7]</ref>, and show that model performance in the temporally distant test set benefited from limited exposure to training data. This generalisation capability resulted in more robust predictions, outperforming all other competing models in this task (see <ref type="bibr" coords="2,353.39,429.76,42.60,10.91">Section 5)</ref>.</p><p>The body of this paper is organised as follows: Section 2 provides an overview of previous related research. Section 3 summarises the details of the task. Our methodological approach is described in Section 4, followed by the analysis of the results in every phase of the competition in Section 5. Section 6 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Performance deterioration over time. Recent work has demonstrated the lack of temporal persistence of machine learning models on NLP tasks. Lukes and Søgaard <ref type="bibr" coords="2,433.84,556.13,12.84,10.91" target="#b7">[8]</ref> studied how shifts in lexical polarity negatively impacted the performance of sentiment classifiers that use logistic regression. Florio et al. <ref type="bibr" coords="2,226.04,583.23,12.84,10.91" target="#b8">[9]</ref> dynamically tested BERT for hate speech detection, showing that shifts in the topical composition of the test data resulted in a decrease in classification scores. Lazaridou et al. <ref type="bibr" coords="2,198.34,610.33,12.84,10.91" target="#b1">[2]</ref> showed that the performance degradation of transformer-based language models over time transfer into downstream tasks such as part-of-speech (POS) tagging or question answering (QA), which cannot be prevented with a larger model size. This last result is in line with the findings of Agarwal and Nenkova <ref type="bibr" coords="2,321.87,650.98,16.24,10.91" target="#b9">[10]</ref>, who also found evidence against the performance deterioration of pre-trained language representations in tasks with more stable label definitions such as sentiment and domain classification. More recently, Alkhalifa et al. <ref type="bibr" coords="3,493.14,86.97,12.84,10.91" target="#b2">[3]</ref> extended the analysis of longitudinal persistence to a larger set of models for text classification, including autorregressive architectures such as Hierarchical Attention Network (HAN) <ref type="bibr" coords="3,469.63,114.06,17.76,10.91" target="#b10">[11]</ref> and Generative Pre-trained Transformer 2 (GPT-2) <ref type="bibr" coords="3,296.89,127.61,16.23,10.91" target="#b11">[12]</ref>. They observed a generalised performance drop across models, with transformer-based classifiers consistently yielding better results. Our work brings new evidence on the longitudinal performance of two generative models that, to the best of our knowledge, have not been tested for this task: GPT-3 and T5.</p><p>Improving temporal persistence. In relation to building classification models that dynamically adapt to non-stationary NLP tasks, Lukes and Søgaard <ref type="bibr" coords="3,363.75,195.36,12.84,10.91" target="#b7">[8]</ref> proposed a feature selection approach based on estimating the lexical polarity rank for a given period of time to induce temporal robustness in sentiment classification. He et al. <ref type="bibr" coords="3,353.86,222.46,17.91,10.91" target="#b12">[13]</ref> introduced two evolutionary learning methods for neural networks based on temporal parameter smoothing and diachronic propagation when training on temporarily split data. Other approaches focus on the continual adaptation of language representations to prevent performance loss in new streams of data <ref type="bibr" coords="3,89.29,276.66,16.54,10.91" target="#b13">[14,</ref><ref type="bibr" coords="3,108.55,276.66,12.41,10.91" target="#b14">15]</ref>. Alkhalifa et al. <ref type="bibr" coords="3,197.19,276.66,17.91,10.91" target="#b15">[16]</ref> mitigated performance drop in neural stance detection by incrementally updating n-gram based embeddings. Alternatively, integrating temporal information into contextual representations; as a result of masked pre-training <ref type="bibr" coords="3,388.24,303.75,16.48,10.91" target="#b16">[17,</ref><ref type="bibr" coords="3,407.45,303.75,12.56,10.91" target="#b17">18,</ref><ref type="bibr" coords="3,422.74,303.75,12.36,10.91" target="#b18">19]</ref>, a combination with temporal embeddings <ref type="bibr" coords="3,210.57,317.30,16.21,10.91" target="#b19">[20]</ref>, or by its inclusion into the model's attention mechanism <ref type="bibr" coords="3,486.93,317.30,16.21,10.91" target="#b20">[21]</ref>, effectively reduced temporal deterioration in downstream tasks. In this regard, Röttger and Pierrehumbert <ref type="bibr" coords="3,138.87,344.40,17.91,10.91" target="#b18">[19]</ref> found that although temporal adaptation did contribute to improved language modelling and text classification, it was less effective than domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description</head><p>The CLEF-2023 LongEval Task 2 <ref type="bibr" coords="3,232.03,416.58,12.69,10.91" target="#b5">[6]</ref> provided participants with a binary sentiment classification challenge, wherein the test data was progressively more distant in time from the training data.</p><p>The training data comprised a corpus of approximately 50,000 distantly-annotated English Tweets from 2016, extracted from the TM-Senti Dataset <ref type="bibr" coords="3,336.47,457.22,16.16,10.91" target="#b21">[22]</ref>. Model performance was assessed using the Macro-F1 score and the relative performance drop (RPD) <ref type="foot" coords="3,373.61,469.02,3.71,7.97" target="#foot_0">1</ref> . The evaluation datasets used by <ref type="bibr" coords="3,102.85,484.32,12.73,10.91" target="#b5">[6]</ref> consisted of human-annotated development and test sets associated with three temporal benchmarks: 2016 ("within time" as this overlapped the training data), 2018 ("short-term"), and 2021 ("long-term"). To facilitate the utilisation of unsupervised methods, an additional set of around 1 million unlabelled data samples, along with their corresponding creation year spanning from 2013 to 2021, was provided.</p><p>The task encompassed three distinct phases. During the development phase, participants were granted access to the training and development data, thereby enabling them to develop and pre-evaluate their classifiers. In the evaluation phase, candidate models underwent testing against one test set per temporal benchmark. The number of submission attempts was limited to a maximum of five. Finally, the post-evaluation phase facilitated further analysis of model performance by disclosing the ground truth labels of the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We focused on three well established transformer-based architectures for natural language tasks: (1) RoBERTa, (2) T5, and (3) GPT-3. Table <ref type="table" coords="4,304.21,124.83,5.09,10.91" target="#tab_0">1</ref> provides an overview of the models used in the present work. RoBERTa, serving as our baseline and representative encoder-only model, has consistently exhibited robust performance in natural language understanding tasks. We fine-tuned<ref type="foot" coords="4,481.49,327.58,3.71,7.97" target="#foot_1">2</ref> two sequence classifiers with RoBERTa_base and RoBERTa_large encoders, respectively. Given the nature of the competition, we selected the best model check-point and hyper-parameters attending to loss minimisation in the "within-time" development set (2016). All RoBERTa classifiers were trained by using AdamW with linear learning rate decay (decay=1e-3). We found 2e-05 and 1e-05 to be the best learning rates for RoBERTa_base and RoBERTa_large, respectively.</p><p>For the evaluation of encoder-decoder models, we chose T5. Specifically, we fine-tuned T5_base and T5_large for binary text-to-text classification. T5 models were optimised with Adafactor, and a learning rate of 1e-04. The maximum sequence length was fixed to 128 during RoBERTa and T5 training.</p><p>Finally, we used GPT-3 to test the performance of decoder-only models at scale. In addition, so as to evaluate the in-context and fine-tuned effectiveness of LLMs, we evaluated GPT-3 in three distinct learning paradigms: zero-shot (0-S) and few-shot (F-S) learning with instruction prompting, as well as fine-tuning. We selected the largest GPT-3 model, 'davinci'<ref type="foot" coords="4,439.68,517.27,3.71,7.97" target="#foot_2">3</ref> , for prompted inference; and the smaller 'babbage' and 'curie' versions for supervised re-training. GPT-3 models were re-trained using the OpenAI API, with learning rates of 4e-5 for GPT-3 babbage and 2.2e-5 for GPT-3 curie. For the few-shot scenario, we retrieved fifteen random samples from the annotated training data. The prompts used for zero and few-shot inference can be found in section A of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>We present the results of the 2023 CLEC-LongEval<ref type="foot" coords="5,316.60,109.52,3.71,7.97" target="#foot_3">4</ref> classification in Table <ref type="table" coords="5,422.26,111.28,3.74,10.91" target="#tab_1">2</ref>. Our winning model is T5, which, surprisingly was not the best performing on the development set. The results in the development and evaluation splits are presented in Tables <ref type="table" coords="5,470.59,278.33,5.17,10.91" target="#tab_2">3</ref> and<ref type="table" coords="5,500.04,278.33,3.81,10.91" target="#tab_3">4</ref>. During the development phase, we observed that the fine-tuned versions of GPT-3 and T5 achieved higher F1 scores compared to the baseline RoBERTa model. However, due to the typical formulation of sentiment analysis as a three-class problem, the zero-shot predictions of GPT-3 often included the 'neutral' label despite our efforts in prompt design, which negatively impacted its score. Although GPT-3 achieved the highest overall F1 score, T5 demonstrated better short-term stability. In the evaluation phase, T5 emerged as the best-performing model overall, achieving an average F1 score of 0.703 across the three temporal benchmarks. However, it was outperformed by GPT-3 in the short-term evaluation. Fine-tuned GPT-3's overall performance was inferior to the benchmark, which we attribute to overfitting towards the 2016 score during the development phase. Notably, GPT-3Âťs largest version outcompeted the baseline when prompted with a few examples in the within time period, but this performance was not sustained further in time. Regarding longitudinal performance drop, the fine-tuned version of GPT-3 exhibited more robust predictions. Surprisingly, the zero and few-shot versions of GPT-3 displayed the highest performance drop among all models.</p><p>To explore the impact of model size on performance, we also investigated the results obtained by the large versions of these models in the post-evaluation phase. For that, we selected the 6.7B-parameter version of GPT-3, 'curie'; T5_large; and RoBERTa_large. All models yielded higher F1 scores than their smaller versions in the three temporal benchmarks. However, a larger number of parameters did not prevent longitudinal performance drop. Again, the encoderdecoder option provided the best overall classification results, showcasing its superiority among the considered models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Longitudinal Overfitting</head><p>Given the significant difference in the model performance between the development and the longitudinally-distant test sets, we investigate the possible causes for performance drop using three tests. The first is a statistical analysis of the similarity of each of the data splits, inspired by Tayyar Madabushi et al. <ref type="bibr" coords="6,213.59,432.76,16.26,10.91" target="#b22">[23]</ref>. The second consists of comparing the topical distribution in each temporal split, inspired by Florio et al. <ref type="bibr" coords="6,285.90,446.31,11.44,10.91" target="#b8">[9]</ref>, who show that such topical differences result in a drop in performance across temporal splits. Finally, we explore the impact of the number of training samples on T5, our best performing model on the test set, given the fact that the training data is, by virtue of the task, similar only to one of the evaluation sets ("within").</p><p>For our statistical exploration of splits, we use the Wilcoxon signed-rank test <ref type="bibr" coords="6,462.16,500.50,16.41,10.91" target="#b23">[24]</ref>. The process involves randomly sampling observations from different data splits and comparing their frequency counts to assess whether the datasets exhibit a statistically similar distribution of words. Table <ref type="table" coords="6,162.04,541.15,5.17,10.91" target="#tab_5">5</ref> presents the range of minimum and maximum p-values obtained from ten runs of each pair, along with their corresponding interpretations. It can be observed that the statistical similarity between the "within dev" (what is optimised for) and the "long test" is the least with a p-value of less than 0.05, which is also what is reflected in terms of the performance drop of our model.</p><p>For our analysis of the topical distribution of the different evaluation splits, we use a BERTbased pre-trained topic classifier to extract topical information <ref type="bibr" coords="6,373.48,622.44,16.34,10.91" target="#b24">[25]</ref>. This result, presented in Table <ref type="table" coords="6,115.19,635.99,3.66,10.91" target="#tab_6">6</ref>, shows that the distribution of most common topics does not exhibit significant changes along the temporal data splits, except for the "News" category, likely associated to the COVID-19 pandemic. Thus, we conclude that while the broader topics were similar, the nature of these discussions, subtopic distributions, or their associated sentiment might have evolved, resulting in a drop in our classifier performance. Finally, we investigate the impact of additional training steps on the longitudinal robustness of T5, our best performing model. Figure <ref type="figure" coords="7,280.30,522.08,5.17,10.91" target="#fig_0">1</ref> illustrates the F1-Score obtained in the "within" development set and the three test splits as the model is fine-tuned with the training data during three epochs. We averaged the results of three training instances initiated with different random seeds and the hyperparameters described in section 4. We note that T5 pre-trainning is enough to offer competitive results in this task. However, this capability includes predicting the label "neutral", which can be adapted to the binary specification within a few hundred steps. All subsequent training results in over-fitting, and consequently, in a drop in longitudinal performance. We believe this to be a result of catastrophic forgetting of the information embedded during pre-training in favour of the new samples and conclude that, for pre-trained models, additional training data might not always be helpful when temporal robustness is at play. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future work</head><p>In this work, we evaluate the robustness of several well-known PLMs to temporal variations in text classification. We find that generative PLMs outperform encoder-only alternatives such as RoBERTa (the task baseline) across most temporal splits. Additional analysis confirms our hypothesis that increased training on the downstream short-term objective can hinder the generalisation capabilities of the pre-trained model, despite the similarity of topical distributions across the different temporal splits. Our findings on the role of model size in this task are in line with previous results in the field, showing that a larger number of parameters does not prevent performance drop.</p><p>In the future, we intend to more thoroughly test and improve the temporal robustness of generative PLMs under a larger set of carefully controlled temporal splits. In particular, exploring methods to improve their zero and few-shot capabilities; such as prompt fine-tuning, chain-ofthought prompting, or informative sample selection, might effectively reduce the re-training requirements of these models. Alternatively, we regard unsupervised methods for identifying polarity shifts towards topics and named entities as a promising research direction towards building more temporally-aware sentiment classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,90.62,257.28,8.93;8,124.49,106.52,343.80,256.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: F1 Score on the evaluation sets along T5 fine-tuning.</figDesc><graphic coords="8,124.49,106.52,343.80,256.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,167.39,352.43,129.65"><head>Table 1</head><label>1</label><figDesc>Summary of models</figDesc><table coords="4,153.86,199.00,287.56,98.03"><row><cell>Model</cell><cell>Type</cell><cell cols="2">Param. Size Vocab Size (EN)</cell></row><row><cell>RoBERTa_base</cell><cell>Encoder</cell><cell>125M</cell><cell>50K</cell></row><row><cell>RoBERTa_large</cell><cell>Encoder</cell><cell>355M</cell><cell>50K</cell></row><row><cell>T5_base</cell><cell>Encoder-Decoder</cell><cell>222M</cell><cell>32K</cell></row><row><cell>T5_large</cell><cell>Encoder-Decoder</cell><cell>737M</cell><cell>32K</cell></row><row><cell>GPT-3 (babbage)</cell><cell>Decoder</cell><cell>1.3B</cell><cell>52K</cell></row><row><cell>GPT-3 (curie)</cell><cell>Decoder</cell><cell>6.7B</cell><cell>52K</cell></row><row><cell>GPT-3 (davinci)</cell><cell>Decoder</cell><cell>175B</cell><cell>52K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,140.29,403.00,105.75"><head>Table 2</head><label>2</label><figDesc>Final leader board for LongEval 2023 Task 2.</figDesc><table coords="5,100.80,171.85,391.19,74.18"><row><cell cols="2">Position Team</cell><cell cols="5">F1 Within F1 Short F1 Long Overall Drop Overall Score</cell></row><row><cell>1</cell><cell>This work</cell><cell>0.7377</cell><cell>0.6739</cell><cell>0.6971</cell><cell>-0.0708</cell><cell>0.7029</cell></row><row><cell>2</cell><cell cols="2">CLEF-LE (baseline) 0.7459</cell><cell>0.6839</cell><cell>0.6549</cell><cell>-0.1025</cell><cell>0.6945</cell></row><row><cell>3</cell><cell>Cordyceps</cell><cell>0.7246</cell><cell>0.6771</cell><cell>0.6549</cell><cell>-0.0669</cell><cell>0.6923</cell></row><row><cell>4</cell><cell>saroyehun</cell><cell>0.7203</cell><cell>0.6674</cell><cell>0.6874</cell><cell>-0.0596</cell><cell>0.6917</cell></row><row><cell>5</cell><cell>pakapro</cell><cell>0.533</cell><cell>0.4648</cell><cell>0.4910</cell><cell>-0.0504</cell><cell>0.4863</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,388.57,366.37,105.75"><head>Table 3</head><label>3</label><figDesc>Results on the development set, both within (2016) and short-term (2018).</figDesc><table coords="5,137.42,420.14,317.94,74.18"><row><cell></cell><cell cols="4">F1 Within F1 Short RP Drop Overall Score</cell></row><row><cell>RoBERTa_base (baseline)</cell><cell>0.788</cell><cell>0.761</cell><cell>-0.034</cell><cell>0.775</cell></row><row><cell>T5_base</cell><cell>0.791</cell><cell>0.771</cell><cell>-0.025</cell><cell>0.781</cell></row><row><cell>GPT3_davinci (0-S)</cell><cell>0.718</cell><cell>0.693</cell><cell>-0.035</cell><cell>0.706</cell></row><row><cell>GPT3_davinci (F-S)</cell><cell>0.746</cell><cell>0.697</cell><cell>-0.066</cell><cell>0.722</cell></row><row><cell>GPT3_babbage (F-T)</cell><cell>0.823</cell><cell>0.798</cell><cell>-0.030</cell><cell>0.811</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,90.49,213.95,20.87"><head>Table 4</head><label>4</label><figDesc>Results on the Test set, within, short, and long term.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,105.85,122.05,381.09,139.43"><head>F1 Within F1 Short F1 Long Overall Drop Overall Score</head><label></label><figDesc></figDesc><table coords="6,105.85,139.48,361.90,122.00"><row><cell>*in evaluation phase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa (baseline)</cell><cell>0.727</cell><cell>0.670</cell><cell>0.687</cell><cell>-0.067</cell><cell>0.695</cell></row><row><cell>T5_base</cell><cell>0.738</cell><cell>0.674</cell><cell>0.697</cell><cell>-0.071</cell><cell>0.703</cell></row><row><cell>GPT3_davinci (0-S)</cell><cell>0.706</cell><cell>0.632</cell><cell>0.647</cell><cell>-0.094</cell><cell>0.662</cell></row><row><cell>GPT3_davinci (F-S)</cell><cell>0.728</cell><cell>0.664</cell><cell>0.656</cell><cell>-0.093</cell><cell>0.683</cell></row><row><cell>GPT3_babbage (F-T)</cell><cell>0.720</cell><cell>0.682</cell><cell>0.675</cell><cell>-0.058</cell><cell>0.692</cell></row><row><cell>*in post-evaluation phase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa_large</cell><cell>0.742</cell><cell>0.679</cell><cell>0.681</cell><cell>-0.084</cell><cell>0.701</cell></row><row><cell>T5_large</cell><cell>0.747</cell><cell>0.682</cell><cell>0.725</cell><cell>-0.058</cell><cell>0.718</cell></row><row><cell>GPT3_curie (F-T)</cell><cell>0.725</cell><cell>0.689</cell><cell>0.687</cell><cell>-0.051</cell><cell>0.700</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.99,90.49,386.00,129.66"><head>Table 5</head><label>5</label><figDesc>Results from the Wilcoxon Signed-rank test for Corpus Similarity and corresponding p-values.</figDesc><table coords="7,159.87,122.10,275.54,98.04"><row><cell>Corpus Pair</cell><cell cols="3">Min p value Max p value %Same</cell></row><row><cell>Train vs Train</cell><cell>0.2302</cell><cell>0.9314</cell><cell>100</cell></row><row><cell>Train vs Within_eval</cell><cell>0.2244</cell><cell>0.9347</cell><cell>100</cell></row><row><cell>Train vs Short_eval</cell><cell>0.1673</cell><cell>0.2969</cell><cell>100</cell></row><row><cell>Train vs Long_eval</cell><cell>0.0123</cell><cell>0.6077</cell><cell>90</cell></row><row><cell>Within_dev vs Within_eval</cell><cell>0.2629</cell><cell>0.9987</cell><cell>100</cell></row><row><cell>Within_dev vs Short_eval</cell><cell>0.0284</cell><cell>0.6752</cell><cell>90</cell></row><row><cell>Within_dev vs Long_eval</cell><cell>0.0108</cell><cell>0.0761</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,88.99,242.39,370.81,212.33"><head>Table 6</head><label>6</label><figDesc>Frequence (%) of top 10 most common of topics in all data splits.</figDesc><table coords="7,135.47,274.01,324.33,180.72"><row><cell></cell><cell cols="5">Daily Life Relationships Celebrities Music Film and TV</cell></row><row><cell>Train</cell><cell>57.34</cell><cell>11.48</cell><cell>8.24</cell><cell>7.40</cell><cell>5.33</cell></row><row><cell>Within_dev</cell><cell>57.89</cell><cell>11.77</cell><cell></cell><cell>6.70</cell><cell>4.02</cell></row><row><cell>Short_dev</cell><cell>54.46</cell><cell>8.33</cell><cell>6.47</cell><cell>7.07</cell><cell>5.73</cell></row><row><cell>Within_eval</cell><cell>62.54</cell><cell>10.57</cell><cell>4.74</cell><cell>5.84</cell><cell>5.84</cell></row><row><cell>Short_eval</cell><cell>54.96</cell><cell>7.71</cell><cell>5.84</cell><cell>6.61</cell><cell>6.50</cell></row><row><cell>Long_eval</cell><cell>48.79</cell><cell>6.17</cell><cell>7.16</cell><cell>6.28</cell><cell>6.72</cell></row><row><cell></cell><cell>Food</cell><cell>Sports</cell><cell cols="2">Education News</cell><cell>Family</cell></row><row><cell>Train</cell><cell>4.72</cell><cell>4.39</cell><cell>3.72</cell><cell>3.24</cell><cell>3.23</cell></row><row><cell>Within_dev</cell><cell>5.06</cell><cell>4.84</cell><cell>4.76</cell><cell>3.72</cell><cell>3.20</cell></row><row><cell>Short_dev</cell><cell>5.51</cell><cell>6.18</cell><cell>2.68</cell><cell>3.27</cell><cell>3.35</cell></row><row><cell>Within_eval</cell><cell>5.18</cell><cell>4.41</cell><cell>4.08</cell><cell>4.52</cell><cell>3.19</cell></row><row><cell>Short_eval</cell><cell>5.29</cell><cell>6.61</cell><cell>2.53</cell><cell>3.63</cell><cell>2.53</cell></row><row><cell>Long_eval</cell><cell>4.19</cell><cell>4.18</cell><cell>3.30</cell><cell>8.37</cell><cell>3.30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.30,670.98,326.01,8.97"><p>The RPD is calculated as the relative change in the F1-score between two testing periods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,649.03,413.42,8.97;4,92.57,659.99,228.62,8.97"><p>Only the labelled sets were used during training in this work. Therefore, future mentions to training data exclude the additional unlabelled samples provided in the competition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.57,670.95,150.55,8.97"><p>https://platform.openai.com/docs/models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,92.57,671.04,213.50,8.97"><p>https://codalab.lisn.upsaclay.fr/competitions/12762#results</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by <rs type="funder">UKRI</rs>, under grant number <rs type="grantNumber">EP/S023437/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EQF6T6C">
					<idno type="grant-number">EP/S023437/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix GPT-3 Prompts</head><p>Prompt for zero-shot classification.</p><p>Please label the following tweet as either positive or negative.</p><p>-Tweet: &lt; Text_sample&gt; -Label:</p><p>Prompt for few-shot (15 examples) classification.</p><p>Please label the following tweet as either Positive or Negative.</p><p>-Tweet: Where else can I get a pumpkin coffee in the morning because DD does not have their shit together.</p><p>-Label: negative -Tweet: my mentions though I couldn't be more grateful for what happened tonight I love Shawn so much -Label: negative -Tweet: I sneezed on the beat and the beat got sicker -Label: positive -Tweet: @MENTION Hi Niall, I hope you're fine, I love you so much! Thanks for everything.</p><p>You make me smile can you follow me please? 1450 -Label: positive -Tweet: @MENTION Are You bored by listening pop and radio music ?</p><p>Join us, This Channel let's You discover new emotions in each track -Label: positive -Tweet: So it smells of weed in the car. Pretty sure it's coming from a certain someone's reading rucksack @MENTION -Label: positive -Tweet: hate to see a good guy get fucked over like wyd girl -Label: negative -Tweet: When Nike leaves the security tag on your shoes and you back to get it off, and the alarm goes off, but didn't go off when you left. -Label: negative -Tweet: Coz' I was born for you.. @MENTION @MENTION HAH joke.. -Label: positive -Tweet: Can't fault her the last nigga spoiled her.</p><p>-Label: positive -Tweet: Trying to have the #bun #hairstyle. Gotta wait a few months but the wait gonna be worth it -Label: positive -Tweet: Fan was soooo AMAZING Loved it You MUST watch it @MENTION was again the BEST actor in the world -Label: positive -Tweet: Bout to take a nap n then wake up n do some with my life!! -Label: positive -Tweet: Send emojis for a tbh because I'm bored asf -Label: negative -Tweet: Pahabol essays and hw are the worst -Label: negative -Tweet: &lt; Text_sample&gt; -Label:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,111.28,393.33,10.91;9,112.66,124.83,393.33,10.91;9,112.66,138.38,393.32,10.91;9,112.66,151.93,393.33,10.91;9,112.66,165.48,394.03,10.91;9,112.66,179.03,185.51,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,323.15,111.28,182.83,10.91;9,112.66,124.83,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="9,327.87,124.83,178.11,10.91;9,112.66,138.38,393.32,10.91;9,112.66,151.93,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="9,112.66,192.57,394.53,10.91;9,112.66,206.12,393.33,10.91;9,112.66,219.67,393.33,10.91;9,112.66,233.22,140.59,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,350.54,206.12,155.45,10.91;9,112.66,219.67,185.25,10.91">Mind the gap: Assessing temporal generalization in neural language models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,307.34,219.67,198.64,10.91;9,112.66,233.22,36.36,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29348" to="29363" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,246.77,393.33,10.91;9,112.66,260.32,387.03,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,290.13,246.77,215.86,10.91;9,112.66,260.32,125.91,10.91">Building for tomorrow: Assessing the temporal persistence of text classifiers</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,246.31,260.32,175.43,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">103200</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,273.87,394.53,10.91;9,112.66,287.42,394.53,10.91;9,112.66,300.97,394.53,10.91;9,112.66,314.52,394.53,10.91;9,112.66,328.07,395.01,10.91;9,112.66,341.62,187.21,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,112.66,328.07,174.33,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,355.17,394.53,10.91;9,112.66,368.71,393.33,10.91;9,112.48,382.26,264.75,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,112.66,368.71,363.77,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,488.38,368.71,17.60,10.91;9,112.48,382.26,170.67,10.91">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,395.81,394.53,10.91;9,112.66,409.36,394.52,10.91;9,112.66,422.91,394.53,10.91;9,112.66,436.46,394.53,10.91;9,112.66,450.01,393.33,10.91;9,112.33,463.56,375.08,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,112.66,436.46,320.92,10.91">Longeval: Longitudinal evaluation of model performance at clef 2023</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tayyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,142.21,463.56,151.66,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,477.11,394.53,10.91;9,112.30,490.66,393.68,10.91;9,112.66,504.21,107.17,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="9,173.53,490.66,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,517.76,393.33,10.91;9,112.66,531.30,393.33,10.91;9,112.66,544.85,111.40,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,212.79,517.76,182.33,10.91">Sentiment analysis under temporal shift</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lukes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,420.76,517.76,85.22,10.91;9,112.66,531.30,393.33,10.91;9,112.66,544.85,34.00,10.91">Proceedings of the 9th workshop on computational approaches to subjectivity, sentiment and social media analysis</title>
		<meeting>the 9th workshop on computational approaches to subjectivity, sentiment and social media analysis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,558.40,393.33,10.91;9,112.66,571.95,352.10,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,345.06,558.40,160.93,10.91;9,112.66,571.95,200.01,10.91">Time of your hate: The challenge of time in hate speech detection on social media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Florio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,321.50,571.95,75.45,10.91">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4180</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,585.50,393.33,10.91;9,112.66,599.05,389.50,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,225.77,585.50,280.22,10.91;9,112.66,599.05,20.77,10.91">Temporal effects on pre-trained models for language processing tasks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,141.51,599.05,276.71,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="904" to="921" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,612.60,393.53,10.91;9,112.66,626.15,393.32,10.91;9,112.66,639.70,394.53,10.91;9,112.66,653.25,90.72,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,347.46,612.60,158.73,10.91;9,112.66,626.15,104.93,10.91">Hierarchical attention networks for document classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.45,626.15,265.53,10.91;9,112.66,639.70,389.93,10.91">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
		<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,666.80,393.33,10.91;10,112.66,86.97,253.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,412.10,666.80,93.89,10.91;10,112.66,86.97,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,262.00,86.97,56.95,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,393.33,10.91;10,112.66,114.06,395.01,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,278.66,100.52,227.33,10.91;10,112.66,114.06,38.57,10.91">Time-evolving text classification with deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,174.41,114.06,268.49,10.91">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.61,10.91;10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,290.62,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,417.00,127.61,90.27,10.91;10,112.66,141.16,276.00,10.91">Lifelong pretraining: Continually adapting language models to emerging corpora</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,419.35,141.16,86.64,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,192.47,10.91">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4764" to="4780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,394.53,10.91;10,112.66,195.36,285.15,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,248.76,181.81,254.05,10.91">Lamol: Language modeling for lifelong language learning</title>
		<author>
			<persName coords=""><forename type="first">F.-K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.29,195.36,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.60,10.91;10,112.66,222.46,393.33,10.91;10,112.66,236.01,187.66,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,292.74,208.91,213.52,10.91;10,112.66,222.46,128.09,10.91">Opinions are made to be changed: Temporally adaptive stance classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,263.44,222.46,242.55,10.91;10,112.66,236.01,109.60,10.91">Proceedings of the 2021 workshop on open challenges in online social networks</title>
		<meeting>the 2021 workshop on open challenges in online social networks</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,395.17,10.91;10,112.66,263.11,393.54,10.91;10,112.66,276.66,201.81,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,480.65,249.56,27.18,10.91;10,112.66,263.11,231.61,10.91">Timeaware language models as temporal knowledge bases</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,352.53,263.11,153.67,10.91;10,112.66,276.66,117.87,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,394.62,10.91;10,112.66,303.75,393.33,10.91;10,112.66,317.30,118.05,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,274.56,290.20,207.11,10.91">Time masking for temporal language models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,303.75,393.33,10.91;10,112.66,317.30,29.78,10.91">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.33,10.91;10,112.66,344.40,393.54,10.91;10,112.66,357.95,277.01,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,237.43,330.85,268.56,10.91;10,112.66,344.40,229.77,10.91">Temporal adaptation of bert and performance on downstream document classification: Insights from social media</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Röttger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pierrehumbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,366.41,344.40,139.79,10.91;10,112.66,357.95,179.03,10.91">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2400" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,394.61,10.91;10,112.66,385.05,393.32,10.91;10,112.66,398.60,394.62,10.91;10,112.66,412.15,154.27,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,301.70,371.50,185.31,10.91">Dynamic contextualized word embeddings</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,385.05,393.32,10.91;10,112.66,398.60,343.97,10.91">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6970" to="6984" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,425.70,393.33,10.91;10,112.28,439.25,348.08,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,223.08,425.70,183.57,10.91">Temporal attention for language models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,436.05,425.70,69.93,10.91;10,112.28,439.25,250.09,10.91">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1498" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,393.33,10.91;10,112.66,466.34,394.61,10.91;10,112.66,479.89,244.15,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,254.09,452.79,251.90,10.91;10,112.66,466.34,238.82,10.91">The emojification of sentiment on social media: Collection and analysis of a longitudinal twitter sentiment dataset</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2108.13898" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,493.44,393.33,10.91;10,112.66,506.99,393.33,10.91;10,112.66,520.54,393.32,10.91;10,112.66,534.09,395.01,10.91;10,112.41,547.64,362.04,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,334.44,493.44,171.55,10.91;10,112.66,506.99,193.34,10.91">Cost-sensitive BERT for generalisable sentence classification on imbalanced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Tayyar</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Castelle</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5018</idno>
		<ptr target="https://aclanthology.org/D19-5018.doi:10.18653/v1/D19-5018" />
	</analytic>
	<monogr>
		<title level="m" coord="10,333.82,506.99,172.16,10.91;10,112.66,520.54,393.32,10.91;10,112.66,534.09,252.94,10.91">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, Association for Computational Linguistics</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,561.19,393.98,10.91;10,112.66,574.74,218.42,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,173.75,561.19,197.87,10.91">Individual comparisons by ranking methods</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/3001968" />
	</analytic>
	<monogr>
		<title level="j" coord="10,381.09,561.19,87.39,10.91">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,393.32,10.91;10,112.66,601.84,393.33,10.91;10,112.66,615.39,393.33,10.91;10,112.66,628.93,350.55,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,446.69,588.29,59.30,10.91;10,112.66,601.84,57.10,10.91">Twitter topic classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Antypas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.299" />
	</analytic>
	<monogr>
		<title level="m" coord="10,194.76,601.84,311.23,10.91;10,112.66,615.39,298.48,10.91">Proceedings of the 29th International Conference on Computational Linguistics, International Committee on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics, International Committee on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3386" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
