<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,355.70,15.42;1,89.29,106.66,406.46,15.42;1,89.29,128.58,173.07,15.43;1,89.29,150.91,220.08,11.96">SEUPD@CLEF: Team GWCA on Longitudinal Evaluation of IR Systems by Using Query Expansion and Learning To Rank Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,77.53,11.96"><forename type="first">Leonardo</forename><surname>Bellin</surname></persName>
							<email>leonardo.bellin@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.46,176.82,111.77,11.96"><forename type="first">Antonino</forename><forename type="middle">Andrea</forename><surname>Car√®</surname></persName>
							<email>antoninoandrea.care@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.88,176.82,70.66,11.96"><forename type="first">Marco</forename><surname>Martini</surname></persName>
							<email>marco.martini.7@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.18,176.82,92.90,11.96"><forename type="first">Maria</forename><forename type="middle">Teresa</forename><surname>Pepaj</surname></persName>
							<email>mariateresa.pepaj@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,190.76,81.20,11.96"><forename type="first">Matteo</forename><surname>Salvalaio</surname></persName>
							<email>matteo.salvalaio@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.14,190.76,70.19,11.96"><forename type="first">Andrea</forename><surname>Segala</surname></persName>
							<email>andrea.segala.3@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.97,190.76,91.42,11.96"><forename type="first">Mariafiore</forename><surname>Tognon</surname></persName>
							<email>mariafiore.tognon@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.39,190.76,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,355.70,15.42;1,89.29,106.66,406.46,15.42;1,89.29,128.58,173.07,15.43;1,89.29,150.91,220.08,11.96">SEUPD@CLEF: Team GWCA on Longitudinal Evaluation of IR Systems by Using Query Expansion and Learning To Rank Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">01ACA6171EFD7003351187F8C36136FF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LongEval 2023</term>
					<term>Information Retrieval</term>
					<term>Temporal Persistence</term>
					<term>GPT query expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the results and work done by team GWCA (University of Padua) on LongEval CLEF 2023 Lab in Task 1 (retrieval), that aims at evaluating the temporal persistence of information retrieval systems. After a careful analysis on the dataset given to train the system, the group decided to first try some common techniques to improve performance and then focus on those that produced better experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This report aims at providing a brief explanation of the Information Retrieval system built for Task 1 of LongEval lab 2023: LongEval Retrieval 1 . The task focuses on evaluating the temporal persistence of information retrieval systems and the goal is to propose a system which can handle changes over time. The task relies on a large set of data (corpus of pages, queries, user interaction) provided by a commercial search engine (Qwant 2 ) and it is designed to reflect the changes of the Web across time, by providing evolving document and query sets. The queries in the collection were collected from Qwant's users over several months and can thus be expected to reflect the changes in the search preferences of the users. The main idea of the GWCA group was to initially focus the work on the creation of an effective IR system and subsequently to support it with improvements aimed at increasing the system's performance and making it persistent as the document collection evolves. In particular, for this last objective, the group focused on Query Expansion (QE) techniques based on the use of GPT3.5 Turbo<ref type="foot" coords="2,150.78,125.86,3.71,7.97" target="#foot_0">3</ref> (hereafter only GPT or GPT3.5). The idea is that as web pages evolve over time based on the evolution of language and trends, GPT3.5 is a quite up-to-date model that dates back to 2021, and is hence equipped to deal with current language peculiar expressions and phrases. The paper is organized as follows: Section 3 describes the approach; Section 4 explains the experimental setup; Section 5 discusses the main findings and results; finally, Section 6 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>For the implementation of the system, the group relied on a basic structure provided by Professor Ferro during the aforementioned Search Engines course <ref type="foot" coords="2,339.10,279.33,3.71,7.97" target="#foot_1">4</ref> . The project contains basic tools for parsing, indexing and searching that were extensively used in the development. Additionally, it must be pointed out that inspiration for QE with Word2vec was drawn from classes developed by one of the previous years' groups from Search Engines course, specifically Team 6musk<ref type="foot" coords="2,481.20,319.98,3.71,7.97" target="#foot_2">5</ref>  <ref type="bibr" coords="2,487.86,321.73,11.28,10.91" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Workflow</head><p>Once a functioning pipeline was built, with respect to this year's task the main focus became the creation of analysis tools suitable for the LongEval WebSearch French collection, which constituted a basis to integrate effectively QE techniques into the system to improve performance. Specifically, queries were expanded either with synonyms provided by GPT or with word embeddings generated by model Word2vec. Furthermore, the usage of proximity queries as possible expansions was explored and LambdaMART re-ranking algorithm was integrated into the system. Finally, it is worth mentioning that, given the size of the corpus, multi-thread options were developed to speed up both the indexing and the searching process, achieving a better efficiency. In the following, the reasoning behind the techniques present in the final system is described, as well as the theoretical foundations behind the implemented solutions. Details about the implementation can be found at Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Base System</head><p>The basic pipeline has the following three components: a parser, an indexer and a searcher. As for the parsing, the choice fell upon the provided JSON format for the documents as it is a much more streamlined format than Trec and several libraries are available to manage such documents. As for the indexing phase, instead, documents' bodies and lengths were stored for each document in the collection: specifically, the latter parameter is exploited in the re-ranking phase. Furthermore, since most of the work was thread-safe, a multi-threading approach was implemented, which resulted in much more efficient indexing, as reported in Section 5. Finally, the search phase is carried out in a multi-threading implementation, once again to pursue the efficiency of the IR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Analysis and choice of the French collection</head><p>Since some of the query translations from French to English were imprecise (see, for example, q06229908: "chateaux a vendre" translated into "knives for sale") and given that the betterperforming English analyzer among the explored options was out-performed by the French one, most of the experiments were carried out on the French collection only. The main goal was to build tools for analysis that could deal with the specificity of the French language, such as frequent elisions and diacritical marks. Specifically, by observing the queries, it was possible to notice that Qwant users often wrote their queries without the proper accent marks and that this could affect precision if not correctly managed. Several off-the-shelves tools were tried on the data, as well as a custom made stoplist with words gathered among the most frequent of the French dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Query Expansion</head><p>As an attempt to improve performance, given the shortness of the provided web queries, different query expansion techniques <ref type="bibr" coords="3,215.70,388.86,12.75,10.91" target="#b2">[3]</ref> were tried out. A brief explanation of the explored approaches follows:</p><p>‚Ä¢ Proximity Queries: given queries were enriched with phrase queries in hope to boost precision. To allow for more flexibility, especially for longer queries, they were turned into proximity queries, with a slop (i.e. an edit distance) heuristically determined on the dataset and dependent on the length of the original query. Such phrases were used as additional boolean clauses, without replacing the single-term clauses derived from the parsing of the original query. ‚Ä¢ GPT generated expansions: the queries were provided to the neural generative model GPT asking for related words, instead of exact synonyms per se, as it was an approach proven to be effective even with earlier models <ref type="bibr" coords="3,321.30,534.67,11.28,10.91" target="#b3">[4]</ref>. The model was tuned with parameters from <ref type="bibr" coords="3,140.85,548.22,11.46,10.91" target="#b3">[4]</ref>, while the number of words added to the queries was heuristically determined.</p><p>To prevent drifting, the weight of GPT terms was lower than those of the words from the original query. ‚Ä¢ Word2vec word embeddings: Word2vec neural model <ref type="bibr" coords="3,381.00,590.23,13.00,10.91" target="#b4">[5]</ref> was used to to produce semantically similar words for the terms contained in the queries. Such model was trained over 20% of the documents of the collection and it retrieves 4 synonyms per word, provided that they reach a certain similarity threshold (fixed at 60%). The aforementioned numerical values were all heuristically determined.</p><p>In an initial phase, Wordnet also considered as a method for query expansion: yet, it was possible to notice that the generated synonyms didn't quite capture the actual meaning of words in the queries they were inserted into. As a consequence, this approach was discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Reranking</head><p>In order to push the relevant documents to the top of the ranking, the implementation of a reranking system was attempted after most of the relevant documents were retrieved. The model used was the LambdaMART model, which is a combination of LambdaRank and MART (Multiple Additive Regression Trees). The difference with the simple MART algorithm which uses gradient boosted decision trees is that LambdaMART introduces a cost function to order any ranking situation <ref type="foot" coords="4,186.81,229.38,3.71,7.97" target="#foot_3">6</ref> . Other re-ranking models were also considered, but the choice fell on LambdaMART, as its effectiveness in reranking queries taken from query logs of a commercial search engine, just like Qwant, was proven during 2010 Yahoo! Learning To Rank Challenge <ref type="bibr" coords="4,492.63,258.24,11.28,10.91" target="#b5">[6]</ref>.</p><p>In this case, re-ranking models were produced using a 2-fold or a 3-fold approach on the given train qrels, with increased performance (see Section 5) but a concrete risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>This section covers the implementation of the main classes developed in the project, together with those that were modified the most with respect to the aforementioned Hello-Tipster baseline. The whole project can be found at the repository linked at Section 4.3. It should be highlighted that execution is delegated to class Main.java, whereas project configuration and parameters are managed by ConfigManager.java.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">JSONLongEvalParser</head><p>The Abstract class DocumentParser was extended and adapted to JSON.</p><p>To accomplish this, the jackson.core<ref type="foot" coords="4,253.40,450.38,3.71,7.97" target="#foot_4">7</ref> and jackson.databind<ref type="foot" coords="4,353.72,450.38,3.71,7.97" target="#foot_5">8</ref> libraries, which are available on GitHub, were extensively used. With the jackson.core library, a JsonParser using JSONFactory was built and the first token, which is expected to be an array was examined. The jackson.databind library was then used to put the token into a JsonNode and an iterator was utilized to cycle through the elements of the Node. The Document Creation is handled by the class ParsedDocument. The documents present only two fields given the structure of the corpora:</p><p>‚Ä¢ ParsedDocument.FIELDS.ID which identifies uniquely the document ‚Ä¢ ParsedDocument.FIELDS.BODY the entire document text</p><p>This approach also improved up to 10% the performances reducing indexing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">LongEvalFrenchAnalyzer</head><p>This class fully implements the analyzer of the IR system. It filters the tokens with ICUFoldingFilter, which makes French phrases and their encoding more uniform. Afterwards the tokens go through a StopFilter, which loads an extended version of the stoplist from Oracle 9 . The extended stoplist contains the most frequent terms of the index found using VocabularyStatistics. Lastly, the tokens go through the FrenchLightStemFilter, which was found to be the best performing among other stemmers. In an initial phase of the development the group built a LowerCaseBrandFilter which lowercased the tokens unless they were in "brands.txt" file, which was taken from GitHub 10 , and then modified so that each brand has no blank spaces (i.e. it is a single token). However, since queries contained names of companies directly in lowercase letters, this approach was dismissed. This version of the analyzer was ultimately selected as the best after trying several stop lists and three different French stemmers:</p><p>FrenchStemFilter, as well as FrenchMinimalStemFilter and FrenchLightStemFilter from org.apache.lucene.analysis.fr.</p><p>Once the chosen analyzer was finalized, additional techniques were explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Vocabulary Statistics</head><p>To verify the validity of the index and to be able to analyze all the fields, the well-known Luke (Lucene Index Toolbox) tool was initially used, but later, for practical reasons, a VocabularyStatistics class was created, so as to obtain a file of text containing the terms contained in the index and some statistics relating to them. In particular, the class has a method for printing on a text file "vocabulary.txt" a dictionary containing all the terms of the index sorted in descending order according to their frequency. Thanks to the analysis of this file, an initial stop list "oracleFrench.txt" 11 was integrated by manually adding some very frequent terms, with the idea of reducing the size of the index and slightly improving performance. Later the class was modified so that it could automatically append the most frequent terms to the stop list, but it was found that the previous version of "oracleFrench.txt" was already very effective and that further extensions were counterproductive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Directory Indexer</head><p>This class is designed to index each individual document in a corpus after applying an analyzer. The implementation closely follows the one of the course Professor's one, but a few additions were made.</p><p>To improve efficiency, parallelization was integrated into this class. During the trial and 9 https://docs.oracle.com/en/database/oracle/oracle-database/21/ccref/oracle-text-supplied-stoplists.html# GUID-2F4889F1-B3A0-4639-95C6-FFD9C5276245 10 https://github.com/anna-hope/brandnames/blob/master/brands.txt 11 https://docs.oracle.com/en/database/oracle/oracle-database/21/ccref/oracle-text-supplied-stoplists.html# GUID-83740606-D05A-49F0-A6C3-0F2290B7DAB0</p><p>error process, it was found that the indexing of documents was the most computationally intensive task, especially when dealing with such a large corpus. By using a fixed thread pool ExecutorService from the java.util.concurrent library package, multithreading solutions were implemented to run tasks asynchronously without exceeding the specified number of threads. The parsed documents are still processed by the main thread, so assuming one has enough threads to handle the volume of documents, the main bottleneck will be caused by the storage speed.</p><p>Overall, this implementation of the class has improved the indexing process, making it faster and more efficient, while still maintaining thread-safety and ensuring the accuracy of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">GPT Query Expander</head><p>One of the approached techniques to expand queries was the usage of GPT3.5 Turbo. The main goal was to ask the model for a list of words related to the query and to create a .tsv file containing all the query expansions for time efficiency reasons. In a first attempt, the IR system was fed with both queries and expansions without performing any other processing. However, this resulted in a noticeable drop in performance, with Mean Average Precision (MAP) being more than halved. The benefits of Query Expansion became visible only after combination with proximity queries and suitable boosting.</p><p>The class GPTQueryExpander was implemented in both Java using the openai-java library provided by Theo Kanning<ref type="foot" coords="6,211.05,378.22,7.41,7.97" target="#foot_6">12</ref> and Python, but socket timeout errors were experienced in the Java implementation. This led the group to switch to the OpenAI official API <ref type="foot" coords="6,421.18,391.77,7.41,7.97" target="#foot_7">13</ref> in Python, which provided the same results but without any connection problems.</p><p>As mentioned before, some model parameters needed to be set: specifically, top_p=0.95, temper-ature=0.75 and max_tokens=300. The meaning of these parameters is reported below:</p><p>‚Ä¢ top_p: regulates the diversity of the text produced by the model: higher values lead to results that are similar to the text sequences on which the model has been trained. ‚Ä¢ temperature: controls the creativity of the model: higher values lead to more unpredictable results. Topic drift can be observed with really high values. ‚Ä¢ max_tokens: GPT was asked for 10 related words, so to be sure they were included a value of 300 was set.</p><p>The results provided by GPT were inseted into .tsv files because their format provides simpler processing. The chosen pipeline for query expansion involved reading a query from the query file, asking the model for the expansion returning a numbered list, gathering the contents of the list items into one single-line string, and writing only the expansion in the output file.</p><p>Characters that may cause problems, such as \n, \r and double quotes, were also removed. Additionally, regular expressions were used to remove numbers and separate the contents of the query expansions with spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6.">Word2Vec Filter</head><p>To achieve Word2vec embedding, the deeplearning4j library was used and a custom filter that extends the TokenFilter class in Lucene was created. The Word2VecFilter takes a token and retrieves the four nearest synonyms based on the trained model, with a threshold of 0.60 for the vector similarity. Furthermore, a duplicate check was added to avoid expanding duplicate terms in a query. The expanded terms are given an increment position of 0, so they are considered as "or" clauses in a Boolean Query.</p><p>To train the model, 20% of the corpus was used, but the 20 most frequent terms, as well as stopwords, were removed. The resulting model was designed to have 512 features for the vector representation of the synonyms and a window size of 10 words, as Word2vec models are trained on a window context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7.">ReRanker</head><p>A Learning To Rank system has been implemented via the RankLib-2.18 14 library, which generates a model from a training set that can be used to predict scores for unlabeled data.</p><p>The model was trained on pairs of document and query IDs, which were retrieved during the searching phase on the training data, along with their relevance assessment from the qrels serving as labels.</p><p>The model was trained using both 3-fold and 2-fold cross-validation techniques on all the retrieved pairs. In order to train the model, the features that were extracted for each document were: the BM25 score and the length of the document in terms of character.</p><p>To train the model the following command was used:</p><p>-train path\_to\_train/train.txt -ranker 6 -kcv 5 \ -kcvmd path\_models/ -kcvmn ca \ -metric2t NDCG@10 -metric2T NDCG@10 where:</p><p>‚Ä¢ train is the path to the training set which has the following format: in the comment section the document ID was added in order to reconstruct the final run at the end ‚Ä¢ ranker is the type of algorithm to use, 6 is the code for LambdaMART ‚Ä¢ kvc number of folds for kfold cross validation ‚Ä¢ metric2t the function used to train the model in this case was NDCG at 10 ‚Ä¢ metric2T the function used to measure the performance on the test set again NDCG at 10 at the end the command will save k number of folds different models. To use the model on the unlabeled data the following command was used:</p><note type="other">label</note><p>-load mymodel.ca -rank run\_with\_features.txt -score predicted\_scores.txt where:</p><p>‚Ä¢ load loads the model used ‚Ä¢ rank means rank the specified set of unlabeled data with the same format as the training set but with fictitious labels ‚Ä¢ score the file where the predicted scores will be saved.</p><p>The predicted scores are in the order of the given test set pairs, in the following format: query ID index Score qidq06223196 0 -0.8938 qidq06223196 1 -1.1028 ... ... ... so to produce the final run the ReRanker class has to match each predicted score to the query ID and Document ID and then order them in a decreasing way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.8.">Searcher</head><p>The final implementation modifies the Hello-Tipster searcher to work in a multi-threaded approach and enriches the original query with different clauses (methods shortQueryExpander and getPhraseQuery). Each BooleanQuery searched through the index can be made up of the following three components:</p><p>‚Ä¢ the original topic title, parsed with the analyzer. The clauses generated by this parsing were never removed in the runs. ‚Ä¢ a Lucene PhraseQuery, added as a Boolean clause. As mentioned in Section 3.1.3, such query is obtained from the topic title and is provided with a slop. Empirically, the slop values that lead to optimality upon the training dataset were determined as 5 and 16 respectively for queries shorter and longer than a certain threshold, heuristically fixed at 3 words. ‚Ä¢ query-related terms generated by GPT. Such expansions are wrapped inside a</p><p>BoostQuery and are assigned a heuristic weight of 0.1585. The amount of terms added depends on the length of the original query and currently a query is expanded until it is made up of 9 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Collection</head><p>This section is dedicated to briefly describing the datasets used both for training and developing the systems and for testing them.</p><p>For the LongEval CLEF 2023 Lab, the experimental collection was provided by Qwant:</p><p>‚Ä¢ Queries are extracted from Qwant's search logs, based on a set of selected topics. The initial set of extracted queries are filtered to exclude spam and queries returning a small number of documents. The query set was created in French and was automatically translated to English. ‚Ä¢ The document collection includes the relevant documents that are selected to be retrieved for each query, by extracting all the documents displayed in SERPs for the queries that were selected. In addition to these documents, potentially non-relevant documents are randomly sampled from Qwant index in order to better represent the nature of a Web test collection. ‚Ä¢ The relevance estimates are obtained through automatic collection of user implicit feedback through the usage of a click model, based on Dynamic Bayesian Networks trained on Qwant data. The output of the click model represents an attractiveness probability, which is turned into a 3-level scale score (0 = not relevant, 1 = relevant, 2 = highly relevant). To better contextualize some issues of this task it is necessary to make some observations on the relevance assessments. Assessing the relevance of a document through a click model is a biased estimation method. Indeed the probability that a web page is clicked is not strictly connected to its relevance, and this discrepancy has been noticed during a self-evaluation of the "train.txt" file containing the relevance estimates. For example, the document doc062200205332 containing the Wikipedia page of stepladder was judged not relevant for the query q062217010 'escabeau' (stepladder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Measures</head><p>To evaluate the systems, two main evaluation measures were used: MAP and Normalized Discounted Cumulated Gain (nDCG):</p><p>‚Ä¢ MAP was used since it provides a consistent and easily interpretable result, especially by comparing the Precision-Recall curves. ‚Ä¢ pure nDCG scores calculated are used to evaluate a single run, this is because it is consistent with Web search, for which the discount emphasises the ordering of the top results. ‚Ä¢ Relative nDCG Drop (RnD), measured by computing the difference between nDCG on within a time heldout test data vs short-or long-term testing sets. This measure captures the goal of the campaign of evaluating the temporal persistence of the systems' performances by assessing the impact of the data changes on the systems' results.</p><p>This report will only contain MAP and nDCG measures for all the collections. In order to get results for the runs tools like trec\_eval<ref type="foot" coords="10,284.13,98.76,7.41,7.97" target="#foot_8">15</ref> and Luke were used in order to easily retrieve information out of the index and check its integrity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Repository</head><p>To develop the project, the GWCA group made use extensively of git in order to collaborate and organize the files. To facilitate reproducibility, a link to the git repository can be found here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hardware</head><p>The hardware used for the runs is:</p><p>‚Ä¢ OS: GNU/Linux ‚Ä¢ CPU: Intel i9 9900k 8c16t 4.7Ghz 16 cores ‚Ä¢ RAM: 32GB 2133Mhz ‚Ä¢ SSD: Crucial P5 512GB</p><p>Note that the hardware used significantly impacts the temporal performance of the systems, and in particular it was possible to notice that even a slight change in the SSD could create a bottleneck and make the indexing phase up to 5 times slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Runs Description</head><p>In this section a short description of each produced run is provided, together with a name that will be later used to show performance results. All the runs were performed with BM25Similarity and make use of the French collection, the filters described in Section 3.2 and stoplist "oracleFrench.txt" unless differently specified. Here is a legend of the terms used in the runID to characterize the runs:</p><p>‚Ä¢ english: the run was carried out on the English collection ‚Ä¢ lovins: the run used Lovins stemmer ‚Ä¢ smartstop: the run used "smart.txt" stoplist for English ‚Ä¢ lightstem: the run used LightFrenchStemFilter ‚Ä¢ stem: the run used FrenchStemFilter ‚Ä¢ minstem: the run used FrenchMinimalStemFilter ‚Ä¢ nostem: the run didn't use stemming ‚Ä¢ word2vec: the run used query expansion with Word2vec</p><p>‚Ä¢ phrase: the run used phrase queries as described in Section 3.2.8</p><p>‚Ä¢ qexp: the run used GPT query expansions with weights as described in Section 3.2.8</p><p>‚Ä¢ stopexp: the run used an extended version of "oracleFrench.txt" enriched with frequent terms from the dataset ‚Ä¢ rerank2f: used a reranking model trained with 2-fold cross-validation ‚Ä¢ rerank3f: used a reranking model trained with 3-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on the training set</head><p>In the table below results for some of the carried-out runs are reported: indeed, it needs to be highlighted than several more runs were needed to tune parameters and are hence omitted. As noticeable, performances of the French analyzer are by far superior to those obtained using the English collection. Furthermore, phrase queries seem to succeed at improving Precision and reach their best performances paired with GPT query expansion. It is necessary to remark that the best results, obtained with the re-ranking algorithm, may be subject to overfitting, and the improvement is not to be expected to be as high on other tests. Furthermore, given the random split between train and validation data, results of the re-ranking process heavily depend on the folds created during the training phase.</p><p>The following table presents all the significant results gathered from some of the runs. In bold the names of the 5 submitted systems are highlighted, while bold results highlight the best performance throughout all the experiments, as well as the best without any reranking, since it possibly leads to overfitting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Efficiency of multi-threaded system</head><p>Below information about the time improvement derived from multithreaded indexing and searching system is provided. Reported values are means over 5 executions of run seupd2223gwca-lightstem-phrase-qexp.</p><p>It is noteworthy that these times were the ones obtained the setup described in Section 4, but trying the same procedure on another machine, with 20 cores, the time dropped further to 81s, suggesting that this procedure can be highly parallelized. As for Searching in this particular computer, the performance didn't improve at all as a single core was fast enough to process any queue on the fly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Heldout Data and Test Collections</head><p>In this section, results for the runs submitted to CLEF are provided: raw performance metrics are paired with statistical analysis so as to better compare the systems and the effectiveness of the different implemented strategies. Specifically, names of the runs are based on the conventions defined in Section 5.1, whereas in boxplots and multiple comparisons each run is identified by a number from one to five, reported beside its name in Tables 3, 5 and 7. It needs to be remarked that statistical testing (i.e., two-way ANOVA and Multiple Comparisons with Tukey's HSD test) was carried out only on nDCG, as the chosen evaluation measure for LongEval Lab, rather than on all the reported metrics. Finally, all tests were performed with a significance level ùõº = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Performance of the runs of Heldout data Run MAP nDCG P@10 R@1000 1 seupd2223-gwca-lightstem-phrase-qexp 0.2524 0.4294 0.1531 0.8908 3 seupd2223-gwca-lightstem-phrase 0.2303 0.4052 0.1418 0.9034 2 seupd2223-gwca-lightstem-phrase-qexp-rerank2f 0.2302 0.4059 0.1347 0.8908 4 seupd2223-gwca-lightstem-phrase-qexp-rerank3f 0.2099 0.3872 0.1378 0.8908 5 seupd2223-gwca-word2vec-nostem 0.2083 0.3843 0.1337 0.8720</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Training Heldout Data</head><p>As far as the training heldout data are concerned, it is noticeable from Table <ref type="table" coords="13,416.68,231.91,4.97,10.91">3</ref> that the re-ranking system did not succeed in producing a general model, also suitable for non-assessed documents, as the run without re-ranking performs significantly better than the -seemingly overfit-3-fold trained model. The absence of outliers in the boxplot Figure <ref type="figure" coords="13,289.69,286.11,8.26,10.91">2a</ref>, as well as the width of whiskers, can possibly be interpreted as the reflection of the great variability in performance dependent on the specific topics, also assessed by the correspondent high value for MS in Table <ref type="table" coords="13,403.47,313.21,3.78,10.91" target="#tab_3">4</ref>. Here, small p-values are evidence against the null hypothesis of the runs being all equivalent. This is noticeable from Figure <ref type="figure" coords="13,121.29,340.31,8.74,10.91">2b</ref>, where the best performing run is highlighted in blue and is statistically different from the red ones. Apparently, as for this dataset, GPT generated query expansions did not provide a statistically significant improvement in nDCG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Short-Term Persistence</head><p>As far as the Short-Term LongEval Test collection is concerned, it is noticeable from Table <ref type="table" coords="13,500.81,548.52,5.17,10.91">5</ref> that the pre-trained reranking models perform worse than all the other runs, in terms of MAP.</p><p>Similarly to the results on Heldout data, F-values and p-values are respectively quite high and quite low, which acts as a further evidence against the null hypothesis. This is noticeable from Figure <ref type="figure" coords="13,121.29,602.71,8.74,10.91" target="#fig_2">3b</ref>, where the best performing run is highlighted in blue and is statistically different from the red ones. Once again, query expansion via GPT3.5 does not seem to add a statistically significant impact on performance, if compared to run 2, i.e. the one with phrase queries only.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Long-Term Persistence</head><p>As for the Long-Term Persistence task, while the null hypothesis is once again discredited by the small p-values, differently from before, query expansion with GPT seems to significantly improve nDCG with respect to the baseline set by the run with phrase queries only, as visible in Figure <ref type="figure" coords="14,119.62,589.33,8.40,10.91" target="#fig_4">4b</ref>. Once again, Word2vec synonyms perform significantly worse in terms of nDCG than the two best runs and the pre-trained re-ranking model end up being harmful for performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Conclusions</head><p>To summarize, results upon the training set seem to suggest that query expansion can actually improve performance, but only if paired with techniques that improve precision, such as proximity queries. This was further confirmed by results upon the test collections. Furthermore, among the explored sources to gather synonyms for QE, GPT3.5 Turbo seems to provide the best related words, while Word2vec doesn't perform as well, possibly because of the very noisy documents the corpus was made up of. Finally, it is worth mentioning that LambdaMART with the 2-fold cross validation appears to increase performances only on the dataset the model is  trained upon, but possibly not as substantially as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Future Work</head><p>In a possible future work, some more attention should be put into the Learning To Rank system, trying to use more features to better capture the relationship between relevance and documents. This could make the pre-trained model more general and suitable for several different collections. Indeed, in this work very few features were used, so the model turned out to be quite inaccurate in a general context. A possible margin of improvement may come from a more careful tuning of parameters for GPT3.5, as here some of them were taken for granted as they were reported in Claveau <ref type="bibr" coords="16,478.99,436.45,11.43,10.91" target="#b3">[4]</ref>. Implementing a weighted score, incorporating standard scores such as BM25 and Dirichlet score and the one predicted by the re-ranking model, could also be explored: getting a fusion of different systems could indeed possibly mitigate the effect of reranking and overfitting. Finally, different options for parameters of the Word2vec model, which may have some impact on performance, could be explored, but special attention should be dedicated to make the generated synonyms less noisy, which seemed to be the problem with this specific trained model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,89.29,215.15,346.82,8.93;12,89.29,84.19,204.18,100.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical visualization for performance of the systems submitted to CLEF</figDesc><graphic coords="12,89.29,84.19,204.18,100.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,89.29,267.82,277.67,8.93;14,89.29,84.19,204.18,153.14"><head>Figure 2 : 5</head><label>25</label><figDesc>Figure 2: Graphs obtained from statistical analysis of Heldout data</figDesc><graphic coords="14,89.29,84.19,204.18,153.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,89.29,267.82,416.69,8.93;15,89.29,84.19,204.18,153.14"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graphs obtained from statistical analysis of nDCG in LongEval Short-Term Test sub-collection</figDesc><graphic coords="15,89.29,84.19,204.18,153.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,147.75,244.20,86.96,9.96;16,314.76,244.20,177.96,9.96"><head></head><label></label><figDesc>(a) Boxplot for nDCG (b) Multiple comparisons of systems' nDCG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,89.29,267.82,416.69,8.93;16,89.29,84.19,204.18,153.14"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graphs obtained from statistical analysis of nDCG in LongEval Long-Term Test sub-collection</figDesc><graphic coords="16,89.29,84.19,204.18,153.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,88.99,394.70,405.92,269.45"><head>Table 1</head><label>1</label><figDesc>Performance of the runs</figDesc><table coords="11,100.37,420.52,18.71,8.93"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,88.99,397.17,283.27,61.66"><head>Table 2</head><label>2</label><figDesc>Time performance of the runs</figDesc><table coords="12,223.02,425.21,149.24,33.62"><row><cell></cell><cell cols="2">16 threads 1 thread</cell></row><row><cell>Indexing</cell><cell>96.6 s</cell><cell>469.8 s</cell></row><row><cell>Searching</cell><cell>8 s</cell><cell>7.4 s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,88.99,407.39,406.58,86.37"><head>Table 4</head><label>4</label><figDesc>Two-way ANOVA results for nDCG on Heldout data</figDesc><table coords="13,99.71,435.43,395.87,58.33"><row><cell>Source</cell><cell cols="3">Sum of Squares Degrees of freedom Mean Squares</cell><cell>F</cell><cell>p-values</cell></row><row><cell>Systems</cell><cell>0.1282</cell><cell>4</cell><cell>0.0320</cell><cell>3.4136</cell><cell>0.0093</cell></row><row><cell>Topics</cell><cell>25.4205</cell><cell>97</cell><cell>0.2621</cell><cell cols="2">27.9137 1.2256e-127</cell></row><row><cell>Error</cell><cell>3.6427</cell><cell>388</cell><cell>0.0094</cell><cell>[]</cell><cell>[]</cell></row><row><cell>Total</cell><cell>29.1914</cell><cell>489</cell><cell>[]</cell><cell>[]</cell><cell>[]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,88.99,417.38,404.26,86.37"><head>Table 6</head><label>6</label><figDesc>Two-way ANOVA results for nDCG on Short-Term LongEval Collection</figDesc><table coords="14,102.02,445.42,391.23,58.33"><row><cell>Source</cell><cell cols="3">Sum of Squares Degrees of freedom Mean Squares</cell><cell>F</cell><cell>p-values</cell></row><row><cell>Systems</cell><cell>0.5982</cell><cell>4</cell><cell>0.1496</cell><cell cols="2">15.9982 5.4916e-13</cell></row><row><cell>Topics</cell><cell>244.0679</cell><cell>881</cell><cell>0.2770</cell><cell>29.6346</cell><cell>0</cell></row><row><cell>Error</cell><cell>32.9437</cell><cell>3524</cell><cell>0.0093</cell><cell>[]</cell><cell>[]</cell></row><row><cell>Total</cell><cell>277.6098</cell><cell>4409</cell><cell>[]</cell><cell>[]</cell><cell>[]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,88.99,419.60,404.26,86.37"><head>Table 8</head><label>8</label><figDesc>Two-way ANOVA results for nDCG on Long-Term LongEval Test sub-collection</figDesc><table coords="15,102.02,447.64,391.23,58.33"><row><cell>Source</cell><cell cols="3">Sum of Squares Degrees of freedom Mean Squares</cell><cell>F</cell><cell>p-values</cell></row><row><cell>Systems</cell><cell>0.5310</cell><cell>4</cell><cell>0.1328</cell><cell cols="2">15.7221 9.1886e-13</cell></row><row><cell>Topics</cell><cell>237.1325</cell><cell>921</cell><cell>0.2575</cell><cell>30.4909</cell><cell>0</cell></row><row><cell>Error</cell><cell>31.1087</cell><cell>3684</cell><cell>0.0084</cell><cell>[]</cell><cell>[]</cell></row><row><cell>Total</cell><cell>268.7722</cell><cell>4609</cell><cell>[]</cell><cell>[]</cell><cell>[]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,92.57,649.12,150.55,8.97"><p>https://platform.openai.com/docs/models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,92.57,660.08,225.88,8.97"><p>https://bitbucket.org/frrncl/se-unipd/src/master/hello-tipster/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,92.57,671.04,208.24,8.97"><p>https://bitbucket.org/upd-dei-stud-prj/seupd2122-6musk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="4,92.57,649.10,203.34,8.97"><p>https://www.educative.io/answers/what-is-lambdamart</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="4,92.57,660.06,160.62,8.97"><p>https://github.com/FasterXML/jackson-core</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="4,92.57,671.02,176.81,8.97"><p>https://github.com/FasterXML/jackson-databind</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6" coords="6,95.35,660.08,166.51,8.97"><p>https://github.com/TheoKanning/openai-java</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7" coords="6,95.35,671.04,173.22,8.97"><p>https://platform.openai.com/docs/api-reference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8" coords="10,95.35,671.04,141.83,8.97"><p>https://github.com/usnistgov/trec_eval</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="17,107.59,111.28,399.60,10.91;17,107.59,124.83,400.24,10.91;17,107.59,138.38,399.60,10.91;17,107.59,151.93,399.69,10.91;17,107.59,165.48,398.39,10.91;17,107.59,179.03,398.40,10.91;17,107.59,192.57,295.68,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,107.59,151.93,380.14,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuscakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,107.59,165.48,398.39,10.91;17,107.59,179.03,304.82,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="17,443.89,179.03,62.09,10.91;17,107.59,192.57,120.03,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,107.59,206.12,398.39,10.91;17,107.59,219.67,398.40,10.91;17,107.59,233.22,399.11,10.91;17,107.23,246.77,133.12,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,411.63,206.12,94.36,10.91;17,107.59,219.67,398.40,10.91;17,107.59,233.22,75.85,10.91">SEUPD@CLEF: Team 6musk on Argument Retrieval for Controversial Questions by Using Pairs Selection and Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mariotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosalen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-254.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,207.06,233.22,206.64,10.91">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,107.59,260.32,399.59,10.91;17,107.20,273.87,159.36,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,220.88,260.32,282.17,10.91">A Survey of Automatic Query Expansion in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,107.20,273.87,114.57,10.91">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,107.59,287.42,53.91,10.91;17,178.56,287.42,241.89,10.91;17,437.49,287.42,68.50,10.91;17,107.59,300.97,107.17,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Claveau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08787</idno>
		<title level="m" coord="17,178.56,287.42,237.86,10.91">Query expansion with artificially generated texts</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,107.59,314.52,398.40,10.91;17,107.59,328.07,398.39,10.91;17,107.59,341.62,317.73,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,345.44,314.52,160.55,10.91;17,107.59,328.07,170.41,10.91">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,300.97,328.07,205.01,10.91;17,107.59,341.62,241.54,10.91">NIPS&apos;13: Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,107.59,355.17,398.64,10.91;17,107.26,368.71,42.07,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,165.49,355.17,279.01,10.91">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,453.51,355.17,39.88,10.91">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
