<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,396.04,15.42;1,89.29,107.08,413.63,11.96;1,89.29,121.03,51.72,11.96">SEUPD@CLEF: Team Squid on LongEval-Retrieval Notebook for the LongEval Lab on Longitudinal Evaluation of Model Performance at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,146.93,79.55,11.96"><forename type="first">Vittorio</forename><surname>Cardillo</surname></persName>
							<email>vittorio.cardillo@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.06,146.93,76.86,11.96"><forename type="first">Alberto</forename><surname>Dorizza</surname></persName>
							<email>alberto.dorizza@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.53,146.93,67.17,11.96"><forename type="first">Mattia</forename><surname>Maglie</surname></persName>
							<email>mattia.maglie@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.32,146.93,66.45,11.96"><forename type="first">Dario</forename><surname>Mameli</surname></persName>
							<email>dario.mameli@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,429.39,146.93,71.22,11.96"><forename type="first">Gianluca</forename><surname>Rossi</surname></persName>
							<email>gianluca.rossi.4@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,160.88,70.37,11.96"><forename type="first">Michele</forename><surname>Russo</surname></persName>
							<email>michele.russo.2@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.66,160.88,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,396.04,15.42;1,89.29,107.08,413.63,11.96;1,89.29,121.03,51.72,11.96">SEUPD@CLEF: Team Squid on LongEval-Retrieval Notebook for the LongEval Lab on Longitudinal Evaluation of Model Performance at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5CD34E1F7590B524B185A91EB0DFF98B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Search Engine</term>
					<term>Information Retrieval</term>
					<term>Query Expansion</term>
					<term>Query Boost</term>
					<term>Word2Vec 1. Introduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the report based on the work done for LongEval Task 1: Retrieval at CLEF 2023, by team Squid (whose participants are from the University of Padua). Information retrieval (IR) systems have played an increasingly important role in our society and people's daily lives. Although they have become more and more powerful during the last decades, their temporal persistence is still causing drops in performance, thus failing to achieve good temporal generalisability. To investigate and improve the resolution of this issue, in this paper, we present and discuss the various solutions submitted to the first CLEF 2023 shared task (LongEval-Retrieval), which precisely requires the development of temporal information retrieval systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• Section 2: here we briefly explain the background knowledge and the starting code on which we built the systems.</p><p>• Section 3: here we describe the workflow of our IR system by examining its' modules and their functioning, delving deeply into the code that is the core of our systems.</p><p>• Section 4: here we explain our experimental setup, describing the data, the evaluation measures, the structure of the repository, and the hardware we used to run the system.</p><p>• Section 5: here we discuss our main findings and compare the different systems in terms of their performance with respect to the evaluation measures taken into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we describe the methodology we adopted, taking a closer look at the overlying architecture of our systems, and their workflows. The description we provide is general, and it examines all the components that make up our systems, which will all be reported in Section 5.</p><p>Starting from the powerful Java library Apache Lucene, we experimented with different applications of Natural Language Processing (NLP), Machine Learning, and Deep Learning, to enhance the systems' performances.</p><p>By examining a wide variety of libraries implemented both in Java and Python, we have constructed multifaceted systems consisting of both independent and interdependent modules, ensuring great versatility in usage.</p><p>All the systems' implementations follow a standard pipeline which can be summarized in the following way:</p><p>• Parser: process of parsing the HyperText Markup Language (HTML) documents</p><p>• Analyzer: the process of analyzing the parsed texts of the documents • Indexer: the process of indexing the documents' content, using the analyzer • Searcher: the process of searching the relevant documents at query time • Re-ranking: the process of ordering the documents depending on the relevance While Parser and Indexer follow the same pattern across all systems, Analyzer, and Searcher have been tested with different implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parser</head><p>In this subsection, we describe the pipeline for parsing the documents of the corpora provided by CLEF.</p><p>By manually inspecting the collection, we found that many documents could be well represented by their first words. Therefore, we added a field called title to the ParsedDocument, where we intended to put those first words, for each document. The other field is body, where we intended to put the whole parsed content of the document, and ID, where the document ID is stored.</p><p>The SquidParser is a modified version of the TipsterParser. The core functionality is therefore kept equal, but some tweaks were applied to take into account the expansion of the ParsedDocument class. SquidParser extends the DocumentParser which is an abstract class whose extensions implement the iterator interface over ParsedDocument type objects. This allows SquidParser to iteratively parse all documents in the collection and create a ParsedDocument object for each document.</p><p>In particular, each document is stripped of the HTML tags and the ID is stored as it is in the respective field. The remaining words are saved in the body field. The first 60 characters of the cleaned document are also saved in the title field, however, to avoid saving cut words at the 60th character, we allow some tolerance by saving the whole final word. This ParsedDocuments will be useful in the indexing phase, as will be explained in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analyzer</head><p>In this subsection, we discuss the analysis methods and instruments we used in order to find the best combination to process documents and queries. The analyzer is one of the most important parts of an Information Retrieval system, it concerns with pre-processing text to create a most fundamental token stream, that will be indexed. Analyzer has three main parts:</p><p>• Tokenizer that divides a sentence into smaller parts called tokens</p><p>• Stop Words removal that removes common words according to a stoplist</p><p>• Stemming that removes prefixes and suffixes by picking a common stem for a token Since the Analyzer is able to delete tokens (using a stop list) and modify tokens, little modifications can have a big impact in terms of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Document inspection</head><p>Inspecting the collection is the first step to creating a good Analyzer.</p><p>By looking into documents, we have seen that they do not have a "native" structure given by tags, which means that they do not have any title tag nor any other.</p><p>We provided to structure documents into the parser, by adding the first 60 characters of each document into the Title field. This is a good trade-off, because of the absence of a title tag we are not able to understand clearly when the title ends, but by an accurate inspection and looking at many runs, we have understood that 60 characters are enough to take only the title and have good performance.</p><p>Moreover, we thought it would be appropriate to insert a new field for the Uniform Resource Locator (URL) since all documents use them, and since we will then boost at query time all the documents that have a URL field whose tokens match with those of the query.</p><p>For this purpose, we used the "url.txt" file given by CLEF.</p><p>Finally, we changed the representation of characters when we open file documents to UTF-8, in order to correctly read French characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Stop lists</head><p>We tested the performance of the system with many stoplists in order to understand better how much stopword removal can change the performance. First of all, we tested all the stop lists mentioned during the lectures:</p><p>• atire.txt 1  After inspecting via Luke (Lucene's open-source tool used for inspecting and debugging Lucene indexes) the index generated without using any document pre-processing (e.g stoplist, stemmer, tokenizer), we created and tested two stoplists consisting of:</p><p>• the first 100 words with the highest frequency in the index • the first 200 words with the highest frequency in the index After seeing that performance with the English language did not increase, and since the given dataset also contains the original documents in French, we tried the following French stoplists:</p><p>• stoplist_fr_691.txt 10 -691 words • stoplist_fr_496.txt 11 -496 words • stoplist_fr_463.txt 12 -463 words • stoplist_fr_nltk.txt 13 -247 words After testing the standard French stoplists, we generated two more stoplists using index inspection with Luke, as we did for the English language:</p><p>• the first 100 French words with the highest frequency in the index • the first 200 French words with the highest frequency in the index</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">FrenchAnalyzer</head><p>The analyzer that we mostly used is the FrenchAnalyzer 14 in the org.apache.lucene.analysis.fr package of Lucene's standard library 15 .</p><p>This class is composed by:</p><p>• StandardTokenizer</p><p>• StandardFilter: normalizes the tokens from StandardTokenizer.</p><p>• ElisionFilter: removes elisions.</p><p>• LowerCaseFilter: sets tokens to lowercase.</p><p>• StopFilter: with Snowball's stopwords 16 .</p><p>• KeywordMarkerFilter: marks terms as keywords via the KeywordAttribute.</p><p>• SnowballFilter: a filter that stems words using a Snowball-generated stemmer.</p><p>actually it is the one which allows us to have the best performances</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Custom Analyzer</head><p>The Custom Analyzer developed is based on the French Analyzer of the Lucene library.</p><p>Implementation It uses the LetterTokenizer instead of the ClassicTokenizer of Lucene. Our developed Custom Analyzer uses both FrenchLightStemFilter and FrenchStemmer based on the SnowballFilter, where the latter attempts to remove common suffixes from words but may also introduce some inaccuracies or over-stemming in certain cases.</p><p>Our thought was that by using both, FrenchLightStemFilter and FrenchStemmer, we could achieve higher performance than the FrenchAnalyzer.</p><p>French Analyzer As reported in Section 3.2.3, the French Analyzer only uses the French-LightStemFilter to stem words. It aims to preserve more of the original word forms while still reducing words to a common base form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Indexer</head><p>In this subsection, we describe how the indexing is carried out. The index we build via Lucene's IndexWriter 17 is a traditional inverted index, which is used in most information retrieval systems. In particular, each entry is described by a field, and there are as many fields as in the ParsedDocuments, from which we want to extract the content.</p><p>The IndexWriter is also built with some specified configurations, which include an analyzer and a similarity measure. We tested different analyzers and similarities, whose combinations will be addressed in Section 5.</p><p>We build a Lucene document with fields ID, BodyField, TitleField, for each ParsedDocument, where we put the content as it is from the respective fields in the ParsedDocument.</p><p>For each Lucene document, we have decided to assign the respective URL as UrlField. To use them we have implemented the ParsedUrls class. This class has the task of parsing the URLs and storing them in a hashmap, separating each word that composes them by removing the special characters.</p><p>These Lucene documents are written by the IndexWriter in the actual inverted index in the following way:</p><p>• ID content as it is in the respective field.</p><p>• BodyField, TitleField, UrlField contents after processing by the specified analyzer, as tokens, in the respective fields.</p><p>For the fields BodyField and TitleField, IndexWriter is also tasked to save not only the references to the documents which contain each token of said field, but also the relative frequencies of the terms within each referenced document and the positions of said terms, to allow proximity-based search and phrase matching, which is then used at query time in the SearcherW2V class.</p><p>For the field UrlField, it only saves the reference to the document which contains each token and the relative term frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Searcher</head><p>In this subsection, we discuss the different implementations of the Searcher, following different approaches. These implementations have been used to produce the runs independently of each other, meaning, each one is part of one system only, as described in Section 4.4.</p><p>For reading the topics, the custom MyTopicsReader has been developed on the basis of the original TopicsReader, to account for the lack of narrative and description fields in the topics given by CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">BasicSearcher</head><p>Starting from the simplest Searcher, BasicSearcher is composed of the following steps:</p><p>• build one boolean query from the original query string via Lucene's QueryParser<ref type="foot" coords="8,480.01,128.30,7.41,7.97" target="#foot_0">18</ref> , for the body, title and URL fields of the index, using SHOULD clause.</p><p>• boost each query with a specific value depending on the field of the index to which it will be matched.</p><p>• concatenate the boosted queries into a single Query using SHOULD clause.</p><p>• search the documents using Lucene's IndexSearcher 19 , considering this final Query.</p><p>The process is obviously iterated for each query given in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">SearcherBoost</head><p>The SearcherBoost class has the goal to boost the terms of the given queries, using the Inverse Document Frequency (IDF) of each term. The Search method that performs the actual search of the given queries, uses four different components: printVocabularyStatistics, rareWords, boostQuery, and boost.</p><p>printVocabularyStatistics This method is responsible for scanning the index for computing the raw frequency and document frequency of each term. Each term is saved in a hashmap as an entry of &lt;String,long[]&gt;, where String is the term and the array long[] contains the raw frequency and document frequency. When the computation is done, the result is saved in a text file. Moreover, to perform the IDF, we save the term and a tuple containing the raw frequency and document frequency into a new hashmap; here we save only the terms that have a document frequency below a set threshold: this is necessary for collecting only the discriminating terms of the document collection.</p><p>rareWords rareWords method saves the term and its document frequency in the hashmap.</p><p>boostQuery boostQuery method is invoked for each input query. Every query is transformed into an array of String, then each String is converted into a Query and boosted based on its IDF value. In the end, the query terms are gathered together with the Boolean query using the Boolean clause SHOULD.</p><p>boost Here we compute the IDF boost using the IDF formula log 2 𝑁 𝑛 𝑖 , where N represents the number of documents in the collection and 𝑛 𝑖 represents the document frequency of that term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">SearcherW2V</head><p>Rationale The task of performing effective query expansion is not trivial. After an unsuccessful first draft using WordNet <ref type="bibr" coords="9,219.05,121.08,12.69,10.91" target="#b1">[2]</ref> for both the English-translated corpus and the French one, we were looking for alternative models. WordNet is a thesaurus, and as such, it has no knowledge about the collection. Therefore it generates synonyms that might be similar to the given word, but unrelated to the distribution of words in the collection or not present at all in it.</p><p>Given the better performances obtained with the French collection overall, and given the lack of pre-trained models for synonym expansion for the French language, we decided to look for a machine learning model that could be trained directly on the French collection. However, we wanted it to generate synonyms that are also "context-aware" in a sense, that is, that might be found close to the given word, and surely within the collection, with the ultimate goal of improving recall.</p><p>To this end, we came across Word2vec <ref type="bibr" coords="9,268.26,256.58,11.28,10.91" target="#b2">[3]</ref>, which we are going to use for synonyms expansion, as will be thoroughly explained in the Query Expansion paragraph <ref type="bibr" coords="9,386.24,270.13,25.31,10.91">(3.4.3)</ref>.</p><p>Training Word2vec has been implemented both in Java, under the Deeplearning4j library, and in Python as a Gensim Word2vec model. For training purposes, we used the Gensim Word2vec model, and the best parameters we have found are:</p><p>• sentences: set to a custom-defined sentence iterator which is able to iterate through all the files in the collection path, preprocessing each sentence by stripping tags, punctuation, and unnecessary symbols, and returning a list of tokens by analyzing the preprocessed sentence with the french tokenizer and french stoplist given by the Natural Language Toolkit (NLTK) python library.</p><p>• vector_size=300: dimensionality of the word vectors.</p><p>• window=8: maximum distance between the current and predicted word within a sentence. Bigger windows allow us to better represent sentences, however, due to stopwords removal, we thought we could afford to keep the size relatively low.</p><p>• min_count=10: ignores all words with total frequency lower than this. We found that roughly 90 percent of the words in the corpus have less than frequency 10 in the whole collection. This big reduction allows for better characterization of the remaining words, which still sum up to almost a million.</p><p>• sg=1: training algorithm: 1 for skip-gram; otherwise Continuous Bag of Words (CBOW).</p><p>• negative=20: if &gt; 0, negative sampling <ref type="bibr" coords="9,300.99,583.02,13.00,10.91" target="#b4">[5]</ref> will be used, the int for negative specifies how many "noise words" should be drawn (usually between 5-20). If set to 0, no negative sampling is used. It is basically used to change the weights of the network in an SGD-like fashion, depending on this number of observations.</p><p>• epochs=5: number of epochs of training over the corpus.</p><p>A significant contribution to the choice of the parameters was given by <ref type="bibr" coords="9,408.90,668.70,11.43,10.91" target="#b5">[6]</ref>.</p><p>Named Entity Recognition Named entities are words that represent real-world "objects", such as a person, location, organization, product, etc., that can be denoted with proper nouns. Since Named Entities refer to words of common interest we did not want to expand them by finding synonyms, so we provided an algorithm of NER in fr_core_news_sm.ipynb , in order to detect Named Entities in queries and insert them into a txt file.</p><p>In practice, we loaded a pre-trained model called fr_core_news_sm <ref type="foot" coords="10,398.90,152.96,7.41,7.97" target="#foot_1">20</ref> . This model is trained using the Le Monde dataset, which is a French text corpus of approximately 45 million tokens. The corpus contains news articles from the French newspaper Le Monde, with coverage ranging from 1987 to 2017. The corpus has been annotated with linguistic information such as parts of speech and named entities using the Universal Dependencies <ref type="bibr" coords="10,353.92,208.91,12.84,10.91" target="#b6">[7]</ref> annotation framework.</p><p>In practice, feeding the model with a query text will provide as output the name of the entities, if present, which we can then use in the following query expansion.</p><p>Another French NER we had looked into is Flair <ref type="bibr" coords="10,320.50,249.56,11.36,10.91" target="#b7">[8,</ref><ref type="bibr" coords="10,334.59,249.56,7.57,10.91" target="#b3">4]</ref>.</p><p>Flair is a NER model trained using a document from Wikipedia 2019, and it has many language implementations. The French Flair implementation fine-tunes the original one, which is a multi-language model, by using Le Monde and European Parliament Proceedings Parallel Corpus 1996-2011 <ref type="bibr" coords="10,134.40,303.75,11.43,10.91" target="#b8">[9]</ref>.</p><p>Both the implementations are valid, but after an accurate inspection of the results we preferred fr_core_news_sm, because Flair is computationally too expensive for our hardware.</p><p>Moreover, due to the fact that it starts as a multi-language model, it struggles to find some named entity, especially when all the words are lowercase.</p><p>Therefore, in order to exploit all its potential, it required better training, which we could not afford due to hardware limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query expansion</head><p>After training the Word2vec model we wanted to use it for the query expansion task of generating synonyms for specific terms of the query. We used the Deeplearning4j library to load the Word2vec model in SearcherW2V. From this point, we modified the original Tipster Searcher in the way queries are generated, by having for each topic a single final Query made by concatenating in OR the following queries:</p><p>• original query:</p><p>1. preparse the title of the topic (hereafter called original query string) by stripping punctuation and special characters.</p><p>2. build the Query by adding the tokens resulting from the application of Lucene's QueryParser on the original query string with the specified analyzer, for each field.</p><p>3. build the corresponding boolean queries and add them to the final Query with specific boosts dependent on the fields.</p><p>• synonyms queries:</p><p>1. elimination of entities found by the NER python program from the original query string.</p><p>2. analyze the remaining query string via Lucene's QueryParser and obtain a set of tokens, which represent all the terms of the original query of which we want to find synonyms.</p><p>3. for each token t: a) retrieves a specific number of synonyms using the Word2vec model. b) analyzes each synonym retrieved using the two QueryParsers for body and title.</p><p>c) uses the resulting tokens to build two separate SynonymQueries, one for each field, with the initial content of the token t, by adding all the other tokens with a weight resulting from the following weighting function:</p><p>model.similarity(t, syn)</p><p>which calculates the cosine similarity between the synonym and the token t.</p><p>NOTE: we use SynonymQueries because, as stated in the official Lucene's documentation:</p><p>"for scoring purposes, these queries try to score the terms as if you had indexed them as one term: it will match any of the terms but only invoke the similarity a single time, scoring the sum of all term frequencies for the document" <ref type="foot" coords="11,314.03,335.71,7.41,7.97" target="#foot_2">21</ref>We found these SynonymQueries to be very helpful for improving recall.</p><p>d) add the SynonymQueries to the final Query, with specific boosts applied depending on the fields.</p><p>• entity queries:</p><p>1. elimination of the terms to expand, found at the initial step of the process for generating synonym queries, from the original query string.</p><p>2. If the resulting query is not empty, then there must be entity terms. These terms are analyzed using the two QueryParsers for title and body, and the resulting tokens are used to form PhraseQueries 22 .</p><p>NOTE: we use PhraseQueries because they enable to build queries whose terms depend on the position of each other, such that a query like "New York" finds matches in the documents only if the two terms are found being no more distant from each other than a specific value.</p><p>3. Finally, add the two queries to the final Query with specific boosts dependent on the fields.</p><p>We now definitively build the final Query which is passed to the Lucene IndexSearcher to perform the actual search and ranking of the documents. Different similarities and analyzers were tested, as reported in Section 5. Finally, we produce the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4.">SearcherW2VRerank</head><p>Reranking methods refer to all processes that aim to improve the relevance and effectiveness of the results of an information retrieval system by reordering the documents initially retrieved based on certain criteria. Often reranking criteria can take into account user preferences and other customizations (e.g. browsing history, feedback, etc.), and at times, they may exploit (as in our case) query expansion. This technique consists of expanding the initial queries through additional terms or synonyms and given the broadening of the term spectrum, it can retrieve more relevant documents than the original queries.</p><p>We must however consider that, certainly, this expansion of the relevant documents (not returned by the initial query) does lead to an increase in the diversity of results and content, but this not only increases the computational complexity of the system but may also increase the noise of the initial query by distancing itself from the true meaning of the initial query with subsequent loss of information.</p><p>We have developed a reranking method based precisely on query expansion in which the key idea behind is a re-ordering of returned documents through the use of:</p><p>• synonymous queries • weighting proportional to the similarity value between the original word and synonym</p><p>• rank position of the document in the related query In detail, the searcher assigns a weight of 2.0 to the original query and a weight equal to the degree of similarity (returned by the previously trained Word2Vec model) between the word (non-entity and non-stopword of the original query) and the new synonym found to each generated synonymous query.</p><p>Subsequently, this weight plus the rank position of the relevant document are taken into account in the reranking phase, as it is explained in Section 3.4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query generation</head><p>The queries used by this searcher are of two types:</p><p>• original query: it is composed of the original term of the input query • synonym queries: these are constructed by adding the synonym of a word belonging to the original query that is not an entity or a stopword. In particular, the added synonym is the one with the highest similarity value returned by the Word2Vec model described in Section 3.4.3.</p><p>Before this implementation, we had tested the direct replacement of the word with its synonym (always replacing it if it was not an entity or a stopword), but the performance dropped, even when we replaced the word with multiple synonyms.</p><p>Reranking methodology All queries created by SearcherW2VRerank are passed to a reranking function that operates as follows:</p><p>1. Identifies the query with the highest weight in the array list of queries passed as input (always the original query in our case).</p><p>2. Saves the documents returned by that highest weighted query.</p><p>3. Scans all documents returned by all other synonym queries.</p><p>4. All documents returned by a query in the input list (different from the original query) that are also present in the documents returned by the query identified in 1. are reranked according to the following formula:</p><formula xml:id="formula_0" coords="13,146.87,260.53,359.77,20.57">final score = OScore + ∑︀ i∈all synonym queries (︁ CScore 𝑖 • ( maxDocsRetrieved -relativeRankPosition 𝑖 ) maxDocsRetrieved )︁<label>(1)</label></formula><p>where:</p><p>• OScore = score of the document in the result list of the highest weighted query (the original query)</p><p>• CScore 𝑖 = score of the document in the result list of the current query (any synonym queries in our case)</p><p>• maxDocsRetrieved = maximum number of documents returned per query (length of the list returned by the search method of Lucene's IndexSearcher)</p><p>• relativeRankPosition 𝑖 = position of the document in the list of documents returned by the current query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5.">SearcherAI using ChatGPT's API</head><p>Generative Pretrained Transformer (GPT) is a language model developed by OpenAI that can be used for natural language processing tasks such as text generation, summarization, and question-answering. We have used it for query expansion.</p><p>The GPT 2 model is publicly available but it is too heavy to use on our local machines. We then opted to use the GPT 3.5 APIs that Open AI makes available.</p><p>The idea is simple, using GPT APIs (via com.theokanning.openai java library<ref type="foot" coords="13,447.38,580.02,7.41,7.97" target="#foot_3">23</ref> ) we send a prompt to produce a list of n synonyms for a given word in a standard format.</p><p>The prompt we used was (asked in french to reduce language errors): "Énumérez n synonymes séparés par des point-virgule de: query. Je ne veux pas la définition. " which translates to: "List n synonyms separated by semicolon of: query. Do not want the definition". In this way, we get a list of synonyms separated by a semicolon. The "Do not want the definition" part was necessary because sometimes ChatGPT replies with a vocabulary definition of the query and not with the synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Example of the prompt used</head><p>We then find a synonym for each term of the query in a similar way we do in SearcherW2V 3.4.3 and add it to the final query as a SynonymQuery.</p><p>Sometimes ChatGPT does not understand correctly what is asked and could give wrong results, in order to clean up the results we perform on the output these tasks:</p><p>• remove non-alphanumeric symbols</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• repeat the prompt if the query parser goes in error</head><p>The pros of using GPT for query expansion are that it can improve the recall of the system by expanding queries with semantically related terms that may not have been included in the original query. This can help to retrieve more relevant documents that might have been missed by the original query. Also, the neural network can understand typos and "fix them".</p><p>The cons are that it can increase the computational cost of the system since it requires additional processing to generate the expanded queries. It can also introduce noise into the system if the expanded queries are not relevant to the user's information needs or if the model makes mistakes. Another problem is that it takes too long to search, requiring every query to make an API request. This would be solved with an offline implementation of GPT3 but it is not yet available to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In this section, we will describe the overall experimental setup which was used to run the system and produce the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data description</head><p>The data which was released by CLEF is divided into one training set and two test sets.</p><p>Training As stated by the organizers of the task <ref type="bibr" coords="15,330.41,201.32,16.25,10.91" target="#b9">[10]</ref>,</p><p>"The collection consists of queries and documents provided by the Qwant search Engine<ref type="foot" coords="15,148.38,233.85,7.41,7.97" target="#foot_4">24</ref> . The queries, which were issued by the users of Qwant, are based on the selected trending topics. The documents in the collection were selected with respect to these queries using the Qwant click model. Apart from the documents selected using this model, the collection also contains randomly selected documents from the Qwant index. All the data were collected over June 2022. In total, the collection contains 672 train queries, with corresponding 9656 assessments coming from the Qwant click model, and 98 heldout queries. The set of documents consists of 1,570,734 downloaded, cleaned, and filtered Web Pages. Apart from their original French versions, the collection also contains translations of the web pages and queries into English. The collection serves as the official training collection for the 2023 LongEval Information Retrieval Lab organized at CLEF. "</p><p>In addition, URLs to the documents were made available, allowing us to expand upon the indexing and searching capabilities of the system, as shown in 3.</p><p>More specifically, we focused our attention on the French part of the dataset, as we understood that documents and, more importantly, queries were translated from Fench, and it did cause a significant drop in the performances of the system when comparing the runs with the qrels for evaluation.</p><p>Testing For testing three datasets were used: the one for training, already discussed, and two more testing datasets. These collections were put together following the same process as the training set and they are composed in the following way: "The collection contains test datasets for two organized sub-tasks: short-term persistence (sub-task A) and long-term persistence (sub-task B). The data for the short-term persistence sub-task was collected over July 2022 and this dataset contains 1,593,376 documents and 882 queries. The data for the long-term persistence sub-task was collected over September 2022 and this dataset consists of 1,081,334 documents and 923 queries. Apart from the original French versions of the webpages and queries, the collection also contains their translations into English." <ref type="bibr" coords="15,116.56,629.35,17.91,10.91" target="#b10">[11]</ref> The previous considerations apply to the test sets as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation measures</head><p>In order to determine the effectiveness of our system and compare our solution we have decided to use four evaluation measures:</p><p>• Mean Average Precision (MAP) is calculated by averaging the precision scores for multiple queries. It measures the ability of the system to retrieve relevant documents by taking into account the number of relevant documents in the ranked list.</p><formula xml:id="formula_1" coords="16,243.56,197.49,263.08,34.56">𝑀 𝐴𝑃 = 1 𝑄 𝑄 ∑︁ 𝑞=1 1 𝑅𝐵 𝐾 ∑︁ 𝑘=1 𝑃 (𝑘)<label>(2)</label></formula><p>With query q, relevant document k and RB as the total number of relevant documents for the query q.</p><p>• Normalized Discounted Cumulated Gain (nDCG) is a measure that takes into account the relevance of the documents being returned, and normalizes the score based on the ideal ranking order.</p><p>𝑛𝐷𝐶𝐺@𝑘 = 𝐷𝐶𝐺@𝑘 𝐼𝐷𝐶𝐺@𝑘 (3)</p><p>• Recall: The recall measures how many relevant documents are retrieved by a system out of the total number of relevant documents in the search space. It is calculated as the ratio of relevant documents retrieved to the total number of relevant documents.</p><formula xml:id="formula_2" coords="16,187.38,400.51,319.26,24.43">𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑁 𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑠 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 𝑁 𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑠<label>(4)</label></formula><p>• Precision @10: This is a variant of the precision metric that evaluates the accuracy of a system by considering only the top 10 results. This means that it measures how well the system performs at returning 10 highly relevant documents.</p><p>𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛@10 = ∑︀ 10 𝑖=1 𝑟𝑖 10 (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Repository</head><p>The repository is organized as follows:</p><p>• code: this folder contains the source code of the developed system.</p><p>• homework-1: this folder contains the report describing the techniques applied and insights gained.</p><p>• homework-2: this folder contains the final paper submitted to CLEF.</p><p>• matlab: this folder contains two subfolders, respectively for the Matlab code of the homework-1 and homework-2.</p><p>• results: this folder contains the performance scores of the runs.</p><p>• runs: this folder contains the runs produced by the developed system.</p><p>• slides: this folder contains the slides used for presenting the conducted project.</p><p>The repository is available at https://bitbucket.org/upd-dei-stud-prj/seupd2223-squid/src/ master/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Systems</head><p>Different systems were composed by combining together the various components described in Section 3. All systems are based on the French collection. Their performances are shown in Section 5. Among these systems, we have used 5 different ones for producing the runs. These 5 systems are:</p><p>• SQUID_W2V: it corresponds to the configuration SearcherW2V, FrenchAnalyzer, BM25</p><p>• SQUID_BOOST: it corresponds to the configuration SearcherBoost, FrenchAnalyzer, BM25</p><p>• SQUID_W2VRerank: it corresponds to the configuration SearcherW2VRerank, FrenchAnalyzer, BM25</p><p>• SQUID_SEARCHERAI: it corresponds to the configuration SearcherAI, CustomAnalyzer, BM25</p><p>• SQUID_BasicSearcher: it corresponds to the configuration BasicSearcher, FrenchAnalyzer, • bodyBoost=1.5f : the boost of the original query to match with the body field • titleBoost=1.8f : the boost of the original query to match with the title field • urlBoost=1.1f : the boost of the original query to match with the url field and its search method takes as parameters:</p><p>• pathOfEntitiesFile : the path of the file that contains all entities of all queries</p><p>• stopListPathFile : the path of the file that contains all stopwords</p><p>• originalQueryWeight : the weight of the original query that will be used in document reranking</p><p>• numSynonymsToAdding : number of synonyms to be added to the query (for each non-entity or stopword term in the original query)</p><p>SearcherBoost:</p><p>• boost=IDF: the value to boost the term in a specific query. The boost is applied only if the term is "rare" in the document collection. The boost is the inverse document frequency of that term in the document collection.</p><p>• DOC_FREQUENCY_THRESHOLD=30 : below this threshold, the term is considered rare, then to boost using IDF.</p><p>• NUM_DOCS=1570734 : number of document in the collection; this value is used to calculate the IDF boost.</p><p>BasicSearcher:</p><p>• bodyBoost=1.0f: the boost of the original query to match with the body field • titleBoost=1.7f: the boost of the original query to match with the title field • urlBoost=1.1f: the boost of the original query to match with the url field </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>This section shows the results obtained by our systems, on the training and test sets, analyzed using the metrics described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Training data</head><p>For discussing the results of the training set, we will take into consideration various systems that were obtained by considering the most interesting combinations of the components described in Section 3.</p><p>The systems which were used to produce the runs are 5 among these combinations and have been reported specifically in Section 4.4.</p><p>The baseline run we will consider, made available by Prof. Petra Galuščáková, is BM25 Terrier fr reranked by T5, which is the best among the ones made available.</p><p>We have reported in Table <ref type="table" coords="19,220.80,452.75,5.10,10.91" target="#tab_1">1</ref> the results of all the systems we tested on the training dataset, including those used to produce the runs, applying the evaluation measures as described in Section 4.2.</p><p>In the system column we have used the following abbreviations:</p><formula xml:id="formula_3" coords="19,107.28,531.01,104.38,77.01">• FA: French Analyzer • CA: Custom Analyzer • BM25: Best Match 25 • CS: Classic Similarity</formula><p>From the results in Table <ref type="table" coords="19,210.59,621.17,3.66,10.91" target="#tab_1">1</ref>, we can see that the best system we have found is the combination of SearcherW2V, FrenchAnalyzer, and BM25Similarity. With respect to the baseline, we achieved:</p><p>• 19.54% improvement in MAP • 14.67% improvement in P@10 • 16.60% improvement in nDCG We can see that the MAP score drops using Custom Analyzer: this has led us to stick with the French Analyzer provided by the Lucene library, since the Custom Analyzer stems too much, leading to errors; however, we have seen that in some systems it performs better than the French Analyzer: this is possible thanks to the use of the LetterTokenizer, which separates words until it finds non-letter characters, as defined by the related documentation <ref type="foot" coords="20,453.41,184.44,7.41,7.97" target="#foot_5">25</ref> .</p><p>All across the board, however, we can denote a general performance boost with FrenchAnalyzer and BM25. The precision-recall curve shows that the better systems are the SearcherW2V-FA-BM25 and the SearcherAI-CA-BM25. The curve also shows how the slope is generally low. The slope measures the rate of change of precision for recall and indicates how much improvement the system provides in terms of precision for each additional relevant document retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on Test data</head><p>For discussing the results of the testing data, we will take into consideration only the 5 systems whose runs on the test sets were submitted. More details on these systems can be found in Section 4.4. In particular, as reported in the Table <ref type="table" coords="21,304.96,573.94,3.66,10.91" target="#tab_2">2</ref>, Table <ref type="table" coords="21,340.91,573.94,4.97,10.91" target="#tab_3">3</ref> and Table <ref type="table" coords="21,393.05,573.94,4.97,10.91" target="#tab_4">4</ref> the best system in terms of MAP, nDCG and P@10 is now SQUID_SEARCHERAI, while for Recall@1000 SQUID_W2V maintains its superiority.</p><p>Furthermore, in light of the findings in the LongEval overview paper <ref type="bibr" coords="21,409.88,614.59,11.45,10.91" target="#b0">[1]</ref>, it is worth noting that SQUID_SEARCHERAI has achieved top 3 performances on all the test sets. This serves only to stress once again the power of Large Language Models such as GPT 3 in the task of achieving temporal persistence. Extensive training on very large datasets, in fact, allows these models to perform very accurate query expansion, that would otherwise not be possible with For what concerns the temporal persistence of the submitted systems we can state that the overall performances of the test set collections (heldout, long, term) are just a little bit worse compared to the results of the training data: thus, it is possible to say that the capability of the submitted systems to retrieve and find relevant documents is maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Statistical Analysis</head><p>In this section, we conduct a statistical analysis of the performances of our systems based on the 5 runs submitted to CLEF. The analysis involves the use of tools such as box plots, two-way ANOVA and Tukey.</p><p>Each of the five systems, reported in Section 4.4, has been labeled in Table <ref type="table" coords="22,432.59,634.16,3.82,10.91" target="#tab_5">5</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Box Plots</head><p>Box plots are graphical tools to represent a distribution of data concisely. In our case, we want to plot the distribution of the scores achieved by our submitted systems on each query of the different test sets, with respect to the evaluation measures of Average Precision (AP) and nDCG. By further examination, we can denote a significant positive (right) skew of the distribution curve in the held-out box plots, indicating high asymmetry since the median value is smaller than the mean, and therefore is in clear contrast to the bell-shaped normal distribution curve. As for the short and long test sets, the skew is not as pronounced, probably due to the higher number of queries, which is able to better stabilize the median value on a more centered position.</p><p>Very few outliers are found across all plots, mainly in the system 3 plots. They are all found beyond the tail of the right whisker.</p><p>Though we can see that by the ordering of the plots in Figure <ref type="figure" coords="24,368.93,276.66,4.97,10.91" target="#fig_2">3</ref> system 1 has the best mean of the AP (that is MAP) parameter across all test sets, given the analysis provided, we can conclude that no significant difference between the 5 systems are detectable, indicating statistically comparable AP performances.</p><p>nDCG With respect to the previous plots, by examining those in Figure <ref type="figure" coords="24,420.87,346.06,3.74,10.91">4</ref>, we can see those medians are overall on the same level for all systems except system 3, which is lower. The same comments about dispersion previously made are valid in these plots as well.</p><p>Across the three test sets, again, we can see that system 3 has a slightly visible shift of the entire box plot towards the lower values of the performance scores compared to the other boxes, indicating somewhat worse performance.</p><p>Similar considerations as the previous plots can be made for the skew of the distribution curve.</p><p>No outliers are found in these plots.</p><p>Though we can see that by the ordering of the plots in Figure <ref type="figure" coords="24,376.59,468.00,5.08,10.91" target="#fig_2">3</ref> system 1 has the best mean of the nDCG parameter across all test sets, given the analysis provided, we can conclude that no significant difference between the 5 systems is detectable, indicating statistically comparable nDCG performances as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Two-way ANOVA</head><p>2-way ANOVA belongs to the class of General Linear Model (GLM), it is a statistical analysis technique used to examine the effects of two independent variables on a dependent variable. It assesses whether there are significant differences in the means of the dependent variable across the different levels of each independent variable, as well as any potential interaction between the two independent variables.</p><p>In our case, the four independent variables respectively refer to the three test sets of topics and the five submitted systems, while the dependent variable is the output measure: AP or nDCG for our tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Multiple Comparisons</head><p>In order to determine which specific groups differ significantly from each other we have used Tukey's Honest Significant Difference (HSD) test: an operation often used subsequently to ANOVA when the 𝐹 has revealed the existence of a significant difference between some of the tested groups. By an inspection of AP of Figure <ref type="figure" coords="27,275.79,148.18,5.08,10.91" target="#fig_3">5</ref> and nDCG of Figure <ref type="figure" coords="27,378.62,148.18,3.74,10.91" target="#fig_4">6</ref>, we can state that system 3 (in blue) is statistically different from the other which instead result really similar.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In conclusion, the goal of maximizing efficiency and effectiveness for our system has led us to the development of a wide variety and combinations of methods, which expanded upon the traditional Information Retrieval systems incorporating different tools from Machine Learning, Deep Learning, and Natural Language Processing fields.</p><p>While we were not able to finalize all the features we described, we still managed to put together a well-functioning pipeline, which can be tuned for different retrieval tasks, depending on the required application.</p><p>We managed to achieve good improvements with respect to the baseline, with the best system on the training set being, as already reported in Section 5.1, the configuration SearcherW2V, FrenchAnalyzer and BM25Similarity, named SQUID_W2V. On the test sets, however, both SQUID_W2V and SQUID_SEARCHERAI perform equally well, with the latter slightly surpassing the former on precision and nDCG, and vice versa for the other parameters, as reported in Section 5.2. It is also possible to see from Figure <ref type="figure" coords="28,254.09,665.21,5.17,10.91" target="#fig_3">5</ref> and Figure <ref type="figure" coords="28,313.09,665.21,3.81,10.91" target="#fig_4">6</ref>, that all systems except Squid_Boost (the blue one in the figures) are statistically similar to each other, across all three test sets collections. This shows the promise of the use of Neural Networks and Natural Language Processing tools to achieve state-of-the-art performances in the field of Information Retrieval.</p><p>While the systems are able to reach high values in the recall, we cannot really say that the precision, particularly P@10, achieves favorable performances.</p><p>We also verified that increasing the number of synonyms in SearcherW2VRerank is not helpful in terms of performance, probably due to the poor quality of synonyms generates noise inside the query, creating a query drift effect.</p><p>Moreover, for SearcherW2VRerank, we verified that changing the stop list does not change the performance so much.</p><p>The most promising feature that we believe could help in the precision department is the re-rank using machine learning, whose description is reported in Section 7.1.2, which is one of the features that we were not able to finalize due to our hardware limitations.</p><p>Overall, we believe that further work could potentially bring very satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Future Work</head><p>In this section, we want to discuss future improvements to our code and expand upon those features that were not able to finalize or implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Word2vec upgrade</head><p>While the Word2vec implementation did help improving the effectiveness of the system, especially in the recall part, we were not able to fully exploit its potential due to limitations on the hardware at our disposal, as already discussed.</p><p>Neural networks do indeed require much computational power for training, and we were only able to train for a very short number of epochs (5), compared to how it is really needed to achieve top-notch performances. More intensive training will most surely improve performance.</p><p>Furthermore, were pre-trained models specific for the French language made available, finetuning the given collection would be a much faster process than training from scratch, requiring less computational time to achieve a good fit, while also being more robust to changes in the collection itself, thanks to a bigger vocabulary with better-trained weights.</p><p>Entity recognition could also be improved, by better training the Flair model, as discussed in Section 3.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Re-ranking</head><p>It is possible to improve the system presented by using machine learning for the re-ranking phase. It was possible to verify its effectiveness only partially, due to limited hardware resources.</p><p>To simplify the computation, a point-wise approach was opted for, so each object (such as documents) is treated as an independent data point.</p><p>A linear regression model had been trained using a total of 32 features (TF, IDF, TF-IDF, DL, BM25, LMIR.ABS, LMIR.DIR and LMIR.JM for the document URL, title, body and the whole document). Given the run output provided by Lucene, each pair of queries and documents will be assigned a degree of relevance, and the final ranking will be adjusted. Our implementation of this model generates an accuracy on the test set of 64% but can be highly improved.</p><p>LSI algorithm can be introduced in order to speed up the algorithm and improve the robustness. In a nutshell, we create the document-term matrix that describes the occurrences of terms in documents.</p><p>The occurrence matrix X is decomposed with Singular Value Decomposition (SVD):</p><formula xml:id="formula_4" coords="30,272.77,348.83,233.87,10.91">𝑋 = 𝑈 Σ𝑉<label>(6)</label></formula><p>We take the k largest eigenvalues, which are also the most informative and reduce the matrix. Once we have the "reduced" matrix we can project into the new space the document d and query q.</p><p>The implementation of reranking algorithm is available in our repository<ref type="foot" coords="30,439.39,425.78,7.41,7.97" target="#foot_6">26</ref> as Learning-ToRank.ipynb. It is also available the code for LSI algorithm as LSI.py.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3.">Evolution of GPT</head><p>When GPT 3 (or a similar Generative AI) will be available to the open-source community we could use it on our local machine enabling us to summarize automatically the documents we want to index. In this way, we could remove useless information and could search in a faster and more efficient way. At the moment this approach is not feasible because the only way we can use GPT 3 is through the Open AI's API and it requires time and money. Also, with GPT 4 (available only through subscription<ref type="foot" coords="30,250.46,569.95,7.41,7.97" target="#foot_7">27</ref> ) we could improve even further thanks to its capability to summarize longer text 28 . Another possible evolution to the system would be to get synonyms of queries from entire phrases rather than singular words, we should study more this approach because of problems caused by non-relevant synonyms that cause lower precision. A possible solution could have</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="17,116.56,436.60,23.74,7.90;17,89.29,463.70,137.32,9.77;17,89.29,483.30,416.69,10.91;17,100.20,510.40,133.51,10.91;17,107.28,531.91,398.71,10.91;17,116.56,545.46,19.39,10.91;17,107.28,567.57,398.70,10.91;17,116.56,581.12,19.39,10.91;17,107.28,603.24,350.65,10.91;17,107.28,625.35,380.50,10.91;17,107.28,647.47,382.08,10.91;17,107.28,669.58,265.43,10.91;18,107.28,86.97,398.71,10.91;18,116.56,100.52,19.39,10.91;18,107.28,123.03,398.70,10.91;18,116.56,136.58,19.39,10.91;18,100.20,160.11,102.86,9.72"><head>BM25 4 . 4 . 1 .</head><label>441</label><figDesc>Searchers ParametersHere is how the parameters of the Searchers of the 5 systems have been set to produce the runs:SearcherW2V &amp; SearcherAI : • originalBodyBoost=1.5f : the boost of the original query to match with the body field • originalTitleBoost=1.8f : the boost of the original query to match with the title field • urlBoost=1.1f : the boost of the original query to match with the url field • entityBoostBody=2f : the boost of the entity query to match with the body field • entityBoostTitle=3f : the boost of the entity query to match with the title field • numSynonyms=15 : the number of synonyms to generate • synonymsBoostBody=1f : the boost of the synonyms query to match with the body field • synonymsBoostTitle=1f : the boost of the synonyms query to match with the title field SearcherW2VRerank:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="21,163.70,407.97,267.88,10.91;21,89.29,84.19,416.68,312.51"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: This is the precision-recall curve for the four runs</figDesc><graphic coords="21,89.29,84.19,416.68,312.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="23,89.29,612.73,416.68,10.91;23,203.88,450.89,187.51,150.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Box plots representations for the AP scores of the five systems on the three test sets</figDesc><graphic coords="23,203.88,450.89,187.51,150.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="27,234.79,502.90,125.69,10.91;27,195.55,338.51,204.18,153.13"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average Precision</figDesc><graphic coords="27,195.55,338.51,204.18,153.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="28,183.11,402.71,229.05,10.91;28,195.55,238.31,204.18,153.13"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Normalized Discounted Cumulative Gain</figDesc><graphic coords="28,195.55,238.31,204.18,153.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="19,95.14,85.49,405.01,161.71"><head>Table 1 :</head><label>1</label><figDesc>systems' performances</figDesc><table coords="19,95.14,110.12,405.01,137.09"><row><cell>Systems</cell><cell cols="4">Anal. Simil. Lang. MAP</cell><cell cols="2">P@10 nDCG R@1000</cell></row><row><cell>SearcherW2V</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2600 0.1524 0.4256</cell><cell>0.8754</cell></row><row><cell>SearcherW2V</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2589 0.1515 0.4211</cell><cell>0.8672</cell></row><row><cell>SearcherBoost</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2019 0.1223 0.3486</cell><cell>0.7778</cell></row><row><cell>SearcherBoost</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2010 0.1232 0.3482</cell><cell>0.7800</cell></row><row><cell>SearcherBoost</cell><cell>FA</cell><cell>CS</cell><cell>fr</cell><cell cols="2">0.2019 0.1223 0.3486</cell><cell>0.7778</cell></row><row><cell>SearcherW2VRerank</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2506 0.1488 0.4111</cell><cell>0.8443</cell></row><row><cell>SearcherAI</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2590 0.1537 0.4206</cell><cell>0.8639</cell></row><row><cell>SearcherAI</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2584 0.1530 0.4234</cell><cell>0.8726</cell></row><row><cell>BasicSearcher</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2519 0.1499 0.4107</cell><cell>0.8466</cell></row><row><cell>BM25 Terrier fr reranked by T5</cell><cell>-</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2175 0.1329 0.3650</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="22,95.67,85.49,401.61,104.56"><head>Table 2 :</head><label>2</label><figDesc>Systems performance on the heldout set</figDesc><table coords="22,95.67,110.34,401.61,79.71"><row><cell>Systems</cell><cell cols="4">Anal. Simil. Lang. MAP</cell><cell cols="2">P@10 nDCG R@1000</cell></row><row><cell>SQUID_BasicSearcher</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2522 0.1551 0.4149</cell><cell>0.8408</cell></row><row><cell>SQUID_BOOST</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2024 0.1255 0.3586</cell><cell>0.7822</cell></row><row><cell>SQUID_SEARCHERAI</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2594 0.1571 0.4279</cell><cell>0.8669</cell></row><row><cell>SQUID_W2V</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2583 0.1582 0.4232</cell><cell>0.8683</cell></row><row><cell>SQUID_W2VRerank</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2538 0.1531 0.4154</cell><cell>0.8246</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="22,95.67,205.04,401.61,104.56"><head>Table 3 :</head><label>3</label><figDesc>Systems performance on the short (July) test set</figDesc><table coords="22,95.67,229.89,401.61,79.71"><row><cell>Systems</cell><cell cols="4">Anal. Simil. Lang. MAP</cell><cell cols="2">P@10 nDCG R@1000</cell></row><row><cell>SQUID_BasicSearcher</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2439 0.1448 0.3998</cell><cell>0.8249</cell></row><row><cell>SQUID_BOOST</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2248 0.1366 0.3702</cell><cell>0.75</cell></row><row><cell>SQUID_SEARCHERAI</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2554 0.1494 0.4141</cell><cell>0.8441</cell></row><row><cell>SQUID_W2V</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2497 0.1486 0.4106</cell><cell>0.8514</cell></row><row><cell>SQUID_W2VRerank</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2440 0.1488 0.3997</cell><cell>0.8208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="22,89.29,324.60,416.69,153.45"><head>Table 4 :</head><label>4</label><figDesc>Systems performance on the long (September) test set</figDesc><table coords="22,89.29,349.44,416.69,128.61"><row><cell>Systems</cell><cell cols="4">Anal. Simil. Lang. MAP</cell><cell cols="3">P@10 nDCG R@1000</cell></row><row><cell>SQUID_BasicSearcher</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2425 0.1589</cell><cell>0.411</cell><cell>0.8441</cell></row><row><cell>SQUID_BOOST</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="2">0.2175 0.1445</cell><cell>0.374</cell><cell>0.7729</cell></row><row><cell>SQUID_SEARCHERAI</cell><cell>CA</cell><cell>BM25</cell><cell>fr</cell><cell cols="3">0.2473 0.1627 0.4177</cell><cell>0.8552</cell></row><row><cell>SQUID_W2V</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell cols="3">0.2444 0.1611 0.4174</cell><cell>0.8648</cell></row><row><cell>SQUID_W2VRerank</cell><cell>FA</cell><cell>BM25</cell><cell>fr</cell><cell>0.242</cell><cell cols="2">0.1569 0.4105</cell><cell>0.8444</cell></row><row><cell cols="8">the same proficiency using lexical databases or locally trained neural networks. Additionally, in</cell></row><row><cell cols="8">the latter case, overfitting phenomena are likely to occur, contrarily to LLMs such as GPT 3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="23,228.10,85.49,139.08,104.56"><head>Table 5 :</head><label>5</label><figDesc>Systems labeling</figDesc><table coords="23,228.10,110.34,139.08,79.71"><row><cell>Systems</cell><cell>Label</cell></row><row><cell>SQUID_SEARCHERAI</cell><cell>1</cell></row><row><cell>SQUID_W2V</cell><cell>2</cell></row><row><cell>SQUID_BOOST</cell><cell>3</cell></row><row><cell>SQUID_W2VRerank</cell><cell>4</cell></row><row><cell>SQUID_BasicSearcher</cell><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_0" coords="8,95.35,660.00,393.61,8.97"><p>https://lucene.apache.org/core/8_0_0/queryparser/org/apache/lucene/queryparser/classic/QueryParser.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_1" coords="10,95.35,671.02,93.63,8.97"><p>https://spacy.io/models/fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_2" coords="11,95.35,660.08,329.50,8.97"><p>https://lucene.apache.org/core/8_1_0/core/org/apache/lucene/search/SynonymQuery.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_3" coords="13,95.35,671.03,166.51,8.97"><p>https://github.com/TheoKanning/openai-java</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_4" coords="15,95.35,671.04,88.77,8.97"><p>https://www.qwant.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_5" coords="20,95.35,671.00,408.12,8.97"><p>https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_6" coords="30,95.35,649.12,248.02,8.97"><p>https://bitbucket.org/upd-dei-stud-prj/seupd2223-squid/src/master/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_7" coords="30,95.35,660.08,122.52,8.97"><p>https://openai.com/product/gpt-4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_8" coords="31,95.35,671.03,296.71,8.97"><p>https://towardsdatascience.com/what-gpt-4-brings-to-the-ai-table-74e392a32ac3</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="31,112.66,172.69,394.53,10.91;31,112.66,186.24,394.52,10.91;31,112.66,199.79,394.53,10.91;31,112.66,213.34,394.52,10.91;31,112.66,226.89,393.33,10.91;31,112.66,240.44,393.33,10.91;31,112.66,253.99,321.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="31,112.66,213.34,389.62,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuscakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,127.14,226.89,378.84,10.91;31,112.66,240.44,327.16,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="31,471.85,240.44,34.14,10.91;31,112.66,253.99,148.46,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaliniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,267.54,393.33,10.91;31,112.66,281.08,282.04,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="31,374.17,267.54,131.81,10.91;31,112.66,281.08,103.20,10.91">Introduction to wordnet: An on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1093/ijl/3.4.235</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,294.63,393.32,10.91;31,112.39,308.18,176.99,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="31,297.98,294.63,208.00,10.91;31,112.39,308.18,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,321.73,393.33,10.91;31,112.66,335.28,393.33,10.91;31,112.28,348.83,394.91,10.91;31,112.66,362.38,70.43,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="31,407.80,321.73,98.18,10.91;31,112.66,335.28,149.13,10.91">FLAIR: An easy-to-use framework for state-of-the-art NLP</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,283.85,335.28,222.14,10.91;31,112.28,348.83,390.09,10.91">NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,375.93,393.33,10.91;31,112.66,389.48,295.17,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m" coord="31,346.41,375.93,159.58,10.91;31,112.66,389.48,171.09,10.91">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,403.03,393.53,10.91;31,112.66,416.58,393.33,10.91;31,112.41,430.13,394.76,10.91;31,112.66,446.12,86.48,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="31,288.03,403.03,218.16,10.91;31,112.66,416.58,267.08,10.91">Word2vec: Optimal hyperparameters and their impact on natural language processing downstream tasks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="DOI">10.1515/comp-2022-0236</idno>
		<ptr target="https://doi.org/10.1515/comp-2022-0236.doi:doi:10.1515/comp-2022-0236" />
	</analytic>
	<monogr>
		<title level="j" coord="31,393.28,416.58,112.71,10.91">Open Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="134" to="141" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,457.22,394.52,10.91;31,112.66,470.77,393.71,10.91;31,112.66,484.32,393.33,10.91;31,112.66,497.87,393.33,10.91;31,112.33,511.42,394.93,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="31,368.90,470.77,137.47,10.91;31,112.66,484.32,137.50,10.91">Universal Dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/L16-1262" />
	</analytic>
	<monogr>
		<title level="m" coord="31,272.00,484.32,233.99,10.91;31,112.66,497.87,393.33,10.91;31,112.33,511.42,29.65,10.91">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16), European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16), European Language Resources Association (ELRA)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1659" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,524.97,394.53,10.91;31,112.66,538.52,395.01,10.91;31,112.41,552.07,48.96,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="31,265.70,524.97,237.20,10.91">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,127.31,538.52,334.29,10.91">COLING 2018, 27th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,565.62,393.32,10.91;31,112.66,579.17,394.61,10.91;31,112.31,592.72,202.19,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="31,158.61,565.62,270.66,10.91">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2005.mtsummit-papers.11" />
	</analytic>
	<monogr>
		<title level="m" coord="31,452.02,565.62,53.96,10.91;31,112.66,579.17,183.52,10.91">Proceedings of Machine Translation Summit X: Papers</title>
		<meeting>Machine Translation Summit X: Papers<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,112.66,606.27,394.53,10.91;31,112.66,619.81,394.53,10.91;31,112.66,633.36,393.33,10.91;31,112.33,646.91,286.84,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Devaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-5010,LINDAT/CLARIAH" />
		<title level="m" coord="31,163.81,619.81,117.90,10.91">LongEval train collection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="32,112.66,86.97,394.53,10.91;32,112.66,100.52,394.53,10.91;32,112.66,114.06,393.33,10.91;32,112.33,127.61,286.84,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Devaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-5139,LINDAT/CLARIAH" />
		<title level="m" coord="32,165.44,100.52,113.92,10.91">LongEval test collection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
