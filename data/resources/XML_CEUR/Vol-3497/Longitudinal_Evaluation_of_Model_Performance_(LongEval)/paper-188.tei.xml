<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.19,416.70,15.42;1,89.29,106.11,370.39,15.42;1,89.29,128.45,220.08,11.96">SEUPD@CLEF: Team FADERIC on A Query Expansion and Reranking Approach for the Longeval Task Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.35,86.42,11.96"><forename type="first">Enrico</forename><surname>Bolzonello</surname></persName>
							<email>enrico.bolzonello@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.36,154.35,96.80,11.96"><forename type="first">Christian</forename><surname>Marchiori</surname></persName>
							<email>christian.marchiori@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.80,154.35,91.03,11.96"><forename type="first">Daniele</forename><surname>Moschetta</surname></persName>
							<email>daniele.moschetta@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.48,154.35,88.15,11.96"><forename type="first">Riccardo</forename><surname>Trevisiol</surname></persName>
							<email>riccardo.trevisiol.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.30,61.53,11.96"><forename type="first">Fabio</forename><surname>Zanini</surname></persName>
							<email>fabio.zanini@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.82,168.30,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.19,416.70,15.42;1,89.29,106.11,370.39,15.42;1,89.29,128.45,220.08,11.96">SEUPD@CLEF: Team FADERIC on A Query Expansion and Reranking Approach for the Longeval Task Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CDB6888B49F53801CE2D1B7C2CAD774A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF</term>
					<term>LongEval</term>
					<term>Information retrieval</term>
					<term>Search engines</term>
					<term>Query expansion</term>
					<term>Reranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report explains and analyzes the system developed by Team FADERIC for the LongEval Lab at CLEF 2023, Task 1 -LongEval-Retrieval. The team members are all students following the Search Engines course a.y. 2022/23 at the Computer Engineering master degree at University of Padua. The system developed is a search engine that has to retrieve documents from a corpus, composed of original files in French language and automatically translated files in English language. The produced IR system exploits the query expansion technique, such as word N-grams and synonyms, and also the use of a reranking to improve the overall performance. Evaluating the longitudinal effectiveness of the system using the multiple collections provided by CLEF, we show that the performances remain satisfactory over time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Search engines have become an indispensable tool for people to retrieve various kinds of information in their daily routine. However, recent research has shown that Information Retrieval (IR) systems tend to perform poorly over time as the test data becomes more distant from the training data. This issue is particularly critical in the field of computer science, where data is constantly updated and information quickly becomes obsolete. Therefore, Conference and Labs of the Evaluation Forum (CLEF) 2023 LongEval task <ref type="bibr" coords="1,343.64,480.65,12.93,10.91" target="#b0">[1]</ref> has gained interest in evaluating the temporal persistence of IR systems. The aim of this report is to present the solution of Team FADERIC to this challenge. The team members are all students following the Search Engines course a.y. 2022/23 at the Computer Engineering master degree at University of Padua. The paper is organized as follows: Section 2 shows the related work we have started from; Section 3 describes our approach; Section 4 explains our experimental setup; Section 5 discusses our main findings; finally, Section 6 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To understand the task and the collections provided by CLEF we have used the paper by LongEval organizers Galuščáková et al. <ref type="bibr" coords="2,261.99,124.83,11.39,10.91" target="#b1">[2]</ref>. This has helped us to tackle the problem by figuring out how given documents and queries were collected and which were the main goals of the task. In the paper are also given some baseline performances that we have used to benchmark our system during its development.</p><p>We decided to exploit query expansion techniques basing our knowledge of the theme on the works from Carpineto and Romano <ref type="bibr" coords="2,267.01,192.57,12.84,10.91" target="#b2">[3]</ref> and Azad and Deepak <ref type="bibr" coords="2,382.98,192.57,11.44,10.91" target="#b3">[4]</ref>. We have decided to use various techniques, such as word N-grams and synonyms, which we have then tried during the development of our system.</p><p>Moreover, we choose to approach the problem by also using reranking. To do so we have used the work from Alkhalifa et al. <ref type="bibr" coords="2,246.42,246.77,11.42,10.91" target="#b4">[5]</ref>, which explains the problem of using a language model to address the longitudinal evaluation task. We build our reranking approach based on the work from Yilmaz et al. <ref type="bibr" coords="2,170.60,273.87,12.84,10.91" target="#b5">[6]</ref> who developed the Birch system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we describe the methodology we have adopted in order to develop an IR system for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parser</head><p>We manually inspected the documents provided by CLEF in order to understand their structure and be able to extract the body and ID of each document from them.</p><p>In order to do that, we created a tool called parser that has been essential for extracting information from documents in the specified format used by Text REtrieval Conference (TREC). The parser helps us create structured objects that are used for analysis and indexing within the IR system.</p><p>Here are the key components we implemented:</p><p>• ParsedDocument: represents a parsed document to be indexed. It has two attributes: ID for the unique identifier of the document and body for the document's content. This class provides functionalities to set and retrieve documents' attributes. • DocumentParser: represents an abstract class providing basic functionalities to iterate over the elements of a ParsedDocument, reading and parsing its content. • LongEvalDocumentParser: specific DocumentParser for the LongEval corpus.</p><p>It provides an implementation of a parser for the documents in the TREC format. The parser reads a document and returns a ParsedDocument that contains the ID and the body of the document.</p><p>We used the LongEvalDocumentParser and the ParsedDocument in the indexer to represent the content of the documents that are in the directory specified by docsDir. The first one has been used to iterate over the content of the specified document, while the second one has been used to represent a document to be indexed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analyzer</head><p>In order to process texts from documents and queries, we have implemented custom analyzers: since the collections are provided in both French and English language, two of them have been implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">French analyzer</head><p>The FrenchLongEvalAnalyzer component is in charge of processing French language texts, it is composed by:</p><p>• Tokenizing: the StandardTokenizer is used, which exploits the Word Break rules from Unicode Text Annex #29 <ref type="bibr" coords="3,252.58,233.37,11.56,10.91" target="#b6">[7]</ref>; • Character folding: the ICUFoldingFilter is used, which applies the foldings from Unicode Technical Report #30 <ref type="bibr" coords="3,254.54,261.77,11.58,10.91" target="#b7">[8]</ref>. This is useful to fold upper cases, accents and other kinds of complex characters; • Elision removal: the ElisionFilter is used, which removes the elision from words; • Stopword removal: the StopFilter is used, which removes the given stopwords from the tokens. In this system we have tried using the default Lucene<ref type="foot" coords="3,411.00,316.82,3.71,7.97" target="#foot_0">1</ref> stoplist and custom one, generated by picking the 50 most frequent terms in the documents; • Position filtering: a custom TokenFilter has been implemented to set the position-Increment attribute of all tokens to a specific value. This will be useful to ignore the positionIncrement due to removed stopwords in the search phase when we will use the proximity between tokens, as explained in Section 3.4.3; • Stemming: this process is useful to reduce words to their base form, in this system we have tried using the Snowball French <ref type="bibr" coords="3,284.98,416.03,12.84,10.91" target="#b8">[9]</ref> and Light <ref type="bibr" coords="3,345.91,416.03,17.91,10.91" target="#b9">[10]</ref> stemmers.</p><p>In Table <ref type="table" coords="3,138.91,438.39,5.07,10.91" target="#tab_0">1</ref> we show an example of the analyzing process for the French language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">English analyzer</head><p>The EnglishLongEvalAnalyzer component is in charge of processing English language texts, it is composed by:</p><p>• Tokenizing: the StandardTokenizer is used, which exploits the Word Break rules from Unicode Text Annex #29 <ref type="bibr" coords="3,252.58,537.13,11.56,10.91" target="#b6">[7]</ref>; • Character folding: the ICUFoldingFilter is used, which applies the foldings from Unicode Technical Report #30 <ref type="bibr" coords="3,254.54,565.53,11.58,10.91" target="#b7">[8]</ref>. This is useful to fold upper cases, accents and other kinds of complex characters; • Possessive removal: the EnglishPossessiveFilter is used, which removes the very frequent possessives ('s) from words; • Stopword removal: the StopFilter is used, which removes the given stopwords from the tokens. In this system we have tried using the default Lucene stoplist and custom one, generated by picking the 50 most frequent terms in the documents;  • Position filtering: a custom TokenFilter has been implemented to set the position-Increment attribute of all tokens to a specific value. This will be useful to ignore the positionIncrement due to removed stopwords in the search phase when we will use the proximity between tokens, as explained in Section 3.4.3; • Stemming: this process is useful to reduce words to their base form, in this system we have tried using the Snowball English (Porter2) <ref type="bibr" coords="4,329.94,583.40,17.91,10.91" target="#b10">[11]</ref> and the Krovetz <ref type="bibr" coords="4,423.99,583.40,17.91,10.91" target="#b11">[12]</ref> stemmers.</p><p>In Table <ref type="table" coords="4,138.91,605.91,5.07,10.91" target="#tab_1">2</ref> we show an example of the analyzing process for the English language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Indexer</head><p>Indexing is a crucial step where we create a searchable database, called index, for the parsed documents. The index contains important information about the documents, such as the words and phrases they contain, their frequency, and their location within the document. Indexing allows us to store the documents in a structured manner, which greatly speeds up the retrieval process by enabling users to search for documents based on keywords or phrases. To achieve this, we developed the following components:</p><p>• DirectoryIndexer: indexes the documents located in a specified directory and its sub-directories. It accepts various parameters, including the directory containing the documents to be indexed, the DocumentParser, the Analyzer, the Similarity to be used for indexing, the expected number of documents and the location where the index will be stored. Our code ensures that the document directory is readable and the index directory is writable before initiating the indexing process. Additionally, it keeps track of statistics, such as the number of indexed files and documents.</p><p>The main component of the class is the index method, which is in charge of performing the actual indexing of the documents. This method iterates through the documents in the directory, extracting their content and adding it to the index. During all the iterations, some statistics indexing is given, such as the time taken every 10 thousand documents. Finally, the index is closed. • BodyField: represents the body field of a document. This field has the following properties:</p><p>it is tokenized, meaning that the body is broken into words, or tokens, to make the search more accurate and flexible; frequencies and also the positions of the tokens are stored, in order to allow for phrase queries with proximity, as explained in Section 3.4.3; -the body content is stored, even if this had an impact on the index size, this was needed in the search phase in order to pass documents bodies to the reranker, as explained in Section 3.4.5.</p><p>In Table <ref type="table" coords="5,139.99,593.10,5.17,10.91" target="#tab_2">3</ref> are reported the index performances obtained using the analyzers described in Section 3.2 and the setup described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Searcher</head><p>In the searcher we have used a boolean query approach, in this way it was possible to create complex queries by combining, using the boolean operators, different components to be matched. The components we have used in our queries are explained from Section 3.4.1 to Section 3.4.5.</p><p>Note that in the following, the word N-grams component explained in Section 3.4.3 will be referenced also with the Lucene's jargon shingles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">BM25</head><p>Since the document collection has a significant size, as a base of our system we opted for a classic BM25 <ref type="bibr" coords="6,147.95,400.85,17.75,10.91" target="#b12">[13]</ref> approach due to the efficiency of it and higher effectiveness compared to other methods.</p><p>The Lucene implementation has default values 𝑘1 = 1.2 and 𝑏 = 0.75 which worked fine, but we decided to fine-tune the parameters to improve measures, in particular, Normalized Discounted Cumulated Gain (nDCG) since it is the relevant measure for the LongEval campaign. The results of our experiments are reported in Table <ref type="table" coords="6,324.22,468.60,3.74,10.91" target="#tab_3">4</ref>.</p><p>The best result was given by two pairs: 𝑘1 = 1.8, 𝑏 = 0.8 and 𝑘1 = 1.6, 𝑏 = 0.7; we chose pair 𝑘1 = 1.8, 𝑏 = 0.8 due to having the highest Mean Average Precision (MAP). Note that the performance gain was minimal, just 0.006 from the score with default parameters, which was expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Fuzzy</head><p>A fuzzy search, or approximate search, is a technique used to search for approximate or partial matches between a search term and documents in a collection of texts. Unlike exact search, in which the match must be exact and precise, fuzzy search allows you to find results even when the words you search for do not exactly match those in the documents. Fuzzy search is particularly useful when you want to get results even if there are misspellings, language variants, abbreviations or other forms of variations in the search terms or texts of the documents. For example, if you search for the term "roam" with a fuzzy search, the document containing the term "foam" might also be returned.</p><p>Fuzzy search techniques are based on the use of algorithms that evaluate the similarity between text strings. One of the most common algorithms used for fuzzy search is the Levenshtein algorithm <ref type="bibr" coords="7,136.52,114.06,16.39,10.91" target="#b13">[14]</ref>, which calculates the edit distance between two strings, that is, the minimum number of operations (character insertions, deletions and substitutions) required to transform one string into the other. Lucene, for example, uses a variant of the algorithm just described, the Damerau-Levenshtein algorithm, named after the Damerau algorithm <ref type="bibr" coords="7,393.66,154.71,16.09,10.91" target="#b14">[15]</ref>, which also considers character traspositions among its allowable operations.</p><p>Lucene also allows you to add an additional (optional) parameter with which to specify the maximum number of changes allowed. In our case we decided to set a manual threshold to choose the value of the parameter; if the word length is greater than or equal to the threshold then the fuzzy parameter is set to 2 otherwise 1 is used. The threshold is called "fuzzyThreshold" and can be set in the configuration file; we decided to set it to 10. Finally, to avoid performance degradation, in our IR system, fuzzy search is applied only if the query contains a single term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Word N-grams</head><p>Word N-grams are a sentence analysis technique that consists in dividing the words of a sentence into sequences of 𝑛 consecutive words. For example, the sentence "the dog barks, " can generate the N-grams "the dog" and "dog barks". Word N-grams are useful because they capture local relationships between words within a text, so this approach helps identify similar, though not identical, phrases and can improve search relevance. The maximum number of words within an N-gram can be set in the configuration file in maxShingleSize; in our case, we decided to use a maximum of 3 words. Also, we avoided generating unigrams as they do not capture any local relationships within the query.</p><p>We then decided to set up a proximity search within each N-gram. The proximity of terms can be used to identify documents in which the search terms occur in a certain spatial relationship. For example, if we are searching for the terms "dog" and "brown" with the proximity of 3 words, we want to find documents in which these two terms appear within a maximum distance of three words from each other. Thus, if a document contains sentences such as "I saw a brown dog in the park" or "The brown dog was running fast", these documents would be considered relevant because the terms "dog" and "brown" are close to each other. In our case, the proximity parameter is set to 5.</p><p>Finally, we applied a boost to all word N-grams based on the numer of generated N-grams for a certain query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4.">Synonyms</head><p>Managing synonyms in a retrieval system is not straightforward. The use of synonyms may not necessarily improve results, and this depends on how they are used.</p><p>Synonyms may be useful in the context of IR to broaden the search to include more related terms. However, the introduction of synonyms can also create problems such as noise in the information retrieval process. Here are some reasons why the use of synonyms may not improve results:</p><p>• Polysemy: words may have more than one meaning. Introducing synonyms might lead to an increase in irrelevant results if a synonym is associated with another meaning of the searched word. For example, if one searches for "bank" in the context of a financial bank, introducing the synonym "riverbank" could generate irrelevant results. • Irrelevant synonyms: not all synonyms are equally relevant in the context of a given query. Some synonyms might be too general or too specific concerning the user's intent, leading to inappropriate or insufficient results. • Redundancy: adding synonyms can lead to redundancy in the answers provided. If multiple synonymous words or phrases are used in the query, there may be significant overlap between the results, reducing the overall effectiveness of the IR system.</p><p>However, it should be kept in mind that the effectiveness of using synonyms in improving the results of the IR system also depends on the specific implementation and the characteristics of the domain or context in which the system is used. In some cases, the use or expansion of synonyms could actually improve the accuracy of information retrieval.</p><p>Furthermore, the use of synonyms can increase computational complexity in the information retrieval process. Because synonyms require accurate correspondence with indexed documents, additional computations must be performed to identify and compare matching synonyms in indexed texts.</p><p>We made several attempts to implement synonyms in our system; a summary description of what we did is given below.</p><p>Firstly, since handling synonyms in the index takes too much computation time, we decided to handle them directly in the search. We added synonyms in the queries with a query expansion approach.</p><p>In addition, we decided to use the WordNet<ref type="foot" coords="8,295.19,408.52,3.71,7.97" target="#foot_1">2</ref> dictionary. WordNet is a lexical database that groups English words into sets of synonyms called synsets, providing semantic relationships and definitions. It offers a comprehensive resource for natural language processing tasks, such as word sense disambiguation, information retrieval, and sentiment analysis. Since WordNet is written in C, it was necessary to use an additional API in order to use the dictionary on our system. That API is called extJWNL<ref type="foot" coords="8,248.96,476.27,3.71,7.97" target="#foot_2">3</ref> (Extended Java WordNet Library) and does not need the WordNet database installed locally.</p><p>Also, to improve dictionary lookup, we decided to use the OpenNLP<ref type="foot" coords="8,415.21,503.36,3.71,7.97" target="#foot_3">4</ref> library for natural language processing and limit polysemy. Each word in the original query was processed by an OpenNLP Part of Speech (PoS) Tagger in order to obtain the tag associated with the word. That function analyzes the context in which the word is used and returns the associated tag. The model used for the pos tagger was en-pos-maxent.bin and the tags are associated with WordNet section as shown in Table <ref type="table" coords="8,249.08,572.86,3.74,10.91" target="#tab_4">5</ref>. Knowing the tag of each word in a query made it possible to look up the word in the corresponding dictionary section. For example, for the query "free software," free was searched in the adjective section and software in the noun section. This strategy improved the metrics very little probably because the queries provided by Long Eval are very short, averaging 2/3 words. In addition, OpenNLP works well with properly formulated sentences, including consideration of capitalization and punctuation. In this case, queries are very crudely formulated, for example, some begin with a capital letter and some do not, as a result, OpenNLP does not always provide the correct tag. Then, this strategy might be more useful in the case of more complex queries, such as those characterizing a conversational IR system. Subsequently, we tried to give a different boost to each synonym. As a first approach, we decided to provide a boost based on the amount of synonyms returned. In this case, the boost was calculated in this way:</p><formula xml:id="formula_0" coords="9,226.08,339.80,280.56,24.43">𝑏𝑜𝑜𝑠𝑡 = 𝐵𝑜𝑜𝑠𝑡𝐵𝑎𝑠𝑒 𝑆𝑦𝑛𝑜𝑛𝑦𝑚𝐿𝑖𝑠𝑡𝐿𝑒𝑛𝑔𝑡ℎ<label>(1)</label></formula><p>This approach was used to limit the importance associated with each synonym if the returned synonym list is long, being more likely to get irrelevant synonyms. Finally, we moved synonym management, creating two new Analyzers: SynonymAnalyzer and SynonymPOSAnalyzer, which are applied only in the search part:</p><p>• SynonymAnalyzer: uses as input a query already previously analyzed with the standard Analyzer, i.e. EnglishLongEvalAnalyzer or FrenchLongEvalAnalyzer, after applies: SynonymGraphFilter, FlattenGraphFilter and StopFilter.</p><p>SynonymGraphFilter represents a filter that can be directly applied to a TokenStream within an Analyzer. The filter creates a synonym graph based on specified configurations and expands the terms found in the analyzed text by adding the corresponding synonyms to the token graph. FlattenGraphFilter converts an incoming graph token stream, such as one from SynonymGraphFilter, into a flat form so that all nodes form a single linear chain with no side paths. Every path through the graph touches every node. This is necessary when indexing a graph token stream because the index does not save PositionLengthAttribute and so it cannot preserve the graph structure. However, at search time, query parsers can correctly handle the graph and this token filter should not be used. This Analyzer uses a list of synonyms in .txt format, available in two versions: standard and custom. Before being processed by the Analyzer, the synonym list is transformed into a SynonymMap object via the AnalyzerUtil's loadSynonymList function. • SynonymPOSAnalyzer: takes as input EnglishLongEvalAnalyzer, then applies an OpenNLPPOSFilter, so that each word is assigned the associated tag. Then, it applies a custom filter called SynonymPOSFilter to manage the tags and look up words in the WordNet dictionary. Creating a custom filter was not trivial as the information about it in the documentation and online is very limited. The filter allows us to:</p><p>1. fetch the input tokens, 2. retrieve their associated tag, 3. search the synonyms in the dictionary based on their tag, 4. process the synonyms with the standard analyzer i.e. EnglishLongEvalAnalyzer 5. and return as output a TokenStream containing all the synonyms found.</p><p>The tokenStream returned as output by both Analyzers is then transformed into a list of strings which is in turn processed by the Searcher to apply a boost. Finally, the synonyms are added to the BooleanQuery along with the original query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5.">Reranker</head><p>After all of the previous steps, we obtained a working system that achieved satisfactory results at a good speed, so we tried to improve it by introducing a second stage, called passage re-ranking, in which each of the documents returned by the first retrieval model would be scored and re-ranked by a more computationally-intensive method involving Machine Learning. We achieved this by leveraging a library called PyGaggle <ref type="foot" coords="10,346.76,561.71,3.71,7.97" target="#foot_4">5</ref> , which provides some deep neural architectures for text ranking and question answering, and two transformer-based models, T5 and BERT, with different checkpoints and even our own trained checkpoint using code from another library <ref type="bibr" coords="10,159.49,604.11,16.25,10.91" target="#b15">[16]</ref>.</p><p>In our system, we can choose how many documents are to be reranked from the top for two reasons: the first is raw computing performance, reranking all 1000 retrieved documents takes a long time and we don't have machines powerful enough to handle it; the second is that we saw that reranking more than 50 documents lowers our measures, even below the baseline measure without reranking. Our first approach to reranking was to consider only the scores returned by the models to rerank the documents but we then switched to an approach where we consider also the BM25 score as follows. Let 𝑆𝑐𝑜𝑟𝑒 𝐵𝑀 25 (𝑖) the score given by BM25 for the document at rank 𝑖 and 𝑆𝑐𝑜𝑟𝑒 𝑟𝑟 (𝑖) the score given by the reranker for the document at rank 𝑖, and let 𝑛 be the total number of reranked documents, we define:</p><formula xml:id="formula_1" coords="11,151.12,189.85,355.52,26.48">𝑛𝑆𝑐𝑜𝑟𝑒 𝑟𝑟 (𝑖) = (︂ 𝑆𝑐𝑜𝑟𝑒 𝑟𝑟 (𝑖) + min 𝑗∈[1,𝑛] 𝑆𝑐𝑜𝑟𝑒(𝑗) )︂ • 𝑆𝑐𝑜𝑟𝑒 𝐵𝑀 25 (1) 𝑆𝑐𝑜𝑟𝑒 𝑟𝑟 (1)<label>(2)</label></formula><p>as the normalized score for the reranked documents, since the models returned a score in the range [-10, +10], which was not suitable for our case.</p><p>In our first approach, we simply passed the score to Lucene's ScoreDoc object and we were done. But in this way, we would lose information about the ranking given by BM25, which is still relevant, so we defined a new score:</p><formula xml:id="formula_2" coords="11,143.07,306.68,359.71,11.42">𝑓 𝑖𝑛𝑎𝑙𝑆𝑐𝑜𝑟𝑒(𝑖) = 𝑚𝑛𝑡𝑟 + (1 -𝛼) • 𝑆𝑐𝑜𝑟𝑒 𝐵𝑀 25 (𝑖) + 𝛼 • 𝑛𝑆𝑐𝑜𝑟𝑒 𝑟𝑟 (𝑖) (<label>3</label></formula><formula xml:id="formula_3" coords="11,502.78,306.68,3.86,10.91">)</formula><p>where 𝑚𝑛𝑡𝑟 is the maximum score from docs which are not reranked, in this way, we preserve the order of this docs. With this approach, we can give a weight to the reranker to find the balance and we do not lose the information given to us by the first stage. Note that 𝛼 = 1 corresponds to considering only scores from the reranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Models</head><p>We tried two transformer-based models, T5 and BERT since they are supported by PyGaggle. First, we tried T5 <ref type="bibr" coords="11,168.95,412.49,16.36,10.91" target="#b16">[17]</ref>, but with BERT <ref type="bibr" coords="11,261.53,412.49,18.02,10.91" target="#b17">[18]</ref> we got better results. Starting from the same base model, t5-base 6 for T5 bert-base-uncased 7 for BERT, we tried different checkpoints 8 fine-tuned specifically for reranking tasks and we even tried to train our checkpoint. The pre-trained checkpoints that we used are:</p><p>• monot5-base-msmarco-10k 9  • bert-base-mdoc-bm25 10   The results for the T5 model are reported in Table <ref type="table" coords="11,320.21,513.07,5.17,10.91" target="#tab_5">6</ref> and the results for the BERT model are reported in Table <ref type="table" coords="11,168.50,526.62,3.74,10.91">7</ref>.</p><p>The BERT pretrained checkpoint with 20 reranked documents improved nDCG by 3.56% and MAP by 8.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training our own checkpoint</head><p>At this point, BERT gave us good results so we took it a step further and we tried to find ways to finetune it to our data. The training process is pretty straightforward: • Data pre-processing. Transformers expect batches of tensors as input, so we need to preprocess our data to the expected format. For processing textual data the tokenizer tool is used, which splits text into tokens; in our case, we exploit the pre-trained BERT tokenizer which returns tokens that are not necessarily words, but rather subwords: frequently used words are (or should) not split into smaller subwords, but rare words should be decomposed into meaningful subwords <ref type="bibr" coords="12,345.04,438.02,16.41,10.91" target="#b18">[19]</ref>. Further, the tokenizer adds, at the beginning and at the end, two special tokens, respectively <ref type="bibr" coords="12,401.00,454.01,29.67,7.90">[CLS]</ref> and <ref type="bibr" coords="12,453.31,454.01,26.77,7.90">[SEP]</ref>. The tokenizer returns a dictionary with three items:</p><p>-input_ids, indices corresponding to each token in the sentence -attention_mask, indicates whether a token should be attended to or not -token_type_ids, identifies which sequence a token belongs to when there is more than one sequence An important note is that BERT accepts input sequences of up to 512 tokens, so the tokenizer truncates longer documents. • Training. The easiest way to train is to use the Trainer<ref type="foot" coords="12,363.95,571.04,7.41,7.97" target="#foot_5">11</ref> API from PyTorch.</p><p>To ease development, we used a package for training deep language model rerankers <ref type="bibr" coords="12,468.95,595.22,17.90,10.91" target="#b15">[16]</ref> and adapted the example code to our collection.</p><p>The convert_to_training.py takes care of converting data to the training file, given the ranking file, the qrels, the query collection and the docs collection. Then it is sufficient to use the trainer.py code to get the trained model. Unfortunately, we don't have access to sufficiently powerful machines so we were forced to train on a Google Colab notebook. This came with a major drawback: the maximum runtime is 12 hours, so we couldn't train on more than 1 epoch since a 2 epoch model was estimated to take 15 hours to train. This obviously tanked our model performances, and, as we can see in Table <ref type="table" coords="13,359.45,249.40,3.74,10.91" target="#tab_6">8</ref>.</p><p>The Colab notebook with all the hyperparameters can be found at https://colab.research.google. com/drive/1oFeYSkR31A-MUibwWrfNcKLUyPxEYbOq?usp=sharing. The trained model can be found at https://huggingface.co/enricobolzonello/clef_longeval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrating with Lucene</head><p>One problem that emerged while working with the reranker was integrating it with Lucene since the reranker is written in Python and our main program is in Java. To solve the issue we came up with three approaches:</p><p>1. passing intermediate text files, with documents, query to the reranker and returning the ranking to Java. This solution was used for initial testing but was deemed too inefficient and prone to errors 2. using Python as the system's entry point and using the PyJNIus<ref type="foot" coords="13,401.30,404.82,7.41,7.97" target="#foot_6">12</ref> library to access Java classes. The same approach was used by Birch <ref type="bibr" coords="13,336.83,420.12,11.58,10.91" target="#b5">[6]</ref>, but we should have changed the classes too much to integrate tightly the reranker and more importantly we didn't want to change the entry point of our system 3. our final solution was to call in some way Python from Java</p><p>The library we used to achieve this is JEP <ref type="foot" coords="13,280.75,480.69,7.41,7.97" target="#foot_7">13</ref> , which uses Java Native Interface (JNI) and the CPython API to start up the Python interpreter inside the Java Virtual Machine (JVM). Thanks to the Python interpreter, we can call, at each query, the reranker which returns a list of generic Objects that is converted to a list of Float.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>The experimental setup of this project consists of the following:</p><p>• The project is available at https://bitbucket.org/upd-dei-stud-prj/seupd2223-faderic/src/ master/; • The collections are taken from https://clef-longeval.github.io/data/;</p><p>• The evaluation tool used is trec_eval v9.0.7, available https://trec.nist.gov/trec_eval/; • To compute the runs we have used the following hardware: CPU AMD Ryzen 7 2700, GPU Zotac GeForce RTX 2060 6 GB, RAM 16 GB DDR4;</p><p>In order to reproduce the runs for this system, it is necessary to follow the instructions given in the project's ReadMe file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this Section we will show and analyze the results obtained by our system. In particular, Section 5.1 will be about the results of all the runs produced by our system on the training collection during its development, while Section 5.2 will consist of a statistical analysis of the runs submitted to CLEF on the test collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training results</head><p>We have combined the different components explained in Section 3 and we conducted a thorough experimentation process to identify the best-performing system for our task. By tuning the parameters of each component and selecting the optimal ones, we were able to build a system that addresses the persistence issue of IR systems. In Table <ref type="table" coords="14,351.82,341.13,5.05,10.91" target="#tab_7">9</ref> are reported the MAP and nDCG scores obtained on those systems.</p><p>The keywords reported in the names of the runs have the following meanings:</p><p>• French: used documents and queries from the French collection • English: used documents and queries from the English collection  During the tuning of our system, we primarily focused on MAP and nDCG. However, in order to perform a more comprehensive analysis, we also considered additional measures such as Precision and Recall: the first one is the fraction of retrieved documents that are relevant to the user's query, indicating a measure of the accuracy of the system, while with the second is a measure of the completeness of the system in retrieving all relevant results, computed by the fraction of relevant documents retrieved.</p><p>In Figure <ref type="figure" coords="15,142.13,453.63,3.66,10.91" target="#fig_2">2</ref>, we show the interpolated Precision-Recall curve, which can be useful to show the inverse relationship between Precision and Recall, indicating the trade-off between these two measures. In Table <ref type="table" coords="16,138.88,462.13,10.14,10.91" target="#tab_0">10</ref> and in Table <ref type="table" coords="16,209.28,462.13,10.14,10.91" target="#tab_0">11</ref> we have reported a more complete list of scores respectively for the French and English runs. For space reasons, we label the runs as:     Observing the nDCG and AP boxplots, shown in Figure <ref type="figure" coords="22,364.73,520.73,3.81,10.91" target="#fig_8">7</ref>, we can notice that the runs performed on the French collection have a similar structure in terms of median values and interquartile ranges. We can also notice that in the AP boxplot the fr_1 run has a longer whisker, while the others show the presence of outliers.</p><p>From the ANOVA2 analysis, which results are reported in Table <ref type="table" coords="22,375.00,574.92,8.20,10.91" target="#tab_4">15</ref>, we can see that 𝑝-𝑣𝑎𝑙𝑢𝑒 &lt; 𝛼, therefore we can reject the null hypothesis. Moreover, from Tukey's HSD multiple comparison shown in Figure <ref type="figure" coords="22,165.75,602.02,3.81,10.91" target="#fig_10">8</ref>, we can derive that runs fr_3, fr_4 can be considered similar, while all the other runs differ from each other.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Long term</head><p>Since some error occurred in the indexing phase when performing the submitted runs fr_1 and fr_2, in the following analysis we will use a fixed version of those runs.</p><p>In Table <ref type="table" coords="25,140.09,294.51,10.35,10.91" target="#tab_10">16</ref> are reported the nDCG and MAP values obtained from the submitted runs on the long term collection, while in Figure <ref type="figure" coords="25,272.21,308.06,5.11,10.91" target="#fig_11">9</ref> is reported the interpolated Precision-Recall curve. Comparing these results with the ones obtained on the short term, shown in Table <ref type="table" coords="25,441.41,321.61,9.94,10.91" target="#tab_9">14</ref> and Figure <ref type="figure" coords="25,501.01,321.61,4.97,10.91" target="#fig_7">6</ref> respectively, we can see that every run suffered a performance drop over all the measures. This worsening was expected and it can be due to the fact that the performances tend to drop over time, however, the decrease is not huge and we can consider the performances of the system to remain satisfactory.</p><p>Observing the nDCG and AP boxplots, shown in Figure <ref type="figure" coords="25,361.64,389.35,8.53,10.91" target="#fig_12">10</ref>, we can notice that the runs performed on the French collection have a similar structure in terms of median values and interquartile ranges. We can also notice that in the AP boxplot all the runs show the presence of outliers.</p><p>From the ANOVA2 analysis, which results are reported in Table <ref type="table" coords="25,372.88,443.55,8.20,10.91" target="#tab_11">17</ref>, we can see that we obtained very different results for the two measures. In particular, on nDCG we have 𝑝-𝑣𝑎𝑙𝑢𝑒 &gt; 𝛼, which means we cannot reject the null hypothesis, while on AP we have 𝑝-𝑣𝑎𝑙𝑢𝑒 &lt; 𝛼, which means we can reject the null hypothesis. The same behavior is reflected in Tukey's HSD multiple comparison, shown in Figure <ref type="figure" coords="25,224.11,497.75,8.53,10.91" target="#fig_13">11</ref>, where on nDCG we can derive that the French runs can be considered to be similar to each other, while on AP we can see that runs fr_1, fr_2, fr_3 and runs fr_3, fr_4 can be considered similar to each other.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>From the obtained results, we can say that the system has kept satisfactory performance from the longitudinal evaluation point of view. In fact, on the short term, there has been no significant worsening, while on the long term, there has been just a moderate performance drop.</p><p>During the developing of the system we understood that query expansion and reranking play a major role in the IR systems. Those two features granted us the biggest performance improvements. Although the system has reached decent scores, our work could be further developed in different ways.</p><p>With respect to query expansion, we could improve the synonyms feature by using other dictionaries, since WordNet is quite outdated and a big portion of the queries were regarding very recent topics, and also by using better French dictionaries, since there are only a few available when working with languages other than English and we had to customize our own. Another query expansion option could be switching from simple dictionaries to neural models, in order to expand a query with words related not just to the meaning of the single words but also to the context of the whole topic the user is looking for.</p><p>Regarding the reranker, there are some things we could improve, mainly focusing on the trained model. As discussed in Section 3.4.5, we did not have the required hardware to properly train a machine learning model, so we trained on Colab with a time limit of 8 hours. This forced us to train with only 1 epoch and we tested only one set of hyperparameters, so, assuming having the necessary hardware, future improvements could focus on training a better model, with more epochs and testing different hyperparameters. More affordable improvements could be done by dropping the libraries for training and inference and working directly with Torch for the interaction with the model, so we could be able to use the latest Torch version, which includes a new implementation of the Transformer API which speeds up training significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,89.29,265.68,175.21,8.93;10,130.96,84.19,333.36,168.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retrieve-than-rerank framework</figDesc><graphic coords="10,130.96,84.19,333.36,168.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,108.19,471.97,46.27,8.93;14,412.23,471.97,28.09,8.93;14,455.83,471.97,22.12,8.93;14,108.19,489.46,291.67,8.87;14,108.19,501.41,52.57,8.87;14,412.23,489.46,25.35,8.87;14,455.83,489.46,25.35,8.87;14,108.19,513.37,278.91,8.87;14,412.23,513.37,25.35,8.87;14,455.83,513.37,25.35,8.87;14,108.19,525.32,288.35,8.87;14,412.23,525.32,25.35,8.87;14,455.83,525.32,25.35,8.87;14,108.19,537.28,262.70,8.87;14,412.23,537.28,25.35,8.87;14,455.83,537.28,25.35,8.87;14,108.19,549.23,198.78,8.87;14,412.23,549.23,25.35,8.87;14,455.83,549.23,25.35,8.87;14,108.19,561.19,260.72,8.87;14,108.19,573.14,41.78,8.87;14,412.23,573.14,25.35,8.87;14,455.83,573.14,25.35,8.87;14,108.19,585.10,265.74,8.87;14,412.23,585.10,25.35,8.87;14,455.83,585.10,25.35,8.87;14,108.19,597.05,237.13,8.87;14,108.19,609.01,72.63,8.87;14,412.23,609.01,25.35,8.87;14,455.83,609.01,25.35,8.87;14,108.19,620.96,202.00,8.87;14,412.23,620.96,25.35,8.87;14,455.83,620.96,25.35,8.87;14,108.19,632.92,260.72,8.87;14,412.23,632.92,25.35,8.87;14,455.83,632.92,25.35,8.87;14,108.19,644.87,200.89,8.87;14,412.23,644.87,25.35,8.87;14,455.83,644.87,25.35,8.87;14,108.19,656.83,188.83,8.87;14,412.23,656.83,25.35,8.87;14,455.83,656.83,25.35,8.87;15,107.28,86.97,280.37,10.91;15,107.28,101.87,399.99,10.91;15,116.56,115.42,75.00,10.91;15,107.28,130.32,293.42,10.91;15,107.28,145.23,231.50,10.91;15,107.28,160.13,398.70,10.91;15,116.16,173.68,136.30,10.91;15,107.28,188.59,165.89,10.91;15,107.28,203.49,143.57,10.91;15,107.28,218.39,161.35,10.91;15,107.28,233.30,315.75,10.91;15,107.28,248.20,305.67,10.91;15,107.28,263.11,186.73,10.91;15,107.28,278.01,339.79,10.91;15,107.28,292.92,398.70,10.91;15,116.56,306.46,68.57,10.91;15,107.28,321.37,398.91,10.91;15,116.56,334.92,27.52,10.91;15,107.28,349.82,375.78,10.91"><head></head><label></label><figDesc>used Okapi BM25 similarity (with default parameters) • BM25Tuned: used Okapi BM25 similarity, with parameters tuned on training collection: k1=1.6 and b=0.7 • LMDirichlet: used Dirichlet smoothing (with default parameter) • StopDefault: used Lucene's default stop words list • Stop50: used stoplist built by picking the 50 most frequent terms in the documents indexed without stoplist and stemming • LightStem: used the Light stemmer • KStem: used Krovetz stemmer • SnowStem: the Snowball stemmer • Shingle: used word N-grams (max window size = 3) query expansion • Fuzzy: used fuzzyness (threshold parameter = 10) query expansion • SynCustom: used custom synonyms list • SynPOS: used WordNet synonym list together with OpenNLP PoS tagging • Rererank20W6: used reranker, reranking 20 documents with weight 0.6 given to the reranker scores • Rererank30: used reranker, reranking 30 documents with weight 1 given to the reranker scores • TrainedRerank30: used reranker, reranking 30 documents using our custom model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="16,89.29,427.53,262.48,8.93;16,89.29,84.19,416.69,330.77"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interpolated Precision-Recall curve on train collection</figDesc><graphic coords="16,89.29,84.19,416.69,330.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,107.28,495.93,351.36,10.91;16,114.72,509.48,61.10,10.91;16,107.28,523.12,341.85,10.91;16,107.28,536.76,351.36,10.91;16,107.28,550.41,324.46,10.91;16,107.28,564.05,253.78,10.91;16,107.28,577.69,375.99,10.91;16,107.28,591.34,258.19,10.91;16,107.28,604.98,370.83,10.91;16,107.28,618.62,331.47,10.91;16,107.28,632.27,325.18,10.91;16,107.28,645.91,259.94,10.91;16,107.28,659.55,245.08,10.91"><head>•</head><label></label><figDesc>fr_1 = FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom -Rerank20W6 • fr_2 = FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-Rerank30 • fr_3 = FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom • fr_4 = FADERIC_French-BM25Tuned-Stop50-LightStem-Shingle-Fuzzy • fr_5 = FADERIC_French-BM25-StopDefault-SnowStem • fr_6 = FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-TrainedRerank30 • fr_7 = FADERIC_French-LMDirichlet-Stop50-LightStem • en_1 = FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-SynPOS-Rerank30 • en_2 = FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-Rerank30 • en_3 = FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-SynPOS • en_4 = FADERIC_English-BM25-StopDefault-SnowStem • en_5 =FADERIC_English-LMDirichlet-Stop50-KStem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="21,89.29,437.93,274.30,8.93;21,89.29,94.50,416.69,330.86"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interpolated Precision-Recall curve on heldout collection</figDesc><graphic coords="21,89.29,94.50,416.69,330.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="21,89.29,652.55,307.61,8.93;21,89.29,481.42,187.51,140.63"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Box plot on heldout collection, the mean values are shown in red</figDesc><graphic coords="21,89.29,481.42,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="22,89.29,483.13,184.38,8.93;22,89.29,312.00,187.51,140.63"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Tukey's HSD on heldout collection</figDesc><graphic coords="22,89.29,312.00,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="23,89.29,438.53,286.09,8.93;23,89.29,94.30,416.69,331.66"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Interpolated Precision-Recall curve on short term collection</figDesc><graphic coords="23,89.29,94.30,416.69,331.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="23,89.29,652.75,319.41,8.93;23,89.29,481.62,187.51,140.63"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Box plot on short term collection, the mean values are shown in red</figDesc><graphic coords="23,89.29,481.62,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="24,89.29,656.16,196.18,8.93;24,89.29,485.03,187.51,140.63"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Tukey's HSD on short term collection</figDesc><graphic coords="24,89.29,485.03,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="26,89.29,437.86,282.61,8.93;26,89.29,94.52,416.69,330.77"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Interpolated Precision-Recall curve on long term collection</figDesc><graphic coords="26,89.29,94.52,416.69,330.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="26,89.29,652.53,321.04,8.93;26,89.29,481.40,187.51,140.63"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Box plot on long term collection, the mean values are shown in red</figDesc><graphic coords="26,89.29,481.40,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="27,89.29,467.51,197.81,8.93;27,89.29,312.01,166.67,125.00"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Tukey's HSD on long term collection</figDesc><graphic coords="27,89.29,312.01,166.67,125.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,344.83,187.42"><head>Table 1</head><label>1</label><figDesc>French analyzer process</figDesc><table coords="4,161.45,122.05,272.37,155.85"><row><cell>Step</cell><cell>Tokens</cell></row><row><cell></cell><cell>La méthode d'analyse de texte est</cell></row><row><cell></cell><cell>essentielle pour l'extraction d'informations.</cell></row><row><cell>Tokenizing</cell><cell>[La, méthode, d'analyse, de, texte, est,</cell></row><row><cell></cell><cell>essentielle, pour, l'extraction, d'informations]</cell></row><row><cell>Character folding</cell><cell>[la, methode, d'analyse, de, texte, est,</cell></row><row><cell></cell><cell>essentielle, pour, l'extraction, d'informations]</cell></row><row><cell>Stopword removal</cell><cell>[methode, analyse, texte,</cell></row><row><cell>(50 most freq.)</cell><cell>essentielle, extraction, informations]</cell></row><row><cell>Stemming</cell><cell>[method, analys, text,</cell></row><row><cell>(Ligth)</cell><cell>esentiel, extraction, inform]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,300.17,340.43,187.42"><head>Table 2</head><label>2</label><figDesc>English analyzer process</figDesc><table coords="4,165.85,331.73,263.57,155.85"><row><cell>Step</cell><cell>Tokens</cell></row><row><cell></cell><cell>The text analysis method's importance</cell></row><row><cell></cell><cell>lies in its role in information extraction.</cell></row><row><cell>Tokenizing</cell><cell>[the, text, analysis, method, importance,</cell></row><row><cell></cell><cell>lies, in, its, role, in, information, extraction]</cell></row><row><cell>Character folding</cell><cell>[the, text, analysis, method, importance,</cell></row><row><cell></cell><cell>lies, in, its, role, in, information, extraction]</cell></row><row><cell>Stopword removal</cell><cell>[text, analysis, method, importance,</cell></row><row><cell>(50 most freq.)</cell><cell>lies, role, information, extraction]</cell></row><row><cell>Stemming</cell><cell>[text, analysis, method, importance,</cell></row><row><cell>(Krovetz)</cell><cell>lie, role, information, extraction]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,90.49,401.56,111.21"><head>Table 3</head><label>3</label><figDesc>Indexing performances</figDesc><table coords="5,104.72,122.05,385.83,79.65"><row><cell cols="2">Collection Docs size</cell><cell>Stoplist</cell><cell cols="2">Stemmer Body terms</cell><cell>Index</cell><cell>Time (s)</cell></row><row><cell></cell><cell>(GB)</cell><cell></cell><cell></cell><cell></cell><cell>size (GB)</cell><cell></cell></row><row><cell>French</cell><cell>7.99</cell><cell>Default</cell><cell>Snowball</cell><cell>7,497,875</cell><cell>6.98</cell><cell>1224</cell></row><row><cell></cell><cell></cell><cell>50 most freq.</cell><cell>Light</cell><cell>7,459,058</cell><cell>6.95</cell><cell>842</cell></row><row><cell>English</cell><cell>7.27</cell><cell>Default</cell><cell>Snowball</cell><cell>7,253,947</cell><cell>6.49</cell><cell>1041</cell></row><row><cell></cell><cell></cell><cell>50 most freq.</cell><cell>Krovetz</cell><cell>7,451,647</cell><cell>6.43</cell><cell>848</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,90.49,381.49,193.68"><head>Table 4</head><label>4</label><figDesc>NDCG results with different BM25 parameters</figDesc><table coords="6,124.80,122.05,345.68,162.11"><row><cell>Run</cell><cell></cell><cell></cell><cell></cell><cell cols="5">FADERIC_French-Stop50-Stem-Shingle-Fuzzy</cell></row><row><cell cols="2">Measure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nDCG</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>1.2</cell><cell>1.4</cell><cell>1.6</cell><cell>1.8</cell><cell>2.0</cell></row><row><cell></cell><cell cols="7">0.30 0.3941 0.3952 0.3954 0.3957 0.3963 0.3936</cell><cell>0.3948 0.3934</cell></row><row><cell></cell><cell cols="7">0.40 0.3967 0.398 0.3984 0.3992 0.3992 0.3992</cell><cell>0.3981 0.3975</cell></row><row><cell></cell><cell cols="7">0.50 0.3999 0.4004 0.4008 0.4013 0.4014 0.4014</cell><cell>0.4014 0.4011</cell></row><row><cell>b</cell><cell cols="8">0.60 0.3999 0.4013 0.4017 0.4025 0.4026 0.4024 0.70 0.4021 0.4029 0.4034 0.4038 0.4043 0.4047 0.4046 0.4039 0.4019 0.4008</cell></row><row><cell></cell><cell cols="7">0.75 0.4018 0.4028 0.4035 0.4039 0.4038 0.4043</cell><cell>0.4037 0.4035</cell></row><row><cell></cell><cell>0.80</cell><cell cols="7">0.401 0.4025 0.4033 0.404 0.4041 0.4043 0.4047 0.4039</cell></row><row><cell></cell><cell cols="7">0.90 0.3985 0.3998 0.4009 0.4014 0.4018 0.4021</cell><cell>0.4015 0.4011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,90.49,293.85,105.74"><head>Table 5</head><label>5</label><figDesc>OpenNLP Tags compared with WordNet Sections</figDesc><table coords="9,212.43,122.05,170.41,74.17"><row><cell>OpenNLP Tag</cell><cell>WordNet Section</cell></row><row><cell>JJ</cell><cell>Adjectives</cell></row><row><cell>VB</cell><cell>Verbs</cell></row><row><cell>RB</cell><cell>Adverbs</cell></row><row><cell>NN</cell><cell>Nouns</cell></row><row><cell>Others</cell><cell>No synonyms retrieved</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,90.49,330.99,253.17"><head>Table 6</head><label>6</label><figDesc>monot5-base-msmarco-10k model with different number of documents to rerank</figDesc><table coords="12,88.99,119.83,309.32,223.82"><row><cell></cell><cell cols="2">nDCG MAP</cell></row><row><cell>0</cell><cell>0.4075</cell><cell>0.2411</cell></row><row><cell>10</cell><cell>0.414</cell><cell>0.2502</cell></row><row><cell>20</cell><cell>0.4119</cell><cell>0.2477</cell></row><row><cell>50</cell><cell>0.4083</cell><cell>0.242</cell></row><row><cell cols="2">100 0.405</cell><cell>0.2376</cell></row><row><cell cols="2">250 0.3987</cell><cell>0.2301</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell></row><row><cell cols="3">bert-base-mdoc-bm25 model with different number of documents to rerank</cell></row><row><cell></cell><cell cols="2">nDCG MAP</cell></row><row><cell>0</cell><cell>0.4075</cell><cell>0.2411</cell></row><row><cell>10</cell><cell>0.4207</cell><cell>0.2608</cell></row><row><cell>20</cell><cell cols="2">0.4222 0.2617</cell></row><row><cell>50</cell><cell>0.4212</cell><cell>0.2598</cell></row><row><cell cols="2">100 0.4184</cell><cell>0.2563</cell></row><row><cell cols="2">250 0.4104</cell><cell>0.2478</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,88.99,90.49,267.86,91.57"><head>Table 8</head><label>8</label><figDesc>Our trained model with different number of documents to rerank</figDesc><table coords="13,253.44,119.83,88.40,62.22"><row><cell></cell><cell cols="2">nDCG MAP</cell></row><row><cell>0</cell><cell>0.4075</cell><cell>0.2411</cell></row><row><cell cols="2">10 0.3910</cell><cell>0.2253</cell></row><row><cell cols="2">20 0.3741</cell><cell>0.1975</cell></row><row><cell cols="2">50 0.3405</cell><cell>0.1580</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,88.99,442.63,172.88,20.87"><head>Table 9</head><label>9</label><figDesc>nDCG and MAP values on train collection</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="22,88.98,90.49,315.20,204.56"><head>Table 13</head><label>13</label><figDesc></figDesc><table coords="22,88.98,102.49,315.20,192.56"><row><cell>ANOVA2 on heldout collection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(a) nDCG</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.01</cell><cell>3</cell><cell cols="2">0.003 0.64</cell><cell>0.58</cell></row><row><cell>Topics</cell><cell cols="2">23.54 97</cell><cell cols="3">0.242 47.20 1.97E-134</cell></row><row><cell>Error</cell><cell>1.49</cell><cell cols="3">291 0.005 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">25.04 391 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b) AP</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.01</cell><cell>3</cell><cell cols="2">0.003 0.68</cell><cell>0.55</cell></row><row><cell>Topics</cell><cell cols="2">23.35 97</cell><cell cols="3">0.240 42.10 8.26E-128</cell></row><row><cell>Error</cell><cell>1.66</cell><cell cols="3">291 0.057 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">25.03 391 -</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="24,88.98,97.19,392.21,361.46"><head>Table 14</head><label>14</label><figDesc></figDesc><table coords="24,88.98,109.20,392.21,349.46"><row><cell cols="3">nDCG and MAP values on short term collection</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run name</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nDCG</cell><cell>MAP</cell></row><row><cell cols="6">FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom-</cell><cell>0.4239</cell><cell>0.2665</cell></row><row><cell>Rerank20W6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-Rerank30</cell><cell>0.4145</cell><cell>0.2546</cell></row><row><cell cols="6">FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom</cell><cell>0.4034</cell><cell>0.2412</cell></row><row><cell cols="5">FADERIC_French-BM25Tuned-Stop50-LightStem-Shingle-Fuzzy</cell><cell></cell><cell>0.4034</cell><cell>0.2414</cell></row><row><cell cols="5">FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-SynPOS</cell><cell></cell></row><row><cell>-Rerank30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3296</cell><cell>0.1931</cell></row><row><cell>Table 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANOVA2 on short term collection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(a) nDCG</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.25</cell><cell>3</cell><cell cols="3">0.085 13.58 8.51E-9</cell></row><row><cell>Topics</cell><cell cols="2">218.58 881</cell><cell cols="3">0.248 39.30 0</cell></row><row><cell>Error</cell><cell>16.68</cell><cell cols="3">2643 0.006 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">235.52 3527 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) AP</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.38</cell><cell>3</cell><cell cols="3">0.129 16.05 2.43E-10</cell></row><row><cell>Topics</cell><cell cols="2">205.64 881</cell><cell cols="3">0.233 28.92 0</cell></row><row><cell>Error</cell><cell>21.32</cell><cell cols="3">2643 0.008 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">227.36 3527 -</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="25,88.99,90.49,392.20,129.65"><head>Table 16</head><label>16</label><figDesc></figDesc><table coords="25,89.29,102.49,391.90,117.64"><row><cell>nDCG and MAP values on long term collection</cell><cell></cell><cell></cell></row><row><cell>Run name</cell><cell>nDCG</cell><cell>MAP</cell></row><row><cell>FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom-</cell><cell>0.4153</cell><cell>0.2473</cell></row><row><cell>Rerank20W6</cell><cell></cell><cell></cell></row><row><cell>FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-Rerank30</cell><cell>0.4146</cell><cell>0.2465</cell></row><row><cell>FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom</cell><cell>0.4091</cell><cell>0.2384</cell></row><row><cell>FADERIC_French-BM25Tuned-Stop50-LightStem-Shingle-Fuzzy</cell><cell>0.4071</cell><cell>0.2350</cell></row><row><cell>FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-SynPOS</cell><cell></cell><cell></cell></row><row><cell>-Rerank30</cell><cell>0.3296</cell><cell>0.1809</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="27,88.98,90.49,317.91,204.57"><head>Table 17</head><label>17</label><figDesc></figDesc><table coords="27,88.98,102.49,317.91,192.57"><row><cell>ANOVA2 on long term collection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(a) nDCG</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.04</cell><cell>3</cell><cell cols="2">0.015 1.51</cell><cell>0.056</cell></row><row><cell>Topics</cell><cell cols="2">202.35 922</cell><cell cols="3">0.219 16.17 0</cell></row><row><cell>Error</cell><cell>16.78</cell><cell cols="3">2766 0.006 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">219.18 3691 -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) AP</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Systems 0.10</cell><cell>3</cell><cell cols="2">0.033 4.47</cell><cell>0.003</cell></row><row><cell>Topics</cell><cell cols="2">188.61 922</cell><cell cols="3">0.204 27.03 0</cell></row><row><cell>Error</cell><cell>20.92</cell><cell cols="3">2766 0.007 -</cell><cell>-</cell></row><row><cell>Total</cell><cell cols="3">209.64 3691 -</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,671.04,176.80,8.97"><p>https://lucene.apache.org/core/9_5_0/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,92.57,649.12,112.74,8.97"><p>https://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,92.57,660.08,128.40,8.97"><p>https://github.com/extjwnl/extjwnl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,92.57,671.04,101.05,8.97"><p>https://opennlp.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="10,92.57,671.03,138.30,8.97"><p>https://github.com/castorini/pygaggle</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5" coords="12,95.35,671.04,231.83,8.97"><p>https://huggingface.co/docs/transformers/main_classes/ trainer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6" coords="13,95.35,660.07,116.75,8.97"><p>https://github.com/kivy/pyjnius</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7" coords="13,95.35,671.03,103.10,8.97"><p>https://github.com/ninia/jep</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Based on these results, we can derive the following considerations:</p><p>• the runs on the French collections are significantly better than the ones on the English one, the reason for this is the automatic translation of the document collection, which has led to errors and inconsistencies in the English one; • the system performs better when configured to use BM25 similarity instead of the Dirichlet smoothing; • the tuned parameters on BM25 just gave slightly betters results, probably overfitting the run we have tuned them on, for this reason we have chosen to use the default ones in most of the runs; • the custom stoplist we have generated by picking the most frequent terms has outperformed Lucene's default ones because since it was based on the specific collection the effectiveness of using a stoplist has been maximized; • in both French and English the Snowball stemmer performed worse than the Light and Krovetz stemmer, respectively; • the use of word N-grams improved the performances, allowing to have more contextual matches by looking for group of words instead of single ones; • synonyms have slightly improved the performances, this is due to the fact that sometimes they can be misleading and retrieve documents that are not contextual with the query; • the reranking is a very powerful tool that has given a huge performance increase to our runs.</p><p>Based on the previous results and these considerations, we have decided to submit to CLEF the following systems:</p><p>• FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom-Rerank20W6, i.e., the best system overall; • FADERIC_English-BM25-Stop50-KStem-Shingle-Fuzzy-SynPOS-Rerank30, i.e., the best system overall on the English collection; • FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-SynCustom, i.e., the best system without the use of reranking; • FADERIC_French-BM25-Stop50-LightStem-Shingle-Fuzzy-Rerank30, i.e., the best system without the use of synonyms; • FADERIC_French-BM25Tuned-Stop50-LightStem-Shingle-Fuzzy, i.e., the best system without the use of both synonyms and reranking;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Test results</head><p>In this Section, we will analyze the performance of the submitted runs on heldout, short term and long term collections. At first, we will tackle the performance changes and then we will perform a statistical analysis. In this last part, we will use ANOVA2 with a significance level 𝛼 = 0.05 in order to find out if we can reject the null hypothesis, i.e. there is no significant statistical difference between the results of the given runs. Then we will use Tukey's Honestly Significant Difference (HSD) test to perform multiple pairwise comparisons and determine which specific run means differ significantly from each other. Since we have submitted four runs performed on the French collection and one on the English collection, in the statistical analysis we will compare to each other only the French runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Heldout</head><p>In Table <ref type="table" coords="20,130.64,334.60,10.35,10.91">12</ref> are reported the nDCG and MAP values obtained from the submitted runs on the heldout collection, while in Figure <ref type="figure" coords="20,267.04,348.15,5.17,10.91">3</ref> is reported the interpolated Precision-Recall curve. Comparing these results with the ones obtained on training, shown in Table <ref type="table" coords="20,440.87,361.69,5.17,10.91">9</ref> and Figure <ref type="figure" coords="20,500.81,361.69,5.17,10.91">2</ref> respectively, we can see that every run suffered a performance drop over all the measures. This worsening was expected and it can be due to the fact that the system has been tuned over a different set of queries. It should also be noticed that this set of queries is more than 6 times smaller compared to the training one, therefore the presence of some outliers could have caused the mean performances to drop and to not be a good descriptor of the system.</p><p>Observing the nDCG and Average Precision (AP) boxplots, shown in Figure <ref type="figure" coords="20,434.50,442.99,3.73,10.91">4</ref>, we can notice that the runs performed on the French collection have a similar structure in terms of median values and interquartile ranges. We can also notice that, in the AP boxplot, the reranked runs fr_1, fr_2 have longer whiskers, while the others show the presence of outliers.</p><p>From the ANOVA2 analysis, which results are reported in Table <ref type="table" coords="20,375.00,497.19,8.20,10.91">13</ref>, we can see that 𝑝-𝑣𝑎𝑙𝑢𝑒 &gt; 𝛼, therefore we cannot reject the null hypothesis. Moreover, from Tukey's HSD multiple comparison shown in Figure <ref type="figure" coords="20,223.34,524.28,3.81,10.91">5</ref>, we can derive that the French runs can be considered to be similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Short term</head><p>In Table <ref type="table" coords="20,128.42,587.16,10.27,10.91">14</ref> are reported the nDCG and MAP values obtained from the submitted runs on the short term collection, while in Figure <ref type="figure" coords="20,265.07,600.71,5.17,10.91">6</ref> is reported the interpolated Precision-Recall curve. Comparing these results with the ones obtained on heldout, shown in Table <ref type="table" coords="20,436.35,614.26,10.35,10.91">12</ref> and Figure <ref type="figure" coords="20,500.81,614.26,5.17,10.91">3</ref> respectively, we can see that every run has increased its performances. This improvement was not expected since the performances should tend to drop over time. This can be due to the fact that this set of runs is almost 9 times bigger than the heldout, therefore we can consider the mean measures obtained to be more reliable than the ones on the heldout collection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="28,112.66,294.63,394.53,10.91;28,112.66,308.18,394.52,10.91;28,112.66,321.73,394.53,10.91;28,112.66,335.28,394.52,10.91;28,112.66,348.83,393.33,10.91;28,112.66,362.38,393.33,10.91;28,112.66,375.93,321.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="28,112.66,335.28,389.62,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuscakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,127.14,348.83,378.84,10.91;28,112.66,362.38,327.16,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="28,471.85,362.38,34.14,10.91;28,112.66,375.93,148.46,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaliniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,389.48,394.53,10.91;28,112.66,403.03,393.33,10.91;28,112.66,416.58,226.80,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03229</idno>
		<title level="m" coord="28,112.66,403.03,393.33,10.91;28,112.66,416.58,44.62,10.91">Longeval-retrieval: French-english dynamic test collection for continuous web search evaluation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="28,112.66,430.13,394.53,10.91;28,112.28,443.67,215.68,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="28,227.44,430.13,275.85,10.91">A survey of automatic query expansion in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="28,112.28,443.67,146.97,10.91">Acm Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,457.22,394.53,10.91;28,112.66,470.77,269.51,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="28,219.59,457.22,283.00,10.91">Query expansion techniques for information retrieval: a survey</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="28,112.66,470.77,175.43,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1698" to="1735" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,484.32,393.33,10.91;28,112.66,497.87,255.22,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="28,289.19,484.32,216.79,10.91;28,112.66,497.87,125.91,10.91">Building for tomorrow: Assessing the temporal persistence of text classifiers</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05435</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,511.42,393.32,10.91;28,112.26,524.97,262.46,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="28,331.99,511.42,174.00,10.91;28,112.26,524.97,45.05,10.91">Applying BERT to Document Retrieval with Birch</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="28,112.66,538.52,394.04,10.91;28,112.66,552.07,23.57,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="28,199.29,538.52,121.64,10.91">Unicode Text Segmentation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chapman</surname></persName>
		</author>
		<ptr target="https://www.unicode.org/reports/tr29/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,565.62,395.01,10.91;28,112.66,579.17,23.21,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="28,189.82,565.62,82.83,10.91">Character Foldings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<ptr target="https://www.unicode.org/reports/tr30/tr30-4.html" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,592.72,395.01,10.91;28,112.66,606.27,166.00,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brault</surname></persName>
		</author>
		<ptr target="https://snowballstem.org/algorithms/french/stemmer.html" />
		<title level="m" coord="28,272.76,592.72,109.52,10.91">Snowball French stemmer</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,619.81,395.17,10.91;28,112.66,633.36,394.53,10.91;28,112.28,646.91,266.93,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="28,157.13,619.81,350.70,10.91;28,112.66,633.36,60.73,10.91">Light stemming approaches for the french, portuguese, german and hungarian languages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,201.32,633.36,300.48,10.91">Proceedings of the 2006 ACM Symposium on Applied Computing</title>
		<meeting>the 2006 ACM Symposium on Applied Computing</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1031" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,86.97,394.03,10.91;29,112.66,100.52,150.99,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Boulton</surname></persName>
		</author>
		<ptr target="https://snowballstem.org/algorithms/english/stemmer.html" />
		<title level="m" coord="29,238.64,86.97,117.63,10.91">Snowball English stemmer</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,114.06,393.98,10.91;29,112.66,127.61,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="29,164.24,114.06,194.85,10.91">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,367.38,114.06,92.60,10.91">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="277" to="294" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,141.16,394.52,10.91;29,112.66,154.71,333.61,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="29,243.47,141.16,258.60,10.91">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,112.66,154.71,254.75,10.91">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>FnTIR)</note>
</biblStruct>

<biblStruct coords="29,112.66,168.26,393.33,10.91;29,112.66,181.81,360.95,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="29,227.45,168.26,278.54,10.91;29,112.66,181.81,37.63,10.91">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,173.03,181.81,98.85,10.91">Soviet physics doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
			<publisher>Soviet Union</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,195.36,394.53,10.91;29,112.66,208.91,209.69,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="29,185.31,195.36,317.64,10.91">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,112.66,208.91,130.83,10.91">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,222.46,394.53,10.91;29,112.66,236.01,328.48,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="29,218.67,222.46,284.36,10.91">Rethink training of bert rerankers in multi-stage retrieval pipeline</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="29,127.29,236.01,283.91,10.91">The 43rd European Conference On Information Retrieval (ECIR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,249.56,394.53,10.91;29,112.66,263.11,393.33,10.91;29,112.66,276.66,168.49,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="29,112.66,263.11,341.66,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="29,462.63,263.11,43.36,10.91;29,112.66,276.66,123.70,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,290.20,393.33,10.91;29,112.66,303.75,211.29,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="29,352.90,290.20,153.09,10.91;29,112.66,303.75,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,112.66,317.30,394.03,10.91;29,112.66,330.85,195.09,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="29,199.04,317.30,93.26,10.91">Subword tokenization</title>
		<author>
			<persName coords=""><surname>Huggingface</surname></persName>
		</author>
		<ptr target="https://huggingface.co/docs/transformers/tokenizer_summary#subword-tokenization" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
