<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,357.06,15.42;1,89.29,106.66,390.89,15.42;1,89.29,132.98,220.08,11.96">SEUPD@CLEF: Team HIBALL on Incremental Information Retrieval System with RRF and BERT Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,162.87,76.36,11.96"><forename type="first">Andrea</forename><surname>Ceccato</surname></persName>
							<email>andrea.ceccato.6@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.84,162.87,63.41,11.96"><forename type="first">Luca</forename><surname>Fabbian</surname></persName>
							<email>luca.fabbian.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.86,162.87,80.68,11.96"><forename type="first">Bor-Woei</forename><surname>Huang</surname></persName>
							<email>borwoei.huang@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.14,162.87,80.43,11.96"><forename type="first">Irfan</forename><forename type="middle">Ullah</forename><surname>Khan</surname></persName>
							<email>irfanullah.khan@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.18,162.87,61.59,11.96"><forename type="first">Harjot</forename><surname>Singh</surname></persName>
							<email>harjot.singh@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,176.82,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,357.06,15.42;1,89.29,106.66,390.89,15.42;1,89.29,132.98,220.08,11.96">SEUPD@CLEF: Team HIBALL on Incremental Information Retrieval System with RRF and BERT Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">500F6B8E5BF037EFFDC97DBB8AE028A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF 2023</term>
					<term>Information Retrieval</term>
					<term>Short-term persistence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report showcases the work of the HIBALL team from the University of Padua on Task 1 LongEval-Retrieval of CLEF 2023 [1] [2]. Our goal was to create a general-purpose information retrieval system, using the CLEF document corpus and judgments as our reference. We explored various approaches, including algorithmic techniques and machine learning methods, and compared their results. Our best-performing system, a fusion between classical and AI techniques, shows promising outcomes and may serve as a foundation for future developments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Information retrieval is an open challenge. While working with well-structured data results in precise answers, the information retrieval field often has to handle with documents written by humans, which are usually messy and vague. The state-of-the-art technology is nowhere close to understand the whole meaning of documents, and thus there is great room for improving rank methods. This is a student group project conducted in the Search Engines course a.y. 2022/23 at the Computer Engineering master degree at University of Padua. In this report, we will compare classical information retrieval system and new AI-powered techniques, and finally draw a new system that combines both. As our ground truth, we used Sub-task A, Short-Term persistence, of the Retrieval task, at LongEval CLEF 2023 Lab <ref type="bibr" coords="1,305.74,557.31,11.32,10.91" target="#b2">[3]</ref>. This collection, with a 1570734 document corpus and 672 queries, also includes some judgement and thus is suitable to perform score evaluation.</p><p>The paper is organized as follows: Section 2 provides an overview of related works; Section 3 describes our approach; Section 4 explains our experimental setup; Section 5 discusses our main findings; finally, Section 6 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Information retrieval has been around for a long time, with first experiments being earlier than computers <ref type="bibr" coords="2,138.38,211.74,11.35,10.91" target="#b3">[4]</ref>. Despite decades of efforts, the field still has many open challenges. While there exists some ground-solid techniques <ref type="bibr" coords="2,253.52,225.29,11.43,10.91" target="#b4">[5]</ref>, they fail to capture the true semantic meaning of the documents involved, and thus new research is done every year <ref type="bibr" coords="2,371.45,238.84,11.43,10.91" target="#b5">[6]</ref>.</p><p>Traditional approaches are primary based on text matching, using explicit rules for indexing, querying and ranking algorithms. New approaches rely on AI for semantic extraction. They employ advanced techniques such as Natural Language Processing (NLP).</p><p>In this paper, we tried to get the best of both world by combining the two approaches into a hybrid one, and we tested the results on two complete document collections and real queries submitted by users of the Qwant search engine. For query selection and corpora retrieval, we leverage on the work done by CLEF <ref type="bibr" coords="2,251.43,341.66,11.43,10.91" target="#b1">[2]</ref>.</p><p>As a starting point for our code, we used the frrncl/hello-tipster example provided by Professor Nicola Ferro. The code showcases basic IR features from parsing documents, indexing the parsed documents, and searching. Everything is performed with Lucene as the underlying engine For the Re-ranker, we got inspired by JIAZHI GUO's notebook on 'Rescore BM25 with BERT' <ref type="bibr" coords="2,89.29,417.37,11.28,10.91" target="#b6">[7]</ref>. For the AI experiments regarding sentence similarity models, we took inspiration from blog tutorials such as <ref type="bibr" coords="2,164.62,430.92,11.43,10.91" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our system is an incremental one made of 3 parts, where each part becomes the grounding of the following one:</p><p>1. BASELINE. First, we performed basic parsing, indexing, and searching using Lucene, a Java library for information retrieval. We tried many runs with different setups. 2. RRF (Reciprocal Rank Fusion). Using Python, we merged two standard runs done Lucene Pipeline, one with BM25 Similarity and one with LMDirichletSimilarity. In this way, we got the best recall so far. 3. BERT, Re-ranking with Machine Learning. Finally, we took the results obtained previously, and we run second phase re-ranker using Python notebooks.</p><p>In parallel, we have done other AI experiments with pre-trained models. The only relevant results, that we have submitted to CLEF, as AI-FIXED and AI-MERGED, were given by the distilbert-base-nli-stsb-mean-tokens, a Sentence Similarity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BASELINE</head><p>Our first component, "BASELINE", is where we performed most of the retrieval heavy lifting. This system, written in Java, employs Lucene API under the hood. Here we indexed the entire data-set and produced our first phase ranked list. The system parses the documents thanks to our LongEvalDocumentJsonParser and produces ParsedDocuments as a result. Then the DirectoryIndexer takes those ParsedDocuments, produces an Inverted Index, and saves it on the disk. These steps are called Parsing and Indexing. The last step is done by the Searcher which loads the Index, parsed the queries, and finally produces the Run. Figure <ref type="figure" coords="3,204.02,649.48,5.07,10.91" target="#fig_0">1</ref> shows these steps.</p><p>Parsing: the first step of our Information Retrieval Pipelines is parsing the documents provided by the organizers at CLEF. CLEF corpus is distributed both in the TREC and the JSON formats. We chose the latter, because it is more popular and thus, both Java and Python have libraries to parse it. You may find more details on the data sets in the section 4.1</p><p>We wrote our parser, LongEvalDocumentJsonParser, which uses JSON Parser provided by "com.fasterxml.jackson.core".</p><p>LongEvalDocumentJsonParser converts JSON documents in ParsedDocuments. These are ready to be used in the subsequent step of the IR chain. We kept the same IDs provided in the JSON document by CLEF.</p><p>Meanwhile, we also wrote the LongEvalTopicsTSVReader parser for the queries, which were provided in the TSV (tab-separated-values) format.</p><p>Indexing: the goal of indexing is to store the document collection in a data structure optimized for lookups. In this way, we will be able to search inside documents as fast as possible. This is done by creating a structure called inverted index, which maps each term to the documents including that term. We used the class DirectoryIndexer provided by Professor Ferro as a template, providing as arguments:</p><p>â€¢ an Analyzer to build TokenStreams from a document â€¢ a Similarity function for scoring</p><p>Searching: the searcher takes as inputs the indexed documents and the topic, and returns a run file containing the document ranking for each query/topic provided. Analyzer and the Similarity function are also required by the Searcher when creating a run. Searcher and Indexer should have the same Analyzer and Similarity function in a pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Analyzer:</head><p>the analyzer is the core part of every IR pipeline and, as mentioned previously, is required by both the Indexer and the Searcher. Its goal is to build TokenStreams from a ParsedDocument.</p><p>In order to reduce boilerplate code, we created our own Analyzer by extending org.apache.lucene.analysis.Analyzer. We added Tokenizer, Stop List, and Stemmer as parameters.</p><p>â€¢ A Tokenizer is the first piece in the pipeline chain. It creates a TokenStream from a Reader. We tested StandardTokenizer, WhitespaceTokenizer, and LetterTokenizer and after each tokenizer, we add LowerCaseFilter to increase the matched token on search time. â€¢ The Stop List Filter aims to get rid of words with low informative value like articles, prepositions, pronouns, and conjunctions. Moreover, this helps to reduce the size of the index (âˆ¼10GB). We tested Lucene, Lingpipe, Snowball, Okapi, Glasgow, Atire, Terrier, Smart, Zettair, and Indri. â€¢ Another way to increase matching is through Stemming. A Stemmer Filter removes prefixes and suffixes, going back to the "root" form of the word. We tested EnglishMinimal, Porter, SnowballPorter, K (Krovetz), and Lovins. â€¢ Other filter we tried are: NGramTokenFilter,ShingleFilter, and LengthFilter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Extra: Url matching</head><p>While looking at the data, we realized some queries are just URLs. This may happen because distracted users mistake the search engine bar for the address bar. In these cases, it's clear the search engine should take the user to the exact website they typed. This made us think about introducing some special rules just for URL handling. We discovered, however, most of these URLs are not in our dataset: even if we know the website we should point to, we could not, because we do not have a corresponding document ID. Thus, we decided to discard those results. We collected our experiments regarding the topic under /code/src/main/ainotebooks/extra-urlmatcher.ipynb.</p><p>Even if we dismissed the experiments, we did it just due to the lack of corrisponding data. Perhaps, this could lead to some improvements if investigated in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RRF (Reciprocal rank fusion)</head><p>After testing different combinations of Stemmers and Stop Lists, we focused on increasing the recall. In the next phase (see section 3.3) we are performing a re-ranking, and so right now we just care about finding the most relevant documents, and we do not aim at higher precisions.</p><p>To increase the recall we tried two Similarities:</p><formula xml:id="formula_0" coords="5,107.28,407.48,399.36,54.38">â€¢ Okapi BM25 (BM25) of a document ğ· ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ·, ğ‘„) = ğ‘› âˆ‘ï¸ ğ‘–=1 ğ¼ğ·ğ¹ (ğ‘ ğ‘– ) ğ‘“ (ğ‘ ğ‘– , ğ·)(ğ‘˜ 1 + 1) ğ‘“ (ğ‘ ğ‘– , ğ·) + ğ‘˜ 1 (1 -ğ‘ + ğ‘ |ğ·| ğ‘ğ‘£ğ‘”ğ‘‘ğ‘™ )<label>(1)</label></formula><p>â€¢ LMDirichletSimilarity (Bayesian smoothing using Dirichlet priors)</p><formula xml:id="formula_1" coords="5,216.76,492.56,289.88,24.43">ğ‘(ğ‘¤ ğ‘– |ğ‘‘) = ğ‘  |ğ‘‘| + ğ‘  ğ‘(ğ‘¤ ğ‘– |ğ‘) + |ğ‘‘| |ğ‘‘| + ğ‘  ğ‘¡ğ‘“ ğ‘– (ğ‘‘) |ğ‘‘|<label>(2)</label></formula><p>and their combinations, as shown in Table <ref type="table" coords="5,280.31,526.77,3.74,10.91">1</ref>, with different rank fusion algorithms:</p><p>â€¢ Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="5,246.22,543.98,11.56,10.91" target="#b8">[9]</ref>:</p><formula xml:id="formula_2" coords="5,222.20,565.40,284.44,25.61">ğ‘…ğ‘…ğ¹ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘‘ âˆˆ ğ·) = âˆ‘ï¸ ğ‘Ÿâˆˆğ‘… 1/(ğ‘˜ + ğ‘Ÿ(ğ‘‘))<label>(3)</label></formula><p>â€¢ CombSUM:</p><formula xml:id="formula_3" coords="5,238.99,611.17,267.65,25.61">ğ¶ğ‘œğ‘šğ‘ğ‘†ğ‘ˆ ğ‘€ (ğ‘‘ âˆˆ ğ·) = âˆ‘ï¸ ğ‘Ÿâˆˆğ‘… ğ‘Ÿ(ğ‘‘)<label>(4)</label></formula><p>CombSUM is provided by Lucene as the default implementation of MultiSimilarity, but no implementation of RRF was found, so we wrote a Python script, and we used the "fusion" function from trectools library for the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>fused_run = fusion.reciprocal_rank_fusion(trec_runs, k,docs_per_query)</head><p>where trec_runs is a list of TrecRun containing the run files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">BERT</head><p>After obtaining as many relevant documents as possible, with the re-ranker our goal was to increase the MAP, P@10, and nDCG. See section 4.2 for Evaluation measures.</p><p>As our second phase re-ranker we chose Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="6,122.14,202.85,16.25,10.91" target="#b9">[10]</ref>, a transformer-based machine learning model.</p><p>BERT comes in two model sizes, the small one ğµğ¸ğ‘…ğ‘‡ ğµğ´ğ‘†ğ¸ , which has around 110 million parameters, and a large one, ğµğ¸ğ‘…ğ‘‡ ğ¿ğ´ğ‘…ğºğ¸ with 340 million parameters <ref type="bibr" coords="6,412.71,233.93,16.25,10.91" target="#b10">[11]</ref>.  As input, the model requires a unique vector of tokens (input_ids), so for each query we had to concatenate together the query itself and the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-base-uncased model</head><p>The model is able to distinguish the query from the document thanks to token_type_ids.</p><p>In the following code snippet, we create an input sequence for BERT where we put 0 for query tokens and 1 for document tokens. We can see from the code above that the input tokens, that exceed 512 in length, are truncated to fit the input window size of the BERT-base model.</p><p>As a final result, the BERT re-ranker estimates a score on the relevance of a candidate document to a query. In detail,the steps performed to get this part done are the following ones:</p><p>â€¢ BERT Fine-Tuning The First part was to fine-tune the BERT-base-uncased model with our training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Pre-processing</head><p>For each query, we consider training instance a relevant document put together with three other non-relevant documents: one "positive" sample with three "negative" samples.</p><p>The "positive" samples are taken from the CLEF assessments (qrel) file and the "negative" samples are randomly chosen from the set obtained by removing all the relevant documents from the run file created in step 3.2.</p><p>To simplify the model (and ultimately consume less computational power) we did not consider the difference between documents graded as "highly relevant" and "relevant".</p><p>We split our training data into training and validation sets with the 80/20 rule. We used the Adam optimizer with an initial learning rate set to 3 * 10 -5 . Due to the time constraints, we trained the network with a very low patient of early stopping, set to 2 and the training stopped at the 4th iteration/epoch with the best loss on the validation set of 0.58664.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Hyperparameter tuning</head><p>After the model training, we had to tune another hyperparameter: ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ for the ğµğ¸ğ‘…ğ‘‡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘  in the ranking. For this we grid searched the ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ to give to ğµğ¸ğ‘…ğ‘‡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘  and get ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘ğ‘’ğ‘Ÿğ‘¡_ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡=0.012 with the best MAP of 0.2298.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Re-ranking</head><p>Finally, after the training and hyperparameter tuning, we could combine the scores from our trained model (ğµğ¸ğ‘…ğ‘‡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘  ) with the scores from step 3.2 (ğ‘…ğ‘…ğ¹ 60 ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘  ) as per:</p><formula xml:id="formula_4" coords="7,122.50,533.96,373.88,7.90">weighted_scores = RRF60_scores + best_bert_weight * BERT_scores</formula><p>After computing the ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ of each document, we resort all the documents by the new ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘  and create the final rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">AI Experiments</head><p>AI-based technologies are game changers in the current era. Tasks like information retrieval are human related and allow for inaccuracies, so it just feels natural to test AI tools in such projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Dismissed experiments</head><p>Generative models: The main idea here was to improve or expand document and/or queries thanks to generative text models, such as the one baking ChatGPT. Most of the time, when we search, we are not looking for a document including the question itself, but rather for a document containing the answer to our question. If the AI is able to partially answer a question, or to rephrase a sentence, those new terms could make the query way more precise.</p><p>This proved to be a dead-end pretty soon. Weaker models such as gpt2 are totally unable to provide a useful result.</p><p>Switching to a more powerful model requires way more hardware resources compared to what we own. The open source competitor of ChatGPT, GPT-j, requires a cluster of GPUs and more than 48 + 16GBs of dedicated RAM.</p><p>We then used the expanded query to perform a search with Lucene. The evaluation scored with trec_eval did not show any significant improvements, sometimes it got even worse (query drifting?).</p><p>We do believe this approach could lead to great results, however we lack the hardware capabilities to investigate systematically.</p><p>Paraphrase models: These models aim to provide a different, improved version of a sentence. They are the AI answer to stemmers (sort of), in a sense that they provide a standardized and less convoluted way to express the same words.</p><p>Unfortunately, this process is not deterministic, leading to unexpected behaviours: similar sentences may be mapped into completely different sentences; thus, if we map a document and a query using the same AI model, we may increase their distance instead of reducing it.</p><p>Moreover, most of these models (even commercial ones like https://instatext.io/), are tuned for long sentences, whereas most of our queries are made of 1-3 words.</p><p>No relevant results here as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Sentence similarity models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI-FIXED:</head><p>Our biggest experiment employed sentence similarity models. This kind of AI extracts features from text in the form of a dense vector. Similar sentences will have similar corresponding vectors; it means we may perform the actual search using a vector space and pick document vectors closer to the query vector.</p><p>After some research, we end up with the distilbert-base-nli-stsb-mean-tokens model, which proved to be a good compromise between performance and accuracy. While it is based on BERT as well, it follows a different approach: it is not meant for re-ranking, but rather to provide a ranking on its own.</p><p>We were able to run the model and index the whole longeval-train-v2 document collection into a 4.5G file (each document is represented as a 768 float 32 array). This was rather process-intensive, and required hours, even with a 3060 NVidia GPU.</p><p>Once indexed, we use another notebook to perform the information retrieval. The full process does not rely on Java/Lucene at all, because it employs Faiss, a powerful python library for working with vector spaces and indexes using hardware acceleration. Faiss is currently the fastest way to look for vector similarity using GPUs, and it's under the hood of popular commercial services such as Amazon Elasticsearch.</p><p>Once performed every query, we are finally able to check our results. However, there is a problem: if you look at the results from the trec_eval tool, we get very different scores. While the classic approach is rewarded, the AI results are given a P_10 and even P_30 of zero.</p><p>We think this happens because the AI finds documents different from the one ranked, and thus unknown to trec_eval. After all, the train dataset only has like 10 scores per query, and these one have been retrieved using a different approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI-MERGED:</head><p>The retrieved documents looked so different from the one suggested by any other method, and this suggested us to try a fusion.</p><p>We decided to fuse the "AI-FIXED" run with the most promising one so far, the one described in Chapter 4 as "BASELINE".</p><p>Usual fusion functions did not work as expected, so we decided to end up with our own. We chose an almost linear function, because it was easier to tune by hand.</p><p>After some attempts, we end up with the following weights:</p><formula xml:id="formula_5" coords="9,160.37,377.05,346.27,24.43">ğ‘† ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ = ğ‘† ğ‘™ğ‘¢ğ‘ğ‘’ğ‘›ğ‘’ + ( 1 ğ·ğ‘“ ğ‘ğ‘–ğ‘ ğ‘  -0.0026) * 1000 * 0.5 * ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦<label>(5)</label></formula><p>Where ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ = 0.25 if the document is short, 1 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In this section, we will provide the setup used for each system/run submitted to the CLEF. We will describe the dataset, evaluation measures and the Hardware we used to train and evaluate our system. The 5 submitted runs to the CLEF are:</p><p>â€¢ HIBALL_BASELINE: described in Sec. 3.1 (with Lucene StopList and K Stemmer), is our first and basic system which requires only Lucene to work; â€¢ HIBALL_RRF60: described in Sec. 3.2, which merges BM25 and LM1000 similarities with python code; â€¢ HIBALL_BERT: described in Sec. 3.3, trained on GPUs with colab notebook; â€¢ HIBALL_AIFIXED: described in Sec. 3.4, showcases our sentence similarity model for which we used GPUs from our physical device; â€¢ HIBALL_AIMERGED: described in Sec. 3.4, which merges the HIBALL_AIFIXED with the HIBALL_BASELINE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data description</head><p>The data provided by CLEF includes a training dataset and a test dataset. The training data can be found at http://hdl.handle.net/11234/1-5010, while the test data is at http://hdl.handle.net/ 11234/1-5139.</p><p>â€¢ training data: includes documents, queries and qrels collected during June 2022. The document corpus has 1.5 million scraped Web pages, provided both in the TREC and JSON formats. We worked on the JSON format documents. Each document as an id and a contents fields. The dataset also provides 672 train queries, in both TREC and TSV formats. In the qrels file, documents are labelled with human-provided assessments. Document are scored as either highly relevant (2), relevant <ref type="bibr" coords="10,389.27,446.78,11.81,10.91" target="#b0">(1)</ref> or irrelevant (0) to the queries. â€¢ testing data: contains two sets of data, short-term dataset collected during July 2022 and long-term dataset collected during September 2022. Queries and documents keeps the same format of the training data.</p><p>We used only the English version of the documents for the training and short-term dataset for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Measures</head><p>We used the trec_eval tool to evaluate the rankings based on query relevance list (qrels file), and the rankings of the documents (run file). The metrics we focused on are:</p><p>â€¢ Average Precision (AP): defined as:</p><formula xml:id="formula_6" coords="10,264.18,650.44,238.60,29.73">ğ´ğ‘ƒ = 1 ğ‘…ğµ âˆ‘ï¸ ğ‘˜âˆˆğ‘… ğ‘ƒ (ğ‘˜) (<label>6</label></formula><formula xml:id="formula_7" coords="10,502.78,657.09,3.86,10.91">)</formula><p>where RB is the recall base and ğ‘… is the set of the rank positions of the relevant retrieved documents â€¢ Mean Average Precision (MAP): defined as the mean of the AP scores for each query in a set of ğ‘„ number of queries:</p><formula xml:id="formula_8" coords="11,258.99,151.68,247.65,29.28">ğ‘€ ğ´ğ‘ƒ = âˆ‘ï¸€ ğ‘„ ğ‘=1 ğ´ğ‘ƒ (ğ‘) ğ‘„<label>(7)</label></formula><p>â€¢ Precision at document Cut-off (P@k) with k = 10, defined as:</p><formula xml:id="formula_9" coords="11,273.21,216.42,233.43,33.58">ğ‘ƒ (ğ‘˜) = 1 ğ‘˜ ğ‘˜ âˆ‘ï¸ ğ‘›=1 ğ‘Ÿ ğ‘›<label>(8)</label></formula><p>â€¢ Recall at document Cut-off (R@k) with k=1000, defined as:</p><formula xml:id="formula_10" coords="11,267.70,284.99,235.08,33.58">ğ‘…(ğ‘˜) = 1 ğ‘…ğµ ğ‘˜ âˆ‘ï¸ ğ‘›=1 ğ‘Ÿ ğ‘› (<label>9</label></formula><formula xml:id="formula_11" coords="11,502.78,295.89,3.86,10.91">)</formula><p>where RB is the recall base â€¢ Discounted Cumulated Gain (DCG)@p, defined as:</p><formula xml:id="formula_12" coords="11,224.50,366.24,282.14,34.29">ğ·ğ¶ğº@ğ‘ = ğ‘ âˆ‘ï¸ ğ‘–=1 ğ‘Ÿğ‘’ğ‘™ ğ‘– ğ‘šğ‘ğ‘¥(1 + ğ‘™ğ‘œğ‘” 2 (ğ‘– + 1))<label>(10)</label></formula><p>â€¢ Normalized Discounted Cumulated Gain ad cut p (nDCG@p), dividing DCG by ideal DCG (iDCG) to normalize the score in [0, 1]:</p><p>ğ‘›ğ·ğ¶ğº@ğ‘ = ğ·ğ¶ğº@ğ‘ ğ‘–ğ·ğ¶ğº@ğ‘ (11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Repository</head><p>For Lucene pipeline, we worked with Java. Additionally, we worked on python notebooks, which have proven to be the best environment for AI prototyping. All the developed code is available on the git repository of the group: https://bitbucket.org/upd-dei-stud-prj/seupd2223-hiball/src/ master/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hardware</head><p>The hardware used to run the experiments are:</p><p>â€¢ Model: HP </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>In this section, we will first introduce some general considerations about each employed technique. We will then provide a result comparison (5.5) based on trec_evals metrics, and finally provide a statistical analysis of it (5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">BASELINE System</head><p>To choose our BASELINE system, we experimented all the possible combinations of stemming and StopList filters with BM25 Similarity.</p><p>The results were not significantly different among them.</p><p>â€¢ The map values were around 0.1465;</p><p>â€¢ the P@10 values were around 0.0937;</p><p>â€¢ Recalls were around 0.6954;</p><p>â€¢ and nDCG values are around 0.2896.</p><p>K-stemming is slightly outperforming Lovins-stemming and Porter-stemming. While "Lucene" and "Lingpipe" Stop Lists have the best overall performances among all stop lists we tested in combination with the stemmers. For the BASELINE system, we chose Lucene StopList and K Stemmer. With this configuration, we have the highest MAP and P@10 values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">RRF (Reciprocal Rank Fusion)</head><p>In the second part of our system development, where the goal was to improve recall, we tried out different Similarities and their combinations. From the Table <ref type="table" coords="13,374.79,422.82,3.68,10.91">1</ref>, we can see the best recall is achieved with RRF rank fusion algorithm, and we chose to use RRF60 for the re-ranking system as it has higher overall performance map value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The results with different Similarities and their combination on the train data runs MAP P@10 recall@1000 nDCG luceneStop-kStem-bm25 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">BERT</head><p>From the results in Table <ref type="table" coords="14,200.46,111.52,3.66,10.91" target="#tab_2">2</ref>, we can see that with our HIBALL-BASELINE, with Lucene StopList-Filter and KStemFilter, we improved slightly the baseline of LongEval-Retrieval <ref type="bibr" coords="14,446.17,125.07,11.48,10.91" target="#b2">[3]</ref>. However, from Tables <ref type="table" coords="14,143.60,138.62,10.93,10.91" target="#tab_2">2 ,</ref><ref type="table" coords="14,154.54,138.62,3.23,10.91">3</ref>, and 4 we can see that the re-ranking system with BERT model, which is based on RRF60 system, significantly improves all the measures MAP, P@10 and nDCG and Recall.</p><p>The BERT model fine-tuned with the parameters described in Sec. 3.3 and the implementation of early stopping show a good generalization of the model on new data. For example, the results on TEST data (unknown to the model) in Tab. 4, are similar to those on the TRAIN data (used for the training and validation) in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">AI experiments</head><p>We trouble measuring the final score. Evaluating our results has been and still is a challenge because the train set provided by CLEF is clueless about some documents retrieved by AI models, while the same documents look right at a human eyes. Sometimes, the conclusion we draw by manually looking at the results were clearly conflicting with the one measured by trec_eval.</p><p>We also realized too late the HIBALL_BERT was so successful, because it did not look so promising at first. Since we fused with HIBALL_BASELINE, the improvements we got are related to the HIBALL_BASELINE and do not look so good compared to the HIBALL_BERT ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Result comparison</head><p>Tab. 2, Tab. 3, Tab. 4 show the results of each submitted run on TRAIN, HELDOUT and TEST data respectively. By comparing the tables, we can see that HIBALL_BERT is clearly the one leading on every dataset, followed by the AI merged with BASELINE (HIBALL_AIMERGED). Then our BASELINE system, followed by HIBALL_RRF60. Lastly, we have the AI system (HIBALL_AIFIXED), which has very poor performance independently. The tables are ordered by decreasing MAP values, but we can see that HIBALL_RRF60 has the same Recall values of the best performing system, as expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Statistical Analysis</head><p>While table <ref type="table" coords="15,144.25,361.53,5.17,10.91">4</ref> shows the means of the measures, the boxplots depict better the performance distribution over the whole data. Figure <ref type="figure" coords="15,264.11,375.08,4.97,10.91" target="#fig_5">3</ref> shows the results on the HELDOUT set, while figure <ref type="figure" coords="15,501.27,375.08,4.97,10.91" target="#fig_6">4</ref> reveals the results on the test set. From the boxplot of the measure of AP (figure <ref type="figure" coords="15,440.36,388.63,7.70,10.91" target="#fig_5">3a</ref>), we noticed that HIBALL_BERT with more high-precision queries outperforming other systems, despite the fact that there are still some queries' precision close to 0. The distribution of the nDCG boxplot (figure <ref type="figure" coords="15,121.54,429.28,9.25,10.91" target="#fig_5">3b</ref>) shows a similar trend. That tells us that the BERT re-ranker improved the AP and nDCG of many queries, but there are still many queries' results do not benefit from the BERT re-ranker.</p><p>Unsurprisingly, the boxplots of the results on the test set are in like manner with that on the heldout set. In figure <ref type="figure" coords="15,182.56,487.46,8.14,10.91" target="#fig_6">4a</ref>, we can see that HIBALL_BERT system has a much larger third quartile toward higher AP value compared with BASELINE. The AP of HIBALL_BERT system has a certain ratio of queries with high AP values above 0.6, which significantly raises the overall MAP performance. The other 3 runs, HIBALL_BASELINE, HIBALL_RRF60 and HIBALL_AIMERGED, do not show much difference. A similar behavior can be observed in the nDCG boxplots (figure <ref type="figure" coords="15,89.04,555.21,7.89,10.91" target="#fig_6">4b</ref>). It is worth noticing that, from the heldout set to the test set, HIBALL_BERT's third quartile of the P@10 distribution stays at around 0.2 (figure <ref type="figure" coords="15,320.50,568.76,7.73,10.91" target="#fig_6">4c</ref>), whereas the BASELINE model's third quartile of P@10 drops from 0.2 to 0.1. Thus, the IR-system with BERT re-ranker sustains its high performance of the P@10 on the TEST set.</p><p>The ANOVA tests were implemented to compare the 5 runs. The p-values of the two-way ANOVA tests of AP and nDCG on the HELDOUT set are close to 0 (table <ref type="table" coords="15,422.34,626.94,5.17,10.91" target="#tab_3">5</ref> and<ref type="table" coords="15,449.79,626.94,3.65,10.91" target="#tab_4">6</ref>), while the p-values on the TEST set are exactly 0 as shown in table 7 and 8. The close to 0 p-values are far below the threshold ğ›¼ = 0.05. We confidently reject the hull hypothesis, i.e., the 5 runs are equal, and claim that the runs are different regarding the measure AP and nDCG.  In figure <ref type="figure" coords="16,130.78,433.56,3.81,10.91" target="#fig_8">5</ref>, the Tukey's Honestly Significant Difference (HSD) clearly reveals the difference of the means among the 5 runs on HELDOUT set. HIBALL_BERT has high AP and nDCG means, while HIBALL_BASELINE, HIBALL_RRF60 and HIBALL_AIMERGED have similar means and variance, which reinforce the argument inferred from the boxplots shown in figure <ref type="figure" coords="16,470.82,474.21,3.81,10.91" target="#fig_5">3</ref>. Once again, the Tukey's HSD in figure <ref type="figure" coords="16,236.75,487.76,5.03,10.91" target="#fig_9">6</ref> shows the difference of the means among the 5 runs on the TEST set. Therefore, we can safely conclude that the 5 runs are significantly different.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this report, we started by introducing a relevant problem in the Information Retrieval domain, and we proposed our System as a solution. Then we explained how we incrementally came up with the final System, i.e., starting from a BASELINE system and then trying to improve recall with RRF, and finally, other measures like MAP, P@10, and nDCG with BERT.</p><p>Finally, we spent some time investigating AI models pretrained on general purpose text. Those provided different results from the classical approach; while those results looks promising on manual inspection, they were outside the assessments' radar, and thus we were not able to provide a solid measure for them.</p><p>In conclusion, from our experiments, BERT re-ranking is the most promising way to improve the search engine metrics. There are, however, flaws in our BERT-base training processes. Firstly, due to the limitation of the GPU and RAM memory, we only feed first 512 tokens to the BERT-base model, which means the model was not trained with sentences large enough for each document (4900 on average). Secondly. the training process is time-consuming, restricted by not enough powerful GPU. We trained the BERT-base model for four epochs only, and we did not make distinction between "highly relevant" and "relevant" documents assessments.</p><p>We could also train the BERT-base model with full training documents by shifting the input window. Moreover, the BERT-base mode can be trained with more epochs to get higher accuracy.</p><p>To further improve the BERT re-ranker performance, the BERT-large model might be a better option than the BERT-base model.</p><p>We should also find a way to measure if the pretrained AI experiments were actually on the spot or not, and try to fuse it with the HIBALL_BERT instead of the HIBALL_BASELINE run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,565.64,106.90,8.93;3,89.29,392.05,416.70,161.02"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lucene Pipeline</figDesc><graphic coords="3,89.29,392.05,416.70,161.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,228.73,266.67,277.46,10.91;6,89.02,280.22,416.96,10.91;6,89.29,293.77,85.41,10.91"><head></head><label></label><figDesc>Given the limited resources, we decided to fine-tune the smaller version of the BERT model for our purpose of re-ranking. The initial model can be downloaded from HuggingFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,459.20,296.21,8.93;6,141.38,316.48,312.53,130.16"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall pre-training and fine-tuning procedures for BERT<ref type="bibr" coords="6,369.04,459.25,16.46,8.87" target="#b9">[10]</ref> </figDesc><graphic coords="6,141.38,316.48,312.53,130.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,136.77,573.98,183.97,7.90;6,136.77,587.53,255.19,7.90;6,136.77,601.08,309.87,7.90;6,136.77,614.63,321.74,7.90;6,136.77,628.18,303.13,7.90;6,136.77,641.73,237.38,7.90;6,136.77,655.28,296.73,7.90;6,136.77,668.83,274.27,7.90"><head>#</head><label></label><figDesc>make input sequences for BERT input_ids = query_token_ids + doc_token_ids token_type_ids = [0 for token_id in query_token_ids] token_type_ids.extend(1 for token_id in doc_token_ids) if len(input_ids) &gt; max_input_length: # truncation input_ids = input_ids[:max_input_length] token_type_ids = token_type_ids[:max_input_length] attention_mask = [1 for token_id in input_ids]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,136.77,154.56,344.21,7.90"><head></head><label></label><figDesc>score = model(input_ids, attention_mask,token_type_ids)[0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,89.29,214.69,418.23,8.93;16,230.97,253.50,133.34,100.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boxplots of the 5 runs in the decreasing order of the mean performance on the HELDOUT set.</figDesc><graphic coords="16,230.97,253.50,133.34,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,89.29,384.01,404.05,8.93;16,89.29,253.50,133.34,100.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Boxplots of the 5 runs in the decreasing order of the mean performance on the TEST set.</figDesc><graphic coords="16,89.29,253.50,133.34,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="17,89.29,653.78,257.96,8.93;17,301.80,461.84,204.18,161.44"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Tukey's HSD test of the 5 runs on the HELDOUT set</figDesc><graphic coords="17,301.80,461.84,204.18,161.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="18,89.29,469.71,234.30,8.93;18,89.29,277.34,204.19,161.88"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Tukey's HSD test of the 5 runs on the TEST set</figDesc><graphic coords="18,89.29,277.34,204.19,161.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,88.99,534.88,333.96,101.12"><head>Table 2</head><label>2</label><figDesc>The results with different configuration on the TRAIN DATA</figDesc><table coords="14,172.32,562.97,250.63,73.03"><row><cell>runs</cell><cell>MAP</cell><cell>P@10</cell><cell cols="2">nDCG Recall</cell></row><row><cell>HIBALL_BERT</cell><cell cols="4">0.2298 0.1429 0.3747 0.7135</cell></row><row><cell>HIBALL_AIMERGED</cell><cell>0.1501</cell><cell>0.0927</cell><cell>0.2943</cell><cell>0.7047</cell></row><row><cell>HIBALL_BASELINE</cell><cell>0.1498</cell><cell>0.0952</cell><cell>0.2939</cell><cell>0.705</cell></row><row><cell>HIBALL_RRF60</cell><cell>0.1429</cell><cell>0.0905</cell><cell cols="2">0.2889 0.7135</cell></row><row><cell>HIBALL_AIFIXED</cell><cell>0.0262</cell><cell>0.0210</cell><cell>0.0891</cell><cell>0.2975</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="16,88.99,530.32,328.92,88.76"><head>Table 5</head><label>5</label><figDesc>Two-way ANOVA analysis of the 5 runs on AP of the HELDOUT set</figDesc><table coords="16,177.36,558.41,240.55,60.67"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Runs</cell><cell>1.0455</cell><cell>4</cell><cell cols="3">0.26138 23.92 1.06341e-17</cell></row><row><cell cols="6">Measures 10.337 97 0.10657 9.75 5.30974e-61</cell></row><row><cell>Error</cell><cell cols="3">4.2404 388 0.01093</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">15.623 489</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="17,88.99,99.57,331.24,88.76"><head>Table 6</head><label>6</label><figDesc>Two-way ANOVA analysis of the 5 runs on nDCG of the HELDOUT set</figDesc><table coords="17,175.05,127.66,245.18,60.67"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Runs</cell><cell>2.9632</cell><cell>4</cell><cell cols="3">0.74081 52.87 1.53768e-35</cell></row><row><cell cols="6">Measures 18.6551 97 0.19232 13.73 1.36549e-80</cell></row><row><cell>Error</cell><cell cols="3">5.4367 388 0.01401</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">27.055 489</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="17,88.99,222.43,326.78,88.76"><head>Table 7</head><label>7</label><figDesc>Two-way ANOVA analysis of the 5 runs on AP of the TEST set</figDesc><table coords="17,179.51,250.52,236.26,60.67"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Runs</cell><cell>13.75</cell><cell>4</cell><cell cols="2">3.43756 228.55</cell><cell>0</cell></row><row><cell cols="4">Measures 121.512 881 0.13792</cell><cell>9.17</cell><cell>0</cell></row><row><cell>Error</cell><cell cols="3">53.002 3524 0.01504</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">188.265 4409</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,88.99,345.28,326.78,88.76"><head>Table 8</head><label>8</label><figDesc>Two-way ANOVA analysis of the 5 runs on nDCG of the TEST set</figDesc><table coords="17,179.51,373.37,236.26,60.67"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Runs</cell><cell>33.172</cell><cell>4</cell><cell cols="2">8.29291 507.17</cell><cell>0</cell></row><row><cell cols="5">Measures 183.696 881 0.20851 12.75</cell><cell>0</cell></row><row><cell>Error</cell><cell cols="3">57.622 3524 0.01635</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">274.49 4409</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="19,112.66,468.78,394.53,10.91;19,112.66,482.33,394.53,10.91;19,112.66,495.88,394.53,10.91;19,112.66,509.43,394.52,10.91;19,112.66,522.98,393.33,10.91;19,112.66,536.53,393.33,10.91;19,112.66,550.08,321.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,112.66,509.43,389.62,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>GaluÅ¡ÄÃ¡kovÃ¡</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,127.14,522.98,378.84,10.91;19,112.66,536.53,327.16,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="19,471.85,536.53,34.14,10.91;19,112.66,550.08,148.46,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaliniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,563.63,261.67,10.91" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="https://clef-longeval.github.io/tasks/" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,577.18,395.17,10.91;19,112.66,590.73,394.52,10.91;19,112.66,604.28,122.77,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03229</idno>
		<title level="m" coord="19,463.43,577.18,44.40,10.91;19,112.66,590.73,389.97,10.91">Longevalretrieval: French-english dynamic test collection for continuous web search evaluation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,617.83,331.37,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,179.04,617.83,73.86,10.91">As we may think</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,261.19,617.83,93.84,10.91">The atlantic monthly</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="101" to="108" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,631.38,394.53,10.91;19,112.28,644.92,393.70,10.91;19,112.41,658.47,338.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,112.28,644.92,347.98,10.91">An exploratory study of information retrieval techniques in domain analysis</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schwanninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rummler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,485.29,644.92,20.70,10.91;19,112.41,658.47,233.66,10.91">2008 12th International Software Product Line Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,86.97,394.61,10.91;20,112.66,100.52,377.47,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<title level="m" coord="20,394.69,86.97,112.57,10.91;20,112.66,100.52,195.31,10.91">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,114.06,118.43,10.91;20,246.53,114.06,260.17,10.91;20,112.66,127.61,154.91,10.91" xml:id="b6">
	<monogr>
		<ptr target="https://www.kaggle.com/code/jerrykuo7727/rescore-bm25-with-bert/notebook" />
		<title level="m" coord="20,112.66,114.06,114.29,10.91">Rescore bm25 with bert</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,141.16,99.63,10.91;20,228.89,141.16,277.09,10.91;20,112.66,154.71,58.55,10.91;20,191.12,154.71,16.75,10.91;20,227.77,154.71,22.68,10.91;20,274.66,154.71,23.14,10.91;20,317.71,154.71,22.39,10.91;20,360.01,154.71,146.69,10.91;20,112.66,168.26,377.51,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stathoulopoulos</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8" />
		<title level="m" coord="20,228.89,141.16,277.09,10.91;20,112.66,154.71,58.55,10.91;20,191.12,154.71,16.75,10.91;20,227.77,154.71,18.90,10.91">How to build a semantic search engine with transformers and faiss</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,181.81,393.33,10.91;20,112.66,195.36,393.33,10.91;20,112.28,208.91,394.91,10.91;20,112.66,222.46,347.80,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,305.76,181.81,200.23,10.91;20,112.66,195.36,161.20,10.91">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="http://doi.acm.org/10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="20,296.62,195.36,209.37,10.91;20,112.28,208.91,342.32,10.91">SIGIR &apos;09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,236.01,393.33,10.91;20,112.66,249.56,311.37,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="20,326.58,236.01,179.40,10.91;20,112.66,249.56,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,263.11,395.01,10.91" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/BERT_(language_model" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>language model</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
