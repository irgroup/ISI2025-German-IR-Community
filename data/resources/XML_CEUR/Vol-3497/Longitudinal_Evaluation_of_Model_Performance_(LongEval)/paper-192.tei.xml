<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,215.33,15.42;1,89.29,129.00,343.84,11.96">SEUPD@CLEF: Team CLOSE on Temporal Persistence of IR Systems&apos; Performance Notebook for the LongEval Lab on Information Retrieval at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,87.15,11.96"><forename type="first">Gianluca</forename><surname>Antolini</surname></persName>
							<email>gianluca.antolini@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.09,154.90,72.39,11.96"><forename type="first">Nicola</forename><surname>Boscolo</surname></persName>
							<email>nicola.boscolocegion.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.12,154.90,70.92,11.96"><forename type="first">Mirco</forename><surname>Cazzaro</surname></persName>
							<email>mirco.cazzaro@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.68,154.90,82.31,11.96"><forename type="first">Marco</forename><surname>Martinelli</surname></persName>
							<email>marco.martinelli.4@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,81.43,11.96"><forename type="first">Seyedreza</forename><surname>Safavi</surname></persName>
							<email>seyedreza.safavi@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.36,168.85,65.66,11.96"><forename type="first">Farzad</forename><surname>Shami</surname></persName>
							<email>farzad.shami@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.02,168.85,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,215.33,15.42;1,89.29,129.00,343.84,11.96">SEUPD@CLEF: Team CLOSE on Temporal Persistence of IR Systems&apos; Performance Notebook for the LongEval Lab on Information Retrieval at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C5B78D21CDC5B6246A3B22E5FA8CF8BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Search Engines</term>
					<term>Longitudinal Evaluation</term>
					<term>Temporal persistence</term>
					<term>Model Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work of the CLOSE group, a team of students from the University of Padua, Italy, for the Conference and Labs of the Evaluation Forum (CLEF) LongEval LAB 2023 Task 1 [1]. Our work involved developing an Information Retrieval (IR) system that can handle changes in data over time while maintaining high performance. We first introduce the problem as stated by CLEF and then describe our system, explaining the different methodologies we implemented. We provide the results of our experiments and analyze them based on the choices we made regarding various techniques. Finally, we propose potential avenues for future improvement of our system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent research has shown that the performance of information retrieval systems can deteriorate over time as the data they are trained on becomes less relevant to current search queries. This problem is particularly acute when dealing with temporal information, as web documents and user search preferences evolve over time. In this paper, we propose a solution to this problem by developing an information retrieval system that can adapt to changes in the data over time, while maintaining high performance.</p><p>Our approach involves using the training data provided by the Qwant <ref type="bibr" coords="1,425.49,492.54,13.00,10.91" target="#b1">[2]</ref> search engine, which includes user searches and web documents in both French and English. We believe that this data will enable our system to better adapt to changes in user search behavior and the content of web documents.</p><p>The remainder of this paper is organized as follows: Section 2 describes our approach in more detail, including the different techniques we used. Section 3 explains our experimental setup, The workflow of our system, starting from the train collection provided by LongEval <ref type="bibr" coords="2,480.74,564.39,11.45,10.91" target="#b0">[1]</ref>, is as follows:</p><p>1. Parsing: The first phase consists of parsing the documents in the collection, which is a pre-processing operation performed to clean them from unnecessary noises. Since the collection is composed of web pages, the documents contain many leftovers like JavaScript scripts, HTML and CSS codes, HTTP and HTTPS URIs, and so on. 2. Indexing: Each parsed document is then analyzed and indexed keeping only the necessary information. Indexed documents are composed of two fields: an id field, containing the identifier of the document in the collection, and a content field, containing the entire body of the document cleaned by the parsing and indexing phases. 3. Query Formulation: Topics are then parsed using the same analyzer used for documents, and used to formulate queries. For each topic, together with the already provided query, around 15 other query variants are generated (through the GPT model) by us and used altogether for searching relevant documents. 4. Re-ranking: Utilizing the sentence transformers model to determine the similarity between the document and the query. This similarity score is then multiplied by the BM25 score, resulting in new ranking scores for the documents. Then the retrieved documents are re-ranked based on these new ranking scores.   The class diagram of the system <ref type="bibr" coords="5,249.48,86.97,13.10,10.91" target="#b2">[3]</ref> retraces the Y model <ref type="bibr" coords="5,362.06,86.97,13.10,10.91" target="#b0">[1]</ref> of an IR system: in fact, it is possible to see it as an indexer class and a searcher class that is, in this order, called by the main class CloseSearchEngine of our system. The Analyzer class is instantiated before all, as it is used both from the two main branches stated before. Here many other Lucene (and Solr for NLP filters trials) tools are instantiated, as long as this component is responsible for analyzing tokens, and here most of our processing phase goes on: in particular, the tokenizer and the stemmer (and some NLP filters used in a trial). The other connected component of the diagram is related to the parsing section: here, while walking on the file tree, the Indexer uses this component to generate, from a JSON document, an actual Java object (ParsedTextDocument) representing it with its fields. The Searcher class firstly parses the queries from the Text REtrieval Conference (TREC) format in a Lucene QualityQuery object, through our ClefQueryParser class. It also instantiates the ReRanker, responsible for the second-ranking phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Class Diagram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Parser</head><p>As stated before, the documents in the collection provided by the CLEF LongEval LAB 2023 <ref type="bibr" coords="5,493.20,312.83,12.79,10.91" target="#b0">[1]</ref> are essentially the corpus of web pages, to better represent the nature of a web test collection. From this, the need for performing a pre-processing phase of parsing the documents before analyzing and indexing them arises. In this phase, the documents are cleaned from all the residuals of codes not useful for our purposes. We first created an abstract class DocumentParser and then extended it by implementing a custom ClefParser class, which contains many functions for removing sundry types of noises that can be present in documents. This was the result of the trial and error approach we adopted for implementing this class:</p><p>• We started with our own read of a large statistical sample size of the documents in the collection to decide which types of noises needed to be removed. • Then we implemented the parser and ran it.</p><p>• The results of the parsing were stored, and a sample of the parsed documents was analyzed to start this procedure again. The types of noises we tried to remove are the following: The final decision about the type of noises to effectively remove for our runs was the most crucial part of this process. For establishing this we used a trial &amp; error approach, and in the end, we decided to remove only the JavaScript scripts and the HTTP and HTTPS URIs. Regarding URI s, although these are usually important because they can contain valuable keywords, we noticed an improvement in Mean Average Precision (MAP) of almost 0.5 points by just removing them. We also identified some patterns of words and symbols to remove:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample of the</head><p>• Two words separated by an underscore, like word1_word2 • Two words separated by a colon, like word1:word2 • Two words separated by a point, like "word1.word2"</p><p>We used Regular Expressions <ref type="bibr" coords="6,217.77,399.75,12.84,10.91" target="#b2">[3]</ref> to identify and remove these patterns.</p><p>The structure of the parsed document is defined in the ParsedTextDocument class, and it is composed of just two fields:</p><p>1. id: the identifier of the document, 2. body: the (parsed) content of the document.</p><p>Inside it, multiple controls about the validity and integrity of the parameters are performed, then an object of the class is instantiated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Analyzer</head><p>The Analyzer is responsible for analyzing the extracted documents and preparing them for the Indexing and Searching phases. It does so by combining a series of techniques of text processing such as tokenization, stemming, stopword removal, and many more. We extended Apache Lucene's Analyzer abstract class <ref type="bibr" coords="6,333.14,586.33,12.89,10.91" target="#b3">[4]</ref> by creating a custom class CloseAnalyzer, which is fully customizable by its parameters that can be chosen when creating an instance of the class. This has been done because we tried different settings and approaches to maximize the results and kept all the possible variations as optional settings. This CloseAnalyzer is passed as a parameter and then used by the DirectoryIndexer and by the Searcher. The constructor of CloseAnalyzer accepts the following parameters:</p><p>• tokenizerType: used to choose between three standard Lucene tokenizers: Whitespace-Tokenizer <ref type="bibr" coords="7,161.56,100.52,11.43,10.91" target="#b4">[5]</ref>, LetterTokenizer <ref type="bibr" coords="7,249.11,100.52,11.43,10.91" target="#b5">[6]</ref>, and StandardTokenizer <ref type="bibr" coords="7,370.25,100.52,11.43,10.91" target="#b6">[7]</ref>. • stemFilterType: the possible choices for the stemming types are four standard Apache Solr <ref type="bibr" coords="7,138.31,128.97,13.00,10.91" target="#b7">[8]</ref> filters: EnglishMinimalStemFilter <ref type="bibr" coords="7,309.82,128.97,11.58,10.91" target="#b8">[9]</ref>, KStemFilter <ref type="bibr" coords="7,386.08,128.97,16.41,10.91" target="#b9">[10]</ref>, PorterStemFilter <ref type="bibr" coords="7,486.67,128.97,16.41,10.91" target="#b10">[11]</ref>,</p><p>and FrenchLightStemFilter <ref type="bibr" coords="7,235.14,142.52,16.29,10.91" target="#b11">[12]</ref>. We also tried using FrenchMinimalStemFilter <ref type="bibr" coords="7,461.05,142.52,17.95,10.91" target="#b12">[13]</ref> and a custom filter called LovinsStemmerFilter based on a LovinsStemmer <ref type="bibr" coords="7,415.49,156.07,17.83,10.91" target="#b13">[14]</ref> implementation but decided to keep them commented as they didn't improve the results. • minLength and maxLength: these are integers that simply specify the minimum and maximum length of a token, applying Lucene's LengthFilter <ref type="bibr" coords="7,384.48,198.07,16.25,10.91" target="#b14">[15]</ref>. • isEnglishPossessiveFilter: specifies whether to use Lucene's EnglishPossessiveFilter <ref type="bibr" coords="7,488.23,212.97,17.76,10.91" target="#b15">[16]</ref> or not. Of course, this can be useful when operating with the English dataset. • stopFilterListName: with this parameter, it's possible to insert the path of an eventual word stoplist .txt file located in the resources folder. To do this we use Lucene's StopFilter <ref type="bibr" coords="7,116.56,268.53,17.98,10.91" target="#b16">[17]</ref> and a custom class called AnalyzerUtil that uses a loadStopList method to read and load all the stoplist words from the specified file. The stoplists we created are based on the standard ones but modified after inspecting the index with the Luke <ref type="bibr" coords="7,446.32,295.62,18.07,10.91" target="#b17">[18]</ref> tool. We have lists of different lengths and different ones for French and English. • Character nGramFilterSize: if specified, this parameter is used to define the size of the n-grams to be applied by Lucene's NGramTokenFilter <ref type="bibr" coords="7,355.35,337.63,16.25,10.91" target="#b18">[19]</ref>. • Word nGramFilterSize: similar to the previous one, if used, this integer number indicates the shingle size to be applied by Lucene's ShingleFilter <ref type="bibr" coords="7,361.01,366.08,17.91,10.91" target="#b19">[20]</ref> that allows the creation of a combination of words. • useNLPFilter: this boolean allows the use of Solr's <ref type="bibr" coords="7,361.70,394.53,13.00,10.91" target="#b7">[8]</ref> OpenNLPPPOSFilter <ref type="bibr" coords="7,471.27,394.53,18.07,10.91" target="#b20">[21]</ref> for Part-Of-Speech Tagging and of a custom class called OpenNLPNERFilter for Named Entity Recognition. To load the .bin models, which are located in the resources folder, we use two methods from AnalyzerUtil: loadPosTaggerModel and loadNerTaggerModel. • lemmatization: specifies whether to use Solr's OpenNLPLemmatizerFilter <ref type="bibr" coords="7,440.64,450.09,17.76,10.91" target="#b21">[22]</ref> by loading a .bin model file in the resources folder using AnalyzerUtil's loadLemmatizerModel function.</p><p>• frenchElisionFilter: we applied this only when using the French dataset by adding Lucene's ElisionFilter <ref type="bibr" coords="7,211.81,492.09,17.80,10.91" target="#b22">[23]</ref> with an array of the following characters: 'l', 'd', 's', 't', 'n', 'm'.</p><p>On top of this, a LowerCaseFilter <ref type="bibr" coords="7,236.43,514.60,17.91,10.91" target="#b23">[24]</ref> is always applied. We also tried Lucene's ASCIIFoldingFilter <ref type="bibr" coords="7,278.56,528.15,18.07,10.91" target="#b24">[25]</ref> and SynonymGraphFilter <ref type="bibr" coords="7,415.14,528.15,16.41,10.91" target="#b25">[26]</ref>. For the second one, only for the French Dataset, we used a SynonymMap <ref type="bibr" coords="7,351.58,541.70,18.05,10.91" target="#b26">[27]</ref> based on a .txt file containing French synonyms.</p><p>After different trials with different variations of the parameters, the following are options used with our CloseAnalyzer implementation: we have opted for the French dataset and by doing so we have the StandardTokenizer, 2 and 15 as minimum and maximum token length, we use frenchElisionFilter, FrenchLightStemFilter, and a list of 662 French words as a stoplist. This stoplist has been built upon a popular French stoplist together with the most frequent stopwords in the collection. We didn't use any of the other parameters. We utilized the Gson library to efficiently parse JSON files that contained query expansions. By leveraging Gson's capabilities, we were able to seamlessly convert the JSON data into Java objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Searcher</head><p>The purpose of the Searcher is to search through the indexed documents to retrieve relevant information based on user queries after analyzing them and to return a ranked list of documents that match the user's information needs. Our implementation does so by accepting the following parameters:</p><p>• analyzer: in this case, an instance of CloseAnalyzer.</p><p>• similarity: we decided to opt for the BM25Similarity <ref type="bibr" coords="8,355.64,185.60,17.88,10.91" target="#b27">[28]</ref> function with the parameters k1 and b tuned at 1.2 and 0.90. • Run options: there are parameters for the index path, the topics path, the run path and the run name, the number of the expected topic (in our case 50), and the maximum number of documents retrieved (in our case 1000). • reRankModel: this is the type of model used to do a Re-Ranking on the retrieved documents. In our case, we use a model called all-MiniLM-L6-v2 <ref type="bibr" coords="8,409.95,269.61,16.40,10.91" target="#b28">[29]</ref>, explained in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Query Expansion</head><p>When running the search function, one of the first actions performed is to generate new queries from the original ones by query expansion <ref type="bibr" coords="8,281.61,346.03,16.25,10.91" target="#b29">[30]</ref>.</p><p>We created a Python script that, given the *.trec topic file, generates all the expanded terms for each query and stores everything in a .json file called result, containing all the expansions. We use OpenAI's Text completion <ref type="bibr" coords="8,240.82,386.68,18.07,10.91" target="#b30">[31]</ref> endpoints to generate the expansions, we can use our need as a prompt and the model will generate the result. We used the davinci model, which is the most powerful one, and we set the temperature parameter to 0.6, which is the value that gives the best results. The sample result for prompt The main difference between davinci-text-002 and davinci-text-003 is that the latter has been trained on a larger dataset, allowing it to generate more accurate results <ref type="bibr" coords="8,412.88,648.90,16.25,10.91" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Expand the following query with num_expansions related terms or phrases for information retrieval (search-engine): query and the result should be in array format</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Query Boosting</head><p>Query boosting is a technique used to assign greater relevance to certain query terms or queries. We tried the following approach that seemed to improve the overall results: when building the queries in the search function of the Searcher (2.5), for each query, a BooleanQuery <ref type="bibr" coords="9,478.02,134.63,17.97,10.91" target="#b32">[33]</ref> is built in the following way: after getting the query expansions, each of them is added to the BooleanQuery with the clause SHOULD (meaning that at least one of them must be satisfied) and a main query is added with the clause MUST, indicating that it must be satisfied. This main query is boosted using Lucene's BoostQuery <ref type="bibr" coords="9,335.54,188.83,16.29,10.91" target="#b33">[34]</ref>, with a boost value tuned at 14.68 multiplied by the number of expansions. We got this value by a trial &amp; error approach we used to fine-tune this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">Document Re-Ranking</head><p>Re-Ranking is the process of ranking documents retrieved by the search function of the Searcher (2.5). To accomplish this task, we utilized sentence transformers <ref type="bibr" coords="9,387.76,278.81,16.41,10.91" target="#b34">[35]</ref>, a Python framework known for its state-of-the-art sentence, text, and image embeddings. Various models were experimented with, and the one yielding the best results was identified as all-MiniLM-L6-v2 <ref type="bibr" coords="9,102.23,319.45,16.09,10.91" target="#b28">[29]</ref>. This particular model aims to train sentence embedding models using a self-supervised contrastive learning objective on vast sentence-level datasets, ultimately mapping sentences and paragraphs to a dense vector space of 384 dimensions.</p><p>To generate embeddings for both the documents and query retrieved by the search function, we loaded the all-MiniLM-L6-v2 model and instantiated a SentenceTransformer object. To calculate similarity, we employed the widely used cosine similarity formula, which computes the similarity between two vectors. The formula is defined as follows:</p><formula xml:id="formula_0" coords="9,255.24,423.60,251.39,25.50">similarity = 𝑡 • 𝑑 𝑖 |𝑡||𝑑 𝑖 |<label>(1)</label></formula><p>Here, 𝑡 represents the query vector, and 𝑑 𝑖 denotes the vector of the 𝑖-th document. Subsequently, we interpolated this similarity score with the BM25 score to improve the ranking of the documents. Among the different approaches we explored, the most successful one was as follows:</p><formula xml:id="formula_1" coords="9,224.92,523.33,281.72,10.91">rank = BM25_score × similarity (2)</formula><p>To visualize the re-ranking operation's effectiveness, we plotted the results for 50 sample queries, as depicted in Figure <ref type="figure" coords="9,221.68,556.91,3.74,10.91" target="#fig_4">5</ref>.</p><p>Finally, we sorted the documents based on the new rank and returned them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Collections</head><p>We developed our model using a collection of 1,593,376 documents and 882 queries provided by Qwant search engine, available at https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/ 1-5010.</p><p>The collection contains information about user web searches and actual web pages corpora. The data was originally all in French but, for both queries and documents, an English translation is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Measures</head><p>To measure the effectiveness of our IR system we used the trec_eval executable by testing it with the resulting runs produced by the model and its different configurations. We tracked improvements of the following evaluation measures generated by trec_eval:</p><p>• num_ret: number of documents retrieved for a given query.</p><p>• num_rel: number of relevant documents for a given query.</p><p>• num _rel_ret: number of relevant documents retrieved for a given query.</p><p>• map: Mean Average Precision, a measure of the average relevance of retrieved documents across all queries. • rprec: R-Precision is the precision score computed at the rank corresponding to the number of relevant documents for a given query.</p><p>• p@5 and p@10: Precision at 5 and at 10 is the precision computed at the top 5 and 10 retrieved documents for a given query. • nDCG: it is a metric used to evaluate ranked lists. It measures the effectiveness of a ranking algorithm by considering item relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Git Repository</head><p>More detailed information about our information retrieval system, including source code, runs, results, homework reports, and presentation slides, can be found in our Git repository at https://bitbucket.org/upd-dei-stud-prj/seupd2223-close/src/master/%7D. The code is available for reproducibility. In this Section, we provide some of the most relevant results we got during the development phase. We are considering five principal milestones that, within many different trials, led us to improve significantly our MAP score and the overall number of relevant documents actually retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Training Data</head><p>First of all, given that we were provided with two different versions of the same document's corpora, our first idea was to try the English version. We noticed that the best combination of basic IR tools was to use the Porter Stemmer <ref type="bibr" coords="11,486.67,597.73,16.41,10.91" target="#b10">[11]</ref>, a length filter from 1 to 10, and a list of stop-word composed by some standard terms and more from the top 600 extracted from the index. The very first big milestone, that helped us to increment the MAP of around 3 points, from 10.1% to 13.1%, was the JavaScript code cleaner since we noticed by inspection that many documents were having these types of scripts inside. Always by inspecting some documents and queries, and also considering that the original collection was the French version (translated then in English), we observed that the translation was very poor: by switching to French by just cleaning the JS code and some other minor cleaning tools, without even using an adequate stop-list and a correct stemmer for the French language, the MAP was increasing by +5%. 2 more MAP points were achieved with a stop list built for French in the same way we did previously for English, the FrenchLightStemFilter <ref type="bibr" coords="12,306.86,503.68,17.84,10.91" target="#b11">[12]</ref> as stemmer, and moving the length filter from 2 to 15 (as we noticed French tends to have longer words). We tried some Natural Language Processing (NLP) techniques for English to see if there were improvements, and in this case, apply them to our main implementation for French with an appropriate model. The obtained results were not interesting, and also the computing time was definitely too costly. In particular, we tried to use Solr OpenNLP Part of Speech Filter <ref type="bibr" coords="12,461.79,571.42,17.75,10.91" target="#b20">[21]</ref> using the en-pos-maxent Part of Speech (PoS) tagger provided by OpenNLP. Another approach we tried and that carried an improvement was to use Query expansion: first we used some generative text models to expand our queries, then we decided to weight different query scores by boosting the original one linearly with respect to the number of expansion used. This made us gain an extra MAP point. We try to generate the embeddings for each document based on word2vector, we use a pre-trained word2vec model frWac_no_postag_no_phrase_500_cbow_cut100 <ref type="bibr" coords="12,435.66,666.27,18.07,10.91" target="#b35">[36]</ref> for French. Then we calculate the embedding for each document and index them as KnnFloatVectorField in Lucene and use KnnFloatVectorQuery <ref type="bibr" coords="13,279.65,382.07,18.06,10.91" target="#b36">[37]</ref> for searching the query to find the k nearest documents to the target vector according to the vectors in the given field, but the results (overall MAP 0.08) were not satisfying, being worse than the case of indexing and searching without embeddings.</p><p>We then tried to combine different similarities rather than using the classic BM25Similarity: we tried to use the Lucene MultiSimilarity <ref type="bibr" coords="13,267.45,449.81,16.41,10.91" target="#b37">[38]</ref>, that allows combining the score of two or more similarity scores, but it does not allow to tune the weights. Then, we tried to reimplement the MultiSimilarity class with tuning options, but the results were always lower than the standard BM25Similarity. Some minor improvements came up by fine-tuning the documentlength normalization b parameter and the term frequency component k1 parameter of the BM25Similarity.</p><p>The last main implementation we did, was to use some Re-ranking techniques to improve the results of the first retrieval phase. We tried to use the SBERT model <ref type="bibr" coords="13,386.86,544.66,16.10,10.91" target="#b38">[39]</ref>, which is a pre-trained model for sentence embeddings, and we used it to calculate the similarity between the query and the document. We tried to use different distance metrics such as CosineSimilarity <ref type="bibr" coords="13,469.09,571.75,17.86,10.91" target="#b39">[40]</ref> and ManhattanDistance [41] for calculating the similarity, but at the end of the day, CosineSimilarity is much better than others. Finally, we sort the documents based on merging the BM25 score and similarity score into one score by multiplying them together.</p><p>Lastly, some minor adding were set on the Analyzer (see Section 2.4) by implementing the Lucene ElisionFilter (for French) <ref type="bibr" coords="13,260.55,639.50,16.41,10.91" target="#b22">[23]</ref>, which aims to remove apostrophes articles and prepositions from tokens (for example, m'appelle and t'appelle become the same token appelle). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Test Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Statistical Analysis</head><p>In this section, we delve into a comprehensive analysis of the retrieval effectiveness for our 5 different runs submitted to CLEF on both the French and English collections (see Table <ref type="table" coords="14,496.72,561.19,3.65,10.91" target="#tab_3">1</ref>). The analysis aims to evaluate the performance of different runs and provides insights into the effectiveness of the system in retrieving and ranking relevant documents. We begin by analyzing the results obtained from the French collection. The overall Normalized Discounted Cumulated Gain (nDCG) and MAP comparison allows us to gain an initial understanding of the performance differences between runs 2, 3, and 5, considering short-term, heldout, and long-term evaluations. By examining the nDCG scores and Relative nDCG Drop (RnD) values, we can identify the run that demonstrates the highest effectiveness in retrieving relevant documents.</p><p>Furthermore, we employ two-way ANalysis Of VAriance (ANOVA) tests to investigate the significance of the observed differences in the long-term and short-term evaluations. Along with this, the analysis also includes the implementation of the Tukey HSD test, a post-hoc analysis for ANOVA that compares group means while controlling for multiple comparisons, ensuring reliable identification of significant differences.</p><p>This analysis provides valuable insights into the performance of the IR system on the French collection, guiding us in identifying areas for improvement.</p><p>Subsequently, we shift our focus to the analysis of results obtained from the English collection. We conduct an analogous evaluation, comparing the performance of runs 1 and 4 in the same way as stated above for the French collection.</p><p>Still, we put here an extract of the first three columns of Table <ref type="table" coords="15,375.99,222.46,3.69,10.91" target="#tab_5">3</ref>, which can be seen as a sort of recap to have at hand of our runs submitted to CLEF: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Analysis of Results on the French Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Overall nDCG Comparison</head><p>The overall nDCG comparison of runs 2, 3, and 5 on the French collection is presented in Figure <ref type="figure" coords="15,120.95,463.99,3.81,10.91" target="#fig_6">7</ref>. This boxplot provides valuable insights into the performance of each run, with the bars representing the nDCG scores for each run. The color coding distinguishes the different evaluation periods, with short-term depicted in green, heldout in orange, and long-term in pink.</p><p>The delta values within the boxes represent the RnD of each run compared to the heldout run.</p><p>Analyzing the overall nDCG comparison, we observe that runs 3 and 5 consistently outperform run 2 across both short and long-term evaluations. These runs achieve higher nDCG scores, indicating their superior effectiveness in capturing and ranking relevant documents. The fact that both runs 3 and 5 exhibit similar levels of performance suggests comparable retrieval capabilities among these two.</p><p>However, when considering the RnD values, we observe some variations among the runs. Run 5 demonstrates a noticeable drop in nDCG compared to the heldout run, as indicated by the RnD delta values. This indicates a potential decrease in retrieval performance when transitioning from the heldout period to the short and long term.</p><p>On the other hand, run 3 displays a relatively smaller drop in nDCG compared to the heldout run, suggesting greater stability and consistency in performance. In contrast, run 2 exhibits relatively lower nDCG scores, particularly in the long-term evaluation. This can be attributed to its more greedy approach, which might compromise its ability to retrieve relevant documents as the collection evolves over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Overall MAP Comparison</head><p>The boxplot shown in Figure <ref type="figure" coords="16,224.98,478.30,5.17,10.91" target="#fig_7">8</ref> presents the overall MAP comparison of runs 2, 3, and 5 on the French Collection. The notation for the used colors is the same as for Figure <ref type="figure" coords="16,454.11,491.85,3.81,10.91" target="#fig_6">7</ref>. From the boxplot, we can observe the distribution of MAP scores for each run and collection type. The height of each box indicates the Interquartile Range (IQR), representing the range of the middle 50% of the data. The horizontal line within each box corresponds to the median value, while the whiskers above and below the box extend to the highest and lowest values within 1.5 times the IQR.</p><p>Analyzing the overall MAP comparison, we observe that runs 3 and 5 consistently outperform run 2 across both short and long-term evaluations, similarly to what we have seen in Section 5.1.1. These runs achieve higher MAP scores, indicating their superior effectiveness in accuracy.</p><p>However, we can see that run 5 demonstrates a noticeable drop in MAP of the long-term compared to the heldout run. This indicates a potential decrease in retrieval performance when transitioning from the heldout period to the long term. On the other hand, run 3 displays a relatively smaller drop in MAP compared to the heldout run, suggesting greater stability and consistency in capturing relevant documents over time. These results, together with the ones we have seen on nDCG in Section 5.1.1, suggests that the changes done from run 3 to run 5 for sure led to improvements, but they have to be better optimized towards temporal persistence.</p><p>In contrast, run 2 exhibits relatively lower MAP scores, particularly in the long-term evaluation. The reasons to which this can be attributed are the same stated in Section 5.1.1.</p><p>Overall, these results confirm what we found in Section 5.1.1: runs 3 and 5 are competitive with each other, while run 2 exhibits lower performances compared to these, due to its more basic implementation (see Table <ref type="table" coords="17,234.39,496.72,3.57,10.91" target="#tab_3">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Two-way ANOVA on the Long Term French Collection</head><p>The results of the two-way ANOVA conducted on the Long Term French Collection are depicted in Figure <ref type="figure" coords="17,131.68,559.60,3.71,10.91" target="#fig_8">9</ref>, which showcases the nDCG and Average Precision (AP) scores for run 2 in red, run 3 in grey, and run 5 in blue. These plots enable us to examine the effects of both the run choice and the long-term evaluation on the performances.</p><p>Analyzing the two-way ANOVA plot above, at first view we can notice, as expected, that there is no interaction among the three runs.</p><p>Then, we observe that run 5 consistently achieves the highest nDCG and AP scores across different evaluation points. Run 2 exhibits lower performance compared to the other runs, while run 3 shows a relatively stable performance, albeit slightly lower than Run 5, confirming what we found in Sections 5.1.1 and 5.1.2. In addition, the overlap we find among the columns of run 3 and run 5 suggests that the small changes done among these two (see Table <ref type="table" coords="18,442.70,298.63,4.25,10.91" target="#tab_3">1</ref>) led to some small albeit noticeable improvements. These findings suggest that both the choice of run and the long-term evaluation have a significant impact on the overall nDCG and AP scores obtained.</p><p>To complement the two-way ANOVA plots, we refer to Tables <ref type="table" coords="18,376.48,339.28,5.00,10.91" target="#tab_7">6</ref> and<ref type="table" coords="18,403.10,339.28,3.68,10.91" target="#tab_8">7</ref>, that provide multiple comparisons between the nDCG and AP scores, variance, and p-values associated with runs 2, 3, and 5.</p><p>The tables present the results of the pairwise comparisons between different runs. The "Run A" and "Run B" columns indicate the runs being compared. The "Lower Limit" and "Upper Limit" columns represent the lower and upper bounds of the confidence interval, while the "A-B" column indicates the mean difference between the runs. The "P-value" column displays the statistical significance of the comparison, indicating the level of significance and suggesting whether there is a significant difference in performance between runs.  Looking at Table <ref type="table" coords="18,181.82,669.58,3.81,10.91" target="#tab_7">6</ref>, the comparison between run 5 and run 3 yields a p-value of 0.1661, indicating that the mean difference in nDCG scores between these runs is not statistically significant. Similarly, the comparison between run 5 and run 2 results in an extremely low p-value of 1.16 • 10 -21 , suggesting a highly significant difference in nDCG scores. Finally, the comparison between run 3 and run 2 also demonstrates a remarkably low p-value of 3.84 • 10 -14 , indicating a significant discrepancy in their nDCG scores.</p><p>The results displayed in Table <ref type="table" coords="19,239.83,154.71,5.17,10.91" target="#tab_8">7</ref> don't say anything new except to confirm what is stated above.</p><p>Finally, the p-values obtained from the comparisons reinforce the observations made from the two-way ANOVA plot, highlighting the superior performance of Run 5 and the relatively stable performance of Run 3 compared to the other runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Two-way ANOVA on the Short Term French Collection</head><p>Similarly, the results of the two-way ANOVA for nDCG and AP performed on the Short Term French Collection are visualized in Figure <ref type="figure" coords="19,277.37,271.79,8.36,10.91" target="#fig_9">10</ref>. Looking at Figure <ref type="figure" coords="19,181.10,501.89,3.70,10.91" target="#fig_8">9</ref>, we can see that the results above obtained on the short-term Collection are almost identical. Therefore, the considerations made in Section 5.1.3 hold in almost the same way for the results the system got on short-term Collection.</p><p>To further investigate the statistical differences among the runs, we refer to the multiple comparisons Tables <ref type="table" coords="19,179.47,556.09,5.07,10.91" target="#tab_9">8</ref> and<ref type="table" coords="19,206.41,556.09,5.07,10.91" target="#tab_10">9</ref> shown below, which uses the same notation as Table <ref type="table" coords="19,451.55,556.09,3.82,10.91" target="#tab_7">6</ref>:  Looking at Table <ref type="table" coords="20,182.78,188.06,3.81,10.91" target="#tab_9">8</ref>, the comparison between run 5 and run 3 yields a p-value of 0.355, indicating that the mean difference in nDCG scores between these runs is not statistically significant. Similarly, the comparison between run 5 and run 2 results in a remarkably low p-value of 3.86 • 10 -12 , suggesting a highly significant difference in nDCG scores. Finally, the comparison between run 3 and run 2 also demonstrates a low p-value of 3.33 • 10 -8 , indicating a significant discrepancy in their nDCG scores.</p><p>In summary, the statistical analysis of the results obtained on the French collection revealed that run 5 is the most effective one among the three, showing a noticeable overall strength in retrieving and ranking relevant documents. This was easily predictable since this is the last and most elaborate and promising implementation we did for the French Collection, as discussed in Section 4. Anyway, by boxplots in Sections 5.1.1 and 5.1.2 emerged that our IR system can still be optimized towards persistence over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Results on the English Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Overall nDCG Comparison</head><p>The overall nDCG comparison for runs 1 and 4 on the English collection is depicted in Figure <ref type="figure" coords="20,89.04,420.95,8.36,10.91" target="#fig_10">11</ref>, which uses the same notation as Figure <ref type="figure" coords="20,282.26,420.95,3.74,10.91" target="#fig_6">7</ref>.</p><p>Upon analyzing the overall nDCG comparison, we first notice that the results on the English Collection are notably worse compared to those observed for the French Collection in Section 5.1.1. This discrepancy is not unexpected, since we focused most of our work into improving the retrieval and ranking performances of our IR system on the French Collection.</p><p>Examining the boxplot in Figure <ref type="figure" coords="20,252.44,488.69,8.53,10.91" target="#fig_10">11</ref>, we can observe that run 4 consistently outperforms run 1 and achieves higher nDCG scores across both the Short and Long Term evaluations on the English Collection. The clear separation between the two runs in the boxplot suggests significant differences in their retrieval performance, indicating the superior effectiveness of Run 4 in retrieving relevant documents.</p><p>Furthermore, by considering the RnD deltas represented by the values inside the boxes, we can observe that run 4 exhibits a good level of stability between the Short and Long Term evaluations, with minimal changes in its nDCG scores. On the other hand, run 1 shows a notable increase in RnD from the Short to the Long Term, more than doubling the delta value. This indicates that run 1's performance significantly deteriorates when transitioning from the Short to the Long Term evaluation of the English Collection.</p><p>In summary, the overall nDCG comparison confirms that run 4 performs better than run 1 in all evaluation settings on the English Collection. Run 4 demonstrates higher nDCG scores and exhibits greater stability between the Short and Long Term evaluations, while run 1 experiences a noticeable decline in performance when moving from the Short to the Long Term. These findings highlight the importance of the improvements achieved in run 4 and emphasize its superior retrieval effectiveness compared to run 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Overall MAP Comparison</head><p>We now turn our attention to the overall MAP comparison of runs 1 and 4 on the English collection, as illustrated in Figure <ref type="figure" coords="21,242.89,491.85,8.52,10.91" target="#fig_11">12</ref>. The boxplot showcases the distribution of MAP scores, using the same notation as Figure <ref type="figure" coords="21,242.19,505.40,3.74,10.91" target="#fig_7">8</ref>.</p><p>Analyzing the boxplot in Figure <ref type="figure" coords="21,247.99,518.95,8.53,10.91" target="#fig_11">12</ref>, we observe that run 4 consistently outperforms run 1 on all three collections: Short Term, Heldout, and Long Term. Run 4 exhibits higher median MAP scores in both the Short Term and Long Term evaluations, indicating its better overall effectiveness in retrieving relevant documents on the English Collection.</p><p>However, in terms of the Heldout Collection, both runs 1 and 4 display comparable median MAP scores, indicating similar retrieval performance within this collection.</p><p>It's also noticeable how run 1 slightly reaches a MAP score of 20%, while run 4 is able to reach around 30% on all the time periods considered.</p><p>These findings reinforce the conclusions drawn in Section 5.2.1 regarding the superior performance of run 4 compared to run 1 on the English Collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Two-way ANOVA on the Long Term English Collection</head><p>The results of the two-way ANOVA performed on the Long Term English Collection are shown in Figure <ref type="figure" coords="22,130.93,435.99,8.20,10.91" target="#fig_12">13</ref>, which illustrates the nDCG and AP scores for Run 1 in red on the left and Run 4 in blue on the right. At first, we can see that, as expected, there is no interaction between these two runs. To reinforce this, we can also see that there is no overlap between their column means. This was fairly expectable, since the improvements done from run 1 to run 4 are numerous (see Table <ref type="table" coords="23,497.03,114.06,3.54,10.91" target="#tab_3">1</ref>).</p><p>Continuing into analyzing the two-way ANOVA plot, we find that run 4 achieves higher nDCG scores compared to run 1 across different evaluation points. This suggests the superiority of run 4 in retrieving relevant documents in the long term on the English collection, keep confirming what we found in Section 5.2.1.</p><p>To provide additional statistical information, we refer to Tables <ref type="table" coords="23,380.16,181.81,10.00,10.91" target="#tab_11">10</ref> and<ref type="table" coords="23,411.79,181.81,8.25,10.91" target="#tab_12">11</ref>, which present the results of multiple comparisons between run 1 and run 4 for the Long Term English Collection. The tables show the lower limit, A-B difference, upper limit, and p-value for the comparisons. In this case, the table presents a single row since it consists in only one comparison between two runs.  Table <ref type="table" coords="23,126.04,411.90,9.94,10.91" target="#tab_11">10</ref> suggests a significant difference between run 1 and run 4 in terms of nDCG scores for the Long Term English Collection. The positive A-B difference indicates that run 4 consistently achieves higher nDCG scores than run 1 across different evaluation points. This consideration is further supported by the small p-value of 6.77 × 10 -25 .</p><p>These findings further support the conclusion that run 4 performs better than run 1 in terms of retrieving relevant documents over the long term on the English collection, as discussed in Sections 5.2.1 and 5.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Two-way ANOVA on the Short Term English Collection</head><p>Similarly, the results of the two-way ANOVA for nDCG and AP performed on the Short Term English Collection are depicted in Figure <ref type="figure" coords="23,273.45,556.07,8.36,10.91" target="#fig_13">14</ref>.</p><p>These results are really close to the ones shown in FIgure <ref type="figure" coords="23,356.93,569.62,8.32,10.91" target="#fig_12">13</ref>, therefore we can consider this as confirming everything we have found until now, indicating the superiority of run 4 on both short-term and long-term Collection.</p><p>To provide additional statistical information, Tables <ref type="table" coords="23,342.05,610.27,10.35,10.91" target="#tab_13">12</ref> and<ref type="table" coords="23,376.21,610.27,10.35,10.91" target="#tab_5">13</ref> present the results of the multiple comparisons between run 1 and run 4.</p><p>Table <ref type="table" coords="23,127.23,637.37,10.23,10.91" target="#tab_13">12</ref> shows that there is a significant difference in the nDCG scores between run 1 and run 4 on the Short Term English Collection. The A-B difference, which represents the difference  in nDCG scores between the two runs, is 0.0564, indicating that run 4 performs substantially better than run 1. The p-value of zero further supports the significance of this difference. These findings consistently demonstrate the superiority of run 4 over run 1 in terms of nDCG and AP scores in both short and long-term evaluations of the English collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Final Considerations on Statistical Analysis</head><p>In this section, we conducted a comprehensive statistical analysis of the retrieval performance of our system on both the French and English collections. We focused on evaluating the overall nDCG and MAP scores, as well as conducting two-way ANOVA tests to examine the effects of different factors on the performance.</p><p>For the French collection, the analysis revealed that run 5 consistently exhibited slightly better or comparable performance compared to run 3 across all evaluation points. This suggests that the modifications in the stoplist made in run 5 resulted in improved retrieval effectiveness. Still, we noticed this change introduced some instability in the temporal persistence of the system, so we cannot be sure a further expansion of the stoplist can be a promising option for future development.</p><p>On the other hand, run 2 displayed significantly worse performance compared to the other runs. Recalling that run 3 and run 5 are basically improved versions of run 1 which implements query expansion and query re-ranking, we can see this as a good result: the implemented changes had a significant positive impact in our IR system. Turning to the English collection, we focused on comparing runs 1 and 4. The results consistently demonstrated that run 4 outperformed run 1 in every aspect of the analysis. It achieved higher nDCG and lower RnD scores, indicating its superior effectiveness in retrieving relevant documents, remaining persistent at changes over time. Additionally, run 4 displayed a higher median MAP score, confirming its superior overall performance in both short and long windows of time. These findings establish run 4 as the more successful option for retrieval on the English collection. This is an expectable result, since it confirms what we have seen with the French collection: the implementation of query expansion and query re-ranking improved by a lot the performances of our, at the time greedy and basic, IR system.</p><p>The two-way ANOVA tests further supported our conclusions. In the French collection, the two-way ANOVA analysis confirmed the superiority of run 5 over run 3 in terms of nDCG scores. This finding aligns with the overall comparison results.</p><p>In the English collection, the two-way ANOVA did not provide any new unexpected result, as the performance differences between run 1 and run 4 were consistently evident in all previous aspects of the analysis.</p><p>In summary, for the French collection run 5 exhibited slightly better or comparable performance compared to run 3, while run 2 displayed significantly worse performance. On the other hand, for the English collection run 4 consistently outperformed run 1 in every aspect of the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this work, we presented our approach to the CLEF Long Eval LAB 2023 task, which aimed to develop an effective and efficient search engine for web documents. Our approach consisted of using a combination of different techniques, including query expansion, re-ranking, and the use of large language models such as ChatGPT and SBERT. Our experiments showed that our approach achieved good results in terms of effectiveness and efficiency, outperforming the baseline system provided by CLEF. Specifically, we found that combining two different scores in the re-ranking phase led to significant improvements in the retrieval performance. Moreover, we identified several areas for future work that could further improve the effectiveness and efficiency of our approach. One possible direction for future work is to find better ways to combine scores or add other scores to the re-ranking phase. We plan to explore different combinations of scores and investigate the use of other large language models, such as other available BERT models trained, or to train some specifically for this task. Another area for future work is to find better prompts <ref type="bibr" coords="25,336.39,606.27,18.04,10.91" target="#b40">[42]</ref> to use in ChatGPT for improving query expansion. We also plan to investigate the use of other Large Language Model (LLM) techniques for query expansion. We also want to explore ways to increase the similarity in SBERT <ref type="bibr" coords="25,381.04,646.91,16.22,10.91" target="#b38">[39]</ref>, to increase the number of relevant documents found in the re-ranking phase. One possible approach is to fine-tune the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,534.74,143.64,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Apache Lucene Y Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,430.03,254.08,8.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Workflow of the IR system implemented by CLOSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,628.98,278.54,8.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diagram of the classes implemented by CLOSE IR system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,634.99,207.13,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Workflow of the parser implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,330.48,343.13,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Plot illustrating the re-ranking operation performed on 50 sample queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,89.29,330.91,238.34,8.93;13,89.29,84.19,416.70,234.16"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Standard Recall Levels vs Interpolated Precision</figDesc><graphic coords="13,89.29,84.19,416.70,234.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,89.29,364.27,335.93,8.93;16,130.96,84.19,333.37,267.52"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overall nDCG Comparison for runs 2, 3, and 5 on the French Collection</figDesc><graphic coords="16,130.96,84.19,333.37,267.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,89.29,364.27,326.72,8.93;17,130.96,84.19,333.37,267.52"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Overall MAP Comparison of runs 2, 3, and 5 on the French Collection</figDesc><graphic coords="17,130.96,84.19,333.37,267.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="18,89.29,261.94,291.68,8.93;18,89.29,84.19,204.18,153.23"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two-way ANOVA plots for the runs on the French Collection</figDesc><graphic coords="18,89.29,84.19,204.18,153.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="19,89.29,472.24,350.34,8.93;19,89.29,294.50,204.18,153.23"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Two-way ANOVA for runs 2, 3, and 5 on the Short Term French Collection</figDesc><graphic coords="19,89.29,294.50,204.18,153.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="21,89.29,364.27,331.65,8.93;21,130.96,84.19,333.37,267.52"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Overall nDCG Comparison for runs 1 and 4 on the English Collection</figDesc><graphic coords="21,130.96,84.19,333.37,267.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="22,89.29,364.27,322.45,8.93;22,130.96,84.19,333.37,267.52"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Overall MAP Comparison of runs 1 and 4 on the English Collection</figDesc><graphic coords="22,130.96,84.19,333.37,267.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="22,89.29,650.00,338.98,8.93;22,89.29,472.25,204.18,153.23"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Two-way ANOVA for runs 1 and 4 on the Long Term English Collection</figDesc><graphic coords="22,89.29,472.25,204.18,153.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="24,89.29,261.94,340.95,8.93;24,89.29,84.19,204.18,153.23"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Two-way ANOVA for runs 1 and 4 on the Short Term English Collection</figDesc><graphic coords="24,89.29,84.19,204.18,153.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,107.28,86.97,169.36,145.05"><head>•</head><label></label><figDesc>JavaScript scripts, • HTTP and HTTPS URIs, • HTML tags and CSS stylesheets, • XML and JSON codes,</figDesc><table coords="6,107.28,146.58,169.36,85.43"><row><cell>• Meta tags and document properties,</cell></row><row><cell>• Navigation menus,</cell></row><row><cell>• Advertisements,</cell></row><row><cell>• Footers,</cell></row><row><cell>• Social media handlers,</cell></row><row><cell>• Hashtags and mentions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.99,291.14,409.50,169.43"><head>Table 1</head><label>1</label><figDesc>Parameters used in the 5 different runs submitted to CLEF</figDesc><table coords="11,95.67,316.96,402.83,143.61"><row><cell>Parameter</cell><cell>Run 1</cell><cell>Run 2</cell><cell>Run 3</cell><cell>Run 4</cell><cell>Run 5</cell></row><row><cell>Token Filter</cell><cell>Porter-</cell><cell>FrenchLight-</cell><cell>FrenchLight-</cell><cell>PorterStem-</cell><cell>FrenchLight-</cell></row><row><cell></cell><cell>StemFilter</cell><cell>StemFilter</cell><cell>StemFilter</cell><cell>Filter</cell><cell>StemFilter</cell></row><row><cell>Tokenizer</cell><cell>Standard</cell><cell>Standard</cell><cell>Standard</cell><cell>Standard</cell><cell>Standard</cell></row><row><cell>Length Filter</cell><cell>2-15</cell><cell>2-15</cell><cell>2-15</cell><cell>2-15</cell><cell>2-15</cell></row><row><cell>Stop Filter</cell><cell>"long-</cell><cell>"long-</cell><cell>"long-</cell><cell>"long-</cell><cell>"new-long-</cell></row><row><cell></cell><cell>stoplist.txt"</cell><cell>stoplist-</cell><cell>stoplist-</cell><cell>stoplist.txt"</cell><cell>stoplist-</cell></row><row><cell></cell><cell></cell><cell>fr.txt"</cell><cell>fr.txt"</cell><cell></cell><cell>fr.txt"</cell></row><row><cell cols="2">Lower Case Filter Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Similarity</cell><cell>BM25</cell><cell>BM25</cell><cell>BM25</cell><cell>BM25</cell><cell>BM25</cell></row><row><cell cols="2">Query Expansion No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Re-ranking</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.99,90.49,414.83,321.09"><head>Table 2</head><label>2</label><figDesc>results for systems (top-1000 documents), on the Train collection and the train query set of LongEval.</figDesc><table coords="12,155.50,118.58,281.78,293.00"><row><cell>metrics</cell><cell>run1</cell><cell>run2</cell><cell>run3</cell><cell>run4</cell><cell>run5</cell></row><row><cell>num_q</cell><cell>657</cell><cell>669</cell><cell>669</cell><cell>667</cell><cell>667</cell></row><row><cell>num_ret</cell><cell cols="5">646525 658446 658347 652222 657903</cell></row><row><cell>num_rel</cell><cell>2550</cell><cell>2611</cell><cell>2611</cell><cell>2603</cell><cell>2600</cell></row><row><cell>num_rel_ret</cell><cell>1772</cell><cell>2182</cell><cell>2191</cell><cell>1866</cell><cell>2232</cell></row><row><cell>map</cell><cell cols="5">0.1307 0.2022 0.2335 0.1856 0.2351</cell></row><row><cell>Rprec</cell><cell cols="5">0.1041 0.1697 0.1989 0.1654 0.2022</cell></row><row><cell cols="6">iprec_at_recall_0.00 0.2553 0.3499 0.4134 0.3584 0.4182</cell></row><row><cell cols="6">iprec_at_recall_0.20 0.2387 0.3324 0.3873 0.3353 0.3927</cell></row><row><cell cols="6">iprec_at_recall_0.40 0.1441 0.2316 0.2732 0.2066 0.2716</cell></row><row><cell cols="6">iprec_at_recall_0.60 0.0965 0.1786 0.1996 0.1388 0.2014</cell></row><row><cell cols="6">iprec_at_recall_0.80 0.0628 0.1178 0.1311 0.0887 0.1295</cell></row><row><cell cols="6">iprec_at_recall_1.00 0.0525 0.0954 0.1031 0.0704 0.1028</cell></row><row><cell>P_10</cell><cell cols="5">0.0848 0.1296 0.1435 0.1126 0.1432</cell></row><row><cell>P_100</cell><cell cols="5">0.0186 0.0256 0.0268 0.0222 0.0268</cell></row><row><cell>P_1000</cell><cell cols="5">0.0027 0.0033 0.0033 0.0028 0.0033</cell></row><row><cell>recall_10</cell><cell cols="2">0.2166 0.3352</cell><cell>0.367</cell><cell cols="2">0.2849 0.3621</cell></row><row><cell>recall_100</cell><cell cols="5">0.4718 0.6426 0.6714 0.5536 0.6723</cell></row><row><cell>recall_1000</cell><cell cols="5">0.6816 0.8192 0.8218 0.7004 0.8392</cell></row><row><cell>ndcg</cell><cell cols="5">0.2719 0.3655 0.3924 0.3291 0.3982</cell></row><row><cell>ndcg_cut_5</cell><cell cols="5">0.1285 0.1908 0.2232 0.1854 0.2269</cell></row><row><cell>ndcg_cut_10</cell><cell cols="5">0.1609 0.2426 0.2739 0.2227 0.2758</cell></row><row><cell>ndcg_cut_100</cell><cell cols="5">0.2351 0.3349 0.3652 0.3016 0.3678</cell></row><row><cell>ndcg_cut_1000</cell><cell cols="5">0.2719 0.3655 0.3924 0.3291 0.3982</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.99,119.58,417.00,365.83"><head>Table 3</head><label>3</label><figDesc>results for systems (top-1000 documents), on the Train collection and the heldout query set of LongEval provied by LongEval.</figDesc><table coords="14,280.65,162.02,31.49,8.87"><row><cell>heldout</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,88.99,265.02,297.36,112.27"><head>Table 5</head><label>5</label><figDesc>Recap of our runs submitted to CLEF.</figDesc><table coords="15,206.43,295.50,179.93,81.79"><row><cell></cell><cell></cell><cell>heldout</cell></row><row><cell cols="2">run language</cell><cell>type</cell></row><row><cell>run2</cell><cell>FR</cell><cell>QUEREXPANSION</cell></row><row><cell>run3</cell><cell>FR</cell><cell>RERANKING</cell></row><row><cell>run5</cell><cell>FR</cell><cell>SBERT_BM25</cell></row><row><cell>run1</cell><cell>EN</cell><cell>JSCLEANER_BM25</cell></row><row><cell>run4</cell><cell>EN</cell><cell>RERANKING_ENGLISH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="18,88.99,476.23,350.42,73.22"><head>Table 6 nDCG</head><label>6</label><figDesc>Multiple Comparisons for runs 2, 3, and 5 on the Long Term French Collection</figDesc><table coords="18,155.37,504.32,284.05,45.13"><row><cell cols="3">Run A Run B Lower Limit</cell><cell>A-B</cell><cell>Upper Limit</cell><cell>P-value</cell></row><row><cell>5</cell><cell>3</cell><cell>-0.0023</cell><cell>0.0077</cell><cell>0.0177</cell><cell>0.1661</cell></row><row><cell>5</cell><cell>2</cell><cell>0.0305</cell><cell>0.0405</cell><cell>0.0505</cell><cell>1.16 • 10 -21</cell></row><row><cell>3</cell><cell>2</cell><cell>0.0228</cell><cell>0.0328</cell><cell>0.0428</cell><cell>3.84 • 10 -14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="18,88.98,580.41,350.43,73.22"><head>Table 7</head><label>7</label><figDesc>AP Multiple Comparisons for runs 2, 3, and 5 on the Long Term French Collection</figDesc><table coords="18,155.37,608.50,284.05,45.13"><row><cell cols="3">Run A Run B Lower Limit</cell><cell>A-B</cell><cell>Upper Limit</cell><cell>P-value</cell></row><row><cell>5</cell><cell>3</cell><cell>-0.0057</cell><cell>0.0049</cell><cell>0.0154</cell><cell>0.523</cell></row><row><cell>5</cell><cell>2</cell><cell>0.0265</cell><cell>0.037</cell><cell>0.0476</cell><cell>3.57 • 10 -16</cell></row><row><cell>3</cell><cell>2</cell><cell>0.0216</cell><cell>0.0322</cell><cell>0.0427</cell><cell>2.50 • 10 -12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="19,88.99,585.10,351.25,73.22"><head>Table 8 nDCG</head><label>8</label><figDesc>Multiple Comparisons for runs 2, 3, and 5 on the Short Term French Collection</figDesc><table coords="19,154.54,613.19,285.71,45.13"><row><cell cols="3">Run A Run B Lower Limit</cell><cell>A-B</cell><cell>Upper Limit</cell><cell>P-value</cell></row><row><cell>5</cell><cell>3</cell><cell>-0.0042</cell><cell>0.0067</cell><cell>0.0177</cell><cell>0.3215</cell></row><row><cell>5</cell><cell>2</cell><cell>0.0232</cell><cell>0.0341</cell><cell>0.0451</cell><cell>7, 97 • 10 -13</cell></row><row><cell>3</cell><cell>2</cell><cell>0.0164</cell><cell>0.0274</cell><cell>0.0384</cell><cell>1.39 • 10 -8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="20,88.98,90.49,350.94,73.22"><head>Table 9</head><label>9</label><figDesc>AP Multiple Comparisons for runs 2, 3, and 5 on the Long Term French Collection</figDesc><table coords="20,154.86,118.58,285.06,45.13"><row><cell cols="3">Run A Run B Lower Limit</cell><cell>A-B</cell><cell>Upper Limit</cell><cell>P-value</cell></row><row><cell>5</cell><cell>3</cell><cell>-0.0095</cell><cell>0.0027</cell><cell>0.0148</cell><cell>0.8631</cell></row><row><cell>5</cell><cell>2</cell><cell>0.0203</cell><cell>0.0324</cell><cell>0.0445</cell><cell>1.18 × 10 -9</cell></row><row><cell>3</cell><cell>2</cell><cell>0.0176</cell><cell>0.0297</cell><cell>0.0419</cell><cell>2.87 × 10 -8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="23,88.99,262.57,370.26,49.31"><head>Table 10 nDCG</head><label>10</label><figDesc>Multiple comparisons for the Long Term English Collection</figDesc><table coords="23,135.53,290.66,323.72,21.22"><row><cell cols="5">Run A Run B Lower Limit A-B Difference Upper Limit</cell><cell>P-value</cell></row><row><cell>4</cell><cell>1</cell><cell>0.0409</cell><cell>0.0508</cell><cell>0.0608</cell><cell>1.49 × 10 -24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="23,88.98,343.74,370.27,49.31"><head>Table 11 AP</head><label>11</label><figDesc>Multiple comparisons for the Long Term English Collection</figDesc><table coords="23,135.53,371.83,323.72,21.22"><row><cell cols="5">Run A Run B Lower Limit A-B Difference Upper Limit</cell><cell>P-value</cell></row><row><cell>4</cell><cell>1</cell><cell>0.0364</cell><cell>0.0464</cell><cell>0.0565</cell><cell>4.44 × 10 -20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="24,88.99,295.09,370.26,118.53"><head>Table 12</head><label>12</label><figDesc>Multiple comparisons for the Short Term English Collection</figDesc><table coords="24,88.99,323.18,370.26,90.44"><row><cell cols="6">Run A Run B Lower Limit A-B Difference Upper Limit P-value</cell></row><row><cell>4</cell><cell>1</cell><cell>0.0458</cell><cell>0.0564</cell><cell>0.0671</cell><cell>0.00</cell></row><row><cell>Table 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Multiple comparisons for the Short Term English Collection</cell><cell></cell><cell></cell></row><row><cell cols="5">Run A Run B Lower Limit A-B Difference Upper Limit</cell><cell>P-value</cell></row><row><cell>4</cell><cell>1</cell><cell>0.0412</cell><cell>0.052</cell><cell>0.0628</cell><cell>1.05 × 10 -21</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>SBERT [39] model on our specific task. Another direction for future work is to index documents as vectors and use them directly, instead of calculating them in re-ranking. This trade-off would result in the loss of one of the scores, but it would increase the re-ranking speed. Finally, we plan to use links inside documents to extract details that may improve the searching results. We may try to find keywords in the URL path and use them to find their domain authority and take this aspect into account in the score computation.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INFORMATION NEED</head><p>INDEXING QUERY</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="26,112.66,226.89,394.62,10.91;26,112.66,240.44,50.36,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Organizers</surname></persName>
		</author>
		<ptr target="https://clef-longeval.github.io/" />
		<title level="m" coord="26,182.28,226.89,101.36,10.91">Longeval clef 2023 lab</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,253.99,353.26,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="26,148.05,253.99,55.85,10.91">About qwant</title>
		<author>
			<persName coords=""><surname>Qwant</surname></persName>
		</author>
		<ptr target="https://about.qwant.com/en/" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,267.54,394.04,10.91;26,112.48,281.08,297.50,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="26,192.53,267.54,90.88,10.91">Regular expressions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Network</surname></persName>
		</author>
		<ptr target="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,294.63,394.03,10.91;26,112.66,308.18,229.79,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="26,161.24,294.63,68.46,10.91">Lucene analyzer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_0_0/core/org/apache/lucene/analysis/Analyzer.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,321.73,394.04,10.91;26,112.66,335.28,364.05,10.91;26,112.66,348.83,122.18,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="26,182.97,321.73,134.52,10.91">Lucene whitespacetokenizer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_4_0/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,362.38,10.18,10.91;26,140.30,362.38,34.87,10.91;26,196.31,362.38,32.53,10.91;26,246.30,362.38,68.54,10.91;26,335.98,362.38,170.71,10.91;26,112.66,375.93,338.15,10.91;26,484.52,375.93,23.14,10.91;26,112.28,389.48,96.76,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="26,196.31,362.38,32.53,10.91;26,246.30,362.38,64.26,10.91">Lucene lettertokenizer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,403.03,394.04,10.91;26,112.66,416.58,382.32,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="26,167.71,403.03,116.47,10.91">Lucene standardtokenizer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/analysis/standard/StandardTokenizer.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,430.13,366.55,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="26,191.73,430.13,51.13,10.91">Apache solr</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,443.67,393.86,10.91;26,112.66,457.22,394.62,10.91;26,112.66,470.77,50.36,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/6_6/filter-descriptions.html#FilterDescriptions-EnglishMinimalStemFilter" />
		<title level="m" coord="26,191.99,443.67,165.44,10.91">Apache solr englishminimalstemfilter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,484.32,394.04,10.91;26,112.66,497.87,371.79,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/6_6/filter-descriptions.html#FilterDescriptions-KStemFilter" />
		<title level="m" coord="26,218.33,484.32,118.97,10.91">Apache solr kstemfilter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,511.42,394.04,10.91;26,112.66,524.97,392.68,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/6_6/filter-descriptions.html#FilterDescriptions-PorterStemFilter" />
		<title level="m" coord="26,207.07,511.42,134.49,10.91">Apache solr porterstemfilter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,538.52,394.04,10.91;26,112.66,552.07,395.17,10.91;26,112.66,565.62,26.38,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/6_6/language-analysis.html#LanguageAnalysis-FrenchLightStemFilter" />
		<title level="m" coord="26,196.05,538.52,149.74,10.91">Apache solr frenchlightstemfilter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,579.17,394.03,10.91;26,112.66,592.72,395.17,10.91;26,112.66,606.27,26.38,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/6_6/language-analysis.html#LanguageAnalysis-FrenchLightStemFilter" />
		<title level="m" coord="26,191.10,579.17,159.84,10.91">Apache solr frenchminimalstemfilter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,619.81,10.18,10.91;26,140.27,619.81,34.86,10.91;26,196.22,619.81,32.53,10.91;26,246.18,619.81,68.71,10.91;26,335.98,619.81,170.71,10.91;26,112.66,633.36,394.62,10.91;26,112.66,646.91,50.36,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="26,196.22,619.81,32.53,10.91;26,246.18,619.81,63.80,10.91">Lucene lovinsstemmer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/6_3_0/analyzers-common/org/tartarus/snowball/ext/LovinsStemmer.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,86.97,394.03,10.91;27,112.66,100.52,394.99,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="27,162.14,86.97,82.09,10.91">Lucene lengthfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_0_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,114.06,310.77,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="27,162.27,114.06,132.01,10.91">Lucene englishpossessivefilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,127.61,251.13,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="27,162.27,127.61,72.79,10.91">Lucene stopfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,141.16,372.58,10.91" xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Code</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename></persName>
		</author>
		<ptr target="https://code.google.com/archive/p/luke/" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,154.71,10.18,10.91;27,137.97,154.71,34.86,10.91;27,191.06,154.71,32.53,10.91;27,238.72,154.71,79.04,10.91;27,335.98,154.71,170.71,10.91;27,112.66,168.26,395.01,10.91;27,112.28,181.81,96.76,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="27,191.06,154.71,32.53,10.91;27,238.72,154.71,74.39,10.91">Lucene ngramtokenfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_1_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,195.36,394.04,10.91;27,112.66,208.91,367.36,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="27,161.76,195.36,85.05,10.91">Lucene shinglefilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/4_3_0/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,222.46,394.04,10.91;27,112.41,236.01,387.00,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/7_3/language-analysis.html#opennlp-part-of-speech-filter" />
		<title level="m" coord="27,191.29,222.46,179.44,10.91">Apache solr opennlp part of speech filter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,249.56,394.04,10.91;27,112.66,263.11,352.13,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://solr.apache.org/guide/7_3/language-analysis.html#opennlp-lemmatizer-filter" />
		<title level="m" coord="27,201.84,249.56,142.67,10.91">Solr opennlp lemmatizer filter</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,276.66,394.03,10.91;27,112.66,290.20,347.53,10.91" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="27,162.14,276.66,83.02,10.91">Lucene elisionfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,303.75,10.18,10.91;27,140.31,303.75,34.87,10.91;27,196.34,303.75,32.53,10.91;27,246.34,303.75,68.48,10.91;27,335.98,303.75,170.71,10.91;27,112.66,317.30,340.88,10.91;27,484.52,317.30,23.14,10.91;27,112.28,330.85,96.76,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="27,196.34,303.75,32.53,10.91;27,246.34,303.75,64.20,10.91">Lucene lowercasefilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_0_0/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,344.40,10.18,10.91;27,138.63,344.40,34.86,10.91;27,192.55,344.40,32.53,10.91;27,240.87,344.40,76.06,10.91;27,335.98,344.40,170.71,10.91;27,112.66,357.95,394.39,10.91;27,112.66,371.50,122.18,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="27,192.55,344.40,32.53,10.91;27,240.87,344.40,72.05,10.91">Lucene asciifoldingfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/4_9_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,385.05,394.04,10.91;27,112.66,398.60,395.01,10.91;27,112.28,412.15,96.76,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="27,184.37,385.05,132.24,10.91">Lucene synonymgraphfilter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/6_4_1/analyzers-common/org/apache/lucene/analysis/synonym/SynonymGraphFilter.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,425.70,10.18,10.91;27,141.26,425.70,34.86,10.91;27,198.46,425.70,32.53,10.91;27,249.39,425.70,64.26,10.91;27,335.98,425.70,170.71,10.91;27,112.66,439.25,353.38,10.91;27,484.52,439.25,23.14,10.91;27,112.28,452.79,96.76,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="27,198.46,425.70,32.53,10.91;27,249.39,425.70,58.42,10.91">Lucene synonymmap</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_3_0/analyzers-common/org/apache/lucene/analysis/synonym/SynonymMap.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,466.34,394.04,10.91;27,112.66,479.89,336.76,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html" />
		<title level="m" coord="27,161.80,466.34,97.54,10.91">Lucene bm25similarity</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,493.44,394.04,10.91;27,112.66,506.99,207.83,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Face</surname></persName>
		</author>
		<ptr target="https://huggingface.co/optimum/all-MiniLM-L6-v2" />
		<title level="m" coord="27,160.34,493.44,183.34,10.91">Hugging face &apos;all-minilm-l6-v2&apos; model</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,520.54,393.53,10.91;27,112.66,534.09,394.61,10.91;27,112.66,547.64,50.36,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="27,309.97,520.54,196.22,10.91;27,112.66,534.09,157.58,10.91">Can chatgpt write a good boolean query for systematic review literature search?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,278.56,534.09,143.04,10.91">Journal of Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2023" to="2025" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,561.19,394.03,10.91;27,112.66,574.74,264.83,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<ptr target="https://platform.openai.com/docs/guides/completion/introduction" />
		<title level="m" coord="27,153.44,561.19,191.43,10.91">Openai text completion api documentation</title>
		<imprint>
			<date type="published" when="2023-05-20">2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,588.29,395.00,10.91;27,112.28,601.84,96.76,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="27,153.45,588.29,228.26,10.91">How do text davinci-002 and text davinci-003 differ?</title>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,389.40,588.29,90.78,10.91">OpenAI Help Center</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,615.39,394.04,10.91;27,112.66,628.93,278.87,10.91" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="27,162.39,615.39,93.00,10.91">Lucene booleanquery</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_1_1/core/org/apache/lucene/search/BooleanQuery.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,642.48,394.04,10.91;27,112.66,656.03,267.92,10.91" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="27,165.47,642.48,84.52,10.91">Lucene boostquery</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/7_3_1/core/org/apache/lucene/search/BoostQuery.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,669.58,395.01,10.91;28,112.28,86.97,96.76,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><surname>Ukplab</surname></persName>
		</author>
		<ptr target="https://github.com/UKPLab/sentence-transformers" />
		<title level="m" coord="27,153.25,669.58,96.60,10.91">Sentence transformers</title>
		<imprint>
			<date type="published" when="2023-05-20">2023. 2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,100.52,362.45,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Fauconnier</surname></persName>
		</author>
		<ptr target="http://fauconnier.github.io" />
		<title level="m" coord="28,187.46,100.52,109.96,10.91">French word embeddings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,114.06,395.01,10.91;28,112.66,127.61,354.64,10.91" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="28,112.66,114.06,169.88,10.91">Apache Lucene, Lucene knnvectorfield</title>
		<ptr target="https://lucene.apache.org/core/9_0_0/core/org/apache/lucene/document/KnnVectorField.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Online documentation</note>
</biblStruct>

<biblStruct coords="28,112.66,141.16,394.04,10.91;28,112.66,154.71,328.82,10.91" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="28,161.87,141.16,97.52,10.91">Lucene multisimilarity</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/8_0_0/core/org/apache/lucene/search/similarities/MultiSimilarity.html" />
		<imprint>
			<date type="published" when="2023-05-20">2023-05-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,168.26,394.53,10.91;28,112.66,181.81,194.16,10.91" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.10084" />
		<title level="m" coord="28,219.42,168.26,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,195.36,394.04,10.91;28,112.66,208.91,239.86,10.91" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="28,154.25,195.36,104.91,10.91">Pytorch cosinesimilarity</title>
		<author>
			<persName coords=""><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Online documentation</note>
</biblStruct>

<biblStruct coords="28,112.66,249.56,393.53,10.91;28,112.66,263.11,287.49,10.91" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="28,309.36,249.56,196.83,10.91;28,112.66,263.11,157.58,10.91">Can chatgpt write a good boolean query for systematic review literature search?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03495</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
