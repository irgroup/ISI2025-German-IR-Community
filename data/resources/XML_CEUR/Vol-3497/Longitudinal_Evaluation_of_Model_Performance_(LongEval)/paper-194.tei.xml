<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,359.89,15.42;1,89.29,106.66,411.24,15.42;1,89.29,128.58,89.00,15.43;1,89.29,150.91,220.08,11.96">SEUPD@CLEF: Team QEVALS on Information Retrieval Adapted to the Temporal Evolution of Web Documents Notebook for the LongEval Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,89.51,11.96"><forename type="first">Enrico</forename><surname>D'alberton</surname></persName>
							<email>enrico.dalberton@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.44,176.82,75.06,11.96"><forename type="first">Saverio</forename><surname>Fincato</surname></persName>
							<email>saverio.fincato@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.14,176.82,100.78,11.96"><forename type="first">Vaidas</forename><surname>Lenartavicius</surname></persName>
							<email>vaidas.lenartavicius@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.57,176.82,70.33,11.96"><forename type="first">Laura</forename><surname>Pallante</surname></persName>
							<email>laura.pallante@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.95,190.76,49.52,11.96"><forename type="first">Yijian</forename><surname>Qiu</surname></persName>
							<email>yijian.qiu@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.46,190.76,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,359.89,15.42;1,89.29,106.66,411.24,15.42;1,89.29,128.58,89.00,15.43;1,89.29,150.91,220.08,11.96">SEUPD@CLEF: Team QEVALS on Information Retrieval Adapted to the Temporal Evolution of Web Documents Notebook for the LongEval Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0D1682BEEAA4E983AE3EAD5F204A1133</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLEF 2023</term>
					<term>LongEval-Retrieval</term>
					<term>Information Retrieval</term>
					<term>Temporal Evolution</term>
					<term>Search Engines</term>
					<term>Short-term Persistence</term>
					<term>Long-term Persistence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report presents the work conducted by our team for LongEval-Retrieval Task 1 [1] in CLEF 2023 <ref type="bibr" coords="1,495.01,256.97,9.27,8.97" target="#b1">[2]</ref>. The primary objective of this task is to develop an information retrieval system that can effectively adapt to the temporal evolution of Web documents. Using the Longeval Websearch collection provided by the commercial search engine Qwant[3], our team has built a retrieval system that addresses the challenges posed by the changing nature of Web documents and user search preferences. This paper discusses our approach to the subtasks of short-term persistence and long-term persistence, as well as the evaluation of our retrieval system's performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, the advent of the internet has led to an exponential growth of the information available online and this brought a more challenging way to find relevant and accurate information. This is the environment where search engines play a vital role that allows us to access the correct information quickly and efficiently just with a few clicks. Information retrieval systems are more crucial than ever, influencing several fields such as healthcare, business and mainly education. Our objective is to investigate the subject of information retrieval in search engines and its adaptability to changes over time. The necessity to create a retrieval system that can successfully handle the dynamic nature of the web is what stimulates this research's development. Our team, QEVALS, is participating in this challenge as a student group project conducted in the Search Engines course a.y. 2022/23 at the Computer Engineering master's degree at the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>The development process was iterative and based heavily on discussions between the group members regarding the subjects covered during the lectures. However, our first results were significantly below the baseline set by the organizers of the LongEval task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Analyzer</head><p>The initial implementations of the classes were very simple and we started adding features to them day by day. The main purpose of an analyzer in an IR model is to pre-process the input data in a way that reduces the complexity of the document representation, while still retaining the relevant information necessary for accurate retrieval. The final models we submitted, present a tokenizer implemented using LetterTokenizer class from Apache Lucene <ref type="bibr" coords="2,474.17,561.19,12.79,10.91" target="#b4">[5]</ref> and it simply divides text at non-letters. Subsequently, we applied a LowerCaseFilter and an ElisionFilter, which, respectively, normalize token text to lowercase and remove elisions from a token. This last filter has been used specifically for our case since the dataset provided by LongEval is in French. We have also applied a ASCIIFoldingFilter to convert alphabetic, numeric, and symbolic Unicode characters that are not in the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents. One of the tools that had the biggest impact on our system's performance was the implementation of a StopFilter. As the name suggests, it is used to remove stopwords (very frequent words that contain little information about the contents of a document or a query) from a stream of text. While testing different stoplists and experimenting with query boosting some parts of the queries, we discovered that for some stoplists, individually boosting query tokens that were non-stopwords would improve system performance, even though the queries were subject to a StopFilter later down the pipeline. We tried using the same stoplist for our query boosting and our StopFilter, as well as different ones and found that the best combination varied on a case-by-case basis. This particular type of query boosting was effective only on less-performing stoplists, thus it isn't included in the final system. These are the results compared: The lists we used are:</p><p>‚Ä¢ CountWordsFree [6]: CountWordsFree is a web-based service for content writers, web developers, and professionals who provide search ranking optimization services such as the list of stopwords we have used; ‚Ä¢ Google Stopwords <ref type="bibr" coords="4,205.76,255.86,12.47,9.76" target="#b6">[7]</ref>: set of common words that are filtered out or ignored by Google's search engine algorithms when processing search queries; ‚Ä¢ Stopwords ISO <ref type="bibr" coords="4,191.51,284.13,12.71,9.76" target="#b7">[8]</ref>: collection of stop words for various languages that are commonly used in natural language processing (NLP) tasks; ‚Ä¢ Ranks.nl <ref type="bibr" coords="4,163.39,312.41,12.59,9.76" target="#b8">[9]</ref>: Dutch website that provides various online marketing services and tools; ‚Ä¢ Savoy Stopwords <ref type="bibr" coords="4,201.04,327.14,17.83,9.76" target="#b9">[10]</ref>: it is a standard library that provides a collection of robust, highperformance libraries for mathematics, statistics, data processing, streams, and more and includes many of the utilities you would expect from a standard library; ‚Ä¢ NoStop: it is simply an empty list, to evaluate if the filter was enhancing the performances or not.</p><p>As we can see from the obtained values, the Google stopwords list is the one that performs best, without stoplist based query boosting. It is worth noting that it is the shortest list that was tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Tested but unused filters</head><p>The systems submitted by our group are the results of numerous trials and runs, and unfortunately many of our attempted approaches proved themselves to be ineffective or unfeasible. In our first prototypes, a LengthFilter was used to remove the words with a number of characters below and over certain thresholds. Initially it incremented the model's scores, but upon further refinements of other parts of the system it started having a negative impact on performance.</p><p>A few different configurations of a ShingleFilter were also attempted to create tokens from overlapping sequences of n words (token n-grams) from a token stream. We found they weren't suited to the LongEval task. Multiple attempts were made to integrate an NLP (Natural Language Processing) library into our system. It could be used for many tasks, such as tokenization, sentence segmentation, part-of-speech tagging and named entity extraction. Specifically, we focused on two different libraries: OpenNLP <ref type="bibr" coords="4,180.92,642.48,18.07,10.91" target="#b10">[11]</ref> and CoreNLP <ref type="bibr" coords="4,267.05,642.48,16.41,10.91" target="#b11">[12]</ref>. We tried running many configurations of both libraries, however for a dataset as large as LongEval's they were requiring computational power well beyond what we had access to, therefore we were forced to discard the idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Indexer</head><p>The Indexer is a class responsible for creating an index of the terms in a collection of documents. The purpose of the indexer is to speed up the retrieval of relevant documents in response to user queries. It creates an internal data structure, the 'index', that allows quicker access to data. An indexer reads through the text data, identifies important and searchable entities, and constructs an index of those entities and their location in the data. Our custom indexer, called LongEvalIndexer, is an update of the initial indexer design user for the Tipster collection discussed during the lectures. The latest version of it, recursively visits each file in the specified document directory and extracts the contained text data through the Parser class. Each document is parsed and javascript or PHP code is removed through the TextFilter class. Subsequently, each document is stored in the index and divided into the following fields:</p><p>‚Ä¢ IDField: this field stores the unique identifier associated with each document; ‚Ä¢ BodyField: the body of the document is indexed and stored in this field after being processed by the parser. The field type is defined using a FieldType object, which is configured to store the term vectors, positions, and offsets; ‚Ä¢ LinkField: the link corresponding to the document is extracted from the URLs file provided in the LongEval dataset and is processed to remove specific characters in order to obtain the words contained in the link. It is then added to the document using a custom LinkField, which is configured to store the term vectors, positions, and offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Searcher</head><p>The Searcher is a class used to represent the user or the query that seeks information from a collection of documents, so its main function is to express the information need of the user in the form of a query. This is the last class to be used in the information retrieval process since to perform research on the documents they have to be indexed first. The final version of this class takes as input a set of queries from the set provided by LongEval and it builds a BoostQuery object from each query. The BoostQuery class allows to give a boost to the wrapped query. Boost values that are less than one will give less importance to this query compared to other ones while values that are greater than one will give more importance to the scores returned by this query. Combined with this class, we exploited the MultiFieldQueryParser class, which is used to associate queries and documents with multiple fields, also defining the weight they will have in the search. In our case, the fields we researched were "body" and "link". Using different weights, we noticed that the weights given to these fields were crucial for the model's performance.</p><p>Other approaches for the implementation of this class were attempted but did not result in performance gains. We tested two types of query expansion techniques, one based on the top relevant documents for each query and another one developed with the help of gpt-3.5-turbo model by OpenAI <ref type="bibr" coords="5,170.95,628.93,16.25,10.91" target="#b12">[13]</ref>. The functioning of the first one was based on an algorithm that found the most relevant words in the top ten retrieved documents for each query and then it added these words to the main query. After that, a search on the collection was repeated with the expanded query. Unfortunately, this technique proved ineffective, as it significantly lowered the accuracy of the system. This due to the low accuracy rate that the system can achieve even in the best-ranked dopuments. This led to adding more noise words than relevant and thus worsening the search, forcing us to abandon this approach.</p><p>The second query expansion we tested made use of an NLP model developed by OpenAI. The idea was to give the model a single query as input and ask it to return an expanded one. The first problem we encountered was linked to the number of calls we could do, by default OpenAI limits users to 3 calls per minute, but each run was about 700 queries. In order to avoid this bottleneck, we formed a prompt to expand 100 queries in each call. Unfortunately, the model was not particularly accurate and sometimes it diverged too much, expanding the queries incorrectly.</p><p>For this reason, we had to abandon this idea too, but we believe that with some refinements it could be a good improvement for a future model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Similarity</head><p>Similarity function is a fundamental component used to measure the similarity, or relatedness, between queries and documents. The function returns a score that reflects how well the document matches the query. During the development process the BM25 similarity model with default parameters was used as a baseline. The systems our team is submitting for the LongEval task have various configurations of similarity models in order to test how they perform with separate but similar datasets and study any trends that emerge. The models being tested are described in the Results 4 section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data description</head><p>The data used in the project are the evaluation collection provided for task 1, "LongEval-Retrieval", consisting of collections of web documents and queries provided by Qwant, a search engine. Both documents and queries were collected in French and then automatically translated into English. This scheme represents the collection process: The data can be described as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Documents</head><p>The collection includes relevant documents that are selected to be retrieved for each query and potentially irrelevant documents randomly sampled from the Qwant index to better represent the nature of a Web test collection.</p><p>‚Ä¢ Train data: consists of 1,570,734 web pages, acquired during June 2022. They can be downloaded from the Lindat/Clarin website.</p><p>‚Ä¢ Test data: consist of 1,593,376 documents and 882 queries, collected over July 2022, for the short-term persistence sub-task and 1,081,334 documents and 923 queries, collected over September 2022, for the long-term persistence sub-task. Both can be downloaded from the Lindat/Clarin website. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Topics</head><p>The queries are extracted from Qwant's search logs, based on a set of selected topics, but exact details regarding them were not made available to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Queries</head><p>The queries are extracted from Qwant's search logs, based on a set of selected topics. The query set was created in French and was automatically translated into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Measures</head><p>The evaluation tools used during development are trec_eval [14], an executable that through qrels allows us to obtain measures of precision, accuracy, etc., and Luke <ref type="bibr" coords="7,420.92,512.84,16.41,10.91" target="#b13">[15]</ref>, a Lucene GUI that allows us to look inside the index, to check the health and consistency of the indexes.</p><p>The main evaluation measures used to check the system during the various stages of development are:</p><p>‚Ä¢ nDCG or Normalized Discounted Cumulated Gain: it measures the effectiveness of a retrieval system by considering the relevance of documents and their positions in the ranked list. In our case, we used an evaluation depth of 5 (nDCG@5). To compute it is necessary to normalize the score by dividing nDCG by the iDCG (Ideal Discounted Cumulated Gain). These are the formulas to compute them:</p><formula xml:id="formula_0" coords="7,236.68,650.12,269.96,33.58">ùê∑ùê∂ùê∫@ùëò = ùëò ‚àëÔ∏Å ùëõ=1 ùëüùëíùëô ùëõ ùëöùëéùë•(1, ùëôùëúùëî ùëè (ùëõ))<label>(1)</label></formula><p>ùëõùê∑ùê∂ùê∫@ùëò = ùê∑ùê∂ùê∫@ùëò ùëñùê∑ùê∂ùê∫@ùëò (2)</p><p>‚Ä¢ MAP or Mean Average Precision: this is the most widely used metric in IR and it measures the average precision across all relevant documents in the retrieval set. It is given by:</p><formula xml:id="formula_1" coords="8,252.82,149.65,253.82,24.89">AP = ‚àëÔ∏Å ùëõ (ùëÖ ùëõ -ùëÖ ùëõ-1 )ùëÉ ùëõ<label>(3)</label></formula><p>where ùëÖ ùëõ and ùëÉ ùëõ are the precision and recall at the ùëõ ùë°‚Ñé threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Repository</head><p>The git repository containing the source code of the system is available in the repository at the link seupd2223-qevals (https://bitbucket.org/upd-dei-stud-prj/seupd2223-qevals). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hardware components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>The numerous runs that were executed on the LongEval training dataset provided us with good baseline analyzer and searcher configurations that we could use to compare how different similarity models performed on the LongEval task. Specifically, we learned that:</p><p>‚Ä¢ Better results were obtained working on the dataset containing documents in the original language compared to the English-translated one. ‚Ä¢ In the case of French queries and documents the best-performing tokenization is obtained through Lucene's LetterTokenizer. ‚Ä¢ Stop Lists can improve performance, but most available lists tend to over-filter the data to the point of performing worse than using no filter at all. ‚Ä¢ In case of stop list over-filtering it is possible to regain some performance by query boosting tokens in the query that are specifically not in a stop list, even though the stop list tokens get filtered later in the pipeline.</p><p>‚Ä¢ The best performing Stop Lists had at most 200 words. Specifically, we chose the Google stop list <ref type="bibr" coords="9,154.42,100.52,12.84,10.91" target="#b6">[7]</ref> as it lightly edged out the Ranks stop list <ref type="bibr" coords="9,354.15,100.52,11.43,10.91" target="#b8">[9]</ref>. ‚Ä¢ The use of stemmers had little effect, but seemed to reduce overall performance. We tested LightFrenchStemmer and MinimalFrenchStemmer, available on Apache Lucene, and chose to proceed without using either. ‚Ä¢ Adding more filters (other than the LowercaseFilter, the ElisionFilter and the ASCIIFold-ingFilter) such as the LengthFilter, in an attempt to exclude less useful terms, led to worse performance. ‚Ä¢ The use of word n-gram tokens was ineffective, possibly because such short queries are often not semantically coherent sentences, so they would not match the contents of the longer documents. ‚Ä¢ Our implementations of Query Expansion did not perform well, though we are uncertain whether the LongEval dataset is ill-suited to this approach or our implementations had inherent issues. ‚Ä¢ Given the very short format of the queries adding the link field of the document to the index led to notable performance improvements.</p><p>Using the configuration described above five different similarity models were tested: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training set</head><p>Following is the summary table reporting the MAP and nDCG values obtained in the training set for each run: As reported, the values of each run are similar, in fact the variances for both sets of measures are very low. This is due to the fact that the changes across the models are not so relevant, indeed the main structure is almost the same.</p><p>Here instead, there is the Interpolated Precision vs Recall graph for the five tested similarity models: The graph shows that the performances of the chosen similarity models are quite close to each other for the LongEval training dataset. In particular, BM25 performs best overall, and changes to the default parameters can result in further (though limited) gains in performance. The language model is very slightly better than BM25 at low recall, but drops off more steeply at around 30% Recall, while the statistical models have comparable performance to BM25 at most points, but lag behind at low recall.</p><formula xml:id="formula_2" coords="10,155.25,537.36,38.07,9.57">0 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Test Set</head><p>This section aims to provide a comprehensive analysis of the performance of the five models submitted for evaluation. Various metrics are used to evaluate the effectiveness and efficiency of each model. These metrics include average precision(MAP), precision(P@10), normalized discounted cumulated gain (nDCG) and recall (Recall_1000). First, we present three tables, each corresponding to one of the test sets. These tables compare the performance of each run based on the metrics listed above. Furthermore, we reported the boxplots calculated from the average precision (AP) respectively for short and long term. These graphical representations are intended to provide a detailed overview of the variability and distortion of the system's accuracy values. Next, there are the comparative graphs of the MAP values obtained with the Tukey's HSD of the runs for short and long term. This will offer a comparison of the mean performance of the systems, highlighting their respective strengths and weaknesses. Subsequently, we reported the ANOVA tables (one-way) respectively for long and short term. These tables show how the sum of squares are distributed according to source of variation, and hence the mean sum of squares. Finally, we have plotted the topics performance at 5 for short and long term. In Tables 6, 7, 8 the observed values of the different "runs" of the system are similar, without significant deviations, and we can observe that the MAP values in both tables remain around 20%. It is interesting to note that the QEVALS_IB run performed well in the heldout set, obtaining the best results (map: 0.2108, p@10: 0.1329, nDCG: 0.3597), indeed it did not scored as expected in the Short and Long Test datasets. The values obtained from run QEVALS_BM25DFLT in the short and long test set seem slightly better than the others. From the boxplots in Figure <ref type="figure" coords="12,226.03,562.72,5.50,9.76" target="#fig_3">3</ref> we can see that the accuracy values obtained from the runs are very similar, also we see that QEVALS_LMDirichlet and QEVALS_BM25CSTM seem to perform well in one test set and less in the other, instead QEVALS_DFR and QEVALS_BM25DFLT seem to maintain performance in both. From the Figure <ref type="figure" coords="13,180.12,291.58,4.08,9.76" target="#fig_4">4</ref>, even if there are not relevant differences in performances, as already confirmed from the previous analysis, we can highlight that the best run is QEVALS_BM25DFLT. From the analysis of the two-way ANOVA, visible in Table <ref type="table" coords="13,367.76,579.84,5.62,9.76" target="#tab_5">9</ref> and Table <ref type="table" coords="13,425.01,579.84,9.11,9.76" target="#tab_0">10</ref>, it is confirmed how there is no substantial difference between the runs. A low F-value statistic suggests that the variation between the groups is relatively small compared to the variation within them; in addition, a high F-value probability indicates that the probability that the observed differences are due to chance is very high. This further confirms that the differences between runs tested are not statistically significant. Similarly to the training dataset, the different runs on the short-term and long-term datasets manifested significant consistency, with similar MAP, nDCG, and Recall values. This indicates a relative stability of system performance and shows the capacity of adaptation to the temporal evolution of the documents.</p><p>As expected, the overall performance of the runs showed no significant variation between our training baseline and the short-term and long-term document sets. Figure <ref type="figure" coords="14,439.90,369.42,3.98,9.76" target="#fig_5">5</ref>, in particular, shows that our system's performance is virtually identical for the entire range of topics of the short-term and long-term datasets. The capability to have consistent performance regardless of the contents of a dataset can be considered an advantage of a system such as ours, which does not use past data to train a model for future applications. One notable aspect that we can infer from the test data is that the fine-tuning of the parameters of the BM25 similarity model is quite dependent on the available data. In fact, for both short-term and long-term dataset test runs, the default BM25 model performed slightly better than our fine-tuned one. However, in general, the overall performances don't change drastically. Indeed, the values of the measurements differ at most ‚àº 2%. We can conclude that for a task such as LongEval the similarity model choice alone has little impact on the overall performance of the system.</p><p>Finally, QEVALS_BM25DFLT (the default configuration of the BM25 similarity model) proved to be the best performing run, achieving the highest MAP, nDCG and Recall values among the different runs evaluated. Our conclusion is that this system is the most suited to the task out of the ones we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>While our systems do not perform much better than the baseline provided by LongEvals, the process to arrive at the systems we are submitting offered us an opportunity to acquire experience and knowledge in dealing with information retrieval systems. Many of our approaches did not perform as hoped, however thanks to our numerous trials we are much better equipped to try new approaches to information retrieval in the future. Moreover, we are aware of many possible enhancements to the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More studies on the datasets</head><p>Our first challenge was the language. At a baseline, performance is better on the dataset in the original language, so our team decided to focus more on the French dataset early on. Our initial attempts on the English dataset seemed less promising, but it's possible that we would have had more success implementing the more complex systems for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Usage of NLP</head><p>The use of an NLP library such as OpenNLP or CoreNLP would have helped us take a big step forward. We learned that using Natural Language Processing technology to implement the whole model is probably unfeasible since it would be too expensive from a computational point of view, but we believe that a part of part-of-speech tagging or named entity extraction would have been helpful. Therefore, if we had a chance to run the project on more powerful computers, and sufficient time to build efficient parallelization algorithms, we believe that the model could gain better results and performances</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Query expansion techniques</head><p>Another important aspect we were not able to implement properly was a query expansion technique, as discussed in the Methodology 4 section. There are many algorithms that could be used and some of them are already present in some libraries. The main problem was that our dataset was in French and we have not been able to find a good dictionary in order to expand the queries. So, we think that a query expansion technique with a proper dictionary could be a good addition to the project. Moreover, the two algorithms we tried to implement could be improved. In fact, the first one, which expands the query using the top retrieved documents' words, may already be in a working state, but in order for it to work well, the rest of the system must provide precise enough results at low recall values. This is due to the fact that a better base model leads to more relevant documents and so, to more relevant words with which to expand the query. Due to low precision at low recall we were adding noise to the initial query that, in the end, was performing worse than the original one.</p><p>The query expansion technique which was using the gpt-3.5-turbo model, was not a success either, however, it must be noted that compared to our other approaches it is the one we worked on the least because of time limitations and OpenAI's API access quota limitations for non-paying users. The problem our team ran into was the unpredictability of the answers: the model was asked to expand each query to five words, but in some instances it expanded queries to up to fifteen words, adding useless noise. For such a system to work, some additional experience in prompt formulation for information retrieval purposes would likely bring much better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,430.17,179.61,8.93;2,172.63,302.84,250.02,114.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scheme of the project's workflow</figDesc><graphic coords="2,172.63,302.84,250.02,114.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,306.15,189.78,8.93;7,172.63,84.19,250.02,209.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scheme of the collection process [1]</figDesc><graphic coords="7,172.63,84.19,250.02,209.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,107.28,342.01,398.71,10.91;9,116.56,355.56,23.27,10.91;9,107.28,370.46,398.71,10.91;9,116.16,384.01,128.81,10.91;9,107.28,398.92,398.71,10.91;9,116.56,412.47,98.07,10.91;9,107.28,427.37,398.70,10.91;9,116.56,440.92,389.42,10.91;9,116.56,454.47,276.38,10.91;9,107.28,469.37,398.71,10.91;9,116.56,482.92,389.42,10.91;9,116.56,496.47,70.15,10.91"><head>‚Ä¢</head><label></label><figDesc>QEVALS_BM25DFLT: BM25 similarity model with default parameters (k1 = 1.2, b = 0.75). ‚Ä¢ QEVALS_BM25CSTM: BM25 similarity model with the best-performing parameters that were found (k1= 1.2, b = 0.9). ‚Ä¢ QEVALS_LMDirichlet: Language model based similarity with Bayesian smoothing using Dirichlet priors. ‚Ä¢ QEVALS_DFR: Probabilistic model that measures the divergence from randomness. The three components chosen were: Geometric approximation of Bose-Einstein, Laplace's law of succession and Uniform distribution of term frequency. ‚Ä¢ QEVALS_IB: Information-based model. The three components chosen were: Log-logistic probabilistic distribution, Total Term Frequency Lambda and Uniform distribution of term frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,89.29,347.12,118.26,8.93;12,164.61,369.89,56.60,9.96;12,371.68,369.89,55.79,9.96;12,90.97,381.94,204.18,153.14"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision BoxPlots (a) Short term (b) Long term</figDesc><graphic coords="12,90.97,381.94,204.18,153.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,89.29,90.17,113.17,8.93;13,172.94,113.20,56.60,9.96;13,363.35,112.94,55.79,9.96;13,297.64,124.99,187.51,130.99"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Runs comparison (a) Short term (b) Long term</figDesc><graphic coords="13,297.64,124.99,187.51,130.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,89.29,90.62,122.53,8.93;14,172.94,113.39,56.60,9.96;14,363.35,113.39,55.79,9.96;14,107.64,125.45,187.51,140.63"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Topics performance (a) Short term (b) Long term</figDesc><graphic coords="14,107.64,125.45,187.51,140.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,210.82,352.58,308.68"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,88.99,222.82,352.58,296.68"><row><cell>MAP values</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CFW</cell><cell>Google</cell><cell>Iso</cell><cell>Rank</cell><cell cols="2">Savoy NoStop</cell></row><row><cell>NoFilter</cell><cell>0.194</cell><cell cols="4">0.2082 0.1932 0.2076 0.1947</cell><cell>0.2018</cell></row><row><cell cols="5">OwnFilter 0.2024 0.2011 0.2011 0.2015</cell><cell>0.2018</cell><cell>0.2018</cell></row><row><cell>FST</cell><cell>0.2021</cell><cell>0.201</cell><cell cols="3">0.2011 0.2007 0.2025</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nDCG values</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CFW</cell><cell>Google</cell><cell>Iso</cell><cell>Rank</cell><cell cols="2">Savoy NoStop</cell></row><row><cell>NoFilter</cell><cell cols="4">0.3256 0.3432 0.3248 0.3422</cell><cell>0.326</cell><cell>0.3376</cell></row><row><cell cols="2">OwnFilter 0.3371</cell><cell>0.3365</cell><cell>0.3361</cell><cell>0.3369</cell><cell>0.3367</cell><cell>0.3376</cell></row><row><cell>FST</cell><cell cols="5">0.3374 0.3359 0.3364 0.3364 0.3376</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P@10 values</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CFW</cell><cell>Google</cell><cell>Iso</cell><cell>Rank</cell><cell cols="2">Savoy NoStop</cell></row><row><cell>NoFilter</cell><cell>0.1265</cell><cell>0.133</cell><cell cols="4">0.1262 0.1316 0.1269 0.1257</cell></row><row><cell cols="2">OwnFilter 0.1255</cell><cell>0.1256</cell><cell>0.1247</cell><cell>0.125</cell><cell>0.1253</cell><cell>0.1257</cell></row><row><cell>FST</cell><cell>0.126</cell><cell>0.1253</cell><cell>0.1252</cell><cell>0.1247</cell><cell>0.1262</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,291.55,105.74"><head>Table 5</head><label>5</label><figDesc>Summary table with MAP and nDCG (training set)</figDesc><table coords="10,214.73,122.10,165.81,74.12"><row><cell></cell><cell>MAP nDCG</cell></row><row><cell cols="2">QEVALS_BM25DFLT 0.2078 0.3433</cell></row><row><cell cols="2">QEVALS_BM25CSTM 0.2082 0.3432</cell></row><row><cell cols="2">QEVALS_LMDirichlet 0.2042 0.3417</cell></row><row><cell>QEVALS_DFR</cell><cell>0.2035 0.3377</cell></row><row><cell>QEVALS_IB</cell><cell>0.2024 0.3383</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.99,350.76,355.44,301.60"><head>Table 6</head><label>6</label><figDesc>Run performance</figDesc><table coords="11,88.99,380.91,355.44,271.45"><row><cell></cell><cell></cell><cell cols="2">Heldout</cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>P@10</cell><cell>nDCG</cell><cell>Recall</cell></row><row><cell>QEVALS_BM25CSTM</cell><cell>0.1975</cell><cell>0.1293</cell><cell>0.3545</cell><cell>0.7688</cell></row><row><cell>QEVALS_BM25DFLT</cell><cell>0.2017</cell><cell>0.1268</cell><cell>0.3584</cell><cell>0.7749</cell></row><row><cell>QEVALS_DFR</cell><cell>0.2015</cell><cell>0.1317</cell><cell>0.3556</cell><cell>0.7635</cell></row><row><cell>QEVALS_LMDirichlet</cell><cell>0.1879</cell><cell>0.1171</cell><cell>0.3461</cell><cell>0.7738</cell></row><row><cell>QEVALS_IB</cell><cell>0.2108</cell><cell>0.1329</cell><cell>0.3597</cell><cell>0.7635</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run performance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Short</cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>P@10</cell><cell>nDCG</cell><cell>Recall</cell></row><row><cell>QEVALS_BM25CSTM</cell><cell>0.1998</cell><cell>0.1303</cell><cell>0.334</cell><cell>0.6874</cell></row><row><cell>QEVALS_BM25DFLT</cell><cell>0.2039</cell><cell>0.131</cell><cell>0.3376</cell><cell>0.6872</cell></row><row><cell>QEVALS_DFR</cell><cell>0.1957</cell><cell>0.1295</cell><cell>0.3304</cell><cell>0.6849</cell></row><row><cell>QEVALS_LMDirichlet</cell><cell>0.2027</cell><cell>0.1254</cell><cell>0.3392</cell><cell>0.6871</cell></row><row><cell>QEVALS_IB</cell><cell>0.1966</cell><cell>0.1296</cell><cell>0.3321</cell><cell>0.6874</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,88.98,333.19,336.33,229.31"><head>Table 9</head><label>9</label><figDesc>ANOVA2 (two-way) Short Term</figDesc><table coords="13,88.98,361.28,336.33,201.22"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Columns</cell><cell>0.0399</cell><cell>4</cell><cell cols="2">0.0099 0.1756</cell><cell>0.9510</cell></row><row><cell>Rows</cell><cell>36.0256</cell><cell cols="4">145 0.2484 4.3667 1.824e-54</cell></row><row><cell>Interaction</cell><cell>2.2227</cell><cell cols="3">580 0.0038 0.0673</cell><cell>1</cell></row><row><cell>Error</cell><cell cols="3">166.1379 2920 0.0568</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">204.4262 3649</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANOVA2 (two-way) Long Term</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell>Columns</cell><cell>0.0203</cell><cell>4</cell><cell cols="2">0.0050 0.1024</cell><cell>0.9816</cell></row><row><cell>Rows</cell><cell>44.0663</cell><cell cols="4">153 0.28801 5.8022 5.2730e-88</cell></row><row><cell>Interaction</cell><cell>2.1034</cell><cell>612</cell><cell cols="2">0.0034 0.0692</cell><cell>1</cell></row><row><cell>Error</cell><cell cols="3">152.8868 3080 0.0496</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">199.0770 3849</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,111.28,262.96,10.91" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Longeval</forename><surname>Clef</surname></persName>
		</author>
		<ptr target="https://clef-longeval.github.io/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,124.83,314.85,10.91" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="http://clef2023.clef-initiative.eu/index.php" />
	</analytic>
	<monogr>
		<title level="j" coord="16,141.70,124.83,18.19,10.91">Clef</title>
		<imprint>
			<date type="published" when="2022">2023. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,138.38,264.13,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,148.05,138.38,55.85,10.91">About qwant</title>
		<author>
			<persName coords=""><surname>Qwant</surname></persName>
		</author>
		<ptr target="https://about.qwant.com/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,151.93,394.53,10.91;16,112.66,165.48,394.52,10.91;16,112.66,179.03,394.53,10.91;16,112.66,192.57,394.52,10.91;16,112.66,206.12,393.33,10.91;16,112.66,219.67,393.33,10.91;16,112.66,233.22,321.57,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,112.66,192.57,389.62,10.91">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alkhalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borkakoty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Ebshihy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gonzalez-Saez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Galuscakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,127.14,206.12,378.84,10.91;16,112.66,219.67,327.16,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="16,471.85,219.67,34.14,10.91;16,112.66,233.22,148.46,10.91">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaliniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,246.77,277.02,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="16,151.32,246.77,63.20,10.91">Apache lucene</title>
		<author>
			<persName coords=""><surname>Apache</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,260.32,395.01,10.91;16,112.66,273.87,103.00,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Countwordsfree</surname></persName>
		</author>
		<ptr target="https://countwordsfree.com/stopwords/french" />
		<title level="m" coord="16,193.72,260.32,150.87,10.91">Countwordsfree french stopwords</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,287.42,394.04,10.91;16,112.66,300.97,139.16,10.91" xml:id="b6">
	<monogr>
		<ptr target="https://meta.wikimedia.org/wiki/Stop_word_list/google_stop_word_list#French" />
		<title level="m" coord="16,112.66,287.42,112.13,10.91">Google french stopwords</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,314.52,394.61,10.91;16,112.31,328.07,187.06,10.91" xml:id="b7">
	<monogr>
		<ptr target="https://github.com/stopwords-iso/stopwords-fr" />
		<title level="m" coord="16,112.66,314.52,314.99,10.91">Stopwords-Iso, Stopwords-iso/stopwords-fr: French stopwords collection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,341.62,352.82,10.91" xml:id="b8">
	<monogr>
		<ptr target="https://www.ranks.nl/stopwords/french" />
		<title level="m" coord="16,112.66,341.62,116.11,10.91">Ranks.nl french stopwords</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,355.17,394.62,10.91;16,112.66,368.71,254.93,10.91" xml:id="b9">
	<monogr>
		<ptr target="https://github.com/stdlib-js/datasets-savoy-stopwords-fr" />
		<title level="m" coord="16,112.66,355.17,334.21,10.91">Stdlib-Js, Stdlib-js/datasets-savoy-stopwords-fr: A list of french stop words</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,382.26,257.76,10.91" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Opennlp</forename><surname>Apache</surname></persName>
		</author>
		<ptr target="https://opennlp.apache.org/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,395.81,320.85,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">N</forename><surname>Group</surname></persName>
		</author>
		<ptr target="https://stanfordnlp.github.io/CoreNLP/" />
		<title level="m" coord="16,169.24,395.81,31.29,10.91">corenlp</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,409.36,388.90,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<ptr target="https://platform.openai.com/docs/models/gpt-3-5" />
		<title level="m" coord="16,152.79,409.36,69.14,10.91">Models -openai</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,436.46,276.20,10.91" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Apache</surname></persName>
		</author>
		<ptr target="https://github.com/DmitryKey/luke" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
