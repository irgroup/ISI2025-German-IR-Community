<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,407.40,15.42;1,89.29,106.66,356.05,15.42;1,89.29,129.00,205.10,11.96">IUEXIST: Multilingual Pre-trained Language Models for Sexism Detection on Twitter in EXIST2023 Notebook for the EXIST Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,154.90,78.98,11.96"><forename type="first">Yash</forename><forename type="middle">A</forename><surname>Hatekar</surname></persName>
							<email>yhatekar@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomingtonm</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.56,154.90,99.48,11.96"><forename type="first">Muhammad</forename><forename type="middle">S</forename><surname>Abdo</surname></persName>
							<email>mabdo@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomingtonm</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.69,154.90,80.36,11.96"><forename type="first">Snigdha</forename><surname>Khanna</surname></persName>
							<email>snkhanna@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomingtonm</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.05,154.90,69.50,11.96"><forename type="first">Sandra</forename><surname>KÃ¼bler</surname></persName>
							<email>skuebler@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomingtonm</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,407.40,15.42;1,89.29,106.66,356.05,15.42;1,89.29,129.00,205.10,11.96">IUEXIST: Multilingual Pre-trained Language Models for Sexism Detection on Twitter in EXIST2023 Notebook for the EXIST Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B754ABB83272CC19405FE9A61FB3778D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SEXISM DETECTION</term>
					<term>TRANSFORMERS</term>
					<term>DEEP LEARNING</term>
					<term>PRE-PROCESSING</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe an approach towards sexism detection in tweets, for the EXIST 2023-Task 1, a shared task on sexism identification. The dataset for this task consists of English and Spanish tweets. Task 1 is a binary classification task, where our system needs to decide whether a given tweet contains sexist expressions or behaviors. We describe our experiments with different machine learning algorithms and vector lengths, algorithms including Multinomial Naive Bayes, SVM, XGBoost, transformers, and Distilbert. The best model performance was achieved by an ensemble of transformers including XLM-Roberta small and large and TwHIN-BERT base and large, combined using XGBoost. The ensemble was trained on the original tweets dataset plus additional training data from the 2021 shared task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The past two decades witnessed an unprecedented surge in the amount of online content produced by social network users. Unfortunately, the rapid growth and ubiquity of this content made them a fertile ground for darker human emotions, including sexism. The definition of sexism often varies, but it generally refers to discriminatory practices or beliefs on the basis of sex or gender. It can take on various forms, which may range from subtle and indirect to overt and hidden expressions. Most often, these forms of discrimination are expressed against women with the aim of humiliating or objectifying them, destroying their reputation, undervaluing their skills and opinions, or making them feel fearful and vulnerable <ref type="bibr" coords="1,394.13,493.43,11.32,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,408.17,493.43,7.46,10.91" target="#b1">2,</ref><ref type="bibr" coords="1,418.35,493.43,7.46,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,428.53,493.43,7.55,10.91" target="#b3">4]</ref>. Hence, hatred, threats, harassment, intimidation, and disparagement may all be the results of such sexist content. Fox et al. <ref type="bibr" coords="1,172.23,520.53,12.84,10.91" target="#b4">[5]</ref> argue that sexist behavior is promoted due to the 'online dis-inhibition effect', i.e., online users who remain anonymous may exhibit behaviors that they would not typically display in face-to-face situations or when their identity is known. They also argue that engaging with sexist content online may lead to sexist attitudes offline. As a result, the automatic detection and classification of this content into distinct categories have become a critical task to promote gender equality and create a safe online environment for everyone.</p><p>Machine learning techniques have proven to be effective in detecting and classifying sexist content. By training machine learning models on large datasets labeled for sexism, algorithms can learn the patterns and features that characterize such content. Both binary classification (i.e., sexist and non-sexist) or a more fine-grained classification, such as implicit and direct sexism exist <ref type="bibr" coords="2,145.43,315.98,11.31,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,159.47,315.98,7.46,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,169.66,315.98,7.54,10.91" target="#b7">8]</ref>. Nevertheless, detecting sexist content on online platforms is challenging, especially on Twitter. Tweets are typically short, making it difficult for the models to extract unique patterns and features which discriminate sexist from non-sexist content. Also, because Twitter users have to limit their tweets to a small number of words, they resort to using nonstandard language, emojis, and abbreviations, among other ways, to send their messages in the shortest form. Additionally, sarcasm, irony, and vague language make it difficult for the models to perform well <ref type="bibr" coords="2,161.35,397.28,11.43,10.91" target="#b8">[9]</ref>.</p><p>In this paper, we present our IUEXIST team submissions for the EXIST 2023 <ref type="bibr" coords="2,439.61,410.83,16.41,10.91" target="#b9">[10,</ref><ref type="bibr" coords="2,458.74,410.83,14.02,10.91" target="#b10">11]</ref> Shared Task 1. For this task, the system needs to decide whether a given tweet in English or Spanish is sexist or not. As per the task, sexist is chosen if the tweet i) is sexist, ii) describes a sexist situation, iii) criticizes a sexist behavior. Table <ref type="table" coords="2,297.13,451.47,5.07,10.91" target="#tab_0">1</ref> shows sample sexist and non-sexist tweets in both English and Spanish.</p><p>The remainder of the paper is organized as follows. In section 2, we present a review of related work on detecting sexist content on various online platforms. In section 3, we describe our methodology, including the dataset, data pre-processing, and the machine learning algorithms used in our experiments. We then present our experimental results and discuss the implications of our findings in section 4. Finally, in section 5, we conclude the paper by summarizing our contributions, discussing the limitations of our study, and outlining avenues for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The SemEval-2023 Shared Task 10 <ref type="bibr" coords="2,248.64,604.94,18.07,10.91" target="#b11">[12]</ref> aimed to improve the automatic detection of online sexism. Unlike previous studies that focused on the binary classification of sexist content, this task introduces a new hierarchical taxonomy of sexist content that contains granular vectors of sexism. The study uses a dataset of 20k social media comments and aims to create more accurate and explainable models for sexism detection. The taxonomy included four categories (Threats, Derogation, Animosity, and Prejudiced discussion) and 11 subcategories (e.g., threats of harm, aggressive attacks, gender stereotypes, and supporting mistreatment of women). The data used in the study was compiled from both Reddit and Gab, and sexist content was later annotated by highly-trained female annotators. The shared task involved three main tasks: a) a binary classification (sexist vs non-sexist), b) a four-category classification, and c) an 11fine-grained-vector classification. The leading system in Task A employed a multi-task DNN structure and performed additional pretraining of DeBERTa-v3 and TwHIN-BERT on the starter kit unlabelled data, as well as an extra dataset. In Task B, the top-performing system utilized an instruction-tuned Pathways Language Model (PaLM) with the model, with a prompt that was parameter-efficient and tuned specifically for the task data. The system used majority voting over six iterations. Lastly, for Task C, the best-performing system conducted further training of DeBERTa-v3 using the starter kit unlabelled data and incorporated a second loss term known as normalized temperature-scaled cross entropy.</p><p>Almanea and Poesio <ref type="bibr" coords="3,192.41,263.11,17.79,10.91" target="#b12">[13]</ref> created a corpus of Arabic misogynistic tweets, annotated by three annotators, and used it to train a model for classifying tweets for misogyny using AraBERT. They trained a binary classifier for each coder with soft loss functions and a majority vote hard training. The results showed that the model trained using CE soft loss had the highest accuracy (77.79%) and F1-score (77.38), but had a relatively higher cross-entropy (0.586) and JSD (0.244) compared to the other models. The overall agreement between the three annotators was low, and the results suggest that annotator subjectivity has a significant impact on the accuracy of machine learning models for classifying sexist language.</p><p>Parikh et ak. <ref type="bibr" coords="3,159.00,371.50,12.84,10.91" target="#b7">[8]</ref> developed a semi-supervised multi-task learning neural framework for the multi-label fine-grained sexism classification of accounts of sexism, using sentence representations from word embeddings and pre-trained models. The study used a dataset of 13 023 accounts of sexism that was tagged with 23 different categories of sexism, created by trained annotators who had formal experience with studying gender and/or sexuality. The study explored different baselines and approaches for classifying sexism. With regard to baselines, random labeling and traditional machine learning methods such as SVM, random forest, and logistic regression were explored. The features chosen in these methods include TF-IDF on character n-grams, word unigrams, and bigrams, ELMo embeddings, and a composite set of features. Additionally, they explored various deep learning architectures, including LSTM-based architectures such as biLSTM, biLSTM-Attention, and hierarchical-biLSTM-Attention. They also included sentence embeddings with biLSTM-attention, CNN-based architectures such as CNN-Kim and C-biLSTM, and CNN-biLSTM-Attention. Logistic regression with averaged ELMo embeddings as features was found to perform best among the traditional ML methods with an F1 score of 0.595, and a macro-F of 0.479. Among the deep learning baselines, biLSTM-Attention (F1: 0.728, macro-F: 0.650) and Hierarchical-biLSTM-Attention (F1: 0.725, macro-F: 0.650) were the best models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>For the development of the EXIST dataset, over 400 popular expressions and terms that are commonly used to undermine women's roles in society, in English and Spanish, were used as search terms. Overall, the original training data consists of 3,660 Spanish tweets and 3,260 English tweets. We also used additional data from the EXIST task1 datasets in 2021 and 2022 <ref type="bibr" coords="4,89.29,114.06,11.23,10.91" target="#b3">[4,</ref><ref type="bibr" coords="4,103.16,114.06,12.23,10.91" target="#b13">14]</ref>. The final size of the training dataset is 8 960 tweets in Spanish and English out of which 5 593 were sexist and 3 367 non-Sexist.</p><p>Since the tweets were provided with six annotator votes, we use a majority voting scheme to label the tweets as either sexist or not-sexist. For tweets for which there was a tie in the annotations, we consider them sexist to partly address the class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Pre-Processing</head><p>The data pre-processing step involved five steps. 1) Any URLs were replaced by 'URL'. 2) Retweet 'RT', which is not relevant to the task, was removed. 3) Usernames were replaced with the word 'USER'. 4) Emojis were converted to their corresponding text equivalents using the Python library 'emoji' (https://pypi.org/project/emoji/). 5) All non-alphanumeric characters, except apostrophes and spaces, were removed.</p><p>A first attempt to eliminate hashtags showed that they are helpful and should not be deleted. For instance, the tweet "#Catcalling is #Harassment. It's Not a Compliment. It's never okay. #feminist #feminism #stopstreetharassment https:// t.co/ g5nJy12sIl'", is labeled as sexist by the majority of annotators in the training dataset. Removing the hashtags results in the removal of all relevant content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classifiers</head><p>We used a range of classifiers: multinomial Naive Bayes and Support Vector Machines using the scikit-learn implementation <ref type="bibr" coords="4,212.93,403.21,16.09,10.91" target="#b14">[15]</ref>, XGBoost <ref type="bibr" coords="4,277.03,403.21,16.09,10.91" target="#b15">[16]</ref>, DistilBERT <ref type="bibr" coords="4,350.87,403.21,16.09,10.91" target="#b16">[17]</ref>, RoBERTa <ref type="bibr" coords="4,417.07,403.21,16.09,10.91" target="#b17">[18]</ref>, XLM-RoBERTa <ref type="bibr" coords="4,89.29,416.76,16.38,10.91" target="#b18">[19]</ref>, and TwHIN <ref type="bibr" coords="4,168.14,416.76,16.38,10.91" target="#b19">[20]</ref>. We used HuggingFace to fine-tune these transformers for our Twitter dataset.</p><p>Since the transformers can only accept input of a predetermined maximum length, we experiment with different vector lengths and found the following lengths optimal: 95 for XML-RoBERTa base and 128 for the remaining transformers.</p><p>For the multinomial Naive Bayes and SVM, we used the default parameters. To hyperparameterize XGBoost, we used GridSearchCV along with a five-fold cross-validation.The best hyperparameters were identified as a maximum depth of 128, a learning rate of 0.1, the number of estimators set to 200, a seed of 47, and the internal evaluation metric set to logloss. We used HuggingFace's Autotrain to fine-tune the transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation</head><p>The official score in task 1 is ICM (Information Contrast Measure) <ref type="bibr" coords="4,395.87,588.43,16.41,10.91" target="#b20">[21]</ref>, thus we report our results using this metric. We also report macro-averaged F1 in the HARD-HARD setting.</p><p>After considering former studies and the gold labels provided by the guidelines, we decided to focus on the Hard-Hard evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Official Results</head><p>We submitted two systems for evaluation. IUEXIST_1 uses XLM-RoBERTa Large trained on the official training set provided by the shared task. IUEXIST_2 uses an ensemble of four transformers, XLM-RoBERTa base and large, and TwHIN base and large. We then train XGBoost on the output of the transformers. All the ensemble models are trained on the combination of the official training set and the additional data (see Section 3.1).</p><p>Table <ref type="table" coords="5,127.84,393.51,5.17,10.91" target="#tab_1">2</ref> provides a summary of the official result or our team's submission. These results show that IUEXIST_2 performs slightly better than IUEXIST_1 in the hard evaluation while IUEXIST_1 performs significantly better in the soft evaluation. The gains of IUEXIST_2 in the hard evaluation are due to gains in Spanish, where the ICM is about 0.02 higher than for IUEXIST_1 (0.5460 for IUEXIST_2 and 0.5294 for IUEXIST_1). These gains are offset by a smaller loss for English. The good performance of IUEXIST_1 in the soft evaluation is due to its performance in English (ICM: 0.7115 vs. 0.6141 for IUEXIST_2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on the Development Set</head><p>In addition to the officially submitted systems, we performed a more extensive evaluation on the development set.</p><p>We trained and evaluated the 10 different classifiers and the ensemble described in Section 3.3, the individual classifiers trained on the original training set, and the ensemble on the extended training set.</p><p>The results of these experiments are shown in Table <ref type="table" coords="5,342.43,592.28,3.81,10.91" target="#tab_2">3</ref>. Since ICM and the F-scores show the same trends, we will focus on ICM scores in this analysis. The best model performance was achieved by the ensemble without pre-processing with an ICM of 0.5873. The non-neural classifiers generally show lower performance than the transformers. Among the latter, the XLM-RoBERTa base shows the best performance, with an ICM of 0.5716.</p><p>When we look at the question of whether pre-processing is useful, we see that some classifiers, such as the multinomial Naive Bayes and RoBERTa base profit from pre-processing, but for most models, pre-processing is detrimental. Since our official results leave us with the question of whether the gains of the IUEXIST_2 model over IUEXIST_1 are due to the ensemble approach or to the extended training set, we perform a comparison including an experiment where we use the ensemble with only the current training set. The results are shown in Table <ref type="table" coords="6,317.02,567.61,3.67,10.91" target="#tab_3">4</ref>. They show that the gains are mostly due to the additional training data, the ICM for the ensemble with this year's training data only reaches 0.5634, in comparison to 0.5534 for the XLM-RoBERTa model. Adding the 2021 training data adds a larger gain to 0.5873.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented our submissions to the EXIST 2023 shared task 1. We found that an ensemble of four different transformers with XGBoost for voting provides the best results in the HARD-HARD evaluation (rank 15). However, for the SOFT-SOFT evaluation, we found XML-RoBERTa to reach significantly higher results. This system was ranked 9th out of 70 submissions.</p><p>As described above, this system was developed as a project in a course on machine learning. For most of us, this was our first time collaborating on a shared NLP task, drawing on experiences and discussions with individuals from various academic backgrounds and cultural perspectives. One of the key challenges we faced was being able to confine ourselves to the definition of Sexism, as multiple examples in the data set seemed to be open to interpretation depending on the context.</p><p>Yet another challenge was knowing how to sift through the extensive amount of technical information and resources available to us, and to direct our attention to problem-solving through continuous learning.</p><p>For the future, we plan on investigating the effect of using additional training data for the different systems since the results showed that adding more training data was more successful than going from a single transformer to the ensemble. On the one hand, adding training data can help combat data sparsity, but it also adds the risk of distorting the class distribution. We are also interested in a long-term evaluation to see to what degree the temporal distance between the training and test has a negative effect on performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,380.40,120.24"><head>Table 1</head><label>1</label><figDesc>Examples from the EXIST dataset.</figDesc><table coords="2,127.55,118.18,341.85,92.55"><row><cell>Sexist</cell><cell>Call me sexist but it just feels wrong that women are reffing the NBA</cell></row><row><cell></cell><cell>like go ref the WNBA.</cell></row><row><cell></cell><cell>Esta gringa sigue llorando por el gamergate, que coincidenciaque tenga</cell></row><row><cell></cell><cell>pronombres en su perfil</cell></row><row><cell cols="2">Non-Sexist Even if you get embarrassed and blush, you can still confront hard</cell></row><row><cell></cell><cell>things. #KeepMoving</cell></row><row><cell></cell><cell>Los polÃ­ticos acostumbran a hablarle al pueblo como si fueran una man-</cell></row><row><cell></cell><cell>ada de estÃºpidos pero lamanada no hacemos nada por contradecirlos.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,350.10,163.29"><head>Table 2</head><label>2</label><figDesc>Official results of the IUEXIST submissions.</figDesc><table coords="5,156.18,119.88,282.92,133.90"><row><cell></cell><cell></cell><cell></cell><cell>HARD-HARD</cell><cell></cell><cell>SOFT-SOFT</cell></row><row><cell cols="2">Language Model</cell><cell>Rank</cell><cell>ICM</cell><cell cols="2">F1 Rank</cell><cell>ICM</cell></row><row><cell>All</cell><cell>IUEXIST_1</cell><cell cols="3">16 0.5313 0.7734</cell><cell>9 0.7115</cell></row><row><cell></cell><cell>IUEXIST_2</cell><cell cols="3">15 0.5341 0.7717</cell><cell>17 0.6141</cell></row><row><cell></cell><cell>baseline</cell><cell cols="2">0 0.9948</cell><cell>1</cell><cell>0 3.1182</cell></row><row><cell>English</cell><cell>IUEXIST_1</cell><cell cols="3">19 0.5225 0.7509</cell><cell>9 0.6802</cell></row><row><cell></cell><cell>IUEXIST_2</cell><cell cols="3">24 0.5059 0.7419</cell><cell>19 0.3893</cell></row><row><cell></cell><cell>baseline</cell><cell cols="2">0 0.9798</cell><cell>1</cell><cell>0 3.1141</cell></row><row><cell>Spanish</cell><cell>IUEXIST_1</cell><cell cols="3">16 0.5294 0.7907</cell><cell>14 0.7076</cell></row><row><cell></cell><cell>IUEXIST_2</cell><cell cols="3">13 0.5460 0.7942</cell><cell>12 0.7479</cell></row><row><cell></cell><cell>baseline</cell><cell cols="2">0 0.9999</cell><cell>1</cell><cell>0 3.1177</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,381.60,278.59"><head>Table 3</head><label>3</label><figDesc>Evaluation of different models and pre-processing settings on the development set.</figDesc><table coords="6,124.68,122.10,345.91,246.97"><row><cell cols="2">Pre-processing Classifier</cell><cell cols="3">ICM (hard) F1 (positive) macro F1</cell></row><row><cell>original</cell><cell>multinomial Naive Bayes</cell><cell>0.1354</cell><cell>0.6785</cell><cell>0.6729</cell></row><row><cell></cell><cell>Support Vector Machines</cell><cell>0.2738</cell><cell>0.7108</cell><cell>0.7180</cell></row><row><cell></cell><cell>XGBoost</cell><cell>0.3220</cell><cell>0.7273</cell><cell>0.7332</cell></row><row><cell></cell><cell>DistilBERT</cell><cell>0.3822</cell><cell>0.7427</cell><cell>0.7522</cell></row><row><cell></cell><cell>ROBERTA base</cell><cell>0.4233</cell><cell>0.7479</cell><cell>0.7650</cell></row><row><cell></cell><cell>XLM-RoBERTa base</cell><cell>0.5716</cell><cell>0.8025</cell><cell>0.8120</cell></row><row><cell></cell><cell>XLM-RoBERTa large</cell><cell>0.5547</cell><cell>0.7965</cell><cell>0.8067</cell></row><row><cell></cell><cell>TwHIN base</cell><cell>0.5130</cell><cell>0.7856</cell><cell>0.7934</cell></row><row><cell></cell><cell>TwHIN large</cell><cell>0.5377</cell><cell>0.7884</cell><cell>0.8014</cell></row><row><cell></cell><cell>ensemble</cell><cell>0.5873</cell><cell>0.8054</cell><cell>0.8171</cell></row><row><cell cols="2">pre-processing multinomial Naive Bayes</cell><cell>0.1446</cell><cell>0.6811</cell><cell>0.6761</cell></row><row><cell></cell><cell>Support Vector Machines</cell><cell>0.2377</cell><cell>0.7009</cell><cell>0.7067</cell></row><row><cell></cell><cell>XGBoost</cell><cell>0.2638</cell><cell>0.7079</cell><cell>0.7149</cell></row><row><cell></cell><cell>DistilBERT</cell><cell>0.3757</cell><cell>0.7416</cell><cell>0.7502</cell></row><row><cell></cell><cell>RoBERTa base</cell><cell>0.4833</cell><cell>0.7749</cell><cell>0.7841</cell></row><row><cell></cell><cell>XLM-RoBERTa base</cell><cell>0.5167</cell><cell>0.7856</cell><cell>0.7947</cell></row><row><cell></cell><cell>XLM-RoBERTa large</cell><cell>0.5487</cell><cell>0.7996</cell><cell>0.8045</cell></row><row><cell></cell><cell>TwHIN base</cell><cell>0.5206</cell><cell>0.7876</cell><cell>0.7958</cell></row><row><cell></cell><cell>TwHIN large</cell><cell>0.5100</cell><cell>0.7841</cell><cell>0.7925</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,391.33,362.04,81.83"><head>Table 4</head><label>4</label><figDesc>Comparing ensemble variations.</figDesc><table coords="6,144.24,422.95,306.79,50.21"><row><cell>Classifier</cell><cell>Train</cell><cell cols="3">ICM (hard) F1 (positive) macro F1</cell></row><row><cell cols="2">XLM-ROBERTA large 2023</cell><cell>0.5547</cell><cell>0.7965</cell><cell>0.8067</cell></row><row><cell>ensemble</cell><cell>2023</cell><cell>0.5634</cell><cell>0.7980</cell><cell>0.8095</cell></row><row><cell>ensemble</cell><cell>extended</cell><cell>0.5873</cell><cell>0.8054</cell><cell>0.8171</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,413.79,395.17,10.91;7,112.66,427.34,393.33,10.91;7,112.66,440.89,76.13,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,255.79,413.79,252.04,10.91;7,112.66,427.34,254.78,10.91">Niyati ad Varma, Fine-grained multi-label sexism classification using a semi-supervised multi-level neural approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abburi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chhaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,375.75,427.34,130.24,10.91">Data Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="359" to="379" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,454.44,393.32,10.91;7,112.66,467.99,393.33,10.91;7,112.66,481.54,394.61,10.91;7,112.31,495.09,153.48,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,445.44,454.44,60.54,10.91;7,112.66,467.99,200.87,10.91">An annotated corpus for sexism detection in French tweets</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chiril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Origgi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coulomb-Gully</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.lrec-1.175" />
	</analytic>
	<monogr>
		<title level="m" coord="7,337.00,467.99,168.99,10.91;7,112.66,481.54,164.48,10.91">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1397" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,508.64,393.33,10.91;7,112.66,522.18,172.57,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,259.22,508.64,246.77,10.91;7,112.66,522.18,37.90,10.91">Understanding subtle sexism: Detection and use of sexist language</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Swim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mallett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stangor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,159.22,522.18,42.08,10.91">Sex Roles</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,535.73,394.53,10.91;7,112.66,549.28,393.33,10.91;7,112.66,562.83,161.06,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,112.66,549.28,300.83,10.91">Overview of EXIST 2021: sEXism Identification in Social neTworks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>RodrÃ­guez-SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Comet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Donoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,422.82,549.28,83.17,10.91;7,112.66,562.83,77.13,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="195" to="207" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,576.38,393.32,10.91;7,112.66,589.93,393.98,10.91;7,112.41,603.48,38.81,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,221.26,576.38,284.72,10.91;7,112.66,589.93,199.66,10.91">Perpetuating online sexism offline: Anonymity, interactivity, and the effects of sexist hashtags on social media</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,321.24,589.93,142.27,10.91">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="436" to="442" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,617.03,393.33,10.91;7,112.66,630.58,393.33,10.91;7,112.66,644.13,311.95,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,195.61,617.03,310.38,10.91;7,112.66,630.58,166.76,10.91">When does a compliment become sexist? Analysis and classification of ambivalent sexism using Twitter data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mamidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,302.41,630.58,203.58,10.91;7,112.66,644.13,148.79,10.91">Proceedings of the Second Workshop on NLP and Computational Social Science</title>
		<meeting>the Second Workshop on NLP and Computational Social Science<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,657.68,393.32,10.91;8,112.66,86.97,393.33,10.91;8,112.66,100.52,164.48,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,218.42,657.68,287.56,10.91;8,112.66,86.97,162.97,10.91">Learning and understanding different categories of sexism using convolutional neural network&apos;s filters</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sharifirad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jacovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,296.71,86.97,209.28,10.91;8,112.66,100.52,16.07,10.91">Proceedings of the 2019 Workshop on Widening NLP</title>
		<meeting>the 2019 Workshop on Widening NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,395.17,10.91;8,112.66,127.61,393.33,10.91;8,112.66,141.16,393.33,10.91;8,112.66,154.71,393.33,10.91;8,112.66,168.26,148.38,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,479.36,114.06,28.47,10.91;8,112.66,127.61,303.12,10.91">Multilabel categorization of accounts of sexism using a neural framework</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abburi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,439.63,127.61,66.35,10.91;8,112.66,141.16,393.33,10.91;8,112.66,154.71,360.43,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1642" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,248.64,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,311.21,181.81,194.77,10.91;8,112.66,195.36,119.23,10.91">Sexism identification using BERT and data augmentation -EXIST2021</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,255.55,195.36,74.83,10.91">IberLEF@ SEPLN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,208.91,394.53,10.91;8,112.66,222.46,393.33,10.91;8,112.66,236.01,394.53,10.91;8,112.66,249.56,393.33,10.91;8,112.66,263.11,333.46,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,112.66,222.46,393.33,10.91;8,112.66,236.01,72.05,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,401.00,249.56,104.99,10.91;8,112.66,263.11,206.17,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,276.66,394.53,10.91;8,112.66,290.20,393.33,10.91;8,112.66,303.75,393.33,10.91;8,112.33,317.30,395.22,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,112.66,290.20,393.33,10.91;8,112.66,303.75,164.31,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,141.67,317.30,335.74,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,330.85,393.33,10.91;8,112.66,344.40,279.69,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>RÃ¶ttger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04222</idno>
		<idno>arXiv</idno>
		<title level="m" coord="8,292.97,330.85,213.01,10.91;8,112.66,344.40,61.78,10.91">SemEval-2023 Task 10: Explainable Detection of Online Sexism</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,112.66,357.95,393.54,10.91;8,112.66,371.50,393.33,10.91;8,112.66,385.05,395.00,10.91;8,112.66,398.60,86.98,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,219.80,357.95,286.40,10.91;8,112.66,371.50,111.85,10.91">ArMIS -the Arabic misogyny and sexism corpus with annotator subjective disagreements</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almanea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.244" />
	</analytic>
	<monogr>
		<title level="m" coord="8,251.88,371.50,254.10,10.91;8,112.66,385.05,98.55,10.91">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2282" to="2291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,412.15,395.17,10.91;8,112.66,425.70,394.62,10.91;8,112.66,439.25,393.98,10.91;8,112.66,452.79,38.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,395.80,425.70,111.47,10.91;8,112.66,439.25,177.79,10.91">Overview of EXIST 2022: Sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>RodrÃ­guez-SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendieta-AragÃ³n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marco-RemÃ³n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Makeienko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,299.39,439.25,164.18,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,466.34,394.53,10.91;8,112.66,479.89,394.53,10.91;8,112.48,493.44,264.75,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,317.81,479.89,184.33,10.91">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.48,493.44,170.67,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,506.99,393.33,10.91;8,112.28,520.54,394.91,10.91;8,112.66,534.09,148.45,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,205.76,506.99,174.84,10.91">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,402.52,506.99,103.47,10.91;8,112.28,520.54,389.74,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD)</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,547.64,394.52,10.91;8,112.66,561.19,331.54,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Wolf</surname></persName>
		</author>
		<idno>ArXiv</idno>
		<title level="m" coord="8,295.03,547.64,212.15,10.91;8,112.66,561.19,120.63,10.91">DistilBERT, a distilled version of BERT: Smaller, Faster, Cheaper and Lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,112.66,574.74,394.53,10.91;8,112.30,588.29,393.68,10.91;8,112.66,601.84,155.87,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,172.21,588.29,281.35,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,112.66,615.39,394.53,10.91;8,112.66,628.93,393.33,10.91;8,112.66,642.48,393.32,10.91;8,112.66,656.03,395.01,10.91;8,112.66,669.58,191.55,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,271.58,628.93,234.41,10.91;8,112.66,642.48,20.10,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>GuzmÃ¡n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.747.doi:10.18653/v1/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m" coord="8,155.39,642.48,350.59,10.91;8,112.66,656.03,46.07,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,394.61,10.91;9,112.28,100.52,394.91,10.91;9,112.33,114.06,210.37,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="9,443.95,86.97,63.32,10.91;9,112.28,100.52,390.35,10.91">TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Florez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Kishky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07562</idno>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,112.66,127.61,394.62,10.91;9,112.66,141.16,393.32,10.91;9,112.33,154.71,122.11,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,223.40,127.61,260.55,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,141.16,393.32,10.91;9,112.33,154.71,23.88,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
