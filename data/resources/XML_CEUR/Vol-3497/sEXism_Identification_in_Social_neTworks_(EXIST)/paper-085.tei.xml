<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.55,451.10,15.35;1,72.00,100.83,451.01,15.35;1,72.00,123.87,75.70,15.35">Towards Robust Online Sexism Detection: A Multi-Model Approach with BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,166.78,87.45,10.54"><forename type="first">Hadi</forename><surname>Mohammadi</surname></persName>
							<email>1h.mohammadi@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Methodology and Statistics</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.76,166.78,101.48,10.54"><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
							<email>a.giachanou@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Methodology and Statistics</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.71,166.78,72.85,10.54"><forename type="first">Ayoub</forename><surname>Bagheri</surname></persName>
							<email>bagheri@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Methodology and Statistics</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.55,451.10,15.35;1,72.00,100.83,451.01,15.35;1,72.00,123.87,75.70,15.35">Towards Robust Online Sexism Detection: A Multi-Model Approach with BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDD5851E1BFE7D3A80589BA523EE5AE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Online Sexism</term>
					<term>Natural Language Processing (NLP)</term>
					<term>Transformer-based Models</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research investigates the application of pre-trained transformer-based models, including BERT, XLM-RoBERTa, and DistilBERT, in the context of the EXIST 2023 shared task, which focuses on identifying and categorizing online sexism. The study emphasizes the crucial role of Natural Language Processing (NLP) in detecting harmful content, and it draws on previous competitions that have incorporated tasks to detect hate speech and abusive language. The methodology combines various advanced techniques from the text classification domain, including the use of additional datasets, data preprocessing, and model building. The research also explores data augmentation techniques and label encoding as preprocessing steps. The study's findings indicate that the developed model performs optimally in English, and it suggests that the use of a voting system and the combination of outputs from multiple models contribute to the overall performance. The research concludes with a call for sustained initiatives to curb the prevalence of harmful content on digital platforms, and it outlines future work directions, including incorporating additional information about annotators, the assessment of annotator reliability, and exploring more sophisticated techniques for handling imbalances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The advent of social media has revolutionized the way we communicate, interact, and disseminate information. However, this digital revolution that allows everyone to publish posts quickly and easily has resulted in a concerning amount of harmful content, such as hate speech, discrimination, and sexism. Despite efforts to mitigate these issues, the identification and categorization of sexism in online platforms remain challenging due to such expressions' nuanced and context-dependent nature <ref type="bibr" coords="1,485.44,543.25,12.84,9.69" target="#b0">[1]</ref> Natural Language Processing (NLP) has witnessed remarkable progress in the past decade, primarily due to the emergence of deep learning algorithms for text classification <ref type="bibr" coords="1,389.58,568.45,11.70,9.69" target="#b1">[2]</ref>. These advancements have resulted in the development of models that can better understand and interpret human language, thereby paving the way for many applications, including identifying and classifying sexist content. Advanced Natural Language Processing (NLP) methods and Language Models can comprehend the subtleties and context-dependent language often employed in sexist expressions and, therefore, can be used to detect harmful content <ref type="bibr" coords="1,144.72,631.81,11.70,9.69" target="#b2">[3]</ref>.</p><p>Several shared tasks and competitions, such as SemEval, have incorporated tasks to detect hate speech and abusive language. In 2023, the EXIST (sEXism Identification in Social neTworks) lab was launched as part of the CLEF (Conference and Labs of the Evaluation Forum) conference focusing on addressing the problem of sexism detection <ref type="bibr" coords="2,272.37,87.49,11.70,9.69" target="#b4">[4]</ref>. To address those challenges, we participated in the EXIST 2023 competition on sexism detection in social media platforms <ref type="bibr" coords="2,397.69,100.21,11.70,9.69" target="#b5">[5]</ref>. EXIST 2023 provides a dataset of over 10,000 labeled tweets in English and Spanish which comprises three tasks: Identification of sexism, source intention, and categorization of sexism. Sexism identification is a binary classification task that requires systems to decide whether a tweet contains or describes sexist expressions or behaviors. Source intention is a categorization task that classifies the intention behind the sexist messages into one of three pre-defined categories: direct, reported, or judgmental. The final task, sexism categorization, is a multiclass and multilabel task that categorizes the sexist messages according to the type(s) of sexism they contain. Those categories, such as ideological and inequality, stereotyping and dominance, objectification, and sexual violence, after considering the undermined aspects of women <ref type="bibr" coords="2,72.00,213.97,11.69,9.69" target="#b6">[6]</ref>.</p><p>In the remainder of this paper, we present our methodology for tackling the EXIST 2023 tasks in Section 2, we discuss our experimental results in Section 3, and finally, in Section 4, we summarize our work and present future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We have developed a methodology combining various advanced techniques from the text classification domain to address the sexism detection task. Our methodology consists of several steps, which are shown in Figure <ref type="figure" coords="2,192.13,302.53,4.29,9.69" target="#fig_0">1</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Dataset Description</head><p>In addition to the EXIST 2023 dataset, we also utilized another valuable resource for our research: the dataset provided by the SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS); this dataset was introduced to address the issue of binary detection of sexism, which often overlooks the diversity of sexist content and fails to provide clear explanations for why something is considered sexist <ref type="bibr" coords="2,99.62,627.25,11.70,9.69" target="#b7">[7]</ref>. This dataset contains 20,000 social media comments with fine-grained labels. By integrating the EDOS dataset into our methodology, we enhanced the robustness and explainability of our models for online sexism detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Model Building</head><p>The model-building phase of this research is a distinctive element of the methodology, with a unique architecture that incorporates three different transformer-based models (BERT, XLM-RoBERTa, DistilBERT). Transformer-based models are a category of models that use self-attention mechanisms and have achieved state-of-the-art results in various natural language processing tasks. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model pre-trained on a large corpus of text data. BERT considers the full context of a word by looking at the words that come before and after it, which is why it is described as bidirectional. Understanding the context from both directions allows BERT better to understand the semantic meaning of words and sentences.</p><p>As our second model, we used XLM-Roberta, a variant of the RoBERTa model (a robustly optimized version of BERT), which is trained on a large amount of multilingual data making it highly effective for multilingual tasks.</p><p>DistilBERT is a lighter version of BERT, created through a process called distillation, where the knowledge of the larger BERT model is transferred to a smaller, faster model. Despite being smaller and faster, DistilBERT maintains most of the performance of BERT, making it a good choice for tasks where computational resources are limited.</p><p>Each transformer model processes the input text independently, generating a unique data representation. These representations are then concatenated and passed through a Conv1D layer. Following the Conv1D layer, a MaxPooling1D layer is used. After the pooling layer, the data is flattened. Flattening is a simple operation that transforms a multi-dimensional array into a onedimensional array. This step is necessary because the output of the MaxPooling1D layer is a 2D tensor, while the subsequent layers require a 1D tensor. The flattened data is then subjected to a Dropout layer. Finally, two Dense layers are used. Dense layers are the regular deeply connected neural network layers. In this case, one Dense layer is used for binary classification (task1), and the other for multiclass classification (task2 and 3). These layers output the final predictions of the model, which can then be evaluated and used for making decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experimental Setup</head><p>In this section, we describe the experimental setup we followed to assess the performance of our models. This setup encompassed a range of processes, from system configuration and hyperparameter optimization to model training and validation, all aimed at ensuring the most accurate and reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Preprocessing</head><p>Data loading and preprocessing are critical initial phases in machine learning or data analysis tasks. This process prepares the raw data for subsequent steps, such as model training and testing. After we loaded the data, we continued with the preprocessing phase. The first step in preprocessing was to convert all textual content to lowercase. This step is essential for normalizing the data and reducing complexity, as machine learning algorithms treat uppercase and lowercase versions of the same character as different entities. By converting all text to lowercase, we can ensure that the same word in different cases is recognized as the same word by the algorithm. Next, we removed elements that contained no information for this task, including URLs, memorable characters, and punctuation. Removing them helps reduce the data's dimensionality and makes extracting meaningful information easier for the algorithm. Following this, the text was tokenized and lemmatized. Tokenization is the process of dividing text into individual words or tokens. This step is necessary because machine learning algorithms work with individual units (like words) rather than entire texts. Lemmatization is a further refinement of this process. It reduces words to their base or root form, grouping different grammatical forms of the same word. This helps reduce the data's complexity and makes it easier for the algorithm to identify patterns. Finally, we incorporated the EDOS dataset to enrich the data pool. We followed the same preprocessing steps. Incorporating additional data can help improve the model's robustness and increase its generalizability to unseen data.</p><p>Data augmentation is a strategy used to increase the diversity and amount of training data without collecting new data. This technique is primarily used to prevent overfitting, a common problem where a model performs well on training data but poorly on unseen data. By creating modified versions of the existing data, the model can learn more robust features and generalize better to new data.</p><p>In this research, we used the following data augmentation techniques from the" nlpaug" library in Python:</p><p>• Synonym Replacement: This technique involves replacing words in the text with their synonyms. This increases the linguistic diversity of the dataset and helps the model to understand that different words can have the same meaning. • Random Word Swapping: This technique involves randomly swapping pairs of words in the text. This introduces a controlled noise level into the data and helps the model learn that the order of words can vary while preserving the overall meaning. • Random Character Insertion: This technique involves randomly inserting characters into words. This also introduces a controlled noise level and helps the model learn to handle typos or other minor errors in the input data. Label encoding is a preprocessing step that converts categorical labels into a format that machine learning algorithms can use. Most algorithms expect numerical input and output, so it is necessary to transform categorical labels into numerical values. In this research, we used a majority voting system to decide the labels for the sexism detection task (task 1). This means that the label assigned to each data point was the one that was chosen by the majority of annotators. These labels were binary (indicating the presence or absence of a particular feature) and were encoded into a binary format for compatibility with the machine learning model. For tasks 2 and 3, a multiclass label encoding process was used. This process involved encoding each class as a unique integer. This is a standard method for handling multiclass problems, where each data point can belong to one of several classes. A variety of machine-learning algorithms can easily use the resulting numerical labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Training</head><p>In the training phase, the model learns to map inputs (features) to outputs (labels) based on the training data. We used the Adam optimizer for training. Adam, short for Adaptive Moment Estimation, is a popular choice for deep learning applications due to its adaptive learning rates, meaning it adjusts the learning rate for each weight in the model individually. A learning rate scheduler from callbacks in TensorFlow with a learning rate of 3e-05 and warmup steps of 200 was incorporated to adjust the learning rate during training dynamically. This helps to fine-tune the learning process, often leading to better model performance. An early stopping mechanism was also utilized to prevent needless training once the model's performance ceased to improve significantly. This not only saves computational resources but also helps prevent overfitting. Mixed precision training was employed to expedite the training process. This method involves using a mix of single-precision (float32) and halfprecision(float16) data types during training, which can significantly reduce the use of computational resources without compromising the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Different Runs</head><p>In We conducted three different runs for Task 1, which involved Sexism Identification. Each run represented a unique combination of methods and data, allowing us to explore various strategies for improving the performance of our model.</p><p>• Task 1 -Run 1 (Binary label-Original dataset): In the first run, we trained our model Without using additional data. We used a voting system that combined the predictions of several models (BERT, XLM-RoBERTa, and DistilBERT) to make a final decision. This approach, often called ensemble learning, can help improve the robustness and accuracy of predictions by leveraging the strengths of multiple models. The task was treated as a binary classification problem, with the model predicting whether each instance was sexist. • Task 1 -Run 2 (Binary label-Original and additional dataset): In the second run, we included additional features derived from the text, metadata, and external resources (EDOS).</p><p>Like the first run, we used a voting system with several models and treated the task as a binary classification problem. The additional information enriched the feature space and gave the model more context for making predictions.</p><p>• Task 1 -Run 3 (Multi-label-Original and additional dataset): In the third run, we again incorporated additional information and used a voting system with several models. However, we treated the task as a multilabel classification problem in this run. This means that each instance could belong to more than one class. We had two labels for the original data (indicating whether the instance was sexist) and two based on the additional data. This approach allowed us to capture more nuanced information about the instances and potentially improve the model's ability to detect different forms or levels of sexism. Our methodology for tasks 2 and 3 is like task 1-run 3. Through different runs, we explored a range of strategies for sexism identification and gained insights into their relative strengths and weaknesses. These findings can inform future work in this area, helping to guide the development of more effective tools for detecting and combating online sexism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Evaluation and Validation</head><p>Model performance is generally defined by its ability to predict unseen data accurately. One of the most popular of these techniques is cross-validation. Cross-validation aims to estimate the model's accuracy on the test set during the model training stage. In this regard, we divide the original data into k partitions of equal size. One partition is designated a validation dataset, and the remaining k-1 partitions are used as training data. These stages enable iterative enhancements to the model and its training process, ultimately creating a reliable machine-learning model that can effectively generalize to new, unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">System Configuration</head><p>Our system leverages a suite of transformers for text processing sourced from the Hugging Face Transformers library, a state-of-the-art platform for natural language processing. After preprocessing, we set a max length of 256 for tokenization, reflecting the average length of the text data. The num-labels parameter was set to 1, indicating binary classification. Hyper-parameters were optimized via Random Search within a defined range of possible values. We examined learning rate values between 1e-5 and 1e-4 and assessed batch sizes 16, 32, and 64 to determine the optimal balance between computational efficiency and model performance. We also implemented a learning rate scheduler that dynamically adjusts the learning rate according to a cosine decay schedule. The warmup period was set to 200 steps, with incremental steps computed based on the number of epochs and the dataset size. We utilized early stopping based on validation loss to prevent overfitting, with the patience set to 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Model Training and Validation Process</head><p>The models were trained using binary cross-entropy loss and the Adam optimizer, a popular choice for deep learning applications due to its adaptive learning rates. We utilized the mixed-precision training policy 'mixed float16' to expedite training without compromising model performance. We employed a custom function to construct models, with each model utilizing a different transformer: 𝑏𝑒𝑟𝑡 -𝑏𝑎𝑠𝑒 -𝑚𝑢𝑙𝑡𝑖𝑙𝑖𝑛𝑔𝑢𝑎𝑙 -𝑢𝑛𝑐𝑎𝑠𝑒𝑑, 𝑥𝑙𝑚 -𝑟𝑜𝑏𝑒𝑟𝑡𝑎 -𝑏𝑎𝑠𝑒, and 𝑑𝑖𝑠𝑡𝑖𝑙𝑏𝑒𝑟𝑡 -𝑏𝑎𝑠𝑒 -𝑚𝑢𝑙𝑡𝑖𝑙𝑖𝑛𝑔𝑢𝑎𝑙 -𝑐𝑎𝑠𝑒𝑑. Outputs from these transformers were concatenated and passed through a 1𝐷 convolutional layer, a max-pooling layer, a dropout layer (rate of 0.5), and a dense layer for binary classification with 𝐿2 regularization.</p><p>Validation was carried out via stratified dataset splitting, with the test size constituting 20 percent of the entire data. This data was tokenized and passed through the trained models. The model training process involved searching for the optimal set of hyperparameters for 3 trials using the validation set. A final model was trained and evaluated after identifying the best hyperparameters (learning rate and batch size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Evaluation and Validation</head><p>To gain a comprehensive understanding of our results, we visualized the data distribution and the proportion of the labels in our data using plots like bar charts, pie charts, or histograms. This provided valuable insights into the imbalance of the dataset. The distribution of task one is given in the diagram below; due to the large volume of task two and three labels, we avoided bringing its diagram. We also conducted a text length analysis to examine the lengths of the text data before and after preprocessing. This helped us determine an appropriate max-length value for the tokenization and padding step. We plotted the loss and accuracy progress over the epochs during the training phase. This provided insight into whether the model was learning effectively or overfitting/underfitting. After training, we evaluated our model on the test data and generated a classification report and a confusion matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Training Performance</head><p>Despite using advanced and similar pre-trained models, various methods, like the early stopping method, have prevented overfitting. The performance of our model on the test dataset was evaluated based on several key metrics: accuracy, precision, recall, and F1 score. In the below table, performance metrics for each task in the test dataset have been shown. According to table 2, in task one, it is clear that the best accuracy was obtained in run two, where the total data of the original and additional data were used. The classification was considered in binary form, indicating that the model has been more accurate on the total data; however, the F index has decreased due to the additional data set in English. Also, the best F1 score is obtained in the third run, related to using the total data and multi-labeling (two labels for the original data and two for the additional data).</p><p>In our model's evaluation, we have utilized Accuracy, Precision, Recall, and F1-score. How-ever, the evaluation competition adopted metrics such as ICM-Hard, ICM-Soft, their normalized versions, and Cross-Entropy, designed to evaluate the model's performance under various scenarios, including hard and soft classifications. The 'hard' scenarios deal with discrete, categorical classifications, similar to Accuracy in our current framework, while the 'soft' scenarios handle continuous, probabilistic classifications. Majority and minority class classifiers were also used as baselines to compare the models' performance over a naïve approach. Our team's best run was ranked 3rd among the participants which was related to 𝑇𝑎𝑠𝑘 3 𝐻𝑎𝑟𝑑 -𝑆𝑜𝑓𝑡 𝐸𝑁. The details of the official result are shown in the below table. The official results, along with a thorough soft and hard analysis, reveal that the developed model performs optimally in English. This outcome may be attributed to the use of a multilingual model as opposed to a Spanish-focused one, as well as the relative abundance of English language information in these models. The use of additional datasets, which were exclusively in English, could also have influenced this result. Future endeavors include experimentation with multiple models catered to different languages. It's also noteworthy to highlight that the developed model outperformed competing teams in the multi-class classification problem (the third task). This success is likely due to the implementation of a voting system and the combination of outputs from multiple models, which collectively contributed to the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusions and Future Work</head><p>Our research presents a comprehensive approach to online sexism detection, leveraging advanced machine learning techniques and natural language understanding methodologies. We utilized a multi-model approach incorporating BERT, XLM-RoBERTa, and DistilBERT to tackle this complex issue. Our methodology was evaluated through a robust experimental setup, and the results demonstrated the effectiveness of our approach in both data sets, but it needs to be improved in dealing with unbalanced datasets by considering the lack of sexist text in the original and additional datasets.</p><p>Looking ahead, we plan to extend our research by incorporating additional information about the annotators, such as gender, age, and other demographic details. This information could provide valuable context that may influence the interpretation and categorization of sexist content. For instance, research has shown that perceptions of sexism can vary significantly based on an individual's personal experiences and perspectives (Burn, 2000; Swim et al., 2005) Moreover, we aim to assess the reliability of the annotators. Annotator reliability is a crucial aspect of any study involving human annotation, as it can significantly impact the quality and validity of the data <ref type="bibr" coords="9,159.77,503.89,11.67,9.69" target="#b8">[8]</ref>. By evaluating the reliability of our annotators, we can ensure that our dataset is robust and reliable, thereby enhancing the validity of our findings. In addition, we plan to explore more sophisticated techniques for handling imbalanced data and methods for dealing with subtlety and context dependency in sexist expressions. We also aim to test our model on different datasets and contexts to assess its generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Training</head><p>In the training phase, the model learns to map inputs (features) to outputs (labels) based on the training data. We used the Adam optimizer for training. Adam, short for Adaptive Moment Estimation, is a popular choice for deep learning applications due to its adaptive learning rates, meaning it adjusts the learning rate for each weight in the model individually. A learning rate scheduler from callbacks in TensorFlow with a learning rate of 3e-05 and warmup steps of 200 was incorporated to adjust the learning rate during training dynamically. This helps to fine-tune the learning process, often leading to better model performance. An early stopping mechanism was also utilized to prevent needless training once the model's performance ceased to improve significantly. This not only saves computational resources but also helps prevent overfitting. Mixed precision training was employed to expedite the training process. This method involves using a mix of single-precision (float32) and half-precision(float16) data types during training, which can significantly reduce the use of computational resources without compromising the model's performance.</p><p>We plotted the loss and accuracy progress over the epochs during the training phase. This provided insight into whether the model was learning effectively or overfitting/underfitting. After training, we evaluated our model on the test data and generated a classification report and a confusion matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Availability</head><p>In this article, the dataset utilized is specifically associated with EXIST 2023 competition. Furthermore, the code utilized in the study was made accessible through a dedicated Zenodo <ref type="bibr" coords="10,163.18,235.57,17.60,9.69" target="#b9">[10]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,203.47,518.86,188.13,9.95"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model Architecture Visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,238.29,433.18,118.63,10.19;6,83.00,272.61,408.09,157.30"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: label 1 distribution</figDesc><graphic coords="6,83.00,272.61,408.09,157.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,220.93,397.66,153.35,10.19;7,86.20,72.00,403.00,316.44"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of Text Length</figDesc><graphic coords="7,86.20,72.00,403.00,316.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="11,256.20,103.09,87.07,604.35"><head></head><label></label><figDesc></figDesc><graphic coords="11,256.20,103.09,87.07,604.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,72.00,120.46,335.09,109.43"><head>Table 1</head><label>1</label><figDesc>Model Performance metrics for each task run on the test dataset</figDesc><table coords="8,168.35,158.86,238.74,71.03"><row><cell>Task</cell><cell cols="2">Accuracy Precision Recall F1 score</cell></row><row><cell>Task 1-run 1</cell><cell>61.03%</cell><cell>37.25% 61.03% 46.26%</cell></row><row><cell>Task 1-run 2</cell><cell>78.06%</cell><cell>64.49% 54.96% 59.34%</cell></row><row><cell>Task 1-run 3</cell><cell>67.38%</cell><cell>61.90% 67.38% 64.19%</cell></row><row><cell>Task 2-run 1</cell><cell>57.56%</cell><cell>33.13% 57.56% 42.06%</cell></row><row><cell>Task 3-run 1</cell><cell>60.02%</cell><cell>48.47% 60.02% 52.32%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,475.18,460.90,287.39"><head>Table 2</head><label>2</label><figDesc>Official competition results for the task 3</figDesc><table coords="8,77.99,504.34,454.91,258.23"><row><cell></cell><cell cols="3">EXIST_2023_Leaderboard_Task3</cell><cell></cell><cell></cell></row><row><cell>Task 3 Soft-Soft ALL</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 14</cell><cell>ICM-Soft -8,3574</cell><cell>ICM-Soft Norm 0,6793</cell><cell></cell></row><row><cell>Task 3 Hard-Hard ALL</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 31</cell><cell>ICM-Hard -2,1587</cell><cell>ICM-Hard Norm 0,1838</cell><cell>F1 0,0017</cell></row><row><cell>Task 3 Hard-Soft ALL</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 4</cell><cell>ICM-Soft -9,504</cell><cell>ICM-Soft Norm 0,6586</cell><cell></cell></row><row><cell>Task 3 Soft-Soft ES</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 11</cell><cell>ICM-Soft -8,5493</cell><cell>ICM-Soft Norm 0,6701</cell><cell></cell></row><row><cell>Task 3 Hard-Hard ES</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 30</cell><cell>ICM-Hard -2,2525</cell><cell>ICM-Hard Norm 0,192</cell><cell>F1 0,0026</cell></row><row><cell>Task 3 Hard-Soft ES</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 6</cell><cell>ICM-Soft -9,6746</cell><cell>ICM-Soft Norm 0,6496</cell><cell></cell></row><row><cell>Task 3 Soft-Soft EN</cell><cell>Run M&amp;S_NLP_1</cell><cell>Rank 11</cell><cell>ICM-Soft -7,939</cell><cell>ICM-Soft Norm 0,6957</cell><cell></cell></row><row><cell>Task 3 Hard-Hard EN</cell><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>To gain a deeper insight into the model utilized, refer to the diagram above which illustrates its structure. The outputs from three pre-trained language models are compiled by a single CNN model, which then categorizes the information based on the task at hand. Detailed information about the number of parameters can be found in the table provided below. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,96.50,294.13,389.64,9.69;10,96.20,307.57,389.93,9.69;10,96.20,321.01,389.89,9.69;10,96.20,334.69,327.56,9.69" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,197.72,294.13,288.41,9.69;10,96.20,307.57,136.45,9.69">Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-2013</idno>
		<ptr target="https://aclanthology.org/N16-2013.doi:10.18653/v1/N16-2013" />
	</analytic>
	<monogr>
		<title level="m" coord="10,264.27,307.57,221.86,9.69;10,96.20,321.01,238.28,9.69">Proceedings of the NAACL Student Research Workshop, Association for Computational Linguistics</title>
		<meeting>the NAACL Student Research Workshop, Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,348.13,366.49,9.69;10,96.55,361.57,332.20,9.69;10,96.55,375.25,234.73,9.69" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,293.28,348.13,169.76,9.69;10,96.55,361.57,127.65,9.69">Recent Trends in Deep Learning Based Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1708.02709</idno>
		<idno type="arXiv">arXiv:1708.02709</idno>
		<ptr target="http://arxiv.org/abs/1708.02709.doi:10.48550/arXiv.1708.02709" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,388.69,324.93,9.69;10,438.56,388.69,48.10,9.69;10,96.55,402.13,255.50,9.69;10,369.73,402.13,116.94,9.69;10,96.55,415.57,26.61,9.69;10,141.76,415.57,344.90,9.69" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,438.56,388.69,48.10,9.69;10,96.55,402.13,255.50,9.69;10,369.73,402.13,116.94,9.69;10,96.55,415.57,22.17,9.69">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,164.26,415.57,255.26,9.69">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,429.25,390.19,9.69;10,96.55,442.69,230.00,9.69" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,456.13,389.55,9.69;10,96.55,469.81,389.45,9.69;10,96.55,483.25,389.66,9.69;10,96.55,496.69,389.51,9.69;10,96.55,510.13,110.37,9.69" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,334.36,456.13,151.74,9.69;10,96.55,469.81,87.31,9.69">Deeper Attention to Abusive User Content Moderation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1117</idno>
		<ptr target="https://aclanthology.org/D17-1117.doi:10.18653/v1/D17-1117" />
	</analytic>
	<monogr>
		<title level="m" coord="10,209.30,469.81,276.70,9.69;10,96.55,483.25,324.26,9.69">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,523.57,390.18,9.69;10,96.55,537.25,390.08,9.69;10,96.55,550.69,390.13,9.69;10,96.55,564.13,390.11,9.69;10,96.55,577.81,390.11,9.69;10,96.55,591.25,142.41,9.69" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,96.55,537.25,308.79,9.69">Overview of EXIST 2023: sEXism Identification in Social NeTworks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-28241-6_68.doi:10.1007/978-3-031-28241-6_68" />
	</analytic>
	<monogr>
		<title level="m" coord="10,431.41,537.25,55.22,9.69;10,96.55,550.69,385.18,9.69;10,246.11,564.13,90.63,9.69">Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023</title>
		<meeting><address><addrLine>Dublin, Ireland; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2023">April 2-6, 2023. 2023</date>
			<biblScope unit="page" from="593" to="599" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct coords="10,96.55,604.93,389.45,9.69;10,96.55,618.37,389.71,9.69;10,96.55,631.81,284.79,9.69" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,96.55,618.37,294.27,9.69">Overview of exist 2023: sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_68</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,420.30,618.37,65.97,9.69;10,96.55,631.81,35.95,9.69">Proceedings of ECIR&apos;23</title>
		<meeting>ECIR&apos;23</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="593" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.55,645.25,389.63,9.69;10,96.55,658.69,9.08,9.69;10,130.15,658.69,29.96,9.69;10,184.81,658.69,35.16,9.69;10,244.67,658.69,24.76,9.69;10,294.13,658.69,25.07,9.69;10,343.98,658.69,142.08,9.69;10,96.55,672.13,232.37,9.69" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,282.49,645.25,203.69,9.69;10,96.55,658.69,9.08,9.69;10,130.15,658.69,29.96,9.69;10,184.81,658.69,30.14,9.69">SemEval-2023 Task 10: Explainable Detection of Online Sexism</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Röttger</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.04222</idno>
		<idno type="arXiv">arXiv:2303.04222</idno>
		<ptr target="http://arxiv.org/abs/2303.04222.doi:10.48550/arXiv.2303.04222" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.50,685.81,397.98,9.69;10,77.90,702.37,392.47,9.69;10,96.20,715.81,140.86,9.69" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,199.84,685.81,294.63,9.69;10,77.90,702.37,32.41,9.69">Survey Article: Inter-Coder Agreement for Computational Linguis-[9] tics</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.07-034-R2</idno>
		<ptr target="https://aclanthology.org/J08-4004.doi:10.1162/coli.07-034-R2" />
	</analytic>
	<monogr>
		<title level="j" coord="10,116.87,702.37,114.14,9.69">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,733.09,352.82,9.69;10,96.20,746.77,376.67,9.69;10,96.20,760.21,364.97,9.69" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,344.76,733.09,116.06,9.69;10,96.20,746.77,376.67,9.69;10,96.20,760.21,150.05,9.69">Code for &quot;Towards Robust Online Sexism Detection: A Multi-Model Approach with BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp;</forename><forename type="middle">A</forename><surname>Bagheri</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.8144300</idno>
		<ptr target="https://doi.org/10.5281/zenodo.8144300" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
