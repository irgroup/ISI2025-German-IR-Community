<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,376.86,15.42;1,89.29,107.08,205.10,11.96">IU-Percival: Linear Models for Sexism Detection Notebook for the EXIST Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">Indiana University Pervasive Technology Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">Lilly Endowment, Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,132.98,76.31,11.96"><forename type="first">Elizabeth</forename><surname>Gabel</surname></persName>
							<email>eligabel@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.25,132.98,70.09,11.96"><forename type="first">Holly</forename><surname>Redman</surname></persName>
							<email>hredman@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.98,132.98,78.89,11.96"><forename type="first">Daniel</forename><surname>Swanson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.87,132.98,69.50,11.96"><forename type="first">Sandra</forename><surname>K칲bler</surname></persName>
							<email>skuebler@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,376.86,15.42;1,89.29,107.08,205.10,11.96">IU-Percival: Linear Models for Sexism Detection Notebook for the EXIST Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0AA52C73E6EFA675C30B1AC799AD3C6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sexism detection</term>
					<term>classification</term>
					<term>perceptron</term>
					<term>syntactic n-grams</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we detail the approach taken by the IU-Percival team in the EXIST 2023 shared task on sexism detection in English and Spanish tweets. Using syntactic n-grams generated by Universal Dependencies parsing as features, we train four classifiers: SVM, random forest, multi-layer perceptron, and single-layer perceptron. While we find that these four classifiers perform similarly, we focus our efforts on the single-layer perceptron as it not only performs slightly better than the rest but also boasts a much quicker training time. Our results for the development data indicate that our approach improves on previous non-deep learning approaches, and provide some support for continued examination of Universal Dependencies' application to Sexism Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes the contribution of IU-Percival to Task 1 of EXIST 2023 <ref type="bibr" coords="1,419.68,377.59,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,433.21,377.59,7.49,10.91" target="#b1">2]</ref>. The objective of this shared task is to promote the development of systems that are capable of automatically detecting sexist comments in social media in both English and Spanish. Task 1 is a binary task that involves labeling tweets as "sexist" or "non-sexist. "</p><p>Sexist comments and various other forms of harmful language are rampant on social media, creating a monumental challenge for content moderation efforts. Therefore, the creation and refinement of efficient systems to automatically detect such messaging is essential. These systems must be able to identify both explicit and implicit sexism, as well as be able to contend with the many irregularities that exist in social media data, such as code-switching within comments, dialectal variations, misspellings, abbreviations, and the perpetually evolving colloquialisms that proliferate in online spaces.</p><p>Our work deviates from the transformer-based methods that have predominated the field in recent years. We focus instead on pre-transformer methods, leveraging Universal Dependencies (UD) <ref type="bibr" coords="1,114.16,553.73,13.00,10.91" target="#b2">[3]</ref> parsing to generate syntactic features. We train SVM, random forest, single-layer perceptron, and multi-layer perceptron classifiers. We find that these traditional methods perform reasonably well on the task. This approach was developed during a course on machine learning in NLP.</p><p>The rest of this work is structured as follows: section 2 introduces related work, focusing on sexism detection with non-transformer approaches; section 3 describes our research methods, including pre-processing steps, feature selection, and our choice of classifiers; section 4 presents our results; section 5 provides a discussion of our findings; and section 6 shares concluding remarks and discusses plans for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The EXIST shared task, starting in 2021, remains one of the primary sources of research into sexism detection in tweets. The majority of submissions to EXIST have focused on transformerbased methods, with little attention paid to the potential of more traditional machine learning methods.</p><p>Rodr칤guez-S치nchez et al. <ref type="bibr" coords="2,212.07,281.08,12.84,10.91" target="#b3">[4]</ref> explored a variety of machine learning methods for the task of sexism detection. They compared the performance of logistic regression, SVM, random forest, bi-LSTMs, and mBERT on sexism detection in Spanish tweets. They found the neural models to be slightly better than the non-neural machine learning algorithms at detecting sexism in the dataset, although random forest achieved the highest precision. The bi-LSTM models were on par with mBERT in terms of F1, accuracy, precision, and recall.</p><p>Rizvi and Jamatia <ref type="bibr" coords="2,183.31,362.38,12.84,10.91" target="#b4">[5]</ref> participated in the 2022 EXIST shared task <ref type="bibr" coords="2,397.44,362.38,11.58,10.91" target="#b5">[6]</ref>. They experimented with logistic regression, Naive Bayes, and SVM systems and found that the logistic regression model worked best for both Spanish and English on both tasks. They used TF-IDF unigram and bigram representations as features for all three models. While their submission ultimately ranked 17th out of 19 submissions in the competition, with an official F1-score of 70.65% overall, their approach showed promise among the few submissions that did not implement pretrained transformer-based models.</p><p>Moldovan et al. <ref type="bibr" coords="2,170.43,457.22,12.84,10.91" target="#b6">[7]</ref> addressed the issue of sexism in Romanian. They used logistic regression, SVM, random forests, Ro-BERT, and mBERT to classify Romanian tweets as sexist or nonsexist. They used BOW-based representations, TF-IDF word representations, and sentence representations generated by mBERT and Ro-BERT as features for the non-neural models. The best performance was achieved with a fine-tuned Ro-BERT model; however, the best recall for non-sexist tweets was achieved by the random forest classifier using TF-IDF-based word representations, by a significant margin.</p><p>Related to the topic of sexism detection is abusive language detection. Steimel et al. <ref type="bibr" coords="2,493.14,552.07,12.84,10.91" target="#b7">[8]</ref> investigated abusive language detection in English and German tweets using topic modeling and a number of neural and non-neural classifiers. They found that SGBoost performed best on the English data while SVMs performed best on the German data. They also found that different sampling methods to address class imbalance led to drastically different outcomes regarding the two data sets. Their work provides evidence that the best classifier and techniques for one language cannot be assumed to perform well for other languages, even if the data sets share similarities. Thus, it is important to experiment with a variety of methods when handling multilingual data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>The dataset was provided by the organizers of EXIST 2023. It is comprised of a selection of Spanish and English tweets, annotated for sexism. The training set contains tweets collected between the 1st of September 2021 to 28th of February of 2022, the development set contains tweets from the 1st to 31st of May of 2022, and the test set contains tweets collected from the 1st of August 2022 to 30th of September.</p><p>In order to avoid author bias, the final selection contains one tweet per author. Additionally, all tweets containing less than five words were removed <ref type="foot" coords="3,339.19,424.96,3.71,7.97" target="#foot_0">1</ref> .</p><p>The training data contains 3 260 English tweets (34.88% sexist) and 3 660 Spanish tweets (42.62% sexist). Table <ref type="table" coords="3,185.89,453.81,5.11,10.91" target="#tab_0">1</ref> shows the distribution of English and Spanish tweets along with their in-language sexist to non-sexist distribution for the training data.</p><p>Since the tweets are not provided with a single gold label but rather with a set six labels given by different annotators, it is necessary to deal with ties. The official evaluation script of EXIST 2023 for the hard-hard evaluation simply discards ties. We, on the other hand, have chosen to include them in the sexist category for training, in order to boost the minority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-Processing</head><p>We extracted the syntactic features using UDPipe2 <ref type="bibr" coords="3,307.10,571.29,11.28,10.91" target="#b8">[9]</ref>, specifically using the available pre-trained models for the largest English and Spanish treebanks: English-EWT <ref type="bibr" coords="3,393.21,584.84,17.89,10.91" target="#b9">[10]</ref> and Spanish-AnCora <ref type="bibr" coords="3,89.29,598.39,16.36,10.91" target="#b10">[11]</ref>. The English EWT contains text from four different web genres; the AnCora treebank is based on news texts. Unfortunately, there does not exist a Spanish UD treebank based on social media data. Since UDPipe2 already performs a significant amount of pre-processing internally, ours was fairly minimal. Links were replaced with URL and user mentions were replaced by USER in English and NOMBRE in Spanish. Additionally, spaces were added after periods to correct for some errors in sentence segmentation.</p><p>Many of the tweets in the data set, particularly those labeled as Spanish, contained codeswitching. Additionally, some tweets labeled as Spanish contained only English text. Because of this, we opted to pass every tweet through both the English parser and the Spanish parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Features</head><p>After parsing, we extracted syntactic unigrams, bigrams, and trigrams. Unigrams consist of a lower-cased lemma and a part of speech tag, such as user NOUN. Bigrams consist of a word, its parent word, and the relation between them, such as user NOUN nsubj ignora VERB or just the unigram and relation if the word is the root of the sentence (hence ignora VERB root). Trigrams, similarly, concatenate a word, its parent, and its grandparent.</p><p>We tested three methods of selecting training features. The first method, "Full", uses all syntactic n-grams present in the training data. The second, "Trimmed", uses only n-grams which are present in both the training set and the development set. Finally "Selected" uses 洧 2 to choose the 500 most informative features. The total number of features for each parser and selection method is listed in Table <ref type="table" coords="4,243.20,451.99,3.74,10.91" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model</head><p>We trained an SVM, a random forest classifier, a multi-layer perceptron, and single-layer perceptron trained with stochastic gradient descent on the Full feature set from both parsers combined, from the scikit-learn toolkit <ref type="bibr" coords="4,266.01,528.81,16.35,10.91" target="#b11">[12]</ref>. Our initial experimentation produced the scores listed in Table <ref type="table" coords="4,154.42,542.36,3.73,10.91" target="#tab_4">5</ref>. The four architectures all gave roughly equivalent performance, so we chose to focus on the single-layer perceptron, which slightly outperformed the others and also had a substantially shorter training time.</p><p>For the final model we performed hyperparemeter tuning over the learning rate and the random seed. We used logistic regression, l2 regularization, and a constant learning rate schedule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Official Results</head><p>We show the official rankings of our three submitted models on the test set in Table <ref type="table" coords="5,464.81,537.45,5.08,10.91" target="#tab_2">3</ref> and the official results in Table <ref type="table" coords="5,188.90,551.00,3.66,10.91" target="#tab_3">4</ref>. Our first model, IU-Percival_1, consists of two single-layer perceptrons (one for English and one for Spanish) which were trained on all syntactic unigrams, bigrams, and trigrams. The second model, IU-Percival_2, is comprised of one single-layer perceptron for both languages and was trained on the same syntactic features. The final model, IU-Percival_3, is one single-layer perceptron trained on the same syntactic features, but excluding all n-grams which do not also occur in the dev set provided for the task. We report ICM scores <ref type="bibr" coords="5,201.08,632.29,16.35,10.91" target="#b12">[13]</ref>, along with the F1-score for the positive class in the Hard-Hard evaluation.</p><p>In terms of rankings, our multilingual model using all features, IU-Percival_2, provided our best ranking for All (both languages) and Spanish, while the combination of monolingual models, IU-Percival_1, ranked the best of our models for English. Our best rankings were under the Soft-Soft evaluation for all languages and models.</p><p>Our results, on the other hand, show that our submitted models all performed best in terms of ICM values under the Hard-Hard evaluation, in particular the ICM-Hard Normalized (although our non-normalized ICM-Hard values also outperformed the non-normalized ICM-Soft). Our best overall model in terms of ICM is IU-Percival_1 for English, although this is not our best ranked model. This difference can partially be explained by the lower number of submissions for the Soft-Soft evaluations.</p><p>For all models, the classifier performed worse on Spanish data than both All data and only English data. Although IU-Percival_3 had better results for Spanish for both non-normalized Hard and Soft evaluations (0.2827 ICM-Hard compared to 0.2363 English and 0.2675 All), following normalization, Spanish once again had the lowest results. In fact, following normalization, our classifier performed best on English for all models and evaluations.</p><p>Interestingly, our best scores for All and English come from IU-Percival_1, or the run with all n-grams, while our best score for Spanish comes from IU-Percival_3, which filters n-grams not occurring in the developmental set. Additionally, while the evaluation metric for the task is ICM, it is still of note that the highest F1 scores for each run (70.50, 71.91, 71.68) were all for Spanish, while the lowest F1 scores (68.57, 66.84, 65.72) were for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Development Set</head><p>We performed more extensive experiments on the development set. For these experiments, we split the official training set into 80% for training and 20% for validation, and we tested on the official development set. This was done to avoid an optimistic evaluation since one of the feature selection methods would otherwise have been operated on the same data set on which we also tested.</p><p>We first tested all classifiers considered for the task. The results are shown in Table <ref type="table" coords="6,472.86,572.33,3.73,10.91" target="#tab_4">5</ref>. They show that the single-layer perceptron outperforms the other classifiers in terms of macroaveraged F1. For this reason, we used this classifier for the remaining experiments.</p><p>We next investigated whether to use the features from the English or Spanish parser since we parse each tweet with both parsers, or a combination of both. Additionally, we compared the Trimmed feature selection method to the full feature set. The results of these experiments are shown in Table <ref type="table" coords="6,176.26,653.63,3.72,10.91" target="#tab_5">6</ref>. In all but one case, the best performing F1 scores corresponded with the best performing ICM scores. Across the board, the best performing classifier for English data is the one trained on English features; for the Spanish data, the best model uses the combined English and Spanish features. Our best performing system on the English data alone had an ICM of 0.3067 and the best for the Spanish data alone got 0.2684. Evaluating these two together on the full development set gave an ICM of 0.2999.</p><p>It is interesting to note that every setting achieves a lower score when trained and tested only on Spanish data than when trained and tested only on English data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>For EXIST 2022, Rizvi and Jamatia <ref type="bibr" coords="7,243.07,425.70,12.84,10.91" target="#b4">[5]</ref> (who also used linear models) ranked 38th of 45 submissions in Task 1, and of those who submitted papers ranked 17th out of 19. Their best model had an F1 score of 70.65 on Combined data, 70.84 on only English data, and 70.24 on only Spanish data.</p><p>Because we do not use the same classifier as Rizvi and Jamatia, nor do we have the same train or test sets, it is impossible to draw a direct comparison between their results and ours. However, the fact that our system reached comparable scores on similar datasets indicates the potential value of syntactic features extracted through Universal Dependencies for the problem of sexism detection.</p><p>Additionally, although our classifier performed worse on Spanish data and Combined data (in keeping with the findings of Steimel et al. <ref type="bibr" coords="7,289.01,561.19,11.40,10.91" target="#b7">[8]</ref>, which indicate that classifiers do not perform universally well on different languages), the presence of both Spanish and English features extracted using UDPipe improved the performance for Spanish and Combined results.</p><p>Since we elected to pass every tweet through both the Spanish parser and the English parser, all tweets, regardless of original language, were used to extract both Spanish and English syntactic features. The universal decline in our classifier's performance on Spanish data may be due to a few different reasons. First, it is possible that the single-layer perceptron performs generally worse on Spanish data than on English data. Regarding the lower performance by the classifier trained using only Spanish features, it is also possible that our decision to use Spanish-AnCora-due to the lack of Spanish treebanks for online language-resulted in less accurate features than those extracted using English-EWT (which, while not including tweets, does include more casual language and language pulled from some social media). The dataset is fairly well balanced and has more Spanish tweets than English tweets, so issues with the data distribution are an unlikely cause. Regardless, the best models for Spanish data are those that use features from both English and Spanish, indicating that the inclusion of features from multiple languages is valuable in cases where code-switching is prevalent in the data, and that using Universal Dependencies to extract additional syntactic features may increase classifier performance for Multilingual Sexism Detection. While the Spanish data set contains many instances of Spanish-English code-switching, the English data set does not, which explains why Spanish features do not improve performance on English data. Future research using Universal Dependencies syntactic features for similar purposes will need to consider these factors when making decisions about which treebanks are the most appropriate to use for a given dataset.</p><p>Here we would like to provide some comments on our perspective as students participating in a shared task, as this work also served as a final project for a course on machine learning for computational linguistics. We had all read literature and completed assignments based on various past shared tasks but had not, until now, gone through the process ourselves. Unsurprisingly, we encountered some challenges along the way. One challenge was simply deciding on a model and feature set, having had limited personal experience with knowing what might work well for this type of task. We also decided against using transformer models even though that is what the vast majority of other participants of this task had done. This decision made our work more difficult in some aspects because there was less of a precedent, and it forced us to be creative in our feature selection. We also had to temper our expectations regarding our eventual placement on the leaderboard. From the beginning, our aim was not to achieve state-of-the-art performance but to investigate less-studied avenues.</p><p>Another source of difficulty for us was collaboration on the practical side, coordinating coding and running experiments with different types of expertise in the group. Finally, we realized rather late in our experimentation process that we had made a serious methodological error; we had created a feature set based on the presence of certain features in both the train and dev set, which we had then used to test on the dev set. Luckily, we were able to rectify this before submission. Overall, we are grateful to have had this opportunity to participate in EXIST and engage with this important field of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>By training four classifiers, we have shown that decent results can be obtained for sexism detection in English and Spanish tweets using machine learning approaches that are not based on transformers. Surprisingly, our best performing system for both datasets has been a singlelayer perceptron, which has an extremely simple architecture and is fast to train.</p><p>Additionally, we have examined the potential of training on syntactic features extracted using Universal Dependencies for the problem of sexism detection, and provided a baseline for future work examining this approach.</p><p>This study has several limitations which provide opportunities for future research. These include the treebanks available to us in both English and Spanish integrated with UDPipe2. While there is a tweet treebank for English, it is not an official Universal Dependencies treebank, and there is no treebank for Spanish which includes internet/online language. Additionally, we encountered some errors in parsing, which could be resolved with additional pre-processing, particularly for the Spanish data. Initially, our pre-processing involved replacing usernames in both the Spanish and English data with the token USER. However, upon examination of the data, we found that the parser we used for Spanish incorrectly tagged USER as a verb. Changing usernames to NOMBRE instead improved the accuracy of the parser. Further examination of the parsed data may reveal other insights that could be utilized to improve parsing accuracy, which would result in better syntactic features.</p><p>Our initial attempts at feature selection with scikit-learn were not fruitful, and due to time limitations we were unable to run sufficient feature selection experiments. Therefore, we plan to continue refining our feature selection methods by adjusting the number of features and utilizing different feature selection methods.</p><p>Another avenue for future investigation is an ensemble method combining multiple classifiers. At this time, we do not know enough about the specific strengths and weaknesses that each of our trained classifiers has, but determining this and combining classifiers could result in a more robust system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,417.00,182.94"><head>Table 1</head><label>1</label><figDesc>Distribution of labels in the training data by language. Numerical labels indicate the number of annotators out of six who labeled a given tweet as containing sexist language.</figDesc><table coords="3,215.69,134.06,161.41,139.37"><row><cell cols="3">Label English Spanish</cell></row><row><cell>0</cell><cell>768</cell><cell>666</cell></row><row><cell>1</cell><cell>545</cell><cell>526</cell></row><row><cell>2</cell><cell>420</cell><cell>442</cell></row><row><cell>3</cell><cell>390</cell><cell>466</cell></row><row><cell>4</cell><cell>423</cell><cell>487</cell></row><row><cell>5</cell><cell>392</cell><cell>580</cell></row><row><cell>6</cell><cell>322</cell><cell>493</cell></row><row><cell>Majority no-sexist</cell><cell>1 733</cell><cell>1 634</cell></row><row><cell>Tie</cell><cell>390</cell><cell>466</cell></row><row><cell>Majority sexist</cell><cell>1 137</cell><cell>1 560</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,364.95,81.83"><head>Table 2</head><label>2</label><figDesc>Total number of syntactic n-grams used from each parser and means of feature selection.</figDesc><table coords="4,208.42,122.10,178.43,50.21"><row><cell></cell><cell cols="3">English Spanish Combined</cell></row><row><cell>Full</cell><cell cols="2">319 923 320 471</cell><cell>591 469</cell></row><row><cell>Trimmed</cell><cell>11 983</cell><cell>12 405</cell><cell>20 254</cell></row><row><cell>Selected</cell><cell>500</cell><cell>500</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,90.49,416.99,165.51"><head>Table 3</head><label>3</label><figDesc>Official Rankings from EXIST 2023 Task 1. Rankings by evaluation metric and run, underlining indicates best performing IU-Percival run in each language</figDesc><table coords="5,169.08,134.06,257.12,121.94"><row><cell>Run</cell><cell>Lang</cell><cell cols="3">Hard-Hard Hard-Soft Soft-Soft</cell></row><row><cell cols="2">IU-Percival_1 All</cell><cell>45</cell><cell>48</cell><cell>39</cell></row><row><cell></cell><cell>English</cell><cell>47</cell><cell>47</cell><cell>39</cell></row><row><cell></cell><cell>Spanish</cell><cell>49</cell><cell>51</cell><cell>42</cell></row><row><cell cols="2">IU-Percival_2 All</cell><cell>46</cell><cell>46</cell><cell>38</cell></row><row><cell></cell><cell>English</cell><cell>50</cell><cell>54</cell><cell>40</cell></row><row><cell></cell><cell>Spanish</cell><cell>47</cell><cell>46</cell><cell>37</cell></row><row><cell cols="2">IU-Percival_3 All</cell><cell>51</cell><cell>52</cell><cell>40</cell></row><row><cell></cell><cell>English</cell><cell>58</cell><cell>60</cell><cell>42</cell></row><row><cell></cell><cell>Spanish</cell><cell>45</cell><cell>47</cell><cell>38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,278.26,416.99,187.20"><head>Table 4</head><label>4</label><figDesc>Official Results from EXIST 2023 Task 1. Columns labeled "Norm" indicate ICM scores rescaled so that the best performing submission had 1.00 and the worst had 0.00. Bolded values are the best performing in their column.</figDesc><table coords="5,96.46,331.56,402.35,133.90"><row><cell></cell><cell></cell><cell>Hard-Hard</cell><cell></cell><cell>Hard-Soft</cell><cell>Soft-Soft</cell></row><row><cell>Run</cell><cell>Lang</cell><cell>ICM-Hard Norm</cell><cell cols="2">F1 ICM-Soft Norm ICM-Soft Norm</cell></row><row><cell cols="2">IU-Percival_1 All</cell><cell cols="2">0.3024 0.5587 69.71</cell><cell>-0.4612 0.4217</cell><cell>-0.4612 0.4217</cell></row><row><cell></cell><cell>English</cell><cell cols="2">0.3655 0.6264 68.57</cell><cell>-0.4952 0.4792</cell><cell>-0.4952 0.4792</cell></row><row><cell></cell><cell>Spanish</cell><cell cols="2">0.2273 0.4885 70.50</cell><cell>-0.5085 0.3629</cell><cell>-0.5085 0.3629</cell></row><row><cell cols="2">IU-Percival_2 All</cell><cell cols="2">0.2964 0.5549 69.81</cell><cell>-0.4435 0.4246</cell><cell>-0.4435 0.4246</cell></row><row><cell></cell><cell>English</cell><cell cols="2">0.2998 0.5865 66.84</cell><cell>-0.6435 0.4578</cell><cell>-0.6435 0.4578</cell></row><row><cell></cell><cell>Spanish</cell><cell cols="2">0.2737 0.5192 71.91</cell><cell>-0.3556 0.3898</cell><cell>-0.3556 0.3898</cell></row><row><cell cols="2">IU-Percival_3 All</cell><cell cols="2">0.2675 0.5365 69.07</cell><cell>-0.5491 0.4075</cell><cell>-0.5491 0.4075</cell></row><row><cell></cell><cell>English</cell><cell cols="2">0.2363 0.5479 65.72</cell><cell>-0.8610 0.4264</cell><cell>-0.8610 0.4264</cell></row><row><cell></cell><cell>Spanish</cell><cell cols="2">0.2827 0.5252 71.68</cell><cell>-0.3572 0.3895</cell><cell>-0.3572 0.3895</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,88.99,90.49,389.71,93.78"><head>Table 5</head><label>5</label><figDesc>F1 scores on the development set (with tied annotations included) for the internal experiments.</figDesc><table coords="6,230.96,122.10,133.36,62.16"><row><cell>Classifier</cell><cell>F1</cell></row><row><cell>Multi-Layer Perceptron</cell><cell>73.93</cell></row><row><cell>Random Forest</cell><cell>73.15</cell></row><row><cell cols="2">Single-Layer Perceptron 74.12</cell></row><row><cell>SVM</cell><cell>73.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.99,90.49,417.00,168.78"><head>Table 6</head><label>6</label><figDesc>Results of using syntactic features from the English and Spanish parser, a combination of both, in combination with feature selection. Asterisks indicate instances where the best F1 score and ICM score are from different runs.</figDesc><table coords="7,142.42,143.79,310.44,115.47"><row><cell></cell><cell></cell><cell cols="2">English Data</cell><cell cols="2">Spanish Data</cell><cell cols="2">Combined Data</cell></row><row><cell>Parser</cell><cell>Features</cell><cell>F1</cell><cell>ICM</cell><cell>F1</cell><cell>ICM</cell><cell>F1</cell><cell>ICM</cell></row><row><cell cols="2">English Full</cell><cell cols="4">73.45 0.3067 70.24 0.2333</cell><cell>71.48</cell><cell>0.2633</cell></row><row><cell></cell><cell cols="5">Trimmed 73.36 0.3058 70.58 0.2433</cell><cell>71.50</cell><cell>0.2645</cell></row><row><cell cols="2">Spanish Full</cell><cell cols="6">72.02 0.2630 70.27 0.2339 70.67* 0.2409*</cell></row><row><cell></cell><cell cols="5">Trimmed 70.49 0.2147 68.70 0.1838</cell><cell>70.55</cell><cell>0.2341</cell></row><row><cell>Both</cell><cell>Full</cell><cell cols="6">72.90 0.2912 71.30 0.2684 72.41 0.2930</cell></row><row><cell></cell><cell cols="5">Trimmed 72.80 0.2903 69.76 0.2189</cell><cell>71.85</cell><cell>0.2756</cell></row><row><cell>Both</cell><cell>Merged</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">72.62 0.2999</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,660.05,294.22,8.97"><p>More detailed information about the dataset collection can be found at the EXIST</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2023" xml:id="foot_1" coords="3,407.76,660.05,98.81,8.97;3,92.57,671.01,38.65,8.97"><p>website: http://nlp.uned.es/ exist2023/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by <rs type="funder">Lilly Endowment, Inc.</rs>, through its support for the <rs type="funder">Indiana University Pervasive Technology Institute</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,461.65,394.53,10.91;9,112.66,475.20,393.33,10.91;9,112.66,488.75,394.53,10.91;9,112.66,502.30,393.33,10.91;9,112.66,515.85,333.46,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,112.66,475.20,393.33,10.91;9,112.66,488.75,72.05,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amig칩</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.00,502.30,104.99,10.91;9,112.66,515.85,206.17,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,529.40,394.53,10.91;9,112.66,542.95,393.33,10.91;9,112.66,556.50,393.33,10.91;9,112.33,570.05,395.22,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,112.66,542.95,393.33,10.91;9,112.66,556.50,164.31,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amig칩</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.67,570.05,335.74,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,583.60,394.53,10.91;9,112.66,597.15,394.53,10.91;9,112.66,610.69,394.52,10.91;9,112.66,624.24,377.61,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,162.24,597.15,340.81,10.91">Universal Dependencies v2: An evergrowing multilingual treebank collection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Haji캜</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.lrec-1.497" />
	</analytic>
	<monogr>
		<title level="m" coord="9,127.50,610.69,374.71,10.91">Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference (LREC)<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4034" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,637.79,393.32,10.91;9,112.66,651.34,395.01,10.91;9,112.66,664.89,179.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,356.76,637.79,149.22,10.91;9,112.66,651.34,230.04,10.91">Automatic classification of sexism in social networks: An empirical study on twitter data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodr칤guez-S치nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3042604</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,349.72,651.34,52.66,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="219563" to="219576" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.66,100.52,394.53,10.91;10,112.39,114.06,396.57,10.91;10,112.66,130.06,86.48,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,211.37,86.97,294.62,10.91;10,112.66,100.52,68.57,10.91">NIT-Agartala-NLP-Team at EXIST 2022: Sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jamatia</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3202/.doi:urn:nbn:de:0074-3202-9" />
	</analytic>
	<monogr>
		<title level="m" coord="10,205.22,100.52,297.52,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF)</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF)<address><addrLine>A Coru침a, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3202</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,394.52,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,395.01,10.91;10,112.66,181.81,335.90,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,358.62,154.71,147.37,10.91;10,112.66,168.26,142.69,10.91">Overview of EXIST 2022: sEXism Identification in Social neTworks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodr칤guez-S치nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendieta-Arag칩n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marco-Rem칩n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Makeienko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6443" />
	</analytic>
	<monogr>
		<title level="j" coord="10,264.37,168.26,160.03,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,393.33,10.91;10,112.66,222.46,394.61,10.91;10,112.66,236.01,358.07,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,337.01,195.36,168.98,10.91;10,112.66,208.91,196.94,10.91">Users hate blondes: Detecting sexism in user comments on online Romanian news</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cs칲r칬s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>-M. Bucur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bercuci</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.woah-1.21</idno>
		<ptr target="https://aclanthology.org/2022.woah-1.21.doi:10.18653/v1/2022.woah-1.21" />
	</analytic>
	<monogr>
		<title level="m" coord="10,333.40,208.91,172.59,10.91;10,112.66,222.46,149.16,10.91">Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)</title>
		<meeting>the Sixth Workshop on Online Abuse and Harms (WOAH)<address><addrLine>Seattle, Washington (Hybrid)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="230" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.66,263.11,393.33,10.91;10,112.28,276.66,395.38,10.91;10,112.66,290.20,380.10,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,306.18,249.56,199.80,10.91;10,112.66,263.11,124.16,10.91">Investigating multilingual abusive language detection: A cautionary tale</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Steimel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dakota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>K칲bler</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-056-4_132</idno>
		<ptr target="https://aclanthology.org/R19-1132.doi:10.26615/978-954-452-056-4_132" />
	</analytic>
	<monogr>
		<title level="m" coord="10,259.20,263.11,246.79,10.91;10,112.28,276.66,224.55,10.91">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP)</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing (RANLP)<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1151" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,394.53,10.91;10,112.28,330.85,395.00,10.91;10,112.66,344.40,343.92,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,162.25,303.75,237.00,10.91">UDPipe 2.0 prototype at CoNLL 2018 UD shared task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-2020</idno>
		<ptr target="https://www.aclweb.org/anthology/K18-2020.doi:10.18653/v1/K18-2020" />
	</analytic>
	<monogr>
		<title level="m" coord="10,422.18,303.75,83.81,10.91;10,112.66,317.30,394.53,10.91;10,112.28,330.85,191.51,10.91">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Association for Computational Linguistics</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,393.70,10.91;10,112.66,371.50,393.32,10.91;10,112.66,385.05,314.57,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,498.93,357.95,7.43,10.91;10,112.66,371.50,198.76,10.91">A gold standard dependency corpus for English</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.43,371.50,171.55,10.91;10,112.66,385.05,284.31,10.91">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC-2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.53,10.91;10,112.66,412.15,395.17,10.91;10,112.66,425.70,395.17,10.91;10,112.66,439.25,140.49,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,308.65,398.60,197.53,10.91;10,112.66,412.15,89.75,10.91">Ancora: Multilevel annotated corpora for catalan and spanish</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A M</forename><surname>Mariona Taul칠</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
		<ptr target="Http://www.lrec-conf.org/proceedings/lrec2008/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,233.63,412.15,274.20,10.91;10,112.66,425.70,188.50,10.91">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,394.53,10.91;10,112.66,466.34,394.53,10.91;10,112.66,479.89,393.32,10.91;10,112.66,493.44,176.63,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,228.10,479.89,182.14,10.91">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.22,479.89,86.76,10.91;10,112.66,493.44,82.55,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,394.62,10.91;10,112.66,520.54,393.32,10.91;10,112.33,534.09,122.11,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,223.40,506.99,260.55,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amig칩</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,520.54,393.32,10.91;10,112.33,534.09,23.88,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
