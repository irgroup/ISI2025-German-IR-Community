<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,399.66,15.42;1,89.29,106.66,217.22,15.42">ZaRa-IU-NLP at EXIST 2023 -Sexism Identification: Specialized or Generalized?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,71.87,11.96"><forename type="first">Zackary</forename><surname>Leech</surname></persName>
							<email>zleech@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.81,134.97,86.93,11.96"><forename type="first">Ravi</forename><surname>Regulagedda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Luddy School of Informatics, Computing, and Engineering</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.73,134.97,69.50,11.96"><forename type="first">Sandra</forename><surname>KÃ¼bler</surname></persName>
							<email>skuebler@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,399.66,15.42;1,89.29,106.66,217.22,15.42">ZaRa-IU-NLP at EXIST 2023 -Sexism Identification: Specialized or Generalized?</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F32D745532C678485896E0AFC94A357D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sexism detection</term>
					<term>RoBERTa</term>
					<term>BETO</term>
					<term>language-specific classification</term>
					<term>language-agnostic classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our approach to EXIST (sEXism Identification in Social neTworks) Task 1, at CLEF 2023, comprising automatic sexism detection in both English and Spanish on Twitter data. We compare two methods, the first being a bi-ensemble method that combines two pre-trained BERT architectures, BETO and RoBERTa, for each specific language, Spanish and English, respectively. The second method utilizes the larger multilingual transformer, RoBERTa-XLM-base, and considers the entire dataset despite language differences. We show that the language-specific ensemble performs better than the generalized model and is a better choice when looking at sexism detection in mixed Spanish and English data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rise in global social media interaction, there is also a rise in the shared expression of the darker side of humanity, including sexist sentiment. Santos-Rios et. al. <ref type="bibr" coords="1,447.67,395.12,12.99,10.91" target="#b0">[1]</ref> point out that "in 2021, 56% of the global population were social media users". UN Women <ref type="bibr" coords="1,454.51,408.67,12.98,10.91" target="#b1">[2]</ref> point to the COVID-19 Pandemic as one contributing factor for the rise, specifically for women, as "... women's lives shifted online for work, education, access to services and social activities" leading to "rapidly escalated" violence against them. A global study by the Economist Intelligence Unit <ref type="bibr" coords="1,89.29,462.87,12.74,10.91" target="#b2">[3]</ref> found that "38% of women have personal experiences of online violence and 85% of women who are online have witnessed digital violence against other women".</p><p>As such, collaborative work to address this growing problem has gained urgency. The sEXism Identification in Social neTworks (EXIST). EXIST 2023 <ref type="bibr" coords="1,339.05,503.52,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,353.53,503.52,9.03,10.91" target="#b4">5]</ref> is the third edition of EXIST at CLEF, it builds on EXIST 2021, the first shared task to address sexism in a broad sense, including detection of implicit bias. The shared task defines sexism as: The tweet is sexist itself, describes a sexist situation, or criticizes a sexist behavior, rather than solely explicit bias sexism, typically including direct name calling or harmful, stereotypical generalizations. The current EXIST shared task expands the task to include bilingual data. This is important as the Economist Intelligence Unit reports that "... there are regional differences in the prevalence of online violence against women" with "the overall prevalence rate by region at 76% in North America and 91% in Latin America and Caribbean" <ref type="bibr" coords="2,277.77,100.52,11.43,10.91" target="#b2">[3]</ref>.</p><p>From outwardly explicit misogyny to more subtle implicit misogyny, automatic classification of sexism will create a more efficient process of creating a safer environment online. With rising hate speech towards women online and little research into the detection of sexism, contributions to this issue are urgent.</p><p>We present the approach by team ZaRa-IU-NLP. This approach was developed during a course on machine learning in NLP. Our work focuses on comparing a multilingual neural model with an ensemble of language specific models.</p><p>The remainder of this report will proceed as follows: Section 2 outlines previous research for prior EXIST shared tasks, Sections 3 and 4 provide details on the task description, the dataset, and the preprocessing methods, respectively. Section 5 describes the two rival model architectures. In section 6 we analyze the results of the experiments before concluding in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Team avacaondata <ref type="bibr" coords="2,171.85,321.73,12.68,10.91" target="#b5">[6]</ref> provided the winning approach to the 2022 EXIST Task 1; they utilized an ensemble of transformer models using BERTweet-large, RoBERTa, and DeBERTa v3 for English, and BETO, BERTIN, MarIA-base, and Robertuito for Spanish. With this combination, the team achieved an overall F1 of 0.7996. To reach optimal performance given the computational load of the models and to avoid overgeneration, the training was carried out in 2 phases, choosing to optimize parameters with smaller amounts of data before expanding to the entire dataset <ref type="bibr" coords="2,482.39,389.48,11.26,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,496.38,389.48,7.51,10.91" target="#b4">5]</ref>.</p><p>Team CIMATCOLMEX ranked first in the evaluation of Spanish tweets, with an accuracy of 0.7801 <ref type="bibr" coords="2,118.86,416.58,11.27,10.91" target="#b3">[4]</ref>. This team utilized an ensemble of 10 RoBERTuito and 10 BERT models. They reached the highest scores for Spanish, surpassing avacaondata in F1 score by 2.27% absolute. Using a bi-ensemble method, Villa-Cueva et. al <ref type="bibr" coords="2,260.14,443.67,12.68,10.91" target="#b6">[7]</ref> merged two transformers ensembles, one for Spanish and one for English. Though there is a high computational cost, this model scored second in Task 1 with a difference of 0.0038 in F1 to the winning team <ref type="bibr" coords="2,365.45,470.77,11.58,10.91" target="#b3">[4]</ref>. In regard to preprocessing, team CIMATCOLMEX used the following normalization steps: lowercasing, removing emojis, replacing usernames with "@user", replacing any URL with the token "&lt;URL&gt;", and removing any whitespace at the beginning or end of the tweet <ref type="bibr" coords="2,324.10,511.42,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description and Dataset</head><p>EXIST provides an opportunity to address the issue of online sexism with a wider reach, by providing participants with data containing bilingual, sexist speech, in both Spanish and English. Task 1 focuses on a binary classification of whether the text is sexist or not for both explicit or implicit examples. The data is constructed out of tweets with any form of oppression or prejudice against women because of their sex, explicit or implicit. It is worth noting that while the dataset labels every tweet as either English or Spanish, it includes tweets mixing both of the languages. The EXIST 2023 dataset consists of 6 920 tweets for training, 1 038 tweets for validation, and 2 076 tweets for testing, where all sets are randomly selected from the 9 000 and 4 000 sets sampled and created by the CLEF 2023 organizers, for balance. The annotation process was carried out by a balanced group of 3 women and 3 men, in order to avoid gender bias. When choosing the label for a given tweet from these 6 labels, we used a simple majority. In the case of ties, we randomly selected either of YES or NO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Preprocessing</head><p>Our focus is on a comparison of two model architectures, a generalized model, processing both English and Spanish tweets, and a specialized model using language specific classifiers for each language. As such, preprocessing was kept to a minimum. Tweets were used with URLs, emojis, duplications of characters, etc in place. Further, the dataset included a set of tweets containing varying levels of both languages. No alterations were made to the content of these individual bilingual tweets. Rather, each architecture was given an opportunity to make judgement on tweets based on the language information provided by the shared task. I.e., tweets labeled English were passed through RoBERTa and tweets labeled Spanish were passed through BETO. The fluidity of language, specifically in the informal context of twitter, creates ample opportunity for bilingual users to hide implicit sexism through language switching and as such, should be included in each model's ability to detect implicit bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Architecture</head><p>In our work, we compare two different approaches, a multilingual model, and a language specific model. The multilingual model uses XLM-RoBERTa, while the language specific model, RoBERTa for English, and BETO for Spanish. Figure <ref type="figure" coords="3,326.61,632.36,5.14,10.91" target="#fig_0">1</ref> shows an overview of the data flow in the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">XLM-RoBERTa</head><p>One of the first approach to sexism detection in a Spanish-English context was using XLM-RoBERTa <ref type="bibr" coords="4,135.46,121.08,11.59,10.91" target="#b7">[8]</ref>. This is a multi-lingual version of RoBERTa <ref type="bibr" coords="4,358.62,121.08,12.99,10.91" target="#b8">[9]</ref> trained on a database of 100 languages. It was initially pre-trained for masked language modeling (MLM).</p><p>We also use this model for our multilingual approach and finetune it on the EXIST dataset on the binary classification task. The data is tokenized using XLM-RoBERTa's tokenizer before passing into the model that was finetuned in 3 epochs. The finetuned model is used for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">RoBERTa + BETO</head><p>This model is a pipeline composed of two models pre-trained on English and Spanish texts respectively. For English, we chose RoBERTa, a model trained on English texts using an MLM objective. RoBERTa is an improved model over the core BERT architecture with more parameters and trained on a larger corpus to provide a more robust performance than the base BERT model.</p><p>We chose BETO <ref type="bibr" coords="4,172.86,279.21,16.09,10.91" target="#b9">[10]</ref>, a BERT-based Spanish language model for the Spanish language tweets in the dataset. BETO was trained on a large Spanish corpus <ref type="bibr" coords="4,360.63,292.75,16.40,10.91" target="#b10">[11]</ref>. Similar to RoBERTa, it was pre-trained on an MLM objective.</p><p>We fine-tuned both these models using the data for their respective languages. Since the data was split fairly evenly between English and Spanish texts, RoBERTa and BETO had about half the training data to finetune in comparison to XLM-RoBERTa.</p><p>For this ensemble, we performed tokenization based on the specific language using the pre-trained tokenizer. The data is then passed into the corresponding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation</head><p>We participated in the HARD-HARD evaluation, i.e., we provided a single label per tweet, which was evaluated against the gold label. For the official evaluation on the test set, we report ICM and F1 for the positive class (sexist). For our internal results on the development set, we report the macro-averaged F1 score, the F1 score per class, and ICM. ICM is a score developed to measure the similarity between two datapoints more accurately <ref type="bibr" coords="4,379.10,477.97,16.36,10.91" target="#b11">[12]</ref>. It is a generalization of Pointwise Mutual Accuracy (PMA) and compares the closeness between two different outputs by comparing them to a ground truth value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Official Results</head><p>Table <ref type="table" coords="4,115.72,584.67,5.06,10.91" target="#tab_0">1</ref> shows the official scores on the test set. These results show the ICM metric scores and the F1 scores of the positive class for the two models in the HARD-HARD evaluation ZaRa-IU-NLP_1 is based on the multilingual XLM-RoBERTa, and ZaRa-IU-NLP_2 on a combination of RoBERTa and BETO. Our scores show that the combination of language specific models outperforms the generalized model in each metric, even though the individual language models were trained on only half the data. The models produce consistent results across languages for sexist tweets, with the language specific models producing an F1 score of 0.7332 for Spanish </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results on the Validation Set</head><p>Table <ref type="table" coords="5,115.52,447.85,5.02,10.91" target="#tab_1">2</ref> compares the performance of our two models on the validation set, including language specific results for both architectures. The results are in line with the official results, ZaRa-IU-NLP_2, the combination of language specific models, outperforms the multilingual ZaRa-IU-NLP_1 with an ICM of 0.4444 as compared to 0.3168. This shows that it is possible to obtain solid results given a small set of data and that the quality of the data (wrt. the language) is more important than the size of the training data. A look at the F1-scores per class shows a balanced performance, the score for the sexist class is only slightly lower than the one for the majority class of non-sexist tweets.</p><p>We then had a closer look at precision and recall per language and per class. These results are shown in Table <ref type="table" coords="5,177.55,569.79,3.78,10.91" target="#tab_2">3</ref>. These results show that the multilingual model ZaRa-IU-NLP_1 shows the same preference for the non-sexist class given the higher recall for this class. However, this trend is much more pronounced for Spanish where the recall for non-sexist reaches 81.08% while the recall for the sexist class is at 63.24%. In the combined model ZaRa-IU-NLP_2, the Spanish classifier has a preference for the non-sexist class (recall: 85.81) while the English classifier has a preference for the sexist class (recall: 86.08). These results suggest that we might be able to gain better performance for Spanish if we upsample the sexist class. We leave this for future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Overall, the study addresses the difficulties of online sexism detection in social networks in a bilingual setting. We carried out a comparison between two distinct architectures: the first, a large, multilingual model that processes the entire data set, and the second, a combination of language specific, smaller models that implement a split in language categorization for processing. Given our minimal pre-processing, the higher accuracy of the language specific model can be attributed to the more specialized language models. Our results, when tested on the developmental set, show that the combination of two specialized models outperforms the single, generalized one by a difference of 3 percent points in macro-averaged F1 score, and a difference in ICM of 0.13. Overall, this study contributes to the process of automatic sexism detection in social networks in highlighting the greater efficiency and accuracy of two specialized models, rather than a multi-lingual model. As described above, this system was developed as a project in a course on machine learning. For this reason, the system was intentionally kept simple, so that it could be carried out in a short amount of time. This is the reason, for example, why we focused on comparing the two systems without preprocessing the data, and without investigating other language models, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,247.17,236.21,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data Flow during training for both approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,376.24,127.43"><head>Table 1</head><label>1</label><figDesc>Official results on the test set.</figDesc><table coords="5,130.05,119.88,335.18,98.03"><row><cell>Model</cell><cell cols="5">Language Rank ICM Hard ICM Hard Norm Macro F1</cell></row><row><cell cols="2">ZaRa-IU-NLP_1 All</cell><cell>47</cell><cell>0.2842</cell><cell>0.5471</cell><cell>0.6955</cell></row><row><cell></cell><cell>Spanish</cell><cell>53</cell><cell>0.1935</cell><cell>0.4661</cell><cell>0.6956</cell></row><row><cell></cell><cell>English</cell><cell>45</cell><cell>0.3692</cell><cell>0.6287</cell><cell>0.6954</cell></row><row><cell cols="2">ZaRa-IU-NLP_2 All</cell><cell>42</cell><cell>0.3914</cell><cell>0.6154</cell><cell>0.7305</cell></row><row><cell></cell><cell>Spanish</cell><cell>44</cell><cell>0.3056</cell><cell>0.5404</cell><cell>0.7332</cell></row><row><cell></cell><cell>English</cell><cell>38</cell><cell>0.4609</cell><cell>0.6844</cell><cell>0.7263</cell></row><row><cell>Baseline</cell><cell>All</cell><cell></cell><cell>-0.4261</cell><cell></cell><cell>0.3272</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,240.17,418.19,168.86"><head>Table 2</head><label>2</label><figDesc>Results comparing the two approaches on the validation set.</figDesc><table coords="5,89.29,271.79,417.89,137.24"><row><cell>Model</cell><cell cols="5">Language F1 sexist F1 non-sexist Macro F1 Precision Recall</cell><cell>ICM</cell></row><row><cell cols="2">ZaRa-IU-NLP_1 All</cell><cell>0.7248</cell><cell>0.7382</cell><cell>0.7315</cell><cell>0.7155 0.7910 0.3168</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.6823</cell><cell>0.7631</cell><cell>0.7631</cell><cell>0.7207 0.8108</cell></row><row><cell></cell><cell>English</cell><cell>0.7750</cell><cell>0.7349</cell><cell>0.7349</cell><cell>0.7081 0.7638</cell></row><row><cell cols="2">ZaRa-IU-NLP_2 All</cell><cell>0.7610</cell><cell>0.7828</cell><cell>0.7719</cell><cell>0.7597 0.8027 0.4444</cell></row><row><cell></cell><cell>Spanish</cell><cell>0.7061</cell><cell>0.7912</cell><cell>0.7912</cell><cell>0.7341 0.8581</cell></row><row><cell></cell><cell>English</cell><cell>0.8289</cell><cell>0.7639</cell><cell>0.7639</cell><cell>0.8051 0.7268</cell></row><row><cell cols="6">tweets and 0.7263 for English tweets, while the single, large model reached 0.6956 and 0.6954,</cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,353.58,105.74"><head>Table 3</head><label>3</label><figDesc>Results comparing precision and recall per language.</figDesc><table coords="6,152.71,122.10,289.86,74.12"><row><cell></cell><cell></cell><cell>Sexist</cell><cell></cell><cell cols="2">Non-sexist</cell></row><row><cell>Model</cell><cell cols="5">Language Precision Recall Precision Recall</cell></row><row><cell cols="2">ZaRa-IU-NLP_1 Spanish</cell><cell>74.07</cell><cell>63.24</cell><cell>72.07</cell><cell>81.08</cell></row><row><cell></cell><cell>English</cell><cell>80.08</cell><cell>75.09</cell><cell>70.82</cell><cell>76.39</cell></row><row><cell cols="2">ZaRa-IU-NLP_2 Spanish</cell><cell>79.31</cell><cell>63.64</cell><cell>73.41</cell><cell>85.81</cell></row><row><cell></cell><cell>English</cell><cell>79.93</cell><cell>86.08</cell><cell>80.51</cell><cell>72.63</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,495.56,393.33,10.91;6,112.66,509.11,393.33,10.91;6,112.33,522.66,393.66,10.91;6,112.66,536.21,394.04,10.91;6,112.66,549.76,123.39,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,288.86,495.56,217.12,10.91;6,112.66,509.11,284.65,10.91">Some experiments on the use of natural language processing for sexism detection and classification in social media</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Santos-Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Alonso</surname></persName>
		</author>
		<idno type="DOI">10.29007/8z6l</idno>
		<ptr target="https://easychair.org/publications/paper/rdrm.doi:10.29007/8z6l" />
	</analytic>
	<monogr>
		<title level="m" coord="6,142.10,522.66,215.83,10.91">Proceedings of V XoveTIC Conference (XoveTIC)</title>
		<title level="s" coord="6,425.41,523.67,80.58,9.72;6,112.66,537.22,56.98,9.72">Kalpa Publications in Computing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Leitao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Ramos</surname></persName>
		</editor>
		<meeting>V XoveTIC Conference (XoveTIC)</meeting>
		<imprint>
			<publisher>EasyChair</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="24" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,563.31,9.54,10.91;6,142.66,563.31,10.23,10.91;6,173.35,563.31,37.57,10.91;6,235.82,563.31,57.36,10.91;6,313.64,563.31,29.05,10.91;6,363.16,563.31,9.12,10.91;6,392.74,563.31,26.97,10.91;6,440.18,563.31,28.60,10.91;6,489.24,563.31,16.75,10.91;6,112.66,576.85,99.62,10.91;6,232.53,576.85,37.83,10.91;6,290.63,576.85,32.64,10.91;6,343.53,576.85,33.63,10.91;6,397.42,576.85,16.75,10.91;6,434.43,576.85,22.44,10.91;6,481.52,576.85,25.76,10.91;6,112.31,590.40,280.40,10.91;6,112.66,603.95,437.96,10.91;6,112.66,617.50,22.69,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">N</forename><surname>Women</surname></persName>
		</author>
		<ptr target="https://www.unwomen.org/en/digital-library/publications/2022/10/accelerating-efforts-to-tackle-online-and-technology-facilitated-violence-against-women-and-girls" />
		<title level="m" coord="6,235.82,563.31,57.36,10.91;6,313.64,563.31,29.05,10.91;6,363.16,563.31,9.12,10.91;6,392.74,563.31,26.97,10.91;6,440.18,563.31,28.60,10.91;6,489.24,563.31,16.75,10.91;6,112.66,576.85,99.62,10.91;6,232.53,576.85,37.83,10.91;6,290.63,576.85,32.64,10.91;6,343.53,576.85,33.63,10.91;6,397.42,576.85,16.75,10.91;6,434.43,576.85,18.70,10.91">Accelerating efforts to tackle online and technology-facilitated violence against women and girls</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,631.05,393.33,10.91;6,112.26,644.60,338.91,10.91" xml:id="b2">
	<analytic>
		<ptr target="https://onlineviolencewomen.eiu.com/" />
	</analytic>
	<monogr>
		<title level="m" coord="6,112.66,631.05,393.33,10.91;6,112.26,644.60,29.47,10.91">The Economist Intelligence Unit, Measuring the prevalence of online violence against women</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,658.15,394.53,10.91;7,112.66,86.97,393.33,10.91;7,112.66,100.52,394.53,10.91;7,112.66,114.06,393.33,10.91;7,112.66,127.61,333.46,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,112.66,86.97,393.33,10.91;7,112.66,100.52,72.05,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,401.00,114.06,104.99,10.91;7,112.66,127.61,206.17,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,141.16,394.53,10.91;7,112.66,154.71,393.33,10.91;7,112.66,168.26,393.33,10.91;7,112.33,181.81,395.22,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,112.66,154.71,393.33,10.91;7,112.66,168.26,164.31,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,141.67,181.81,335.74,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,195.36,394.61,10.91;7,112.66,208.91,209.46,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,189.20,195.36,298.41,10.91">Detecting and classifying sexism by ensembling transformers models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaca-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,208.91,100.77,10.91">Proceedings of IberLEF</title>
		<meeting>IberLEF<address><addrLine>A CoruÃ±a, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,222.46,393.53,10.91;7,112.66,236.01,377.34,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,360.43,222.46,145.76,10.91;7,112.66,236.01,144.95,10.91">Bi-ensembles of transformer for online bilingual sexism detection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villa-Cueva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>SÃ¡nchez-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>LÃ³pez-Monroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,280.54,236.01,100.77,10.91">Proceedings of IberLEF</title>
		<meeting>IberLEF<address><addrLine>A CoruÃ±a, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,249.56,394.53,10.91;7,112.66,263.11,393.33,10.91;7,112.66,276.66,393.32,10.91;7,112.66,290.20,394.62,10.91;7,112.66,303.75,386.47,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,271.58,263.11,234.41,10.91;7,112.66,276.66,20.10,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>GuzmÃ¡n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.747.doi:10.18653/v1/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m" coord="7,155.39,276.66,350.59,10.91;7,112.66,290.20,238.14,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,317.30,395.17,10.91;7,112.66,330.85,393.32,10.91;7,112.33,344.40,296.49,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<title level="m" coord="7,140.43,330.85,261.00,10.91">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,357.95,393.66,10.91;7,112.66,371.50,393.33,10.91;7,112.66,385.05,152.55,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,390.93,357.95,115.38,10.91;7,112.66,371.50,114.67,10.91">Spanish pre-trained BERT model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>CaÃ±ete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,250.05,371.50,255.94,10.91;7,112.66,385.05,18.15,10.91">Practical ML for Developing Countries Workshop @ ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,398.60,394.04,10.91;7,112.41,412.15,260.80,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,157.55,398.60,224.25,10.91">Compilation of large Spanish unannotated corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>CaÃ±ete</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3247731</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3247731.doi:10.5281/zenodo.3247731" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,425.70,394.62,10.91;7,112.66,439.25,393.32,10.91;7,112.33,452.79,122.11,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,223.40,425.70,260.55,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,439.25,393.32,10.91;7,112.33,452.79,23.88,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
