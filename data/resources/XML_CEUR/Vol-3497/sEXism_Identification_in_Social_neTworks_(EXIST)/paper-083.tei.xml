<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.19,410.69,15.42;1,89.29,106.11,286.81,15.42">ROH_NEIL@EXIST2023: Detecting Sexism in Tweets using Multilingual Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.42,86.75,11.96"><forename type="first">Rohit</forename><surname>Koonireddy</surname></persName>
							<email>rohit.koonireddy@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<postCode>8006</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.19,134.42,66.21,11.96"><forename type="first">Niloofar</forename><surname>Adel</surname></persName>
							<email>niloofar.adel@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<postCode>8006</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.19,410.69,15.42;1,89.29,106.11,286.81,15.42">ROH_NEIL@EXIST2023: Detecting Sexism in Tweets using Multilingual Language Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">ED3FCB1EA06D64492754C98BB2F9AB9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transformers</term>
					<term>Cross-lingual Language Models</term>
					<term>Multilingual Language Models</term>
					<term>Sexism Detection</term>
					<term>Sexism Categorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a submission to the EXIST 2023 challenge for binary, multi-class, and multi-label classification tasks, targeting the detection of sexism in English and Spanish tweets. Our approach employs a cross-lingual transformer model for these tasks, with a specific emphasis on "discrete" (hardlabel) classification. Through a comparative analysis with other transformer-based models, our final model demonstrates effectiveness of the cross-lingual transformer models in achieving competitive performance. Notably, our model also achieves favorable rankings in hard-labeling for all the tasks. These results underscore the potential of cross-lingual models in accurately classifying English and Spanish text data without relying on language-specific models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Gender-based discrimination in the form of sexism remains a pervasive issue that continues to affect digital interactions, posing significant challenges in the creation of inclusive and respectful online spaces. The rapid growth of social media platforms has exacerbated the dissemination of sexist content, emphasizing the urgent need for automated approaches to detect and classify such content.</p><p>The objective of this paper is to identify sexism in tweets by utilizing pre-trained transformer models and leveraging the data provided by the EXIST 2023 challenge. This study aims to harness the capabilities of pre-trained transformer models, specifically tailored to excel in natural language processing (NLP), to develop a robust and meticulous classification system capable of distinguishing tweets containing sexist content. The EXIST 2023 challenge provides a carefully curated dataset to detect sexism in both English and Spanish languages' tweets, serving as an optimal foundation for training and evaluating our models. The implications of sexism classification research extend beyond academia and hold the potential to make a profound impact on combating sexism within online spaces.</p><p>In the following sections, we will delve into pertinent existing literature, describe the dataset used for this challenge, detail the methodology centered on pre-trained transformer models, present experimental results, draw conclusions, and outline future directions for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Detecting sexism and offensive language in social media conversations and general text has garnered significant attention, leading to the development of various models and approaches. Lexical and linguistic analysis, leveraging word embeddings and semantic similarity, has proven effective in identifying instances of gender bias and potential sexism <ref type="bibr" coords="2,395.73,213.34,11.41,10.91" target="#b0">[1]</ref>. Until the recent past, machine learning techniques, particularly Recurrent Neural Networks (RNNs), have been widely adopted for the classification of social media posts as sexist or non-sexist by capturing sequential dependencies in the text <ref type="bibr" coords="2,200.29,253.99,11.43,10.91" target="#b1">[2]</ref>.</p><p>With the advent of transformers since the seminal work of "Attention is All You Need" by Vaswani et al. <ref type="bibr" coords="2,176.15,281.08,11.58,10.91" target="#b2">[3]</ref>, the utilization of transformers for classification tasks has garnered significant attention. Transformers have demonstrated superior accuracy compared to their predecessors. Notably, XLM models have recently achieved state-of-the-art performance in various benchmark tasks <ref type="bibr" coords="2,205.27,321.73,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,219.61,321.73,7.65,10.91" target="#b4">5]</ref>. XLM transformers have emerged as a powerful tool for text classification tasks, particularly in the domain of cross-lingual language modeling. XLM, which stands for Cross-lingual Language Model, is specifically designed to effectively handle multiple languages <ref type="bibr" coords="2,137.88,362.38,11.58,10.91" target="#b3">[4]</ref>. By leveraging large-scale pre-training on diverse multilingual corpora, XLM transformers capture comprehensive language representations that can be transferred across different languages <ref type="bibr" coords="2,178.06,389.48,11.51,10.91" target="#b4">[5]</ref>. This cross-lingual capability enables XLM transformers to generalize well to languages with limited training data, facilitating effective knowledge transfer from high-resource to low-resource languages <ref type="bibr" coords="2,280.32,416.58,11.59,10.91" target="#b5">[6]</ref>. Furthermore, XLM transformers incorporate cross-lingual alignment techniques, enabling the alignment of word and sentence embeddings across multiple languages. This facilitates cross-lingual transfer learning and leads to enhanced performance in multilingual text classification tasks <ref type="bibr" coords="2,325.85,457.22,11.54,10.91" target="#b4">[5]</ref>. Domain-specific models tailored for sentiment analysis on Twitter, such as the XLM-T models, have also demonstrated exceptional performance <ref type="bibr" coords="2,148.69,484.32,11.43,10.91" target="#b6">[7]</ref>. This is the third edition of the EXIST Shared Task <ref type="bibr" coords="2,325.40,497.87,11.48,10.91" target="#b7">[8]</ref>. In the editions of the past two years, plenty of methods and models have been used to solve the challenge. In the first edition in 2021, ensembles of language-specific models tailored for English (RoBERTa) and Spanish (BETO) achieved the best results. Similarly, in the second edition in 2022, ensembles of different languagespecific transformer models, including BERTweet-large <ref type="bibr" coords="2,344.61,552.07,11.58,10.91" target="#b8">[9]</ref>, RoBERTa, DeBERTa v3 <ref type="bibr" coords="2,471.79,552.07,18.07,10.91" target="#b9">[10]</ref> for English, and BETO, BERTIN <ref type="bibr" coords="2,217.32,565.62,16.25,10.91" target="#b10">[11]</ref>, MarIA-base <ref type="bibr" coords="2,294.86,565.62,16.24,10.91" target="#b11">[12]</ref>, and RoBERTuito for Spanish, achieved the best results.</p><p>Given the enduring success of transformer-based models, we initially employed languagespecific models to solve the tasks and subsequently explored the use of superior Multilingual Language Models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset and Tasks Description</head><p>EXIST 2023 edition presents a hierarchical classification task. First, each tweet is labeled manually by 6 different annotators either as "SEXIST" (indicated with "YES") or as "NON-SEXIST" (indicated with "NO"). The percentage of annotators categorizing a tweet as "SEXIST" and "NON-SEXIST" are given as a soft-labels. The label with larger percentage is considered the hard-label (SEXIST% &gt; NON-SEXIST% =&gt; "YES", else if "NO", else "-"). Tweets are labeled with "-" in tasks 2 and 3 when an annotator classifies them as "NON-SEXIST" or "NO" in the first task. Additionally, the label "UNKNOWN" is assigned to tweets for which no clear label was provided by the majority of annotators <ref type="bibr" coords="3,226.57,206.12,11.58,10.91" target="#b7">[8]</ref>. We safely ignore these instances of "-" and "unknown" as these labels wont affect the final classification.. Once the tweet is labeled as either, annotators further label them in tasks 2 and 3. We observed (almost) even distribution of tweets across languages, labels, annotators, and age groups, as shown in Figure <ref type="figure" coords="3,391.41,246.77,3.81,10.91">1</ref>. More details about the labelling process can be found in EXIST2023 overview <ref type="bibr" coords="3,327.09,260.32,14.11,10.91" target="#b7">[8]</ref>.</p><p>Table <ref type="table" coords="3,127.31,273.87,5.13,10.91" target="#tab_0">1</ref> shows the details about the given "hard-labels" training and development data sets for EXIST 2023. Tables <ref type="table" coords="3,192.96,287.42,7.47,10.91">8,</ref><ref type="table" coords="3,203.16,287.42,3.74,10.91" target="#tab_2">9</ref>, 10 present examples of hard-labels and soft-labels for each task.</p><p>The test data was a mix of English (978) and Spanish(1098) tweets accounting for a total of 2076 tweets. Values are not shown since each tweet was given multiple labels. A representation of data can be seen in Table <ref type="table" coords="3,423.67,524.33,7.64,8.87" target="#tab_0">10</ref>.</p><p>.</p><p>EXIST 2023 demonstrates a higher level of evolution and complexity in comparison to its previous two editions. The dataset used for classification purposes is smaller in size than the datasets employed in previous instances. Furthermore, the number of tasks has increased from two to three, and soft-labels are now incorporated. Hence, in an attempt to use larger corpus, we use data from EXIST 2021 for one run of task 1 hard-label classification. Some more information about the datasets used in EXIST 2021 and EXIST 2022 can be found in the Table <ref type="table" coords="3,450.33,640.12,8.36,10.91" target="#tab_0">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tasks in EXIST 2023</head><p>All the three tasks in the hierarchy are classification tasks, two in the area of classification (binary and multi-class classification) and the last one is multi-label classification (categorization). Compared to the discrete label classification tasks from EXIST 2021 and EXIST 2022, this edition requires participants to publish probability values associated with each prediction which are called soft-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Task 1-Binary Classification</head><p>The first task was to carry out a binary classification in which the models had to classify the tweets into two classes, namely: "SEXIST" tweets and "NON-SEXIST" tweets. It is important to note that this task is used as an initial step for the other two tasks, meaning that tweets classified as not sexist in the first task were not considered in the second and third tasks. Some examples of tweets in the dataset with the predicted labels are shown in Table 8 <ref type="bibr" coords="4,443.66,265.26,16.92,10.91" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Task 2 -Multi-class Classification</head><p>The second task is a ternary classification task. The tweets that were classified as Sexist (with the label YES) in the previous task, must be classified into 3 different classes in accordance with their creator's intention, thus revealing the role that social networks play in generating and disseminating sexist posts. Table <ref type="table" coords="4,237.72,355.23,5.08,10.91" target="#tab_2">9</ref> demonstrates some examples of labeled tweets in this task. The classes are <ref type="bibr" coords="4,154.72,368.78,17.16,10.91" target="#b12">[13]</ref>:</p><p>1. DIRECT 2. REPORTED 3. JUDGEMENTAL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Task 3 -Multi-label Classification</head><p>Task 3 is a multi-label classification task. Here, in contrast to the previous tasks, each annotator could label the tweet with a list of labels and as a result, a list of hard-labels are expected instead of just one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We decided to exclusively utilize pre-trained Transformer models based on their impressive performance in previous editions of EXIST. To ensure both robustness and ease of implementation, we selected HuggingFace's <ref type="bibr" coords="4,232.45,652.88,17.78,10.91" target="#b13">[14]</ref> Trainer API and Optuna <ref type="bibr" coords="4,361.96,652.88,17.79,10.91" target="#b14">[15]</ref> as our helping architectures, minimizing the need for custom code development. Our experimentation involved a comprehensive evaluation of existing General Language and Tweet-specific language (NLP) models.</p><p>For the initial model evaluation, we focused on Task 1, specifically the prediction of hard-labels. Various parameter configurations were tested for each transformer model, as shown in the Table <ref type="table" coords="5,89.29,141.16,3.66,10.91" target="#tab_3">2</ref>. To perform the initial model evaluation, the training data and development data were merged into a single file and then split into an 80:10:10 ratio for training, validation, and testing folds. Two distinct sets of parameters were selected to assess the performance of each pre-trained language model. Once the best model(s) is chosen from this initial selection stage, Optuna <ref type="bibr" coords="5,442.26,195.36,18.07,10.91" target="#b14">[15]</ref> was used to search for the best hyperparameters for each task again for both hard-label and soft-label predictions. Details about the hyperparameters tuning are available in the GitHub repository <ref type="bibr" coords="5,89.29,236.01,16.12,10.91">[16]</ref>. Once the best hyperparameters were found for each task and sub-task, the merged data is split into 85:15 ratio for training and validation sets. The final models are used to predict the official test data items and corresponding labels were submitted according to the organizers' requirements <ref type="bibr" coords="5,152.28,276.66,11.57,10.91" target="#b7">[8]</ref>. For ease of understanding, the flowchart 2 illustrates the steps involved to complete this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initial Model Selection</head><p>In the 2021 edition of EXIST, a majority of those who submitted the results, approached the tasks using transformer-based models. Key architectures used were BERT <ref type="bibr" coords="5,381.83,353.48,21.33,10.91" target="#b15">[17]</ref> (or multilingual BERT -mBERT), Spanish version of BERT called BETO <ref type="bibr" coords="5,295.99,367.03,21.02,10.91" target="#b16">[18]</ref>, RoBERTa <ref type="bibr" coords="5,363.27,367.03,21.88,10.91" target="#b17">[19]</ref> and a multilingual version of RoBERTa called XLM-R <ref type="bibr" coords="5,204.09,380.58,16.10,10.91" target="#b3">[4]</ref>. In the 2022 edition, a majority of those who submitted used transformer models. DeBERTa <ref type="bibr" coords="5,220.48,394.13,21.87,10.91" target="#b9">[10]</ref> and RoBERTuito <ref type="bibr" coords="5,315.81,394.13,20.66,10.91" target="#b18">[20]</ref> was also used along with transformer models employed in 2021.</p><p>Based on the performance of these models in the past two years <ref type="bibr" coords="5,403.89,421.23,16.55,10.91" target="#b19">[21,</ref><ref type="bibr" coords="5,424.12,421.23,12.59,10.91" target="#b20">22,</ref><ref type="bibr" coords="5,440.39,421.23,12.59,10.91" target="#b21">23,</ref><ref type="bibr" coords="5,456.66,421.23,14.11,10.91" target="#b22">24]</ref> and an exploration of available HuggingFacemodels, we carefully selected a group of large language models pre-trained for classification, as presented in Table <ref type="table" coords="5,360.14,448.32,3.81,10.91" target="#tab_3">2</ref>. (Please note that this is not a comprehensive list but presents the outlook of the scale of exploration. The models are chosen such that are equipped with both general language and twitter data understanding.) In this table 2, under the column "Data Type" three labels: "xlm" , "es" and "en" can be identified which represent the type of the data used for model training and evaluation. "xlm" represents combined data of Spanish and English tweets, as given for each task. "en" represents only English tweets where the original Spanish tweets were translated to their English versions using google translate <ref type="bibr" coords="5,186.04,543.17,16.76,10.91" target="#b23">[25]</ref>. "es" represents only Spanish language tweets where English tweets are translated into the Spanish language.</p><p>Once the datasets are prepared as required, each model is tested with varying hyperparameters including choosing an embedding length. All the pre-trained transformer models used in the task are "Sequence Classification" models as required by the tasks in this challenge.</p><p>Upon inspecting the models' performance 2, we observed that the cross-lingual model pretrained on twitter data called "sdadas/xlm-roberta-large-twitter" provides the best test scores. In the rest of the paper, this model is called "XLM-T-10-L" model. This is a XLM-RoBERTa-large model tuned on a corpus of over 156 million tweets in ten languages: English, Spanish, Italian, Portuguese, French, Chinese, Hindi, Arabic, Dutch and Korean. The model has been trained from the original XLM-RoBERTA-large checkpoint for 2 epochs with a batch size of 1024. The model has 560 million parameters. <ref type="bibr" coords="6,245.11,100.52,16.25,10.91" target="#b24">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preprocessing and Data Augmentation</head><p>In our study, we adopt a straightforward preprocessing approach, which aligns with the recommended practices outlined in the usage guide of the XLM-T-10-L model. The preprocessing method focuses on handling tags and URLs present in the tweets. Any user handle encountered in the tweets is replaced with "@USER", while URLs are replaced with "#HTTPURL". We refrain from implementing additional preprocessing steps in order to retain the intricacies and unique characteristics inherent in the tweet data. By avoiding excessive preprocessing, we aim to preserve the originality and nuances of the tweet content, allowing the models to capture the genuine nature of the tweets during the subsequent analysis and classification stages.</p><p>We also utilized the data from the EXIST 2021 edition to augment the provided training data for one run of the first task. This additional data was also translated into full Spanish and full English versions using googletrans <ref type="bibr" coords="6,244.06,285.69,18.36,10.91" target="#b23">[25]</ref>. This data was also used to examine the performance of language-specific models such as BERTweet-large <ref type="bibr" coords="6,330.47,299.24,13.00,10.91" target="#b8">[9]</ref> RoberTuito <ref type="bibr" coords="6,399.77,299.24,18.07,10.91" target="#b18">[20]</ref> on the English and Spanish data respectively. These models performed well, but not as well as the XLM-RoBERTa-Large <ref type="bibr" coords="6,113.21,326.34,19.13,10.91" target="#b24">[26]</ref> based models as seen in the Table <ref type="table" coords="6,287.37,326.34,3.74,10.91" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyperparameter Search</head><p>It is imperative to test hyperparameters in machine learning models to determine performance, particularly in transfer learning tasks and also when training data is limited, as in this challenge. Since hyperparameter tuning is known to play an important role in improving the performance and generalization of the model, an open-source framework that allows hyperparameters search, called Optuna <ref type="bibr" coords="6,147.06,430.21,20.22,10.91" target="#b14">[15]</ref> together with HuggingFace Transformers and TrainerAPI is used. The Optuna solution offers a versatile and effective way to identify optimal hyperparameters automatically, which reduces the need for manual tuning and boosts the development speed of your application, thus reducing the reliance on manual tuning.</p><p>Optuna works by defining an objective function that represents the evaluation metric or loss function of the model. This objective function takes an arbitrary set of hyperparameters and their search space as input and returns a score or value that represents the performance of the model with those hyperparameters. This function could be a loss function to minimize or a metric to maximize. Optuna's goal is to find the set of hyperparameters that optimize this objective function through Optuna's method called create_study <ref type="bibr" coords="6,374.04,552.16,18.13,10.91" target="#b14">[15]</ref>.</p><p>We used Optuna determine the "Learning Rate", "Weight Decay" and "Number of Epochs" for each task and its subsequent hard and soft-label predictions as shown in Table <ref type="table" coords="6,440.20,579.25,3.73,10.91" target="#tab_5">3</ref>. More details can be found here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training Model</head><p>Once we found desired best hyperparameters using Optuna, we trained models for each task using HuggingFace trainer API as shown in the part of source code.. We ran each task (except task 2) three times and the results were published as runs 1, 2, 3. During training, as described For Run 1 -Task 1 and Task 2-The model is trained with best hyperparameters to predict hard-labels. Once hard-labels are predicted, their corresponding logits from the model are converted to probabilities using the Softmax function. The predicted hard-labels and soft-labels are submitted together as Run 1. Binary Cross Entropy loss (BCELoss), and Categorical Cross Entropy loss (CCELoss) are used here.</p><p>For Run 1 -Task 3 -Instead of the Softmax function, Sigmoid is applied on logits to calculate the probabilities of hard-labels. BCELoss for each label is applied separately for the third task.</p><p>For Run 2 -Task 1, Task 2, Task 3 -The model is trained with best hyperparameters to predict the given soft-labels (probabilities for each label) and the output predictions of soft-labels are combined with output predictions of hard-labels from Run 1. PyTorch's existing functionality of CCELoss <ref type="bibr" coords="8,137.71,404.85,21.04,10.91" target="#b25">[27]</ref> which handles soft-labels is used for all the tasks as shown here.</p><p>For Run3 -Task 1 -Large augmented dataset which combined 2021 editions' twitter tweets data along with current data is used to train the model for hard-label predictions. Predictions and corresponding Softmax on logits are submitted.</p><p>For Run3 -Task 3 -Soft-label predictions from Run 2 and its corresponding "argmax" hardlabels are submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Metrics</head><p>A variety of evaluation metrics, such as F1 Score, Cross Entropy, ICM-Soft, ICM-Soft Norm, ICM-Hard, and ICM-Hard Norm, are used by the organizers to assess model performance. Particularly, the Information Contrast Measure (ICM) is employed as a similarity function for evaluating hierarchical classification outputs, providing insights into class relationships and similarities (higher the better) <ref type="bibr" coords="8,222.25,576.52,16.52,10.91" target="#b26">[28]</ref>.</p><p>In the evaluation, three types of assessment criteria are employed: hard-hard, hard-soft, and soft-soft. Hard-hard evaluation compares the system's output with the majority class determined by annotators' labels. Hard-soft evaluation compares the system's categories with the probabilities assigned to each category in the ground truth. Soft-soft evaluation compares the system's probabilities with those assigned by human annotators. These details can be observed in the Section 5 tables .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4 Description and Direction of Evaluation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Description and Direction ICM <ref type="bibr" coords="9,110.03,130.93,19.68,8.87" target="#b26">[28]</ref> Information Contrast Measure (ICM) is a similarity function used to evaluate the outputs of classification systems in hierarchical classification tasks. It generalizes Pointwise Mutual Information (PMI) and measures the resemblance between the system's output and the ground truth labels. Higher values of ICM indicate better performance. ICM-Soft ICM-Soft is an evaluation metric that compares the categories assigned by the system with the probabilities assigned to each category in the ground truth. It considers the distribution of labels and the number of annotators assigned to each instance to determine the probability of the classes. Higher values of ICM-Soft indicate better performance. ICM-Hard ICM-Hard evaluation involves comparing the system's "hard" output with the hard ground truth labels. A probabilistic threshold is employed to extract the hard-labels from the ground truth, considering the approval of multiple annotators for each task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The overview of the final results of this study's submissions can be found in the Tables <ref type="table" coords="9,461.96,537.56,7.32,10.91" target="#tab_6">5,</ref><ref type="table" coords="9,471.54,537.56,4.10,10.91">6</ref>,7 with the prefix "roh-niel" and with a prefix 1 or 2 or 3 depending on the run that achieves the shown results (Information about each run is explained in previous sections). For comparison, metrics of the gold-labels (as provided by the organizers) and the best ranked results are provided for each task and criterion.</p><p>This study achieves favourable rankings in hard-labelling for Task 1, Task 2 and Task 3, , using pre-trained language model based on "XLM-T-10-L" <ref type="bibr" coords="9,343.63,618.85,19.41,10.91" target="#b24">[26]</ref>, as shown below. These results highlight the potential of cross-lingual models to accurately classify English and Spanish text data without the need for language-specific model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Why "XLM-T-10-L" model performs well?</head><p>First, the XLM-T-10-L is purposefully designed and pre-trained on a comprehensive corpus of Twitter data, allowing it to capture the distinctive characteristics and intricate patterns inherent in tweets. This specialized training equips the model with the ability to excel in our specific context. Second, the model harnesses the transformer architecture renowned for its capacity to capture extensive contextual information and long-range dependencies in text. Despite the inherent brevity of tweet texts, the transformer architecture empowers the model to proficiently discern the underlying semantics and contextual nuances, taking into account the intricate interplay between words. Through the acquisition of comprehensive representations, the model adeptly extracts meaningful features and intricate patterns from tweet text.</p><p>Third, the model undergoes training on a diverse array of tweets spanning ten distinct languages, thereby demonstrating its aptitude for handling multilingual data. By leveraging the principles of cross-lingual transfer learning, the model effectively generalizes across languages, even in scenarios characterized by limited training data. This innate cross-lingual capability confers significant advantages in the domain of tweet classification, given the multilingual nature of tweets and the potential occurrence of transliteration phenomena.</p><p>In conclusion, the exemplary performance of the XLM-T-10-L can be attributed to its meticulous training on Twitter data, meticulous fine-tuning tailored for tweet classification tasks, the inherent effectiveness of the transformer architecture for representation learning, and the valuable cross-lingual capabilities it possesses. The synergistic interplay of these factors enables the model to achieve accurate understanding and proficient classification of tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this study, our best solutions for tasks 1,2 and 3 of EXIST2023 are discussed and presented. A detailed study about the performance of existing transformer-based Language Models <ref type="bibr" coords="11,485.61,635.80,20.37,10.91" target="#b13">[14]</ref> is conducted. To arrive at the final results of the task, a two-level approach is taken. First, a study to find the best models using Task 1 hard-label classification is conducted. Second, the best model is fine-tuned for each task and sub-task requirements using Optuna. Fine-tuned is trained on more data and then predictions are acquired and published. This study's performance is sub-optimal for the soft-label classifications for each task. Poor performance for soft-labels may be majorly accounted to these 2 reasons:</p><p>• Utilizing incorrect loss function:</p><p>While it is clear that using the logistic loss function is the appropriate approach for solving the soft-label task, we initially attempted to implement custom loss functions for each task based on matching distributions of the soft-labels. Specifically, we explored the adoption of the Kullback-Liebler Divergence loss and utilized the "sum reduction" variant of Cross Entropy Loss for each task. However, our analysis and findings suggest that a straightforward implementation of the logistic loss function will likely achieve superior soft-label metrics.</p><p>• Relying on task 1 hard-labels classification as the basis for model selection and subsequently applying it to soft-label tasks:</p><p>In our methodology, we initially relied on task 1 hard-labels classification as the foundation for selecting the most suitable model. Subsequently, we utilized the same model for the soft-label tasks. This way of approaching the task may also have resulted in poor performance in the soft-label tasks.</p><p>It is certain that using the correct loss functions with the same models will better this study's results for all the tasks reflecting those of hard-label scores. The next steps would be to fine-tune models with standard logistic loss functions to achieve better soft-label predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,422.52,251.28,100.42,8.87;9,141.64,263.23,381.22,8.87;9,141.64,275.19,54.06,8.87;9,95.27,287.54,22.25,8.87;9,95.27,299.50,16.69,8.87;9,95.27,311.45,23.71,8.87;9,141.64,287.54,381.05,8.87;9,141.64,299.50,381.05,8.87;9,141.64,311.45,381.05,8.87;9,141.64,323.41,381.04,8.87;9,141.64,335.36,116.29,8.87;9,95.27,347.72,22.25,8.87;9,95.27,359.67,21.18,8.87;9,95.27,371.63,23.71,8.87;9,141.64,347.72,381.04,8.87;9,141.64,359.67,381.04,8.87;9,141.64,371.63,381.05,8.87;9,141.64,383.58,116.29,8.87;9,95.27,395.93,427.41,8.87;9,141.64,407.89,381.05,8.87;9,141.64,419.85,381.23,8.87;9,141.41,431.80,192.00,8.87;9,95.27,444.15,23.53,8.87;9,95.27,456.11,32.74,8.87;9,141.64,444.15,381.04,8.87;9,141.64,456.11,381.29,8.87;9,141.64,468.06,381.23,8.87;9,141.64,480.02,81.94,8.87"><head></head><label></label><figDesc>Only the most popularly labeled classes are included in this evaluation. Higher values of ICM-Hard indicate better performance. ICM-Soft Norm ICM-Soft Norm is a normalized version of ICM-Soft that takes into account the number of annotators assigned to each instance and adjusts the probabilities accordingly. It handles instances labeled as "UNKNOWN" by reducing the number of annotators considered based on the count of "UNKNOWN" labels associated with them. Higher values of ICM-Soft Norm indicate better performance. ICM-Hard Norm ICM-Hard Norm is a normalized version of ICM-Hard that adjusts the hard-labels based on the number of annotators assigned to each instance. It considers instances labeled as "UNKNOWN" and adjusts the threshold for label extraction accordingly. Higher values of ICM-Hard Norm indicate better performance. F1 Score F1 Score is a commonly used evaluation metric in classification tasks. It is the harmonic mean of precision and recall, weighted by the same values. The F1 score treats false positives and false negatives equally, assuming that both types of errors have the same consequences. Higher values of F1 Score indicate better performance. Cross Entropy Cross Entropy is a metric used to measure the difference between the predicted probabilities and the true probabilities. It quantifies the average amount of information needed to identify the true class given the predicted probabilities. Lower values of Cross Entropy indicate better model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,341.08,195.25,20.87"><head>Table 1</head><label>1</label><figDesc>EXIST 2023 Dataset distribution for hard-labels,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,95.27,352.15,404.74,203.89"><head>EXIST2023 Twitter Training Development Spanish English Spanish English Total Task 1</head><label></label><figDesc></figDesc><table coords="3,95.27,416.47,402.65,139.57"><row><cell>Sexist</cell><cell>1560</cell><cell>1137</cell><cell>261</cell><cell>194</cell><cell>3152</cell></row><row><cell>Non-sexist</cell><cell>1634</cell><cell>1733</cell><cell>229</cell><cell>250</cell><cell>3846</cell></row><row><cell></cell><cell>Task 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Direct</cell><cell>749</cell><cell>545</cell><cell>117</cell><cell>87</cell><cell>1498</cell></row><row><cell>Reported</cell><cell>265</cell><cell>194</cell><cell>40</cell><cell>35</cell><cell>534</cell></row><row><cell>Judgemental</cell><cell>228</cell><cell>148</cell><cell>55</cell><cell>28</cell><cell>459</cell></row><row><cell></cell><cell>Task 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ideological-inequality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Misogyny-non-sexual-violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Objectification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sexual-violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stereotyping-dominance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,89.29,485.86,416.69,92.20"><head>Table 9</head><label>9</label><figDesc></figDesc><table /><note coords="4,176.05,485.86,329.93,10.91;4,89.29,499.41,20.49,10.91;4,103.64,512.96,171.58,10.91;4,103.64,526.50,180.14,10.91;4,103.64,540.05,100.90,10.91;4,103.64,553.60,104.99,10.91;4,103.64,567.15,213.14,10.91"><p><p><p>demonstrates some examples of labeled tweets in this task. The classes are</p><ref type="bibr" coords="4,89.29,499.41,16.39,10.91" target="#b12">[13]</ref></p>: 1. IDEOLOGICAL AND INEQUALITY 2. STEREOTYPING AND DOMINANCE 3. OBJECTIFICATION 4. SEXUAL VIOLENCE 5. MISOGYNY AND NON-SEXUAL VIOLENCE</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,249.44,20.87"><head>Table 2</head><label>2</label><figDesc>Results for testing models using Task1 hard-label predictions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,95.04,122.05,409.20,659.55"><head>Example Parameters (one of the two parameter sets used)</head><label></label><figDesc></figDesc><table coords="7,95.04,134.06,409.20,647.55"><row><cell>TrainingArguments( output_dir="./output",</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>num_train_epochs=3,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>per_device_train_batch_size=16,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>per_device_eval_batch_size=16,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>learning_rate=2e-5,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>weight_decay=0.01,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>logging_dir="./logs",</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>evaluation_strategy="epoch",</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>save_strategy="epoch",</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>logging_strategy="epoch")</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Data</cell><cell>Embed</cell><cell cols="2">Validation Set</cell><cell></cell><cell>Test Set</cell></row><row><cell></cell><cell>Type</cell><cell>Length</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy F1-</cell><cell cols="2">Accuracy F1-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Score</cell><cell></cell><cell>Score</cell></row><row><cell>amberoad/bert-multilingual-passage-</cell><cell>xlm</cell><cell>512</cell><cell>0.793</cell><cell cols="2">0.775 0.793</cell><cell>0.775</cell></row><row><cell>reranking-msmarco</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cardiffnlp/twitter-roberta-base-sentiment</cell><cell>xlm</cell><cell>512</cell><cell>0.814</cell><cell cols="2">0.797 0.809</cell><cell>0.782</cell></row><row><cell>microsoft/Multilingual-MiniLM-L12-H384</cell><cell>xlm</cell><cell>512</cell><cell>0.540</cell><cell cols="2">0.000 0.551</cell><cell>0.000</cell></row><row><cell>papluca/xlm-roberta-base-language-</cell><cell>xlm</cell><cell>512</cell><cell>0.540</cell><cell cols="2">0.000 0.551</cell><cell>0.000</cell></row><row><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>symanto/xlm-roberta-base-snli-mnli-anli-</cell><cell>xlm</cell><cell>512</cell><cell>0.710</cell><cell cols="2">0.633 0.701</cell><cell>0.600</cell></row><row><cell>xnli</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cross-encoder/mmarco-mMiniLMv2-L12-</cell><cell>xlm</cell><cell>128</cell><cell>0.793</cell><cell cols="2">0.775 0.793</cell><cell>0.775</cell></row><row><cell>H384-v1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>xlm-roberta-large</cell><cell>xlm</cell><cell>256</cell><cell>0.540</cell><cell cols="2">0.000 0.551</cell><cell>0.000</cell></row><row><cell>jhu-clsp/bernice</cell><cell>xlm</cell><cell>128</cell><cell>0.846</cell><cell cols="2">0.833 0.851</cell><cell>0.835</cell></row><row><cell>sdadas/xlm-roberta-large-twitter</cell><cell cols="2">xlm 256</cell><cell>0.844</cell><cell cols="2">0.830 0.856</cell><cell>0.835</cell></row><row><cell>sdadas/xlm-roberta-large-twitter</cell><cell cols="2">xlm 128</cell><cell>0.851</cell><cell cols="2">0.839 0.870</cell><cell>0.856</cell></row><row><cell>xlm-roberta-base</cell><cell>xlm</cell><cell>128</cell><cell>0.827</cell><cell cols="2">0.820 0.829</cell><cell>0.811</cell></row><row><cell>Twitter/twhin-bert-base</cell><cell>xlm</cell><cell>128</cell><cell>0.850</cell><cell cols="2">0.839 0.844</cell><cell>0.828</cell></row><row><cell>Josue/BETO-espanhol-Squad2</cell><cell>es</cell><cell>512</cell><cell>0.737</cell><cell cols="2">0.679 0.701</cell><cell>0.623</cell></row><row><cell>cardiffnlp/twitter-roberta-base-sentiment</cell><cell>es</cell><cell>512</cell><cell>0.540</cell><cell cols="2">0.000 0.551</cell><cell>0.000</cell></row><row><cell>finiteautomata/beto-sentiment-analysis</cell><cell>es</cell><cell>256</cell><cell>0.827</cell><cell cols="2">0.807 0.830</cell><cell>0.811</cell></row><row><cell>dccuchile/bert-base-spanish-wwm-cased</cell><cell>es</cell><cell>256</cell><cell>0.836</cell><cell cols="2">0.821 0.847</cell><cell>0.829</cell></row><row><cell>mariav/bert-base-spanish-wwm-cased-</cell><cell>es</cell><cell>256</cell><cell>0.834</cell><cell cols="2">0.818 0.840</cell><cell>0.819</cell></row><row><cell>finetuned-tweets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bertin-project/bertin-roberta-base-spanish</cell><cell>es</cell><cell>256</cell><cell>0.827</cell><cell cols="2">0.809 0.787</cell><cell>0.764</cell></row><row><cell>pysentimiento/robertuito-base-cased</cell><cell>es</cell><cell>128</cell><cell>0.844</cell><cell cols="2">0.829 0.840</cell><cell>0.823</cell></row><row><cell>JosePezantes/finetuned-robertuito-base-</cell><cell>es</cell><cell>128</cell><cell>0.840</cell><cell cols="2">0.826 0.814</cell><cell>0.801</cell></row><row><cell>cased-V-P-G</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hackathon-pln-es/paraphrase-spanish-</cell><cell>es</cell><cell>128</cell><cell>0.831</cell><cell cols="2">0.817 0.829</cell><cell>0.808</cell></row><row><cell>distilroberta</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mrm8488/bert-spanish-cased-finetuned-ner</cell><cell>es</cell><cell>128</cell><cell>0.836</cell><cell cols="2">0.824 0.823</cell><cell>0.807</cell></row><row><cell>Hate-speech-CNERG/dehatebert-mono-</cell><cell>es</cell><cell>256</cell><cell>0.800</cell><cell cols="2">0.783 0.773</cell><cell>0.747</cell></row><row><cell>spanish</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMG/xlm-roberta-base-sa-spanish</cell><cell>es</cell><cell>128</cell><cell>0.829</cell><cell cols="2">0.821 0.820</cell><cell>0.804</cell></row><row><cell>JonatanGk/roberta-base-bne-finetuned-</cell><cell>es</cell><cell>128</cell><cell>0.800</cell><cell cols="2">0.778 0.779</cell><cell>0.741</cell></row><row><cell>cyberbullying-spanish</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dccuchile/albert-base-spanish-finetuned-xnli es</cell><cell>128</cell><cell>0.799</cell><cell cols="2">0.783 0.799</cell><cell>0.771</cell></row><row><cell>cardiffnlp/roberta-base-tweet-sentiment-en</cell><cell>en</cell><cell>256</cell><cell>0.816</cell><cell cols="2">0.806 0.797</cell><cell>0.779</cell></row><row><cell>cardiffnlp/twitter-roberta-base-sentiment-</cell><cell>en</cell><cell>256</cell><cell>0.851</cell><cell cols="2">0.839 0.821</cell><cell>0.803</cell></row><row><cell>latest</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NLP-LTU/bertweet-large-sexism-detector</cell><cell>en</cell><cell>256</cell><cell>0.540</cell><cell cols="2">0.000 0.551</cell><cell>0.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,90.49,416.99,176.23"><head>Table 3</head><label>3</label><figDesc>Best hyperparameters for each task determined by Optuna earlier, given training data and development data are combined, shuffled, and then split into 85:15 training and validation datasets with which training takes place. Each run of the task was submitted to achieve the best scores in both hard-labeling and soft-labeling sub-tasks.</figDesc><table coords="8,171.74,118.53,251.80,86.23"><row><cell>Task</cell><cell cols="3">Learning Rate Weight Decay Epochs</cell></row><row><cell cols="2">Task 1 (Hard) 3.3951 × 10 -5</cell><cell>0.0049</cell><cell>3</cell></row><row><cell>Task 2 (Hard)</cell><cell>0.0004</cell><cell>0.0054</cell><cell>4</cell></row><row><cell cols="2">Task 3 (Hard) 1.5592 × 10 -5</cell><cell>0.0002</cell><cell>2</cell></row><row><cell>Task 1 (Soft)</cell><cell>2.5174 × 10 -5</cell><cell>0.0096</cell><cell>3</cell></row><row><cell>Task 2 (Soft)</cell><cell>0.0006</cell><cell>6.2090 × 10 -5</cell><cell>2</cell></row><row><cell>Task 3 (Soft)</cell><cell>1.1213 × 10 -6</cell><cell>0.0005</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,88.99,90.49,391.40,483.93"><head>Table 5</head><label>5</label><figDesc>Results for Task 1</figDesc><table coords="10,283.52,115.90,28.23,8.92"><row><cell>Task 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,88.99,90.49,380.48,232.03"><head>Table 7</head><label>7</label><figDesc>Results for Task 3</figDesc><table coords="11,125.80,115.90,343.67,206.62"><row><cell></cell><cell></cell><cell>Task 3</cell><cell></cell><cell></cell></row><row><cell>soft-soft all</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Rank ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell>-</cell></row><row><cell>EXIST2023_test_gold_soft</cell><cell>0</cell><cell>9.4686</cell><cell>1</cell><cell>-</cell></row><row><cell>AI-UPV_3</cell><cell>1</cell><cell>-2.3183</cell><cell>0.7879</cell><cell>-</cell></row><row><cell>roh-neil_1</cell><cell>7</cell><cell>-6.6622</cell><cell>0.7098</cell><cell>-</cell></row><row><cell>hard-hard all</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="4">Rank ICM-Hard ICM-Hard Norm F1 Score</cell></row><row><cell>EXIST2023_test_gold_hard</cell><cell>0</cell><cell>2.1533</cell><cell>1</cell><cell>1</cell></row><row><cell>roh-neil_1</cell><cell>1</cell><cell>0.4433</cell><cell>0.6763</cell><cell>0.6296</cell></row><row><cell>hard-soft all</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Rank ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell>-</cell></row><row><cell>EXIST2023_test_gold_soft</cell><cell>0</cell><cell>9.4686</cell><cell>1</cell><cell>-</cell></row><row><cell cols="2">EXIST2023_oracle_most_voted 1</cell><cell>-8.3816</cell><cell>0.6788</cell><cell>-</cell></row><row><cell cols="2">EXIST2023_test_majority_class 2</cell><cell>-8.7089</cell><cell>0.6729</cell><cell>-</cell></row><row><cell>Mario_1</cell><cell>3</cell><cell>-9.1398</cell><cell>0.6652</cell><cell>-</cell></row><row><cell>roh-neil_1</cell><cell>13</cell><cell>-12.195</cell><cell>0.6102</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to extend our sincere gratitude to the <rs type="institution">University of Zurich, Department of Computational Linguistics</rs>, for their invaluable support. Our sincere thanks go to <rs type="person">Mr. Andrianos Michail</rs> and <rs type="person">Dr. Simon Clematide</rs> for their invaluable guidance and assistance.</p><p>Additionally, we would like to express our deep appreciation to the organizers of EXIST2023 for providing us with the opportunity to participate in this esteemed event. We are truly grateful for the chance to present our work and contribute to the progress of our field.</p><p>We would like to once again acknowledge and appreciate the support and encouragement received from all individuals involved, as their contributions have been instrumental in our accomplishments.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The source code and the final submission files can be accessed through the following official GitHub repository for EXIST2023:</p><p>• GitHub for EXIST2023 roh-neil ': 0.0, 'NO': 0.0 DIRECT @Dan_Brocklebank Wonder if he was able to make the time to get that slapper round the back of the building and give her a ride ?? @MattHancock" 'OBJECTIFICATION': 0.5, 'NO': 0.17, 'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.17, 'STEREOTYPING-DOMINANCE': 0.17   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Information and Illustrations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,601.84,393.33,10.91;12,112.66,615.39,394.53,10.91;12,112.28,628.93,225.40,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,352.43,601.84,153.56,10.91;12,112.66,615.39,146.14,10.91">Word embeddings, bias in ai, and semantically unfair comparisons</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,285.23,615.39,221.96,10.91;12,112.28,628.93,147.40,10.91">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,642.48,393.33,10.91;12,112.66,656.03,393.33,10.91;12,112.66,669.58,196.07,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,213.64,642.48,292.35,10.91;12,112.66,656.03,52.08,10.91">Recurrent neural network models for sexist content detection in social media</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,188.09,656.03,317.90,10.91;12,112.66,669.58,102.44,10.91">2018 9th International Conference on Information, Intelligence, Systems and Applications (IISA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,394.53,10.91;13,112.66,100.52,245.78,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,177.50,100.52,107.76,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,114.06,125.50,10.91;13,263.67,114.06,244.00,10.91;13,112.66,130.06,97.35,7.90" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m" coord="13,263.67,114.06,205.94,10.91">Cross-lingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,141.16,394.53,10.91;13,112.66,154.71,393.33,10.91;13,112.66,168.26,393.32,10.91;13,112.66,181.81,394.62,10.91;13,112.66,195.36,386.47,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,271.58,154.71,234.41,10.91;13,112.66,168.26,20.10,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.747.doi:10.18653/v1/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m" coord="13,155.39,168.26,350.59,10.91;13,112.66,181.81,238.14,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,393.33,10.91;13,112.66,222.46,393.53,10.91;13,112.66,236.01,395.00,10.91;13,112.41,249.56,397.73,10.91;13,112.66,265.55,74.11,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,273.80,208.91,232.18,10.91;13,112.66,222.46,68.18,10.91">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.421.doi:10.18653/v1/2020.acl-main.421" />
	</analytic>
	<monogr>
		<title level="m" coord="13,211.27,222.46,294.92,10.91;13,112.66,236.01,312.93,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,276.66,393.33,10.91;13,112.66,290.20,393.33,10.91;13,112.66,303.75,142.24,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,323.75,276.66,182.23,10.91;13,112.66,290.20,189.63,10.91">Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,329.22,290.20,176.77,10.91;13,112.66,303.75,112.25,10.91">International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,317.30,393.72,10.91;13,112.66,330.85,393.32,10.91;13,112.33,344.40,393.65,10.91;13,112.66,357.95,328.66,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,463.64,317.30,42.74,10.91;13,112.66,330.85,393.32,10.91;13,112.33,344.40,87.11,10.91">Overview of exist 2023 -learning with disagreement for sexism identification and characterization (extended overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,467.49,344.40,38.49,10.91;13,112.66,357.95,297.96,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,393.33,10.91;13,112.66,385.05,393.33,10.91;13,112.66,398.60,231.35,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,274.20,371.50,231.79,10.91;13,112.66,385.05,26.41,10.91">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,161.11,385.05,344.88,10.91;13,112.66,398.60,200.50,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,412.15,393.33,10.91;13,112.66,425.70,395.01,10.91;13,112.66,439.25,168.74,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,286.89,412.15,219.10,10.91;13,112.66,425.70,261.43,10.91">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pengcheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Weizhu</surname></persName>
		</author>
		<idno>CoRR abs/2111.09543</idno>
		<ptr target="https://arxiv.org/abs/2111.09543" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,452.79,394.53,10.91;13,112.66,466.34,393.33,10.91;13,112.66,479.89,256.19,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,112.66,466.34,314.90,10.91">Bertin: A spanish bert-based model for natural language processing tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L F</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A A</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M T</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C G</forename><surname>Ramírez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,449.04,466.34,56.95,10.91;13,112.66,479.89,168.44,10.91">International Conference on Computational Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,493.44,394.61,10.91;13,112.33,506.99,393.65,10.91;13,112.66,520.54,215.85,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,413.48,493.44,68.29,10.91;13,112.33,506.99,341.13,10.91">Transfer learning and ensemble techniques for hate speech detection in spanish</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">A</forename><surname>Gutiérrez-Fandiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,475.10,506.99,30.89,10.91;13,112.66,520.54,127.27,10.91">Iberian Languages Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
	<note>Maria at iberlef</note>
</biblStruct>

<biblStruct coords="13,112.66,534.09,394.61,10.91;13,112.66,547.64,234.21,10.91" xml:id="b12">
	<monogr>
		<ptr target="http://nlp.uned.es/exist2023/" />
		<title level="m" coord="13,364.03,534.09,48.88,10.91">EXIST 2023</title>
		<meeting><address><addrLine>Website</addrLine></address></meeting>
		<imprint>
			<publisher>Universidad Nacional de Educación a Distancia (UNED)</publisher>
			<date type="published" when="2023-07-10">2023. July 10, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,561.19,394.53,10.91;13,112.66,574.74,393.33,10.91;13,112.66,588.29,395.00,10.91;13,112.66,604.28,97.35,7.90" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,264.51,574.74,241.47,10.91;13,112.66,588.29,89.55,10.91">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1910.03771" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,615.39,395.17,10.91;13,112.66,628.93,260.32,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10902</idno>
		<title level="m" coord="13,329.78,615.39,178.05,10.91;13,112.66,628.93,128.32,10.91">Optuna: A next-generation hyperparameter optimization framework</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,656.03,393.33,10.91;13,112.66,669.58,393.33,10.91;14,112.66,86.97,393.32,10.91;14,112.66,100.52,393.33,10.91;14,112.66,114.06,394.03,10.91;14,112.66,127.61,185.51,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,323.15,656.03,182.83,10.91;13,112.66,669.58,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="13,327.87,669.58,178.11,10.91;14,112.66,86.97,393.32,10.91;14,112.66,100.52,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="14,112.66,141.16,393.32,10.91;14,112.66,154.71,266.40,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,395.98,141.16,110.00,10.91;14,112.66,154.71,115.82,10.91">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,251.30,154.71,97.80,10.91">PML4DC at ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,168.26,395.17,10.91;14,112.66,181.81,395.01,10.91;14,112.66,197.80,97.35,7.90" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="14,211.24,181.81,263.05,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,393.33,10.91;14,112.66,222.46,395.00,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,277.48,208.91,228.51,10.91;14,112.66,222.46,33.42,10.91">Robertuito: A spanish roberta model for sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Alayón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Salas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,168.58,222.46,252.61,10.91">International Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="424" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,236.01,394.53,10.91;14,112.66,249.56,395.01,10.91;14,112.66,263.11,61.59,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="14,112.66,249.56,291.01,10.91">Overview of exist 2021: sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Comet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Donoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Proces. del Leng. Natural</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,395.17,10.91;14,112.66,290.20,394.61,10.91;14,112.66,303.75,393.98,10.91;14,112.66,317.30,38.81,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,401.23,290.20,106.04,10.91;14,112.66,303.75,177.17,10.91">Overview of exist 2022: sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendieta-Aragón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marco-Remón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Makeienko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,298.87,303.75,164.56,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,330.85,395.17,10.91;14,112.39,344.40,395.45,10.91;14,112.66,357.95,393.32,10.91;14,112.66,371.50,394.53,10.91;14,112.66,385.05,393.33,10.91;14,112.66,398.60,183.50,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Á</forename><surname>Álvarez-Carmona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Álvarez Mellado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Gómez</forename><surname>Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2943/" />
		<title level="m" coord="14,452.18,357.95,53.80,10.91;14,112.66,371.50,254.18,10.91;14,391.16,385.05,114.83,10.91;14,112.66,398.60,178.94,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021)</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gutiérrez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jiménez Zafra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lima-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Plaza-De Arco</surname></persName>
		</editor>
		<editor>
			<persName><surname>Taulé</surname></persName>
		</editor>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2021)<address><addrLine>MÁlaga, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Conference of the Spanish Society for Natural Language Processing</note>
</biblStruct>

<biblStruct coords="14,112.66,412.15,395.17,10.91;14,112.66,425.70,394.52,10.91;14,112.66,439.25,393.33,10.91;14,112.66,452.79,394.62,10.91;14,112.66,466.34,393.61,10.91;14,112.66,479.89,148.34,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Casavantes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Á</forename><surname>Álvarez-Carmona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bel-Enguix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3202/,xXXVIII" />
		<title level="m" coord="14,390.72,439.25,115.27,10.91;14,112.66,452.79,194.35,10.91;14,352.39,466.34,153.88,10.91;14,112.66,479.89,143.78,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Escalada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rodríguez-Sánchez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rosá</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sobrevilla-Cabezudo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Taulé</surname></persName>
		</editor>
		<editor>
			<persName><surname>Valencia-García</surname></persName>
		</editor>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2022)<address><addrLine>A Coruña, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Conference of the Spanish Society for Natural Language Processing</note>
</biblStruct>

<biblStruct coords="14,112.66,493.44,296.52,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://github.com/ssut/py-googletrans" />
		<title level="m" coord="14,147.07,493.44,52.08,10.91">Googletrans</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,506.99,32.56,10.91;14,197.74,506.99,117.57,10.91;14,367.83,506.99,138.87,10.91;14,112.40,520.54,142.18,10.91" xml:id="b24">
	<monogr>
		<ptr target="https://huggingface.co/sdadas/xlm-roberta-large-twitter" />
		<title level="m" coord="14,112.66,506.99,32.56,10.91;14,197.74,506.99,113.05,10.91">sdadas, xlm-roberta-large-twitter</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,534.09,394.61,10.91;14,112.31,547.64,309.73,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,193.91,534.09,182.57,10.91">crossentropyloss, PyTorch Documentation</title>
		<author>
			<persName coords=""><forename type="first">Torch</forename><surname>Pytorch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nn</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" />
		<imprint>
			<date type="published" when="2023-07-06">July 6, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,561.19,394.62,10.91;14,112.66,574.74,393.32,10.91;14,112.33,588.29,394.86,10.91;14,112.66,601.84,397.48,10.91;14,112.66,617.83,74.11,7.90" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,223.33,561.19,260.58,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.399</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.399.doi:10.18653/v1/2022.acl-long.399" />
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,574.74,393.32,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
