<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.40,84.74,386.57,15.42;1,89.29,106.66,392.32,15.42;1,89.29,128.58,339.11,15.43;1,89.29,150.49,375.50,15.43">When Multiple Perspectives and an Optimization Process Lead to Better Performance, an Automatic Sexism Identification on Social Media With Pretrained Transformers in a Soft Label Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,178.81,61.51,11.96"><forename type="first">Johan</forename><surname>Erbani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">UCBL</orgName>
								<orgName type="institution" key="instit5">LIRIS</orgName>
								<address>
									<addrLine>UMR5205, 20 Avenue Einstein</addrLine>
									<postCode>69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.63,178.81,105.55,11.96"><forename type="first">El≈ëd</forename><surname>Egyed-Zsigmond</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">UCBL</orgName>
								<orgName type="institution" key="instit5">LIRIS</orgName>
								<address>
									<addrLine>UMR5205, 20 Avenue Einstein</addrLine>
									<postCode>69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.92,178.81,85.99,11.96"><forename type="first">Diana</forename><surname>Nurbakova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">UCBL</orgName>
								<orgName type="institution" key="instit5">LIRIS</orgName>
								<address>
									<addrLine>UMR5205, 20 Avenue Einstein</addrLine>
									<postCode>69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.38,178.81,110.92,11.96"><forename type="first">Pierre-Edouard</forename><surname>Portier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">UCBL</orgName>
								<orgName type="institution" key="instit5">LIRIS</orgName>
								<address>
									<addrLine>UMR5205, 20 Avenue Einstein</addrLine>
									<postCode>69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.40,84.74,386.57,15.42;1,89.29,106.66,392.32,15.42;1,89.29,128.58,339.11,15.43;1,89.29,150.49,375.50,15.43">When Multiple Perspectives and an Optimization Process Lead to Better Performance, an Automatic Sexism Identification on Social Media With Pretrained Transformers in a Soft Label Context</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">2DA2A5325E7CECC181BE66F98EAC5487</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sexism Identification</term>
					<term>Natural Language Processing</term>
					<term>Transformer Models</term>
					<term>BERT</term>
					<term>Ensemble modeling</term>
					<term>Sentiment Analysis</term>
					<term>Sexism Detection</term>
					<term>Twitter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Even if today, the sexism is socially widely disapproving, it remains an omnipresent phenomenon in our society. But faced with huge quantities of data, social platforms are struggling to identify it. This highlights the need to develop automatic detection tools that can subtly assess the sexistness of usergenerated content. That's what sEXism Identification in Social neTworks (EXIST) is all about. The EXIST 2023 contest consists of three classification tasks : 1. detect sexism, 2. clarify the author's intention and 3. explicit the sexism type. Thanks to these three tasks, each data could be seen from three different points of view. This idea, combined with fine-tuned BERTs, model stacking and an optimization process, enabled us to rank 1 st in the task 2 and 4 th in the task 3 in a soft label context. This paper describes our approach, our negative results and some possible perspectives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Identifying sexism automatically remains an open problem in Natural Language Processing (NLP). To address this issue, a series of scientific events called EXIST has been established with the objective of comprehending sexism in its widest scope. This includes not only explicit sexism but also more subtle forms of implicit sexist behavior. These scientific initiatives have the potential to raise awareness about women's rights issues and promote social cohesion. This paper describes the DRIM team's contribution to the three EXIST 2023 shared tasks.</p><p>This work is structured as follows: Section 2 briefly provides a description of several earlier studies. Section 3 will then present an explanation of tasks 1, 2 and 3, along with the corpus provided by the organizers. Following that, Section 4 and 5 will outline the experimental methodology and evaluation results respectively. Finally, in Section 6, we will present the key findings and conclusions of our studies, as well as some potential directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works and Contributions</head><p>According to previous EXIST rapports, transformer-based models performed better than the other technique <ref type="bibr" coords="2,163.82,124.83,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,178.04,124.83,7.65,10.91" target="#b1">2]</ref>. Given this and the limited availability of labeled data, we employed a fine-tuning approach using models that were initially pre-trained in a self-supervised manner using extensive amounts of unlabeled data. More specifically, we have chosen to work with BERT which is commonly used in state-of-the-art approaches across various NLP problems.</p><p>As explain in <ref type="bibr" coords="2,162.27,179.03,11.54,10.91" target="#b2">[3]</ref>, previous studies <ref type="bibr" coords="2,256.32,179.03,11.45,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,270.49,179.03,7.51,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,280.71,179.03,9.01,10.91" target="#b2">3]</ref> have highlighted the issue of high performance dependency on the seed value when fine-tuning BERT for downstream tasks, particularly when the training data is limited. One way of reducing this undesirable effect is to use ensembles of models as in <ref type="bibr" coords="2,159.93,219.67,11.47,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,174.12,219.67,7.65,10.91" target="#b5">6]</ref>. Ensemble Modeling is an approach that encompasses the combination of multiple models. It is based on the premise that individual models may possess distinct strengths and limitations, and by merging them, an improved overall performance can be achieved. The key benefit of employing model ensembles lies in their capacity to diminish variance and enhance predictive accuracy.</p><p>The paper <ref type="bibr" coords="2,150.17,287.42,13.00,10.91" target="#b6">[7]</ref> used a Multi-Task Learning approach to solve a previous EXIST challenge edition. Multi-Task Learning is a machine learning method where a model is trained to perform several different tasks simultaneously. Instead of training separate models for each task, multitask learning aims to share the knowledge and representations learned between the different tasks, which can potentially improve the overall performance of the model.</p><p>Our contributions relate to the development of a strategy that combines elements from both Ensemble Modeling and Multi-Task Learning methods. Our model is built upon three stacked BERT. Unlike previous approaches, our proposal focuses on observing the same object from multiple perspectives. The underlying idea was to provide the model with input various aspects to enhance its comprehension and interpretation abilities. Experimental results show that our strategy could outperform single-view models. Furthermore, we propose incorporating a prediction refinement mechanism on top of our models through an optimization process. This refinement process does not alter the model's weights but it has enabled us to outperform other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and Tasks</head><p>In 2023, the EXIST event involved the categorization of several thousands tweets written in English and Spanish. This categorization process encompassed three distinct tasks: detect sexism, clarify the author's intention and explicit the sexism type. Approximately, whatever the tasks, the Non-sexist class accounting for half the total annotator votes. The other classes are evenly distributed among the remaining votes. The data, the tasks and the classes are summarized in the tables 1, 2 and 3, respectively (see <ref type="bibr" coords="2,335.63,589.93,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,350.25,589.93,9.03,10.91" target="#b9">9]</ref> for more details). Each tweet is annotated by 6 annotators of different ages and genders, with the aim of 1. obtaining less biased labels and 2. learn more subtly to recognize the different categories. Indeed, the assumption that natural language expressions have a single and clearly identifiable interpretation in a given context is a convenient idealization, but it's far from reality, as explained in <ref type="bibr" coords="2,424.97,644.13,16.19,10.91" target="#b10">[10]</ref>. To cope with this, EXIST 2023 proposes to learn directly from different annotators' votes. The organisers leave participants free to choose the type of labels predicted:</p><p>‚Ä¢ Soft-Soft. Provide the vote distribution;</p><p>‚Ä¢ Hard-Soft. Provide the vote distribution and a hard-label;</p><p>‚Ä¢ Hard-Hard. Provide only the hard-label. We participated in all tasks in the Soft-Soft context.</p><p>For all tasks, whatever the context, the evaluation metric was the Information Contrast Measure (IMC) <ref type="bibr" coords="3,159.14,420.73,16.32,10.91" target="#b11">[11]</ref>. It is a similarity function that generalizes Pointwise Mutual Information (PMI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we describe our approach and the experimental framework employed. We present here only the attempts that we consider scientifically interesting, the successful ones as well as the unsuccessful ones, in order to maximize the usefulness of the paper for the reader.</p><p>We began by separating the dataset according to the language of the tweets. All the exploratory work was carried out on the English data. Once the best method had been determined, we replicated it on the Spanish data with BERT multilingual. In the following, as the procedures are identical, we will only describe the process of the English data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline</head><p>In order to compare our different attempts, we began by implementing a BERT base uncased baseline using a traditional approach. The architecture used is illustrated in Figure <ref type="figure" coords="3,475.54,637.48,3.81,10.91" target="#fig_0">1</ref>. The process involves taking the BERT classifier token [CLS], applying dropout, passing it through a dense layer, and finally using a softmax activation function. While using softmax for tasks 1 and   2 is appropriate because they are single-label tasks, it may not be suitable for task 3, which is a multi-label task. However, in the specific context of the competition, as explained in section 4.6, this issue is not problematic. Although the influence of the hyperparameter dropout is minor, we empirically determined that among the {0.1, 0.3, 0.5} values the optimal rates were 0.5, 0.1 and 0.5 for task 1, 2 and 3 respectively. In accordance with the original BERT paper <ref type="bibr" coords="4,468.59,660.74,18.02,10.91" target="#b12">[12]</ref> and with our experiments, we have chosen a batch size of 32. In order to increase the generalization capacity of the model, we preferred to optimize with AdamW rather than Adam as proposed in <ref type="bibr" coords="5,89.29,114.06,17.77,10.91" target="#b13">[13]</ref> with learning rate ùëôùëü = 1ùëí -5. We use the cross-entropy loss as cost function. Training of the model was stopped at the peak of the ICM on the development set, which turned out to be 4, 7 and 9 epochs for tasks 1, 2 and 3 respectively. For more stability in the initial phase of training, we applied linear learning rate warm-up during the first steps of the updates followed by a linear decay. However, we did not observe any positive effect. We also tried to apply the layer-wise learning rate decay technique. As explained in <ref type="bibr" coords="5,147.99,195.36,16.55,10.91" target="#b14">[14,</ref><ref type="bibr" coords="5,167.43,195.36,14.11,10.91" target="#b15">15]</ref> lower layers encode more general information and top layers are more specific to the training task. Consequently, the higher a layer is, the larger its learning rate should be and conversely, the lower it is, the smaller it should be. But again, even if the learning was faster during the first epochs at the end, we have not seen any improvement. Consequently, we did not retain these two strategies. However, a more detailed studies of hyperparameters might have led to further gains.</p><p>In the following, we will refer to ‚Ñ¨1, ‚Ñ¨2 and ‚Ñ¨3 as the three BERTs fine-tuned according to the protocol described above on tasks 1, 2 and 3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preprocessing</head><p>We tried different preprocessing listed in the Table <ref type="table" coords="5,319.00,339.93,3.77,10.91" target="#tab_2">4</ref>. When process involves new tokens, we update the BERT tokenizer to include new tokens, enabling the model to learn from them. In our study, we found that pre-processing had a minor influence on model performance. The best results were obtained using the first two approaches. Note that giving the meaning of hashtags and emojis as we did not seem to provide more information on average. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Different data perspectives</head><p>An intuitive idea illustrated in the Figure <ref type="figure" coords="5,270.95,600.14,4.98,10.91" target="#fig_1">2</ref> is that multiple perspectives of an object can lead to a better understanding of it. We combined the models introduced in section 4.1 ‚Ñ¨1, ‚Ñ¨2, and ‚Ñ¨3, to create a meta-model. ‚Ñ¨1, ‚Ñ¨2, and ‚Ñ¨3 were trained on the same data but on different tasks 1, 2 and 3 respectively. After training the baseline models, we froze them to prevent further training and then stacked them together. On top of the stacked models, we added some additional layers. The resulting architecture is illustrated in Figure <ref type="figure" coords="5,309.53,667.89,3.76,10.91" target="#fig_2">3</ref>. This strategy worked well for tasks 1 and The optimal hidden size dimension was empirically determined as 12 i.e. the total number of categories all tasks combined. The output size is the number categories of the task ùëñ.</p><p>2, but it did not work for task 3. Additionally, the decision of whether to keep softmax on the top of the frozen networks depended on the task. It was better to keep softmax for task 1 and better to exclude it for task 2. In the following, we will refer to ‚Ñ≥1 and ‚Ñ≥2 to designate the two meta-models described above. Based on our submission, it's difficult to pinpoint the exact reasons why the strategy worked for some tasks but not for others. However we assume that task 3 is the most difficult one and requires subtleties that are not adequately captured by the baseline models ‚Ñ¨1 and ‚Ñ¨2. A supporting evidence is that this strategy exhibited the highest effectiveness on task 1, which is considered the least complex among the tasks. Consequently, ‚Ñ¨2 and ‚Ñ¨3 could presumably transfer their captured nuances to ‚Ñ¨1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Manual features</head><p>In our effort to enhance the performance of our models, we sought out manual features that could uncover valuable information potentially overlooked by our existing models. These manual features were subjected to normalization before being stacked with the frozen networks as shown in Figure <ref type="figure" coords="6,176.26,593.56,3.74,10.91" target="#fig_3">4</ref>. Here is a list of the specific additional features we incorporated:</p><p>‚Ä¢ Tokens. Give the number of &lt; hashtag &gt;, &lt; mention &gt; and &lt; url &gt;;</p><p>‚Ä¢ Text statistics. Give the number of characters, upper characters, words, sentences, digits, citations, question marks and exclamation marks;</p><p>‚Ä¢ Sentiment analysis. With the python library TextBlob, we provide the subjectivity and the polarity;</p><p>‚Ä¢ Part-of-speech tagging. Give the number of adjectives, noun, verb, adverbs...</p><p>‚Ä¢ Tenses and modal. Give the number of verbs conjugated in the future, present or past tense and the number of modal verbs;</p><p>‚Ä¢ Complexity. Provide the average word size, the average sentence size and a readability index (Flesch Reading Ease);</p><p>‚Ä¢ Latent Dirichlet Allocation (LDA). We group test and train sets in a big corpus to identify main topics by the LDA process. Regrettably, utilizing our current methodology, none of the manually engineered features demonstrated a discernible enhancement in the model's performance across various tasks. It is postulated that these features encompass information that has already been assimilated by our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Data augmentation</head><p>Three distinct approaches were implemented and evaluated. Unfortunately, none of them yielded desirable outcomes.</p><p>The first approach involved translating the Spanish data and augmenting the model's training set. Against intuition, this augmentation did not lead to performance improvements; instead, it resulted in decreased performance.</p><p>The second attempt aimed to test the assumption that the translated data was noisy. To counterbalance this noise, we have tried to select only tweets with a significant informational content. The methodology entailed training the model on the English data, subsequently testing it on the translated data, and identifying the instances where the model exhibited significant errors. These error-prone tweets were presumed to possess heavy information for the model's learning. Subsequently, the model's weights were reset, and the training was repeated on the base data alongside the translated data with the "most significant" information content. Unfortunately, this strategy failed to improve the model's performance.</p><p>Lastly, a third approach was undertaken by applying a similar strategy as described in section 4.3, but with other datasets. Two distinct datasets were utilized: the Spanish data translated from EXIST 2023 and the EXIST 2022 dataset. The methodology involved stacking ‚Ñ¨1, ‚Ñ¨2, and ‚Ñ¨3 with additional frozen baseline models trained on specific tasks, such as task 1 of 2022 or the translated Spanish data of task 3. The underlying principle was to harness the collective insights of diverse people, represented by different models trained on distinct data types, in order to synthesize their outputs and generate an improved solution. Regrettably, this method has not produced convincing results.</p><p>These unsuccessful results led us to make two hypotheses. Firstly, we posited that tweets exhibit significant dissimilarities based on their cultural or temporal origins. Secondly, we hypothesized that the cultural disparity between Spanish and English annotators could lead to different evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Best possible distribution</head><p>The dataset construction results in non-continuous labels, despite their apparent continuity. This is due to the presence of six annotators for each task, which limits the set of possible label distributions to a finite number. Specifically, all label values are multiples of 1/6. For instance, in task 1, there exist only seven feasible outputs denoted as ùëÇ = (ùëú 1 , ùëú 2 ), where ùëú 1 = ùëñ/6 for ùëñ = 0, 1, ... , 6, and ùëú 2 = 1 -ùëú 1 . Similarly, task 2 encompasses 84 possibilities, while task 3 encompasses a substantial number of 28 546 possibilities due to its multi-label nature.</p><p>Exploiting this knowledge, after training the model, it becomes feasible to select, from the set of possible distributions, the closest one for each prediction. In particular, the multi-label task 3 could be solved with a softmax on the top of our model. The advantage is that the model benefits from the gradient backpropagation offered by the softmax function during its learning and has a good approximation to a label that does not sum to 1 thanks to the optimization trick. It is important to note that the process described is an optimization problem and not a deep-learning problem. Consequently, this optimization procedure does not impact the model during the training. This sneaky idea has considerably increased the model's performance. This optimization procedure will be noted ùí™ in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we provide a concise overview of the performance attained by our models configurations in the EXIST 2023 evaluation. Our rankings are indicated in the Table <ref type="table" coords="8,478.02,655.99,3.81,10.91" target="#tab_3">5</ref>. It is noteworthy to point out that our current performance has improved slightly on that obtained during the competition. This is due to the extra time we had to refine the hyperparameters of our models.</p><p>We can see in the Table <ref type="table" coords="9,206.89,114.06,5.02,10.91" target="#tab_4">6</ref> that the optimization process significantly improves model performance. Our meta-model strategy was also effective, but to a lesser extent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The submission made to the EXIST 2023 evaluation has yielded a considerable number of negative or neutral results. These outcomes are valuable as they indicate that specific approaches or hypotheses within our particular context did not produce favorable results. However, we have put forth two effective strategies. Firstly, we proposed an innovative approach that combines ensemble modeling and the multi-learning model. This approach entails training the same architecture on multiple tasks using the same data and subsequently stacking the frozen sub-models in a meta-model. Our findings demonstrate that this architecture has the ability to surpass the limitations of single view models, leading to improved performance. Furthermore, we suggested incorporating an optimization process into our model. Our ranking highlights that in a competition it is more important to be close to labels rather the ground truth. This optimization process plays a key role in refining the model's predictions and enhancing its overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,91.38,510.06,266.47,9.96;4,91.38,522.01,264.98,9.96;4,91.38,533.97,279.55,9.96;4,91.38,545.92,271.13,9.96;4,91.38,557.88,51.13,9.96;4,177.68,404.31,96.41,99.21"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture ‚Ñ¨ùëñ for task ùëñ where ùëñ could be 1, 2 or 3. At the bottom, BERT base uncased from which we get the classifier, followed by dropout and linear layer. The output size matching with the number of categories of the task ùëñ. The top activation function is a softmax.</figDesc><graphic coords="4,177.68,404.31,96.41,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,383.06,510.61,114.07,9.96;4,383.06,522.57,100.61,9.96;4,383.06,535.41,121.62,8.96;4,383.06,547.37,35.77,8.96;4,383.06,404.86,123.23,99.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the idea When multiple perspectives lead to a better understanding.</figDesc><graphic coords="4,383.06,404.86,123.23,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,306.89,416.69,9.96;6,89.29,318.84,416.69,9.96;6,89.29,330.80,416.70,9.96;6,89.29,342.75,345.81,9.96;6,130.96,84.19,333.36,216.16"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Architecture meta-model ‚Ñ≥ùëñ for task ùëñ where ùëñ could be 1, 2 or 3. In yellow, ‚Ñ¨1, in red ‚Ñ¨2 and in purple ‚Ñ¨3. On top, three linear layers with respectively a ReLU, a ReLU and a softmax as activation function. The optimal hidden size dimension was empirically determined as 12 i.e. the total number of categories all tasks combined. The output size is the number categories of the task ùëñ.</figDesc><graphic coords="6,130.96,84.19,333.36,216.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,306.89,416.69,9.96;7,89.29,318.84,416.69,9.96;7,88.79,331.47,141.87,9.18;7,130.96,84.19,333.36,216.16"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Modified architecture of ‚Ñ≥ùëñ for task ùëñ where ùëñ could be 1, 2 or 3. Except for the inclusion of manual features, the only difference with the Mi architecture is the hidden size, which here is equal to 12+the number of manual features.</figDesc><graphic coords="7,130.96,84.19,333.36,216.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.98,90.49,338.59,193.37"><head>Table 1</head><label>1</label><figDesc>An overview of EXIST 2023 data for both languages and both sets.</figDesc><table coords="3,88.98,117.64,338.59,166.22"><row><cell></cell><cell>Language</cell><cell cols="2">Data set</cell><cell>Number of tweets</cell></row><row><cell></cell><cell>English</cell><cell>Train</cell><cell></cell><cell>3260</cell></row><row><cell></cell><cell></cell><cell cols="2">Development</cell><cell>489</cell></row><row><cell></cell><cell>Spanish</cell><cell>Train</cell><cell></cell><cell>3660</cell></row><row><cell></cell><cell></cell><cell cols="2">Development</cell><cell>549</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">An overview of EXIST 2023 tasks.</cell><cell></cell><cell></cell></row><row><cell cols="3">Task n¬∞Question</cell><cell cols="2">Number of classes</cell><cell>Type</cell></row><row><cell>1</cell><cell cols="2">Sexist or not</cell><cell></cell><cell>2</cell><cell>Mono label</cell></row><row><cell>2</cell><cell cols="2">Author's intention</cell><cell></cell><cell>4</cell><cell>Mono label</cell></row><row><cell>3</cell><cell cols="2">Sexism type</cell><cell></cell><cell>6</cell><cell>Multi-label</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.98,90.49,410.63,286.26"><head>Table 3</head><label>3</label><figDesc>An overview of the class statistics for both the training and development datasets at EXIST 2023.</figDesc><table coords="4,95.67,119.31,403.94,178.53"><row><cell cols="2">Task n¬∞Class</cell><cell>Definition</cell><cell cols="2">Train Set Dev. Set</cell></row><row><cell>1</cell><cell>Non-sexist</cell><cell>Not sexist</cell><cell>55%</cell><cell>52%</cell></row><row><cell></cell><cell>Sexist</cell><cell>Sexist or about sexism (e.g. if the author de-</cell><cell>45%</cell><cell>48%</cell></row><row><cell></cell><cell></cell><cell>nounces a sexist act or fact)</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Non-sexist</cell><cell>Not sexist</cell><cell>55%</cell><cell>52%</cell></row><row><cell></cell><cell>Direct</cell><cell>Sexist by itself</cell><cell>22%</cell><cell>22%</cell></row><row><cell></cell><cell>Reported</cell><cell>Report a sexist situation</cell><cell>11%</cell><cell>12%</cell></row><row><cell></cell><cell>Judgemental</cell><cell>Decrying a social injustice against women</cell><cell>12%</cell><cell>14%</cell></row><row><cell>3</cell><cell>Non-sexist</cell><cell>Not sexist</cell><cell>47%</cell><cell>43%</cell></row><row><cell></cell><cell>Ideological</cell><cell>Discredits the feminist movement, rejects in-</cell><cell>12%</cell><cell>13%</cell></row><row><cell></cell><cell>Inequality</cell><cell>equality between men and women</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Stereotyping</cell><cell>Promotes gender stereotypes, superiority of</cell><cell>14%</cell><cell>15%</cell></row><row><cell></cell><cell>Dominance</cell><cell>men, and limits women's abilities</cell><cell></cell><cell></cell></row></table><note coords="4,137.18,302.23,61.06,9.96;4,214.73,302.23,189.69,9.96;4,214.73,314.18,139.27,9.96;4,426.10,302.89,18.26,8.74;4,473.69,302.89,18.26,8.74;4,137.18,328.53,267.24,9.96;4,214.73,340.48,134.11,9.96;4,426.10,329.19,18.26,8.74;4,476.18,329.19,13.28,8.74;4,137.18,354.83,66.86,9.96;4,137.18,366.78,64.20,9.96;4,214.73,354.83,189.70,9.96;4,426.10,355.50,18.26,8.74;4,473.69,355.50,18.26,8.74"><p><p><p><p><p><p>Objectification</p>Women are presented as objects apart from their dignity and personal aspects 11% 11%</p>Sexual Violence Sexual suggestions, requests for sexual favors or harassment of a sexual nature</p>07% 8%</p>Misogyny Non-Sexual Violence</p>Expresses hatred and violence towards women 09% 10%</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,423.14,410.62,110.92"><head>Table 4</head><label>4</label><figDesc>Exploration of the various types of preprocessing techniques.</figDesc><table coords="5,95.67,451.96,375.18,82.09"><row><cell cols="2">Idx Description</cell><cell>Original tweet</cell><cell>Pre-processed tweet</cell></row><row><cell>1</cell><cell>Tokens hashtags,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>mentions, urls</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Standardize text</cell><cell>I hate U &amp; f*ck you !!</cell><cell>I hate you and fuck you !!</cell></row><row><cell>3</cell><cell>Explicit hashtag</cell><cell>#Metoo</cell><cell>&lt; hashtag &gt; me too</cell></row><row><cell>4</cell><cell>Explicit emoji</cell><cell>I love U :)</cell><cell>I love U &lt; emoji &gt; happy</cell></row></table><note coords="5,203.71,469.10,295.90,9.96"><p>@Bob43 #Metoo see u http... &lt; mention &gt; &lt; hashtag &gt; see u &lt; url &gt;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.98,155.11,322.54,148.30"><head>Table 5</head><label>5</label><figDesc>An overview of the ranking achieved at EXIST 2023.</figDesc><table coords="9,181.26,182.27,230.26,121.15"><row><cell cols="5">Task n¬∞Language Ranking ICM Cross Entropy</cell></row><row><cell>1</cell><cell>Both</cell><cell>22/56</cell><cell>0.54</cell><cell>0.89</cell></row><row><cell></cell><cell>English</cell><cell>5/56</cell><cell>0.73</cell><cell>0.95</cell></row><row><cell></cell><cell>Spanish</cell><cell>25/56</cell><cell>0.34</cell><cell>0.84</cell></row><row><cell>2</cell><cell>Both</cell><cell>1/27</cell><cell>-1.34</cell><cell>1.78</cell></row><row><cell></cell><cell>English</cell><cell>1/27</cell><cell>-1.15</cell><cell>1.80</cell></row><row><cell></cell><cell>Spanish</cell><cell>4/27</cell><cell>-1.54</cell><cell>1.77</cell></row><row><cell>3</cell><cell>Both</cell><cell>4/25</cell><cell>-3.68</cell><cell>-</cell></row><row><cell></cell><cell>English</cell><cell>4/25</cell><cell>-3.08</cell><cell>-</cell></row><row><cell></cell><cell>Spanish</cell><cell>4/25</cell><cell>-4.16</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.98,344.05,417.00,146.08"><head>Table 6</head><label>6</label><figDesc>An analysis of the performance exhibited by diverse models employed during EXIST 2023 on English data.</figDesc><table coords="9,161.49,380.94,269.82,109.19"><row><cell cols="2">Task n¬∞Model</cell><cell cols="3">ICM ICM Gain Over ‚Ñ¨ùëñ Cross Entropy</cell></row><row><cell>1</cell><cell>‚Ñ¨1</cell><cell>0.84</cell><cell>-</cell><cell>0.51</cell></row><row><cell></cell><cell>‚Ñ≥1</cell><cell>1.01</cell><cell>+20%</cell><cell>0.52</cell></row><row><cell></cell><cell cols="2">‚Ñ≥1 + ùí™ 1.15</cell><cell>+36%</cell><cell>0.52</cell></row><row><cell>2</cell><cell>‚Ñ¨2</cell><cell>-0.93</cell><cell>-</cell><cell>0.97</cell></row><row><cell></cell><cell>‚Ñ≥2</cell><cell>-0.88</cell><cell>+5%</cell><cell>0.98</cell></row><row><cell></cell><cell cols="2">‚Ñ≥2 + ùí™ -0.52</cell><cell>+44%</cell><cell>0.98</cell></row><row><cell>3</cell><cell>‚Ñ¨3</cell><cell>-3.54</cell><cell>-</cell><cell>1.67</cell></row><row><cell></cell><cell>‚Ñ¨3 + ùí™</cell><cell>-2.84</cell><cell>+20%</cell><cell>1.67</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Johan Erbani</rs> would like to sincerely thank his superiors and the entire <rs type="institution">DRIM</rs> team for the trust they have placed in him. Special thanks are due to <rs type="person">Pierre-Yves Genest</rs> and <rs type="person">Ousmane Touat</rs> for their precious advice and expertise, and to interns <rs type="person">Maud Andruszak</rs> and <rs type="person">Thanh Lam</rs> for their diligent research efforts, who made some contributions through their work on manual features.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,285.51,258.57,10.91;10,417.08,285.51,90.11,10.91;10,112.33,299.06,393.65,10.91;10,112.66,312.61,177.07,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,161.57,299.06,272.28,10.91">Overview of exist 2021: sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodr√≠guez-S√°nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Comet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Donoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,441.75,299.06,64.23,10.91;10,112.66,312.61,93.13,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="195" to="207" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,326.16,395.17,10.91;10,112.66,339.71,394.61,10.91;10,112.66,353.26,393.98,10.91;10,112.66,366.81,38.81,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,401.23,339.71,106.04,10.91;10,112.66,353.26,177.17,10.91">Overview of exist 2022: sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodr√≠guez-S√°nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendieta-Arag√≥n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marco-Rem√≥n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Makeienko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,298.87,353.26,164.56,10.91">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,380.36,393.53,10.91;10,112.66,393.91,178.84,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villa-Cueva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sanchez-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>L√≥pez-Monroy</surname></persName>
		</author>
		<title level="m" coord="10,360.43,380.36,145.76,10.91;10,112.66,393.91,146.92,10.91">Bi-ensembles of transformer for online bilingual sexism detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,407.46,393.33,10.91;10,112.66,421.01,230.18,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<title level="m" coord="10,382.12,407.46,123.87,10.91;10,112.66,421.01,48.09,10.91">Revisiting few-sample bert fine-tuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,434.55,393.33,10.91;10,112.66,448.10,393.33,10.91;10,112.66,461.65,107.17,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<title level="m" coord="10,407.34,434.55,98.64,10.91;10,112.66,448.10,320.22,10.91">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,475.20,393.33,10.91;10,112.66,488.75,393.33,10.91;10,112.66,502.30,107.17,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F M</forename><surname>De Paula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Schlicht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04551</idno>
		<title level="m" coord="10,320.46,475.20,185.52,10.91;10,112.66,488.75,317.11,10.91">Sexism prediction in spanish and english tweets using monolingual and multilingual bert and ensemble models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,515.85,395.17,10.91;10,112.66,529.40,393.33,10.91;10,112.66,542.95,393.33,10.91;10,112.66,556.50,393.32,10.91;10,112.66,570.05,394.53,10.91;10,112.66,583.60,55.16,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,439.88,515.85,67.95,10.91;10,112.66,529.40,257.72,10.91">Sexism identification in social networks using a multi-task learning system</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del Arco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Molina-Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mart√≠n-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.50,529.40,113.48,10.91;10,112.66,542.95,393.33,10.91;10,112.66,556.50,393.32,10.91;10,112.66,570.05,236.24,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2021), XXXVII International Conference of the Spanish Society for Natural Language Processing</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2021), XXXVII International Conference of the Spanish Society for Natural Language Processing<address><addrLine>M√°laga, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2943. 2021</date>
			<biblScope unit="page" from="491" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,597.15,394.53,10.91;10,112.66,610.69,393.54,10.91;10,112.66,624.24,395.17,10.91;10,112.66,637.79,393.32,10.91;10,112.66,651.34,137.99,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,243.61,610.69,262.59,10.91;10,112.66,624.24,185.99,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amig√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.84,624.24,201.99,10.91;10,112.66,637.79,393.32,10.91;10,112.66,651.34,79.10,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<meeting>the Fourteenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,253.38,651.34,253.81,10.91;11,112.66,86.97,395.16,10.91;11,112.66,100.52,385.66,10.91" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-09">September 2023</date>
			<pubPlace>Thessaloniki, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,394.53,10.91;11,112.66,127.61,393.54,10.91;11,112.66,141.16,393.59,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,216.61,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,243.61,127.61,262.59,10.91;11,112.66,141.16,280.44,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization(Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amig√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,400.71,141.16,105.54,10.91;11,112.66,154.71,228.31,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,394.53,10.91;11,112.66,195.36,393.33,10.91;11,112.41,208.91,394.32,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,163.81,195.36,233.18,10.91">Semeval-2021 task 12: Learning with disagreements</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dumitrache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.50,195.36,84.49,10.91;11,112.41,208.91,237.26,10.91">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
		<meeting>the 15th International Workshop on Semantic Evaluation<address><addrLine>SemEval-</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="338" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,394.62,10.91;11,112.66,236.01,393.32,10.91;11,112.33,249.56,394.86,10.91;11,112.66,263.11,397.48,10.91;11,112.66,279.10,74.11,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,223.33,222.46,260.58,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.399</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.399.doi:10.18653/v1/2022.acl-long.399" />
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,236.01,393.32,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.33,10.91;11,112.66,303.75,363.59,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="11,353.43,290.20,152.55,10.91;11,112.66,303.75,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,317.30,393.33,10.91;11,112.66,330.85,107.17,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="11,238.15,317.30,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.60,10.91;11,112.66,357.95,146.44,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="11,206.56,344.40,266.66,10.91">Universal language model fine-tuning for text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,254.91,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11299</idno>
		<title level="m" coord="11,220.90,371.50,285.08,10.91;11,112.66,385.05,72.32,10.91">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
