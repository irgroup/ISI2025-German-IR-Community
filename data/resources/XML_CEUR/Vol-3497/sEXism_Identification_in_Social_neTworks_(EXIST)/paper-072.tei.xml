<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,85.05,376.47,15.39;1,89.29,106.97,76.47,15.39">Tlatlamiztli: Fine-Tuned RoBERTuito for Sexism Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.29,71.43,10.68"><forename type="first">Hardik</forename><surname>Asnani</surname></persName>
							<email>hasnani@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.38,136.29,70.06,10.68"><forename type="first">Andrew</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.10,136.29,86.57,10.68"><forename type="first">Aaryana</forename><surname>Rajanala</surname></persName>
							<email>aarajana@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.68,136.29,69.50,10.68"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
							<email>skuebler@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,85.05,376.47,15.39;1,89.29,106.97,76.47,15.39">Tlatlamiztli: Fine-Tuned RoBERTuito for Sexism Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B69518DB4598628CAA185A2A794BF8CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sexism Detection</term>
					<term>machine translation</term>
					<term>RoBERTuito</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach for detecting sexism in tweets containing English, Spanish, and a mix of both languages. We participated in task 1 of the EXIST 2023 shared task. Our research question focuses on evaluating the performance of a transformer-based model fine-tuned on a Spanish tweet dataset generated by translating tweets containing English, Spanish, and mixed English-Spanish text into Spanish. We use a RoBERTuito model fine-tuned on the EXIST2021 dataset. Our results show that our model placed 31st from 70 evaluated models in the HARD-HARD evaluation, but placed 10th in the SOFT-SOFT evaluation. When evaluating only the Spanish data, we ranked 12th for HARD-HARD and 4th for SOFT-SOFT, showing that our approach works well for the Spanish data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hate speech detection has emerged as a critical area of focus within sentiment analysis, particularly in light of the widespread use of social media. While occurring in various forms, sexism is a specific type of hate speech that occurs rampantly on social media platforms and in a multitude of languages "in many forms in social networks, includes a wide range of behaviours (such as stereotyping, ideological issues, sexual violence, etc.)" <ref type="bibr" coords="1,349.65,434.19,11.58,9.74" target="#b0">[1]</ref>. The EXIST shared task, which started in 2021, is the first of its kind to specifically address sexism detection in tweets written in both English and Spanish.</p><p>EXIST2023 <ref type="bibr" coords="1,150.56,474.84,11.24,9.74" target="#b1">[2,</ref><ref type="bibr" coords="1,164.54,474.84,8.88,9.74" target="#b2">3]</ref> has three sexism detection tasks, we focused on Task 1. This task is defined as a binary classification to determine whether a given tweet includes expressions or behaviors that are sexist. This includes instances where the tweet itself is sexist, it describes a situation that involves sexism, or it criticizes sexist behavior.</p><p>The dataset consisted of tweets written in English, tweets written in Spanish, and tweets that contained a mixture of both languages. As a result, it was important for us to address the tweets that had a combination of English and Spanish content. We aimed to develop a method that could effectively handle this mixed language scenario. We investigate whether translating all tweets to Spanish results in a more robust model. Our approach was developed during a course on machine learning in NLP.</p><p>The remainder of this paper is structured as follows: Section 2 provides an overview of the prior research on this topic, while Section 3 describes our system architecture. In Section 4, we describe the experiments we conducted to evaluate our approach, and in Section 5, we analyze our results. Finally, we conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The EXIST (sEXism Identification in Social neTworks) at IberLEF 2021 was the first shared task in this series. It encouraged the progress and development of automatic identification of sexisms in a broad sense. We focus on publications from the 2021 and 2022 EXIST Task 1, a binary classification task. The curated dataset for the task in 2021 consisted of 6 977 tweets for training and 3 386 tweets for testing. In 2022, the training set was based on the EXIST 2021 dataset. The EXIST 2022 dataset contained the same number of tweets for training and testing as the 2021 set. Additionally, the dataset contained 982 gabs, 492 in English and 490 in Spanish. The Gab information was labeled following the same process as the tweets in the EXIST 2022 dataset <ref type="bibr" coords="2,123.53,322.86,11.43,9.74" target="#b0">[1]</ref>.</p><p>We examined the pre-processing techniques of teams from previous years and the results they obtained. López-López et al. <ref type="bibr" coords="2,213.44,349.95,12.84,9.74" target="#b3">[4]</ref> filtered punctuation and stop words and performed tokenization and lemmatization. They ranked 32nd with an F1 score of 0.7237 for Spanish and English using a majority vote between SDG, XGBoost, RoBERTa, and BERT.</p><p>De Paula et al. <ref type="bibr" coords="2,175.24,390.60,13.00,9.74" target="#b4">[5]</ref> conducted a study on sexism detection and explored the efficacy of utilizing Google Translate to convert data into both English and Spanish, thereby increasing the availability of training data. The authors categorized their models into three types. The first model was a multilingual classifier applied to the dataset without any translation. The second and third models were monolingual classifiers (English and Spanish) applied to the dataset without any translation and the dataset translated to the matching language of the classifier, respectively.</p><p>Their findings revealed that the multilingual model exhibited inferior performance compared to the two types of monolingual models. Additionally, the models incorporating translation outperformed the models without translation. Notably, for the Spanish language, the translated model achieved the same F1-score as the monolingual model and a slightly higher accuracy for Task 1.</p><p>Plaza-del Arco et al. <ref type="bibr" coords="2,193.61,553.19,11.47,9.74" target="#b5">[6]</ref>, ranking fourth with a macro-averaged F1-score of 0.7841 in 2022, unpacked hashtags and split them into their constituent words and converted emojis to their alias. Both Plaza-del Arco et al. <ref type="bibr" coords="2,233.87,580.29,12.84,9.74" target="#b5">[6]</ref> and Talavera et al. <ref type="bibr" coords="2,335.12,580.29,12.84,9.74" target="#b6">[7]</ref> used BETO for Spanish <ref type="bibr" coords="2,458.71,580.29,11.58,9.74" target="#b6">[7]</ref>. BETO, short for Bidirectional Encoder Representations from Transformers for Spanish, is a pre-trained model that captures the contextual information and semantic representations of Spanish text, enabling more accurate natural language processing tasks <ref type="bibr" coords="2,339.30,620.94,11.28,9.74" target="#b7">[8]</ref>. Plaza-del Arco et al. <ref type="bibr" coords="2,444.60,620.94,12.84,9.74" target="#b5">[6]</ref> used BERT whereas Talavera et al. <ref type="bibr" coords="2,194.98,634.49,12.84,9.74" target="#b6">[7]</ref> used RoBERTa for English. Vaca-Serrano <ref type="bibr" coords="2,398.36,634.49,11.46,9.74" target="#b8">[9]</ref>, the best performing approach from the 2022, used different combinations of BERT ensembles, including RoBERTa and BERTweet, and found that domain specific models produced the best results. The most  successful teams also used ensembles and variants of BERT, particularly those trained on data from Twitter <ref type="bibr" coords="3,149.13,332.87,11.43,9.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Overview</head><p>In our system, we employ a transformer-based model that has been fine-tuned on a large dataset of Spanish text. To ensure the versatility of our model in processing English and Spanish data, we adopt a preprocessing step wherein we transform the entire dataset, comprising tweets in English, Spanish, and a mixture of both languages, into Spanish. This process involves leveraging the googletrans python library for machine translation, automatically converting English tweets and any mixed-language tweets into Spanish. By unifying the dataset in Spanish, we create a homogeneous corpus that consists of tweets in the same language.</p><p>The transformed dataset, now entirely in Spanish, is used to fine-tune the transformer model. This enables it to capture the linguistic patterns and nuances specific to the Spanish language, while still maintaining its ability to understand and process English text effectively. An overview of the complete pipeline is shown in Figure <ref type="figure" coords="3,284.60,526.99,3.74,9.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>The data is split into a training and development set with the former containing 6 920 tweets and the latter 1 038. The test set contains 2 076 tweets in English, Spanish, and a mix of both languages. Figure <ref type="figure" coords="3,172.78,633.68,5.17,9.74" target="#fig_1">2</ref> shows an example from the data where the tweet contains both English and Spanish text as well as emojis.</p><p>The shared task focuses on learning under uncertainty. Thus, there are no gold labels per se. Instead each tweet is annotated by six annotators, with an equal split of three female and three male annotators. As a result of having an even number of annotators, there were instances where certain tweets resulted in tied classifications. Specifically, there were 856 ties in the training set and 104 ties in the development set. For the HARD-HARD evaluation, the shared task considers the majority vote as the hard label, and tweets with ties are ignored. We followed this procedure and removed all the tweets with annotator ties from our training and development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pre-Processing</head><p>Our pre-processing consists of deleting and filtering aspects of the text and then translating the text to prepare it for our experiments. First, we lowercased the text and then deleted URLs and user mentions. The next step was to convert emojis to their textual representation using spaCymoji (https://spacy.io/universe/project/spacymoji) as well as segment hashtags with wordsegment (https://pypi.org/project/wordsegment/) Finally, we deleted punctuation and extra white spaces between words. Non-Latin characters as well as spelling errors were left unchanged.</p><p>The entire pre-processed dataset was then converted to Spanish using the Google Translate API (https://pypi.org/project/googletrans/) to address the issue of tweets containing English, Spanish, and mixed English-Spanish textual data <ref type="bibr" coords="4,308.45,341.05,16.25,9.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model</head><p>As domain-specific models were shown to produce the best results in the previous EXIST Task, we opted to employ a version of RoBERETuito, namely the model developed by de Paula et al. <ref type="bibr" coords="4,89.29,417.88,12.84,9.74" target="#b4">[5]</ref> (https://huggingface.co/hackathon-pln-es/twitter_sexismo-finetuned-robertuito-exist2021). The resource can be accessed through HuggingFace and is fine-tuned on the EXIST2021 dataset <ref type="bibr" coords="4,89.29,444.98,16.25,9.74" target="#b10">[11]</ref>.</p><p>We then fine-tuned the model. For the official submission to the shared task, we trained on this year's shared task training data and the development set. For the internal experiments, we trained on the training data and tested on the development set.</p><p>We tokenized all tweets using the tokenizer and pad or truncate the sequences to a maximum length of 128 tokens. We employed the 'pysentimiento/robertuito-base-uncased' tokenizer, which is a variant of the RoBERTa tokenizer designed for Spanish text processing. This tokenizer is specifically trained on a large corpus of Spanish text data and utilizes the uncased strategy, treating all text as lowercase to facilitate better generalization. By utilizing this tokenizer, we converted each tweet into a sequence of tokens, allowing us to process the text at the token level.</p><p>We set the hyperparameters for the training process, such as the number of epochs, the evaluation strategy, and learning rate after optimizing them through a series of 11 experiments. We found that our model was overfitting quickly and that it performed best with just one training epoch. We found that the default learning rate, 5E-5 performed best. With a single training epoch and a default learning rate of 5E-5, our macro average F1 score, precision, recall, and F2 score were all consistently high, with values of 86.71%, 86.74%, 86.69%, and 85.80% respectively, showcasing the robustness and overall effectiveness of our approach in addressing the task at hand. We retrieve the hard label and soft labels (probabilities of each class) from the classifier. The hard label is a binary label (YES or NO) obtained by choosing the label with the highest probability, and the soft label distribution consists of the probabilities per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation</head><p>The shared task results are evaluated in three different scenarios, we focus on HARD-HARD and SOFT-SOFT. For the HARD-HARD evaluation, the gold label consists of the class annotated by the majority of annotators. Any items with no majority class are removed from the evaluation. Here, the model produces a single label, which is compared to the gold label. In the SOFT-SOFT evaluation, the system provides probabilities for each class, and this distribution is evaluated against the distribution of the annotator decisions.</p><p>The official metric for the shared task is the Information Contrast Measure (ICM) <ref type="bibr" coords="5,467.14,397.40,16.34,9.74" target="#b11">[12]</ref>. For the official evaluation, we report our rank, ICM, and F1 for the HARD-HARD evaluation, and our rank and ICM for the SOFT-SOFT evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Official Results</head><p>As described above, our system consists of a Spanish transformer that was originally fine-tuned on the 2021 training data, and that we further fine-tuned on this year's training data. The official results are shown in Table <ref type="table" coords="5,208.28,531.19,3.72,9.74" target="#tab_0">1</ref>. Since ICM and F1 show the same trends, we focus on ICM in our analysis. The results show that overall our system ranks 31st out of 70 submissions, with an ICM of 0.5013 for the HARD-HARD evaluation. For the SOFT-SOFT evaluation, however, our system ranks 10th overall (ICM: 0.6879) and 4th for Spanish (ICM: 0.9021). This shows on the one hand that our system may not be ideal for a hard evaluation, but it is suitable for modeling the distribution of opinions of the different annotators. On the other hand, our system reaches high results for Spanish but much lower results for English, showing that unsurprisingly, sexism is more difficult to detect in translated tweets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on the Development Set</head><p>We also decided to have a closer look at how the language model influences the results and thus compared our model, which is based on RoBERTuito, with that original model. The results of these experiments are shown in Table <ref type="table" coords="6,280.95,259.81,3.81,9.74" target="#tab_1">2</ref>. The results show that our model outperforms RoBERTuito by a small margin, showing that the first fine-tuning on the 2021 data was helpful, even though that dataset will have a different distribution to this year's data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In our contribution to the shared task, we addressed the problem of detecting sexism in tweets that contain both English and Spanish text. We fine-tuned a RoBERTuito-based transformer model using a Spanish tweet dataset that was generated by translating tweets containing English, Spanish, and mixed English-Spanish text into Spanish. Our ranking, especially in the SOFT-SOFT setting, shows that our approach is effective in detecting sexism in tweets. Our experiments also demonstrate the importance of using existing data in a double fine-tuning setting, first on the 2021 training data, then on the current training set.</p><p>As desribed above, this system was developed as a project in a course on machine learning. Participating in an evaluation campaign of this magnitude offered us invaluable practical experience in real-world NLP tasks. It allowed us to apply our course knowledge and techniques to address the intricate problem of detecting sexism in social media.</p><p>The dataset's inherent diversity posed a significant challenge as we grappled with tweets composed in both English and Spanish, as well as a combination of the two. This multifaceted language composition presented intricate complexities in our language processing endeavors. We dedicated substantial time to devising effective strategies for handling language complexities and maintaining high performance across the dataset, accommodating language variations and switching within tweets.</p><p>In conclusion, our involvement in the CLEF evaluation campaign as NLP course students has been an enriching experience. It deepened our understanding of the challenges associated with real-world NLP tasks, enhanced our technical skills, and offered a glimpse into the dynamic and evolving landscape of NLP research.</p><p>We are planning to further investigate the differences in our approach between the HARD-HARD and SOFT-SOFT evaluation. We are also planning to investigate an architecture where we translate each training tweet into the other language, and then classify each tweet in an ensemble of four classifiers, to see if we can improve the English results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,196.46,416.87,8.91;3,89.29,208.42,416.69,8.91;3,89.29,220.37,27.59,8.91;3,89.29,84.19,416.69,104.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture of our Model. From left to right, the EXIST2023 dataset has annotator ties removed, is pre-processed, translated to Spanish, and classified via a fine-tuned transformer-based model.</figDesc><graphic coords="3,89.29,84.19,416.69,104.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,280.69,418.22,8.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tweet from EXIST2023 Data that contains a mix of English and Spanish text as well as emojis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.67,318.06,93.80"><head>Table 1</head><label>1</label><figDesc>Official results of our system.</figDesc><table coords="5,188.22,122.26,218.83,62.21"><row><cell></cell><cell cols="2">HARD-HARD</cell><cell></cell><cell>SOFT-SOFT</cell></row><row><cell cols="2">Language Rank</cell><cell>ICM</cell><cell cols="2">F1 Rank</cell><cell>ICM</cell></row><row><cell>All</cell><cell cols="3">31 0.5013 0.7535</cell><cell>10 0.6879</cell></row><row><cell>English</cell><cell cols="3">40 0.4314 0.7090</cell><cell>22 0.3663</cell></row><row><cell>Spanish</cell><cell cols="3">12 0.5482 0.7867</cell><cell>4 0.9021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.67,401.10,93.80"><head>Table 2</head><label>2</label><figDesc>Results of our experiments on using different language models, evaluated on the development set.</figDesc><table coords="6,129.71,122.26,335.86,62.21"><row><cell>Model</cell><cell cols="4">translated ICM (HARD-HARD) ICM (SOFT-SOFT) macro F1</cell></row><row><cell>ours</cell><cell>yes</cell><cell>0.6007</cell><cell>0.9127</cell><cell>86.71</cell></row><row><cell></cell><cell>no</cell><cell>0.5722</cell><cell>0.8776</cell><cell>85.63</cell></row><row><cell cols="2">RoBERTuito yes</cell><cell>0.5776</cell><cell>0.9040</cell><cell>85.94</cell></row><row><cell></cell><cell>no</cell><cell>0.5694</cell><cell>0.8652</cell><cell>85.96</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,112.40,395.17,9.74;7,112.66,125.95,394.60,9.74;7,112.66,139.50,393.97,9.74;7,112.66,153.05,38.81,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,395.79,125.95,111.47,9.74;7,112.66,139.50,181.87,9.74">Overview of EXIST 2022: sEXism Identification in Social neTworks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mendieta-Aragón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marco-Remón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Makeienko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,303.64,139.50,160.19,9.74">Procesamiento de Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,166.60,394.53,9.74;7,112.66,180.15,393.32,9.74;7,112.66,193.70,394.52,9.74;7,112.66,207.25,393.32,9.74;7,112.66,220.80,333.46,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,112.66,180.15,393.32,9.74;7,112.66,193.70,71.97,9.74">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,400.99,207.25,104.99,9.74;7,112.66,220.80,206.17,9.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,234.35,394.53,9.74;7,112.66,247.89,393.32,9.74;7,112.66,261.44,393.32,9.74;7,112.33,274.99,395.20,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,112.66,247.89,393.32,9.74;7,112.66,261.44,164.23,9.74">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,141.67,274.99,335.72,9.74">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,288.54,393.32,9.74;7,112.66,302.09,393.32,9.74;7,112.66,315.64,175.77,9.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,311.05,288.54,194.94,9.74;7,112.66,302.09,393.32,9.74;7,112.66,315.64,49.20,9.74">Combining transformer-based models with traditional machine learning approaches for sexism identification in social networks at EXIST 2021</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,185.40,315.64,72.06,9.74">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,329.19,393.32,9.74;7,112.66,342.74,388.91,9.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,301.55,329.19,204.43,9.74;7,112.66,342.74,276.03,9.74">Sexism prediction in spanish and english tweets using monolingual and multilingual bert and ensemble models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F M</forename><surname>De Paula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Schlicht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,426.55,343.98,18.39,8.14">r X i v</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1 1 1</biblScope>
			<biblScope unit="page" from="5" to="6" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,356.29,394.53,9.74;7,112.66,369.84,393.32,9.74;7,112.66,383.39,166.12,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,112.66,369.84,393.32,9.74;7,112.66,383.39,39.20,9.74">Exploring the use of different linguistic phenomena for sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del Arco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-D</forename><surname>Molina-González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Ureña-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Martín-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,175.76,383.39,72.06,9.74">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,396.94,393.32,9.74;7,112.66,410.49,394.61,9.74;7,112.66,424.03,103.02,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,302.49,396.94,203.48,9.74;7,112.66,410.49,369.64,9.74">System description for EXIST shared task at IberLEF 2021: Automatic misogyny identification using pretrained transformers</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Talavera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Fidalgo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vila-Suero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,424.03,72.06,9.74">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,437.58,393.32,9.74;7,112.66,451.13,263.99,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,395.97,437.58,110.01,9.74;7,112.66,451.13,115.82,9.74">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,236.67,451.13,32.66,9.74">Pml</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2020</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,464.68,394.61,9.74;7,112.66,478.23,103.02,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,189.18,464.68,298.41,9.74">Detecting and classifying sexism by ensembling transformers models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaca-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,478.23,72.06,9.74">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,491.78,395.16,9.74;7,112.66,505.33,195.65,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,202.79,491.78,305.03,9.74;7,112.66,505.33,69.36,9.74">EXIST2021: Detecting sexism with transformers and translationaugmented data</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">García</forename><surname>Subies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.29,505.33,72.06,9.74">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,518.88,71.57,9.74;7,200.46,518.88,84.47,9.74;7,301.16,518.88,206.67,9.74;7,112.66,532.43,393.86,9.74;7,112.66,545.98,179.53,9.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grandury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Del Campo</surname></persName>
		</author>
		<ptr target="https://huggingface.co/hackathon-pln-es/twitter_sexismo-finetuned-robertuito-exist2021" />
		<title level="m" coord="7,301.16,518.88,206.67,9.74;7,112.66,532.43,89.58,9.74">hackathon-pln-es/twitter_sexismo-finetunedrobertuito-exist2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,559.53,394.61,9.74;7,112.66,573.08,393.32,9.74;7,112.33,586.62,122.11,9.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,223.33,559.53,260.57,9.74">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,573.08,393.32,9.74;7,112.33,586.62,23.88,9.74">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
