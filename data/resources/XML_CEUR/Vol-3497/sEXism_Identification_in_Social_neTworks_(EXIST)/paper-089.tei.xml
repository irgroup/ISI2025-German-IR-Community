<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,390.29,15.42;1,89.29,106.66,135.86,15.42;1,89.29,129.00,205.10,11.96">LSTM-Attention Architecture for Online Bilingual Sexism Detection Notebook for the EXIST Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,69.39,11.96"><forename type="first">Srinivasa</forename><surname>Ravi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Karnataka</orgName>
								<address>
									<settlement>Surathkal</settlement>
									<region>Mangaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,176.84,154.90,82.59,11.96"><forename type="first">Siddharth</forename><surname>Kelkar</surname></persName>
							<email>siddharthkelkar.211it067@nitk.edu.in</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Karnataka</orgName>
								<address>
									<settlement>Surathkal</settlement>
									<region>Mangaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.94,154.90,124.81,11.96"><forename type="first">Anand</forename><surname>Kumar Madasamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology Karnataka</orgName>
								<address>
									<settlement>Surathkal</settlement>
									<region>Mangaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,390.29,15.42;1,89.29,106.66,135.86,15.42;1,89.29,129.00,205.10,11.96">LSTM-Attention Architecture for Online Bilingual Sexism Detection Notebook for the EXIST Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C01139817BEEC32D5FE589A4CB124BFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LSTM</term>
					<term>Attention</term>
					<term>GloVe</term>
					<term>fastText</term>
					<term>Sexism Identification</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper describes the results submitted by 'Team-SMS' at EXIST 2023. A dataset of 6920 tweets for training, 1038 for validation, and 2076 tweets for testing was provided by the task organizers to train and test our models. Our models include LSTM models coupled with attention layers and without attention. For calculation of soft scores according to the task we tried to mimic human performance by taking an average of different machine learning model predictions using Multinomial Naive Bayes, Linear Support Vector Classifier, Multi Layer Perceptron, XGBoost, LSTM using GloVe embeddings, and LSTM using fastText embeddings. We discuss our approach to remove the ambiguity in the labeling process and detailed description of our work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sexism, a pervasive form of gender-based discrimination, continues to be a critical social issue across the globe. It manifests in various contexts, such as workplaces, educational institutions, and online platforms, reinforcing harmful stereotypes and limiting opportunities for individuals based on their gender. As the world becomes increasingly interconnected through digital platforms and social media, detecting and addressing sexism in online content has become a pressing concern.</p><p>Deep learning, a subset of machine learning, has emerged as a powerful tool for tackling complex tasks by automatically learning intricate patterns and representations from vast amounts of data. Leveraging the capabilities of deep learning, researchers and technologists have been exploring innovative approaches to develop automated systems capable of detecting and combating sexism in online communications.</p><p>The shared tasks on sEXism Identification in Social neTworks (EXIST) at CLEF 2023 <ref type="bibr" coords="1,482.85,547.63,11.42,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,496.99,547.63,8.99,10.91" target="#b1">2]</ref> represent a systematic benchmark that attempts to tackle this challenge via machine learning and Natural Language Understanding (NLU). Also, the assumption that natural language expressions have a single and clearly identifiable interpretation in a given context is a convenient idealization, but far from reality, especially in highly subjective tasks such as sexism identification. The learning with disagreements paradigm in this shared task aims to deal with this by letting systems learn from datasets where no gold annotations are provided but information about the annotations from all annotators, in an attempt to gather the diversity of view.</p><p>This paper presents our contribution to the tasks, algorithms to extract useful information from multiple annotations, describes our overall approach and the methods and models applied and summarizes the obtained results. We summarize our results for two tasks: Sexism Identification (Task 1) and Source Intention (Task 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Ambiguity removal</head><p>A major challenge of the EXIST tasks <ref type="bibr" coords="2,262.89,274.95,11.49,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,277.37,274.95,9.03,10.91" target="#b1">2]</ref> is the small size of dataset (around 7000 tweets) which is uniformly divided into English and Spanish tweets. This small size provides difficulty in robust training of any model. Apart from that around 10% of the annotated tweets have ambiguous labels i.e. the 6 annotations provided for the label have no clear majority which if not processed further reduces the dataset size. To counter this, we formulated the following algorithms to calculate a hard label from the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trust score</head><p>The 'trust score' for each of the annotators in the training data can be calculated by finding the ratio of correct annotations (from the tweets having majority labels) to total annotations by each annotator. The result is stored in a dictionary. This provides us a metric to remove ambiguities from the annotations and give hard labels to each ambiguous tweet by giving priority to annotations of annotators with higher trust score. The distribution for each annotator is uniformly distributed in the dataset thus removing any chances of bias. The generalized algorithm (Algorithm 1) to calculate the Trust Score in any similar dataset is presented here.</p><p>To further generalize this, according to the info provided in the training data about the annotators, they can be grouped into 6 classes with respect to gender and age (18-22F, 23-45F, 46+F, 18-22M, 23-45M, and 46+M). The individual trust scores are grouped into these classes and their average is taken to calculate the class-wise trust score. This gives us another metric to remove ambiguity from the tweets. The generalized algorithm (Algorithm 2) to do this is presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ambiguity resolution</head><p>Once the individual and class-wise trust scores are calculated, they can be used to resolve ambiguity by labelling ambiguous tweets. Either the individual trust scores or the class-wise trust scores can be used for this purpose. For each annotator giving a 'YES' or 'NO' annotation, the respective trust score (individual or class-wise) is added to the classes. The class having higher score will act as the label. This provides a way to remove ambiguity from the tweets </p><formula xml:id="formula_0" coords="3,100.14,200.11,143.05,354.62">ğ’Ÿ ğ´ â† ğœ‘ ğ’Ÿ ğµ â† ğœ‘ foreach ğ‘’ âˆˆ â„° do ğ’© (ğ‘’) â† 0 ğ‘‡ (ğ‘’) â† 0 foreach ğ‘‘ âˆˆ ğ’Ÿ do ğ‘› ğ´ â† 0 ğ‘› ğµ â† 0 foreach ğ‘’ âˆˆ ğ¸ ğ‘‘ do ğ’© (ğ‘’) â† ğ’© (ğ‘’) + 1 if ğ‘’(ğ‘‘) = ğ´ then ğ‘› ğ´ â† ğ‘› ğ´ + 1 else ğ‘› ğµ â† ğ‘› ğµ + 1 if ğ‘› ğ´ &gt; ğ‘› ğµ then ğ’Ÿ ğ´ â† ğ’Ÿ ğ´ âˆª {ğ‘‘} else if ğ‘› ğµ &gt; ğ‘› ğ´ then ğ’Ÿ ğµ â† ğ’Ÿ ğµ âˆª {ğ‘‘} foreach ğ‘‘ âˆˆ ğ’Ÿ do if ğ‘‘ âˆˆ ğ’Ÿ ğ´ then ğ¶ â† ğ´ else ğ¶ â† ğµ foreach ğ‘’ âˆˆ ğ¸ ğ‘‘ do if ğ‘’(ğ‘‘) = ğ¶ then ğ‘‡ (ğ‘’) â† ğ‘‡ (ğ‘’) + 1 ğ’© (ğ‘’)</formula><p>and helps in retaining the training data. The generalized algorithm (Algorithm 3) to do this is presented here. After multiple runs, using the class-wise trust score for this purpose appears to perform better than using the individual scores, probably because the former provides more generalization and removes any individual bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Text preprocessing</head><p>The following preprocessing steps were performed on each tweet:</p><p>1. Stopword Removal: The function stopword_removal is applied to each row of the 'tweet' column using the apply method. It is assumed that the stopword_removal function removes commonly used words (stopwords) from the text, which are typically irrelevant for natural language processing tasks. This was done with the help of the NLTK [3] library. 2. Lowercasing: The lower() method is applied to the 'tweet' column, converting all the text to lowercase. This step is performed to ensure consistency in the text and to avoid treating words with different cases as different entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">URL Removal:</head><p>The regex, r'https?:\/\/\S+' is used to match and remove URLs from each tweet. It looks for patterns starting with 'http://' or 'https://' followed by any non-whitespace characters. This step aims to eliminate URLs, which are often irrelevant for text analysis or modeling. 4. Website Removal: The regex, r"www\.[a-z]?\.?(com)+|[a-z]+\.(com)" is used to match and remove website addresses from the tweets. It searches for patterns starting with 'www.' followed by an optional single letter and optional 'com', or it matches any combination of lowercase letters followed by '.com'. This step targets website links that were not captured by the previous URL removal step. 5. HTML Entity Removal: The regex, r'&amp;[a-z]+;' matches and removes HTML entities from the tweets. HTML entities are special characters represented using codes like '&amp;' for '&amp;', '&lt;' for '&lt;', etc. This step is useful when dealing with text scraped from websites or social media platforms. 6. Special Character and Symbol Removal: r'[^a-z\s\(\-:\)\\\/\];=\'#]' is the regex used to match and remove any character that is not a lowercase letter, whitespace, parentheses, hyphen, colon, backslash, forward slash, semicolon, equal sign, single quote, or hash symbol. This step removes special characters and symbols that might not carry significant meaning for text analysis or modeling. 7. Mention Removal: The regex, r'@mention' matches and removes the string '@mention' from the tweets. This step targets mentions of users or handles often found in social media text.</p><p>In summary, the code applies a series of text preprocessing steps to clean the 'tweet' column. These steps include stopword removal, lowercasing, URL and website removal, HTML entity removal, special character and symbol removal, and mention removal. The resulting 'tweet' column should contain cleaned and standardized text that is ready for further analysis or modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation metrics</head><p>For all tasks and all types of evaluation (hard-hard, hard-soft, and soft-soft), the official metric: ICM <ref type="bibr" coords="5,111.99,656.03,12.99,10.91" target="#b3">[4]</ref> is used. ICM is a similarity function that generalizes Pointwise Mutual Information (PMI) and can be used to evaluate system outputs in classification problems by computing their similarity to the ground truth categories. As there is not any current metric that fits hierarchical multi-label classification problems in a learning with disagreement scenario, the organizers have defined an extension of ICM (ICM-soft) that accepts both soft system outputs and soft ground truth assignments.</p><p>1. Hard-Hard evaluation: For systems that provide a hard, conventional output, the organizers provided a hard-hard evaluation. To derive the hard labels in the ground truth from the different annotators' labels, a probabilistic threshold was computed for each task. As a result, for Task 1, the class annotated by more than 3 annotators is selected; and for Task 2, the class annotated by more than 2 annotators is selected. Items for which there is no majority class (i.e. no class receives more probability than the threshold) are removed in this evaluation scheme. The official metric is the original ICM <ref type="bibr" coords="6,413.34,232.22,11.58,10.91" target="#b3">[4]</ref>. The systems are also compared with F1 (the harmonic average of precision and recall). In Task 1, F1 is used for the positive class, and in Task 2, the average of F1 for all classes is used. 2. Hard-Soft evaluation: For systems that provide a hard output, the organizers provided a hard-soft evaluation, comparing the categories assigned by the system with the probabilities assigned to each category in the ground truth. ICM-soft is the official evaluation metric in this variant. The probabilities of the classes for each instance are calculated according to the distribution of labels and the number of annotators for that instance. 3. Soft-Soft evaluation: For systems that provide probabilities for each category, the organizers provided a soft-soft evaluation that compares the probabilities assigned by the system with the probabilities assigned by the set of human annotators. As in the previous case, ICM-soft is used as the official evaluation metric in this variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 1: Sexism Identification</head><p>The 'Sexism Identification' task involved classifying tweets as being sexist or not. To do this, we employed the usage of various simple machine learning models and neural networks and analyzed the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simple models</head><p>We tested five models, namely Multinomial Naive Bayes <ref type="bibr" coords="6,333.02,519.42,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="6,346.46,519.42,7.49,10.91" target="#b5">6]</ref>, Linear SVC <ref type="bibr" coords="6,411.10,519.42,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="6,424.53,519.42,7.49,10.91" target="#b5">6]</ref>, Bernoulli Naive Bayes <ref type="bibr" coords="6,116.86,532.97,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="6,130.13,532.97,7.49,10.91" target="#b5">6]</ref>, Multilayer Perceptron <ref type="bibr" coords="6,241.05,532.97,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="6,254.31,532.97,8.88,10.91" target="#b5">6]</ref> and XGBoost <ref type="bibr" coords="6,324.19,532.97,11.28,10.91" target="#b8">[9]</ref>. The evaluation was done by removing ambiguous tweets from the validation set and the metric of the evaluation is the ICM and F1 score using the hard-hard evaluation mode of the python script provided. All implementations considered a 5-fold cross-validation process during the training stage. A hyperparameters analysis was conducted for all models, considering: different learning rates, alpha, beta, hidden layer sizes for the models for which they are applicable and optimal parameters were found.</p><p>The results are detailed in Table <ref type="table" coords="6,239.19,614.27,3.81,10.91" target="#tab_0">1</ref>. XGBoost <ref type="bibr" coords="6,294.85,614.27,11.58,10.91" target="#b8">[9]</ref>, Multilayer Perceptron <ref type="bibr" coords="6,417.14,614.27,11.49,10.91" target="#b7">[8,</ref><ref type="bibr" coords="6,431.79,614.27,9.03,10.91" target="#b5">6]</ref> and Bernoulli Naive Bayes <ref type="bibr" coords="6,145.53,627.82,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="6,159.48,627.82,8.88,10.91" target="#b5">6]</ref> appear to perform significantly better than the others with ICM <ref type="bibr" coords="6,452.19,627.82,12.68,10.91" target="#b3">[4]</ref> scores of around 0.27 as compared to scores of 0.2026 and 0.2370 for Multinomial Naive Bayes <ref type="bibr" coords="6,464.03,641.37,11.30,10.91" target="#b4">[5,</ref><ref type="bibr" coords="6,478.06,641.37,8.92,10.91" target="#b5">6]</ref> and Linear SVC <ref type="bibr" coords="6,142.37,654.92,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="6,156.46,654.92,8.96,10.91" target="#b5">6]</ref> respectively. The same models were also run after applying the ambiguity resolution algorithm with the class-wise trust scores. The results for this are detailed in Table <ref type="table" coords="7,373.78,364.48,3.74,10.91">2</ref>.</p><p>As seen in the results after applying ambiguity resolution, the accuracy decreases marginally with an average reduction in the ICM <ref type="bibr" coords="7,257.34,391.57,12.74,10.91" target="#b3">[4]</ref> score of 8.49% across the models. So, it is not helping us to extract more useful features by increasing the training data. In the future, the algorithms could be improved by using a more complex combination of metrics to group and compute scores to increase the accuracy. These models are not directly part of the main submission but are used for the calculation of soft scores for Task 1. We tried to replicate human annotator performance using these models and taking a probability of the predictions spread across these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Soft-score calculation</head><p>The assumption that natural language expressions have a single and clearly identifiable interpretation in a given context is a convenient idealization, but far from reality, especially in highly subjective task as sexism identification. The learning with disagreements paradigm of EXIST 2023 <ref type="bibr" coords="7,111.89,563.24,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="7,125.83,563.24,8.88,10.91" target="#b1">2]</ref> aims to deal with this by letting systems learn from datasets from information about the annotations from all annotators, in an attempt to gather the diversity of views. Taking this into consideration for soft score calculation we sought to replicate human annotator expertise through the utilization of various models. The soft score calculation involved aggregating prediction probabilities (For example, if 4 models predicted YES and 2 predicted NO then the soft score would be YES : 0.67, NO : 0.33) from six distinct models, namely Multinomial Naive Bayes <ref type="bibr" coords="7,118.16,644.54,11.38,10.91" target="#b4">[5,</ref><ref type="bibr" coords="7,132.26,644.54,7.58,10.91" target="#b5">6]</ref>, Linear Support Vector Classifier <ref type="bibr" coords="7,291.98,644.54,11.38,10.91" target="#b6">[7,</ref><ref type="bibr" coords="7,306.09,644.54,7.58,10.91" target="#b5">6]</ref>, Multilayer Perceptron <ref type="bibr" coords="7,421.27,644.54,11.38,10.91" target="#b7">[8,</ref><ref type="bibr" coords="7,435.37,644.54,7.58,10.91" target="#b5">6]</ref>, XGBoost <ref type="bibr" coords="7,491.91,644.54,11.45,10.91" target="#b8">[9]</ref>, LSTM <ref type="bibr" coords="7,119.42,658.09,16.55,10.91" target="#b9">[10,</ref><ref type="bibr" coords="7,138.86,658.09,9.03,10.91" target="#b5">6]</ref> using GloVe <ref type="bibr" coords="7,208.32,658.09,18.07,10.91" target="#b10">[11]</ref> word embeddings , and LSTM using word fastText <ref type="bibr" coords="7,461.52,658.09,18.06,10.91" target="#b11">[12]</ref> word embeddings. By leveraging the collective insights from these diverse models, we aimed to attain a robust and accurate soft score, mirroring the performance of human annotators. This approach allowed us to harness the strengths of each model and effectively gauge the quality of the text processing framework. The exact procedure is detailed in Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4: Soft Score Calculation</head><p>Data: ğ’¯ , pieces of text to be classified, ğ‘š : ğ’¯ â†’ {YES, NO} âˆˆ â„³, the set of models being used to classify Result: Soft scores, ğ’®, where ğ’® :</p><formula xml:id="formula_1" coords="8,99.74,485.75,357.46,160.95">ğ’¯ â†’ {{Y, N} | 0 â‰¤ Y, N âˆˆ R â‰¤ 1, Y + N = 1} Algorithm: ğ‘ = ğ‘›(â„³) foreach ğ‘¡ âˆˆ ğ’¯ do Y = 0 N = 0 foreach ğ‘š âˆˆ â„³ do if ğ‘š(ğ‘¡) = YES then Y = Y + 1 ğ‘ else N = N + 1 ğ‘ ğ’®(ğ‘¡) = {Y, N}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Deep learning models</head><p>We submitted a total for 3 runs for this task and used neural network models for all of them. The final models for all the runs were trained on both the training and validation data. The exact details are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 1</head><p>We employed 200-dimensional GloVe <ref type="bibr" coords="9,252.47,183.59,17.76,10.91" target="#b10">[11]</ref> Twitter word embeddings for English texts, while 300dimensional fastText <ref type="bibr" coords="9,185.06,197.14,17.96,10.91" target="#b11">[12]</ref> word embeddings were utilized for Spanish texts. For English texts, the model architecture comprised of a dense <ref type="bibr" coords="9,287.07,210.69,12.76,10.91" target="#b7">[8]</ref> layer designed to convert the 200-dimensional embeddings into a more compact representation of 120 dimensions. Subsequently, a 120-unit LSTM <ref type="bibr" coords="9,119.78,237.79,18.07,10.91" target="#b9">[10]</ref> layer was incorporated, with a 20% dropout <ref type="bibr" coords="9,344.82,237.79,18.07,10.91" target="#b14">[15]</ref> rate to enhance generalization.</p><p>To capture the contextual dependencies within the text, an attention <ref type="bibr" coords="9,398.68,251.33,17.98,10.91" target="#b12">[13]</ref> layer and a pooling <ref type="bibr" coords="9,89.29,264.88,17.95,10.91" target="#b15">[16]</ref> layer were included in the model. Finally, a dense layer with softmax <ref type="bibr" coords="9,420.63,264.88,17.95,10.91" target="#b16">[17]</ref> activation was employed to convert the representations into two dimensions (for classification), facilitating meaningful predictions. The model architecture is shown in Figure <ref type="figure" coords="9,389.63,291.98,3.74,10.91" target="#fig_0">1</ref>.</p><p>For the model for Spanish texts, a dense layer was used to convert the 300-dimensional embeddings to 150-dimensions. This was followed by a 150 unit LSTM layer with dropout followed by a Dense layer of 32 units with dropout and finally output (of 2 dimensions) using softmax activation. The soft score calculation involved the usage of Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 2</head><p>In this run, we employed 300-dimensional fastText <ref type="bibr" coords="9,314.41,395.13,17.81,10.91" target="#b11">[12]</ref> word embeddings for both English and Spanish texts. The model architecture was designed to maximize performance and accuracy, comprising a 150-unit LSTM <ref type="bibr" coords="9,216.37,422.23,17.76,10.91" target="#b9">[10]</ref> layer, followed by a Dropout <ref type="bibr" coords="9,364.12,422.23,17.76,10.91" target="#b14">[15]</ref> layer to mitigate overfitting. This was succeeded by a Dense <ref type="bibr" coords="9,236.77,435.78,13.00,10.91" target="#b7">[8]</ref> layer with 32 units, followed by another Dropout layer, ultimately culminating in the final output utilizing softmax <ref type="bibr" coords="9,352.65,449.33,17.82,10.91" target="#b16">[17]</ref> activation. This run employed same model architecture for both English and Spanish texts. The soft score prediction employed the same process of taking the probabilistic score of various model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 3</head><p>The model and word embedding specifications for this run are the same as in Run 2. This run employed our ambiguity resolution algorithm using class-wise trust score to increase the data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Run 1 received a rank of 26/53 on soft-soft evaluation, 51/65 on hard-hard evaluation and 52/65 on hard-soft evaluation on ALL text instances. The significantly better soft-soft score indicates that such a method of employing various machine learning models to mimic human performance and creating soft probabilistic scores can be useful in future work in other tasks.</p><p>Run 2 received a rank of 27/53 on soft-soft evaluation, 52/65 on hard-hard evaluation and rank 53/65 on hard-soft evaluation on ALL text instances. Run 3 received a rank of 49/65 on hard-hard evaluation and rank 50/65 on hard-soft evaluation. This run with the algorithm performed marginally better which indicates that improving the algorithm may lead to better results. The complete results are detailed in Tables <ref type="table" coords="10,446.92,552.97,15.27,10.91" target="#tab_5">3, 4</ref> and<ref type="table" coords="10,484.07,552.97,3.74,10.91" target="#tab_6">5</ref>.</p><p>The results for hard-hard and hard-soft evaluation are not very promising which indicates other training methods should be employed to get better results. Combined training rather than parallel training of Spanish and English texts could prove helpful (which we tried for runs of Task 2). On the other hand, the runs gave very promising results on the soft score metric which proves that such an attempt to employ various models to create soft score can be developed further to get better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 2: Source Intention</head><p>The 'Source Intention' task involved identifying whether a tweet was sexist or not and subsequently classifying the sexist tweets based on their source intention into Direct, Reported or Judgemental categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Deep learning models</head><p>We submitted a total of 3 runs for this task each of whose details are given below. The final models for runs 1 and 2 were trained on both training and validation data whereas the model for run 3 was trained only on the training data. The exact details of the models are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 1</head><p>For this run, we used 200-dimensional GloVe <ref type="bibr" coords="11,288.04,487.97,17.78,10.91" target="#b10">[11]</ref> embeddings for English and 300-dimensional fastText <ref type="bibr" coords="11,128.15,501.52,18.06,10.91" target="#b11">[12]</ref> embeddings for Spanish. For labelling, we selected the label which was in clear majority with respect to all the annotations. The model consists of two parallel paths. One of the paths starts with a dense <ref type="bibr" coords="11,216.44,528.62,12.69,10.91" target="#b7">[8]</ref> layer (size 200 for English and 300 for Spanish) which squeezes the input tensors into 50-dimensions. This is followed by a dropout <ref type="bibr" coords="11,397.61,542.17,18.06,10.91" target="#b14">[15]</ref> of 20%, followed by a 50-unit LSTM <ref type="bibr" coords="11,162.78,555.71,18.07,10.91" target="#b9">[10]</ref> layer with a dropout of 25%. This is finally passed to an attention layer whose scores are fed to the other path. The other parallel path is a replica of this path but without the final attention <ref type="bibr" coords="11,207.10,582.81,17.76,10.91" target="#b12">[13]</ref> layer. The output from the LSTM layer is concatenated with the attention scores from the other path. This is followed by pooling <ref type="bibr" coords="11,386.42,596.36,18.07,10.91" target="#b15">[16]</ref> and a dense layer of 4 units with softmax <ref type="bibr" coords="11,176.52,609.91,17.98,10.91" target="#b16">[17]</ref> activation for final classification. The model architecture is shown in Figure <ref type="figure" coords="11,120.36,623.46,3.74,10.91" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 2</head><p>For this run, 200-dimensional fastText <ref type="bibr" coords="12,263.29,478.72,18.01,10.91" target="#b11">[12]</ref> embeddings (reduced from 300-dimensions) were used for both Spanish and English texts and a single model was trained on both languages. The labelling procedure is the same as in Run 1. The model architecture is also identical to that of Run 1, with the slight difference that 80-unit LSTM <ref type="bibr" coords="12,314.12,519.36,17.76,10.91" target="#b9">[10]</ref> networks were used instead of 50-units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for Run 3</head><p>This run is perfectly identical to Run 2 with only a slight difference in training methodology where the validation data set was not used to train the final model unlike the previous runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>Run 1 received a rank of 19/32 on hard-hard evaluation and 10/33 on hard-soft evaluation on ALL text instances. It also received a rank of 2/32 on hard-soft evaluation on ES text instances. The significantly better hard-soft evaluation performance indicates that such an model performs most like human annotators and using such an architecture might be useful in future tasks. Run 2 received a rank of 23/32 on hard-hard evaluation and 6/33 on hard-soft evaluation on ALL text instances. It also received ranks of 8/32 and 8/33 on hard-soft evaluation on ES and EN text instances respectively. The results of this run appear to be the most consistent across languages and indicates that training on both languages together might result in better performance on texts containing both of the languages. Run 3 received similar ranks to Run 2 on ALL text instances but a rank of 3/32 on hard-soft evaluation on ES text instances (compared to 8/32) and 16/33 on hard-soft evaluation on EN text instances (compared to 8/33). Therefore, it seems to perform slightly better at Spanish and worse in English. The complete results are detailed in Tables 6, 7 and 8.</p><p>All the runs were performed on the same general architecture with variation in the training procedure. The results for hard-hard evaluation are not promising indicating other architectures might be more suitable for it. However, the results of hard-soft evaluation indicates that such an architecture might be more suitable for mimicking the human annotating process (as it appears to predict correctly for higher probabilities) and may be explored and developed further for future tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper, we have detailed the work done by the SMS team in Task 1: Sexism identification and Task 2: Source Intention of the EXIST Lab at CLEF 2023 <ref type="bibr" coords="14,353.45,281.08,11.36,10.91" target="#b0">[1,</ref><ref type="bibr" coords="14,367.53,281.08,7.58,10.91" target="#b1">2]</ref>. The aim of our experiments was to test how LSTM + Attention models give results by incorporating different training methods of parallel or combined training, different word embeddings, different architectures etc.</p><p>We have come to the conclusion that combined processing of multilingual training data gives the best overall performance due to the results of run 2 of Task 2 which scored an overall ICM-Soft Norm of 0.6978, the highest among the three runs and performed more consistently across English and Spanish (with a rank of 8 in both) as compared to the other two runs.</p><p>In the future, we plan to continue to develop and assess our ambiguity resolution algorithms to improve the training phase of the system by removing the ambiguity and getting more features to train. The algorithms can be tested on larger datasets with a similar structure of being labelled by individual annotators. As we have utilized a combination of age and gender to group and compute scores for ambiguity resolution, a more complex combination of metrics could be used in datasets where more information regarding the annotators is available which might yield better results. We also plan to do a detailed analysis on the mimicking of human performance using various machine learning models as represented in this paper on other similar tasks and refine the process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,325.75,417.05,8.93;8,89.29,337.76,110.00,8.87;8,243.84,84.19,107.60,234.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: LSTM [10] + Attention<ref type="bibr" coords="8,230.95,325.80,16.60,8.87" target="#b12">[13]</ref> architecture for Task 1: Sexism Identification, via TensorFlow Keras<ref type="bibr" coords="8,114.64,337.76,15.05,8.87" target="#b13">[14,</ref><ref type="bibr" coords="8,132.18,337.76,8.23,8.87" target="#b5">6]</ref> plot_model.</figDesc><graphic coords="8,243.84,84.19,107.60,234.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,89.29,408.95,417.05,8.93;12,89.29,420.96,110.00,8.87;12,243.84,84.18,107.60,318.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Parallel LSTM [10] + Attention<ref type="bibr" coords="12,258.64,409.00,16.38,8.87" target="#b12">[13]</ref> architecture for Task 2: Source Intention, via TensorFlow Keras<ref type="bibr" coords="12,114.64,420.96,15.05,8.87" target="#b13">[14,</ref><ref type="bibr" coords="12,132.18,420.96,8.23,8.87" target="#b5">6]</ref> plot_model.</figDesc><graphic coords="12,243.84,84.18,107.60,318.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,94.31,89.50,361.81,107.03"><head>Algorithm 1 :</head><label>1</label><figDesc>Individual Trust Score Data: Dataset ğ’Ÿ with ğ‘ samples, Each sample classified (into two class types, ğ´ and ğµ) by a set of entities (entity ğ‘’ âˆˆ â„°, ğ‘’ : ğ’Ÿ â†’ {ğ´, ğµ}), ğ¸ ğ‘‘ (âŠ‚ â„°) âˆ€ ğ‘‘ âˆˆ ğ’Ÿ from the set of all entities â„° Result: ğ‘‡ : â„° â†’ R, where ğ‘‡ (ğ‘’) = Trust Score of ğ‘’ âˆ€ ğ‘’ âˆˆ â„°, Set of successfully classified samples ğ’Ÿ ğ´ and ğ’Ÿ ğµ , Set of ambiguous samples ğ’Ÿ ambi Algorithm:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,94.31,92.57,343.73,395.69"><head>Algorithm 2: Class-Wise Trust Score Data:</head><label></label><figDesc>Set of all entities, â„°, Set of all entity classes, ğ’, ğ‘† : â„° â†’ ğ’, where ğ‘†(ğ‘’) = The entity class ğ‘’ belongs to âˆ€ ğ‘’ âˆˆ â„°, ğ‘‡ : â„° â†’ R, where ğ‘‡ (ğ‘’) = Trust Score of ğ‘’ âˆ€ ğ‘’ âˆˆ â„° Result: ğ‘‡ ğ‘” : ğ’ â†’ R, where ğ‘‡ ğ‘” (ğ‘) = Class-Wise Trust Score of ğ‘ âˆ€ ğ‘ âˆˆ ğ’ Dataset ğ’Ÿ, â„°, ğ¸ ğ‘‘ âˆ€ ğ‘‘ âˆˆ ğ’Ÿ (same definitions as in Algorithm 1), Set of successfully classified samples ğ’Ÿ ğ´ and ğ’Ÿ ğµ , Set of ambiguous samples ğ’Ÿ ambi , ğ‘‡ : â„° â†’ R, where ğ‘‡ (ğ‘’) = Trust Score of ğ‘’ âˆ€ ğ‘’ âˆˆ â„°, Classification bias ğ›½ âˆˆ {ğ´, ğµ} Result: Successfully classified ğ’Ÿ into ğ’Ÿ ğ´ and ğ’Ÿ ğµ , such that ğ’Ÿ ğ´ âˆª ğ’Ÿ ğµ = ğ’Ÿ</figDesc><table coords="4,94.31,176.28,163.93,311.97"><row><cell>Algorithm:</cell><cell></cell></row><row><cell cols="2">foreach ğ‘ âˆˆ ğ’ do</cell></row><row><cell cols="2">ğ‘‡ ğ‘” (ğ‘) â† 0</cell></row><row><cell cols="2">ğ‘ (ğ‘) â† 0</cell></row><row><cell cols="2">foreach ğ‘’ âˆˆ â„° do</cell></row><row><cell cols="2">ğ‘ (ğ‘†(ğ‘’)) â† ğ‘ (ğ‘†(ğ‘’)) + 1</cell></row><row><cell cols="2">ğ‘‡ ğ‘” (ğ‘†(ğ‘’)) â† ğ‘‡ ğ‘” (ğ‘†(ğ‘’)) + ğ‘‡ (ğ‘’)</cell></row><row><cell cols="2">foreach ğ‘ âˆˆ ğ’ do</cell></row><row><cell>ğ‘‡ ğ‘” (ğ‘) â†</cell><cell>ğ‘‡ğ‘”(ğ‘) ğ‘ (ğ‘)</cell></row><row><cell cols="2">Algorithm 3: Ambiguity Resolution</cell></row><row><cell>Data: Algorithm:</cell><cell></cell></row><row><cell cols="2">foreach ğ‘‘ âˆˆ ğ’Ÿ ambi do</cell></row><row><cell>ğ‘  ğ´ â† 0</cell><cell></cell></row><row><cell>ğ‘  ğµ â† 0</cell><cell></cell></row><row><cell cols="2">foreach ğ‘’ âˆˆ ğ¸</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,100.14,478.49,130.11,190.20"><head>ğ‘‘ do if ğ‘’(ğ‘‘) = ğ´ then ğ‘  ğ´ â† ğ‘  ğ´ + ğ‘‡ (ğ‘’) else ğ‘  ğµ â† ğ‘  ğµ + ğ‘‡ (ğ‘’) if ğ‘  ğ´ &gt; ğ‘  ğµ then ğ’Ÿ</head><label></label><figDesc>ğ´ â† ğ’Ÿ ğ´ âˆª {ğ‘‘} ğ’Ÿ ambi â† ğ’Ÿ ambi -{ğ‘‘} else if ğ‘  ğµ &gt; ğ‘  ğ´ then ğ’Ÿ ğµ â† ğ’Ÿ ğµ âˆª {ğ‘‘} ğ’Ÿ ambi â† ğ’Ÿ ambi -{ğ‘‘} foreach ğ‘‘ âˆˆ ğ’Ÿ ambi do ğ’Ÿ ğ›½ â† ğ’Ÿ ğ›½ âˆª {ğ‘‘} ğ’Ÿ ambi â† ğ’Ÿ ambi -{ğ‘‘}</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,364.00,233.73"><head>Table 1</head><label>1</label><figDesc>Results without applying ambiguity resolution</figDesc><table coords="7,88.99,122.10,364.00,202.11"><row><cell>Model</cell><cell cols="3">ICM Score [4] F1:YES F1:NO MARCO:F1</cell></row><row><cell>Multinomial Naive Bayes</cell><cell>0.2026</cell><cell>0.7021 0.7623</cell><cell>0.7322</cell></row><row><cell>Linear SVC</cell><cell>0.2370</cell><cell>0.7072 0.7778</cell><cell>0.7425</cell></row><row><cell>Bernoulli Naive Bayes</cell><cell>0.2714</cell><cell>0.7378 0.7753</cell><cell>0.7566</cell></row><row><cell>XGBoost</cell><cell>0.2740</cell><cell>0.7344 0.7796</cell><cell>0.7570</cell></row><row><cell>Multilayer Perceptron</cell><cell>0.2641</cell><cell>0.7286 0.7782</cell><cell>0.7534</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Results after applying ambiguity resolution</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">ICM Score [4] F1:YES F1:NO MARCO:F1</cell></row><row><cell>Multinomial Naive Bayes</cell><cell>0.1899</cell><cell>0.6987 0.7575</cell><cell>0.7281</cell></row><row><cell>Linear SVC</cell><cell>0.2249</cell><cell>0.7343 0.7492</cell><cell>0.7418</cell></row><row><cell>Bernoulli Naive Bayes</cell><cell>0.2361</cell><cell>0.7254 0.7642</cell><cell>0.7488</cell></row><row><cell>XGBoost</cell><cell>0.2515</cell><cell>0.7264 0.7725</cell><cell>0.7495</cell></row><row><cell>Multilayer Perceptron</cell><cell>0.2380</cell><cell>0.7163 0.7722</cell><cell>0.7443</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,90.49,356.67,195.56"><head>Table 3</head><label>3</label><figDesc>Results for all runs submitted for Task 1 in the ALL INSTANCES category</figDesc><table coords="10,147.59,118.58,298.08,167.47"><row><cell></cell><cell></cell><cell cols="2">Soft vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell>Cross Entropy</cell></row><row><cell>SMS_1</cell><cell>26</cell><cell>0.3142</cell><cell>0.547</cell><cell>1.665</cell></row><row><cell>SMS_2</cell><cell>27</cell><cell>0.3142</cell><cell>0.547</cell><cell>1.665</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_3</cell><cell>52</cell><cell>0.2469</cell><cell>0.5233</cell><cell>0.6717</cell></row><row><cell>SMS_1</cell><cell>54</cell><cell>0.2369</cell><cell>0.517</cell><cell>0.6787</cell></row><row><cell>SMS_2</cell><cell>55</cell><cell>0.2369</cell><cell>0.517</cell><cell>0.6787</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_3</cell><cell>53</cell><cell>-0.5638</cell><cell>0.4052</cell><cell></cell></row><row><cell>SMS_1</cell><cell>55</cell><cell>-0.6017</cell><cell>0.399</cell><cell></cell></row><row><cell>SMS_2</cell><cell>56</cell><cell>-0.6017</cell><cell>0.399</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,88.99,305.95,356.67,195.56"><head>Table 4</head><label>4</label><figDesc>Results for all runs submitted for Task 1 in the ES category</figDesc><table coords="10,147.59,334.04,298.08,167.47"><row><cell></cell><cell></cell><cell cols="2">Soft vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell>Cross Entropy</cell></row><row><cell>SMS_1</cell><cell>26</cell><cell>0.3061</cell><cell>0.506</cell><cell>1.6297</cell></row><row><cell>SMS_2</cell><cell>27</cell><cell>0.3061</cell><cell>0.506</cell><cell>1.6297</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_3</cell><cell>52</cell><cell>0.1937</cell><cell>0.4663</cell><cell>0.6766</cell></row><row><cell>SMS_1</cell><cell>54</cell><cell>0.1809</cell><cell>0.4578</cell><cell>0.6888</cell></row><row><cell>SMS_2</cell><cell>55</cell><cell>0.1809</cell><cell>0.4578</cell><cell>0.6888</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_3</cell><cell>53</cell><cell>-0.5495</cell><cell>0.3557</cell><cell></cell></row><row><cell>SMS_1</cell><cell>56</cell><cell>-0.6433</cell><cell>0.3392</cell><cell></cell></row><row><cell>SMS_2</cell><cell>57</cell><cell>-0.6433</cell><cell>0.3392</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,88.99,90.49,356.67,195.56"><head>Table 5</head><label>5</label><figDesc>Results for all runs submitted for Task 1 in the EN category</figDesc><table coords="11,147.59,118.58,298.08,167.47"><row><cell></cell><cell></cell><cell cols="2">Soft vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell>Cross Entropy</cell></row><row><cell>SMS_1</cell><cell>27</cell><cell>0.253</cell><cell>0.5871</cell><cell>1.7046</cell></row><row><cell>SMS_2</cell><cell>28</cell><cell>0.253</cell><cell>0.5871</cell><cell>1.7046</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_3</cell><cell>49</cell><cell>0.3015</cell><cell>0.5875</cell><cell>0.6658</cell></row><row><cell>SMS_1</cell><cell>51</cell><cell>0.2833</cell><cell>0.5764</cell><cell>0.665</cell></row><row><cell>SMS_2</cell><cell>52</cell><cell>0.2833</cell><cell>0.5764</cell><cell>0.665</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_3</cell><cell>51</cell><cell>-0.641</cell><cell>0.4581</cell><cell></cell></row><row><cell>SMS_1</cell><cell>52</cell><cell>-0.6411</cell><cell>0.4581</cell><cell></cell></row><row><cell>SMS_2</cell><cell>53</cell><cell>-0.6411</cell><cell>0.4581</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,88.99,90.49,340.20,146.55"><head>Table 6</head><label>6</label><figDesc>Results for all runs submitted for Task 2 in the ALL INSTANCES category</figDesc><table coords="13,147.59,118.58,281.60,118.46"><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_1</cell><cell>19</cell><cell>-0.0892</cell><cell>0.6533</cell><cell>0.3654</cell></row><row><cell>SMS_3</cell><cell>20</cell><cell>-0.1226</cell><cell>0.6461</cell><cell>0.3504</cell></row><row><cell>SMS_2</cell><cell>23</cell><cell>-0.2571</cell><cell>0.6175</cell><cell>0.3246</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_2</cell><cell>6</cell><cell>-5.627</cell><cell>0.6978</cell><cell></cell></row><row><cell>SMS_3</cell><cell>8</cell><cell>-5.9579</cell><cell>0.6894</cell><cell></cell></row><row><cell>SMS_1</cell><cell>10</cell><cell>-6.0774</cell><cell>0.6863</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,88.99,256.94,340.20,146.54"><head>Table 7</head><label>7</label><figDesc>Results for all runs submitted for Task 2 in the ES category</figDesc><table coords="13,147.59,285.03,281.60,118.46"><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_1</cell><cell>20</cell><cell>-0.1314</cell><cell>0.6185</cell><cell>0.328</cell></row><row><cell>SMS_3</cell><cell>21</cell><cell>-0.1314</cell><cell>0.6185</cell><cell>0.328</cell></row><row><cell>SMS_2</cell><cell>23</cell><cell>-0.2641</cell><cell>0.5892</cell><cell>0.3295</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_1</cell><cell>2</cell><cell>-4.693</cell><cell>0.6871</cell><cell></cell></row><row><cell>SMS_3</cell><cell>3</cell><cell>-4.693</cell><cell>0.6871</cell><cell></cell></row><row><cell>SMS_2</cell><cell>8</cell><cell>-5.3008</cell><cell>0.6697</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="13,88.99,423.39,340.20,146.55"><head>Table 8</head><label>8</label><figDesc>Results for all runs submitted for Task 2 in the EN category</figDesc><table coords="13,147.59,451.48,281.60,118.46"><row><cell></cell><cell></cell><cell cols="2">Hard vs Hard</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Hard</cell><cell>ICM-Hard Norm</cell><cell>F1</cell></row><row><cell>SMS_1</cell><cell>18</cell><cell>-0.0554</cell><cell>0.6949</cell><cell>0.3828</cell></row><row><cell>SMS_3</cell><cell>20</cell><cell>-0.1261</cell><cell>0.6805</cell><cell>0.3592</cell></row><row><cell>SMS_2</cell><cell>25</cell><cell>-0.2786</cell><cell>0.6495</cell><cell>0.3142</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard vs Soft</cell><cell></cell></row><row><cell>Run</cell><cell>Rank</cell><cell>ICM-Soft</cell><cell>ICM-Soft Norm</cell><cell></cell></row><row><cell>SMS_2</cell><cell>8</cell><cell>-6.3311</cell><cell>0.7271</cell><cell></cell></row><row><cell>SMS_3</cell><cell>16</cell><cell>-8.2171</cell><cell>0.6857</cell><cell></cell></row><row><cell>SMS_1</cell><cell>19</cell><cell>-8.4236</cell><cell>0.6812</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,556.50,393.33,10.91;14,112.66,570.05,394.53,10.91;14,112.66,583.60,395.17,10.91;14,112.66,597.15,394.53,10.91;14,112.66,610.69,393.32,10.91;14,112.66,624.24,339.02,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,452.58,556.50,53.41,10.91;14,112.66,570.05,390.09,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,329.46,597.15,177.72,10.91;14,112.66,610.69,393.32,10.91;14,112.66,624.24,161.78,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF 2023)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsirika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09">September 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,637.79,393.33,10.91;14,112.66,651.34,393.33,10.91;15,112.33,86.97,393.65,10.91;15,112.66,100.52,328.66,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,452.58,637.79,53.41,10.91;14,112.66,651.34,393.33,10.91;15,112.33,86.97,89.78,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>AmigÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,467.81,86.97,38.17,10.91;15,112.66,100.52,297.96,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,393.33,10.91;15,112.66,127.61,253.14,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="15,228.64,114.06,277.34,10.91;15,112.66,127.61,122.59,10.91">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,141.16,394.62,10.91;15,112.66,154.71,393.32,10.91;15,112.33,168.26,394.86,10.91;15,112.66,181.81,397.48,10.91;15,112.66,197.80,74.11,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,223.33,141.16,260.58,10.91">Evaluating extreme hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.399</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.399.doi:10.18653/v1/2022.acl-long.399" />
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,154.71,393.32,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5809" to="5819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="15,112.66,208.91,394.03,10.91;15,112.66,222.46,394.04,10.91;15,112.41,236.01,37.91,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,193.22,208.91,160.64,10.91">Idiot&apos;s bayes: Not so stupid after all?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/1403452" />
	</analytic>
	<monogr>
		<title level="j" coord="15,361.28,208.91,145.41,10.91;15,112.66,222.46,156.78,10.91">International Statistical Review / Revue Internationale de Statistique</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="385" to="398" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,249.56,303.23,10.91" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,394.53,10.91;15,112.66,276.66,394.51,10.91;15,112.66,292.65,129.02,7.90" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="15,239.71,263.11,121.71,10.91">Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30162-4_415</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-30162-4_415.doi:10.1007/978-0-387-30162-4_415" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="928" to="932" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,303.75,368.23,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,161.23,303.75,204.86,10.91">Neural networks: a comprehensive foundation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Prentice Hall PTR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,317.30,393.33,10.91;15,112.66,330.85,394.52,10.91;15,112.66,344.40,394.04,10.91;15,112.66,357.95,233.99,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,211.45,317.30,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="15,421.80,317.30,84.19,10.91;15,112.66,330.85,394.52,10.91;15,112.66,344.40,36.69,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,371.50,393.98,10.91;15,112.41,385.05,48.96,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,253.72,371.50,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,376.65,371.50,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,398.60,394.61,10.91;15,112.66,412.15,393.33,10.91;15,112.33,425.70,395.33,10.91;15,112.66,439.25,314.57,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,283.23,398.60,204.21,10.91">GloVe: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://aclanthology.org/D14-1162.doi:10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,412.15,393.33,10.91;15,112.33,425.70,236.37,10.91">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,452.79,393.33,10.91;15,112.66,466.34,393.98,10.91;15,112.41,479.89,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,331.70,452.79,174.29,10.91;15,112.66,466.34,52.01,10.91">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,177.72,466.34,288.70,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,493.44,393.32,10.91;15,112.66,506.99,179.85,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m" coord="15,259.02,493.44,246.97,10.91;15,112.66,506.99,56.17,10.91">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,520.54,394.53,10.91;15,112.48,534.09,395.34,10.91;15,112.66,547.64,394.53,10.91;15,112.66,561.19,395.17,10.91;15,112.66,574.74,394.53,10.91;15,112.30,588.29,395.36,10.91;15,112.66,601.84,330.68,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,160.36,588.29,316.02,10.91">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>ManÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/,softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,615.39,393.33,10.91;15,112.26,628.93,393.73,10.91;15,112.41,642.48,323.32,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,424.65,615.39,81.34,10.91;15,112.26,628.93,215.02,10.91">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j" coord="15,335.32,628.93,170.66,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,656.03,394.53,10.91;15,112.66,669.58,395.01,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="15,271.63,656.03,230.91,10.91">Pooling methods in deep neural networks, a review</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gholamalinezhad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2009.07485" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.66,100.52,359.27,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,162.86,86.97,343.13,10.91;16,112.66,100.52,201.21,10.91">Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,336.90,100.52,104.40,10.91">NATO Neurocomputing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
