<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,83.21,363.21,17.22;1,89.29,105.13,412.08,17.22;1,89.29,127.04,179.66,17.22">I2C-UHU at CLEF-2023 EXIST task: Leveraging Ensembling Language Models to Detect Multilingual Sexism in Social Media</title>
				<funder ref="#_TzkDfHE #_S2KTnyq #_p4DxT6u #_69NqnEK #_a3bWaGy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,64.10,11.96"><forename type="first">Pablo</forename><surname>Cordón</surname></persName>
							<email>pablo.cordon113@alu.uhu.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">I2C Research Group</orgName>
								<orgName type="institution">University of Huelva</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.05,156.89,59.23,11.96"><forename type="first">Jacinto</forename><surname>Mata</surname></persName>
							<email>mata@uhu.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">I2C Research Group</orgName>
								<orgName type="institution">University of Huelva</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.83,156.89,78.08,11.96"><forename type="first">Victoria</forename><surname>Pachón</surname></persName>
							<email>vpachon@dti.uhu.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">I2C Research Group</orgName>
								<orgName type="institution">University of Huelva</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.37,156.89,104.99,11.96"><forename type="first">Juan</forename><surname>Luis Domínguez</surname></persName>
							<email>juan.dominguez@dti.uhu.es</email>
							<affiliation key="aff0">
								<orgName type="laboratory">I2C Research Group</orgName>
								<orgName type="institution">University of Huelva</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,83.21,363.21,17.22;1,89.29,105.13,412.08,17.22;1,89.29,127.04,179.66,17.22">I2C-UHU at CLEF-2023 EXIST task: Leveraging Ensembling Language Models to Detect Multilingual Sexism in Social Media</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">33C7D3A25B421B298E9E5FAC6AC65410</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Transformers</term>
					<term>Hyperparameter</term>
					<term>Ensembles</term>
					<term>Twitter</term>
					<term>Sexism</term>
					<term>Hate Speech</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approaches developed by the I2C Group to participate on sub-task 1 in the CLEF 2023 task EXIST: sEXism Identification in Social neTworks. Our main contribution is to show the benefits of translating a bilingual dataset to a single language, as well as the effectiveness of using a group of classifiers based on transformers architecture. By combining different models, the individual advantages were exploited, resulting in better performance than using a single model. Moreover, the importance of choosing suitable hyperparameters during the model training process was highlighted by the results. Through careful experimentation and evaluation of different hyperparameter combinations, the settings that achieved the best performance for the given task were found. In our experiments we fine-tuned several pre-trained language models and decided to ensemble the three models that reached the best F1-scores. With this approach, we achieved an ICM-Hard score of 0.5075, ranking 25th in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sexism is a form of discrimination or prejudice based on gender that affects the dignity and rights of women and other marginalized groups. It is often expressed through language, such as insults, stereotypes, jokes, threats, or harassment. As Rodriguez-Sánchez et al (2020) expounds <ref type="bibr" coords="1,89.29,475.02,11.28,10.91" target="#b0">[1]</ref>, social media platforms, such as Twitter, are widely used for communication and information sharing, but they also provide a space for sexist discourse and hate speech. Detecting and preventing sexism in social media is a challenging and important task for ensuring a respectful and inclusive online environment.</p><p>In this paper, we present a novel approach for on-line sexism detection in Spanish and English tweets using transformer models and natural language processing participating in subtask 1 of EXIST: sEXism Identification in Social neTworks from CLEF 2023 <ref type="bibr" coords="1,396.90,556.31,11.50,10.91" target="#b1">[2]</ref>. This task is a binary classification on sexism in a multi-lingual dataset of annotated tweets in Spanish and English, collected using different popular expressions and terms, commonly used to understimate the role of women in our society <ref type="bibr" coords="2,219.12,86.97,11.35,10.91" target="#b2">[3]</ref>. Given the sate-of-the-art results Transformers have achieved in various natural language processing tasks <ref type="bibr" coords="2,284.46,100.52,12.51,10.91" target="#b3">[4]</ref>, all the models we developed are based on this technology. Regarding our final results, we translated our dataset to only one language, trained three models and built an ensemble to improve the binary classifier performance <ref type="bibr" coords="2,451.07,127.61,11.43,10.91" target="#b4">[5]</ref>.</p><p>Transformers are neural network architectures that rely on self-attention mechanisms to encode the semantic and syntactic information of natural language. They have achieved stateof-the-art results in various natural language processing tasks, such as machine translation, text classification, and sentiment analysis. We use different transformer models, such as BERT, RoBERTa, and XLM-RoBERTa, to encode the tweets and classify them into sexist or non-sexist categories. We also explore the use of multilingual models that can handle both languages simultaneously.</p><p>The following section reviews some relevant literature. Section 3 presents Task 1 and the Corpus provided by the organizers. Section 4 and 5 report the experimental methodology and evaluation results. Section 6 concludes the study and outlines some directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several studies have addressed the problem of sexism detection on social media using natural language processing and machine learning techniques. Most of them rely on deep neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to learn features from text and classify it as sexist or not. For example, in <ref type="bibr" coords="2,422.43,362.38,13.00,10.91" target="#b5">[6]</ref> A. Kalra and A. Zubiaga (2021) used CNNs and RNNs to detect sexism in tweets and gabs, a social network known for hosting extremist content. They also applied data augmentation and transfer learning with BERT and DistilBERT models to improve their performance.</p><p>However, deep neural networks require large amounts of labeled data to achieve good results, which is often scarce or imbalanced for sexism detection. Moreover, they may not capture the nuances and subtleties of sexist language, which can vary across cultures, contexts, and domains. Therefore, some recent studies have explored the use of transformer models, which are pre-trained on large corpora of text and can leverage contextual information and semantic representations. For example, in <ref type="bibr" coords="2,231.66,484.32,12.69,10.91" target="#b6">[7]</ref> M. Schütz et al (2021) used multilingual transformer models based on BERT and XLM-R to perform binary and multiclass sexism detection on tweets in Spanish and English. They also used unsupervised pre-training and fine-tuning with additional datasets to adapt the transformers to the task . Similarly, in <ref type="bibr" coords="2,364.42,524.97,13.00,10.91" target="#b7">[8]</ref> A. <ref type="bibr" coords="2,393.71,524.97,88.28,10.91" target="#b7">Gómez et al. (2021)</ref> used ensembles of transformer models trained on different background corpora and fine-tuned on the EXIST2021 dataset, which contains tweets and gabs labeled for sexism.</p><p>Transformer-based models have shown great potential in detecting hate messages, specially against women. Researchers have trained these models on large, annotated datasets and finetuned them for hate speech detection, achieving significant improvements in identifying and categorizing discriminatory content. Transformer models have been instrumental in capturing the complex linguistic features of hate speech, enabling more effective moderation of online platforms, the protection of vulnerable communities, and the creation of a safer and more inclusive digital environment <ref type="bibr" coords="2,222.71,646.91,11.43,10.91" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>This paper is focused on subtask 1: Binary classification. The corpus was provided by organizers, and contains three datasets, one for training, other for validation and the last one for testing. Training and validation datasets have 11 columns each, which are: id EXIST, lang, tweet, number annotators, annotators, gender annotators, age annotators, labels task1, labels task2, labels task3 and split. As test dataset is not labelled yet, it only has columns id EXIST, lang, tweet, and split. The columns we used for training our models in subtask 1 were:</p><p>• id EXIST: Id of the tweet in the competition.</p><p>• lang: Language of the tweet ("en" for English or "es" for Spanish).</p><p>• tweet: Raw text of the tweet.</p><p>• labels task 1: This field contains one label per annotator, indicating if the tweet is sexist or not. A small processing had to be done in order to achieve a binary label. The majority of labels in the list was elected as the definitive label (we labelled 0 for NO and 1 for YES), and in case of tie, the row was eliminated from the dataset.</p><p>Regarding datasets, training dataset consists of 6920 tweets, validation dataset 1038, and test 2076. Tables <ref type="table" coords="3,146.25,328.94,5.07,10.91" target="#tab_0">1</ref> and<ref type="table" coords="3,173.19,328.94,5.07,10.91">2</ref> show the distribution of the labels and languages for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>The methods used in this study involved several important steps. First, we converted the entire dataset into English and Spanish using the Google translator python library Googletrans A , to A https://py-googletrans.readthedocs.io/en/latest/ compare how well multilingual and single-language classification models performed. Next, we searched for the best hyperparameters to train the models for this specific task. Lastly, we built a classification model by combining the three top models we found and using hard voting methods to improve the results. The pre-trained models selected, obtained from the Hugging Face Transformers library B , were:</p><p>• xml-roberta-base <ref type="bibr" coords="4,204.11,180.22,16.39,10.91" target="#b9">[10]</ref>: Multilingual version of RoBERTa.</p><p>• bert-base-multilingual <ref type="bibr" coords="4,230.87,195.12,16.39,10.91" target="#b10">[11]</ref>: Multilingual version of BERT.</p><p>• PlanTL-GOB-ES/roberta-base-bne <ref type="bibr" coords="4,288.02,210.03,16.54,10.91" target="#b11">[12]</ref>: RoBERTa base model pre-trained using the largest Spanish corpus known to date. • bert-base-uncased <ref type="bibr" coords="4,208.92,238.48,16.44,10.91" target="#b10">[11]</ref>: Base version of BERT, trained with 110 million parameters in English language. • bert-large-uncased <ref type="bibr" coords="4,209.14,266.93,16.22,10.91" target="#b10">[11]</ref>: Advanced version of BERT, trained with 340 million parameters.</p><p>• NLP-LTU/distilbert-sexism-detector <ref type="bibr" coords="4,300.67,281.84,16.55,10.91" target="#b12">[13]</ref>: Distilled version of BERT, with 40% less parameters but 95% of performance compared to BERT. This models has been fine-tuned previously with a sexism classification corpus.</p><p>To compare the results obtained by the different models and developed strategies, all models were fine-tuned using the training dataset, and their performance was measured with the validation dataset. Multilingual models used the datasets with tweets both in English and Spanish, as provided by the organizers. The roberta-base-bne model uses the datasets fully translated into Spanish, while the rest of the models use them fully translated into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preprocessing</head><p>The same small text preprocessing was done for all datasets in all languages. It consisted on removing links, usernames, numbers, words with length of one character and emojis. Hashtags (#) were preserved as they can be a key indicator of sexism in some tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hyperparameter optimization</head><p>The hyperparameter optimization <ref type="bibr" coords="4,241.94,515.19,17.87,10.91" target="#b13">[14]</ref> is a crucial step for models fine-tuning. For this reason, multiple iterations of training and evaluation were performed using different combinations of the most significant Transformers hyperparameters. The platform used for this purpose was WandB (Weights &amp; Biases) C , which provides a clear graphical interface for tracking and visualizing machine learning experiments. Table <ref type="table" coords="4,310.28,569.38,5.14,10.91" target="#tab_1">3</ref> shows the hyperparameters space used in this experimentation phase.</p><p>The optimal hyperparameters for each model are presented in Table <ref type="table" coords="4,410.04,596.48,3.79,10.91">4</ref>. Using these values, we obtained the results shown in Table <ref type="table" coords="4,263.31,610.03,3.68,10.91" target="#tab_2">5</ref>. These results demonstrate the benefits of conducting a proper hyperparameter search for fine-tuning, and using specific-language models instead of multilingual for this task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ensemble Technique</head><p>The final output was determined by a hard voting technique <ref type="bibr" coords="5,370.94,480.34,16.42,10.91" target="#b14">[15]</ref>, which selected the most frequent prediction among the models. This ensured a more reliable and robust prediction based on consensus. The ensemble <ref type="bibr" coords="5,217.36,507.44,17.82,10.91" target="#b15">[16]</ref> and model voting techniques improved the overall predictive performance by combining the strengths and diversity of multiple models, resulting in more precise and accurate predictions. The models included in the ensembles were the three best ones in terms of f1-scores, namely bert-base, bert-large, and distilbert-sexism.</p><p>Since the three models were pre-trained with English tweets, the dataset provided by the organizers was fully translated into English for the evaluation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we present the final results submitted to the competition. The predictions were evaluated using the official competition metrics, specifically the ICM-Hard and F1-Score. Results were given for the full test dataset, only the Spanish tweets and only the English Tweets. The final prediction was constructed using a voting scheme among the three models. The achieved ICM-Hard and F1-Score for this task including all languages was respectively 0.5075 and 0.7611, resulting in a 25th position out of 69 participants. Table <ref type="table" coords="6,394.34,226.12,5.13,10.91" target="#tab_3">6</ref> shows our ranking and results achieved in each language.</p><p>The results obtained demonstrate the effectiveness of our approach, and how translating all the datasets to English for training made us rank higher classifying tweets in this language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, our approach for sEXism Identification in Social neTworks and the results obtained in subtask 1 for CLEF 2023 are presented. Our proposal consisted on fine-tuned transformerbased models using different approaches for each classifier to optimize the results. Six different models were fine-tuned using hyperparameter optimization, generating more that 300 different combinations. Finally an ensemble of the three best models was done using hard voting for binary classification achieving a ICM-Hard of 0.5075 and a f1-score of 0-7611, being ranked 25 out of 69 participants.</p><p>In future works we will apply data augmentation using backtranslation and other techniques as well as new ensembles approaches. Moreover, we will conduct a thorough hyperparameter search to train the models in order to enhance the detection of sexist messages on social media.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,357.07,296.45,211.40"><head>Table 1</head><label>1</label><figDesc>Distribution of labels</figDesc><table coords="3,88.99,386.41,296.45,182.05"><row><cell cols="3">Label Training Validation</cell><cell></cell></row><row><cell>0</cell><cell>3367</cell><cell>479</cell><cell></cell></row><row><cell>1</cell><cell>2697</cell><cell>455</cell><cell></cell></row><row><cell>tie</cell><cell>856</cell><cell>104</cell><cell></cell></row><row><cell>Total</cell><cell>6920</cell><cell>1038</cell><cell></cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Distribution of languages</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell cols="3">Training Validation Test</cell></row><row><cell>Spanish</cell><cell>3660</cell><cell>549</cell><cell>1098</cell></row><row><cell>English</cell><cell>3260</cell><cell>489</cell><cell>978</cell></row><row><cell>Total</cell><cell>6920</cell><cell>1038</cell><cell>2076</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,85.74,89.60,426.04,204.06"><head>Table 3</head><label>3</label><figDesc>Hyperparameters used in experimentation</figDesc><table coords="5,85.74,121.17,426.04,172.49"><row><cell></cell><cell cols="2">Hyperparameter</cell><cell></cell><cell>Values</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Num Epochs</cell><cell></cell><cell>[2, 3, 4, 6]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Learning Rate</cell><cell cols="2">[5e-05, 4e-05, 3e-05, 2e-05]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Batch Size</cell><cell></cell><cell>[16, 32]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Weigth Decay</cell><cell cols="2">[0, 0.1, 0.01]</cell><cell></cell><cell></cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Best Hyperparameters per model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hyperparam</cell><cell cols="6">xlm-roberta bert-multilang roberta-bne bert-base bert-large distilbert-sexism</cell></row><row><cell>Nº Epochs</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell>Learning Rate</cell><cell>3e-05</cell><cell>4e-05</cell><cell>5e-05</cell><cell>3e-05</cell><cell>3e-05</cell><cell>5e-05</cell></row><row><cell>Batch Size</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>16</cell></row><row><cell>Weigth Decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0</cell><cell>0.1</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,314.49,267.49,118.58"><head>Table 5</head><label>5</label><figDesc>Best results obtained with hyperparameter search</figDesc><table coords="5,238.79,346.05,117.69,87.01"><row><cell>Model</cell><cell>f1-score</cell></row><row><cell>xlm-roberta</cell><cell>0.8019</cell></row><row><cell>bert-multilingual</cell><cell>0.8327</cell></row><row><cell>roberta-base-bne</cell><cell>0.8293</cell></row><row><cell>bert-base</cell><cell>0.8379</cell></row><row><cell>bert-large</cell><cell>0.8508</cell></row><row><cell>distilbert-sexism</cell><cell>0.8436</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,89.60,309.46,82.71"><head>Table 6</head><label>6</label><figDesc>Final results achieved in the competition</figDesc><table coords="6,196.82,121.17,201.63,51.15"><row><cell cols="4">Languages ICM-Hard f1-score ranking</cell></row><row><cell>All</cell><cell>0.5075</cell><cell>0.7611</cell><cell>25</cell></row><row><cell>English</cell><cell>0.5453</cell><cell>0.7532</cell><cell>13</cell></row><row><cell>Spanish</cell><cell>0.46687</cell><cell>0.7672</cell><cell>32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="4,89.29,658.45,3.51,5.98;4,93.30,660.02,84.73,8.97;4,89.29,669.47,3.86,5.98;4,93.65,671.04,75.15,8.97"><p>B https://huggingface.co/ C https://wandb.ai/site</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This paper is part of the <rs type="projectName">I+D+i Project</rs> titled "<rs type="projectName">Conspiracy Theories and hate speech online: Comparison of patterns in narratives and social networks about COVID-19, immigrants</rs>, refugees and <rs type="projectName">LGBTI people</rs> [<rs type="projectName">NONCONSPIRA-HATE!</rs>]", <rs type="grantNumber">PID2021-123983OB-I00</rs>, funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033/</rs> and by "<rs type="programName">ERDF A way of making Europe</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TzkDfHE">
					<orgName type="project" subtype="full">I+D+i Project</orgName>
				</org>
				<org type="funded-project" xml:id="_S2KTnyq">
					<orgName type="project" subtype="full">Conspiracy Theories and hate speech online: Comparison of patterns in narratives and social networks about COVID-19, immigrants</orgName>
				</org>
				<org type="funded-project" xml:id="_p4DxT6u">
					<orgName type="project" subtype="full">LGBTI people</orgName>
				</org>
				<org type="funded-project" xml:id="_69NqnEK">
					<idno type="grant-number">PID2021-123983OB-I00</idno>
					<orgName type="project" subtype="full">NONCONSPIRA-HATE!</orgName>
				</org>
				<org type="funding" xml:id="_a3bWaGy">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033/</idno>
					<orgName type="program" subtype="full">ERDF A way of making Europe</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,605.24,398.40,10.91;6,107.59,618.78,342.32,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,355.19,605.24,150.80,10.91;6,107.59,618.78,248.60,10.91">Automatic Classification of Sexism in Social Networks: An Empirical Study on Twitter Data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,363.61,618.78,54.37,10.91">IEEE Access</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,632.33,399.60,10.91;6,107.59,645.88,398.40,10.91;6,107.59,659.43,400.08,10.91;7,107.59,86.97,398.66,10.91;7,107.59,100.52,25.94,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,118.60,645.88,387.38,10.91;6,107.59,659.43,73.42,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,188.30,659.43,315.01,10.91;7,107.59,86.97,367.86,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<meeting>the Fourteenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,107.59,114.06,399.60,10.91;7,107.59,127.61,398.40,10.91;7,107.59,141.16,398.40,10.91;7,107.59,154.71,398.40,10.91;7,107.59,168.26,99.10,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,118.60,127.61,387.38,10.91;7,107.59,141.16,165.23,10.91">Overview of EXIST 2023 -Learning with Disagreement for Sexism Identification and Characterization (Extended Overview)</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,280.18,141.16,225.80,10.91;7,107.59,154.71,107.82,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,181.81,64.95,10.91;7,191.14,181.81,10.07,10.91;7,216.64,181.81,29.78,10.91;7,261.85,181.81,38.51,10.91;7,315.80,181.81,10.60,10.91;7,341.84,181.81,61.46,10.91;7,418.73,181.81,20.08,10.91;7,454.25,181.81,8.02,10.91;7,477.69,181.81,30.13,10.91;7,107.59,195.36,399.06,10.91;7,106.39,208.91,262.43,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>García-Subies</surname></persName>
		</author>
		<ptr target="&lt;https://oa.upm.es/view/institution/ETSI=5FInformatica/" />
		<title level="m" coord="7,261.85,181.81,38.51,10.91;7,315.80,181.81,10.60,10.91;7,341.84,181.81,61.46,10.91;7,418.73,181.81,20.08,10.91;7,454.25,181.81,8.02,10.91;7,477.69,181.81,30.13,10.91;7,107.59,195.36,82.98,10.91">Modelos de Transformers para la clasificación de texto</title>
		<meeting><address><addrLine>UPM</addrLine></address></meeting>
		<imprint>
			<publisher>E.T.S. de Ingenieros Informáticos</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Tesis</note>
</biblStruct>

<biblStruct coords="7,107.59,222.46,376.82,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,188.53,222.46,290.81,10.91">Ensemble learning: pattern classification using ensemble methods</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,236.01,398.68,10.91;7,107.59,249.56,142.34,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,186.79,236.01,285.13,10.91">Automatic sexism detection with multilingual transformer models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schütz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04908</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,263.11,398.40,10.91;7,107.59,276.66,219.27,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,232.76,263.11,273.23,10.91;7,107.59,276.66,39.20,10.91">Sexism identification in tweets and gabs using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03612</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,290.20,398.40,10.91;7,107.59,303.75,103.07,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,205.92,290.20,214.65,10.91">Transformer ensembles for sexism detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gómez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15905</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,107.59,317.30,398.40,10.91;7,107.59,330.85,398.40,10.91;7,107.18,344.40,400.65,10.91;7,107.59,357.95,246.24,10.91" xml:id="b8">
	<monogr>
		<ptr target="https://saturdays.ai/2021/10/04/deteccion-de-discurso-de-odio-en-redes-sociales-mediante-transformers-y-natural-language-processing/" />
		<title level="m" coord="7,107.59,317.30,398.40,10.91;7,107.59,330.85,97.36,10.91">Detección de Discurso de Odio en Redes Sociales mediante Transformers y Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021-10-04">2021. October 4. June 2, 2023</date>
		</imprint>
	</monogr>
	<note>from Saturdays.AI website</note>
</biblStruct>

<biblStruct coords="7,112.66,371.50,394.52,10.91;7,107.59,385.05,398.40,10.91;7,107.59,398.60,221.88,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,317.01,385.05,188.98,10.91;7,107.59,398.60,70.89,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,412.15,393.33,10.91;7,107.59,425.70,354.67,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,379.28,412.15,126.71,10.91;7,107.59,425.70,247.97,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,439.25,394.53,10.91;7,107.41,452.79,398.58,10.91;7,107.59,466.34,77.65,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gutiérrez-Fandiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estapé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pàmies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Llop-Palao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Silveira-Ocampo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Carrino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07253</idno>
		<title level="m" coord="7,313.23,452.79,116.00,10.91">Spanish language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,479.89,394.61,10.91;7,107.59,493.44,256.57,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,337.18,479.89,170.09,10.91;7,107.59,493.44,150.85,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR, abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,506.99,395.01,10.91;7,107.41,520.54,377.86,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,194.54,506.99,308.09,10.91">Optimizing Hyperparameters: A Comparative Study of Search Methods</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1234/jmlr.2022.12345</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,107.41,520.54,168.37,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1256" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,534.09,395.17,10.91;7,107.59,547.64,349.60,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,208.64,534.09,299.18,10.91;7,107.59,547.64,11.69,10.91">Exploring Hard Voting Techniques for Predictions Using Transformers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1234/jai.2023.67890</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,125.90,547.64,139.44,10.91">Journal of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="567" to="589" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,561.19,393.53,10.91;7,107.59,574.74,398.40,10.91;7,107.59,588.29,399.60,10.91;7,107.59,601.84,190.42,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,244.74,561.19,261.45,10.91;7,107.59,574.74,80.91,10.91">An effective ensemble deep learning framework for text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kora</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2021.11.001</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2021.11.001" />
	</analytic>
	<monogr>
		<title level="j" coord="7,199.93,574.74,306.06,10.91;7,107.59,588.29,36.18,10.91">Journal of King Saud University -Computer and Information Sciences</title>
		<idno type="ISSN">1319-1578</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="8825" to="8837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
