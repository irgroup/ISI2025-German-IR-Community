<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,105.69,377.38,15.42;1,89.29,127.60,71.50,15.43;1,89.29,149.94,90.08,11.96">Overview of Touch√© 2023: Argument and Causal Retrieval Extended Version*</title>
				<funder>
					<orgName type="full">Basal ANID</orgName>
				</funder>
				<funder ref="#_tseCStD #_HbTC4he">
					<orgName type="full">National Center for Artificial Intelligence CENIA</orgName>
				</funder>
				<funder>
					<orgName type="full">OpenWebSearch.eu</orgName>
				</funder>
				<funder ref="#_pfpfgM8">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_ybXZKAY">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,175.85,112.73,11.96"><forename type="first">Alexander</forename><surname>Bondarenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universit√§t Jena</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.24,175.85,55.47,11.96"><forename type="first">Maik</forename><surname>Fr√∂be</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universit√§t Jena</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.36,175.85,77.09,11.96"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universit√§t Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.09,175.85,85.77,11.96"><forename type="first">Ferdinand</forename><surname>Schlatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universit√§t Jena</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.90,189.79,83.67,11.96"><forename type="first">Valentin</forename><surname>Barriere</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Centro Nacional de Inteligencia Artificial (CENIA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.22,189.79,68.92,11.96"><forename type="first">Brian</forename><surname>Ravenet</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Universit√© Paris</orgName>
								<address>
									<settlement>Saclay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.79,189.79,71.47,11.96"><forename type="first">L√©o</forename><surname>Hemamou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Sanofi R&amp;D</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.42,189.79,57.88,11.96"><forename type="first">Simon</forename><surname>Luck</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Alma Mater Studiorum -Universit√† di Bologna</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.10,203.74,99.63,11.96"><forename type="first">Jan</forename><forename type="middle">Heinrich</forename><surname>Reimer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universit√§t Jena</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.38,203.74,58.99,11.96"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bauhaus-Universit√§t Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.01,203.74,76.81,11.96"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Leipzig University and ScaDS.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.82,203.74,77.83,11.96"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universit√§t Jena</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,105.69,377.38,15.42;1,89.29,127.60,71.50,15.43;1,89.29,149.94,90.08,11.96">Overview of Touch√© 2023: Argument and Causal Retrieval Extended Version*</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F64E1090429C34F1DDCE80C6D2AEE7B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument retrieval</term>
					<term>Causal retrieval</term>
					<term>Image retrieval</term>
					<term>Stance classification</term>
					<term>Argument quality</term>
					<term>Causality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is a report on the fourth edition of the Touch√© lab on argument and causal retrieval hosted at CLEF 2023. With the goal of creating a collaborative platform for research on computational argumentation and causality, we organized four shared tasks: (a) argument retrieval for controversial topics (retrieve web documents that contain high-quality argumentation and detect the documents' stances), (b) causal retrieval (retrieve web documents that contain causal statements and detect the documents' causal stances), (c) image retrieval for arguments (retrieve images that support a pro or con stance towards some controversial topic), and (d) multilingual multi-target stance classification (detect the stance of comments on proposals from an online multilingual participatory democracy platform).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Making informed decisions and forming opinions on a matter often involves not only weighing pro and con arguments but also considering cause-effect relationships <ref type="bibr" coords="1,396.71,552.56,11.28,10.91" target="#b1">[2]</ref>. To make decisions or to get an overview of different standpoints on some topic, a lot of facts, opinions, arguments, etc. can be found on the Web. However, conventional web search engines are primarily optimized for returning relevant results that match a query but not for argument or causal analyses (e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Lab Overview and Statistics</head><p>We used TIRA <ref type="bibr" coords="2,155.49,467.27,12.83,10.91" target="#b6">[7]</ref> as the submission platform for Touch√© 2023 through which the participants could either submit software or upload run files. <ref type="foot" coords="2,300.95,479.06,3.71,7.97" target="#foot_4">5</ref> We particularly encouraged software submissions, as they increase reproducibility and also allow for later running the software on different data with the same format (e.g., on topics and collections from a previous year). To submit software, a team had to deploy their approach in a Docker image that they then uploaded to their dedicated Docker registry in TIRA. Software submissions in TIRA are immutable, and after a Docker image has been submitted, a team could specify a to-be-executed command-thus, the same Docker image could be used for multiple software submissions (e.g., by changing some parameters). A team could upload as many Docker images as needed (the images were not public while the shared tasks were ongoing). To improve reproducibility, TIRA executes software in a sandbox by blocking the internet connection. This ensures that the software is fully installed in the Docker image, which simplifies running the software later. For the execution, the participants could select the resources out of 4 options: (1) 1 CPU core with 10 GB RAM, (2) 2 cores with 20 GB RAM, (3) 4 cores with 40 GB RAM, or (4) 1 CPU core with 10 GB RAM and 1 Nvidia GeForce GTX 1080 GPU with 7 GB RAM. A software could be run multiple times using different resources to investigate the scalability and reproducibility (e.g., whether the software executed on a GPU yields the same results as on a CPU). TIRA used a Kubernetes cluster with 1,620 CPU cores, 25.4 TB RAM, and 24 GeForce GTX 1080 GPUs to schedule and execute the software submissions, allocating the resources that the participants selected for their submissions.</p><p>Overall, for the fourth edition of the Touch√© lab, we received 41 registrations from 21 countries (vs. 58 registrations in 2022). But from the 41 registered teams, only 7 teams actively participated by submitting valid results (1 team in Task 1, 1 in Task 2, 3 in Task 3, and 2 in Task 4)-5 of the 7 teams submitted software. Note that the number of active teams substantially decreased compared to the previous editions of Touch√© <ref type="bibr" coords="3,284.69,276.66,41.27,10.91">(23 active</ref> teams in 2022, 27 in 2021, and 17 in 2020). We thus decided to pause the argument and causal retrieval tasks for now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 1: Argument Retrieval for Controversial Questions</head><p>The goal of the first task was to support individuals who search for opinions and arguments on socially important, controversial topics like "Are social networking sites good for our society?". The previous task iterations explored different granularities of argument retrieval and analysis: a focused crawl of debates on various controversial topics from several online debating portals and the arguments' concise gist <ref type="bibr" coords="3,209.22,403.03,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,222.64,403.03,7.42,10.91" target="#b8">9,</ref><ref type="bibr" coords="3,232.25,403.03,12.23,10.91" target="#b9">10]</ref>. For the fourth edition of the task, our focus shifted towards retrieving argumentative web documents from the web crawl corpus ClueWeb22-B <ref type="bibr" coords="3,464.91,416.58,16.35,10.91" target="#b10">[11]</ref>. The topics and manual judgments from the previous task iterations were provided to the participants to enable approaches that leverage training and parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Definition</head><p>Given a controversial topic and a collection of web documents, the task was to retrieve and rank documents by relevance to the topic, ideally also ranking higher documents that contain high-quality arguments, and to (optionally) detect the document's stance. Participants of Task 1 needed to retrieve documents from the ClueWeb22-B crawl for 50 search topics.</p><p>To lower the entry barrier for participants who could not index the whole ClueWeb22-B corpus on their side, we provided a first-stage retrieval possibility via the API of the BM25Fbased search engine ChatNoir <ref type="bibr" coords="3,220.46,574.70,17.76,10.91" target="#b11">[12]</ref> and a smaller version of the corpus that contained one million documents per topic. To identify arguments (claims and premises) in documents, participants could use any existing argument tagging tool such as the TARGER API <ref type="bibr" coords="3,420.72,601.80,18.07,10.91" target="#b12">[13]</ref> hosted on our servers or develop their own tools if necessary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative</head><p>Highly relevant arguments discuss social networking in general or particular networking sites, and its/their positive or negative effects on society. Relevant arguments discuss how social networking affects people, without explicit reference to society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Description</head><p>Topics. For the task on argument retrieval for controversial questions (Task 1), we provided 50 search topics representing various debated societal issues. These issues were chosen from the online debate portals (debatewise.org, idebate.org, debatepedia.org, and debate.org), with the largest number of user-generated comments and thus representing the highest societal interest.</p><p>For each such case, we formulated a topic's title (i.e., a question on a controversial issue), a description specifying the particular search scenario, and a narrative that served as a guideline for the human assessors (see Table <ref type="table" coords="4,245.96,385.61,5.07,10.91" target="#tab_0">1</ref> for an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Collection.</head><p>The retrieval collection was the ClueWeb22 (Category B) corpus <ref type="bibr" coords="4,488.23,414.36,17.76,10.91" target="#b10">[11]</ref> that contains 200 million multilingual most frequently visited web pages like Wikipedia articles, news websites, etc. The indexed corpus was available via the ChatNoir API <ref type="foot" coords="4,432.43,439.71,3.71,7.97" target="#foot_5">6</ref> and its Python module<ref type="foot" coords="4,122.55,453.26,3.71,7.97" target="#foot_6">7</ref> integrated in PyTerrier <ref type="bibr" coords="4,233.95,455.01,16.25,10.91" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Setup</head><p>Our human assessors labeled the ranked lists of documents submitted by the task participants both for their general topical relevance and for the rhetorical argument quality <ref type="bibr" coords="4,441.01,518.29,16.19,10.91" target="#b14">[15]</ref>, i.e., "wellwrittenness": (1) whether the document contains arguments and whether the argument text has a good style of speech, (2) whether the argument text has a proper sentence structure and is easy to follow, and (3) whether it includes profanity, has typos, etc. Also, the documents' stance towards the search topics was labeled as 'pro', 'con', 'neutral', or 'no stance'. Analogously to the previous Touch√© editions, our volunteer assessors annotated the document's topical relevance with three labels: 0 (not relevant), 1 (relevant), and 2 (highly relevant). The argument quality was also labeled with three classes: 0 (low quality or no arguments in the document), 1 (average quality), and 2 (high quality). We provided the annotators with detailed annotation guidelines, including examples. In the training phase, we asked 4 annotators to label the same 20 randomly selected documents (initial Fleiss' kappa values: relevance ùúÖ=0.39 (fair agreement), argument quality ùúÖ=0.34 (fair agreement), and ùúÖ=0.51 (moderate agreement) for labeling the stance) and in the follow-up discussion clarified potential misinterpretations. Afterward, each annotator independently judged the results for disjoint subsets of the topics (i.e., each topic was judged by one annotator only). We used this annotation policy due to a high annotation workload. Our human assessors labeled in total 747 documents pooled from 8 runs using a top-10 pooling strategy implemented in the TrecTools library <ref type="bibr" coords="5,430.46,181.81,16.25,10.91" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Submitted Approaches and Evaluation Results</head><p>In 2023, only one team participated in Task 1 and submitted seven runs. We, thus, decided to evaluate all the participant's runs and an additional baseline. Below, we summarize and describe the submitted approaches to the task and evaluation results.</p><p>The task's baseline run by Puss in Boots used the results that ChatNoir <ref type="bibr" coords="5,414.50,272.19,17.89,10.91" target="#b11">[12]</ref> returned for the topics' titles used as queries without any pre-processing. ChatNoir is an Elasticsearch-based search engine for the ClueWeb and Common Crawl web corpora that employs BM25F ranking (fields: document title, keywords, main content, and the whole document) and SpamRank scores <ref type="bibr" coords="5,119.93,326.38,16.36,10.91" target="#b16">[17]</ref>. The document stance for the baseline run was predicted by zero-shot prompting the Flan-T5 model <ref type="bibr" coords="5,171.58,339.93,17.76,10.91" target="#b17">[18]</ref> <ref type="foot" coords="5,189.34,338.18,3.71,7.97" target="#foot_7">8</ref> after summarizing the document's main content with BART <ref type="bibr" coords="5,461.55,339.93,16.09,10.91" target="#b18">[19]</ref>. <ref type="foot" coords="5,481.66,338.18,3.71,7.97" target="#foot_8">9</ref> The summarization step was necessary to meet the Flan-T5 input limit of 512 tokens.</p><p>Team Renji Abarai <ref type="bibr" coords="5,189.69,367.03,18.07,10.91" target="#b19">[20]</ref> submitted seven runs in total. Their baseline run used the top-10 results returned by ChatNoir for the pre-processed topics' titles used as queries. During pre-processing, stop words were first removed using their own handcrafted list of terms; the remaining query terms were then lowercased and lemmatized with the Stanza NLP library <ref type="bibr" coords="5,487.54,407.68,16.10,10.91" target="#b20">[21]</ref>. For the other six runs, the results of the baseline run were re-ranked based on the predicted argument quality and predicted document stance. Argument quality was predicted using either a meta-classifier (random forests) trained on the class predictions and class probabilities of six base classifiers or by prompting ChatGPT. Each base classifier (feedforward neural network, LightGBM <ref type="bibr" coords="5,138.35,475.42,16.22,10.91" target="#b21">[22]</ref>, logistic regression, na√Øve Bayes, SVM, and random forests) was trained in two variants: (1) using a set of 32 handcrafted features (e.g., sentiment, spelling errors, the ratio of arguments in documents, etc.) and (2) using documents represented with the instruction-based fine-tuned embedding model INSTRUCTOR <ref type="bibr" coords="5,278.21,516.07,16.33,10.91" target="#b22">[23]</ref>. All the classifiers were trained on the manual argument quality labels from the Touch√© 2021 Task 1 <ref type="bibr" coords="5,319.56,529.62,11.28,10.91" target="#b8">[9]</ref>, which was also used to select examples for few-shot prompting ChatGPT. The resulting ranked lists submitted by Renji Abarai differed in the type of argument quality classifiers used for re-ranking, whether predicted classes or probabilities were used, or if the predicted document stance was considered. The document stance for all the runs was predicted using ChatGPT.</p><p>Table <ref type="table" coords="5,126.99,597.37,5.07,10.91">2</ref> shows the results for all evaluated runs with respect to relevance, argument quality, and stance detection (more detailed results for each submitted run, including the 95% confidence intervals, are in Tables <ref type="table" coords="5,194.80,624.46,10.35,10.91" target="#tab_0">10</ref> and<ref type="table" coords="5,227.51,624.46,10.35,10.91" target="#tab_0">11</ref> in Appendix B). Overall, none of the submitted participant</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results of all runs submitted to Task 1: Argument Retrieval for Controversial Questions. Reported are the mean nDCG@10 for relevance and argument quality and macro-averaged F 1 for stance detection. Since Renji Abarai re-ranked the same set of documents for all the runs, this yields identical stance detection results. The task baseline run by Puss in Boots is shown in bold. results outperformed the argumentation-agnostic BM25F-based task baseline. This is due to the worse effectiveness of the team's initial retrieval results ('team_baseline' run in Table <ref type="table" coords="6,498.15,310.48,4.25,10.91">2</ref>) that were used in the re-ranking step. Five participants' re-ranking strategies were able to improve over their initial ranking. The most effective participant approach ('stance_ChatGPT' run in Table <ref type="table" coords="6,144.95,351.13,4.08,10.91">2</ref>) exploited ChatGPT to predict the argument quality and stance. Then, a two-step re-ranking strategy was used: (1) move the 'no stance' documents to the bottom of the ranked list, and then (2) re-rank the remaining documents based on the predicted argument quality labels in the descending order. Thus, the promising future direction can be to apply the proposed re-ranking approach to the official task baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 2: Evidence Retrieval for Causal Questions</head><p>The goal of the Touch√© 2023 lab's second task was to support users who search for answers to causal yes-no questions like "Do microwave ovens cause cancer?", supported by relevant evidence instances. In general, such causal questions ask if something causes or does not cause something else.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task Definition</head><p>Given a causality-related topic and a collection of web documents, the task was to retrieve and rank documents by relevance to the topic. For 50 search topics, participants of Task 2 needed to retrieve documents from the ClueWeb22-B crawl that contain relevant causal evidence. An optional task was to detect the document's causal stance. A document can provide supportive evidence (a causal relationship between the cause and effect from the topic holds), refutative (a causal relationship does not hold), or neutral (in some cases holds and in some does not). Like in Task 1, ChatNoir <ref type="bibr" coords="6,178.83,635.62,17.91,10.91" target="#b11">[12]</ref> could be used for first-stage retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative</head><p>Highly relevant documents will provide information on a potential causal connection between microwave ovens and cancer. This includes documents stating or giving evidence that the first is (or is not) a cause of the other. Documents stating that there is not enough evidence to decide either way are also highly relevant. Relevant documents may contain implicit information on whether the causal relationship exists or does not exist. Documents are not relevant if they either mention one or both concepts, but do not provide any information about their causal relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Description</head><p>Topics. The 50 search topics for Task 2 described scenarios where users search for confirmation of whether some causal relationship holds. For example, a user may want to know the possible reason for a current physical condition. Each of these topics had a title (i.e., a causal question), cause and effect entities, a description specifying the particular search scenario, and a narrative serving as a guideline for the assessors (see Table <ref type="table" coords="7,308.88,438.85,3.53,10.91" target="#tab_2">3</ref>). The topics were manually selected from a corpus of causal questions <ref type="bibr" coords="7,207.87,452.40,17.83,10.91" target="#b23">[24]</ref> and a graph of causal statements <ref type="bibr" coords="7,375.36,452.40,17.83,10.91" target="#b24">[25]</ref> such that they spanned a diverse set of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Collection.</head><p>The same document collection as in Task 1 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Setup</head><p>Relevance assessments were gathered with volunteer human assessors. The assessors were instructed to label documents as not relevant (0), relevant (1), or highly relevant <ref type="bibr" coords="7,431.02,557.98,10.27,10.91" target="#b1">(2)</ref>. The direction of causality was considered, i.e., a document stating that B causes A was considered off-topic (not relevant) for the topic "Does A cause B?". The document's stance was also labeled to evaluate the optional stance detection task. The labeling procedure was analogous to Task 1, where volunteer assessors participated in training and a discussion. Agreement on the same 20 randomly selected documents across 4 annotators was measured with Fleiss' kappa. Before the discussion, the agreement was ùúÖ = 0.58 for relevance and ùúÖ = 0.55 for stance assessment (both indicate a moderate agreement). After discussing discrepancies, similar to Task 1, each Relevance results of all runs submitted to Task 2: Evidence Retrieval for Causal Questions. Reported are the mean nDCG@5 for relevance and macro-averaged F 1 for stance detection; Puss in Boots baseline is in bold. The dagger ‚Ä† indicates a statistically significant improvement (ùëù &lt; 0.05, Bonferroni corrected) over the Puss in Boots baseline. Team He-Man did not detect the stance. annotator labeled a disjoint set of topics. We pooled the top-5 documents from each submitted run (plus additional baseline) and labeled 718 documents in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Submitted Approaches and Evaluation Results</head><p>One team He-Man <ref type="bibr" coords="8,173.43,315.93,17.96,10.91" target="#b25">[26]</ref> participated in Task 2 and submitted three runs. Like the baseline run Puss in Boots, all three participant runs used ChatNoir <ref type="bibr" coords="8,325.22,329.48,17.75,10.91" target="#b11">[12]</ref> for first-stage retrieval. For two runs, first, the cause and effect events were extracted from the topic title field using dependency tree parsing. Next, query expansion and query reformulation approaches were applied. In the query expansion approach, the topic title was expanded with semantically related concepts from the CauseNet, a graph of causal relations <ref type="bibr" coords="8,263.37,383.68,16.42,10.91" target="#b24">[25]</ref>. For this, all relations in the CauseNet-Precision variant were embedded using BERT <ref type="bibr" coords="8,246.64,397.23,16.09,10.91" target="#b26">[27]</ref>. Next, the embedding's cosine similarity was compared with the embedding of the topic's relation. The top-5 terms from the documents linked to the matched CauseNet relation were then used to expand the query. The second approach, the query reformulation technique, fed the deconstructed topic title in a semi-structured JSON format to ChatGPT. The chatbot was then prompted to generate new query variants, exchanging causes, effects, and causal phrases. All three query variants (original topic title, expanded query, and reformulated query) were then submitted to ChatNoir. Finally, all approaches re-ranked the results using a position bias. Documents containing the causal relationship from the topic earlier in the document were ranked higher. To detect the position of the relation, the same dependency tree parsing developed for the query deconstruction was used. The task's baseline run of Puss in Boots additionally predicted the document stance by first summarizing a document's main content with BART <ref type="bibr" coords="8,325.42,546.27,16.20,10.91" target="#b18">[19]</ref>, <ref type="foot" coords="8,345.67,544.51,7.41,7.97" target="#foot_9">10</ref> and then zero-shot prompting the Flan-T5 model <ref type="bibr" coords="8,156.15,559.82,16.25,10.91" target="#b17">[18]</ref>. <ref type="foot" coords="8,176.47,558.06,7.41,7.97" target="#foot_10">11</ref>Table <ref type="table" coords="8,127.92,573.37,5.17,10.91" target="#tab_3">4</ref> shows the evaluation results for Task 2 (more detailed results for each submitted run, including the 95% confidence intervals, is in Table <ref type="table" coords="8,332.10,586.92,10.01,10.91" target="#tab_0">12</ref> in Appendix B). We report nDCG@5 for relevance-based retrieval effectiveness and macro-averaged F 1 for stance detection. The Puss in Boots baseline was more effective in terms of relevance than the two participant runs that used query expansion. However, the participant run, which only applied re-ranking,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Relevance results divided by topic type for Task 2: Evidence Retrieval for Causal Questions. Inverse topics include four inverted topic pairs. The difficult topics column includes one topic pair with an unrealistic causal search scenario. Plain topics include all other topics. Reported are the macro-averaged arithmetic mean and macro-averaged harmonic mean of nDCG@5. Puss in Boots baseline is in bold. statistically significantly outperformed the baseline. This suggests that the participants' query expansion techniques degrade the first-stage retrieval results, and the re-ranking approach applied afterward cannot compensate for the substantially worse performance of the query expansion. The participating team opted not to detect the stance. Therefore, only the baseline run could be evaluated for stance detection, achieving an F 1 -score of 0.256. We additionally investigate whether the retrieval approaches correctly handle the causal direction of queries. We, therefore, chose 5 of the 50 topics to be the inverse direction of an already existing topic. Four of these topic pairs are realistic scenarios, e.g., 'Can depression lead to a lack of sleep?' and 'Can a lack of sleep lead to depression?'. The final pair contains a somewhat unrealistic and challenging scenario: 'Can earthquakes cause tsunamis?' and 'Can tsunamis cause earthquakes?' (i.e., is it feasible that a giant tsunami causes an earthquake?). Table <ref type="table" coords="9,116.74,398.87,5.17,10.91">5</ref> lists the evaluation results split by topic type. For topic pairs, we report the macroaveraged arithmetic and harmonic mean. The arithmetic mean shows overall effectiveness. The harmonic mean reveals if the approaches are equally effective for both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>We find that the baseline run is substantially less effective on the inverted topics than on the plain topics. The participant approach, which re-ranks according to the causal relation, performs much better. Additionally, the substantial difference between the arithmetic and harmonic mean for the inverse topics shows that the approaches are not equally effective for both directions. Effectiveness for one of the directions is usually much higher than the inverse direction. Finally, none of the approaches retrieved a relevant document for the challenging inverse topic, as revealed by the harmonic mean of 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 3: Image Retrieval for Arguments</head><p>The goal of the third task was to provide argumentation support through image search. The retrieval of relevant images should provide both a quick visual overview of frequent arguments on some topic and compelling images to support one's argumentation. To this end, the second edition of this task continued with the retrieval of images which can be posted to either indicate an agreement or disagreement to some stance on a given topic. Images should be retrieved as two separate lists, similar to a textual argument search (e.g., https://args.me).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Task Definition</head><p>Given a controversial topic and a collection of web documents with images, the task was to retrieve for each stance (pro and con) images that indicate support for that stance. Participants of Task 3 should retrieve and rank images, possibly utilizing the corresponding web documents, from a focused crawl of 55,691 images and for a given set of 50 topics (which were used by other tasks in previous years) <ref type="bibr" coords="10,194.81,161.73,16.08,10.91" target="#b27">[28]</ref>. Like in the last edition of this task, the focus is on providing users with an overview of public opinions on controversial topics, for which we envision a system that provides not only textual but also visual support for each stance in the form of images. Participants were able to use the approximately 6,000 relevance judgments from the last edition of the task for training supervised approaches <ref type="bibr" coords="10,300.78,215.93,16.41,10.91" target="#b28">[29]</ref>. <ref type="foot" coords="10,321.29,214.17,7.41,7.97" target="#foot_11">12</ref> Similar to the other tasks, participants were free to use any additional existing tools and datasets or develop their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data Description</head><p>Topics. Task 3 employs 50 controversial topics from earlier Touch√® editions (e.g., used in 2021), but which were not used in the first edition of this task. As for Task 1 (cf. Section 3), we provided for each topic a title, description, and narrative. The description and narrative were adapted as needed to fit the image retrieval setting. Document Collection. This task's document collection stems from a focused crawl of 55,691 images and associated web pages from late 2022. We downloaded the top-100 images and associated web pages from Google's image search for 2,209 queries. Nearly half of the queries (namely 1,050) were created like in the first edition of this task, by appending filter words like "good, " "meme, " "stats, " "reasons, " or "effects" to a manually created query for each topic. The remaining 1,159 queries were collected from participants in an open call, which allowed anyone to submit queries until the end of December 2022. Of these queries, 557 were created manually (57 by team Neville Longbottom, 250 by team Hikaru Sulu, and 250 by us), and the remaining were created using ChatGPT by team Neville Longbottom: they asked ChatGPT for a list of pro and con arguments for each topic, then for an image description illustrating the respective arguments, and then for a search query to match the description. From the search results we attempted to download 147,264 images, but discarded 5,666 for which we could not download the image, 6,619 for which the image was more than 2,000 pixels wide or high, <ref type="foot" coords="10,498.07,509.45,7.41,7.97" target="#foot_12">13</ref>20,696 for which an initial text recognition using Tesseract<ref type="foot" coords="10,357.38,523.00,7.41,7.97" target="#foot_13">14</ref> yielded more than 20 words, <ref type="foot" coords="10,498.07,523.00,7.41,7.97" target="#foot_14">15</ref>8,538 for which the web page could not be downloaded, 484 for which the web page contained no text, and 45,254 for which we could not find the image URL in the web page DOM. After a duplicate detection using pHash, <ref type="foot" coords="10,230.91,563.64,7.41,7.97" target="#foot_15">16</ref> the final dataset contains 55,691 images. The dataset contains various resources for each image, including the associated page for which it was retrieved as an HTML page and as a detailed web archive, <ref type="foot" coords="10,274.30,590.74,7.41,7.97" target="#foot_16">17</ref> information on how Google ranked the image, and information from Google's Cloud Vision API,<ref type="foot" coords="11,290.85,85.21,7.41,7.97" target="#foot_17">18</ref> e.g., detected text and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Setup</head><p>Our two volunteer human assessors labeled the ranked results by the task participants (i.e., the images) for their relevance to the topic's narrative. First, assessors decided whether an image is on topic (yes or no). If so, they also decided whether an image is relevant according to the pro-side of the narrative, its con-side, or both: 0 (not relevant), 1 (relevant), and 2 (highly relevant), though we did not distinguish between levels 1 and 2 in our evaluation. However, assessors were instructed that an image could not be highly relevant for both pro and con to indicate a preference. We provided the assessors with guidelines, discussed several examples, and discussed edge cases as they came up. Achieved Fleiss' ùúÖ values (measured on three topics for which all assessors labeled all images) were for on-topic 0.38 (fair), for pro 0.34 (fair), and for con 0.31 (fair). Without distinguishing levels 1 and 2, the agreement increases to 0.45 for pro (moderate) and 0.52 for con (moderate). Our human assessors labeled in total 6,692 images.</p><p>Although rank-based metrics for single image grids exist <ref type="bibr" coords="11,356.38,285.73,16.27,10.91" target="#b29">[30]</ref>, none have been proposed so far for a 'pro-con' layout. Therefore, participants' submitted results were evaluated by the ratio of relevant images among 20 retrieved images, namely 10 images per stance (precision@10). We again used three increasingly strict definitions of relevance, corresponding to three precision@10 evaluation measures: being on-topic, being in support of some stance (i.e., an image is "argumentative"), and being in support of the stance for which the image was retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Submitted Approaches and Evaluation Results</head><p>In total, three teams participated in Task 3 and submitted 12 runs in total, not counting the submitted queries described above. Table <ref type="table" coords="11,268.94,416.76,4.97,10.91">6</ref> shows the results of all submitted runs (more detailed results for each submitted run, including the 95% confidence intervals, are in Tables <ref type="table" coords="11,478.17,430.31,12.80,10.91" target="#tab_2">13,</ref><ref type="table" coords="11,494.39,430.31,8.53,10.91" target="#tab_3">14</ref>, and 15 in Appendix B). Overall, scores are considerably lower than last year, where precision@10 for stance relevance was as high as 0.425. We attribute this to the new set of topics, which contained much more questions that were hard to picture.</p><p>As a baseline (team Minsc), we used the model of <ref type="bibr" coords="11,311.42,484.50,16.09,10.91" target="#b30">[31]</ref>, which was developed by a collaboration of two teams that participated in last year's task: Aramis and Boromir. <ref type="foot" coords="11,390.79,496.30,7.41,7.97" target="#foot_18">19</ref> The approach employed standard retrieval and a set of handcrafted features for argumentativeness detection. For retrieval, the approach used Elasticsearch's BM25 (default settings: ùëò 1 =1.2 and ùëè=0.75) with each image (document) represented by the text from the web page around the image and text recognized in the image using Tesseract. 14 For argumentativeness detection, the approach used a neural network classifier based on thirteen different features (color properties, image type, and textual features), and trained on the ground-truth annotations from last year. The features are calculated from, amongst others, the query, the image text, the HTML text around the image, the interrelation and sentiments of the mentioned texts, and the colors in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Relevance results of all runs submitted to Task 3: Image Retrieval for Argumentation. Reported are the mean precision@10 for all three definitions of relevance; Minsc baseline is in bold. The dagger ‚Ä† indicates a statistically significant improvement (ùëù &lt; 0.05, Bonferroni corrected) over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run The approach used random stance assignment. Since this baseline performed much worse than anticipated, we expect a bug in the implementation. Team Hikaru Sulu submitted two valid runs. Their approach used CLIP <ref type="bibr" coords="12,417.75,381.66,17.89,10.91" target="#b31">[32]</ref> to calculate the similarity between keywords and images and retrieved, per topic, the images most similar to one of the keywords. For the first run, they used the topic title as a keyword, but for the second run, they extracted all nouns and verbs from the topic title and extended that list with synonyms and antonyms from WordNet <ref type="bibr" coords="12,226.28,435.86,16.41,10.91" target="#b32">[33]</ref>. The stance was determined randomly, which performed in their internal evaluation better than using different keywords for pro and con. As Table <ref type="table" coords="12,500.81,449.41,5.17,10.91">6</ref> shows, the extended list lead to retrieving more on-topic images, but less argumentative ones.</p><p>Team Jean-Luc Picard <ref type="bibr" coords="12,199.92,476.51,17.94,10.91" target="#b33">[34]</ref> submitted five valid runs. Their first run used the web page text indexed by PyTerrier's BM25 <ref type="bibr" coords="12,225.95,490.06,18.07,10.91" target="#b13">[14]</ref> (default settings: ùëò 1 =1.2 and ùëè=0.75). For the other runs, they used a pipeline of query preprocessing, the same BM25-based retrieval as their first run, stance detection, and re-ranking. For query preprocessing, they created a parse tree of the topic and filtered out frequent words to create a short query. The runs correspond to four different stance detection approaches: (1) random or (2) using a zero-shot classification based on the pre-trained BART MultiNLI model<ref type="foot" coords="12,238.81,556.05,7.41,7.97" target="#foot_19">20</ref> that assigns the image to pro, contra, or neutral (i.e., will be discarded) based on the (a) web page text, (b) the image text, or (c) both texts. After that, images were re-ranked: for each topic, images were generated with Stable Diffusion <ref type="bibr" coords="12,442.28,584.90,18.07,10.91" target="#b34">[35]</ref> using the preprocessed query as prompt, then SIFT keypoints were identified <ref type="bibr" coords="12,391.45,598.45,17.93,10.91" target="#b35">[36]</ref> in both retrieved and generated image and matched between the two images, and then the result list was re-ranked as per the number of matched keypoints in descending order. Similar to the internal evaluation of team Hikaru Sulu, a random stance assignment performed best.</p><p>Team Neville Longbottom <ref type="bibr" coords="13,217.98,86.97,18.07,10.91" target="#b36">[37]</ref> submitted five valid runs. They first employed ChatGPT <ref type="foot" coords="13,498.26,85.21,7.41,7.97" target="#foot_20">21</ref>to generate image descriptions for each topic and stance (neither description nor narrative was used). Then, they retrieved images with these descriptions, either (1) using the web page text close to the image indexed via PyTerrier's BM25 <ref type="bibr" coords="13,326.41,127.61,17.92,10.91" target="#b13">[14]</ref> (default settings: ùëò 1 =1.2 and ùëè=0.75) or (2) using CLIP <ref type="bibr" coords="13,168.75,141.16,18.04,10.91" target="#b31">[32]</ref> for ranking images by their similarity to the description. For runs 3-5, the approach continued by re-ranking the result list, either (a) by penalizing the BM25-score of an image with the BM25-score of the image for the respective other stance's description (re-ranking the results of run (1)) or (b) by using IBM's debater pro-con score <ref type="bibr" coords="13,446.46,181.81,18.06,10.91" target="#b37">[38]</ref> between the topic title and the text close to the image on the web page (2 runs; re-ranking results of run (1) or ( <ref type="formula" coords="13,137.81,208.91,3.45,10.91">2</ref>)). The CLIP method without re-ranking performed best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Task 4: Multilingual Multi-Target Stance Classification</head><p>In this edition of the Touch√© lab, we proposed a new task on multilingual multi-target stance classification of comments to proposals coming from an online participatory democracy platform. The goal of the fourth task was to build technologies that help analyze opinions on a wide range of socially important topics. Large-scale deployment of such technologies faces challenges like multilingualism or high variability of the topics of interest and hence is the target of this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Task Definition</head><p>Given a proposal on a socially important issue, its title, and its topic, the task was to classify whether a comment on the proposal is 'in favor', 'against, or 'neutral' towards the commented proposal. The participants needed to classify multilingual comments written in 6 different languages <ref type="foot" coords="13,133.00,410.35,7.41,7.97" target="#foot_21">22</ref> into the 3 stance classes. Comments to the proposals could be written in a different language than the proposal itself, and multiple comments could target the same proposal.</p><p>Within the task, we organized two subtasks: (1) Cross-debate Classification, where the participants were not allowed to use for training comments on those proposals that also had comments in the test set, and (2) All-data-available Classification, where the participants could use all the available data. Also, the participants could use any additional existing tools or previously published datasets like Debating Europe <ref type="bibr" coords="13,270.87,493.40,17.91,10.91" target="#b38">[39]</ref> or X-Stance <ref type="bibr" coords="13,346.33,493.40,16.25,10.91" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Data Description</head><p>The proposals and comments used in Task 4 stem from the Conference on the Future of Europe (CoFE), <ref type="foot" coords="13,157.46,554.92,7.41,7.97" target="#foot_22">23</ref> an online debating platform where users can write proposals and comment on the suggested ideas. The initially obtained dataset was comprised of 4,247 proposals and 20,102 comments written in 26 languages (24 official languages of the European Union plus Catalan and Esperanto) <ref type="bibr" coords="13,196.40,597.33,16.45,10.91" target="#b40">[41,</ref><ref type="bibr" coords="13,215.57,597.33,12.33,10.91" target="#b41">42]</ref>. As shown in Figure <ref type="figure" coords="13,324.75,597.33,3.74,10.91" target="#fig_0">1</ref>, English, German, and French were the most commonly used languages on the platform. An example of a proposal, a corresponding comment, and the stance of the comment is shown in Table <ref type="table" coords="14,356.59,285.34,3.74,10.91" target="#tab_7">7</ref>. 24  For developing stance classifiers, participants were provided with three datasets: (1) CF U : a large set of unlabeled comment-proposal pairs, (2) CF S : a large set of comment-proposal pairs where comment authors selected either 'in favor' or 'against' stance (no 'neutral' label was available for selection), and ( <ref type="formula" coords="14,238.43,339.53,3.88,10.91">3</ref>) CF E-D : a smaller set of comment-proposal pairs manually annotated by expert native speakers with three stance labels. The fourth dataset CF E-T was also labeled by experts and was used to evaluate submitted approaches (see Table <ref type="table" coords="14,439.47,366.63,3.64,10.91" target="#tab_8">8</ref>). The dataset contained texts written in 6 most common languages omitting Spanish (see Figure <ref type="figure" coords="14,474.47,380.18,3.65,10.91" target="#fig_0">1</ref>). For labeling the CF E-D and CF E-T datasets, untranslated comments and English translations of the proposals-to better understand the context-were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Submitted Approaches and Evaluation Results</head><p>Two teams participated in Task 4 and submitted 8 runs in total. Below, we briefly describe the participants' approaches plus additional baseline runs.</p><p>Team Cavalier was our baseline that implemented three stance classifiers. For Subtask 1 (cross-debate classification), we implemented two baseline classifiers: The first one (Cavalier Simple) simply always predicts the majority class ('in favor'). The second baseline (Cavalier) is based on the transformer-based multilingual masked language model XLM-R <ref type="bibr" coords="14,447.39,524.75,16.47,10.91" target="#b42">[43,</ref><ref type="bibr" coords="14,466.58,524.75,12.36,10.91" target="#b41">42]</ref>. This model was first fine-tuned on the X-Stance dataset <ref type="bibr" coords="14,313.20,538.30,17.77,10.91" target="#b39">[40]</ref> and the CF ùëÜ dataset to classify just two stance classes ('in favor' or 'against') and subsequently fine-tuned again on the Debating Europe dataset <ref type="bibr" coords="14,123.49,565.40,17.91,10.91" target="#b38">[39]</ref> to classify all three stance classes ('in favor', 'against', or 'neutral'). All comments on proposals appearing in the test set CF E-T were removed before fine-tuning. The baseline classifier for Subtask 2 (all-data-available classification) used the same model and analogous training steps as for Subtask 1, including comments on proposals that appeared in the test set.</p><p>Team Silver Surfer <ref type="bibr" coords="14,185.26,619.60,18.05,10.91" target="#b43">[44]</ref> submitted six valid runs to Subtask 2. Their stance classifiers were based on fine-tuning pre-trained English and multilingual transformer models: a RoBERTa  model <ref type="bibr" coords="15,119.28,423.32,16.18,10.91" target="#b44">[45]</ref>, <ref type="foot" coords="15,139.51,421.57,7.41,7.97" target="#foot_23">25</ref> an XLM-R model <ref type="bibr" coords="15,228.36,423.32,16.18,10.91" target="#b42">[43]</ref>, <ref type="foot" coords="15,248.59,421.57,7.41,7.97" target="#foot_24">26</ref> and two BERT models <ref type="bibr" coords="15,359.99,423.32,16.19,10.91" target="#b26">[27]</ref>. <ref type="foot" coords="15,380.23,421.57,7.41,7.97" target="#foot_25">27</ref> To increase the size of the training data, the team applied data augmentation using back-translation (i.e., translating texts to other languages and then back to the original language) <ref type="bibr" coords="15,346.47,450.42,17.76,10.91" target="#b45">[46]</ref> and used label spreading <ref type="bibr" coords="15,476.86,450.42,17.75,10.91" target="#b46">[47]</ref> to transfer labels from the CF E-D dataset to the CF U dataset. The team first fine-tuned a RoBERTa model (Run 2, comments translated to English) and an XLM-R model (Run 3, no translation) on the CF S dataset as well as on the CF U dataset after applying label spreading. Run 4 used the CF E-D dataset after data augmentation using back-translation to fine-tune an XLM-R model. For Run 5, the team fine-tuned a RoBERTa model on the comments from the CF E-D dataset, translating all comments to English. The team's Run 6 used a two-step training approach, where they first fine-tuned an English BERT model on binary stance classification based on the translated comments from the CF S dataset and subsequently fine-tuned the model to classify all three stance classes on translated comments from the CF E-D dataset. Finally, Team Silver Surfer combined comment metadata features (e.g., number of upvotes/downvotes, endorsements) and the output probabilities from six fine-tuned transformer models in an XGBoost classifier (Run 1):</p><p>(1) RoBERTa fine-tuned on the CF E-D dataset (comments translated to English, same as Run 5), (2) XLM-R fine-tuned on the CF E-D dataset (no translation), (3) RoBERTa fine-tuned on the CF S dataset (translation to English), (4) XLM-R fine-tuned on the CF S dataset (no translation), ( <ref type="formula" coords="16,92.90,417.35,3.94,10.91">5</ref>) English BERT fine-tuned on the CF S and CF E-D datasets (two-step fine-tuning, comments translated to English, same as Run 6), and (6) multilingual BERT fine-tuned on the CF S and CF E-D datasets (two-step fine-tuning, no translation, analogous to Run 6).</p><p>Team Queen of Swords <ref type="bibr" coords="16,204.39,457.99,18.07,10.91" target="#b47">[48]</ref> submitted two valid runs that were trained in a two-step finetuning setting on a combination of the labeled (CF S and CF E-D ) and unlabeled datasets (CF U ). To derive labels for the CF U dataset, the team first fine-tuned a multilingual BERT model <ref type="bibr" coords="16,480.29,485.09,17.79,10.91" target="#b26">[27]</ref>  <ref type="foot" coords="16,498.07,483.34,7.41,7.97" target="#foot_26">28</ref>only on the CF S and CF E-D datasets and used the fine-tuned model to predict labels on the CF U dataset. Their final BERT-based classifier was then again fine-tuned on the predicted labels for the CF U dataset (only the comment-proposal pairs whose labels were predicted above a certain probability were used) and the ground-truth labels from the CF S and CF E-D datasets. The team submitted their best configuration (probability threshold: 0.9) for Subtask 1 and used the same hyperparameters to fine-tune a BERT model on the larger dataset of Subtask 2.</p><p>The submitted approaches were evaluated using macro-averaged F 1 -scores (to account for the class imbalance; see CF E-T in Table <ref type="table" coords="16,265.83,593.48,4.25,10.91" target="#tab_8">8</ref>) and accuracy. Table <ref type="table" coords="16,369.17,593.48,5.17,10.91" target="#tab_9">9</ref> shows the evaluation results per language and across all languages in the test set. None of the submitted participant runs outperformed the baseline (Cavalier) in both subtasks.</p><p>Hungarian was the most challenging language for the baselines, which is the most morphosyntactically distant from the other languages. Conversely, the participants' classifiers were least effective for the German language and did not consistently struggle with Hungarian. Interestingly, the Cavalier baseline for Subtask 2 yielded better scores for Italian comments, even though most of the other runs performed better on English comments. However, we could not observe patterns regarding the use of multilingual transformer models or English models with translation before classification. Both approaches seemed to work equally well.</p><p>The best runs of Subtask 2 (our baseline and Silver Surfer Run 6) used a two-step fine-tuning setting, where the model was first trained to learn binary stance classification and subsequently was fine-tuned on three stance labels (including 'neutral'). These results indicate that breaking down stance classification into several steps can improve its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>The fourth edition of the Touch√© lab featured four tasks: (1) argument retrieval for controversial topics, (2) causal retrieval, (3) image retrieval for arguments, and (4) multilingual multi-target stance classification. In contrast to the prior iterations of the Touch√© lab, the main challenge for the participants was to apply argument analysis methodology on long web documents. Furthermore, we expanded the lab's scope by introducing new tasks on evidence retrieval for causal relationships and on predicting the stance of multilingual texts.</p><p>Overall, 7 teams participated in the tasks and submitted a total of 30 runs. The participants often used approaches that were effective in previous Touch√© editions, like sparse retrieval for an initial result set that then is re-ranked based on argument quality estimation and stance prediction. This year, many also used generative language models like ChatGPT as classifiers with various prompt-engineering techniques.</p><p>For Tasks 1 and 2, the teams used ChatNoir as their first-stage retrieval system and then re-ranked documents based on the predicted argument quality and stance (Task 1) or based on the presence of causal relationships (Task 2). Both re-ranking ideas improved the retrieval effectiveness compared to the first-stage retrieval results. For Task 3, the four most effective runs all employed CLIP embeddings to find images that are similar to some text, which means dense retrieval approaches outperformed traditional approaches this year. However, none of the systems could predict an image's stance better than random guessing. To classify the stance of multilingual texts (Task 4), the participants used BERT-based models, and the most successful runs employed a two-step fine-tuning: first, using binary stance labels and then learning the 'neutral' class. Overall, stance prediction remained the hardest task across all four tasks.</p><p>As the number of active teams substantially decreased in the fourth edition of Touch√© (7 active teams in 2023 compared to 23 in 2022, 27 in 2021, and 17 in 2020), we decided to pause the argument and causal retrieval tasks for now. Still, to support researchers working on argument or causal retrieval, all Touch√© resources will remain freely available, including the topics, the manual judgments (relevance, argument quality, stance), and the runs from the teams.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="15,89.29,375.99,417.29,8.93;15,89.29,387.99,133.71,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of proposals and comments per language (using ISO 3166-1 alpha-2 country codes) for the 4 datasets used in Task 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,378.08,110.12"><head>Table 1</head><label>1</label><figDesc>Example topic for Task 1: Argument Retrieval for Controversial Questions.</figDesc><table coords="4,129.29,123.35,337.78,77.25"><row><cell>Number</cell><cell>34</cell></row><row><cell>Title</cell><cell>Are social networking sites good for our society?</cell></row><row><cell>Description</cell><cell>Democracy may be in the process of being disrupted by social media,</cell></row><row><cell></cell><cell>with the potential creation of individual filter bubbles. So a user</cell></row><row><cell></cell><cell>wonders if social networking sites should be allowed, regulated, or</cell></row><row><cell></cell><cell>even banned.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,141.44,157.20,312.41,113.35"><head>Run Tag nDCG@10 F 1 macro Rel. Qua. Stance Puss in Boots ChatNoir [12]</head><label></label><figDesc></figDesc><table coords="6,141.44,188.86,304.29,81.70"><row><cell></cell><cell></cell><cell>0.834</cell><cell>0.831</cell><cell>0.203</cell></row><row><cell>Renji Abarai</cell><cell>stance_ChatGPT</cell><cell>0.747</cell><cell>0.815</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>stance-certainNO_ChatGPT</cell><cell>0.746</cell><cell>0.811</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>ChatGPT_mmGhl</cell><cell>0.718</cell><cell>0.789</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>ChatGPT_mmEQhl</cell><cell>0.718</cell><cell>0.789</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>meta_qual_score</cell><cell>0.712</cell><cell>0.771</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>team_baseline</cell><cell>0.708</cell><cell>0.766</cell><cell>0.599</cell></row><row><cell>Renji Abarai</cell><cell>meta_qual_prob</cell><cell>0.697</cell><cell>0.774</cell><cell>0.599</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,378.52,130.68"><head>Table 3</head><label>3</label><figDesc>Example topic for Task 2: Evidence Retrieval for Causal Questions.</figDesc><table coords="7,129.29,123.35,338.22,97.81"><row><cell>Number</cell><cell>39</cell></row><row><cell>Title</cell><cell>Do microwave ovens cause cancer?</cell></row><row><cell>Cause</cell><cell>microwave ovens</cell></row><row><cell>Effect</cell><cell>cancer</cell></row><row><cell>Description</cell><cell>A user has recently learned that radiation waves can cause cancer.</cell></row><row><cell></cell><cell>They are wondering if their microwave oven produces radiation</cell></row><row><cell></cell><cell>waves and if these are dangerous enough to cause cancer.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,32.19,8.93"><head>Table 4</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,88.99,90.49,375.46,150.29"><head>Table 7</head><label>7</label><figDesc>Example data instance for Task 4: Multilingual Multi-Target Stance Classification.</figDesc><table coords="14,132.49,123.35,331.96,117.42"><row><cell>Number</cell><cell>34</cell></row><row><cell>Title</cell><cell>Set up a program for returnable food packaging made from recy-</cell></row><row><cell></cell><cell>clable materials</cell></row><row><cell>Proposal</cell><cell>The European Union could set up a program for returnable food</cell></row><row><cell></cell><cell>packaging made from recyclable materials (e.g. stainless steel, glass).</cell></row><row><cell></cell><cell>These packaging would be produced on the basis of open standards</cell></row><row><cell></cell><cell>and cleaned according to [. . . ]</cell></row><row><cell>Comment</cell><cell>Ja, wir m√ºssen den Verpackungsm√ºl reduzieren</cell></row><row><cell>Label</cell><cell>In favor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="15,87.73,90.49,419.72,263.95"><head>Table 8</head><label>8</label><figDesc>Number of languages, comments, proposals, and stance label distribution of the datasets used in Task 4.en de fr es it hu nl el fi pl cs sv pt sk ca eo ro da bg lt hr lv et sl ga</figDesc><table coords="15,87.73,123.73,412.54,230.71"><row><cell></cell><cell>Dataset</cell><cell cols="3"># Languages # Comments # Proposals</cell><cell></cell><cell>Stance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">In favor Against Neutral</cell></row><row><cell></cell><cell>CF U</cell><cell>25</cell><cell>13,213</cell><cell>2,892</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CF S</cell><cell>25</cell><cell>7,002</cell><cell>2,731</cell><cell>77.7 %</cell><cell>22.3 %</cell><cell>-</cell></row><row><cell></cell><cell>CF E-D</cell><cell>4</cell><cell>1,414</cell><cell>936</cell><cell>53.3 %</cell><cell>8.3 %</cell><cell>38.4 %</cell></row><row><cell></cell><cell>CF E-T</cell><cell>6</cell><cell>1,228</cell><cell>771</cell><cell>55.2 %</cell><cell>17.7 %</cell><cell>27.1 %</cell></row><row><cell>Count</cell><cell>10 2 10 3 10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Proposals Comments (CFS) Comments (CFU) Comments (CFE D) Comments (CFE T)</cell></row><row><cell></cell><cell>10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Language</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="16,88.99,90.49,417.00,286.24"><head>Table 9</head><label>9</label><figDesc>Results of Task 4 (Multilingual Multi-Target Stance Classification) for two subtasks evaluated using macro-averaged F 1 (per language and overall, using ISO 3166-1 alpha-2 country codes) and overall accuracy (Acc.). Sorted by overall F 1 . Run IDs from the TIRA leaderboard are included for reference. The Cavalier baseline is shown in bold. Silver Surfer Run 6 2023-05-12-18-56-56 36.7 33.9 30.2 37.8 38.0 33.3 35.0 55.1 Silver Surfer Run 4 2023-05-12-18-40-25 35.3 30.4 26.1 35.3 34.8 27.8 32.9 53.7 Silver Surfer Run 1 2023-05-12-17-49-58 35.0 30.3 20.0 37.5 41.7 25.0 32.3 52.4 Queen of Swords 2023-05-19-07-51-35 35.1 31.5 26.2 40.9 43.0 35.7 32.4 61.6 Silver Surfer Run 5 2023-05-12-18-51-42 28.5 25.6 24.3 32.9 21.5 22.8 27.0 46.3 Silver Surfer Run 2 2023-05-12-18-29-46 26.3 21.1 18.9 19.1 30.0 23.3 23.9 46.1 Silver Surfer Run 3 2023-05-12-18-30-25 41.4 23.2 21.2 14.1 22.8 32.8 17.7 21.6</figDesc><table coords="16,345.12,156.53,143.84,9.72"><row><cell>F 1 macro</cell><cell>Acc.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,91.85,616.23,414.13,8.97;2,92.57,627.19,288.94,8.97"><p>'Touch√©' is commonly "used to acknowledge a hit in fencing or the success or appropriateness of an argument, an accusation, or a witty point. " [https://merriam-webster.com/dictionary/touche]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,638.15,91.74,8.97"><p>https://futureu.europa.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,92.57,649.11,74.41,8.97"><p>https://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,92.57,660.07,87.61,8.97"><p>https://touche.webis.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,92.57,671.03,48.17,8.97"><p>https://tira.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,92.57,660.06,161.81,8.97"><p>https://github.com/chatnoir-eu/chatnoir-api</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,92.57,671.01,183.12,8.97"><p>https://github.com/chatnoir-eu/chatnoir-pyterrier</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,92.57,649.11,413.42,8.97;5,92.57,660.07,55.81,8.97"><p>Pre-trained model: https://huggingface.co/google/flan-t5-base; maximum generated tokens: 3; the prompt is given in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="5,92.57,671.03,409.16,8.97"><p>Pre-trained model: https://huggingface.co/facebook/bart-large-cnn; minimum length: 64; maximum length: 256.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="8,95.35,649.12,409.16,8.97"><p>Pre-trained model: https://huggingface.co/facebook/bart-large-cnn; minimum length: 64; maximum length: 256.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="8,95.35,660.08,410.63,8.97;8,95.35,671.04,55.81,8.97"><p>Pre-trained model: https://huggingface.co/google/flan-t5-base; maximum generated tokens: 3; the prompt is given in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="10,95.35,616.24,156.43,8.97"><p>https://webis.de/data.html#touche-corpora</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12" coords="10,95.04,627.20,377.64,8.97"><p>As one use case for our task is getting a quick overview of arguments, we excluded overly large images</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13" coords="10,95.35,638.16,152.50,8.97"><p>https://github.com/tesseract-ocr/tesseract</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14" coords="10,95.08,649.12,410.90,8.97"><p>To sharpen our focus on images, this year we tried to exclude images that are merely screenshots of text documents</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_15" coords="10,95.35,660.08,234.99,8.97"><p>https://www.phash.org/; same procedure as in the previous year</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_16" coords="10,95.04,671.03,192.92,8.97"><p>Archived using https://github.com/webis-de/scriptor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_17" coords="11,95.35,649.08,115.40,8.97"><p>https://cloud.google.com/vision</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_18" coords="11,95.35,660.04,410.64,8.97;11,95.35,670.99,175.87,8.97"><p>Since no stance model convincingly outperformed naive baselines in their evaluation, we use the simple both-sides baseline that assigns each image to both stances</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_19" coords="12,95.35,671.02,177.33,8.97"><p>https://huggingface.co/facebook/bart-large-mnli</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_20" coords="13,95.35,649.06,104.65,8.97"><p>https://chat.openai.com/chat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_21" coords="13,95.35,660.02,205.80,8.97"><p>English, French, German, Greek, Hungarian, and Italian.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_22" coords="13,95.35,670.97,91.74,8.97"><p>https://futureu.europa.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_23" coords="15,95.35,649.12,130.85,8.97"><p>https://huggingface.co/roberta-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_24" coords="15,95.35,660.08,150.49,8.97"><p>https://huggingface.co/xlm-roberta-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_25" coords="15,95.35,671.04,370.67,8.97"><p>https://huggingface.co/bert-base-uncased and https://huggingface.co/bert-base-multilingual-uncased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_26" coords="16,95.35,671.04,190.91,8.97"><p>https://huggingface.co/bert-base-multilingual-cased</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs> in the project "<rs type="projectName">ACQuA 2.0: Answering Comparative Questions with Arguments"</rs> (project <rs type="grantNumber">376430233</rs>) as part of the <rs type="programName">priority program</rs> "<rs type="projectName">RATIO: Robust Argumentation Machines"</rs> (<rs type="grantNumber">SPP 1999</rs>). <rs type="person">V. Barriere</rs>'s work was funded by the <rs type="funder">National Center for Artificial Intelligence CENIA</rs> <rs type="grantNumber">FB210017</rs>, <rs type="funder">Basal ANID</rs>. This work has been partially supported by the <rs type="funder">OpenWebSearch.eu</rs> project (funded by the <rs type="funder">EU</rs>; <rs type="grantNumber">GA 101070014</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ybXZKAY">
					<idno type="grant-number">376430233</idno>
					<orgName type="project" subtype="full">ACQuA 2.0: Answering Comparative Questions with Arguments&quot;</orgName>
					<orgName type="program" subtype="full">priority program</orgName>
				</org>
				<org type="funded-project" xml:id="_tseCStD">
					<idno type="grant-number">SPP 1999</idno>
					<orgName type="project" subtype="full">RATIO: Robust Argumentation Machines&quot;</orgName>
				</org>
				<org type="funding" xml:id="_HbTC4he">
					<idno type="grant-number">FB210017</idno>
				</org>
				<org type="funding" xml:id="_pfpfgM8">
					<idno type="grant-number">GA 101070014</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Zero-shot Prompts</head><p>The zero-shot prompts used for the stance prediction baselines are given in Listing 1 (for Task 1, see Section 3) and in Listing 2 (for Task 2, see Section 4).</p><p>Given a query, predict the stance of a given text. The stance should be one of the following four labels: PRO: The text contains opinions or arguments in favor of the query "&lt;query&gt;". CON: The text contains opinions or arguments against the query "&lt;query&gt;". NEU: The text contains as many arguments in favor of as it contains against the query "&lt;query &gt;". UNK: The text is not relevant to the query "&lt;query&gt;", or it only contains factual information .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: &lt;summary&gt;</head><p>Listing 1: Zero-shot prompt to predict the stance of a document towards a query (Task 1). The placeholder &lt;query&gt; is replaced by the topic titles, and &lt;summary&gt; for a short summary of the retrieved document's text. The UNK label is mapped to NO.</p><p>Given a query, predict the stance of a given text. The stance should be one of the following four labels: SUP: According to the text, &lt;cause&gt; causes &lt;effect&gt;. REF: According to the text, &lt;cause&gt; does not cause &lt;effect&gt;. UNK: The text is not relevant to &lt;cause&gt; and &lt;effect&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: &lt;summary&gt;</head><p>Listing 2: Zero-shot prompt to predict the causal stance of a document towards a query (Task 2). The placeholders &lt;cause&gt; and &lt;effect&gt; are replaced with the query's cause and effect entities, and &lt;summary&gt; with a short summary of the retrieved document's text. The UNK label is mapped to NO. The NEU label is not considered in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Full Evaluation Results of Touch√© 2023: Argument and Causal Retrieval Table 10</head><p>Relevance results of all runs submitted to Task 1: Argument Retrieval for Controversial Questions.</p><p>Reported are the mean nDCG@10 and the 95% confidence intervals. The baseline Puss in Boots is shown in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 12</head><p>Relevance results of all runs submitted to Task 2: Evidence Retrieval for Causal Questions. Reported are the mean nDCG@5 and the 95% confidence intervals. The baseline Puss in Boots is shown in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 13</head><p>On-topic relevance results of all runs submitted to Task 3: Image Retrieval for Argumentation. Reported are the mean precision@10 and the 95% confidence intervals. The baseline Minsc is shown in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 14</head><p>Argumentativeness results of all runs submitted to Task 3: Image Retrieval for Argumentation. Reported are the mean precision@10 and the 95% confidence intervals. The baseline Minsc is shown in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,186.24,394.53,10.91;18,112.48,199.79,393.50,10.91;18,112.66,213.34,394.53,10.91;18,112.66,226.89,153.26,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,297.08,199.79,208.90,10.91;18,112.66,213.34,36.53,10.91">Overview of Touch√© 2023: Argument and causal retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schlatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravenet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hemamou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,174.39,213.34,116.80,10.91">Proceedings of CLEF 2023</title>
		<title level="s" coord="18,299.05,213.34,159.54,10.91">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2023<address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,240.44,393.32,10.91;18,112.66,253.99,224.42,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,153.43,240.44,184.17,10.91">The social psychology of decision making</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ajzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,361.32,240.44,144.66,10.91;18,112.66,253.99,68.30,10.91">Social Psychology: Handbook of Basic Principles</title>
		<imprint>
			<publisher>Guilford Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,267.54,394.53,10.91;18,112.66,281.08,395.01,10.91;18,112.41,294.63,38.81,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="18,428.87,267.54,72.97,10.91;18,127.16,281.08,114.19,10.91">Proceedings of TREC 1994</title>
		<meeting>TREC 1994</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
	<note>Okapi at TREC-3</note>
</biblStruct>

<biblStruct coords="18,112.66,308.18,393.32,10.91;18,112.66,321.73,397.91,10.91;18,112.33,336.52,48.22,9.20" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,303.91,308.18,202.08,10.91;18,112.66,321.73,22.77,10.91">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1031171.1031181</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,158.90,321.73,118.34,10.91">Proceedings of CIKM 2004</title>
		<meeting>CIKM 2004</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,348.83,393.33,10.91;18,112.66,362.38,393.33,10.91;18,112.66,375.93,394.04,10.91;18,112.66,389.48,339.05,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,399.83,348.83,106.15,10.91;18,112.66,362.38,303.10,10.91">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>R√ºckl√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html" />
	</analytic>
	<monogr>
		<title level="m" coord="18,439.39,362.38,66.59,10.91;18,112.66,375.93,57.86,10.91">Proceedings of NeurIPS 2021</title>
		<meeting>NeurIPS 2021</meeting>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,403.03,393.33,10.91;18,112.26,416.58,395.40,10.91;18,112.66,430.13,169.49,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,438.99,403.03,67.00,10.91;18,112.26,416.58,137.61,10.91">Simplified data wrangling with ir_datasets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463254</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,273.46,416.58,111.55,10.91">Proceedings of SIGIR 2021</title>
		<meeting>SIGIR 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2429" to="2436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,443.67,394.52,10.91;18,112.66,457.22,394.62,10.91;18,112.66,470.77,395.01,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,175.98,457.22,292.96,10.91">Continuous integration for reproducible shared tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,470.77,109.37,10.91">Proceedings of ECIR 2023</title>
		<title level="s" coord="18,229.03,470.77,151.29,10.91">Lecture Notes in Computer Science</title>
		<meeting>ECIR 2023</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,484.32,394.52,10.91;18,112.66,497.87,393.33,10.91;18,112.66,511.42,394.53,10.91;18,112.66,524.97,316.23,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,336.55,497.87,169.43,10.91;18,112.66,511.42,36.32,10.91">Overview of Touch√© 2020: Argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2696/paper_261.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,171.78,511.42,128.45,10.91">Working Notes of CLEF 2020</title>
		<title level="s" coord="18,378.83,512.43,128.36,9.72;18,112.66,524.97,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,538.52,394.52,10.91;18,112.66,552.07,393.33,10.91;18,112.66,565.62,394.53,10.91;18,112.66,579.17,383.15,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,336.55,552.07,169.43,10.91;18,112.66,565.62,36.32,10.91">Overview of Touch√© 2021: Argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2936/paper-205.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,171.78,565.62,128.45,10.91">Working Notes of CLEF 2021</title>
		<title level="s" coord="18,378.83,566.63,128.36,9.72;18,112.66,579.17,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2936</biblScope>
			<biblScope unit="page" from="2258" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,592.72,395.17,10.91;18,112.66,606.27,395.17,10.91;18,112.66,619.81,394.42,10.91;18,112.66,633.36,395.01,10.91;18,112.66,646.91,17.97,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,361.25,606.27,146.58,10.91;18,112.66,619.81,60.74,10.91">Overview of Touch√© 2022: Argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-247.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,195.76,619.81,125.48,10.91">Working Notes of CLEF 2022</title>
		<title level="s" coord="18,398.10,620.83,108.98,9.72;18,112.66,633.36,45.42,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="2867" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,660.46,395.17,10.91;19,112.66,86.97,394.63,10.91;19,112.66,101.76,100.58,9.20" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,269.71,660.46,238.12,10.91;19,112.66,86.97,42.92,10.91">ClueWeb22: 10 billion web documents with rich in-formation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Overwijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3536321</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,181.55,86.97,122.68,10.91">Proceedsings of SIGIR 2022</title>
		<meeting>eedsings of SIGIR 2022</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3360" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,114.06,393.33,10.91;19,112.66,127.61,393.33,10.91;19,112.41,141.16,394.89,10.91;19,112.66,155.96,141.85,9.20" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,352.08,114.06,153.90,10.91;19,112.66,127.61,194.62,10.91">Elastic ChatNoir: Search engine for the ClueWeb and the Common Crawl</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-76941-7_83</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,342.53,127.61,120.01,10.91">Proceedings of ECIR 2018</title>
		<title level="s" coord="19,152.87,142.18,149.60,9.72">Lecture Notes in Computer Science</title>
		<meeting>ECIR 2018</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10772</biblScope>
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,168.26,394.53,10.91;19,112.28,181.81,393.70,10.91;19,112.28,195.36,306.97,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,180.89,181.81,236.09,10.91">TARGER: Neural argument mining at your fingertips</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-3031</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,439.76,181.81,66.22,10.91;19,112.28,195.36,40.90,10.91">Proceedings of ACL 2019</title>
		<meeting>ACL 2019</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,208.91,395.17,10.91;19,112.66,222.46,394.52,10.91;19,112.66,236.01,237.52,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,349.01,208.91,158.82,10.91;19,112.66,222.46,196.59,10.91">PyTerrier: Declarative experimentation in Python from BM25 to dense retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482013</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,331.87,222.46,116.46,10.91">Proceedings of CIKM 2021</title>
		<meeting>CIKM 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4526" to="4533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,249.56,394.53,10.91;19,112.66,263.11,393.33,10.91;19,112.66,276.66,313.04,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,112.66,263.11,305.26,10.91">Computational argumentation quality assessment in natural language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1017</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,441.02,263.11,64.97,10.91;19,112.66,276.66,46.97,10.91">Proceedings of EACL 2017</title>
		<meeting>EACL 2017</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,290.20,393.54,10.91;19,112.66,303.75,393.33,10.91;19,112.66,317.30,345.27,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,293.43,290.20,212.77,10.91;19,112.66,303.75,303.61,10.91">TrecTools: an open-source Python library for information retrieval practitioners involved in TREC-like campaigns</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331399</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,439.89,303.75,66.10,10.91;19,112.66,317.30,46.53,10.91">Proceedings of SIGIR 2019</title>
		<meeting>SIGIR 2019</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1325" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,330.85,393.33,10.91;19,112.66,344.40,395.01,10.91;19,112.66,357.95,184.08,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,338.63,330.85,167.35,10.91;19,112.66,344.40,167.67,10.91">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-011-9162-z</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,288.61,344.40,133.72,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,371.50,394.52,10.91;19,112.66,385.05,394.53,10.91;19,112.66,398.60,394.53,10.91;19,112.48,412.15,393.50,10.91;19,112.66,425.70,279.90,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.11416</idno>
		<title level="m" coord="19,328.95,412.15,177.03,10.91;19,112.66,425.70,29.25,10.91">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,439.25,395.17,10.91;19,112.66,452.79,395.17,10.91;19,112.66,466.34,395.01,10.91;19,112.41,479.89,260.95,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,148.38,452.79,359.46,10.91;19,112.66,466.34,180.53,10.91">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,320.19,466.34,112.94,10.91">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,493.44,395.17,10.91;19,112.66,506.99,394.53,10.91;19,112.66,520.54,22.69,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,297.97,493.44,209.87,10.91;19,112.66,506.99,25.08,10.91">Argument quality prediction for ranking documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Buchm√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<ptr target=".org" />
	</analytic>
	<monogr>
		<title level="m" coord="19,162.58,506.99,129.81,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="19,299.96,506.99,179.44,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,534.09,393.33,10.91;19,112.66,547.64,394.53,10.91;19,112.66,561.19,267.15,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,347.90,534.09,158.09,10.91;19,112.66,547.64,204.62,10.91">Stanza: A python natural language processing toolkit for many human languages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.14</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,340.75,547.64,110.67,10.91">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,574.74,394.62,10.91;19,112.28,588.29,243.16,10.91;19,371.46,588.29,134.52,10.91;19,112.66,601.84,394.04,10.91;19,112.66,615.39,208.53,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,460.85,574.74,46.42,10.91;19,112.28,588.29,239.18,10.91">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,392.53,588.29,113.45,10.91;19,112.66,601.84,18.21,10.91">Proceedings of NeurIPS 2017</title>
		<meeting>NeurIPS 2017</meeting>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,628.93,394.53,10.91;19,112.33,642.48,395.33,10.91;19,112.66,656.03,182.58,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.09741</idno>
		<title level="m" coord="19,145.26,642.48,292.14,10.91">One embedder, any task: Instruction-finetuned text embeddings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,669.58,394.53,10.91;20,112.66,86.97,394.62,10.91;20,112.66,100.52,394.03,10.91;20,112.66,114.06,79.34,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="20,227.87,86.97,254.85,10.91">CausalQA: A benchmark for causal question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heindorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bl√ºbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><forename type="middle">N</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.291" />
	</analytic>
	<monogr>
		<title level="m" coord="20,112.66,100.52,128.79,10.91">Proceedings of COLING 2022</title>
		<meeting>COLING 2022</meeting>
		<imprint>
			<publisher>ICCL</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3296" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,127.61,394.62,10.91;20,112.33,141.16,394.85,10.91;20,112.66,154.71,262.94,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,460.89,127.61,46.39,10.91;20,112.33,141.16,221.36,10.91">CauseNet: Towards a causality graph extracted from the web</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heindorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Scholten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412763</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,357.51,141.16,116.40,10.91">Proceedings of CIKM 2020</title>
		<meeting>CIKM 2020</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3023" to="3030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,168.26,393.32,10.91;20,112.66,181.81,393.33,10.91;20,112.66,195.36,150.03,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,336.99,168.26,168.99,10.91;20,112.66,181.81,162.93,10.91">Evidence retrieval for causal questions using query expansion and reranking</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Reinhold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zeit-Altpeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rausch</surname></persName>
		</author>
		<ptr target=".org" />
	</analytic>
	<monogr>
		<title level="m" coord="20,298.57,181.81,125.56,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="20,431.35,181.81,74.64,10.91;20,112.66,195.36,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,208.91,393.33,10.91;20,112.66,222.46,394.53,10.91;20,112.66,236.01,243.80,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,319.25,208.91,186.74,10.91;20,112.66,222.46,185.64,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,324.11,222.46,152.39,10.91">Proceedings of NAACL-HLT 2019</title>
		<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,249.56,395.01,10.91;20,112.66,263.11,162.95,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7497994</idno>
		<title level="m" coord="20,251.44,249.56,225.32,10.91">Dataset Touch√©23-Image-Retrieval-for-Arguments</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,276.66,395.01,10.91;20,112.66,290.20,162.95,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6786948</idno>
		<title level="m" coord="20,251.44,276.66,225.32,10.91">Dataset Touch√©22-Image-Retrieval-for-Arguments</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,303.75,393.33,10.91;20,112.66,317.30,395.01,10.91;20,112.66,330.85,169.49,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,407.79,303.75,98.19,10.91;20,112.66,317.30,127.22,10.91">Grid-based evaluation metrics for web image search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313514</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,262.68,317.30,119.75,10.91">Proceedings of WWW 2019</title>
		<meeting>WWW 2019</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2103" to="2114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,344.40,394.53,10.91;20,112.66,357.95,393.33,10.91;20,112.66,371.50,277.24,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="20,155.05,357.95,258.26,10.91">On stance detection in image retrieval for argumentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Carnot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Heinemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Braker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schreieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1145/3539618.3591917</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,438.72,357.95,67.26,10.91;20,112.66,371.50,46.53,10.91">Proceedings of SIGIR 2023</title>
		<meeting>SIGIR 2023</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,385.05,394.53,10.91;20,112.66,398.60,393.33,10.91;20,112.66,412.15,393.33,10.91;20,112.66,425.70,395.00,10.91;20,112.66,439.25,124.38,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="20,318.79,398.60,187.19,10.91;20,112.66,412.15,129.35,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="20,265.37,412.15,116.06,10.91;20,454.88,413.16,51.11,9.72;20,112.66,426.71,129.03,9.72">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>Proceedings of ICML 2021</note>
</biblStruct>

<biblStruct coords="20,112.66,452.79,321.00,10.91" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="20,170.66,452.79,182.39,10.91">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,466.34,393.33,10.91;20,112.66,479.89,394.61,10.91;20,112.14,493.44,362.20,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="20,427.81,466.34,78.18,10.91;20,112.66,479.89,374.15,10.91">Comparing image generation, stance detection and feature matching for image retrieval for arguments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>M√∂bius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Enderling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bachinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Luc</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Touch√©</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="20,112.14,493.44,126.78,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="20,246.19,493.44,159.16,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,506.99,393.33,10.91;20,112.26,520.54,395.41,10.91;20,112.66,534.09,202.22,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="20,364.74,506.99,141.25,10.91;20,112.26,520.54,122.27,10.91">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01042</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,257.89,520.54,115.45,10.91">Proceedings of CVPR 2022</title>
		<meeting>CVPR 2022</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10674" to="10685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,547.64,393.33,10.91;20,112.48,561.19,398.08,10.91;20,112.66,575.98,54.76,9.20" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="20,173.24,547.64,262.03,10.91">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
	</analytic>
	<monogr>
		<title level="j" coord="20,446.71,547.64,59.28,10.91;20,112.48,561.19,126.69,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,588.29,393.33,10.91;20,112.33,601.84,394.94,10.91;20,112.14,615.39,362.20,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="20,174.58,601.84,312.35,10.91">Image retrieval for arguments using ChatGPT, CLIP and IBM Debater</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elagina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-A</forename><surname>Heizmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lahmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ortlepp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neville</forename><surname>Longbottom</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Touch√©</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="20,112.14,615.39,126.78,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="20,246.19,615.39,159.16,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,628.93,393.33,10.91;20,112.66,642.48,397.91,10.91;20,112.33,657.28,179.62,9.20" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="20,352.33,628.93,153.66,10.91;20,112.66,642.48,99.28,10.91">Project debater APIs: Decomposing the AI grand challenge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.31</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,234.99,642.48,123.76,10.91">Proceedings of EMNLP 2021</title>
		<meeting>EMNLP 2021</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,669.58,393.32,10.91;21,112.66,86.97,394.53,10.91;21,112.66,100.52,275.00,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="20,271.71,669.58,234.27,10.91;21,112.66,86.97,167.13,10.91">Debating Europe: A multilingual multi-target stance classification dataset of online debates</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravenet</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.politicalnlp-1.3" />
	</analytic>
	<monogr>
		<title level="m" coord="21,302.76,86.97,143.88,10.91">Proceedings of PoliticalNLP 2022</title>
		<meeting>PoliticalNLP 2022</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,114.06,394.52,10.91;21,112.66,127.61,395.01,10.91;21,112.66,141.16,111.92,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="21,217.47,114.06,285.40,10.91">X-stance: A multilingual multi-target dataset for stance detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vamvas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2624/paper9.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,127.24,127.61,184.91,10.91">Proceedings of SwissText/KONVENS 2020</title>
		<title level="s" coord="21,319.40,127.61,21.72,10.91">CEUR</title>
		<meeting>SwissText/KONVENS 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,154.71,395.17,10.91;21,112.66,168.26,394.61,10.91;21,112.66,181.81,241.55,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="21,285.22,154.71,222.61,10.91;21,112.66,168.26,374.98,10.91">CoFE: A new dataset of intra-multilingual multitarget stance classification from an online european participatory democracy platform</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hemamou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,112.66,181.81,153.71,10.91">Proceedings of AACL-IJCNLP 2022</title>
		<meeting>AACL-IJCNLP 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="418" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,195.36,393.33,10.91;21,112.66,208.91,190.91,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="21,226.84,195.36,279.15,10.91;21,112.66,208.91,57.49,10.91">Multilingual multi-target stance recognition in online public consultations</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,178.50,208.91,57.26,10.91">Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2161</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,222.46,394.53,10.91;21,112.66,236.01,393.32,10.91;21,112.66,249.56,394.63,10.91;21,112.66,264.35,114.17,9.20" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="21,276.23,236.01,229.75,10.91;21,112.66,249.56,32.02,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzm√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,169.52,249.56,112.39,10.91">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,276.66,393.33,10.91;21,112.66,290.20,393.33,10.91;21,112.66,303.75,292.08,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="21,279.19,276.66,226.80,10.91;21,112.66,290.20,303.99,10.91">Silver Surfer team at Touch√© task 4: Testing data augmentation and label propagation for multilingual stance detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Centeno</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="21,439.38,290.20,66.61,10.91;21,112.66,303.75,56.66,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="21,176.59,303.75,159.16,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,317.30,394.53,10.91;21,112.30,330.85,395.36,10.91;21,112.66,344.40,182.58,10.91" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<title level="m" coord="21,171.89,330.85,269.26,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,357.95,393.33,10.91;21,112.66,371.50,395.01,10.91;21,112.66,385.05,181.75,10.91" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="21,237.75,357.95,268.24,10.91;21,112.66,371.50,120.75,10.91">Data augmentation using back-translation for context-aware neural machine translation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yoshinaga</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6504</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,256.89,371.50,177.83,10.91">Proceedings of DiscoMT@EMNLP 2019</title>
		<meeting>DiscoMT@EMNLP 2019</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,398.60,393.33,10.91;21,112.66,412.15,394.62,10.91;21,112.31,425.70,395.48,10.91;21,112.66,439.25,23.21,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="21,368.39,398.60,137.59,10.91;21,112.66,412.15,50.34,10.91">Learning with local and global consistency</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2003/hash/87682805257e619d49b8e0dfdc14affa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m" coord="21,191.00,412.15,115.82,10.91">Proceedings of NIPS 2003</title>
		<meeting>NIPS 2003</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,452.79,393.33,10.91;21,112.66,466.34,394.53,10.91;21,112.66,479.89,90.78,10.91" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="21,173.70,452.79,332.28,10.91;21,112.66,466.34,107.48,10.91">Queen of Swords at Touch√© 2023: Intra-multilingual multi-target stance classification using BERT</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Schaefer</surname></persName>
		</author>
		<ptr target=".org" />
	</analytic>
	<monogr>
		<title level="m" coord="21,243.82,466.34,124.27,10.91">Working Notes of CLEF 2023</title>
		<title level="s" coord="21,375.23,466.34,131.96,10.91;21,112.66,479.89,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
