<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,410.29,15.42;1,89.29,106.66,417.05,15.42;1,89.29,129.00,381.75,11.96">Neville Longbottom at Touché 2023: Image Retrieval for Arguments using ChatGPT, CLIP and IBM Debater Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,66.42,11.96"><forename type="first">Daria</forename><surname>Elagina</surname></persName>
							<email>daria.elagina@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller University Jena</orgName>
								<address>
									<postCode>07743</postCode>
									<settlement>Jena</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.36,154.90,128.65,11.96"><forename type="first">Bernd-Albrecht</forename><surname>Heizmann</surname></persName>
							<email>bernd-albrecht.heizmann@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller University Jena</orgName>
								<address>
									<postCode>07743</postCode>
									<settlement>Jena</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.65,154.90,49.38,11.96"><forename type="first">Max</forename><surname>Koch</surname></persName>
							<email>m.koch@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller University Jena</orgName>
								<address>
									<postCode>07743</postCode>
									<settlement>Jena</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.67,154.90,83.45,11.96"><forename type="first">Gustav</forename><surname>Lahmann</surname></persName>
							<email>gustav.lahmann@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller University Jena</orgName>
								<address>
									<postCode>07743</postCode>
									<settlement>Jena</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,85.98,11.96"><forename type="first">Christian</forename><surname>Ortlepp</surname></persName>
							<email>christian.ortlepp@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller University Jena</orgName>
								<address>
									<postCode>07743</postCode>
									<settlement>Jena</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,410.29,15.42;1,89.29,106.66,417.05,15.42;1,89.29,129.00,381.75,11.96">Neville Longbottom at Touché 2023: Image Retrieval for Arguments using ChatGPT, CLIP and IBM Debater Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5DE5F1B96F5D3D3F320FC228DE47594E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CLIP</term>
					<term>ChatGPT</term>
					<term>IBM Debater</term>
					<term>boilerpy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Argumentative search engines could benefit from images visualizing each side of the presented pro and con arguments. The objective of this task is to retrieve 10 images supporting and 10 images opposing a controversial topic from a given dataset of images. We describe a way to prompt ChatGPT for arguments backing each stance, which will then be used to find matching images. Our first approach uses BM25 on the website text to represent an image, while our second method uses OpenAI's CLIP to analyze the images themselves and match them with arguments. Both retrieval methods can then be refined using IBM Debater's stance detection capabilities to further increase stance awareness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In order to find images conveying either a supporting (PRO) or opposing (CON) stance towards a controversial topic, we considered it being helpful to know what talking points each position commonly uses. Since this requires some outside knowledge, we wanted to make use of recent advances in generative large language models by using the knowledge embedded inside them, originating from their training data. As ChatGPT was trained on large portions of the Internet until 2021 and none of the 50 topics presented in this task are about a phenomenon only emerging after that time, we used it to generate a list of arguments supporting or opposing each topic.</p><p>Using these two sets of arguments, we built one pipeline for retrieving PRO images, and one for retrieving CON images for a topic. Our first approach uses the concatenation of all arguments of a stance for a topic to search in a BM25 index of the page contents in the dataset. The resulting images are ranked based on the BM25 score of the document they are found within. Our second approach uses OpenAI's pre-trained CLIP model <ref type="bibr" coords="1,348.33,565.09,14.85,10.91" target="#b0">[1]</ref> to retrieve images based on their actual content. The model was trained to map image data as well as the corresponding image description (short phrases of words) into the same 300-dimensional vector space. Using the separate arguments as the description phrase we get a point in this vector space. By returning the images closest to this point we retrieve images whose content matches the argument. After doing this for every argument separately, the distance of each image to their respective argument point is used as the score to rank by.</p><p>While the retrieved images are already stance aware due to the stance specific arguments, we tried to further eliminate neutral, on-topic images by using IBM Debater to rerank the results of the BM25 or CLIP retrieval stage. Five surrounding sentences for each image are passed to the API alongside the topic, returning a score indicating whether they both convey the same stance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Query expansion</head><p>To find arguments of a particular stance towards a given topic, we prompted ChatGPT using the following template, in which $STANCE is replaced by "for" for PRO and "against" for CON. The original query is passed as $TOPIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name a list of arguments $STANCE "$TOPIC"</head><p>The answers were always correctly formatted markdown lists (either enumerated or with bullet points), which could be easily parsed into a list of separate arguments. Since our project began with the start of the public beta of ChatGPT, no official API was available yet, so the answers were fetched from the browser interface of the ChatGPT version deployed at that time using an unofficial puppeteer script. For reproducibility, all answers for both stances and topics 51 to 100 are hard coded in a JSON file published with our source code. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Retrieval</head><p>After expanding the topic for a given stance into a list of arguments, we try to find corresponding images from the dataset in two different ways. The first one uses the surrounding text of the image, whereas the second approach exclusively works on the image data itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name a list of arguments for "Should the US Electoral college be abolished?"</head><p>• The Electoral College is undemocratic and gives disproportionate weight to certain states. • ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For each of the arguments, what could be an image description of an image illustrating the issue?</head><p>• An image of a map of the United States with certain states highlighted to show how the Electoral College disproportionately affects the outcome of elections in those states. • ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Query expansion with PRO arguments for topic 98: ChatGPT conveniently answers using markdown formatted lists. The generated image description wasn't used in the end, as the argument phrases as input for both BM25 and CLIP yielded better results on our testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BM25</head><p>In this approach, we assume the image's stance towards a topic is reflected by the text surrounding it. Following the idea that the text in closer spatial proximity to the image in the document is more likely to be connected to the image's content, we implemented an algorithm to extract paragraphs from the document starting at the image and then alternating between paragraphs directly above and below the image. These paragraphs will be concatenated and used as the document representation in the BM25 index.</p><p>To find nodes containing meaningful text in the HTML document, we used the DefaultExtractor 2 of the boilerpy3 library, because the default ArticleExtractor left out too many relevant parts of the document. The extractor employs some heuristics and marks nodes in the HTML document, which are likely to contain meaningful text, using a special tag. We then used the xPaths of the images provided in the image dataset to find image occurrences in the tagged HTML document. Starting with the alt text of the image, if available, text from below and above the image is extracted alternatingly.</p><p>Using the first 4096 characters of the concatenated paragraphs obtained this way, a BM25 index was built for the whole dataset using the default implementation of pyterrier. The document IDs are the ID of the image, whose containing HTML page was used for content extraction. Given a list of arguments for a certain stance, we concatenate all the arguments and use the resulting string as the input query for the BM25 search, whose results will be further processed by a reranker as described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CLIP</head><p>The CLIP <ref type="bibr" coords="3,134.47,615.34,13.00,10.91" target="#b0">[1]</ref> neural network was designed and trained by OpenAI to map images as well as short phrases of text describing images into the same 512-dimensional vector space. While projects like Stable Diffusion use this model to obtain vector representations for prompts, we wanted to find images representing a certain topic, or even argument.</p><p>For every image in the dataset, we used the pre-trained openai/clip-vit-base-patch32 3 model to infer the corresponding vector representation, which was then normalized to length one and added to a BallTree index from sklearn. This enables us to use a k-nearest-neighbor search, which results in the same ranking as when sorting by cosine similarity.</p><p>Based on a list of separate arguments, we looked up the 100 nearest images to the vector representing the argument text for each argument. The retrieval result is sorted by the distance to the respective argument, so images obtained for different argument phrases will share the top ranks. If an image appears in the 100 nearest neighbors of multiple arguments, the lowest of the distances is used as the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Reranking</head><p>While the retrieval operations discussed before can be efficiently applied on the whole dataset and result in on-topic images most of the time, we attempted to further refine the results towards a certain stance. For this, we used the top 500 images obtained using either CLIP or BM25 and passed them to our reranking stage.</p><p>The parameters used in both rerankers were chosen using a grid search on the 2022 dataset on topics 12, 13, 20, 46 and 49, for which we manually judged the union of the top 1000 CLIP and BM25 results using the original topic as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Diff</head><p>Often on-topic images appeared in the top results of both stances. Therefore, in this reranking we subtracted 0.5 times the BM25 score of the same document for the opposite stance from the actual BM25 score, in an attempt to lower the score of neutral but on-topic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Debater</head><p>We used IBM's Project Debater Early Access API <ref type="bibr" coords="5,312.62,319.13,12.99,10.91" target="#b1">[2]</ref> to calculate a new score for each image based on the first 5 sentences of our document representation, i.e. the sentences closest to the image on the page, including the image's alt text.</p><p>For this, we used the "pro-con" endpoint, which given two sentences returns the stance of the first sentence relative to the second sentence as a number between -1 (CON) and 1 (PRO). We calculated this score for each sentence paired with the original topic text and averaged them to get the new image score. When ranking for CON instead of PRO, the score was simply inverted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We submitted a single Docker image to TIRA <ref type="bibr" coords="5,283.75,459.05,11.27,10.91" target="#b2">[3]</ref>, which can run any of the pipeline combinations when passing the corresponding name as an environment variable. The arguments generated by ChatGPT as well as the scores from the IBM Debater api were fetched in advance and are mounted into the container to allow for reproducible runs without Internet access.</p><p>Finding on-stance images is the hardest part of this task. When looking at the on stance score, the raw pipelines without the reranking stage outperforming both the diff and debater rerankers for BM25 as well as CLIP retrieval, show they are less reliable than our retrieval stage on its own. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,410.22,416.70,8.93;4,89.29,422.22,418.36,8.87;4,89.29,434.18,407.66,8.87;4,193.47,94.15,208.36,294.69"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Nodes marked by boilerpy3's Default Extractor are highlighted in pink. The numbers indicate the order in which the content of the nodes are concatenated to form the document representation. Missing numbers correspond to nodes not found in the DOM when creating this visualization.</figDesc><graphic coords="4,193.47,94.15,208.36,294.69" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,111.28,399.60,10.91;6,107.59,124.83,398.40,10.91;6,107.59,138.38,257.64,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m" coord="6,316.79,124.83,189.20,10.91;6,107.59,138.38,127.62,10.91">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,151.93,398.39,10.91;6,107.59,165.48,226.44,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01029</idno>
		<title level="m" coord="6,352.93,151.93,153.05,10.91;6,107.59,165.48,96.47,10.91">Project debater apis: Decomposing the ai grand challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,179.03,399.60,10.91;6,107.59,192.57,399.68,10.91;6,107.41,206.12,399.77,10.91;6,107.20,219.67,398.78,10.91;6,107.59,233.22,398.80,10.91;6,107.27,246.77,399.90,10.91;6,107.59,262.76,123.08,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,170.50,192.57,298.77,10.91">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-28241-6_20.doi:10.1007/978-3-031-28241-6_20" />
	</analytic>
	<monogr>
		<title level="m" coord="6,189.26,219.67,316.73,10.91;6,107.59,233.22,91.10,10.91">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="6,205.60,233.22,151.87,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
