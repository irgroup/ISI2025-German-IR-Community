<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,404.85,15.42;1,89.29,106.66,359.75,15.42;1,89.29,129.00,381.75,11.96">Queen of Swords at Touché 2023: Intra-Multilingual Multi-Target Stance Classification using BERT Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
				<funder>
					<orgName type="full">Lernlabor Cybersicherheit&quot; (LLCS)</orgName>
				</funder>
				<funder>
					<orgName type="full">Hessian Ministry of Higher Education, Research, Science and the Arts</orgName>
				</funder>
				<funder>
					<orgName type="full">ATHENE -CRISIS</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,154.90,65.26,11.96"><forename type="first">Karla</forename><surname>Schäfer</surname></persName>
							<email>karla.schaefer@sit.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<orgName type="institution" key="instit2">ATHENE -National Research Center for Applied Cybersecurity</orgName>
								<address>
									<addrLine>Rheinstraße 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,404.85,15.42;1,89.29,106.66,359.75,15.42;1,89.29,129.00,381.75,11.96">Queen of Swords at Touché 2023: Intra-Multilingual Multi-Target Stance Classification using BERT Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B505B7BADA2FD676E4C2C34A07EF5088</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stance Detection</term>
					<term>BERT</term>
					<term>Fine-tuning</term>
					<term>Self-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stance classification can be used in various scenarios, such as fake news detection or public opinion measurement. However, little work has been done on stance detection in multilingual data. For this reason, this work uses a multilingual, multi-target, and multi-topic dataset to develop a classifier for detecting stance in such data. The classifier was trained using pre-trained BERT models, with various experiments showing superior performance of a fine-tuned multilingual BERT model with self-training. Since the dataset was unbalanced, with the main label being "in favor", the macro-averaged F1 score was used for measurement. The best performing model achieved a macro-average F1 score of 0.8862 using the same proposals in stance classification for training and testing. The same approach was used to train two classifiers for the CLEF 2023 Touché Lab Task 4 Subtask 1 and 2, using new, previously unseen proposals for testing. However, by using new unseen proposals, the results deteriorated significantly, and in the challenge only a macro F1 score of 0.324 and 0.417 was achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rise of the Internet, people can publish their opinions at any time, e.g. in user forums, blogs or social media platforms, and with stance detection, these comments on various topics can be automatically evaluated. Given a proposal and comments on this proposal, the task of stance detection is to identify the stance of the comment author towards a target (proposal) <ref type="bibr" coords="1,89.29,497.34,11.58,10.91" target="#b0">[1]</ref>. This information can then be used, e.g., in fake news detection to classify the stance of headlines to their article bodies to determine if the title is related to the content <ref type="bibr" coords="1,449.92,510.89,11.52,10.91" target="#b1">[2]</ref>. Another application scenario is document retrieval tasks, e.g. to measure public opinion towards an event (or entity), such as the Brexit <ref type="bibr" coords="1,248.05,537.99,12.84,10.91" target="#b2">[3]</ref> or the US elections <ref type="bibr" coords="1,350.23,537.99,11.43,10.91" target="#b3">[4]</ref>.</p><p>Stance classification tasks can be divided according to language (mono-or multilingual), topic and target (number of target labels). In this paper, an approach to classify multilingual and multi-target stances has been developed as part of the Shared Task 4 of Touché ("Intra-Multilingual Multi-Target Stance Classification") <ref type="bibr" coords="1,311.32,592.19,11.47,10.91" target="#b4">[5,</ref><ref type="bibr" coords="1,325.53,592.19,7.65,10.91" target="#b5">6]</ref>. Using Barriere and Balahur's dataset <ref type="bibr" coords="1,89.29,605.73,11.43,10.91" target="#b6">[7]</ref>, I fine-tuned a multilingual BERT classifier and applied self-training.</p><p>In the following, the related work is presented first (Section 2). Then, in Section 3.2, the dataset is briefly introduced and the experimental methods are explained. In Section 4, first results on a test set are presented, together with the results of the Touché Challenge Task 4. The final Section (Section 5) provides a conclusion that identifies limitations and presents future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In supervised learning, learning-based stance classification approaches can be divided into traditional machine learning, deep learning and ensemble learning approaches <ref type="bibr" coords="2,438.57,213.34,11.36,10.91" target="#b7">[8]</ref>. Traditional machine learning uses feature-based learning such as support vector machines (SVM) or decision trees. Predefined features such as ngrams, POS tags or sentiments are used to train the classifier <ref type="bibr" coords="2,89.29,253.99,11.43,10.91" target="#b7">[8]</ref>. Deep learning based approaches often use classifiers such as LSTMs and CNNs <ref type="bibr" coords="2,461.25,253.99,11.43,10.91" target="#b7">[8]</ref>.</p><p>Tran et al. <ref type="bibr" coords="2,154.22,267.54,13.00,10.91" target="#b8">[9]</ref> used CNN with BERT for stance detection in the low-resource language Vietnamese. BERT was used to extract contextual word embeddings, followed by a CNN for classification. The averaged accuracy was compared with Bi-LSTM using different word2vec and BERT embedding approaches. The best performer was BERT-CNN.</p><p>In the 2017 Fake News Challenge (FNC-1), the task was to estimate the stance of articles toward a given headline (i.e., claim). The best performing system used an ensemble based on a gradient-boosted decision tree and a convolutional neural network (CNN), along with textual features <ref type="bibr" coords="2,127.60,362.38,16.25,10.91" target="#b9">[10]</ref>.</p><p>As a traditional linear classifier with bag of words representations was compared with a multilingual BERT, which performed better than the traditional approach <ref type="bibr" coords="2,433.52,389.48,16.41,10.91" target="#b10">[11]</ref>. While the cross-lingual performance of BERT increased when all questions were in English.</p><p>According to Ghosh et al. <ref type="bibr" coords="2,211.42,416.58,16.08,10.91" target="#b11">[12]</ref>, BERT outperformed feature-based and other neural approaches in the area of stance detection in a monolingual English environment. Based on these good results, this paper takes a closer look at BERT as a stance classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The dataset used to train and evaluate the different BERT models used is called CoFE, created by Barriere et al. <ref type="bibr" coords="2,167.96,515.85,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,182.21,515.85,12.42,10.91" target="#b12">13]</ref>. The following is a brief introduction to this dataset, followed by an explanation of the classification method used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>Only the CoFE dataset <ref type="bibr" coords="2,188.97,579.13,17.76,10.91" target="#b12">[13]</ref> provided by the challenge was used. The dataset contains comments on proposals on socially important issues from an online debate platform. The dataset consists of proposals consisting of a title and a text, both in English and in the native language, where the native language can be any of the 24 EU languages (plus Catalan and Esperanto). Additional metadata, such as topic and the name of the native language name, are also provided, but not used here. The comments on the proposals are written in the native language and contain other information such as the topic, language, upvotes and downvotes. The comments can be divided into the CF_S, CF_U and CF_E-Dev subsets. I merged the title and the proposal and used only these data together with the corresponding comments for training the classifier. The other data (topic, downvotes, etc.) were not used for classification. Due to time constraints, the main experiments were performed on the Subtask 1 dataset only. This limited the training data to the part of the CoFE dataset without the comments from the debates of the CF_E-Test test set. No other datasets were used.</p><p>The CF_E-Dev dataset contains a small set of comments annotated with three stance labels, the CF_S dataset is a larger set with binary self-annotations (labels: "in favor" or "against"). These two datasets have been used primarily. The CF_U dataset contains unlabeled data, so the labels had to be determined first. This dataset was used in the second part of the training for self-training. In total, there are 4247 different proposals in the dataset. These proposals are linked to the comments by an ID (id_prop). An overview of the used datasets can be found in Table <ref type="table" coords="3,115.79,402.23,3.74,10.91" target="#tab_0">1</ref>.</p><p>Only the dataset from Subtask 1 was used to determine the hyperparameters. To do this, I combined the CF_E-Dev and CF_S datasets, resulting in the CF_EDevS dataset with 5,046 labeled entries. Since the dataset is very unbalanced, with 3,710 entries labeled "in favor", the macro-averaged F1 score was used for the following evaluation.</p><p>The CF_EDevS dataset was used to create a training (3,633 entries), validation (404 entries), and test (1,009 entries) dataset for evaluating the different approaches. For participation in the Touché Task 4 challenge, only a split between validation (505 entries) and training (4,541 entries) was made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approach</head><p>Due to the great success of BERT in related work <ref type="bibr" coords="3,307.45,560.35,16.33,10.91" target="#b10">[11,</ref><ref type="bibr" coords="3,326.51,560.35,12.25,10.91" target="#b11">12]</ref>, I decided to use different pre-trained BERT models <ref type="bibr" coords="3,153.05,573.90,18.07,10.91" target="#b13">[14]</ref> for stance classification. In stance detection, sentence pairs (proposal and comment) are passed to BERT. In creating this input, I followed the guidelines of Devlin et al. <ref type="bibr" coords="3,89.29,601.00,17.98,10.91" target="#b13">[14]</ref> and passed the two specifications, separated by a special token ([SEP]), as input to BERT. In addition, the token type ids were stored and passed to the model (segment embedding). The sequence is preceded by the special token <ref type="bibr" coords="3,277.56,629.11,22.43,9.72">[CLS]</ref>.</p><p>Since sentence pairs consisting of longer texts were given as input, different truncation strategies were tested. The BERTokenizer of each pre-trained model was used as the tokenizer. First, the average number of tokens was manually calculated for the proposal and the comment separately. Since the proposal contained more tokens on average than the comments (proposal: 227 tokens on average, comments: 86 tokens on average in the non-translated dataset; proposal in the native language), the truncation strategy longest or truncation first was chosen (for results of different truncation strategies see Table <ref type="table" coords="4,311.94,298.23,5.07,10.91" target="#tab_1">2</ref> in Section 4.1). The truncation length was set to 512 as the longest possible input length for BERT.</p><p>Since the dataset is multilingual, a multilingual model<ref type="foot" coords="4,346.32,323.58,3.71,7.97" target="#foot_0">1</ref> pre-trained on 104 languages and recommended by the developers<ref type="foot" coords="4,236.52,337.13,3.71,7.97" target="#foot_1">2</ref> was tried first. Since many pre-trained models are trained on English data, the dataset was translated into English using GoogleTranslator<ref type="foot" coords="4,460.10,350.67,3.71,7.97" target="#foot_2">3</ref> . On this translated dataset, I applied another BERT pre-trained on English data<ref type="foot" coords="4,403.64,364.22,3.71,7.97" target="#foot_3">4</ref> only. The pre-trained BERT model was combined with a linear layer for sentence classification. In a second approach, BERT was implemented with a Bi-LSTM layer.</p><p>First, the CF_EDevS dataset (from Subtask 1) was used to fine-tune the different BERT models (Fine-tuning 1). Different combinations of the dataset language were tried (see Table <ref type="table" coords="4,455.64,420.18,4.97,10.91" target="#tab_1">2</ref> in Section 4.1 for the results). The resulting fine-tuned models were used to make predictions for the unlabeled CF_U dataset, including probabilities for classified labeling. Subsequently, the now labeled CF_U dataset was used together with the CF_EDevS dataset to fine-tune the BERT model again (so-called self-training; Fine-tuning 2). Different probability thresholds were tried. Only those comment-proposal pairs from the CF_U dataset whose labels were predicted above a certain probability were used for training. For the whole process, a batch size of 8 was used and different learning rates (5e-5 ,3e-5 ,2e-5) were tried, number of epochs: 5, 10. For a summary of the process, see Figure <ref type="figure" coords="4,192.48,528.57,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>First, initial trials were conducted locally with the dataset in different languages. For this, the respective BERT model was fine-tuned only once (Section 4.1). After the first parameters (language, truncation strategy and method) were determined, a longer training including the second fine-tuning on the CF_U dataset was performed on a GPU (Section 4.2). The best performing model was then used to determine the results for the challenge (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results for Fine-tuning 1</head><p>First, I translated the comments and computed the results of the classifiers (simply BERT finetuned) locally (see Table <ref type="table" coords="5,200.81,316.89,5.12,10.91" target="#tab_1">2</ref> for results). All results were obtained after 4 epochs of fine-tuning and a learning rate of 2e-5. This shows that different truncation strategies do not have a big impact on the results (slightly better: truncation end or longest). In the following experiments, truncation longest was used.</p><p>Next, I tried different combinations of the dataset in different languages. I first used the proposals in English and the comments in their native language (Table <ref type="table" coords="5,399.70,384.64,3.66,10.91" target="#tab_1">2</ref>, No.1-3) and compared them with proposals and comments in their native language (Table <ref type="table" coords="5,390.10,398.19,3.74,10.91" target="#tab_1">2</ref>, No.4) and everything in English (Table <ref type="table" coords="5,155.82,411.74,3.76,10.91" target="#tab_1">2</ref>, No.5). The best results were obtained for the whole dataset in English (No.5; 0.8235) and everything in the native language (No.4; 0.8067). In another experiment (No.6), I implemented BERT with a Bi-LSTM layer. However, the results here were worse than with the English dataset (No.5). Therefore, this approach was not explored further here, but future work could explore this approach in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results for the entire Process</head><p>After the best results were obtained in the first fine-tuning trials (Section 4.1) with the English and native-language datasets, these were used in the subsequent trials. First, different learning rates and epochs were tried. In most cases, overfitting occurred after 2 epochs and training was stopped. The best results for the first fine-tuning step were obtained on the native language dataset with a learning rate of 2e-5 after 2 epochs (Table <ref type="table" coords="5,346.01,569.86,3.80,10.91" target="#tab_2">3</ref>, No.5). Subsequently, the labels of the CF_U dataset were predicted using this best model (creating weak labels) and the second fine-tuning (self-training) was performed, similar to the approach of Barriere et al. <ref type="bibr" coords="5,458.98,596.96,16.25,10.91" target="#b14">[15]</ref>.</p><p>Different thresholds for the amount of data from the CF_U dataset were tried. More data gave better results. At a threshold of 90% probability for the predicted label, I stopped to avoid adding too many weak labels to the training dataset. The best classifier achieved a macro-averaged F1 score of 0.8862 on the test set (Table <ref type="table" coords="5,267.47,651.15,3.78,10.91" target="#tab_2">3</ref>, No.6). The second fine-tuning on the CF_U dataset increased the F1 score by 0.08.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results of the Challenge</head><p>For the challenge, one model each was trained for Subtask 1 and Subtask 2, using two different sized datasets (see Table <ref type="table" coords="6,197.00,417.72,3.50,10.91" target="#tab_0">1</ref>). Both models were trained using the methodology already presented in Section 3.2. The parameters were taken from the model with the best performance in Table <ref type="table" coords="6,501.01,431.27,4.97,10.91" target="#tab_2">3</ref> (language of the dataset: native; learning rate: 2e-5; epochs:2; threshold for the dataset CF_U &gt; 0.9). To train the classifier for Subtask 1, 3,304 entries of the CF_U dataset, i.e. weak labels, were used. For the Subtask 2 model, 10,726 entries of the CF_U dataset were used. The results of the Subtask 1 and 2 models are shown in Table <ref type="table" coords="6,311.79,485.47,3.74,10.91" target="#tab_3">4</ref>, together with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, different approaches for fine-tuning BERT models (English and multilingual) on a multilingual multi-target dataset were evaluated. Although a satisfactory result with a macro-averaged F1 score of 0.8862 was obtained with the self-generated CF_EDevS dataset, the results in the challenge were rather poor. This is probably because I trained, validated, and tested the model using the same proposals. The challenge then used different proposals for testing, which led to the poor results in Section 4.3.</p><p>Only the CoFE dataset provided by the challenge was used, training BERT on more data, such as the X-Stance dataset, might improve the results. Other works have achieved very good results with ensemble methods on stance classification tasks. For example, the fine-tuned models could be combined with a decision tree to improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,219.98,345.28,8.93;4,141.73,84.20,311.77,123.22"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the approach, divided into Fine-tuning 1 and 2 (self-training).</figDesc><graphic coords="4,141.73,84.20,311.77,123.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,369.69,108.96"><head>Table 1</head><label>1</label><figDesc>Overview of the Datasets.</figDesc><table coords="3,136.60,119.84,322.09,79.61"><row><cell></cell><cell cols="3">Dataset Subtask 1 (CF_EDevS dataset)</cell><cell></cell><cell cols="2">Dataset Subtask 2</cell></row><row><cell>Feature</cell><cell cols="3">CF_S CF_U CF_E-Dev</cell><cell cols="3">CF_S CF_U CF_E-Dev</cell></row><row><cell>entries</cell><cell>4145</cell><cell>5785</cell><cell>901</cell><cell>7002</cell><cell cols="2">13213 1414</cell></row><row><cell>number labels</cell><cell>2</cell><cell>0</cell><cell>3</cell><cell>2</cell><cell>0</cell><cell>3</cell></row><row><cell cols="2">label (in favor) 3214</cell><cell>-</cell><cell>496</cell><cell>5440</cell><cell>-</cell><cell>753</cell></row><row><cell>label (against)</cell><cell>931</cell><cell>-</cell><cell>64</cell><cell>1562</cell><cell>-</cell><cell>118</cell></row><row><cell>label (others)</cell><cell>-</cell><cell>-</cell><cell>341</cell><cell>-</cell><cell>-</cell><cell>543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,414.66,122.95"><head>Table 2</head><label>2</label><figDesc>Results for Fine-tuning 1.</figDesc><table coords="5,95.27,122.06,408.38,91.37"><row><cell cols="2">No. model</cell><cell cols="4">language (proposal) language (comment) truncation strategy macro-averaged F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>score</cell></row><row><cell>1</cell><cell cols="2">multilingual english</cell><cell>native</cell><cell>end</cell><cell>0.8097</cell></row><row><cell>2</cell><cell cols="2">multilingual english</cell><cell>native</cell><cell>first</cell><cell>0.7938</cell></row><row><cell>3</cell><cell cols="2">multilingual english</cell><cell>native</cell><cell>longest</cell><cell>0.8017</cell></row><row><cell>4</cell><cell cols="2">multilingual native</cell><cell>native</cell><cell>longest</cell><cell>0.8067</cell></row><row><cell>5</cell><cell>english</cell><cell>english</cell><cell>english</cell><cell>longest</cell><cell>0.8235</cell></row><row><cell>6</cell><cell>Bi-LSTM</cell><cell>english</cell><cell>english</cell><cell>longest</cell><cell>0.8145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,413.80,144.06"><head>Table 3</head><label>3</label><figDesc>Results for the whole process (Fine-tuning 1+2).</figDesc><table coords="6,95.27,122.06,407.53,112.49"><row><cell cols="2">No. language</cell><cell cols="5">learning rate epochs threshold CF_U count CF_U used macro-averaged F1</cell></row><row><cell></cell><cell>(dataset)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>score</cell></row><row><cell>1</cell><cell>english</cell><cell>2e-5</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>0.7604</cell></row><row><cell>2</cell><cell>english</cell><cell>3e-5</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>0.7604</cell></row><row><cell>3</cell><cell>english</cell><cell>5e-5</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>0.7663</cell></row><row><cell>4</cell><cell>native</cell><cell>5e-5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>0.7842</cell></row><row><cell>5</cell><cell>native</cell><cell>2e-5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>0.8079</cell></row><row><cell>6</cell><cell>native</cell><cell>2e-5</cell><cell>2</cell><cell>&gt;0.9</cell><cell>3 304 entries</cell><cell>0.8862</cell></row><row><cell>7</cell><cell>native</cell><cell>2e-5</cell><cell>2</cell><cell>&gt;0.93</cell><cell>2 955 entries</cell><cell>0.8402</cell></row><row><cell>8</cell><cell>native</cell><cell>2e-5</cell><cell>2</cell><cell>&gt;0.99</cell><cell>746 entries</cell><cell>0.7897</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,256.74,415.47,100.22"><head>Table 4</head><label>4</label><figDesc>Results in the CLEF 2023 Touché Lab Task 4 Challenge.</figDesc><table coords="6,95.00,288.31,409.46,68.65"><row><cell>Team</cell><cell>Run timestamp</cell><cell cols="3">all-accuracy all-macro f1-score all-micro f1-score</cell></row><row><cell>touche23-queen-of-swords</cell><cell cols="2">2023-05-19-07-51-03 0.605</cell><cell>0.417</cell><cell>0.605</cell></row><row><cell>(Subtask 1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>touche23-queen-of-swords</cell><cell cols="2">2023-05-19-07-51-35 0.616</cell><cell>0.324</cell><cell>0.616</cell></row><row><cell>(Subtask 2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>touche23-baseline</cell><cell cols="2">2023-04-09-12-20-42 0.552</cell><cell>0.237</cell><cell>0.552</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,638.16,190.91,8.97"><p>https://huggingface.co/bert-base-multilingual-cased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,649.12,254.49,8.97"><p>https://github.com/google-research/bert/blob/master/multilingual.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.57,660.08,219.80,8.97"><p>https://pypi.org/project/deep-translator/#google-translate-1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,92.57,671.03,152.16,8.97"><p>https://huggingface.co/bert-base-uncased</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> and the <rs type="funder">Hessian Ministry of Higher Education, Research, Science and the Arts</rs> within their joint support of "<rs type="funder">ATHENE -CRISIS</rs>" and "<rs type="funder">Lernlabor Cybersicherheit" (LLCS)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,197.00,394.62,10.91;7,112.66,210.55,393.33,10.91;7,112.66,224.10,193.59,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,410.58,197.00,96.70,10.91;7,112.66,210.55,111.65,10.91">Semeval-2016 task 6: Detecting stance in tweets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,245.80,210.55,260.18,10.91;7,112.66,224.10,115.70,10.91">Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016)</title>
		<meeting>the 10th international workshop on semantic evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,237.65,393.33,10.91;7,112.66,251.20,393.33,10.91;7,112.26,264.75,330.77,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,285.17,237.65,220.82,10.91;7,112.66,251.20,229.86,10.91">From clickbait to fake news detection: an approach based on detecting the stance of headlines to articles</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bourgonje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,364.76,251.20,141.22,10.91;7,112.26,264.75,253.04,10.91">Proceedings of the 2017 EMNLP workshop: natural language processing meets journalism</title>
		<meeting>the 2017 EMNLP workshop: natural language processing meets journalism</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,278.30,393.33,10.91;7,112.66,291.85,393.32,10.91;7,112.66,305.40,385.51,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,267.29,278.30,238.69,10.91;7,112.66,291.85,81.92,10.91">Stance classification in texts from blogs on the 2016 british referendum</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Simaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Paradis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,218.26,291.85,287.72,10.91;7,112.66,305.40,18.15,10.91;7,302.72,305.40,65.21,10.91">Speech and Computer: 19th International Conference, SPECOM 2017</title>
		<meeting><address><addrLine>Hatfield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 12-16, 2017. 2017</date>
			<biblScope unit="page" from="700" to="709" />
		</imprint>
	</monogr>
	<note>Proceedings 19</note>
</biblStruct>

<biblStruct coords="7,112.66,318.95,394.61,10.91;7,112.66,332.50,393.33,10.91;7,112.66,346.05,394.53,10.91;7,112.66,359.59,394.87,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,323.45,318.95,183.82,10.91;7,112.66,332.50,238.95,10.91">Friends and enemies of clinton and trump: using context for detecting stance in political tweets</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Hernández Farías</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,378.32,332.50,127.66,10.91;7,112.66,346.05,390.08,10.91">Advances in Computational Intelligence: 15th Mexican International Conference on Artificial Intelligence, MICAI 2016</title>
		<meeting><address><addrLine>Cancún, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 23-28, 2016. 2017</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,373.14,329.86,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,112.66,373.14,90.39,10.91">Overview of Touché</title>
	</analytic>
	<monogr>
		<title level="m" coord="7,231.83,373.14,138.29,10.91">Argument and Causal Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,386.69,394.53,10.91;7,112.48,400.24,393.50,10.91;7,112.66,413.79,393.33,10.91;7,112.66,427.34,393.53,10.91;7,112.66,440.89,291.42,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,295.69,400.24,210.29,10.91;7,112.66,413.79,37.55,10.91">Overview of Touché 2023: Argument and Causal Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schlatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravenet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hemamou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.83,413.79,333.15,10.91;7,112.66,427.34,268.87,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 14th International Conference of the CLEF Association (CLEF 2023)</title>
		<title level="s" coord="7,388.23,427.34,117.96,10.91;7,112.66,440.89,31.10,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="7,112.66,454.44,393.33,10.91;7,112.66,467.99,190.91,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,226.84,454.44,279.15,10.91;7,112.66,467.99,57.49,10.91">Multilingual multi-target stance recognition in online public consultations</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,178.50,467.99,57.26,10.91">Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2161</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,481.54,394.62,10.91;7,112.66,495.09,252.17,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,203.05,481.54,123.25,10.91">Stance detection: A survey</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Küçük</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<idno type="DOI">10.1145/3369026</idno>
		<ptr target="https://doi.org/10.1145/3369026.doi:10.1145/3369026" />
	</analytic>
	<monogr>
		<title level="j" coord="7,339.32,481.54,93.81,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,508.64,393.32,10.91;7,112.66,522.18,393.33,10.91;7,112.66,535.73,197.44,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,260.37,508.64,245.62,10.91;7,112.66,522.18,107.34,10.91">Using convolution neural network with bert for stance detection in vietnamese</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">X</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,249.61,522.18,256.37,10.91;7,112.66,535.73,99.15,10.91">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7220" to="7225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,549.28,394.53,10.91;7,112.66,562.83,325.26,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Sean Baird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="https://blog.talosintelligence.com/talos-fake-news-challenge/" />
		<title level="m" coord="7,223.71,549.28,279.19,10.91">Talos targets disinformation with fake news challenge victory</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,576.38,394.53,10.91;7,112.66,589.93,393.33,10.91;7,112.66,603.48,394.04,10.91;7,112.66,617.03,150.29,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,217.21,576.38,285.67,10.91">X-Stance: A multilingual multi-target dataset for stance detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vamvas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2624/paper9.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,126.96,589.93,379.03,10.91;7,112.66,603.48,206.12,10.91">Proceedings of the 5th Swiss Text Analytics Conference (SwissText) &amp; 16th Conference on Natural Language Processing (KONVENS)</title>
		<meeting>the 5th Swiss Text Analytics Conference (SwissText) &amp; 16th Conference on Natural Language Processing (KONVENS)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,630.58,393.32,10.91;7,112.66,644.13,393.33,10.91;8,112.66,86.97,394.52,10.91;8,112.66,100.52,344.79,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,351.01,630.58,154.98,10.91;7,112.66,644.13,117.25,10.91">Stance detection in web and social media: a comparative study</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,251.84,644.13,254.14,10.91;8,112.66,86.97,351.29,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 10th International Conference of the CLEF Association, CLEF 2019</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="75" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,395.17,10.91;8,112.66,127.61,394.53,10.91;8,112.66,141.16,393.33,10.91;8,112.66,154.71,393.33,10.91;8,112.66,168.26,179.48,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,292.69,114.06,215.15,10.91;8,112.66,127.61,389.89,10.91">Cofe: A new dataset of intra-multilingual multitarget stance classification from an online european participatory democracy platform</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hemamou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,129.12,141.16,376.87,10.91;8,112.66,154.71,393.33,10.91;8,112.66,168.26,91.61,10.91">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="418" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,393.33,10.91;8,112.66,208.91,393.32,10.91;8,112.66,222.46,393.33,10.91;8,112.66,236.01,394.03,10.91;8,112.66,249.56,185.51,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,323.15,181.81,182.83,10.91;8,112.66,195.36,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="8,327.87,195.36,178.11,10.91;8,112.66,208.91,393.32,10.91;8,112.66,222.46,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="8,112.66,263.11,393.33,10.91;8,112.66,276.66,393.33,10.91;8,112.66,290.20,300.63,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,272.21,263.11,233.78,10.91;8,112.66,276.66,171.34,10.91">Debating europe: A multilingual multi-target stance classification dataset of online debates</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.36,276.66,198.62,10.91;8,112.66,290.20,223.04,10.91">Proceedings of the LREC 2022 workshop on Natural Language Processing for Political Sciences</title>
		<meeting>the LREC 2022 workshop on Natural Language Processing for Political Sciences</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
