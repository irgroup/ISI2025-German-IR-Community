<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.96,84.74,393.63,15.42;1,89.29,106.66,405.61,15.42;1,89.29,128.58,270.51,15.43;1,89.29,150.91,381.75,11.96">Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,65.97,11.96"><forename type="first">Max</forename><surname>Moebius</surname></persName>
							<email>max.moebius@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<postCode>07737</postCode>
									<settlement>Jena</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.41,176.82,106.44,11.96"><forename type="first">Maximilian</forename><surname>Enderling</surname></persName>
							<email>maximilian.enderling@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<postCode>07737</postCode>
									<settlement>Jena</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.36,176.82,92.13,11.96"><forename type="first">Sarah</forename><forename type="middle">T</forename><surname>Bachinger</surname></persName>
							<email>sarah.bachinger@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<postCode>07737</postCode>
									<settlement>Jena</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.96,84.74,393.63,15.42;1,89.29,106.66,405.61,15.42;1,89.29,128.58,270.51,15.43;1,89.29,150.91,381.75,11.96">Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments Notebook for the Touché Lab on Argument and Causal Retrieval at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C4D98B69B78F46DBCBD5B5B669871A3E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Retrieval</term>
					<term>Image Generation</term>
					<term>Feature Matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Participating in the shared task "Image Retrieval for arguments", we used different pipelines for image retrieval containing Image Generation, Stance Detection, Preselection and Feature Matching. We submitted four different runs with different pipeline layout and compare them to a given baseline. Our pipelines perform similarly to the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the saying goes, "a picture is worth a thousand words". A convincing argument in writing should be accompanied by an equally convincing image. There are no perfect out-of-thebox solutions so far, which is why we participated in the shared task "Image Retrieval for arguments" <ref type="bibr" coords="1,138.78,418.24,14.85,10.91" target="#b0">[1]</ref> from Touché <ref type="bibr" coords="1,215.80,418.24,11.43,10.91" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In the following section, related work covering image generation with Stable Diffusion and Feature Matching is reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stable Diffusion</head><p>"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. " <ref type="bibr" coords="2,218.51,121.08,12.70,10.91" target="#b2">[3]</ref> Stable Diffusion is a neural network that generates an image corresponding to a given text input (so-called prompt). It is possible to specify a certain image style (e.g. "comic") by including it in the prompt.</p><p>For our approach, the version stable-diffusion-v1-4 was used, which was created by resuming training from stable-diffusion-v1-2 with 225,000 steps at a resolution of 512×512 pixels <ref type="bibr" coords="2,153.18,229.48,12.36,10.91" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Matching</head><p>"Feature matching refers to the act of recognizing features of the same object across images with slightly different viewpoints" <ref type="bibr" coords="2,246.23,292.75,13.95,10.91" target="#b3">[4]</ref>. In this context, features are defined by keypoints, which causes their name feature keypoints. These feature keypoints mark an area that is particularly interesting or defining in an image.</p><p>SIFT is a feature descriptor used to detect, describe and match local features of images. For that, the descriptor uses a database of images to compare with. Every feature of the new image is compared to the database with Euclidean distance of the feature vectors to recognize objects <ref type="bibr" coords="2,164.39,387.60,12.64,10.91" target="#b4">[5]</ref>. Broadly speaking, SIFT extracts feature keypoints and feature descriptor from an image. The descriptors contain the visual description of images and are commonly used to determine the similarity between images.</p><p>FLANN stands for Fast Library for Approximate Nearest Neighbors and is used for fast nearest neighbor search in large datasets or images. In general, a matcher takes the descriptors of two images, builds pairs of features with one feature from each image, and calculates for every pair a distance. The smaller the distance, the more similar the features are to each other. With clustering and search in multidimensional spaces, the matching by FLANN is more efficient for larger datasets compared to the often used BFMatcher <ref type="bibr" coords="2,418.45,511.98,15.84,7.90" target="#b5">[6]</ref>.</p><p>Using a threshold, the feature matches are filtered. Every match has a distance value while the threshold is also a value. All matches with a distance under the threshold value are accepted and determined as good. The smaller the accepted distance values, the more similar are the matches between two images. That means the similar images are to each other.</p><p>Additionally, homography is used to determine the transformation between points in an image and projects them on to an image plane with a normalized camera. That means objects are viewed usually from different angles in two images, but they show the same objects. Therefore, the images have a different perspective. With homography, the objects in the image are made comparable by bringing the images into the same perspective. (Compare <ref type="bibr" coords="2,458.07,658.58,12.07,10.91" target="#b6">[7]</ref>) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our implementation is available on GitHub<ref type="foot" coords="3,288.76,282.66,3.71,7.97" target="#foot_0">1</ref> . The complete pipeline containing all steps is shown in Figure <ref type="figure" coords="3,164.29,297.96,3.74,10.91" target="#fig_0">1</ref>. In its current state, it's a full-rank pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Query Preprocessing</head><p>To isolate possible important terms for the following queries, the spaCy <ref type="bibr" coords="3,403.70,347.69,15.05,10.91" target="#b7">[8]</ref> library was used to parse the topic questions and generate a parse tree. Then, all tokens that were identified as punctuation and the root word (a verb) were excluded. From the remaining, only words that had a lower Zipf frequency <ref type="bibr" coords="3,379.51,388.34,14.58,10.91" target="#b8">[9]</ref> (log base 10 of times the word appears per billion words) than 5.6 were kept. This threshold was chosen because it was the average Zipf word frequency in the given corpus. After this, it was assumed that only relevant words were kept. To support the query, the root word was placed in front of the remaining words, which kept their order. For example, the phrase "Do we need sex education in schools?" was decomposed into the root word "need" and the remaining words "sex", "education", and "schools". The resulting intermediate query would be "need sex education schools". For PRO queries, the intermediate query was used as it is. For the CON queries, the negation "not" was put in front of the intermediate query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Preselection</head><p>For the initial image preselection, an index was constructed with PyTerrier <ref type="bibr" coords="3,420.57,560.01,18.31,10.91" target="#b9">[10]</ref> containing the ID of a document and its text content. Here, at most 4096 characters were used. Then, BM25 <ref type="bibr" coords="3,484.50,573.55,21.48,10.91" target="#b10">[11]</ref> was used to retrieve the best 50 images for a given query, generated as described in 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stance Detection</head><p>For the stance detection, we used a Hugging Face pipeline<ref type="foot" coords="4,355.32,105.78,3.71,7.97" target="#foot_1">2</ref> that implements the work from Wenpeng Yin and Roth <ref type="bibr" coords="4,191.40,121.08,17.91,10.91" target="#b11">[12]</ref> for zero-shot classification with the BART model after being trained on the MultiNLI data set. The pipeline was freely available and can be loaded with the "zeroshot-classification" pipeline from hugging face <ref type="foot" coords="4,296.76,146.43,3.71,7.97" target="#foot_2">3</ref> . The model receives a text and different labels and outputs the probability of each label being a good descriptor for the given text. Hence, either "contra", "pro" or "neutral" was added in front of the given query. The highest probability was assumed to show the stance of the image. They were sorted according to the score and the image IDs returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Image Generation</head><p>The image generation is used to generate images with given queries/phrases. For that, we used Stable Diffusion.</p><p>A generated image for a query displays the information of the query visually. With the use of image comparison methods (like Feature Matching), every image of a dataset is compared to the generated image. The more similar an image is to the generated one, the better the image to the query. The next section explains that in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Image Ranking (Feature Matching)</head><p>For a given query, a series of images are ranked using Feature Matching. The set of images may include a variety of styles, therefore feature matching with generated images of various styles can improve query matching overall. As a result, cartoonish and photorealistic visuals are produced. For this, we concatenate a style prompt ("a photograph about the topic:" or "an image in comic style about the topic:") and the query, which is then processed by Stable Diffusion to generate an image.</p><p>Matching features between a pair of images are detected using feature matching, always between one generated image and one from the queried dataset (see section 2.2). As a result, for each image, the number of matches above the threshold is returned. The assumption is that an image with many good matches is a desirable result for the query. The images are rated according to the numbers of matches, and the top ten images are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Submission</head><p>We submitted 5 runs through Tira <ref type="bibr" coords="4,240.14,625.09,17.76,10.91" target="#b12">[13]</ref> with different combinations of the approaches described in Section 3, namely: In every approach, Image Generation and Image Ranking was used to determine the final ranked results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relevance evaluation</head><p>From the 5 pipelines, we collected the top 10 images returned by the pro and con queries for each of the 50 topics, gathering a total of 5000 images. After duplicate removal, 2091 images remained and were independently judged by three annotators with the labels off-topic, pro, con and neutral. The Fleiss-Kappa for the tree annotators was 0.38. For the evaluation of the different approaches, the image judgments were curated as follows:</p><p>• if two annotators agree on a label, this labels is chosen • if there is no majority agreement and the image is labeled by more than one as on topic, its label will be neutral</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pipeline Evaluation</head><p>We evaluated the pipelines with the curated data as described above and with the judgments from the Touché Team <ref type="bibr" coords="5,198.87,557.42,11.58,10.91" target="#b1">[2]</ref>. Precision@10, precision@1 and mean average precision were calculated. Furthermore, we used a student's t-test to find out whether the difference in values for average precision of the individual runs is significant compared to the -1 (baseline) run. Table <ref type="table" coords="5,115.79,598.07,5.07,10.91" target="#tab_0">1</ref> shows the results for our curated judgements, Table <ref type="table" coords="5,356.96,598.07,5.07,10.91" target="#tab_1">2</ref> for the Touché judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The results above indicate that even though the precision values vary across the different pipelines, none are statistically significant different from the baseline. We observed relatively low values for precision on average. This may be partially due to missing relevant pictures in the corpus. Judging by the low Fleiss-Kappa, the task of evaluating the stance and relevance of an image for a certain topic was ambiguous, which was confirmed by the annotators. For future applications, an annotation guideline should be given to the annotators to avoid confusion. Overall, the results with the different judgements seem to agree. We see that the inclusion of stance detection on the website text seems to be not beneficial in our case. Future work could determine if other stance detection models also suffer from this. Furthermore, selecting a higher amount of pictures than the current number of 50 during pre-selection could lead to different results. This is supported by the fact that out of the 5000 results for the 5 pipelines, actually only 1938 were unique. Since all of our results are chosen from the same 50 (# topics) •2 (# of stances) •50 (# of pre-selected images per query via BM25, see section 3.2)= 5000 images, this may be part of the poor results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>As seen in Section 4, our pipelines perform not significantly better or worse than the baseline.</p><p>The assumption was that with the inclusion of more information like stance detection the pipeline would perform better, namely pipeline 3 would perform best. We could not find evidence for that with our approach. In the experimental setting, the pipeline 0 was the best in precision@10 for both judged images and pipeline 2 the best MAP for both judged images. For precision@1, pipeline 0 was best with our curated data, for the given data pipeline 2 performed best.</p><p>Different things could be changed in order to get better results like testing other variations of stance detection and image generation with image ranking. Other, untested approaches and a change in the approach combination used may lead to different results. As mentioned in the section 5, maybe the corpus is at fault and more pictures with a clearer stance may lead to better results. Increasing the number of pre-selected images could also serve as a topic for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,222.50,119.91,8.93;3,89.29,84.19,416.69,125.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The whole pipeline</figDesc><graphic coords="3,89.29,84.19,416.69,125.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,417.00,234.46"><head>Table 1</head><label>1</label><figDesc>Precision@10, precision@1, mean average precision (MAP), and p-value for student's test for the pipelines with the curated data corpus Query Preprocessing and Image Preselection with Stance Detection on text 4. Pipeline 2: Query Preprocessing and Image Preselection with Stance Detection on image text 5. Pipeline 3: Query Preprocessing and Image Preselection with Stance Detection on text and image text</figDesc><table coords="5,103.64,134.06,340.55,136.36"><row><cell>Pipeline ID</cell><cell>Precision@10</cell><cell>Precision@1</cell><cell>MAP</cell><cell>p-value</cell></row><row><cell>-1</cell><cell>0.146</cell><cell>0.130</cell><cell>0.122</cell><cell></cell></row><row><cell>0</cell><cell>0.155</cell><cell>0.160</cell><cell>0.111</cell><cell>0.699</cell></row><row><cell>1</cell><cell>0.131</cell><cell>0.150</cell><cell>0.092</cell><cell>0.271</cell></row><row><cell>2</cell><cell>0.115</cell><cell>0.181</cell><cell>0.146</cell><cell>0.491</cell></row><row><cell>3</cell><cell>0.149</cell><cell>0.120</cell><cell>0.109</cell><cell>0.658</cell></row><row><cell cols="4">1. Pipeline -1: Baseline provided by the Touché Team [2]</cell><cell></cell></row><row><cell cols="4">2. Pipeline 0: Query Preprocessing and Image Preselection only</cell><cell></cell></row><row><cell>3. Pipeline 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,417.00,117.85"><head>Table 2</head><label>2</label><figDesc>Precision@10, precision@1, mean average precision (MAP), and p-value for student's test for the pipelines given by the Touché Team</figDesc><table coords="6,129.72,134.06,314.47,74.28"><row><cell>Pipeline ID</cell><cell>Precision@10</cell><cell>Precision@1</cell><cell>MAP</cell><cell>p-value</cell></row><row><cell>-1</cell><cell>0.127</cell><cell>0.130</cell><cell>0.097</cell><cell></cell></row><row><cell>0</cell><cell>0.141</cell><cell>0.170</cell><cell>0.100</cell><cell>0.911</cell></row><row><cell>1</cell><cell>0.115</cell><cell>0.120</cell><cell>0.084</cell><cell>0.602</cell></row><row><cell>2</cell><cell>0.090</cell><cell>0.106</cell><cell>0.113</cell><cell>0.621</cell></row><row><cell>3</cell><cell>0.122</cell><cell>0.110</cell><cell>0.096</cell><cell>0.956</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,670.91,184.33,8.97"><p>https://github.com/ArcticF0x99/ir_image_retrieval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,660.08,177.33,8.97"><p>https://huggingface.co/facebook/bart-large-mnli</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,92.57,671.04,297.31,8.97"><p>classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="institution">Touché organization team</rs> and <rs type="person">Prof. Hagen</rs> and his chair for their continued helpful suggestions and support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,111.28,393.33,10.91;7,112.66,124.83,393.33,10.91;7,112.28,138.38,394.91,10.91;7,112.66,151.93,394.51,10.91;7,112.66,167.92,109.72,7.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,334.11,111.28,171.87,10.91;7,112.66,124.83,135.41,10.91">Image Retrieval for Arguments Using Stance-Aware Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.argmining-1.4</idno>
		<ptr target="https://aclanthology.org/2021.argmining-1.4/.doi:10.18653/v1/2021.argmining-1.4" />
	</analytic>
	<monogr>
		<title level="m" coord="7,431.19,124.83,74.79,10.91;7,112.28,138.38,390.76,10.91">8th Workshop on Argument Mining (ArgMining 2021) at EMNLP, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stede</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,179.03,394.53,10.91;7,112.48,192.57,393.50,10.91;7,112.66,206.12,393.33,10.91;7,112.66,219.67,393.53,10.91;7,112.66,233.22,291.42,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,295.69,192.57,210.29,10.91;7,112.66,206.12,37.55,10.91">Overview of Touché 2023: Argument and Causal Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schlatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barriere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ravenet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hemamou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.83,206.12,333.15,10.91;7,112.66,219.67,268.87,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 14th International Conference of the CLEF Association (CLEF 2023)</title>
		<title level="s" coord="7,388.23,219.67,117.96,10.91;7,112.66,233.22,31.10,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="7,112.66,246.77,393.33,10.91;7,112.26,260.32,393.94,10.91;7,112.30,273.87,281.16,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,364.74,246.77,141.25,10.91;7,112.26,260.32,123.42,10.91">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,259.20,260.32,247.00,10.91;7,112.30,273.87,172.53,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,287.42,394.03,10.91;7,112.66,300.97,196.90,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,160.10,287.42,101.69,10.91">Local feature matching</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Roelke</surname></persName>
		</author>
		<ptr target="https://cs.brown.edu/courses/cs143/2013/results/proj2/rroelke/,visitedon2023-03-08" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,314.52,393.33,10.91;7,112.66,328.07,197.64,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,173.24,314.52,262.03,10.91">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,446.71,314.52,59.28,10.91;7,112.66,328.07,118.78,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,341.62,173.36,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,164.16,341.62,92.01,10.91">The OpenCV Library</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,355.17,393.32,10.91;7,112.66,368.71,193.93,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,247.29,355.17,171.85,10.91">The geometric error for homographies</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.24,355.17,77.74,10.91;7,112.66,368.71,115.07,10.91">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="86" to="102" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,382.26,393.32,10.91;7,112.66,395.81,324.50,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1212303</idno>
		<title level="m" coord="7,353.07,382.26,152.91,10.91;7,112.66,395.81,140.35,10.91">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,409.36,395.17,10.91;7,112.66,422.91,142.26,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,161.78,409.36,321.12,10.91">The psycho-biology of language: An introduction to dynamic philology</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Psychology Press</publisher>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,436.46,393.32,10.91;7,112.66,450.01,205.58,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,244.82,436.46,261.16,10.91;7,112.66,450.01,37.16,10.91">Declarative experimentation ininformation retrieval using pyterrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.51,450.01,115.78,10.91">Proceedings of ICTIR 2020</title>
		<meeting>ICTIR 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,463.56,393.33,10.91;7,112.66,477.11,395.17,10.91;7,112.66,490.66,393.32,10.91;7,112.66,504.21,334.53,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,232.98,463.56,273.01,10.91;7,112.66,477.11,157.60,10.91">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,296.29,477.11,211.54,10.91;7,112.66,490.66,393.32,10.91;7,112.66,504.21,38.01,10.91">SIGIR&apos;94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Dublin City University</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,517.76,395.17,10.91;7,112.66,531.30,386.39,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,238.73,517.76,269.10,10.91;7,112.66,531.30,129.17,10.91">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.00161" />
	</analytic>
	<monogr>
		<title level="m" coord="7,265.24,531.30,30.76,10.91">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,544.85,394.52,10.91;7,112.66,558.40,394.62,10.91;7,112.48,571.95,394.70,10.91;7,112.28,585.50,393.71,10.91;7,112.66,599.05,393.33,10.91;7,112.66,612.60,397.48,10.91;7,112.36,628.59,152.76,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,174.00,558.40,296.53,10.91">Continuous Integration for Reproducible Shared Tasks with TIRA</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kolyada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grahm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elstner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Loebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-28241-6_20.doi:10.1007/978-3-031-28241-6_20" />
	</analytic>
	<monogr>
		<title level="m" coord="7,192.86,585.50,313.12,10.91;7,112.66,599.05,95.19,10.91">Advances in Information Retrieval. 45th European Conference on IR Research (ECIR 2023)</title>
		<title level="s" coord="7,215.26,599.05,158.83,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="236" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
