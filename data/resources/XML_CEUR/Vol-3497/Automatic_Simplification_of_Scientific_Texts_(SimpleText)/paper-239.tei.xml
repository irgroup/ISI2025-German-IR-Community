<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,361.11,15.42;1,89.29,106.66,378.14,15.42">Overview of the CLEF 2023 SimpleText Task 2: Difficult Concept Identification and Explanation</title>
				<funder ref="#_em3guTg">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,79.43,11.96"><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
							<email>liana.ermakova@univ-brest.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Bretagne Occidentale</orgName>
								<orgName type="institution" key="instit2">HCTI</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.37,134.97,96.95,11.96"><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Elsevier</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.95,134.97,60.91,11.96"><forename type="first">Sarah</forename><surname>Bertin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Bretagne Occidentale</orgName>
								<orgName type="institution" key="instit2">HCTI</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.86,134.97,83.83,11.96"><forename type="first">Olivier</forename><surname>Augereau</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">ENIB</orgName>
								<orgName type="laboratory" key="lab2">Lab-STICC UMR CNRS 8265</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,361.11,15.42;1,89.29,106.66,378.14,15.42">Overview of the CLEF 2023 SimpleText Task 2: Difficult Concept Identification and Explanation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">62F3E492C227830CFDD9FD5623A0045F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic text simplification</term>
					<term>terminology</term>
					<term>background knowledge</term>
					<term>scientific article</term>
					<term>science popularization</term>
					<term>contextualization</term>
					<term>term difficulty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an overview of the "Task 2: What is unclear? Difficult concept identification and explanation" within the context of the Automatic Simplification of Scientific Texts (SimpleText) lab, run as part of CLEF 2023. The primary objective of the SimpleText lab is to advance the accessibility of scientific information by facilitating automatic text simplification, thereby promoting a more inclusive approach to scientific knowledge dissemination. Task 2 focuses on complexity spotting within scientific texts (passage). Thus, the goal is to detect the terms/concepts that require specific background knowledge for understanding the passage, assess their complexity for non-experts, and provide explanations for these detected difficult concepts. A total of 39 submissions were received for this task, originating from 12 distinct teams. In this paper, we describe the data collection process, task configuration, and evaluation methodology employed. Additionally, we provide a brief summary of the various approaches adopted by the participating teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction learned.</head><p>Text simplification techniques can play a crucial role in enabling readers to stay informed about scientific advancements. Traditional methods of simplification aim to eliminate complex terms and structures in order to enhance readability <ref type="bibr" coords="2,315.18,127.61,11.27,10.91" target="#b0">[1]</ref>. However, this approach may not always be feasible, particularly when dealing with scientific literature. In such cases, readers relying on popular science publications draw upon their experience in processing new information. They can identify instances where they require definitions or clarifications for unfamiliar terms, as their understanding of the underlying concepts may be limited. This recognition of the need for additional explanations or clarifications reflects readers' awareness of their own comprehension gaps in relation to unfamiliar terminology which is perceived as a difficulty.</p><p>We argue that a text simplification method should offer essential information required for understanding complex scientific concepts to address the issue of inadequate background knowledge hindering proper comprehension <ref type="bibr" coords="2,287.60,249.56,11.30,10.91" target="#b1">[2]</ref>. This objective is one of the focal points of the CLEF 2023 SimpleText lab. Although there have been notable advancements in automatic text simplification, such as the work by Maddela et al. on controllable simplification <ref type="bibr" coords="2,453.94,276.66,11.59,10.91" target="#b2">[3]</ref>, there is still an ongoing challenge in automatically enhancing the comprehensibility of scientific texts and adapting them to different target audiences.</p><p>The CLEF 2023 SimpleText track <ref type="foot" coords="2,245.43,315.55,3.71,7.97" target="#foot_1">1</ref> is a new evaluation lab that follows up on the CLEF 2021 SimpleText Workshop <ref type="bibr" coords="2,193.98,330.85,13.00,10.91" target="#b3">[4]</ref> and CLEF 2022 SimpleText Track <ref type="bibr" coords="2,368.31,330.85,11.58,10.91" target="#b4">[5]</ref>. The track offers valuable data and benchmarks to facilitate discussions on the challenges associated with automatic text simplification. It presents an interconnected framework that encompasses various tasks, providing a comprehensive view of the complexities involved: Task 1: What is in (or out)? Selecting passages to include in a simplified summary.</p><p>Task 2: What is unclear? Difficult concept identification and explanation (definitions, abbreviation deciphering, context, applications,..).</p><p>Task 3: Rewrite this! Given a query, simplify passages from scientific abstracts.</p><p>This paper focuses on the second task of complexity spotting. The goal of this task is to detect difficult terms and provide contextual explanations for them. Identifying and effectively explaining difficult terms is crucial for promoting accessibility and comprehension of scientific texts. Please refer for details of the other tasks to the overview papers of Task 1 <ref type="bibr" coords="2,443.13,515.76,12.77,10.91" target="#b5">[6]</ref> and Task 3 <ref type="bibr" coords="2,89.29,529.31,11.43,10.91" target="#b6">[7]</ref>, as well as the Track overview paper <ref type="bibr" coords="2,269.81,529.31,11.43,10.91" target="#b7">[8]</ref>. The rest of this paper is structured in the following way. A comprehensive description of the Task 2 is presented in Section 2. Following that, Section 3 provides an overview of the dataset used, including its composition, size, and relevant characteristics. In Section 4, the paper discusses the evaluation metrics employed to assess the performance of the participants' runs. Section 5 delves into the details of the systems and approaches employed by the participants. In Section 6, we discuss the results of the official submissions. We end with Section 7 discussing the results and findings, and lessons for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task description</head><p>The objective of Task 2 is twofold, identification of difficult that require contextualization through definition, example, and/or use case, as well as the provision of clear and informative explanations for these concepts. Consequently, the task can be divided into two subtasks:</p><p>• to retrieve up to 5 difficult terms in a given passage from a scientific abstract; • to provide an explanation (one/two sentences) of these difficult terms (e.g. definition, abbreviation deciphering, example, etc.).</p><p>In the context of the SimpleText track, difficult terms are defined as words or phrases that present challenges for readers due to their complexity, specialized meanings, or technical nature. These terms require additional explanation or clarification to ensure a better understanding for readers who may not be familiar with them. By providing explanations for such difficult terms, readers can overcome the potential obstacles they pose and enhance their comprehension of the text. Difficult terms often involve scientific jargon, complex theories, mathematical equations, or intricate scientific concepts that may be unfamiliar to the general reader or even to experts in other scientific domains.</p><p>Participants in Task 2 are required to submit a ranked list of difficult terms for each passage, along with corresponding difficulty scores on a scale of 0 to 2. A score of 2 indicates the highest level of difficulty, whereas a score of 0 implies that the meaning of the term can be inferred or guessed. Optionally, participants can provide definitions for the identified difficult terms. It is important to note that passages (sentences) are treated as independent entities, meaning that repetition of difficult terms across multiple passages is allowed and evaluated separately. Table <ref type="table" coords="3,501.27,393.98,4.97,10.91" target="#tab_0">1</ref> serves as a reference, providing examples that illustrate different levels of term difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets for Task 2.1</head><p>As part of the task, participants were supplied with a training set consisting of 203 pairs of sentences and their corresponding scientific terms with ground truth annotations of difficulty scores for each term on a scale of 0-2. These sentence and difficult term pairs were extracted from relevant abstracts obtained from Task 1 <ref type="bibr" coords="3,291.62,527.77,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="3,305.71,527.77,12.32,10.91" target="#b9">10]</ref>.</p><p>To build the test set for Task 2.1, a total of 116,763 sentences were extracted from the DBLP abstracts. Then, a subset of 1,262 unique sentences was manually evaluated to assess the performance of various models in terms of their capability to identify difficult terms and assign appropriate difficulty scores. To obtain a comprehensive evaluation, a pooling mechanism was implemented, resulting in the annotation of 5,142 distinct pairs of sentence-term combinations. Each evaluated source sentence contained the aggregated results from all participating participants. This process ensured a reliable and robust assessment of the performance of different models in detecting difficult terms and estimating their difficulty scores.</p><p>To promote a degree of overlap among the partial runs submitted by participants, a set of three test sets was provided: small, medium, and large. It was anticipated that participants would prefer employing Language Models (LLMs), which could lead to the generation of partial runs due to the limitations in efficiency associated with these models. By offering different test sets, we aimed to accommodate diverse computational limitations and facilitate the participation of various approaches, including those leveraging LLMs.</p><p>The small dataset was embedded within the medium dataset, and the medium dataset was, in turn, encompassed within the large dataset. By evaluating systems on the small test sets, we ensured some common ground for comparing the partial runs generated by different participants. This approach facilitated the comparison and analysis of system outputs, despite being derived from different test sets.</p><p>Inclusion of the train data within the small dataset allowed for a comparison of system performance on both training and testing data. This integration of train data facilitated evaluating how well the systems could generalize to unseen test data by assessing their performance on familiar training examples. Such a comparison provided valuable insights into the effectiveness and robustness of the systems across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Datasets for Task 2.2</head><p>For Task 2.2, the training set encompasses the same set of 203 difficult terms as in Task 2.1. However, in Task 2.2, the training set is augmented with the addition of corresponding definitions for each of these difficult terms.</p><p>To evaluate the performance of the submitted runs for this task, a test set comprising approximately 800 terms with ground truth definitions is utilized. This test set serves as the benchmark against which the performance of the participants' systems is measured. The runs were assessed by comparing the outputs of the systems with the ground truth definitions. The utilization of a substantial number of terms in the test set ensures a comprehensive evaluation of the systems' performance in interpreting and providing accurate definitions for the given terms. From this set, ∼300 terms are selected for annotation using a pooling mechanism, ensuring that the test set contains a sufficient number of annotated samples for most runs. The test set comprises a total of 15,056 sentences that contain at least one of these terms. These sentences are utilized for evaluating the performance of the submitted runs. For the evaluation of abbreviation expansion, a set of ∼1K abbreviations is manually annotated. Additionally, an extra 4,374 abbreviations are extracted using the Schwartz and Hearst <ref type="bibr" coords="5,346.18,380.58,17.91,10.91" target="#b10">[11]</ref> algorithm from the sentences in Task 1, resulting in a total of ∼5K abbreviations. The final test set consists of 38,416 sentences that contain at least one of these abbreviations, and this set of sentences is employed for the final evaluation of this subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Input format</head><p>The train and the test data are provided in JSON and TSV formats with the following fields: snt_id a unique passage (sentence) identifier doc_id a unique source document identifier query_id a query ID query_text difficult terms should be extracted from sentences with regard to this query source_snt passage text Input example:</p><p>[{"query_id":"G14.2", "query_text":"end to end encryption", "doc_id":"2884788726", "snt_id":"G14.2_2884788726_2", "source_snt":"However, in information-centric networking (ICN) the end-to-end encryption makes the content caching ineffective since encrypted content stored in a cache is useless for any consumer except those who know the encryption key."}, ˓→ ˓→ ˓→ {"snt_id":"G06.2_2548923997_3", "doc_id":2548923997, "query_id":"G06.2", "query_text":"self driving", "source_snt":"These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks."}] ˓→ ˓→</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Output format</head><p>Results should be provided in a TREC-style JSON or TSV format with the following fields:</p><p>run_id Run ID starting with (team_id)_(task_id)_(method_used), e.g. UBO_task_2.1_TFIDF</p><p>manual Whether the run is manual {0, 1}.</p><p>snt_id a unique passage (sentence) identifier from the input file.</p><p>term Term or another phrase to be explained.</p><p>term_rank_snt term difficulty rank within the given sentence.</p><p>difficulty difficulty scores of the retrieved term on the scale 0-2 (2 to be the most difficult terms, while the meaning of terms scored 0 can be derived or guessed) definition (only used for Task 2.2) short (one/two sentence) explanations/definitions for the terms. For the abbreviations, the definition would be the extended abbreviation.</p><p>Output example Task 2.1:</p><p>[{"snt_id":"G14.2_2884788726_2", "term":"content caching", "difficulty":1.0, "term_rank_snt":1, "run_id":"team1_task_2.1_TFIDF", "manual":0}]</p><p>Output example Task 2.2:</p><p>[{"snt_id":"G14.2_2884788726_2", "term":"content caching", "difficulty":1.0, "term_rank_snt":1, "definition":"Content caching is a performance optimization mechanism in which data is delivered from the closest servers for optimal application performance.", ˓→ "run_id":"team1_task_2.2_TFIDF_BLOOM", "manual":0}] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation metrics</head><p>In this section, we describe different evaluation metrics used to evaluate the performance of submissions for Task 2.1 and Task 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation metrics for Task 2.1</head><p>We have evaluated the performance of different submissions for Task 2.1 based on:</p><p>• correctness of detected term span (limits): this metric reflects whether the retrieved difficult terms are well limited or not. This is a binary label assigned to each retrieved term. • difficulty scores: we used a three-scale terms difficulty score which reflects how difficult the term is in the context for an average user and how necessary it is to provide more context about the term: 0 score corresponds to an easy term (explanation might be given but not required); 1 corresponds to somewhat difficult (explanation could help); 2 corresponds to difficult (explanation is necessary). Table <ref type="table" coords="7,374.39,445.58,5.17,10.91" target="#tab_0">1</ref> contains examples of terms with different difficulty scores.</p><p>If an extracted term is considered to be a scientific term, we then assessed its limits, i.e. that it refers to a scientific concept mentioned in the context sentence, and its difficulty. For difficult scientific terms, after correcting the term limits if necessary, we assessed the difficulty of the scientific term. Finally, if any scientific terms have not been extracted, we added them to the list.</p><p>Table <ref type="table" coords="7,128.21,546.77,5.17,10.91" target="#tab_1">2</ref> provides some examples of the annotation for Task 2. TERM refers to the terms retrieved by participants, Correct limits is a binary category showing whether the retrieved terms is well limited, Corrected is an eventual correction of retrieved term limits, Difficulty is a term difficulty score in scale 0-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics for Task 2.2</head><p>For this task, we use the following evaluation metrics:</p><p>• BLEU score <ref type="bibr" coords="7,176.88,656.03,18.07,10.91" target="#b11">[12]</ref> between the reference (ground truth definition) and the predicted definitions.</p><p>• ROUGE L F-measure <ref type="bibr" coords="8,218.95,86.97,17.76,10.91" target="#b12">[13]</ref> which measures the ROUGE F-measure based on the Longest Common Subsequence between the reference and the predicted definitions. • Semantic match between the reference and predicted definitions measured using the all-mpnet-base-v2<ref type="foot" coords="8,199.80,127.21,3.71,7.97" target="#foot_2">2</ref> sentence transformer model which is an advanced model for sentence similarity. This measure is the average semantic similarity between reference and predicted definitions for all detected terms. • Exact match is specifically applied to the task of abbreviation extension. In this task, participants are required to provide extensions (full forms) for the detected difficult abbreviations. The exact match metric quantifies the number of cases where the reference and predicted extensions for the abbreviations match exactly. It measures the accuracy of the predicted extensions by considering the extent to which they align perfectly with the provided reference extensions. • Partial match evaluates the similarity between reference and predicted abbreviation extensions by considering non-identical matches with a Levenshtein distance lower than 4 characters. It quantifies the number of cases where the extensions exhibit slight variations, such as differences in plural and non-plural forms, while still being considered as acceptable matches. The partial match metric captures the level of similarity between the reference and predicted extensions, allowing for minor discrepancies within a certain threshold.</p><p>Table <ref type="table" coords="8,126.14,357.43,4.97,10.91" target="#tab_2">3</ref> shows a set of examples of terms and their ground truth defintions used for evaluating the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Participants' approaches</head><p>12 distinct teams submitted 39 runs in total.</p><p>National Polytechnic Institute of Mexico (NLPalma) <ref type="bibr" coords="8,351.70,458.37,17.96,10.91" target="#b13">[14]</ref> submitted a total of 2 runs for Task 2, a single run for each of Task 2.1 and Task 2.2. They experimented with BLOOMZ to produce description-style prompts given by text input on a task and a binary classifier based on BERT-multilingual for term difficulty.</p><p>University of Amsterdam (UAms) <ref type="bibr" coords="8,268.40,527.77,18.07,10.91" target="#b14">[15]</ref> submitted a single run for Task 2 focusing on complexity spotting. Their approach aimed to demonstrate the relative effectiveness of simple and straightforward approaches, and made use of standard TF-IDF based term-weighting using the large test set as a source for within-domain term statistics.</p><p>University of Cadiz/Split (Smroltra) <ref type="bibr" coords="8,269.12,597.18,17.85,10.91" target="#b15">[16]</ref> submitted a total of 20 runs for Task 2, with both 10 runs for Task 2.1 and 10 runs for Task 2.2. They experimented with a range of keyword extraction approaches (KeyBERT, RAKE, YAKE!, BLOOM, T5, TextRank) for the first task, and a Wikipedia extraction approach, BERT, and BLOOMZ for the second task. and information filtering, in which case they evaluate replies and return only the relevant data. " "Information Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers). " 2 "In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. "</p><p>"Cryptojacking is the act of hijacking a computer to mine cryptocurrencies against the user's will, through websites, or while the user is unaware. "</p><p>University of Guayaquil/Jaén (SINAI) <ref type="bibr" coords="9,284.12,381.87,18.07,10.91" target="#b16">[17]</ref> submitted a total of 6 runs for Task 2, with 4 runs for Task 2.1 and 2 runs for Task 2.2. They investigated zero-shot and few-shot learning strategies over the auto-regressive model GPT-3, and in particular effective prompt engineering.</p><p>University of Kiel (TeamCAU) <ref type="bibr" coords="9,238.41,437.73,17.75,10.91" target="#b17">[18]</ref> submitted 6 runs for Task 2, based on three different large pre-trained language models (SimpleT5, AI21, and BLOOM). They made three and corresponding submissions to both Task 2.1 and 2.2, and also note the complexities of adapting models with limited train data.</p><p>University of Kiel/Split/Malta (MicroGerk) <ref type="bibr" coords="9,299.62,507.13,17.75,10.91" target="#b18">[19]</ref> submitted a total of 8 runs for Task 2, with 4 runs for Task 2.1 and 4 runs for Task 2.2. They experimented with a range of models (YAKE!, TextRank, BLOOM, GPT-3) for the first task, and a range of models (Wikipedia, SimpleT5, BLOOMZ, GPT-3) for the second task.</p><p>University of Southern Maine (Aiirlab) <ref type="bibr" coords="9,280.73,576.54,17.75,10.91" target="#b19">[20]</ref>  University of Kiel/Cadiz/Gdansk (TheLangVerse) submitted a total of 2 runs for Task 2, a single run for both Task 2.1 and Task 2.2 using GPT-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We evaluate the performance of the submissions separately for the difficult terms spotting (Task 2.1) and definition extraction/generation (Task 2.2) using separate test sets created per task. In this section, we describe the main results of different submissions per task. The performance of the submissions is evaluated separately for two distinct tasks: difficult terms spotting (Task 2.1) and definition extraction/generation (Task 2.2). Each task is evaluated using its respective test set. In this section, we provide an overview of the key results obtained from different submissions for each task. By examining the results of the submissions, we gain insights into the effectiveness of various approaches employed for difficult terms spotting and definition extraction/generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results of Task 2.1: difficult term spotting</head><p>In this section, we focus on the results of the submissions for Task 2.1. A total of 12 teams participated and submitted a combined total of 39 runs for this task. The outcomes of these runs are presented in Table <ref type="table" coords="10,190.98,484.83,3.76,10.91" target="#tab_4">4</ref>, which contains the following metrics: the total number of evaluated terms, the number of terms with correct term limits, the number of correctly attributed scores (regardless of term limits), and the number of correctly limited terms with correctly attributed scores (+Limits). Among all the runs for Task 2.1, the SINAI_task_2.1_PRM_ZS_TASK2_1_V1 run achieved the highest number of correctly detected terms and scores. The performance of different runs on both train and test sets (only on correctly limited terms) is presented in Table <ref type="table" coords="10,89.29,566.13,3.74,10.91" target="#tab_5">5</ref>. Most approaches achieve comparable performance on both train and test sets. The estimation of difficulty scores for terms proved to be a challenging task, as the majority of the submitted runs struggled to provide accurate scores for more than half of the detected difficult terms. This indicates the difficulty and subjectivity inherent in determining the level of complexity or difficulty associated with specific terms in scientific texts. The variability in assessing the difficulty of terms highlights the need for further research and improvement in developing robust methods for accurately assigning difficulty scores to terms. Participants employed a range of approaches, including Large Language Models (LLMs) and unsupervised methods, to tackle the task. However, several runs were limited or incomplete due to token constraints imposed by LLMs or the time required for their execution. It was observed that the results of the same methods varied significantly depending on the specific implementation, fine-tuning techniques, and prompts utilized during the process. In terms of difficult term detection, LLMs demonstrated comparable performance to other methods such as RareIDF, TextRank, and YAKE!. However, it is worth noting that the term difficulty scores assigned by the models differed considerably from the lay annotations. For Task 2.2, a total of 10 teams submitted 29 runs. The main results for this task can be found in Table <ref type="table" coords="13,161.28,556.41,3.81,10.91" target="#tab_6">6</ref>. It is important to note that the low number of evaluated sentences for most runs is due to the fact that they were conducted on a smaller subset of sentences from the test set. Nevertheless, the remaining runs demonstrated strong performance in terms of the semantic similarity between their provided definitions and the ground truth definitions. Notably, the runs UBO_task_2.1_-FirstPhrases_Wikipedia, Croland_task_2_PKE_Wiki, and MiCroGerk_task_2.2_GPT-3_-Wikipedia achieved high scores in terms of BLEU metric. This indicates that even though these runs used different sets of words compared to the ground truth definitions, they were able to provide explanations for the difficult terms that were semantically similar to the reference definitions. Moreover, it is worth mentioning that the runs based on Wikipedia as a resource displayed the highest similarity with the ground truth definitions. This suggests that leveraging Wikipedia as a knowledge source yielded favorable results in terms of aligning the provided definitions with the reference definitions. We observe a similar ranking of the runs on the train set presented in Table <ref type="table" coords="14,280.64,488.98,3.66,10.91" target="#tab_8">7</ref>. An interesting observation is made regarding the scores on the training set being lower than those on the test set. This discrepancy can potentially be explained by the inclusion of a significant number of abbreviations in the training set. The presence of these abbreviations may have introduced additional complexity and challenges during the training process, resulting in lower scores on the training set. To mitigate this issue and ensure a fair evaluation, the test set was designed to consider abbreviations separately. This separate consideration of abbreviations in the test set allows for a more accurate assessment of the models' performance in handling and interpreting these specific elements. Table <ref type="table" coords="14,128.72,597.38,5.17,10.91" target="#tab_9">8</ref> presents the performance of the runs on the abbreviation expansion task. The MiCroGerk_task_2.2_GPT-3_BLOOMZ run achieved the highest performance among all the runs for this task. This top-performing model successfully provided 326 identical expansions to the true expansions and 185 partially correct expansions. Overall, LLMs (such as BLOOMz and GPT-3) demonstrated the best performance in terms of abbreviation expansion. It is important to note that the scores provided in the table are averaged over the number of evaluated instances, giving preference to smaller runs. The presence of many partial runs can be attributed to the token or time constraints imposed by LLMs. Furthermore, it should be acknowledged that the evaluation results are dependent on the terms extracted in Task 2.1. The performance of the abbreviation expansion task is influenced by the quality and accuracy of the detected difficult terms in the previous task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Analysis of terms and definitions' difficulty.</head><p>A master student in translation and technical writing manually assigned difficulty scores on a scale of 1-7 to the syntax and vocabulary of 319 simplified sentences from the participants' runs corresponding to 17 distinct source sentences. Table <ref type="table" coords="15,349.25,628.93,5.12,10.91" target="#tab_10">9</ref> provides evidence that automatic simplification is effective in terms of reducing syntax difficulty. However, lexical difficulty, i.e. the presence of difficult scientific terms, is much higher, remaining the main barrier to understanding a scientific text. More details can be found in <ref type="bibr" coords="15,359.23,669.58,11.43,10.91" target="#b6">[7]</ref>.   To assess the effectiveness of the provided definitions, a master's student in translation and technical writing conducted a manual evaluation. The student assigned difficulty scores on a scale of 1 to 3 (1 being easy, 2 being difficult, and 3 being very difficult) to a total of 744<ref type="foot" coords="16,501.78,396.88,3.71,7.97" target="#foot_3">3</ref> definitions and corresponding terms pooled from the participants' runs and the ground truth. Out of the initial 744 instances, a total of 386 instances were retained for further analysis and evaluation. The decision to exclude the remaining instances was based on the fact that they were deemed incorrect in the given context. The analysis covered 135 unique terms, and the student evaluated the level of difficulty associated with each definition to gauge its helpfulness in understanding the terms.</p><p>Most of the difficulty level 3 scientific terms we have identified were abbreviations or highly specific terms within a particular domain. An abbreviation that is not explained within a given excerpt is often incomprehensible and impossible to guess for a general reader. The other difficulty level 3 terms we have identified are also highly specific to a domain and cannot be understood unless one is specialized in that field. They are also terms that are difficult to define in a simple manner for the most part.</p><p>Figure <ref type="figure" coords="16,132.71,574.77,5.17,10.91" target="#fig_1">1</ref> presents the distribution of easy, difficult, and very difficult definitions in both the participants' runs and the ground truth. The figure offers compelling evidence that in almost half of the cases for both the runs and the ground truth, the definitions are perceived as easy by a non-expert in computer science. Notably, in our ground truth, there is a higher proportion of difficult definitions and a lower proportion of very difficult definitions compared to the participants' runs. This discrepancy suggests that the ground truth definitions tend to be  relatively more challenging, with fewer easy definitions, as compared to the definitions provided by the participants. It highlights the importance of considering the difficulty level of definitions and striving for a balance that caters to a diverse audience, including non-experts in the field.</p><p>Although the majority of definitions are considered to be easy, this evidence is not enough to make a conclusion about their helpfulness. Therefore, we decided to compare the term difficulty and the corresponding definitions' difficulty. Figure <ref type="figure" coords="17,315.48,359.11,4.97,10.91" target="#fig_3">2</ref> illustrates the histogram of the differences between the difficulty of terms and the difficulty of their corresponding definitions. The X-axis represents the values of these differences. Positive values indicate helpful definitions, where the term difficulty is higher than the difficulty of the corresponding definition. A value of 0 represents an unhelpful definition, as it shares the same difficulty as the term it aims to explain. Negative values on the X-axis indicate increased difficulty, meaning that the definition is more challenging than the corresponding term.</p><p>The results depicted in Figure <ref type="figure" coords="17,242.56,453.96,5.17,10.91" target="#fig_3">2</ref> reveal that approximately 30%-40% of definitions can be classified as either unhelpful or even more difficult than the terms they are meant to clarify. This suggests that a significant portion of the definitions may not effectively assist readers in understanding the associated terms. The ground truth definitions, in contrast to the participants' runs, do not exhibit such harmful patterns, implying that they maintain a higher level of coherence and clarity.</p><p>An interesting observation made during the evaluation is that for highly complex terms, the corresponding definitions often turned out to be equally difficult to understand. Explaining a complicated term may require introducing additional complex terms, leading to longer and more intricate definitions. This complexity stems from the necessity of explaining not just one term, but multiple related terms in order to provide a comprehensive understanding. Additionally, it was noticed that certain terms like "spins" or "chips" had generated definitions that were incorrect in the context of the given text. Although these terms are commonly used in everyday language, they hold distinct meanings in specific domains such as quantum physics or computer science. Assigning the correct definitions to these terms within their relevant scientific or technical contexts is of paramount importance to ensure accurate comprehension and avoid potential misunderstandings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and future work</head><p>For Task 2 focused on difficult concept identification and explanation, we created a corpus of sentences extracted from the abstracts of scientific publications, with manual annotations of scientific term difficulty and their definitions. 12 distinct teams participated in the task and submitted 39 runs demonstrating a diverse range of approaches in addressing the task's objectives and challenges varying from traditional statistic methods to LLMs.</p><p>Several noteworthy observations emerge from the study. Firstly, it is evident that even when employing similar models, the results achieved by the same methods can vary significantly. This variance can be attributed to factors such as the specific implementation approach, the finetuning techniques employed, and the choice of prompts utilized during the simplification process. These implementation-related aspects play a crucial role in determining the effectiveness and outcomes of the methods applied. Hence, careful consideration and optimization of these factors are essential to achieving desirable results in text simplification tasks.</p><p>Another significant observation is that efficiency plays a vital role alongside effectiveness in text simplification tasks, including difficulty spotting and explanation. Many partial runs were received due to the limitations of tokens or time constraints imposed by LLMs. The results of difficult term detection achieved by LLMs were found to be comparable to those obtained through unsupervised methods both for difficult term spotting and providing definitions (e.g. definitions retrieved from Wikipedia). This observation highlights the trade-off between efficiency and effectiveness in text simplification, where the choice of approach must consider both aspects to strike the right balance.</p><p>The third observation highlights the ongoing challenge of achieving robustness in approaches. Specifically, it is noted that a significant percentage, ranging from 30% to 40%, of the generated definitions are either unhelpful or even more difficult than the corresponding terms themselves. The complexity of highly complex terms often translates into equally intricate definitions, as it may be necessary to introduce additional complex terms to provide a comprehensive explanation. This can result in longer and more convoluted definitions, which may still pose difficulties for readers. This indicates that there is room for improvement in ensuring that the generated definitions effectively simplify and enhance the understandability of the terms for the target audience. Overcoming this challenge requires developing more robust and accurate approaches that can consistently provide helpful and simplified explanations for difficult terms.</p><p>Furthermore, the issue of incorrect definitions for terms like "spins" or "chips" highlights the importance of context and domain-specific knowledge. Although these terms have everyday usage, their meanings can differ significantly in scientific or technical domains such as quantum physics or computer science. A significant portion of the provided definitions, specifically nearly half of them, were deemed incorrect. Providing accurate definitions that align with the specific context is crucial for enabling accurate comprehension and avoiding potential misunderstandings.</p><p>These observations underscore the need for precise and contextually appropriate definitions that strike a balance between simplicity and accuracy. Addressing these challenges will require developing advanced techniques that can accurately capture the nuances of complex terms while providing simplified and accurate explanations tailored to the target audience and context.</p><p>So the general upshot of the CLEF 2023 SimpleText track is both that we observed great progress, but at the same time that there is also still a lot of room for improvement.</p><p>The simplification techniques are successful in simplifying the structure and syntax of the text, making it more accessible and easier to comprehend. However, despite the improvements in syntax, the challenge of lexical difficulty remains a significant barrier to understanding scientific texts.</p><p>In future work, there are plans to focus on classifying difficult term explanations, including definitions, examples, and abbreviation deciphering. We consider conducting the systems' evaluation based on the usefulness and complexity of the explanations they provide for scientific terms.</p><p>Further details about the lab can be found at the SimpleText website: http://simpletext-project. com. Please join us and help to make scientific results understandable!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="16,89.29,322.30,416.95,8.93;16,89.29,334.31,34.87,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram of the difficulties of the definitions on a scale of 1-3 (1 -easy; 2 -difficult; 3 -very difficult)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="17,89.29,229.85,418.35,8.93;17,89.29,241.86,416.70,8.87;17,89.29,253.81,206.32,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference between term difficulty and definition difficulty on a scale of 1-3 (1 -easy; 2difficult; 3 -very difficult). Positive values on X axis show helpful definitions. 0 refers to unhelpful definitions. Negative values increase the difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.93,90.49,417.47,437.90"><head>Table 1</head><label>1</label><figDesc>Examples of the term difficulty scale used for evaluation: grades 0-2. Difficult terms are highlighted with the green color</figDesc><table coords="4,90.54,134.06,415.85,394.33"><row><cell>Grade</cell><cell></cell><cell cols="3">Non-abbreviated (ordinary) term</cell><cell></cell><cell>Abbreviation</cell></row><row><cell>2</cell><cell cols="5">"We have proven that transfer learning is not</cell><cell>"The steering commands from the</cell></row><row><cell></cell><cell cols="5">only applicable in this field, but it requires smaller</cell><cell>source and target network are finally</cell></row><row><cell></cell><cell cols="5">well-prepared training datasets, trains significantly</cell><cell>merged according to the LDL and the</cell></row><row><cell></cell><cell cols="5">faster and reaches similar accuracy compared to</cell><cell>merged command is utilized for con-</cell></row><row><cell></cell><cell cols="5">the original method, even improving it on some</cell><cell>trolling a car in the target domain. "</cell></row><row><cell></cell><cell>aspects. "</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"Various machine learning techniques</cell></row><row><cell></cell><cell>"The</cell><cell>entropy</cell><cell>is</cell><cell>derived</cell><cell>using</cell><cell>like Random Forest, SVM as well as</cell></row><row><cell></cell><cell cols="5">singular value decomposition of the compo-</cell><cell>deep learning models has been pro-</cell></row><row><cell></cell><cell cols="5">nents of stock market indices in financial markets from selected developed economies, i.e., France, Germany, the United Kingdom, and the United</cell><cell>posed for classifying traffic signs. " "We compared XCSFHP to XCSF on several problems. "</cell></row><row><cell></cell><cell>States.. "</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">" Blockchain as a new technology has created</cell></row><row><cell></cell><cell cols="5">a great amount of hype and hope for different</cell></row><row><cell></cell><cell cols="2">applications. "</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="5">"In this paper, we present the development of a</cell><cell>NIST (The National Institute of Stan-</cell></row><row><cell></cell><cell cols="5">remote server that provides a user-friendly access</cell><cell>dards and Technology) in "Recently</cell></row><row><cell></cell><cell cols="5">to advanced electrocardiographic (ECG) signal pro-</cell><cell>NIST has published the second draft</cell></row><row><cell></cell><cell cols="2">cessing techniques. "</cell><cell></cell><cell></cell><cell></cell><cell>document of recommendation for the</cell></row><row><cell></cell><cell cols="5">"An attacker can obtain the password, private-key ,</cell><cell>entropy sources used for random bit</cell></row><row><cell></cell><cell cols="3">and public-key of the user. "</cell><cell></cell><cell></cell><cell>generation. "</cell></row><row><cell></cell><cell cols="5">" Cloud computing provides an effective business model for the deployment of IT infrastructure, plat-form, and software services. "</cell><cell>"Applications to increase the function-ality of PDA are constantly being de-veloped, and occasionally application software must be installed. "</cell></row><row><cell>0</cell><cell cols="5">The World Wide Web is a potentially powerful</cell><cell>2D</cell><cell>(2-dimensional),</cell><cell>3D</cell><cell>(3-</cell></row><row><cell></cell><cell cols="3">channel for misinformation. "</cell><cell></cell><cell></cell><cell>dimensional) maps as in "The</cell></row><row><cell></cell><cell cols="5">"On the other hand, a 3dimensional (3D) map, which</cell><cell>3D maps will give more intuitive</cell></row><row><cell></cell><cell cols="5">is one of major themes in machine vision research,</cell><cell>information compared to conventional</cell></row><row><cell></cell><cell cols="5">has been utilized as a simulation tool in city and</cell><cell>2-dimensional ( 2D ) ones. "</cell></row><row><cell></cell><cell cols="5">landscape planning , and other engineering fields. "</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,416.99,128.68"><head>Table 2</head><label>2</label><figDesc>SimpleText Task 2: Examples of the annotation</figDesc><table coords="7,89.29,122.02,40.02,8.92"><row><cell>Sentence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.99,90.49,411.12,143.33"><head>Table 3</head><label>3</label><figDesc>Examples of difficult terms and their definitions. Difficult terms are highlighted with the green color</figDesc><table coords="9,105.13,122.10,386.68,111.71"><row><cell>Grade</cell><cell></cell><cell></cell><cell>Sentence</cell><cell></cell><cell></cell><cell>Definition</cell></row><row><cell>1</cell><cell cols="5">"In the modern era of automation and</cell><cell>" Autonomous vehicles (AVs) use technol-</cell></row><row><cell></cell><cell cols="5">robotics, autonomous vehicles are cur-</cell><cell>ogy to partially or entirely replace the hu-</cell></row><row><cell></cell><cell cols="5">rently the focus of academic and indus-</cell><cell>man driver in navigating a vehicle from an</cell></row><row><cell></cell><cell cols="2">trial research. "</cell><cell></cell><cell></cell><cell></cell><cell>origin to a destination while avoiding road</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hazards and responding to traffic condi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tions. "</cell></row><row><cell>1</cell><cell>"They</cell><cell>can</cell><cell>be</cell><cell>used</cell><cell>for</cell></row><row><cell></cell><cell cols="3">information retrieval</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,89.29,576.54,416.69,93.87"><head></head><label></label><figDesc>submitted a total of 6 runs for Task 2, consisting 3 runs for Task 2.1 and 3 runs for Task 2.2. They experimented with keyword extraction approaches (YAKE!, KBIR) and IDF weighting for the first task, and definition detection in top-ranked documents based on a trained classifier.University of Western Brittany (UBO)<ref type="bibr" coords="9,284.28,645.95,17.97,10.91" target="#b20">[21]</ref> submitted a total of 8 runs for Task 2, no less than 7 runs for Task 2.1 and a single run for Task 2.2. They experimented with a range of keyword extraction approaches (FirstPhrase, TF-IDF, YAKE!, TextRank, SingleRank, TopicRank, PositionRank) for the first task and a Wikipedia extraction approach for the second task.University of Split (Croland) submitted a total of 4 runs for Task 2, specifically 2 runs for Task 2.1 and 2 runs for Task 2.2. They applied GPT-3 and TF-IDF for difficult term detection. They extracted definitions from Wikipedia and applied GPT-3 to generate explanations.</figDesc><table coords="10,88.96,185.13,417.23,24.46"><row><cell>University of Liverpool (UOL-SRIS) submitted a single run for Task 2, specifically for</cell></row><row><cell>Task 2.1 by applying KeyBERT.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.99,90.49,416.99,481.61"><head>Table 4</head><label>4</label><figDesc>SimpleText Task 2.1: Results for the official runs</figDesc><table coords="11,299.43,122.02,206.56,25.76"><row><cell>Total</cell><cell>Evaluated</cell><cell>Score</cell></row><row><cell></cell><cell>+Limits</cell><cell>+Limits</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,90.49,417.00,493.56"><head>Table 5</head><label>5</label><figDesc>SimpleText Task 2.1: Results for the official runs on train and test sets. The evaluation is done based on the terms with thier limits correctly detected.</figDesc><table coords="12,301.30,133.98,204.69,26.13"><row><cell>Total</cell><cell>Test</cell><cell>Train</cell></row><row><cell></cell><cell cols="2">Evaluated Score Evaluated Score</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,88.99,90.49,419.18,442.79"><head>Table 6</head><label>6</label><figDesc>SimpleText Task 2.2: Results for the official runs on the test set</figDesc><table coords="13,89.29,158.37,19.23,8.92"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,102.92,522.57,251.94,10.71"><head>. Reslts of Task 2.2: difficult term explanation.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="14,88.99,90.49,419.18,331.14"><head>Table 7</head><label>7</label><figDesc>SimpleText Task 2.2: Results for the official runs on the train set. Runs with less than 5 evaluated sentences are excluded from this table.</figDesc><table coords="14,89.29,168.10,19.23,8.92"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="15,88.99,90.49,419.18,381.18"><head>Table 8</head><label>8</label><figDesc>SimpleText Task 2.2: Results for the official runs on the abbreviation expansion task</figDesc><table coords="15,89.29,158.37,19.23,8.92"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="16,88.99,90.49,411.02,69.87"><head>Table 9</head><label>9</label><figDesc>Statistics on the levels of the difficulty of simplified sentences on the scale of 1-7</figDesc><table coords="16,89.29,122.02,410.72,38.33"><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>syntax complexity</cell><cell>259</cell><cell>51</cell><cell>9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>lexical complexity</cell><cell>93</cell><cell>119</cell><cell>62</cell><cell>26</cell><cell>19</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,89.29,449.27,418.37,10.91;1,89.29,462.82,416.69,10.91;1,89.29,476.37,416.69,10.91;1,89.29,489.92,416.98,10.91;1,89.29,503.47,418.37,10.91;1,89.29,517.02,418.54,10.91;1,89.29,530.57,416.69,10.91;1,89.29,544.12,416.69,10.91;1,89.29,557.66,418.37,10.91;1,88.96,571.21,417.23,10.91;1,89.29,584.76,416.97,10.91"><p>Scientific literature has become more accessible to the general public through digitalization. However, there still exists a significant barrier preventing individuals from accessing objective scientific knowledge directly from the original sources. One of the main challenges stems from the high complexity of scientific texts, which poses difficulties for non-experts due to the lack of necessary background knowledge, including the comprehension of specialized terminology. Even for native speakers, understanding terminology outside their area of expertise can be challenging. However, individuals with a basic set of terms acquired through secondary and college education can comprehend popular science publications to a certain extent. Comprehension of the term implies grasping the concept it represents without the need for an explicit definition. To understand a concept, it often requires incorporating it into a structured system within our semantic memory, which may necessitate additional knowledge beyond what we have already</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="2,92.57,670.99,111.16,8.97"><p>https://simpletext-project.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="8,92.57,671.01,237.75,8.97"><p>https://huggingface.co/sentence-transformers/all-mpnet-base-v2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="16,92.57,671.02,280.52,8.97"><p>Note that new instances were treated compared to the results reported in<ref type="bibr" coords="16,362.54,671.02,10.55,8.97" target="#b7">[8]</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was funded, in whole or in part, by the <rs type="funder">French National Research Agency (ANR)</rs> under the project <rs type="grantNumber">ANR-22-CE23-0019-01</rs>. We would like to thank <rs type="person">Jaap Kamps</rs>, <rs type="person">Radia Hannachi</rs>, <rs type="person">Silvia Araújo</rs>, <rs type="person">Pierre De Loor</rs>, <rs type="person">Olga Popova</rs>, <rs type="person">Diana Nurbakova</rs>, <rs type="person">Quentin Dubreuil</rs>, and all the other colleagues and participants who helped run this track.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_em3guTg">
					<idno type="grant-number">ANR-22-CE23-0019-01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="19,111.11,402.49,395.07,9.96;19,111.11,414.45,396.06,9.96;19,111.11,426.40,186.12,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,197.54,402.49,308.64,9.96;19,111.11,414.45,84.49,9.96">A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1410" />
	</analytic>
	<monogr>
		<title level="m" coord="19,216.18,414.45,82.83,9.96">Proc. of EMNLP 2018</title>
		<meeting>of EMNLP 2018<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3749" to="3760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,438.36,394.87,9.96;19,111.11,450.31,396.41,9.96;19,111.11,462.27,145.87,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,243.18,438.36,262.80,9.96;19,111.11,450.31,144.92,9.96">How Much Knowledge Is Too Little? When a Lack of Knowledge Becomes a Barrier to Comprehension</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sabatini</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797619862276</idno>
		<ptr target="https://journals.sagepub.com/doi/10.1177/0956797619862276" />
	</analytic>
	<monogr>
		<title level="j" coord="19,268.75,450.31,87.07,9.96">Psychological Science</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,474.22,394.88,9.96;19,110.81,486.18,179.37,9.96" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2010.11004" />
		<title level="m" coord="19,269.89,474.22,236.10,9.96">Controllable Text Simplification with Explicit Paraphrasing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,498.13,395.97,9.96;19,111.11,510.09,394.88,9.96;19,111.11,522.04,394.88,9.96;19,111.11,534.00,394.88,9.96;19,111.11,545.95,357.08,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,111.11,510.09,394.88,9.96;19,111.11,522.04,23.55,9.96">Overview of simpletext 2021 -CLEF workshop on text simplification for scientific information access</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-85251-1_27" />
	</analytic>
	<monogr>
		<title level="m" coord="19,155.49,522.04,350.49,9.96;19,111.11,534.00,207.25,9.96">CLEF&apos;21: Experimental IR Meets Multilinguality, Multimodality, and Interaction -12th International Conference of the CLEF Association</title>
		<title level="s" coord="19,397.48,534.92,108.51,8.88;19,111.11,546.88,26.57,8.88">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12880</biblScope>
			<biblScope unit="page" from="432" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,557.91,395.97,9.96;19,111.11,569.86,394.88,9.96;19,111.11,581.82,394.88,9.96;19,109.42,593.77,396.56,9.96;19,111.11,605.73,357.08,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,208.80,569.86,297.18,9.96;19,111.11,581.82,55.84,9.96">Overview of the CLEF 2022 simpletext lab: Automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-13643-6_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-13643-6_28" />
	</analytic>
	<monogr>
		<title level="m" coord="19,187.29,581.82,318.69,9.96;19,109.42,593.77,221.02,9.96">CLEF&apos;22: Experimental IR Meets Multilinguality, Multimodality, and Interaction -13th International Conference of the CLEF Association</title>
		<title level="s" coord="19,403.72,594.70,102.26,8.88;19,111.11,606.66,26.57,8.88">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13390</biblScope>
			<biblScope unit="page" from="470" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,617.68,394.88,9.96;19,111.11,629.64,201.22,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="19,286.41,617.68,219.58,9.96;19,111.11,629.64,137.51,9.96">Overview of the CLEF 2023 SimpleText Task 1: Passage selection for a simplified summary</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,111.11,641.59,396.06,9.96;19,111.11,653.55,174.41,9.96" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mccombie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<title level="m" coord="19,312.85,641.59,194.31,9.96;19,111.11,653.55,111.84,9.96">Overview of the CLEF 2023 SimpleText Task 3: Scientific text simplification</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,87.68,395.11,9.96;20,111.11,99.63,395.98,9.96;20,110.81,111.59,395.17,9.96;20,110.81,123.54,396.27,9.96;20,111.11,135.50,207.91,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="20,415.02,87.68,91.19,9.96;20,111.11,99.63,261.32,9.96">Overview of the CLEF 2023 SimpleText Lab: Automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,137.80,123.54,365.19,9.96">CLEF&apos;23: Proceedings of the Fourrteenth International Conference of the CLEF Association</title>
		<title level="s" coord="20,111.11,135.50,141.60,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,147.45,396.06,9.96;20,111.11,159.41,396.05,9.96;20,111.11,171.36,122.68,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,306.14,147.45,201.02,9.96;20,111.11,159.41,167.61,9.96">Overview of the CLEF 2022 SimpleText Task 1: Passage selection for a simplified summary</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/" />
	</analytic>
	<monogr>
		<title level="m" coord="20,349.49,160.33,109.12,8.88">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,183.32,395.97,9.96;20,111.11,195.27,394.88,9.96;20,111.11,207.23,395.98,9.96;20,111.11,219.18,394.88,9.96;20,111.11,231.14,395.98,9.96;20,111.11,243.09,20.72,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,206.57,195.27,299.42,9.96;20,111.11,207.23,56.64,9.96">Overview of the CLEF 2022 SimpleText Lab: Automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,349.53,219.18,156.46,9.96;20,111.11,231.14,202.78,9.96">CLEF&apos;22: Proceedings of the Thirteenth International Conference of the CLEF Association</title>
		<title level="s" coord="20,320.55,231.14,142.99,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,255.05,394.88,9.96;20,111.11,267.00,238.18,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,243.87,255.05,262.11,9.96;20,111.11,267.00,60.91,9.96">A simple algorithm for identifying abbreviation definitions in biomedical text</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,192.51,267.00,76.56,9.96">Biocomputing 2003</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,278.96,394.88,9.96;20,111.11,290.91,330.96,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,288.90,278.96,217.08,9.96;20,111.11,290.91,42.47,9.96">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,174.39,290.91,186.48,9.96">Proc. of the 40th annual meeting on ACL, ACL</title>
		<meeting>of the 40th annual meeting on ACL, ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,302.87,396.06,9.96;20,111.11,314.82,394.88,9.96;20,110.81,326.78,181.07,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,188.98,302.87,300.64,9.96">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,111.11,314.82,394.88,9.96;20,110.81,326.78,53.05,9.96">Proc. of the 2003 Conference of the North American Chapter of the ACL on Human Language Technology</title>
		<meeting>of the 2003 Conference of the North American Chapter of the ACL on Human Language Technology</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,338.74,395.18,9.96;20,111.11,350.69,215.18,9.96" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="20,278.89,338.74,227.39,9.96;20,111.11,350.69,152.70,9.96">NLPalma @ CLEF 2023 SimpleText: BLOOMZ and BERT for complexity and simplification task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P P</forename><surname>Victor Manuel Palma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,362.65,394.88,9.96;20,111.11,374.60,131.67,9.96" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="20,327.00,362.65,157.34,9.96">University of Amsterdam at the CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Adib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sutmullera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>SimpleText Track</note>
</biblStruct>

<biblStruct coords="20,111.11,386.56,396.06,9.96;20,110.76,398.51,395.23,9.96;20,110.81,410.47,112.92,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,196.26,386.56,310.90,9.96;20,110.76,398.51,361.65,9.96">CLEF 2023 SimpletText Tasks 2 and 3: Enhancing Language Comprehension: Addressing Difficult Concepts and Simplifying Scientific Texts Using GPT, BLOOM, KeyBert</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dadić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,478.88,398.51,27.10,9.96;20,110.81,410.47,49.48,9.96">Simple T5 and More</title>
		<imprint>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,422.42,394.88,9.96;20,111.11,434.38,366.55,9.96" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Ortiz-Zambrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Espin-Riofrio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montejo-Ráez</surname></persName>
		</author>
		<title level="m" coord="20,342.17,422.42,163.81,9.96;20,111.11,434.38,303.59,9.96">SINAI participation in SimpleText Task 2 at CLEF 2023: GPT-3 in Lexical Complexity Prediction for general audience</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,446.33,394.88,9.96;20,111.11,458.29,289.55,9.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="20,212.48,446.33,293.51,9.96;20,111.11,458.29,207.64,9.96">Automatic Simplification of Scientific Texts using Pre-trained Language Models: A Comparative Study at CLEF Symposium</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lieberum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,470.24,394.88,9.96;20,111.11,482.20,132.50,9.96" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="20,251.17,470.24,254.81,9.96;20,111.11,482.20,69.22,9.96">CLEF2023 SimpleText Task 2, 3: Identification and Simplification of Difficult Terms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Davari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prnjak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,494.15,395.12,9.96;20,111.11,506.11,126.82,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,346.86,494.15,159.37,9.96">AIIR and LIAAD Labs Systems for CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Durgin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,132.13,506.11,42.81,9.96">SimpleText</title>
		<imprint>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,518.06,394.88,9.96;20,111.11,530.02,158.02,9.96" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="20,166.69,518.06,339.29,9.96;20,111.11,530.02,95.34,9.96">UBO Team @ CLEF SimpleText 2023 Track for Task 2 and 3 -Using IA models to simplify Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dubreuil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,111.11,541.97,395.98,9.96;20,111.11,553.93,208.36,9.96" xml:id="b21">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="20,111.11,541.97,391.08,9.96">Proceedings of the Working Notes of CLEF 2023: Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="20,111.11,553.93,160.27,9.96">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting>the Working Notes of CLEF 2023: Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
