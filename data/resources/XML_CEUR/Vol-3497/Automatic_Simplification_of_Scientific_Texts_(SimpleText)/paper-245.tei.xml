<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,341.91,15.42;1,89.29,106.66,324.70,15.42">Elsevier at SimpleText: Passage Retrieval by Fine-tuning GPL on Scientific Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,72.79,11.96"><forename type="first">Artemis</forename><surname>Capari</surname></persName>
						</author>
						<author>
							<persName coords="1,172.74,134.97,94.37,11.96"><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
						</author>
						<author>
							<persName coords="1,278.78,134.97,102.32,11.96"><forename type="first">Georgios</forename><surname>Tsatsaronis</surname></persName>
						</author>
						<author>
							<persName coords="1,408.57,134.97,61.31,11.96"><forename type="first">Zubair</forename><surname>Afzal</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Elsevier</orgName>
								<address>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Genetics and Molecular Biology</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="department" key="dep3">Economics, Agricultural and Biological Sciences, Biochemistry</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Econometrics and Finance, Toxicology and Pharmaceutical Science</orgName>
								<orgName type="department" key="dep2">Chemical Engineering</orgName>
								<orgName type="department" key="dep3">Veterinary Science and Veterinary Medicine, Chemistry</orgName>
								<orgName type="department" key="dep4">Materials Science</orgName>
								<orgName type="department" key="dep5">Earth and Planetary Sciences, Engineering</orgName>
								<orgName type="department" key="dep6">Food Science</orgName>
								<orgName type="department" key="dep7">Im-munology and Microbiology, Mathematics</orgName>
								<orgName type="department" key="dep8">Nursing and Health Professions, Medicine and Dentistry, Neuroscience, Pharmacology, Psychology, Physics and Astronomy</orgName>
								<orgName type="institution">Social Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,341.91,15.42;1,89.29,106.66,324.70,15.42">Elsevier at SimpleText: Passage Retrieval by Fine-tuning GPL on Scientific Documents</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3540B0934C06F668ED2AB84B6206AC49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Scientific Documents</term>
					<term>Domain Adaptation</term>
					<term>Scholarly Document Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CLEF SimpleText Lab is centered around finding relevant passages from a large collection of scientific documents in response to a lay query, detecting and explaining difficult terminology within those passages, and finally simplifying the passages. The first task is similar to the ad-hoc retrieval task in which given a topic/query, the goal is to retrieve relevant passages, but in addition to the relevance, ranking models should assess documents based on their readability/complexity as well. This paper describes our approach towards building a ranking model to tackle the first task. To build the ranking model, we first evaluate performance of several models on a proprietary test collection constructed based on scientific documents across multiple science domains. Then, we fine-tune the best performing model on a large collection of unlabelled documents using the Generative Pseudo Labeling approach.</p><p>The key contribution and findings of our approach is that a bi-encoder model, trained on the MS-Marco dataset, fine-tuned further on a large collection of unlabelled scientific passages achieves the highest performance on the proprietary dataset which is specifically designed for the scientific passage retrieval task. Finally, fine-tuning a model in the same fashion, but only using the Computer Science queries from the test collection has proven to be successful for SimpleText Task 1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scientists and researchers employ specialized language and ideas to effectively communicate information. Consequently, there exists a substantial, increasing volume of scientific concepts and information within any given scientific field, which contributes to the challenges scientists face in keeping pace with the expanding scope of technical concepts and novel content. Understanding scientific documents is even more challenging for the public audience. It has been shown that the readability of scientific documents is decreasing over time <ref type="bibr" coords="1,441.12,513.08,11.36,10.91" target="#b0">[1]</ref>. This poses challenges and opportunities towards both researchers and publishers to think about way to increase the readability of complex scientific documents for public audience.</p><p>SimpleText Lab <ref type="bibr" coords="1,172.53,553.73,12.99,10.91" target="#b1">[2]</ref> is specifically focused around addressing these challenges. The aim of this lab is to first find relevant passages to users' queries, spot and explain difficult terminology within relevant passages, and finally simplify the passage by re-writing it in a more readable way. The very first task in the series of tasks associated with this lab, is a passage retrieval task namely "What is in (out)", where the goal is, given a query/topic, to retrieve all passages relevant to the query/topic that can be used to create a simplified summary around the topic.</p><p>In addition to the relevance, ranking models should also consider the complexity of passages when ranking them and prioritize less complex passages.</p><p>The state-of-the-art ranking models are semantic matching models using either a crossencoder or bi-encoder (or a combination of) architectures <ref type="bibr" coords="2,356.01,127.61,11.58,10.91" target="#b2">[3]</ref>. These models are trained on publicly available datasets such as MS-Marco <ref type="bibr" coords="2,296.83,141.16,13.00,10.91" target="#b3">[4]</ref> which do not contain scientific documents. The retrieval task of the SimpleText lab itself and the underlying training/evaluation sets are centred around scientific documents. Therefore, existing ranking models might not perform very well in this setting as the language of scientific documents is usually more complex and there might be specific scientific terminology within scientific documents that is specific for such documents.</p><p>In this paper we build our model on top of the existing state-of-the-art ranking models. To address the domain difference challenge, we use a domain adaptation technique, namely Generative Pseudo Labeling (GPL) to fine-tune the pre-trained models on a set of unlabelled scientific documents. To evaluate ranking models and fine-tune them, we build a proprietary test collection containing 5000 query document-pairs annotated by relevance labels. Our results on this dataset, shows that a bi-encoder model fine-tuned on a large collection of scientific unlabelled documents achieves a stronger performance than the zero-shot counterpart. We use this model to re-rank documents ranked by the Elastic Search system. Our results show that some of the fine-tuned models achieve a better performance than the zero-shot models on the SimpleText dataset as well. In the remainder of the paper, we briefly review related work in Section 2, we describe the technical details of the designed system in Section 3, we empirically evaluate the models in Sections 4 and 5 and we conclude in Section 6 by arraying some limitations of the current technical solution and provide pointers to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dense retrieval models are a type of information retrieval (IR) model that use fixed-length dense vector representations to represent both queries and documents, allowing for efficient and accurate retrieval of relevant information from a large corpus of text by computing the similarity score between query-and document vectors. These models have been shown to outperform traditional sparse retrieval models, such as BM25 <ref type="bibr" coords="2,357.13,497.87,11.28,10.91" target="#b4">[5]</ref>, in a variety of tasks, including open-domain question answering and document ranking. Two popular types of such dense retrieval models are bi-encoders and cross-encoders. Both models still have the same objective, i.e. capturing the semantic meaning of queries and documents into dense vector representations, but differ in the architecture of the neural network used to learn their representations.</p><p>Bi-encoders use two separate encoders to independently encode the query and the document into dense vectors, which are then compared using a similarity function to produce a relevance score. One of the most popular bi-encoders is the Dense Passage Retrieval (DPR) model <ref type="bibr" coords="2,469.29,606.27,11.30,10.91" target="#b5">[6]</ref>. DPR uses a two-stage retrieval process, in which a large set of passages is first retrieved using sparse techniques, which is used in turn to compute a dense vector representation of each passage using a pre-trained language model such as BERT <ref type="bibr" coords="2,310.68,646.91,11.34,10.91" target="#b6">[7]</ref>. The query is represented using a similar dense vector representation as well. The passages are then ranked based on the cosine similarity between the query and passage vectors.</p><p>Cross-Encoders however, use a single encoder to encode the query and document into a joint embedding space. Documents are then ranked based on the similarity score that is computed between this joint embedding and the learned representation of the positive document. They can capture more complex interactions between query and document. However, they are computationally more expensive as it requires a unique embedding for each query-document pair, while bi-encoders encode queries and documents separately and therefore it only requires a single document corpus for all queries <ref type="bibr" coords="3,269.90,181.81,11.42,10.91" target="#b7">[8]</ref>. Therefore, they are often only used as re-rankers <ref type="bibr" coords="3,89.29,195.36,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="3,103.37,195.36,12.55,10.91" target="#b9">10,</ref><ref type="bibr" coords="3,118.65,195.36,12.55,10.91" target="#b10">11,</ref><ref type="bibr" coords="3,133.92,195.36,12.55,10.91" target="#b11">12,</ref><ref type="bibr" coords="3,149.19,195.36,12.55,10.91" target="#b12">13,</ref><ref type="bibr" coords="3,164.47,195.36,12.32,10.91" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>To train and fine-tune our models, we first build a test collection using a set of scientific documents. Then, we fine-tune existing ranking models using this dataset as well as a large collection of scientific documents to make these model more suitable for retrieving scientific passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Test Collection</head><p>To build a test collection, we select 100 queries spread across 20 different scientific domains 1 . We select the queries to be a known scientific concept on which we can collect credible and relevant documents/passages. Once the queries are selected, we then use the well-known pooling mechanism to retrieve candidate documents to be annotated per query. We select five different models (two lexical matching, two bi-encoders, and one cross-encoder) as the models to be used to build the pool. These models are selected based on their performance on a small set or to ensure the diversity of models (and hence diversity of document within the pool). We select 50 documents per query using the pooling approach. These documents are then labeled by experts per domains as "relevant", "partially relevant", or "non-relevant". We use this dataset to evaluate the performance of different ranking models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GPL</head><p>Generative Pseudo Labeling (GPL) is an unsupervised domain adaptation method first introduced in <ref type="bibr" coords="3,100.91,529.58,16.28,10.91" target="#b14">[15]</ref>. The proposed framework leverages the structure of a pre-trained generative model to generate pseudo labels for the target domain data, which are then used to train a retrieval model in a supervised manner. GPL outperforms existing unsupervised domain adaptation methods on several benchmark datasets and achieves state-of-the-art performance in unsupervised domain adaptation of dense retrieval. Considering we intend to use-and experiment with dense-retrieval models, and the importance of large amounts of data has often been highlighted in previous work on dense retrieval methods <ref type="bibr" coords="4,241.91,352.07,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,256.33,352.07,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="4,266.78,352.07,7.65,10.91" target="#b2">3]</ref>, our manually annotated dataset might not suffice as it only consists of 5000 snippets from a set of a 100 queries. However, there are many more snippets and possible queries that can be extracted from a large collection of unlabeled scientific documents (research articles), which could be labeled by GPL on their relevance in order to fine-tune and adapt the existing ranking models to the scientific document retrieval task.</p><p>We adapt GPL to our use-case, by first removing the query generation part. Instead, we select a set of known scientific concepts per domain, and then per concept, we find all passages mentioning the concept.</p><p>Finding an exact mention of a scientific concept in a document can be a very good indicator of relevance of the document to the concept. Then, per concept, each document mentioning it is regarded as positive, and a bi-encoder is used to find negative document per query.</p><p>The GPL framework uses a cross-encoder as a teacher model on the collected positive and negative documents to fine-tune the underlying bi-encoder model, which is used to adapt the bi-encoder model to our scientific document ranking setting. For our use-case, we have fine-tuned two different bi-encoders msmarco-distilbert-base-v4 <ref type="bibr" coords="4,372.24,542.77,13.60,9.72" target="#b7">[8]</ref> (MS-DB-v4) and msmarcodistilbert-base-tas-b <ref type="bibr" coords="4,177.83,556.32,16.86,9.72" target="#b15">[16]</ref> (MS-DB-tas-b) using our whole test collection, spanning 20 different scientific domains, consisting of 5 queries each. We found that msmarco-distilbert-base-tas-b was most suitable for tasks that require understanding of a wide range of domains.</p><p>However, as the SimpleText task aims at finding references in Computer Science, we have also fine-tuned the aforementioned models on queries and articles from just the Computer Science and Mathematics domains. Naturally, these models were fine-tuned on far less data (See Table <ref type="table" coords="4,136.89,636.60,3.57,10.91">1</ref>).</p><p>Each of the models were fitted on pseudo labels created with ms-marco-MiniLM-L-6-v2, using the Adam Optimiser <ref type="bibr" coords="4,182.73,663.70,17.91,10.91" target="#b16">[17]</ref> with a learning rate of 2e-5 and 1000 warm-up steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have applied our models in several settings before selecting the final 10 submitted runs. Different variations of the best performing models (on the proprietary test collection) were selected to make the final submissions. As shown in Table <ref type="table" coords="5,379.15,138.38,3.81,10.91" target="#tab_0">2</ref>, the rankings for runs 1-7 were retrieved by taking the top-k documents found for each of the 29 queries from Simpletext_2023_task1_train.qrels by the Elastic Search API. These were then reranked using our fine-tuned models. The rankings for the first 4 runs were obtained with the model that was only fine-tuned on Computer Science and Mathematics data, while we used the model fine-tuned on all Science Direct Domains for runs 5-7. For run 8, the top-500 documents were retrieved by searching for "query, topic", and then re-ranked using our CS fine-tuned model, again using "query, topic" as the query input. For run 9, we used the model that performed best on our own test collection to search the entire corpus for each query, rather than pre-filtering with Elastic Search. Finally, we used our best CS-trained model once again, but searched per topic instead of per query.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We have selected our runs based on our own evaluation, which uses the qrels provided to us. However, to our knowledge, these qrels are biased towards passages retrieved by ElasticSearch, which is a lexical search method. Naturally, the recall for our semantic search models may therefore be limited. As the test qrels that have been used for the official evaluation are based on pooling the submissions of 2023 participants <ref type="bibr" coords="5,304.05,586.42,11.42,10.91" target="#b1">[2]</ref>, these qrels include passages from various types of neural rankers as well as lexical matching models. Hence the results from our own evaluation differ from the official results. Nonetheless, they are included as they still provide insight on our training process and our decisions behind selecting certain runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Selecting Best Runs</head><p>In this section, we describe the results of fine-tuning different ranking models on a large collection of unlabeled documents using the GPL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Evaluated per Query: Performance of zero-shot models vs fine-tuned models on</p><p>Simpletext_2023_task1_train.qrels Model P@10 R@10 RR@10 nDCG@5 nDCG@10 nDCG@50 nDCG@100 0-shot While ms-marco-distilbert-base-tas-b proved most suitable for fine-tuning on our use-case, Table <ref type="table" coords="6,117.47,276.85,5.17,10.91">3</ref> shows that it underperforms its zero-shot equivalent on the train set. A possible explanation could be the pooling bias or the shallow depth of the training set. To be able to explain this result and make solid conclusions based on these results, we need to evaluate the performance of these models on an unseen test set. On the other hand, the fine-tuned ms-marcodistilbert-base-v4 model outperforms the zero-shot version which shows the effectiveness of fine-tuning on the performance of this model.    The converged model has a significantly higher performance than the zero-shot version in terms of most evaluation metrics. While the distilbert-base-v4 gets improved by more training steps, the same behavior is not observed for the distilbert-base-tas-b model. In fact, this model's performance steadily drops by more training steps. A more detailed analysis on a larger test collection (with more queries and deeper depth) is required to explain this behavior of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Submitted Runs</head><p>Table <ref type="table" coords="8,114.80,245.09,4.97,10.91" target="#tab_2">4</ref> shows the performance of the submitted runs on the training set using queries. As can be seen, different variations of the MS-DB-v4 model fine-tuned by GPL an the CS data using queries has the best performance in terms of most metrics. The main variation in the performance of different versions of this model comes from the number of top ranked documents, retrieved by the Elastic Search system, used for re-ranking. Increasing the number of ES documents from 100 to 500 has a negative impact on ùëõùê∑ùê∂ùê∫@50 and ùëõùê∑ùê∂ùê∫@100. This result again shows a possible pooling bias towards the ES model in the training set. Run P@10 R@10 MRR@10 nDCG@5 nDCG@10 nDCG@50 nDCG@100 Table <ref type="table" coords="8,126.51,542.52,4.97,10.91">5</ref> shows the performance of the submitted runs on the training set using topics. Performance of the models based on topics is similar to their query-based performance. However, the model used to re-rank top 5000 documents of the ES system achieves the higher performance in topic-based evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Evaluated per Topic: Performance of official runs on Simpletext_2023_task1_train.qrels Run P@10 R@10 RR@10 nDCG@5 nDCG@10 nDCG@50 nDCG@100 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Official Results</head><p>As per Table <ref type="table" coords="9,148.40,314.13,3.80,10.91" target="#tab_5">6</ref>, where the results are sorted on the primary measure, nDCG@10, we see that our submitted runs (e.g. Elsevier) dominate the top of the scoreboard.</p><p>In particular, the highest performing result, run 8, was obtained by re-ranking top-500 passages retrieved by ElasticSearch when searching for "query, topic" with MS-DB-v4-GPL-CS, again searching with query, topic. The selection of configurations(see Table <ref type="table" coords="9,461.88,368.33,4.25,10.91" target="#tab_0">2</ref>) for our submissions were based on our own evaluation on the set of qrels provided to us, which indicated that searching only for the query with MS-DB-v4-GPL-CS outperformed our best model for the KAPR task: MS-DB-tas-b-GPL-all. However, this set might not have been representative of SimpleText's official evaluation set as most of the other high-ranking results, were obtained with MS-DB-tas-b-GPL-all. For instance, run 7 can directly be compared with run 3 as they use the same type of query input and the same type of corpus (i.e. top-1000 ElasticSearch results). This also applies for run 5 versus run 2 and run 6 versus run 1. In each of these settings, the tas-b model fine-tuned on our entire benchmark set outperformed the v4 model fine-tuned on only the Computer Science portion of our test collection.</p><p>This indicates that even for the SimpleText task, MS-DB-tas-b-GPL-all performs better than MS-DB-v4-GPL-CS, and that the success of run 8 could thus be partly attributed to the fact that it was the only run that used "query, topic" as its query input. Using MS-DB-tas-b-GPL-all with "query, topic" might thus have outperformed our winning run. Nonetheless, these results show that the model fine-tuned for our specific scientific passage retrieval task still generalizes well to other datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we designed several ranking models to address the document retrieval task of the SimpleText lab. To this end, we first built a test collection containing 5000 query-document pairs annotated by relevance labels. The documents in this test collection are extracted from scientific documents which makes it suitable to evaluate performance of ranking models on the scientific document retrieval task. We, then evaluated the performance of existing ranking models on this test collection and selected a few models based on their performance to build our ranking models (used to create our SimpleText submissions). Since these models are trained on generic datasets created for the ad-hoc document retrieval task, they might not have a strong performance on the specific task of scientific document retrieval. To address this issue, we used a domain adaptation technique, namely Generative Pseudo Labeling (GPL) to fine-tune the selected ranking models to the scientific document retrieval task by means of a large collection of unlabeled scientific documents. Our results on the SimpleText training dataset shows the effectiveness of fine-tuning on the performance of our best ranking model. The distilbert-base-v4 model fine-tuned using GPL on a large collection of documents in Computer Science domain which is used to re-rank top-500 documents retrieved by a Elastic Search system using "topic, query" as the query input has the highest performance compared to the other fine-tuned models.</p><p>Using the relevance labels from Computer Science-related domains to fine-tune state-of-the-art ranking models proved successful. However, as only a small portion of our test collection consisted of Computer Science queries, future work could explore labeling a larger set of queries in Computer Science-related domains to fine-tune a model in the same fashion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,216.12,384.50,8.93;4,97.31,84.19,400.64,119.36"><head>Figure 1 : 1</head><label>11</label><figDesc>Figure 1: Generative Pseudo Labeling (GPL) for training domain-adapted dense retriever<ref type="bibr" coords="4,457.34,216.17,16.46,8.87" target="#b14">[15]</ref> </figDesc><graphic coords="4,97.31,84.19,400.64,119.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,179.43,351.93,4.63,8.87;5,215.52,351.93,23.79,8.87;5,264.72,351.93,45.05,8.87;5,333.41,351.93,79.21,8.87;5,179.43,363.88,4.63,8.87;5,215.52,363.88,23.79,8.87;5,264.72,363.88,45.05,8.87;5,333.41,363.88,79.21,8.87;5,179.43,375.84,4.63,8.87;5,215.52,375.84,23.79,8.87;5,264.72,375.84,49.68,8.87;5,333.41,375.84,79.21,8.87;5,179.43,387.79,4.63,8.87;5,215.52,387.79,23.79,8.87;5,264.72,387.79,49.68,8.87;5,333.41,387.79,79.21,8.87;5,179.43,399.75,4.63,8.87;5,215.52,399.75,23.79,8.87;5,264.72,399.75,45.05,8.87;5,333.41,399.75,88.47,8.87;5,179.43,411.70,4.63,8.87;5,215.52,411.70,23.79,8.87;5,264.72,411.70,45.05,8.87;5,333.41,411.70,88.47,8.87;5,179.43,423.66,4.63,8.87;5,215.52,423.66,23.79,8.87;5,264.72,423.66,49.68,8.87;5,333.41,423.66,88.47,8.87;5,179.43,435.61,4.63,8.87;5,203.24,435.61,106.53,8.87;5,333.41,435.61,79.21,8.87;5,179.43,447.57,4.63,8.87;5,215.52,447.57,23.79,8.87;5,264.72,447.57,157.16,8.87;5,177.12,459.52,9.27,8.87;5,217.18,459.52,20.44,8.87;5,264.72,459.52,45.05,8.87;5,333.41,459.52,79.21,8.87"><head></head><label></label><figDesc>-tas-b-GPL-all 8 query, topic ES Top-500 MS-DB-v4-GPL-CS 9 query Whole corpus MS-DB-tas-b-GPL-all 10 topic ES Top-500 MS-DB-v4-GPL-CS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,605.71,416.69,8.93;6,89.02,617.72,416.97,8.87;6,89.29,629.62,138.95,8.96;6,94.64,366.29,406.00,232.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of distilbert-base-v4 finetuned with GPL on CS data at various training steps on Top-100 Elastic Search Documents retrieved per query. Dashed lines indicate the performance of the zero-shot distilbert-base-v4 model.</figDesc><graphic coords="6,94.64,366.29,406.00,232.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,100.20,656.03,405.78,10.91;6,89.29,669.58,416.70,10.91"><head>Furthermore, Figures 4 , 2 ,</head><label>42</label><figDesc>and 3 show the performance of the GPL-based fine-tuned model at different training steps for different configurations. As can be seen, the distilbert-base-v4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,332.01,416.69,8.93;7,89.02,344.01,416.97,8.87;7,89.29,355.92,138.95,8.96;7,94.64,92.58,406.00,232.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of distilbert-base-v4 finetuned with GPL on CS data at various training steps on Top-500 Elastic Search Documents retrieved per query. Dashed lines indicate the performance of the zero-shot distilbert-base-v4 model.</figDesc><graphic coords="7,94.64,92.58,406.00,232.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,632.78,416.69,8.93;7,89.29,644.78,416.69,8.87;7,89.29,656.69,149.89,8.96;7,94.64,393.35,406.00,232.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of distilbert-base-tas-b finetuned with GPL on CS data at various training steps on Top-500 Elastic Search Documents retrieved per query. Dashed lines indicate the performance of the zero-shot distilbert-base-tas-b model.</figDesc><graphic coords="7,94.64,393.35,406.00,232.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,302.88,270.60,40.48"><head>Table 2</head><label>2</label><figDesc>Configurations of official submissions</figDesc><table coords="5,173.40,334.49,186.19,8.87"><row><cell>Run Query Input Corpus</cell><cell>Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,355.39,411.99,20.88"><head>Table 4 Evaluated per Query:</head><label>4</label><figDesc>Performance of Official Runs on Simpletext_2023_task1_train.qrels</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,88.99,90.49,406.58,325.96"><head>Table 6</head><label>6</label><figDesc>Official Results of Simple Text Task 1 -CLEF 2023</figDesc><table coords="10,99.70,132.23,395.87,284.21"><row><cell>Run</cell><cell cols="5">MRR P@10 P@20 P@30 nDCG@10 nDCG@20 nDCG@30 BPREF MAP</cell></row><row><cell>ElsevierSimpleText_run8</cell><cell>0.8082 0.5618 0.3515 0.2696</cell><cell>0.5881</cell><cell>0.4422</cell><cell>0.3803</cell><cell>0.2371 0.1633</cell></row><row><cell>ElsevierSimpleText_run7</cell><cell>0.7136 0.5618 0.4103 0.3441</cell><cell>0.5704</cell><cell>0.4627</cell><cell>0.4158</cell><cell>0.2626 0.1915</cell></row><row><cell>maine_CrossEncoder1</cell><cell>0.7309 0.5265 0.4500 0.4216</cell><cell>0.5455</cell><cell>0.4841</cell><cell>0.4687</cell><cell>0.3337 0.2754</cell></row><row><cell>maine_CrossEncoderFinetuned1</cell><cell>0.7338 0.4971 0.4000 0.3529</cell><cell>0.4859</cell><cell>0.4295</cell><cell>0.4062</cell><cell>0.3443 0.2385</cell></row><row><cell>ElsevierSimpleText_run5</cell><cell>0.6600 0.4765 0.3838 0.3314</cell><cell>0.4826</cell><cell>0.4186</cell><cell>0.3834</cell><cell>0.2542 0.1828</cell></row><row><cell>ElsevierSimpleText_run2</cell><cell>0.7010 0.4676 0.4059 0.3480</cell><cell>0.4791</cell><cell>0.4282</cell><cell>0.3912</cell><cell>0.2528 0.1942</cell></row><row><cell>ElsevierSimpleText_run6</cell><cell>0.6402 0.4676 0.3853 0.3284</cell><cell>0.4723</cell><cell>0.4185</cell><cell>0.3828</cell><cell>0.2557 0.1809</cell></row><row><cell>ElsevierSimpleText_run4</cell><cell>0.6774 0.4529 0.3794 0.3422</cell><cell>0.4721</cell><cell>0.4116</cell><cell>0.3876</cell><cell>0.2485 0.1898</cell></row><row><cell>ElsevierSimpleText_run9</cell><cell>0.5933 0.4735 0.3176 0.2500</cell><cell>0.4655</cell><cell>0.3595</cell><cell>0.3102</cell><cell>0.1758 0.1238</cell></row><row><cell>ElsevierSimpleText_run1</cell><cell>0.6821 0.4588 0.3824 0.3353</cell><cell>0.4626</cell><cell>0.4071</cell><cell>0.3786</cell><cell>0.2573 0.1823</cell></row><row><cell>maine_CrossEncoderFinetuned2</cell><cell>0.7082 0.4706 0.3926 0.3637</cell><cell>0.4617</cell><cell>0.4089</cell><cell>0.3969</cell><cell>0.3259 0.2253</cell></row><row><cell>UAms_CE1k_Filter</cell><cell>0.6403 0.4765 0.3559 0.2941</cell><cell>0.4533</cell><cell>0.3743</cell><cell>0.3334</cell><cell>0.2727 0.1936</cell></row><row><cell>ElsevierSimpleText_run3</cell><cell>0.6502 0.4471 0.3779 0.3324</cell><cell>0.4460</cell><cell>0.3994</cell><cell>0.3709</cell><cell>0.2558 0.1785</cell></row><row><cell>UAms_ElF_Cred44</cell><cell>0.6888 0.4324 0.3338 0.2951</cell><cell>0.4103</cell><cell>0.3499</cell><cell>0.3300</cell><cell>0.2395 0.1719</cell></row><row><cell>UAms_CE100</cell><cell>0.6779 0.3971 0.3456 0.3137</cell><cell>0.4016</cell><cell>0.3642</cell><cell>0.3483</cell><cell>0.2658 0.1792</cell></row><row><cell>maine_Pl2TFIDF</cell><cell>0.5626 0.4176 0.2809 0.2206</cell><cell>0.4014</cell><cell>0.3218</cell><cell>0.2887</cell><cell>0.2155 0.1364</cell></row><row><cell>UAms_Elastic</cell><cell>0.6424 0.4059 0.3456 0.2990</cell><cell>0.3910</cell><cell>0.3541</cell><cell>0.3314</cell><cell>0.2501 0.1895</cell></row><row><cell>UAms_ElF_Cred53</cell><cell>0.6429 0.4088 0.3382 0.3010</cell><cell>0.3883</cell><cell>0.3468</cell><cell>0.3292</cell><cell>0.2454 0.1833</cell></row><row><cell>UAms_ElF_Cred44Read</cell><cell>0.6625 0.3971 0.3147 0.2775</cell><cell>0.3723</cell><cell>0.3282</cell><cell>0.3101</cell><cell>0.2123 0.1403</cell></row><row><cell>UAms_CE1k</cell><cell>0.5880 0.4147 0.3515 0.3098</cell><cell>0.3706</cell><cell>0.3398</cell><cell>0.3250</cell><cell>0.2700 0.1865</cell></row><row><cell>UAms_CE1k_Combine</cell><cell>0.5880 0.4147 0.3515 0.3098</cell><cell>0.3706</cell><cell>0.3398</cell><cell>0.3250</cell><cell>0.2700 0.1865</cell></row><row><cell>UAms_ElF_Read25</cell><cell>0.6076 0.3735 0.3074 0.2833</cell><cell>0.3539</cell><cell>0.3190</cell><cell>0.3105</cell><cell>0.2194 0.1522</cell></row><row><cell>UAms_ElF_Cred53Read</cell><cell>0.6088 0.3676 0.3059 0.2784</cell><cell>0.3469</cell><cell>0.3153</cell><cell>0.3042</cell><cell>0.2133 0.1456</cell></row><row><cell>maine_tripletloss</cell><cell>0.5502 0.3382 0.2176 0.1608</cell><cell>0.3353</cell><cell>0.2561</cell><cell>0.2145</cell><cell>0.1335 0.0696</cell></row><row><cell>uninib_DoSSIER_2</cell><cell>0.5201 0.2853 0.2515 0.2118</cell><cell>0.2980</cell><cell>0.2683</cell><cell>0.2403</cell><cell>0.1898 0.1141</cell></row><row><cell>uninib_DoSSIER_4</cell><cell>0.5202 0.2853 0.2441 0.2108</cell><cell>0.2972</cell><cell>0.2632</cell><cell>0.2392</cell><cell>0.1873 0.1111</cell></row><row><cell>run-LIA.bm25</cell><cell>0.4536 0.1912 0.1338 0.1108</cell><cell>0.2192</cell><cell>0.1700</cell><cell>0.1505</cell><cell>0.1384 0.0515</cell></row><row><cell>run-LIA.all-MiniLM-L6-v2.query</cell><cell>0.3505 0.2000 0.1662 0.1353</cell><cell>0.2019</cell><cell>0.1767</cell><cell>0.1540</cell><cell>0.1956 0.0667</cell></row><row><cell cols="2">run-LIA.all-MiniLM-L6-v2.query-topic 0.3655 0.1765 0.1485 0.1245</cell><cell>0.1912</cell><cell>0.1647</cell><cell>0.1476</cell><cell>0.2043 0.0591</cell></row><row><cell cols="2">run-LIA.all-mpnet-base-v2.query-topic 0.3506 0.1647 0.1294 0.1098</cell><cell>0.1835</cell><cell>0.1517</cell><cell>0.1357</cell><cell>0.2073 0.0523</cell></row><row><cell>run-LIA.all-mpnet-base-v2.query</cell><cell>0.3302 0.1647 0.1529 0.1294</cell><cell>0.1802</cell><cell>0.1644</cell><cell>0.1462</cell><cell>0.1956 0.0602</cell></row><row><cell>run-LIA.lda</cell><cell>0.3138 0.1824 0.1456 0.1245</cell><cell>0.1666</cell><cell>0.1488</cell><cell>0.1387</cell><cell>0.1402 0.0521</cell></row><row><cell>run-LIA.es</cell><cell>0.3056 0.1118 0.0912 0.0804</cell><cell>0.1277</cell><cell>0.1080</cell><cell>0.0989</cell><cell>0.1935 0.0342</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,199.79,393.33,10.91;11,112.66,213.34,268.21,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,423.78,199.79,82.21,10.91;11,112.66,213.34,166.99,10.91">The readability of scientific texts is decreasing over time</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Plav√©n-Sigray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Schiffler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,288.01,213.34,20.17,10.91">Elife</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27725</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,226.89,393.33,10.91;11,112.66,240.44,395.16,10.91;11,112.66,253.99,394.53,10.91;11,112.66,267.54,393.98,10.91;11,112.66,281.08,393.33,10.91;11,112.66,294.63,301.67,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,449.73,226.89,56.26,10.91;11,112.66,240.44,322.05,10.91">Overview of simpletext -clef-2023 track on automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,460.78,240.44,47.04,10.91;11,112.66,253.99,24.11,10.91;11,112.66,281.08,393.33,10.91;11,112.66,294.63,271.72,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,112.66,308.18,393.33,10.91;11,112.66,321.73,393.33,10.91;11,112.66,335.28,107.17,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,406.33,308.18,99.66,10.91;11,112.66,321.73,313.37,10.91">Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>R√ºckl√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,348.83,393.71,10.91;11,112.66,362.38,368.30,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,449.54,348.83,56.83,10.91;11,112.66,362.38,258.71,10.91">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,379.54,362.38,28.54,10.91">choice</title>
		<imprint>
			<biblScope unit="volume">2640</biblScope>
			<biblScope unit="page">660</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,375.93,393.33,10.91;11,112.66,389.48,305.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,272.86,375.93,233.13,10.91;11,112.66,389.48,42.95,10.91">Improving zero-shot retrieval using dense external expansion</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,164.31,389.48,175.43,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">103026</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,403.03,393.33,10.91;11,112.66,416.58,393.33,10.91;11,112.33,430.13,29.19,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,477.87,403.03,28.12,10.91;11,112.66,416.58,241.58,10.91">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oƒüuz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,443.67,393.33,10.91;11,112.66,457.22,363.59,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="11,353.43,443.67,152.55,10.91;11,112.66,457.22,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,470.77,394.53,10.91;11,112.66,484.32,395.17,10.91;11,112.66,497.87,60.20,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,219.74,470.77,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.42,484.32,380.41,10.91;11,112.66,497.87,30.43,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,511.42,395.01,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="11,205.30,511.42,124.33,10.91">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,524.97,393.59,10.91;11,112.66,538.52,146.44,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<title level="m" coord="11,283.63,524.97,187.99,10.91">Document expansion by query prediction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,552.07,393.32,10.91;11,112.66,565.62,207.59,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06713</idno>
		<title level="m" coord="11,240.25,552.07,265.74,10.91;11,112.66,565.62,24.89,10.91">Document ranking with a pretrained sequence-to-sequence model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,579.17,393.53,10.91;11,112.66,592.72,393.33,10.91;11,112.66,606.27,320.29,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,335.61,579.17,170.58,10.91;11,112.66,592.72,80.36,10.91">Cedr: Contextualized embeddings for document ranking</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,216.35,592.72,289.63,10.91;11,112.66,606.27,222.86,10.91">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 42nd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,619.81,393.33,10.91;11,112.66,633.36,395.17,10.91;11,112.66,646.91,393.33,10.91;11,112.66,660.46,183.22,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,468.45,619.81,37.54,10.91;11,112.66,633.36,350.92,10.91">Efficient document re-ranking for transformers by precomputing term representations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,488.38,633.36,19.45,10.91;11,112.66,646.91,393.33,10.91;11,112.66,660.46,105.84,10.91">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,86.97,393.32,10.91;12,112.66,100.52,286.64,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09093</idno>
		<title level="m" coord="12,311.35,86.97,194.63,10.91;12,112.66,100.52,104.30,10.91">Parade: Passage representation aggregation for document reranking</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,114.06,393.53,10.91;12,112.66,127.61,393.57,10.91;12,112.33,141.16,29.19,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,337.74,114.06,168.45,10.91;12,112.66,127.61,234.27,10.91">Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07577</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.33,10.91;12,112.66,168.26,393.33,10.91;12,112.28,181.81,395.39,10.91;12,112.41,195.36,38.81,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,342.98,154.71,163.00,10.91;12,112.66,168.26,201.88,10.91">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofst√§tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,338.13,168.26,167.85,10.91;12,112.28,181.81,349.72,10.91">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,393.33,10.91;12,112.66,222.46,102.10,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="12,251.96,208.91,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
