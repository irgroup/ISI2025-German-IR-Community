<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,85.40,62.38,17.04;1,166.85,85.40,167.26,17.04;1,351.33,85.40,171.47,17.04;1,72.02,106.16,246.66,17.04;1,72.02,136.42,233.53,10.80">NLPalma</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,162.94,70.18,10.80"><forename type="first">Victor</forename><surname>Manuel</surname></persName>
						</author>
						<author>
							<persName coords="1,145.22,162.94,74.98,10.80"><forename type="first">Palma</forename><surname>Preciado</surname></persName>
							<email>c.palma.p0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Bretagne Occidentale</orgName>
								<address>
									<settlement>HCTI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.45,162.94,119.08,10.80"><forename type="first">Carolina</forename><forename type="middle">Palma</forename><surname>Preciado</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.15,162.94,76.35,10.80"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
							<email>sidorov@cic.ipn.mx</email>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,85.40,62.38,17.04;1,166.85,85.40,167.26,17.04;1,351.33,85.40,171.47,17.04;1,72.02,106.16,246.66,17.04;1,72.02,136.42,233.53,10.80">NLPalma</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44618C6533FFA87F0716B2397F8CF16A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>1 simplification</term>
					<term>concept identification</term>
					<term>term difficulty</term>
					<term>term classification 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The following work has the purpose of describing the participation in the SimpleText 2023 track, on the identification of the term and its identification of the terms and their difficulty as a term, among themselves, all this belonging to Task 2. To solve this task, we used an approach of language models using Bloom but opted for its BLOOMZ version for a fine-tuning more focused on human instructions or in a more understandable way with more description-style prompts given by text input on a task. To solve the handling of the difficulty between terms a very simple classifier based on BERT-multilingual was used since this was developed as a binary classification and for the term vs. term evaluation a small algorithm was taken to accommodate the internal term classifications. On the other hand, we also participated in Task 3 in which the objective was to simplify passages extracted from abstracts, using the same approach as Task 2, BLOOMZ was used for the simplification of this text since different prompts were tested in case it was necessary to make several passes with those parts that yielded poor results or null results. Given that this was the first time participating in such tasks we can say that the results obtained were quite satisfactory even though we believe that they can be substantially improved with some other approach, which would have to be further reviewed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>For this work the main objective we seek to fulfill the tasks as completely as possible, we believe that language models can present a good starting point to develop the tasks of this track, therefore BLOOMZ <ref type="bibr" coords="1,121.34,550.89,12.78,9.94" target="#b1">[2]</ref> was taken as a model to use because its qualities allow us to have input more consistent with human instruction, This means that you have to be very clear with the instructions given to the model and therefore the prompt to be used, it is also important to provide the model with as much context as possible to get the desired results. In this case, the simplification of the passage can alter the results depending on the previous context given, the same happening if the prompt is altered.</p><p>A very important part that had to be taken into account is the extraction of important terms in the given sentence for the task, besides assigning a number according to its complexity, so this complexity can have different parameters to measure the level of complexity whether it is the size of the word, the linguistic context, among others, so taking any of them could perhaps impact this type of classification based on complexity, so it was decided to opt for a binary classification.</p><p>It is true that the starting point is BLOOMZ but we also have to use other types of models to do classification, although BLOOMZ is able to do this classification with the correct prompt, we believe that the use of other tools can speed up the process without having to rely entirely on BLOOMZ in which the more parameters you try to take, the longer it will take to give results, obviously this in many cases allows better results despite the delay that could represent. To classify we use a BERT-type model, in this case the multilingual one, to foresee any eventuality of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach to the task</head><p>From the task proposed by SimpleText <ref type="bibr" coords="2,264.58,195.64,11.93,9.94" target="#b0">[1]</ref>, Task 2 and Task 3 were carried out by implementing NLP models specifically deep learning models like transformers. In Task 2 identification and explanation of difficult concepts in the paragraph are done with the aim to define the meaning of key concepts for better conceptualization and understanding of a paragraph. On the other hand, Task 3 aims to create summarized text from scientific abstracts written in English, this concise version should maintain the original meaning in a simplified form.</p><p>The data <ref type="bibr" coords="2,126.14,271.63,12.80,9.94" target="#b3">[3]</ref> provided is divided into three dataset sizes (small, medium, and large), for this approach it was decided to use the medium dataset since is the middle ground of a sample of the text and sufficient data to observe the performance of the applied method. Text summarization is a difficult task that can be achieved from different approaches, in this case, a deep learning model known as BLOOMZ &amp; mT0 is used to generate text summarization and determine difficult words in a paragraph. Since there are multiple versions of this model with different parameters, different versions were used when performing the tasks as the resources available were not sufficient to run some models.</p><p>The first step to complete Task 2, was to identify up to five of the most difficult terms in a sentence the first pass was done by using mt0-xl since this version could be loaded in a local environment without running out of memory, the first pass resolved around half of the dataset (2,320 sentences). After having several empty results with various prompts like "Give me up to five of the most difficult words of the next text:", "Give me up to five of the most difficult words of the next sentence:", "Give me the top five most difficult words of the next text:", and "Suggest me at up five of the most difficult words on the next text:" it was decided to do the second pass and third pass with the Hugging Face inference API <ref type="bibr" coords="2,72.02,448.65,12.92,9.94" target="#b7">[7]</ref> that uses the BLOOMZ finetuned model with 176B parameters.</p><p>To explain the terms obtained in the first step, the same model is now implemented with prompts such as "Meaning of:", "Definition of:", "Give me the definition:", "Give me the meaning:" all of this prompt were used in succession if one of them did not produce results, the next prompt was used until a favorable output was obtained. Since the task accepts both the definition of the word and an example of it, no further process was done.</p><p>The next step consists in scoring and ranking the terms, for the first part a multilingual BERT <ref type="bibr" coords="2,509.93,524.61,13.20,9.94" target="#b6">[6]</ref> was trained with the corpus provided, the score in the training text varies from 1 to 2 so the terms were assigned this range of scoring, 2 indicating a difficult term and 1 an easier one. The BERT model trained for the scoring was executed with the help of the Ktrain <ref type="bibr" coords="2,319.68,562.53,12.91,9.94" target="#b5">[5]</ref> wrapper that allows to load and perform the fine-tuning process.</p><p>Then Term ranking is done using the score and the term length, fist the words of the sentences are found and separated by the score, then a sub-ranking is designated from the length of the words, where the term with the most characters is the most difficult and the term with the fewest is the easiest term. If the terms are the same length they are arranged in order of appearance. Finally, when both lists for scores 1 and 2 are ordered, they are united to create the final ranking.</p><p>To elaborate Task 3 BLOOMZ is also implemented, this time instead of using the inference API a system for inference and fine-tuning called Petals <ref type="bibr" coords="2,291.05,663.72,12.80,9.94" target="#b8">[8]</ref> is used since the Hugging Face API, this platform joins computer sources from different servers to increase the computational power. The summarization prompts include "Summarize the text:" and "Summarize the sentence:". As with previous processes, multiple runs were executed to obtain the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Resources employed</head><p>Among the resources used to train and evaluate the models, Google's Colab environment was used, this platform allows Python programming and execution. It also allows an easier use of GPU, which served to perform the described tasks faster since this resource allows performing multiple simultaneous computations. The server used has the following specifications GPU NVIDIA-SMI 525.85.12, CUDA v12.0, and 25 of RAM.</p><p>As part of the resources used, it was decided to use a combination of Google Collaboratory with Petals for Task 2 in complexity spotting and Task 3 for simplification of scientific text, since we do not have the computational capacity to run models of parameter size 176B, which represent a larger amount of computation than we can handle, so the use of Petals becomes indispensable for those jobs that do not have enough computational capacity since it allows the benefits of crowd computing to make an inference with this type of large models in a relatively easy/semi-efficient way and in a way having the flexibility of an API and the power of PyTorch.</p><p>On the other hand, for the complexity ranking, we used BERT-multilingual <ref type="bibr" coords="3,420.82,257.59,11.77,9.94" target="#b6">[6]</ref>, and for the internal complexity between terms of the same sentence we used a small algorithm that takes the length of the sentence and the value obtained from the previous classification to make 2 groups, if necessary, between those that have the value of 1 and 2 in complexity to generate the internal complexity ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>As part of the results that we will present below, we will explain from a very general point of view the type of results obtained for each task and some of their particularities. We will explain some of the cases that we believe could be interesting to take into account for our future participation in these tasks for SimpleText in subsequent deliveries, taking into account that perhaps we could use some other more ad hoc models for tasks such as simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2: "What is unclear?" Difficult concept identification and explanation</head><p>In the following example, we can see that the word that was extracted is CNN, but it did not know how to obtain the context of the sentence, so when throwing the meaning or the explanation it misunderstands CNN (Convolutional Neural Network) for CNN (Cable News Network) and this is completely incorrect, since as we can infer they are completely different things, one refers to a Machine learning model and the other to a news network, so there is no point of comparison and the result obtained refers to the latter.</p><p>• A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. • Term rank: 2 • Word: CNN • Word Meaning or explanation: CNN.com It is logical to think that cnn.com refers to the web address of the news channel and clearly not to the type of neural network since there is no way that such a domain refers to an abstract element rather than a company.</p><p>The following example is one that we believe represents a term that could be difficult to explain, since being acronyms these can mean different things, which will depend on the field of study that makes reference to the context of the sentence, so it is extremely important to take this into account and in this case, it is clear that the training data off ease in the way of how to find a difficult term and therefore the context that has these acronyms, throw a correct explanation.</p><p>• This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices.</p><p>• Term Rank: 2</p><p>• Word: IoT • Word Meaning or explanation: Internet of Things (IoT) is a network of physical objects or "things" embedded with electronics It could also refer to another acronym such as EM but since this term could be complex, but is already explained in that sentence, it would not make much sense to take it into account.</p><p>The classification was another aspect that had to be applied in the sentences, this is a classification for the term extracted given its complexity and if more than one term was extracted from the sentence these had to be classified among themselves obtaining an ascending value given its complexity, this can be seen in the following words extracted from the same passage: It is observed that the length of the word does not play such an important role in the rank assigned to a term under its difficulty, the explanation can only be inferred but that certain words are ranked higher than others is due to the role they play in the context and the difficulty to explain them in the given context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 3: Rewrite this! Rewriting scientific text</head><p>The case of simplifying a passage is a very interesting one, the idea of compacting a sentence or idea to such an extent, can be complicated, since this can lead, if it is very compact, to lose information and not be able to fully understand the idea or lend itself to an erroneous interpretation, on the other hand, that the idea is quite loose and does not explain things well, by summarizing incorrect ideas.</p><p>As can be seen, the following sentence was summarized erroneously since, as can be seen, it loses the focus of the information and does not mention that the data were collected by researchers, the center of information is the process, not the people who collected it.</p><p>• Excerpt: Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process. • Simplified excerpt: The data was collected by a team of trained researchers In this positive case of simplification, we can observe the type of simplification of the information in terms of content while maintaining the context and even explaining it in much simpler terms, instead of saying that the information is produced and accessed, it changes it to a source of information, which in this context means the same thing, although the final part of the sentence could be omitted and would not affect the result producing a more compact fragment.</p><p>• Excerpt: The advent of WWW changed the way we can produce and access information.</p><p>• Simplified excerpt: The Web has become a major source of information for many people</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We can conclude that our decisions in the approach taken can give us good results and observe that they can be improved in a substantial way in which the results are presented, for example in the simplification task it can be observed that many times the context of the original sentence is lost, perhaps giving more information about the same sentence could solve this problem, On the other hand, for the task of term extraction, we could observe that it is difficult to obtain an accurate way of knowing that the term is really difficult in terms of its context or that perhaps it is a compound term or an acronym that makes it complex, this leaves open different possibilities for improvement that we believe can be exploited in subsequent works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,104.18,229.61,254.56,10.05;4,104.18,243.05,261.53,10.05;4,104.18,256.52,250.96,10.05;4,104.18,269.96,272.56,10.05;4,104.18,283.40,266.68,10.05"><head>•</head><label></label><figDesc>attack Term difficulty: 2 Internal term difficulty: 1 • system Term difficulty: 2 Internal term difficulty: 2 • blind Term difficulty: 2 Internal term difficulty: 3 • False data Term difficulty: 1 Internal term difficulty: 4 • injection Term difficulty: 1 Internal term difficulty: 5</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,108.02,157.72,414.87,9.94;5,108.02,170.32,415.00,9.94;5,108.02,183.04,414.81,9.94;5,108.02,195.64,414.88,9.94;5,108.02,208.36,414.86,9.94;5,108.02,220.96,414.79,9.94;5,108.02,233.56,146.19,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,195.69,170.32,327.33,9.94;5,108.02,183.04,83.58,9.94">Overview of SimpleText -CLEF-2023 track on Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,325.70,208.36,197.19,9.94;5,108.02,220.96,414.79,9.94;5,108.02,233.56,81.69,9.94">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="5,108.02,246.28,414.61,9.94;5,108.02,258.91,414.81,9.94;5,108.02,271.63,29.44,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,108.02,258.91,269.43,9.94">Cross-lingual generalization through multitask finetuning</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<idno>ArXiv:2211. 01786</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct coords="5,137.46,271.63,385.60,9.94;5,108.02,284.23,261.90,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,282.04,271.63,241.02,9.94;5,108.02,284.23,30.33,9.94">Deciding equivalances among conjunctive aggregate queries</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nutt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sagic</surname></persName>
		</author>
		<idno type="DOI">10.1145/1219092.1219093</idno>
	</analytic>
	<monogr>
		<title level="j" coord="5,145.32,284.23,34.90,9.94">J. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,296.83,405.59,9.94;5,108.02,309.44,415.13,10.05;5,108.02,322.04,146.19,10.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,172.32,309.44,350.82,10.05;5,108.02,322.04,29.37,10.04">CLEF 2023 SimpleText Track: What Happens If General Users Search Scientific Texts?</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,145.07,322.04,104.42,10.04">ECIR 2023 Proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,334.87,391.84,9.94;5,108.02,347.47,405.30,9.94;5,108.02,360.07,169.95,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,375.51,334.87,124.35,9.94;5,108.02,347.47,246.44,9.94">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805I" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,372.79,414.95,9.94;5,108.02,385.39,120.78,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,211.84,372.79,278.47,9.94">ktrain: A Low-Code Library for Augmented Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Maiya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10703</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,108.02,398.11,414.83,9.94;5,108.02,410.71,165.75,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,331.68,398.11,184.59,9.94">How Multilingual is Multilingual BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1493</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1493" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,423.43,415.13,9.94;5,108.02,436.05,415.05,9.94;5,108.02,448.65,198.15,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,108.02,423.43,128.74,9.94;5,355.03,436.05,168.04,9.94;5,108.02,448.65,69.30,9.94">Spanish PreTrained BERT Model and Evaluation Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,196.68,448.65,81.96,9.94">PML4DC at ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Inference API -Hugging Face</note>
</biblStruct>

<biblStruct coords="5,108.02,461.37,414.84,9.94;5,108.02,473.97,415.10,9.94;5,108.02,486.69,348.40,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,266.30,473.97,256.82,9.94;5,108.02,486.69,30.73,9.94">Petals: Collaborative Inference and Fine-tuning of Large Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Borzunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chumachenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Samygin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2209.01188</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2209.01188" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
