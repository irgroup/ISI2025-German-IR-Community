<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,398.73,15.42;1,89.29,106.66,362.69,15.42;1,89.29,129.00,227.95,11.96">An Evaluation of MUSS and T5 Models in Scientific Sentence Simplification: A Comparative Study Notebook for the SimpleText Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,66.11,11.96"><forename type="first">Running</forename><surname>Hou</surname></persName>
							<email>running.hou@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Binzmühlestrasse 14</addrLine>
									<postCode>8050</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.56,154.90,48.11,11.96"><forename type="first">Xinyi</forename><surname>Qin</surname></persName>
							<email>xinyi.qin@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Binzmühlestrasse 14</addrLine>
									<postCode>8050</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,398.73,15.42;1,89.29,106.66,362.69,15.42;1,89.29,129.00,227.95,11.96">An Evaluation of MUSS and T5 Models in Scientific Sentence Simplification: A Comparative Study Notebook for the SimpleText Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">55C742C88F3905FBED72433DE36512A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses a study by the QH Research Group at the University of Zurich aimed at simplifying scientific text for SimpleText@CLEF-2023's Task 3. Using the pre-trained MUSS and T5 models, we explored their effectiveness in reducing sentence complexity without loss of essential information. Performance comparison across various scientific fields was undertaken, using both quantitative and qualitative measures for assessing simplification quality and fluency. Results highlight the substantial potential of both models, yet revealing distinct strengths and weaknesses. Strategies for further enhancements are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the modern era of rapidly advancing knowledge, accessibility to complex scientific research is an issue of increasing importance. As a significant portion of scientific knowledge remains confined within academia, it often becomes challenging for non-specialists to comprehend due to the inherent complexity of scientific language <ref type="bibr" coords="1,309.17,440.15,11.47,10.91" target="#b0">[1]</ref>. One proposed solution to this challenge lies in the realm of Natural Language Processing (NLP): the simplification of scientific sentences <ref type="bibr" coords="1,89.29,467.25,11.43,10.91" target="#b1">[2]</ref>.</p><p>Scientific sentence simplification aims at reformulating scientific texts to make them more understandable, thereby bridging the knowledge gap between expert and non-expert audiences <ref type="bibr" coords="1,89.29,507.90,11.58,10.91" target="#b2">[3]</ref>. This can lead to increased democratization of science, allowing a broader audience to engage with and benefit from scientific discoveries <ref type="bibr" coords="1,317.99,521.45,11.43,10.91" target="#b3">[4]</ref>.</p><p>Recent advancements in NLP models have shown promise in text simplification tasks. In this paper, we focus on two such pre-trained models, MUSS (Multilingual Sentence Simplifier) <ref type="bibr" coords="1,475.17,548.55,12.68,10.91" target="#b4">[5]</ref> and T5 (Text-to-Text Transfer Transformer) <ref type="bibr" coords="1,263.73,562.10,11.31,10.91" target="#b5">[6]</ref>. MUSS and T5, both are reputed models in sentence simplification, have been chosen for their established capabilities in handling multilingual and large-scale text corpora, respectively.</p><p>Our research aim is to evaluate the effectiveness of these models in reducing the complexity of scientific sentences whilst ensuring that the core information remains intact. To this end, we have conducted an in-depth evaluation, comparing the performance of the two models across multiple scientific domains.</p><p>This paper employs a combination of quantitative and qualitative metrics to assess the quality and fluency of the simplification provided by these models. As each model exhibits unique strengths and limitations, we also delve into these attributes, discussing potential strategies for further improvement.</p><p>We hope this research will contribute to the expanding field of scientific text simplification and assist in the broader efforts of making science more accessible and democratic. The insights drawn from our study could potentially direct future work in this area and lead to more effective models for scientific text simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>This section elucidates our research methods, primarily focusing on the adaptation of the Multilingual Unsupervised Sentence Simplification (MUSS) model to a HuggingFace BART model, as well as the deployment of a T5-large model. The employed data, comprising a comprehensive corpus of English scientific sentences from the Cross-Language Evaluation Forum (CLEF), is also outlined. The training and finetuning processes of these models are explored in-depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>Our research utilizes a substantial dataset provided by the CLEF organizers, which comprises an array of scientific sentences from various domains. This dataset is distinguished by its extensive scale and diverse representation of different scientific fields, rendering it apt for our study.</p><p>The dataset contains only 648 training entries with original sentences as input and humangenerated simplified sentences as the target. Testing data is categorized into small (2,234 entries), medium (4,797 entries), and large (152,073 entries) sets. Data in this dataset is derived from abstracts of scientific articles. These abstracts are segmented into sentences, with each sentence treated as an individual data point. Therefore, the column 'query' in the dataset refers to the original article's topic of the sentence.</p><p>The wide-ranging topics include 'drones', 'self-driving', 'cryptocurrency', 'digital marketing', and 'gene editing' among others, with the most specific focus on various aspects of 'muscle hypertrophy' and 'exercise training'. The diversity in the corpus, spanning several scientific domains, offers an opportunity to evaluate the versatility of MUSS and T5 models in scientific text simplification. The complexity of the sentences also serves as an appropriate challenge for these advanced models, effectively testing their capabilities <ref type="bibr" coords="2,355.32,601.80,11.43,10.91" target="#b6">[7]</ref>.</p><p>In order to maintain the coherence of the simplified sentence with the main topic, we insert the query words at the end of the original sentence, connected by the phrase 'related to'. For example, as shown in Table <ref type="table" coords="2,211.11,642.44,3.66,10.91">1</ref>, the topic of the first sentence is 'How many training per week for hypertrophy?'. In addition, we the keyword simplify is added at the beginning of each source sentence to mark it as a simplification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Models</head><p>MUSS, the Multilingual Sentence Simplifier, has demonstrated superior performance in sentencelevel simplification tasks across multiple languages, hence justifying its selection for this research <ref type="bibr" coords="3,128.85,134.63,11.30,10.91" target="#b4">[5]</ref>. We use similar control tokens as defined by Martin <ref type="bibr" coords="3,373.23,134.63,12.71,10.91" target="#b4">[5]</ref> to control different aspects of simplification including compression ratio (Chars), paraphrasing (Levenshtein similarity), lexical complexity (word rank), syntactic complexity (the depth of the dependency tree). In addition, we add another aspect of compression ratio (Words) as we believe that simple texts should contain fewer words. The T5(Text-to-Text Transfer Transformer) model, specifically the T5-large variant, has demonstrated proficiency in a number of Natural Language Processing (NLP) tasks including translation, summarization, and sentence simplification, making it a promising choice for our study <ref type="bibr" coords="3,229.67,229.48,11.43,10.91" target="#b5">[6]</ref>.</p><p>For our research, both the MUSS and T5-large models were trained for 8 epochs with a learning rate of 3e-5 and a batch size of 8, which optimizes their performance in our specific context of scientific sentence simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">control tokens</head><p>Five control tokens are embedded into input sentences:</p><p>• Character Length Ratio (C): The ratio of the number of characters in the target sentence to the number of characters in the source sentence. • Normalized Levenshtein Similarity (L): The normalized similarity at the character level between the source and target sentences, based on the Levenshtein distance. • WordRank (WR): The inverse frequency order of all words in the target sentence compared to the source sentence. • Dependency Tree Depth Ratio (DTD): The ratio of the maximum depth of the dependency tree in the target sentence to that of the source sentence. • Word Ratio (W): The ratio of the number of words in the target sentence to the number of words in the source sentence.</p><p>Table <ref type="table" coords="3,115.20,492.25,4.97,10.91">1</ref> presents instances of sentences that have been encoded with control tokens for training. During inference, control tokens are assigned predetermined fixed values. These values are hyperparameters that can be adjusted according to the target ratio we want the model to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">lexical Complexity</head><p>The lexical complexity score for a given sentence is calculated by first converting each sentence into a list of words. This is done through a process of tokenization, removing punctuation, and filtering out common "stop words". The list of words is then further refined to include only those words which are present in our preprocessed word ranking dictionary, effectively filtering out unknown words. We then convert each word in the sentence into its respective rank obtained from our preprocessed dictionary. These ranks are logged (to smooth out the distribution), and the 75th percentile (the third quartile) of these ranks is taken as the sentence's lexical complexity score. This means that we mainly consider the top 25% most complex words in the sentence when assessing the sentence's overall complexity. In the case of batch processing, we calculate the score for each pair of simple and complex sentences, take a safe division of the scores, and then calculate the mean of these ratios. Thus, our method provides a single numerical score that represents the lexical complexity of a sentence, or the average complexity ratio between two lists of sentences, which can be utilized to compare and assess different textual contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Sentence Simplification using Control Tokens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Text</head><p>Target Text simplify: W_0.67 C_0.64 L_0.59 WR_0.97 DTD_0.67 Meta-regression analysis of nonvolume-equated studies showed a significant effect favoring higher frequencies, although the overall difference in magnitude of effect between frequencies of 1 and 3+ days per week was modest, related to How many training Iper week for hypetrophy?.</p><p>Analysis of studies with different training volumes showed better results for higher frequencies, although the difference between frequencies of 1 and 3+ days per week was small. simplify: W_0.78 C_0.76 L_0.86 WR_1.06 DTD_1.00 Four major capabilities were identified, each of which evolves as a result of using the tools, related to digital marketing.</p><p>Four major capabilities were identified, each of which evolves as a result of using the tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and Discussion</head><p>The evaluation metrics chosen for this study were designed to reflect our goals of sentence simplification. We aimed to measure the level of semantic similarity, the preservation of essential information, the reduction of extraneous details, the addition of suitable words, and the linguistic quality and readability of the simplified sentences.</p><p>Our research findings contribute to our understanding of MUSS and T5's capabilities in the field of scientific sentence simplification. Both models showed promise, each presenting unique strengths and weaknesses when confronted with the nuances of scientific sentences from chosen disciplines. Tables 2, 3, and 4 provide numerical insights into model performance according to the applied evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Metrics</head><p>SARI (System output, Automatic and Reference Inputs) contributed to our evaluation by focusing on three facets of text simplification: the preservation of meaning (KEEP), the addition of appropriate words (ADD), and the deletion of unnecessary information (DELETE) <ref type="bibr" coords="4,455.83,598.08,11.43,10.91" target="#b7">[8]</ref>.</p><p>BLEU (Bilingual Evaluation Understudy), a metric developed to measure the overlap of ngrams between machine-generated translations and multiple reference translations <ref type="bibr" coords="4,469.92,625.18,11.58,10.91" target="#b8">[9]</ref>, was used to gauge the linguistic quality of the simplified sentences in our context. FKGL (Flesch-Kincaid Grade Level), a readability test that calculates a score based on the average number of syllables per word and the average number of words per sentence <ref type="bibr" coords="4,486.67,665.83,16.41,10.91" target="#b9">[10]</ref>, helped us understand the extent to which the complexity of the scientific sentences was reduced by the models.</p><p>The Compression Ratio evaluates the extent to which the simplified sentence is shorter than the original sentence. This metric is helpful for assessing the extent of reduction in the complexity of the sentence after simplification.</p><p>Levenshtein Similarity, on the other hand, measures the number of single-character edits required to change one sentence into the other. In our context, it helps us assess how much the simplified sentence differs from the original one, thus providing a measure of information preservation and modification.</p><p>The Lexical Complexity Score helps us evaluate the linguistic complexity of the simplified sentences. It provides insights into the readability of the simplified sentences.</p><p>These chosen metrics are widely regarded in the field of automatic text simplification and allowed us to evaluate different aspects of the models' performances, from semantic accuracy to readability. The results based on these metrics are detailed in Tables <ref type="table" coords="5,406.39,263.11,7.47,10.91" target="#tab_0">2,</ref><ref type="table" coords="5,413.86,263.11,3.74,10.91" target="#tab_1">3</ref>, and 4 below.</p><p>Table <ref type="table" coords="5,126.10,276.66,4.97,10.91" target="#tab_0">2</ref> illustrates examples of sentences before and after simplification by MUSS and T5. This table exemplifies the different approaches each model took to simplification. For instance, in the context of "penetration testing", MUSS retained the original sentence structure and content, while T5 removed important details, potentially affecting the understanding of the concept.</p><p>When simplifying a sentence regarding "decompression", MUSS smoothly retained the essence of the original sentence, while T5 removed the connective term "though", subtly impacting the sentence tone. For "classification algorithm", MUSS again preserved the original sentence, while T5 decided to eliminate the temporal marker "finally", subtly altering the sentence's flow. In the case of the term "steganographic approach", MUSS maintained the original sentence but replaced 'imperceptible' with 'undetectable', potentially enhancing the sentence's comprehensibility. However, T5's simplification was incomplete, cutting off at the end of the sentence, and leaving important information out.</p><p>Table <ref type="table" coords="5,127.46,439.25,5.16,10.91" target="#tab_1">3</ref> compares MUSS and T5's handling of specific scientific terms. It reveals how each model navigates complex terminology during the simplification process. For instance, with the term "decompression", both MUSS and T5 successfully integrated the concept in a simplified manner. Yet, when it came to terms like "penetration testing" or "classification algorithm", T5 omitted crucial information, potentially compromising the coherence of the scientific concepts involved.</p><p>Table <ref type="table" coords="5,127.57,520.54,5.17,10.91" target="#tab_2">4</ref> provides a quantitative comparison of the models based on our chosen evaluation metrics. In terms of Compression ratio, Levenshtein similarity, and Lexical complexity score, MUSS showed superior performance, indicating its ability to reduce sentence length, maintain semantic similarity to the original sentence, and achieve a lower complexity level. The SARI and BLEU scores followed a similar pattern, with MUSS scoring higher, suggesting it performed better in preserving the original meaning while deleting unnecessary information and matching reference translations.</p><p>However, in terms of FKGL, which measures readability, T5 outperformed MUSS with a lower score, indicating that T5 might produce simpler sentences, even though they may lose some crucial information. This contrast underscores a tension between readability and semantic preservation, which is a key challenge in text simplification tasks.</p><p>In conclusion, the results highlight that while both models exhibit potential for scientific Although penetration testing has traditionally focussed on technical aspects, the field has started to realise the importance of the human in the organisation, and the need to ensure that humans are resistant to cyberattacks.</p><p>Although penetration testing has traditionally focussed on technical aspects, the field has started to realise the importance of the human in the organisation, and the need to ensure that humans are resistant to cyberattacks.</p><p>Despite the importance of the human in the organisation, and the need to ensure that humans are resistant to cyberattacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T15.1_1 5763372 84_7</head><p>Though decompression is not required.</p><p>There is no need for decompression, though.</p><p>Decompression is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T13.3_2 9441887 49_4</head><p>Finally, use the 10 sets of imbalanced data in the KEEL database as test objects, and F-value and G-mean are used as evaluation indicators to verify the performance of the classification algorithm.</p><p>Finally, use the 10 sets of imbalanced data in the KEEL database as test objects, and F-value and G-mean are used as evaluation indicators to verify the performance of the classification algorithm.</p><p>Use the 10 sets of imbalanced data in the KEEL database as test objects, and F-value and G-mean are used as evaluation indicators to verify the performance of the classification algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T15.1_1 5763372 84_5</head><p>To enlarge the capacity of the hidden secret information and to provide an imperceptible stego-image for human vision, a novel steganographic approach called triway pixel-value differencing (TPVD) is used for embedding.</p><p>To enlarge the capacity of the hidden secret information and to provide an imperceptible stego-image for human vision, a novel approach called tri-way pixelvalue differencing (TPVD) is used for embedding.</p><p>To enlarge the capacity of the hidden secret information and to provide an imperceptible stego-image for human vision, a novel steganographic approach called triway pixel-value differencing (TP  but at the potential cost of omitting key information. This points towards the importance of finding a balance between readability and information preservation in text simplification tasks, a topic warranting further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>This research aimed to investigate the efficiency of the MUSS and T5 models in the challenging task of scientific sentence simplification. Our findings highlight the significant potential of both models, while also shedding light on their unique strengths and weaknesses. MUSS showed consistent performance in maintaining the original sentence's structure and meaning, suggesting it to be a reliable choice for preserving technical details in complex sentences. T5, while demonstrating reasonable proficiency, did occasionally omit important details, suggesting areas for further improvement. However, it is crucial to acknowledge the limitations of our study. The use of only two models restricts the generalizability of our findings to all sentence simplification models. Moreover, the performance of these models may vary across different scientific domains and levels of sentence complexity, beyond what was covered by the CLEF dataset.</p><p>Future research should extend this analysis to a broader range of models and datasets, including different languages and scientific fields, to understand better the models' performances. The models themselves could also be improved, particularly in terms of preserving essential information while simplifying text and handling very complex sentences more efficiently. These improvements may involve fine-tuning existing models, developing novel training methodologies, or even creating new models altogether.</p><p>The implications of this work are significant. By making scientific literature more accessible through sentence simplification, we can democratize science and promote knowledge sharing among non-experts. The use of models like MUSS and T5 could become an integral part of the future of scientific communication, making the world of research more inclusive and approachable for everyone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.49,336.41,73.22"><head>Table 2</head><label>2</label><figDesc>Examples of sentences before and after simplification</figDesc><table coords="6,95.67,118.53,329.74,45.18"><row><cell>snt_id Original</cell><cell>Muss</cell><cell>T5-large</cell></row><row><cell>T15.1_2</cell><cell></cell><cell></cell></row><row><cell>9520022</cell><cell></cell><cell></cell></row><row><cell>52_2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,495.64,418.20,180.82"><head>Table 3</head><label>3</label><figDesc>Comparison of Text Simplification Examples by MUSS and T5-Large</figDesc><table coords="6,89.29,527.21,417.90,149.25"><row><cell>Scientific Term</cell><cell>MUSS -GPU</cell><cell>T5-Large -GPU</cell></row><row><cell cols="2">Penetration Testing Original</cell><cell>Omitted Crucial Infor-</cell></row><row><cell></cell><cell></cell><cell>mation</cell></row><row><cell>Decompression</cell><cell>Smooth Integration</cell><cell>Removed 'Though'</cell></row><row><cell>Classification</cell><cell>Original</cell><cell>Removed 'Finally'</cell></row><row><cell>Algorithm</cell><cell></cell><cell></cell></row><row><cell>Steganographic</cell><cell>Changed 'impercepti-</cell><cell>Cut off at the end</cell></row><row><cell>Approach</cell><cell>ble' to 'undetectable'</cell><cell></cell></row><row><cell cols="3">text simplification, MUSS generally shows better performance in semantic preservation and</cell></row><row><cell cols="3">coherence, which are crucial in scientific contexts. Meanwhile, T5 seems to prioritize readability,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,411.00,93.78"><head>Table 4</head><label>4</label><figDesc>Performance Metrics Comparison for MUSS and T5-Large Models</figDesc><table coords="7,95.27,122.05,404.73,62.21"><row><cell>Model</cell><cell>SARI</cell><cell>BLUE</cell><cell>FKGL</cell><cell>Compression</cell><cell>Levenshtein</cell><cell>Lexical</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ratio</cell><cell>similarity</cell><cell>complexity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>score</cell></row><row><cell>Muss</cell><cell>26.5</cell><cell>21.2</cell><cell>12.5</cell><cell>0.94</cell><cell>0.92</cell><cell>8.50</cell></row><row><cell>T5</cell><cell>27.6</cell><cell>20.2</cell><cell>12.7</cell><cell>0.90</cell><cell>0.91</cell><cell>8.50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,130.96,316.33,376.01,8.97;1,130.69,327.29,100.19,8.97"><p>Scientific Sentence Simplification, Multilingual Sentence Simplifier, Text-to-Text Transfer Transformer, Text Complexity Reduction</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to express our gratitude to the <rs type="institution">Cross-Language Evaluation Forum (CLEF)</rs> for providing the dataset used in this study. We also thank <rs type="person">Professor Simon Clematide</rs>, <rs type="person">Tannon Kew</rs> and <rs type="person">Andrianos Michail</rs> who have provided invaluable insights and feedback throughout the course of this research.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The source code is accessible via • GitHub</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,159.14,306.77,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,158.54,159.14,132.94,10.91">Is there a language of science?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,299.73,159.14,30.70,10.91">Nature</title>
		<imprint>
			<biblScope unit="volume">467</biblScope>
			<biblScope unit="page" from="153" to="155" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,172.69,393.32,10.91;8,112.66,186.24,118.08,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,153.81,172.69,232.43,10.91">The complexities of computational text simplification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,394.32,172.69,111.67,10.91;8,112.66,186.24,40.32,10.91">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12323</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,199.79,393.33,10.91;8,112.66,213.34,393.33,10.91;8,112.66,226.89,364.94,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,272.49,199.79,233.49,10.91;8,112.66,213.34,123.95,10.91">Towards a better understanding of the challenge of scientific text simplification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mandya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Orasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,260.25,213.34,245.73,10.91;8,112.66,226.89,287.31,10.91">Proceedings of the Third Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)</title>
		<meeting>the Third Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,240.44,393.33,10.91;8,112.66,253.99,343.44,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,260.50,240.44,245.49,10.91;8,112.66,253.99,200.75,10.91">Science communication 2.0: The impact of online media and popular science infotainment on sciences</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Scharrer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,321.66,253.99,46.54,10.91">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">230432</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,267.54,394.62,10.91;8,112.66,281.08,394.61,10.91;8,112.66,294.63,134.11,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,306.34,267.54,176.64,10.91">Muss: Multilingual sentence simplifier</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,281.08,394.61,10.91;8,112.66,294.63,36.14,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3254" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,308.18,394.53,10.91;8,112.66,321.73,393.59,10.91;8,112.66,335.28,146.44,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" coord="8,112.66,321.73,358.56,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,348.83,395.17,10.91;8,112.66,362.38,393.33,10.91;8,112.66,375.93,395.17,10.91;8,112.66,389.48,393.32,10.91;8,112.66,403.03,393.33,10.91;8,112.66,416.58,306.11,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,376.98,348.83,130.85,10.91;8,112.66,362.38,249.78,10.91">Overview of simpletext -clef-2023 track on automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Stéphane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jaap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.33,389.48,59.65,10.91;8,112.66,403.03,393.33,10.91;8,112.66,416.58,252.02,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,112.66,430.13,393.33,10.91;8,112.66,443.67,393.33,10.91;8,112.66,457.22,127.28,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,371.43,430.13,134.56,10.91;8,112.66,443.67,150.09,10.91">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,273.25,443.67,232.74,10.91;8,112.66,457.22,48.41,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,470.77,393.33,10.91;8,112.66,484.32,393.53,10.91;8,112.66,497.87,203.57,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,310.52,470.77,195.46,10.91;8,112.66,484.32,88.86,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,224.89,484.32,281.30,10.91;8,112.66,497.87,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,511.42,393.61,10.91;8,112.66,524.97,393.53,10.91;8,112.66,538.52,393.33,10.91;8,112.33,552.07,119.22,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,375.48,511.42,130.79,10.91;8,112.66,524.97,393.53,10.91;8,112.66,538.52,102.86,10.91">Derivation of new readability formulas (Automated Readability Index, Fog Count, and Flesch Reading Ease Formula) for Navy enlisted personnel</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Naval Technical Training Command Millington TN Research Branch</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
