<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,78.78,450.81,17.64;1,72.00,103.19,259.88,17.64;1,72.00,149.39,224.16,10.58">CLEF2023 SimpleText Task 2, 3: Identification and Simplification of Difficult Terms Notebook for the SimpleText Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,172.38,83.65,10.54"><forename type="first">Dennis</forename><forename type="middle">R</forename><surname>Davari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University od Kiel</orgName>
								<address>
									<addrLine>Christian-Albrechts-Platz 4</addrLine>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.25,172.38,78.32,10.54"><forename type="first">Antonela</forename><surname>Prnjak</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Split</orgName>
								<address>
									<addrLine>Ruđera Boškovića 31</addrLine>
									<settlement>Split</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.16,172.38,79.00,10.54"><forename type="first">Kristina</forename><surname>Schmitt</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Malta</orgName>
								<orgName type="institution" key="instit2">University of Malta</orgName>
								<address>
									<postCode>MSD 2080</postCode>
									<settlement>Msida</settlement>
									<country key="MT">Malta</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,78.78,450.81,17.64;1,72.00,103.19,259.88,17.64;1,72.00,149.39,224.16,10.58">CLEF2023 SimpleText Task 2, 3: Identification and Simplification of Difficult Terms Notebook for the SimpleText Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55B9CCE6EA67962186750E0BD29F914F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>reading comprehension</term>
					<term>scientific texts</term>
					<term>text simplification</term>
					<term>CLEF 2023 SimpleText lab competition [1]</term>
					<term>natural language processing (NLP)</term>
					<term>GPT</term>
					<term>BLOOMZ</term>
					<term>ST5</term>
					<term>WIKI</term>
					<term>AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Comprehending scientific texts is crucial for succeeding in education and beyond. However, individuals who are not experts in a particular scientific field often face difficulties understanding such documents. This is because scientific articles contain challenging vocabulary, complicated language, and lengthy structures, which make them difficult to comprehend without prior knowledge. This working note contains the results for SimpleText task 2 (identification and explanation of difficult concepts) and SimpleText task 3 (simplification of difficult text passages). Several statistical and AI-based models were used to solve these tasks. The results of the models were assessed according to the quality of the results. In order to solve the tasks mostly autoregressive Large Language Models (LLMs) were used. In order for BLOOM and BLOOMZ to work effectively prompts with examples were needed while for GPT-3 simple command prompts were more effective. Apart from LLMs statistical and graph-based models were also used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Insufficient fundamental knowledge can inhibit reading comprehension and limit access to information. Simplifying scientific texts can serve as a helpful tool as it aims to enhance the reader's comprehension by connecting complex content to fundamental vocabulary. Hence, scientific concepts become accessible by adjusting the linguistic level. However, traditional methods of text simplification may eradicate crucial aspects of concepts and structures. Simplification aims to reduce text complexity while preserving the original information and meaning. For the CLEF 2023 SimpleText lab competition <ref type="bibr" coords="1,304.75,660.81,12.16,9.66" target="#b1">[2]</ref>[3], task 2, which seeks to identify difficult concepts and subsequently explain them, and task 3, which aims to produce a simplified version of a scientific text, were dealt with. Different models were used to execute the tasks while taking their quality of the results into account as a final way of assessment.</p><p>Nowadays, the trend for sentence simplification has shifted towards utilizing deep learning techniques, which is in line with most natural language processing (NLP) tasks. One area of research involves a so-called sequence-to-sequence-based neural network to forecast explicit edit operations to create a simplified sentence taken from the original data. Dong et al. <ref type="bibr" coords="2,420.39,172.89,12.82,9.66" target="#b3">[4]</ref> proclaim a neural Programmer-Interpreter approach as an approximation to the human ability of repetitious cognitive adaption. Later, Cumbicus-Pineda et al. <ref type="bibr" coords="2,259.85,200.19,12.82,9.66" target="#b4">[5]</ref> added a graph module that incorporated the syntactic information of sentences to facilitate detecting complex phrases during the simplification process. Meanwhile, however, it is common practice to take advantage of existing pre-trained language models such as BERT <ref type="bibr" coords="2,136.98,241.14,11.68,9.66" target="#b5">[6]</ref>, RoBERTa <ref type="bibr" coords="2,202.41,241.14,11.68,9.66" target="#b6">[7]</ref>, or GPT-3 <ref type="bibr" coords="2,264.29,241.14,11.68,9.66" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Task description</head><p>The goal of task 2 is to identify up to five difficult terms in a given sentence <ref type="bibr" coords="2,427.11,300.63,11.68,9.66" target="#b1">[2]</ref>. For each term an explanation needs to be provided <ref type="bibr" coords="2,223.80,314.28,11.68,9.66" target="#b1">[2]</ref>. Additionally an identified difficult term needs to be rated on a scale from 0 to 2 where 0 denotes a term where the meaning can be derived and 2 denotes a term where deep knowledge is needed to comprehend the term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Data description</head><p>The train dataset used for this task consists of different sentences from abstracts from high-ranked scientific publications <ref type="bibr" coords="2,174.51,393.97,11.91,9.66" target="#b1">[2,</ref><ref type="bibr" coords="2,190.67,393.97,7.94,9.66" target="#b2">3]</ref>. For each sentence a difficult term along with a difficulty rating and a definition for the term are given. The total amount of rows (i. e. sentences) for the train dataset consists of 453 rows. For solving this task a subset of 203 rows were used. The subset of the train set only contains difficulty ratings of 1 (moderately difficult term) and 2 (very difficult term).</p><p>For the test set a total amount of 152,072 sentences (i. e. rows) are given. This dataset is denoted as the large dataset. The medium dataset is a subset of this dataset and consists of 4,797 sentences and the small dataset is a subset of the medium dataset and consists of 2,234 sentences. Task 2 was solved using the medium dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Method description 2.2.1 Identifying difficult terms</head><p>For the identification of difficult terms four different models were used. Two AI models which were used are BLOOM and GPT-3. One statistical model, YAKE, and one graph-based model, TextRank, was used. No manual interventations were made for the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YAKE</head><p>YAKE is an unsupervised model which is able to automatically extract keywords in a given text based on statistical properties of the text <ref type="bibr" coords="2,233.02,633.05,11.68,9.66" target="#b8">[9]</ref>. YAKE was used through the PKE package <ref type="bibr" coords="2,451.26,633.05,16.85,9.66" target="#b9">[10]</ref>. No special parameter modification was made when using YAKE since the default parameters for the implementation via PKE were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TextRank</head><p>Just like YAKE TextRank also automatically extracts keywords in a given text <ref type="bibr" coords="2,437.23,699.09,16.85,9.66" target="#b9">[10]</ref>. In contrast to YAKE is TextRank a graph-based model <ref type="bibr" coords="2,258.28,712.74,16.52,9.66" target="#b10">[11]</ref>. TextRank was also accessed via the PKE package <ref type="bibr" coords="2,509.87,712.74,12.82,9.66" target="#b8">[9]</ref> and the default model parameters were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLOOM</head><p>BLOOM is an LLM which is trained to continue text from a prompt <ref type="bibr" coords="3,375.43,74.99,16.85,9.66" target="#b11">[12]</ref>. The prompt which was used can be found in the appendix. The examples which were used in the prompt are a small subset of the train dataset. BLOOM was used through the Hosted Inference API from Hugging Face <ref type="bibr" coords="3,470.88,102.29,18.32,9.66" target="#b12">[13]</ref> and no changes to the default parameters were made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3</head><p>GPT-3 is also an LLM and was introduced in 2020 <ref type="bibr" coords="3,320.02,154.68,16.85,9.66" target="#b13">[14]</ref>. The prompt which was used is in the appendix. The model which was used is text-davinci-003. The temperature was set to 0.7. The maximum amount of tokens is 256. The top probability was set to 1 and the frequence and presence penalty were set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.1">Rating of difficult terms</head><p>For the rating of the identified difficult terms BLOOM was used for the difficult terms generated by YAKE, TextRank and BLOOM. GPT-3 was used for rating the difficult terms generated by GPT-3.</p><p>The prompts for BLOOM and GPT-3 can be found in the appendix. The prompt used for BLOOM is based on examples from the train dataset. Since the rating only consists of a one-digit integer the maximum output length parameter was decreased for both GPT-3 and BLOOM to save tokens and computation time. Other than that the parameters for BLOOM and GPT-3 remained unchanged. Both prompts can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.2">Explanation of difficult terms</head><p>The best performing dataset of the previous subtask (extraction and rating of difficult terms) was used for all models of this subtask. In our case it was the dataset where GPT-3 was used for identifying and rating difficult terms. We evaluated the quality of the results of the different models manually by looking at a subset of the results for each model. In order to explain difficult terms four different models were used: Wikipedia, simpleT5, BLOOMZ and GPT-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia</head><p>Wikipedia is a library for Python which retrieves a summarized version of the Wikipedia definition for a given term <ref type="bibr" coords="3,131.94,478.10,16.85,9.66" target="#b14">[15]</ref>. If no entry exists for this term then no output is returned <ref type="bibr" coords="3,406.21,478.10,16.85,9.66" target="#b14">[15]</ref>. In our case a missing Wikipedia entry for a term is characterized by an output of -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>simpleT5</head><p>simpleT5 is an LLM which is based on the Transformer architecture <ref type="bibr" coords="3,389.72,530.49,16.85,9.66" target="#b15">[16]</ref>. For solving this task the pretrained model t5-base was used. The model was also trained using the test dataset. Here the first 40 rows were used for evaluation and the remaining 163 rows were used for training the model. For training the maximum token length for the input is 128 and the maximum output length is 512. The batch size was set to 8 and the model was trained for 5 epochs. The precision was set to 32. Out of the 5 generated models the model with the lowest value loss was chosen. The model which was chosen had a value loss of 2.7969.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLOOMZ</head><p>BLOOMZ is a finetuned variant of BLOOM <ref type="bibr" coords="3,275.99,651.13,16.85,9.66" target="#b16">[17]</ref>. For this task the test dataset was split in two parts.</p><p>One part consists of difficult terms which are abbreviations and the other part is the remaining dataset. The regular expression which was used to extract the abbreviations can be found in the appendix. If an extracted abbreviation already contains the deciphering such as "IoT (Internet of Things)" then there was no need to further use any model to get the deciphering of the abbreviation. For abbreviations and non-abbreviations different prompts were used which can be both found in the appendix. Just like with BLOOM the Hosted Inference API was used and except for the maximum token length which was set to 512 all parameters were set to the default values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3</head><p>For GPT-3 the dataset was also split in abbreviations and non-abbreviations using the same regular expression. If an extracted abbreviation already contains the deciphering such as "IoT (Internet of Things)" then there was no need to further use any model to get the deciphering of the abbreviation.</p><p>The prompt for both subsets can be found in the appendix. The model text-davinci-003 was used. The temperature was set to 0.7 and the maximum token length was set to 256. The top probability was set to 1 and the frequence and presence penalty were set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SimpleText task 3</head><p>The goal of task 3 is to simplify a given scientific sentence. <ref type="bibr" coords="4,363.05,199.07,12.82,9.66" target="#b0">[1]</ref> For solving this task simpleT5, BLOOMZ and GPT-3 were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Data description</head><p>The train data consists of 648 sentences from different scientific domains where there exists a simplified sentence for a given complex sentence. [1, 2, 3] Just like for task 2 there are three differently sized datasets for the test dataset which only contain the non-simplified sentences. <ref type="bibr" coords="4,490.35,278.76,12.82,9.66" target="#b0">[1]</ref> The medium-sized dataset was chosen for this task. This dataset consists of 4,797 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Method description simpleT5</head><p>The first 130 sentences of the train dataset were used to evaluate the training process and the remaining 518 sentences were used for training the simpleT5 model. For training the maximum token length of the input complex sentences was set to 512 and the maximum token length for the output simplified sentences was also set to 512. The batch size was set to 4 and amount of epochs was set to 5. The precision was set to 32. Moreover, the pretrained t5-base model was also loaded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLOOMZ</head><p>The prompt can be found in the appendix. The prompt constitutes a small subset of the train dataset. The Hosted Inference API was used and except for the maximum token length which was set to 512 all parameters were set to the default values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3</head><p>The prompt can be found in the appendix. The model which was used was text-davinci-003. The temperature was set to 0.7 and the maximum amount of tokens to 512. The top probability was set to 1 and the frequency and presence penalty were set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The performance of each model was assessed in comparison to each other. For this purpose, task 2 and 3 were ranked individually. The performance assessment was executed manually and subjectively by analysing the data by carried out spot checks.</p><p>The following ranking was established (from highest performance to lowest performance: Task 2: 1. GPT 2. BLOOMZ 3. ST5 4. WIKI Task 3:</p><p>1. GPT 2. BLOOMZ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ST5</head><p>Regarding Task 2 of the CLEF 2023 SimpleText lab competition, the most effective models for identifying difficult concepts and explaining them were GPT, which provided the best definitions that were easy to understand at a good level, followed by BLOOMZ, which provided good definitions but were not as good as GPT. ST5 sometimes paraphrased without simplification and also defined non-complex expressions, while WIKI displayed a lot of gaps in providing definition and simplification, resulting in a score of -1.</p><p>As for Task 3, the top-performing models for producing a simplified version of a scientific text were GPT, which provided good simplifications of the text, followed by BLOOMZ, which often maintained the original wording but sometimes provided a simplified version. ST5, on the other hand, often retained the exact same expression as before and did not provide a simplified version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The SimpleText Subtasks 2 and 3 aim to identify complex expressions and in a next step provide a simplified version of identified target word and its linguistic surrounding. However, as it became evident throughout the execution of the task the models at hand displayed a different level of performative quality. This is precipitated in the lack of precision when outlining difficult terms and subsequently simplifying them. Future research should aim to focus on the precision with which a potential AI model circles out the target expression. Along these lines, co-text related aspects that impact the specific denotation of an expression needs to be considered (e.g., acronyms that can represent multiple technical expression). Additionally, future models need to have enough explanatory data at hand that truely simplifies the target expressions without only rephrasing it.</p><p>In conclusion, future work should emphasise on precision and recall when analysing and training models. openai.api_key = "..." response = openai.Completion.create( model="text-davinci-003", prompt="Return difficult terms from the following text:\n" + input +\ "\nDifficult terms: ", temperature=0. try: p = "The abbreviation \"" + term + "\" occurs in this text:\n\"" + text + "\"\nWhat does \"" + term + "\" stand for?" output = query({ "inputs": p, "parameters": {"max_new_tokens": 512} }) s=re.split(r"\" stand for?",output <ref type="bibr" coords="8,284.42,125.58,4.31,9.66">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,456.75,202.84,9.66;5,72.00,469.40,450.56,9.66;5,72.00,482.05,450.91,9.66;5,72.00,494.70,248.77,9.66;5,72.00,507.35,194.55,9.66;5,72.00,520.00,11.61,9.66;5,72.00,532.65,450.73,9.66;5,72.00,545.30,451.00,9.66;5,72.00,557.95,451.08,9.66;5,72.00,570.60,171.31,9.66;5,72.00,583.24,221.12,9.66;5,72.00,595.89,11.61,9.66;5,72.00,608.54,450.65,9.66;5,72.00,621.19,451.19,9.66;5,72.00,633.84,44.87,9.66;5,72.00,646.49,282.37,9.66;5,72.00,659.14,11.61,9.66;5,72.00,671.79,450.99,9.66;5,72.00,684.44,382.51,9.66;5,72.00,697.08,297.44,9.66;5,72.00,709.73,11.61,9.66;5,72.00,722.38,451.21,9.66;5,72.00,735.03,119.97,9.66;5,72.00,747.68,194.90,9.66;6,72.00,74.99,13.04,9.66;6,72.00,100.28,175.82,9.66;6,108.00,112.93,15.27,9.66;6,144.00,125.58,227.75,9.66;6,144.00,138.23,73.75,9.66;6,144.00,150.88,47.15,9.66;6,144.00,163.53,8.94,9.66;6,144.00,176.18,216.03,9.66;6,72.00,188.83,153.23,9.66;6,144.00,201.48,33.27,9.66;6,108.00,214.12,98.58,9.66;6,144.00,226.77,33.88,9.66;6,72.00,264.72,191.45,9.66;6,72.00,277.37,63.80,9.66"><head>BLOOM prompt for finding</head><label></label><figDesc>difficult terms prompt = "Text: The proposed algorithm and the results presented in this paper are first step towards conducting a systematic analysis of real-coded EDAs and towards developing a design theory for development of scalable and robust real-coded EDAs.\n\ Difficult terms (up to 5): real-coded EDA\n\ \n\ Text: Our results suggest that the hyperplane tiles improve the generalization capabilities of the tile coding approximator: in the hyperplane tile coding broad generalizations over the problem space result only in a soft degradation of the performance, whereas in the usual tile coding they might dramatically affect the performance.\n\ Difficult terms (up to 5): hyperplane tile coding\n\ \n\ Text: Recently, Lu and Cao proposed a new simple three-party key exchange S-3PAKE protocol and claimed that it is not only very simple and efficient, but also can survive against various known attacks.\n\ Difficult terms (up to 5): three-party key exchange, S-3PAKE\n\ \n\ Text: A proof-of-concept implementation gathers network management system data and exposes abstract maps through the Application-Layer Traffic Optimization (ALTO) protocol.\n\ Difficult terms (up to 5): Application-Layer Traffic Optimization\n\ \n\ Text: Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware.\n\ Difficult terms (up to 5): MATLAB, ROS\n\ \n" def generate_from_prompt(prompt,snt): try: p=prompt+'Text: '+snt+'\nDifficult terms (up to 5): ' output = query({ "inputs": p }) s=re.split(r'Difficult terms \(up to 5\): ',output[0]\ ['generated_text'],re.DOTALL)[-1] return s except Exception as e: return e GPT-3 prompt for finding difficult terms def gpt(input):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,125.58,451.27,642.11"><head></head><label></label><figDesc>In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot.\n\ simplified text: Researchers propose data-driven solutions allowing drones to autonomously navigate city streets, learning to fly by imitating an expert pilot.\n\ \n\ original text: We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments.\n\ simplified text: Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments.\n\ \n\ original text: It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the property-based optimization of compound selection and library design processes, thus improving the odds of success in lead discovery projects.\n\ simplified text: It is suggested that the analysis could be used as an additional tool for the optimization of compound selection and library design processes.\n\ \n\ original text: We argue that misinformation and disinformation are related yet distinct sub-categories of information.\n\ simplified text: We argue that misinformation and disinformation are related but distinct sub-categories of information.\n\ \n\ original text: Given the importance of nutrition in optimizing athletic performance, there is a concern about the effects of IFast on athletics.\n\ simplified text: Nutrition is important for the optimization of sport performance so there is a concern about the effects of intermittent fasting on performance.\n\ \n"</figDesc><table coords="8,72.00,125.58,451.27,642.11"><row><cell cols="2">presence_penalty=0</cell></row><row><cell>)</cell><cell></cell></row><row><cell cols="2">return response</cell></row><row><cell></cell><cell>0]\</cell></row><row><cell cols="2">['generated_text'],re.DOTALL)[-1][2:] BLOOMZ prompt for simplifying sentences</cell></row><row><cell>prompt = "\</cell><cell>return s</cell></row><row><cell cols="3">except Exception as e: return e BLOOMZ prompt for explaining non-abbreviations def bloomz_non_abbrevs_without_text(term): try: p = "Explain the following term in one to two sentences: \"" + output = query({ "inputs": p, "parameters": {"max_new_tokens": 512} }) s=output[0]['generated_text'][len(p):] return s return output except Exception as e: return e GPT-3 prompt for explaining abbreviations def gpt(input): openai.api_key = "..." response = openai.Completion.create( model="text-davinci-003", prompt="What does this abbreviation stand for?\n" + input + "\nReturn only the result.\n\n", term + "\"." temperature=0.7, max_tokens=256, original text: def bloomz(prompt, text):</cell></row><row><cell cols="2">top_p=1, try:</cell></row><row><cell cols="2">frequency_penalty=0, p = prompt + "original text: " + text + "\nsimplified text:"</cell></row><row><cell cols="2">presence_penalty=0 output = query({</cell></row><row><cell>)</cell><cell>"inputs": p,</cell></row><row><cell cols="2">return response "parameters": {"max_new_tokens": 512}</cell></row><row><cell></cell><cell>})</cell></row><row><cell></cell><cell>s=output[0]['generated_text'][len(p):]</cell></row><row><cell cols="2">GPT-3 prompt for explaining non-abbreviations return s</cell></row><row><cell cols="2">def gpt(input): except Exception as e:</cell></row><row><cell></cell><cell>return e</cell></row><row><cell cols="2">openai.api_key = "..."</cell></row><row><cell cols="2">response = openai.Completion.create(</cell></row><row><cell cols="2">model="text-davinci-003",</cell></row><row><cell cols="2">prompt="Return a short definition of the following term: " + input +</cell><cell>"\n\n",</cell></row><row><cell cols="2">temperature=0.7,</cell></row><row><cell cols="2">max_tokens=256,</cell></row><row><cell cols="2">top_p=1,</cell></row><row><cell cols="2">frequency_penalty=0,</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,89.07,104.06,433.89,9.66;10,72.00,117.71,451.08,9.66;10,72.00,131.36,450.69,9.66;10,72.00,145.01,450.57,9.66;10,72.00,158.67,451.04,9.66;10,72.00,172.31,333.97,9.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,137.62,117.71,385.47,9.66;10,72.00,131.36,22.00,9.66">Overview of SimpleText -CLEF-2023 track on Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,104.31,158.67,418.74,9.66;10,72.00,172.31,272.29,9.66">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,89.07,193.97,433.89,9.66;10,72.00,207.62,450.85,9.66;10,72.00,221.27,451.00,9.66;10,72.00,234.92,450.94,9.66;10,72.00,248.57,304.30,9.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,142.12,207.62,380.74,9.66;10,72.00,221.27,23.78,9.66">CLEF 2023 SimpleText Track: What Happens if General Users Search Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-28241-6_62" />
	</analytic>
	<monogr>
		<title level="m" coord="10,118.18,221.27,404.81,9.66;10,72.00,234.92,52.07,9.66;10,306.08,234.92,97.48,9.66">Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023</title>
		<meeting><address><addrLine>Dublin, Ireland; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2023-04-02">2023. April 2-6, 2023</date>
			<biblScope unit="page" from="536" to="545" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct coords="10,92.07,270.22,430.72,9.66;10,72.00,283.87,450.54,9.66;10,72.00,297.52,450.95,9.66;10,72.00,311.17,450.73,9.66;10,72.00,324.82,450.82,9.66;10,72.00,338.47,304.30,9.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,451.11,283.87,71.43,9.66;10,72.00,297.52,329.33,9.66">Overview of the CLEF 2022 SimpleText Lab: Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sílvia</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radia</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elise</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-13643-6_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-13643-6_28" />
	</analytic>
	<monogr>
		<title level="m" coord="10,420.38,297.52,102.57,9.66;10,72.00,311.17,450.73,9.66;10,72.00,324.82,108.31,9.66">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2022-09-05">2022. September 5-8, 2022</date>
			<biblScope unit="page" from="470" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,95.07,360.12,427.48,9.66;10,72.00,373.77,451.24,9.66;10,72.00,387.42,451.07,9.66;10,72.00,401.07,152.34,9.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,428.99,360.12,93.55,9.66;10,72.00,373.77,369.86,9.66">Editnts: An neural programmer-interpreter model for sentence simplification through explicit editing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.11,373.77,56.13,9.66;10,72.00,387.42,451.07,9.66;10,72.00,401.07,48.55,9.66">ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3393" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.32,422.72,434.90,9.66;10,72.00,436.37,451.06,9.66;10,72.00,450.02,125.50,9.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,351.75,422.72,171.46,9.66;10,72.00,436.37,85.76,9.66">A Syntax-Aware Edit-based System for Text Simplification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Cumbicus-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,172.17,436.37,346.31,9.66">International Conference Recent Advances in Natural Language Processing</title>
		<imprint>
			<publisher>RANLP</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="324" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.07,471.67,433.70,9.66;10,72.00,485.32,221.83,9.66" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,337.78,471.67,184.98,9.66;10,72.00,485.32,184.87,9.66">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.57,506.97,390.33,9.66" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,150.13,506.97,272.01,9.66">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.82,528.62,432.86,9.66;10,72.00,542.27,117.26,9.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,187.79,528.62,175.36,9.66">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,375.63,528.62,147.05,9.66;10,72.00,542.27,84.85,9.66">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.07,563.92,434.05,9.66;10,72.00,577.57,450.67,9.66;10,72.00,591.22,87.02,9.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,447.17,563.92,75.95,9.66;10,72.00,577.57,276.43,9.66">YAKE! Keyword extraction from single documents using multiple local features</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,359.66,577.57,100.42,9.66">Informaration Sciences</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="257" to="289" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.06,612.87,429.85,9.66;10,72.00,626.52,451.13,9.66;10,72.00,640.17,289.60,9.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,199.65,612.87,273.84,9.66">pke: an open source python-based keyphrase extraction toolkit</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Boudin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-2015" />
	</analytic>
	<monogr>
		<title level="m" coord="10,480.17,612.87,42.74,9.66;10,72.00,626.52,412.59,9.66">Presented at the 26th International Conference on Computational Linguistics: System Demonstrations</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.40,661.82,426.67,9.66;10,72.00,675.47,450.68,9.66;10,72.00,689.13,201.59,9.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,238.79,661.82,170.52,9.66">TextRank: Bringing Order into Text</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-3252" />
	</analytic>
	<monogr>
		<title level="m" coord="10,438.36,661.82,84.72,9.66;10,72.00,675.47,298.59,9.66">Proc. of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2004 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2004-07">Jul. 2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.06,710.78,411.28,9.66" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,176.57,710.78,322.17,9.66">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Scao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,100.56,732.43,422.26,9.66;10,72.00,746.08,107.44,9.66" xml:id="b12">
	<monogr>
		<ptr target="https://huggingface.co/docs/api-inference/index" />
		<title level="m" coord="10,105.71,732.43,193.22,9.66">Hosted Inference API&quot; Hugging Face</title>
		<imprint>
			<date type="published" when="2022">Mai 14, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.06,74.99,275.43,9.66" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,182.17,74.99,181.51,9.66">Language Models are Few-Shot Learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,85.74,96.64,375.29,9.66" xml:id="b14">
	<monogr>
		<ptr target="https://pypi.org/project/wikipedia/" />
		<title level="m" coord="11,97.88,96.64,68.54,9.66">wikipedia 1.4.0</title>
		<imprint>
			<date type="published" when="2022">Mai 14, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,95.31,118.29,427.66,9.66;11,72.00,131.94,450.60,9.66;11,72.00,145.59,450.61,9.66;11,72.00,159.24,194.16,9.66" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,185.46,118.29,315.22,9.66">mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Linting</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.naacl-main.41" />
	</analytic>
	<monogr>
		<title level="m" coord="11,72.00,131.94,450.60,9.66;11,72.00,145.59,212.59,9.66">Proc. of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021-06">Jun. 2021</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.06,180.89,365.84,9.66" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="11,198.56,180.89,255.57,9.66">Crosslingual Generalization through Multitask Finetuning</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
