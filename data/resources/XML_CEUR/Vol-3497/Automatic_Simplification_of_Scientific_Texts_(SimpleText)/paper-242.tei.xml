<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,386.16,15.42;1,89.29,106.66,408.18,15.42;1,89.29,128.58,197.34,15.43">Automatic Simplification of Scientific Texts using Pre-trained Language Models: A Comparative Study at CLEF Symposium 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,156.89,63.97,11.96"><forename type="first">Aftab</forename><surname>Anjum</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Christian-Albrechts-Universität zu Kiel</orgName>
								<address>
									<addrLine>Christian-Albrechts-Platz 4</addrLine>
									<postCode>24118</postCode>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.01,156.89,92.86,11.96"><forename type="first">Nikolaus</forename><surname>Lieberum</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Christian-Albrechts-Universität zu Kiel</orgName>
								<address>
									<addrLine>Christian-Albrechts-Platz 4</addrLine>
									<postCode>24118</postCode>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,386.16,15.42;1,89.29,106.66,408.18,15.42;1,89.29,128.58,197.34,15.43">Automatic Simplification of Scientific Texts using Pre-trained Language Models: A Comparative Study at CLEF Symposium 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CD3588F6E5D15E6F573B853137A6F1FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Simplification</term>
					<term>Natural Language Processing</term>
					<term>Difficult Term Extraction</term>
					<term>Difficult Term Explanation</term>
					<term>Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The complexity of scientific texts often creates a barrier to understanding for non-specialist readers. This barrier inhibits the democratization of knowledge and prevents the wider public from engaging with scientific discourse. We argue that applying artificial intelligence to the tasks of identifying and explaining difficult concepts (Complexity Spotting), and simplifying scientific text, has the potential to democratize access to scientific knowledge. We investigate a range of cutting-edge deep learning models for their efficacy in these tasks. The models are trained and evaluated on a dataset of scientific articles, annotated for complex concepts and their simpler explanations. We present a comparative analysis of the performance of these models, illuminating the strengths and weaknesses of each. Our findings reveal promising avenues for future research and development in the field of automated text simplification, contributing to the broader goal of making scientific knowledge accessible to all.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scientific texts are known for their complexity, technical jargon, and specialized vocabulary, which can often pose challenges for a wide range of readers. Researchers, scientists, and students alike often struggle to comprehend and digest the content of scientific papers, making it difficult for them to stay up-to-date with the latest advancements in their respective fields. To bridge this gap and promote accessibility of scientific information, there is a growing need for automatic simplification techniques that can transform intricate scientific texts into more comprehensible versions without compromising the integrity and accuracy of the original content. In recent years, significant progress has been made in natural language processing (NLP) and machine learning, enabling the development of various text simplification techniques. One such technique, called SimpleText, focuses specifically on the automatic simplification of scientific texts. SimpleText aims to address the linguistic and structural complexities present in scientific writing while preserving the essential scientific concepts and ensuring the accuracy of the simplified content. However, the unique characteristics of scientific texts present additional challenges for automatic simplification. Scientific texts often contain domain-specific terminology, complex sentence structures, and intricate logical reasoning, which require a deep understanding of the underlying concepts. Existing text simplification approaches, designed for general-purpose texts, may not adequately capture the nuanced relationships and meaning in scientific content.</p><p>For the Blended Intensive Program (BIP) Artificial Intelligence (AI) for Humanities: from Text Simplification to Automatic Humor Analysis, we explore the application of advanced deep learning models namely AI21, ST5, and BLOOM to address the challenges of text simplification. These models, each with their unique capabilities, are applied to two interconnected tasks: 'Complexity Spotting, ' where the objective is to identify and explain difficult concepts in scientific texts, and 'Text Simplification,' where the aim is to convert complex scientific sentences into simpler ones that are easier for a general audience to understand.</p><p>The overarching aim of this paper is to investigate the efficacy of these deep learning models in simplifying scientific texts and spotting complex concepts. By doing so, we hope to contribute valuable insights to the field of automated text simplification, ultimately promoting the wider accessibility and understanding of scientific knowledge.</p><p>The structure of the paper is as follows: Section 2 provides a concise overview of the related work in the field. Section 3 presents the experiments conducted in this study, including detailed descriptions of the tasks, attributes of the dataset used, a discussion on the utilization of existing models, and a comprehensive analysis of the obtained results. Finally, Section 4 concludes the paper by summarizing the key findings and highlighting the significance of the advancements made. It also addresses the future directions and potential strategies for further improving the performance of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatic text simplification is a research area that focuses on developing computational methods to simplify complex texts and make them more accessible to a wider audience, including individuals with cognitive or linguistic challenges, non-native speakers, or people with low literacy levels. This field combines techniques from natural language processing (NLP), machine learning, and linguistics to analyze and modify the structure, vocabulary, and syntax of texts.</p><p>There has been significant research and development in automatic text simplification, aiming to create algorithms and models that can effectively simplify texts while preserving their meaning. Some common approaches i am doing to discuss here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lexical Based automatic text simplification</head><p>Lexical-based automatic text simplification is an approach that focuses on simplifying texts by replacing complex words or phrases with simpler alternatives while preserving the overall meaning. This technique leverages lexical resources, such as dictionaries, thesauri, and word frequency lists, to identify and substitute complex terms with simpler equivalents.</p><p>The research community has made notable strides in the domain of lexical-based automatic text simplification. For instance, Ciprian-Octavian and Andrei-Ionut Stan <ref type="bibr" coords="2,421.18,655.99,12.93,10.91" target="#b0">[1]</ref> introduces Sim-pLex, a lexical text simplification architecture designed to automatically simplify complex texts.</p><p>The architecture focuses on lexical substitutions, where complex words or phrases are replaced with simpler alternatives. SimpLex leverages a combination of linguistic resources, such as WordNet and SimpleWiki, along with machine learning techniques to identify suitable substitutions. The system is evaluated on a large corpus of news articles and achieves significant improvements in readability while preserving essential content. The research demonstrates the effectiveness of a lexical-based approach to text simplification and provides a valuable resource for improving the accessibility of written texts for a wider range of readers.</p><p>An other work Proposed Debabrata and Tambe <ref type="bibr" coords="3,307.77,181.81,12.68,10.91" target="#b1">[2]</ref> where they used lexical for text simplification approach using WordNet. The authors propose a method that identifies complex words in a given text and replaces them with simpler synonyms from WordNet. The approach involves measuring semantic relatedness between words and selecting the most suitable substitution based on a combination of contextual and lexical cues. Evaluation results demonstrate the effectiveness of the proposed method in improving the readability of complex texts while preserving the core meaning. The research contributes to the field of text simplification by providing a valuable and accessible solution, leveraging the rich lexical information offered by WordNet to automatically simplify complex vocabulary.</p><p>In a related context, Jipeng Qiang <ref type="bibr" coords="3,248.03,303.75,12.68,10.91" target="#b2">[3]</ref> introduces LSBert, a lexical simplification method based on BERT, a popular language representation model. The authors propose an approach that leverages BERT's contextualized embeddings to generate simplified versions of complex words or phrases. LSBert employs a two-step process: first, it identifies complex words in the input text, and then it generates simpler alternatives by selecting candidate substitutions based on their semantic similarity to the original word. The simplification is performed by fine-tuning BERT on a large corpus of simplified pairs. Evaluation results demonstrate the effectiveness of LSBert in simplifying complex vocabulary while maintaining the overall coherence and meaning of the text. This research contributes to the field by harnessing the power of BERT in lexical simplification tasks and offers a promising solution for enhancing the accessibility and understandability of written content.</p><p>Moreover, in 2019 Sanja and Horacio <ref type="bibr" coords="3,269.26,452.79,12.95,10.91" target="#b3">[4]</ref> focuses on improving the lexical coverage of text simplification systems specifically designed for the Spanish language. The authors address the challenge of limited lexical resources available for Spanish text simplification by proposing a method that combines rule-based strategies and machine learning techniques. They leverage existing resources, such as WordNet and specialized corpora, to build a comprehensive lexicon specifically tailored for Spanish text simplification. The proposed method effectively identifies complex lexical items and suggests appropriate substitutions. Evaluation results demonstrate that the enhanced lexical coverage significantly improves the performance of Spanish text simplification systems, leading to more accurate and effective simplifications. This research provides a valuable contribution to the field by addressing the lexical challenges specific to the Spanish language and enhancing the accessibility and understandability of Spanish texts for a wider audience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Based automatic text simplification</head><p>Semantic-based automatic text simplification is an approach that aims to simplify texts while preserving their underlying meaning and intention. By leveraging semantic analysis, this technique identifies complex linguistic structures and replaces them with simpler alternatives, enhancing the accessibility of the text for a broader readership.</p><p>Numerous research studies have delved into the realm of semantic-based automatic text simplification, yielding valuable contributions to the field. For instance, Elior Sulem presents a novel approach <ref type="bibr" coords="4,160.34,141.16,12.83,10.91" target="#b4">[5]</ref> to evaluate the effectiveness of text simplification techniques. The authors propose a framework that combines semantic analysis and structural evaluation to assess the quality of simplified texts. The framework considers both the preservation of the original meaning and the improvement in readability. The authors conduct experiments on a large dataset of simplified texts and demonstrate that their approach outperforms existing evaluation methods in capturing both semantic and structural changes. The findings of this study contribute to the development of more accurate and reliable evaluation metrics for text simplification systems, ultimately leading to improved accessibility and comprehension for diverse readers.</p><p>In another study by Sanja and goran paper proposes a novel approach <ref type="bibr" coords="4,428.80,249.56,12.99,10.91" target="#b5">[6]</ref> to automated text simplification by leveraging event-based semantics. The authors recognize that complex sentence structures pose significant challenges to comprehension, especially for individuals with limited language proficiency. To address this, they introduce a method that focuses on identifying key events and their participants in a sentence. By simplifying sentence structures while preserving the fundamental meaning conveyed by these events, the proposed approach aims to improve the accessibility and understandability of complex texts. Evaluation results demonstrate promising performance, with the method successfully simplifying sentences while maintaining their semantic coherence and preserving critical information. This research provides valuable insights into the use of event-based semantics for text simplification, offering a potentially effective solution to enhance the accessibility of complex texts for various user groups.</p><p>Similarly, Shuming and Xu Sun Introduces a method <ref type="bibr" coords="4,330.16,398.60,12.68,10.91" target="#b6">[7]</ref> that leverages event-based semantics for automated text simplification. The authors propose an approach that focuses on simplifying complex sentences by representing their semantic structure in terms of events and their participants. By identifying the main event and its semantic roles, the method generates simpler versions of the sentences while maintaining the core meaning. Evaluation results demonstrate that the proposed approach effectively simplifies complex sentences while preserving semantic coherence. This research contributes to the field by providing a novel perspective on text simplification, emphasizing the importance of event-based semantics in simplifying complex texts and making them more accessible to a wider range of readers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformer models for Automatic text simplification</head><p>Transformer models have emerged as powerful tools for automatic text simplification, offering state-of-the-art performance in various natural language processing tasks. In the context of text simplification, transformer models have been widely applied and have shown promising results. Some of the study in this domain i will put here. In 2018, Sanqiang and Rui Meng <ref type="bibr" coords="4,89.29,610.92,12.82,10.91" target="#b7">[8]</ref> proposes an approach for sentence simplification that integrates Transformer models with paraphrase rules. The authors acknowledge the challenges of simplifying sentences while maintaining their meaning and grammatical correctness. To address this, they combine the power of Transformer models, known for their ability to learn contextual representations, with manually curated paraphrase rules. The method involves generating multiple simplified versions of a source sentence using the Transformer model and then applying the paraphrase rules to ensure simplicity and coherence. Evaluation results demonstrate that the proposed approach outperforms existing methods in terms of simplicity and grammaticality. This research contributes to the field by presenting a comprehensive approach that combines the strengths of Transformer models and human-created paraphrase rules, offering a promising solution for sentence simplification.</p><p>Similarly, Robert-Mihai proposed a study <ref type="bibr" coords="5,280.72,168.26,12.68,10.91" target="#b8">[9]</ref> where they explores the application of sequenceto-sequence (Seq2Seq) models for automated text simplification. The authors recognize the importance of enhancing the accessibility of complex texts for individuals with lower reading abilities. To address this, they propose a method where a Seq2Seq model is trained to generate simplified versions of input sentences. The model is trained on a large dataset of sentence pairs, consisting of complex and simplified versions. By learning to map complex sentences to simpler equivalents, the Seq2Seq model offers a promising solution for text simplification. Evaluation results demonstrate that the proposed approach significantly improves readability while maintaining the core meaning of the original text. This research contributes to the field by showcasing the effectiveness of Seq2Seq models for automated text simplification, highlighting their potential to make complex texts more understandable and inclusive.</p><p>Moreover, Takumi Maruyama <ref type="bibr" coords="5,234.39,317.30,17.85,10.91" target="#b9">[10]</ref> focuses on the challenging problem of text simplification for languages with extremely low resources. The authors propose an approach that leverages pre-trained Transformer-based language models, such as BERT, to overcome the limitations of data scarcity. By fine-tuning these models on small amounts of labeled simplification data, they are able to generate simplified versions of complex sentences. Evaluation results demonstrate the effectiveness of this approach, with the generated simplifications achieving high levels of simplicity and readability. The research demonstrates the potential of pre-trained Transformer models to address low resource scenarios, opening up possibilities for text simplification in languages with limited available data. This work contributes to the field by providing insights into adapting large-scale language models for low resource text simplification, making it a valuable contribution for improving the accessibility of complex texts in low resource language contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task description for CLEF (2023) Simple-Text</head><p>The Simple-Text data-set and benchmarks contribute to the research on automatic text simplification by introducing three interconnected tasks.</p><p>Task 1: What is in (or out)? Select passages to include in a simplified summary, given a query.</p><p>Task 2: What is unclear? Given a passage and a query, rank terms/concepts that are required to be explained for understanding this passage (definitions, context, applications,..).</p><p>Task 3: Rewrite this! Given a query, simplify passages from scientific abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>In our research, we leverage the DBLP abstracts corpus as the main source of our data. Specifically, we utilize the Citation Network Dataset, known as DBLP+Citation, which is the 12th version released in 2020. This dataset consists of a vast collection of 4,894,063 scientific articles. By using this comprehensive corpus, we ensure a diverse and extensive range of scholarly publications for our analysis and experimentation. The DBLP+Citation dataset serves as a valuable resource, providing us with the necessary information and content to investigate various research questions and explore the intricacies of scientific articles in our study.</p><p>For the task of Complexity Spotting, we rely on an annotated database that includes the following columns: query_id (e.g., G11.1), query_text (e.g., drones), snt_id (e.g., G11.1_2892036907_1), source_snt, term (e.g., autonomous), difficulty, and definition. This database serves as a comprehensive resource for training and evaluating our models, providing diverse examples of complex terms along with their definitions and levels of difficulty.</p><p>For the Text Simplification task, we utilize another dataset which includes the columns: query_id, query_text, doc_id, snt_id, source_snt, and simplified_snt. This dataset offers a variety of complex sentences from scientific texts alongside their simplified versions, offering a strong foundation for the evaluation and improvement of our models' simplification capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Utilization of Existing Models</head><p>In the following, the approach used for the two tasks involved in this research will be illustrated. Detailed procedures for data gathering and subsequent preprocessing are covered, along with the process for extracting features. Moreover, we elucidate the choice and preparation of the machine learning models, coupled with the performance evaluation metrics utilized for its assessment.</p><p>SimpleT5 Simple T5 is a model built on top of PyTorch Lightning and Transformers. It allows users to quickly train their T5 models, including T5, mT5, and byT5 models, with only a few lines of code <ref type="bibr" coords="6,177.56,468.89,16.58,10.91" target="#b9">[10]</ref>. The T5 models, which can be trained using SimpleT5, are versatile and can be used for a variety of natural language processing (NLP) tasks. These tasks include summarization, question answering (QA), question generation (QG), translation, text generation, and more <ref type="bibr" coords="6,134.13,509.54,16.25,10.91" target="#b10">[11]</ref>.</p><p>AI21 Labs -Jurassic-2 Grande Instruct The J2-Grande-Instruct model is a variation of the Jurassic-2 series developed by AI21. It is an auto-regressive language model based on the Transformer architecture and designed with modifications for improved efficiency. The models diverge from their GPT-3 counterparts in several aspects, including vocabulary size and the depth/width ratio of the neural net. <ref type="bibr" coords="6,290.17,577.29,18.07,10.91" target="#b11">[12]</ref> This model is specifically trained to handle instructions-only prompts, also known as "zero-shot" prompts, without the need for examples or "few-shot" prompts. It aims to provide a natural way to interact with large language models and is designed to give users an idea of the optimal output for their task without needing any examples.</p><p>BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) The BLOOM model is an autoregressive Large Language Model (LLM) that leverages a decoder-only transformer architecture, derived from Megatron-LM GPT-2. It underwent training on approximately 366 billion tokens between March and July 2022, utilizing 1.6 Terabytes of preprocessed text. This extensive dataset included 350 billion unique tokens, encompassing 46 natural languages and 13 programming languages, enabling BLOOM to grasp a wide range of linguistic and programming contexts <ref type="bibr" coords="7,256.46,141.16,16.25,10.91" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results Analysis</head><p>In this section, we present a detailed analysis of the results obtained from our experiments. We begin by providing an overview of the experimental setup and methodology employed for the evaluation. Subsequently, we delve into the quantitative analysis of the performance metrics, followed by a qualitative assessment of the generated outputs.</p><p>For Bloom, AI21 and T5, the training processes of these models involve large-scale language modeling, leveraging vast amounts of text data. Bloom utilizes a combination of unsupervised and supervised training techniques, incorporating linguistic knowledge and fine-tuning on specific downstream tasks. AI21 adopts a similar approach, employing a Transformer-based architecture and training on a diverse dataset. T5, on the other hand, employs a unified framework that incorporates both supervised and unsupervised learning, enabling it to perform multiple tasks. These pre-trained models can be utilized by fine-tuning them on specific downstream tasks, such as text classification, summarization, or question-answering. By adapting the pretrained models to target tasks, researchers and practitioners can benefit from their powerful language understanding capabilities and achieve improved performance in a range of NLP applications.</p><p>This study we did not optimised the hyper parameters of the above utilized models for simple tasks. we used the default parameters for Task 2.2 and Task 2.1. we focused on utilizing pre-trained language models without performing fine-tuning. The models, including Bloom, AI21, and T5, were accessed through their respective APIs to obtain results for the tasks at hand. Specifically, Table <ref type="table" coords="7,201.34,448.32,5.17,10.91" target="#tab_0">1</ref> and Table <ref type="table" coords="7,256.09,448.32,5.17,10.91" target="#tab_1">2</ref> present the performance of the T5 model for Task 2.2. Similarly, for Task 2.1, no fine-tuning was conducted, and the pre-trained models were utilized as is. Upon analyzing the provided tables (1, 2, 3, 4), it becomes evident that the achieved performance is not particularly high. This can be attributed to the fact that the models used are not specifically tailored to the domain of the study. However, it is important to note that by undertaking the process of fine-tuning the models with our specific training data, the results are anticipated to exhibit significant improvements. Fine-tuning the models to align with the domain-specific requirements of the study would enhance their performance and generate more favorable outcomes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The comprehension of scientific texts can be challenging for non-specialist readers, acting as a barrier that restricts access to scientific knowledge and inhibits public engagement in scientific discussions. To address this issue and foster the democratization of scientific information, we propose leveraging artificial intelligence techniques for identifying and explaining complex concepts (referred to as Complexity Spotting) and simplifying scientific texts. This study investigates state-of-the-art deep learning models for their effectiveness in these tasks. We train and evaluate the models using a data-set of scientific articles that have been annotated to identify complex concepts and their corresponding simpler explanations. Through a comparative analysis, we provide insights into the strengths and weaknesses of each model's performance. Our findings highlight promising opportunities for future research and development in automated text simplification, which contributes to the overarching objective of making scientific knowledge more accessible to a wider audience.</p><p>In this study, we did not perform fine-tuning on the large language models such as T5, Bloom, and AI21. Despite this, the performance of the models remained reasonable. For our future investigations, we plan to conduct thorough data exploration and analysis as a preliminary step. Subsequently, we intend to fine-tune these models using the provided dataset. Given that these models have already been trained on extensive data, fine-tuning can be achieved with a smaller amount of additional data. Additionally, these models exhibit some capability in handling data imbalance, albeit to a certain extent, if the dataset is not heavily skewed. By pursuing these steps, we aim to further enhance the performance and applicability of the models in the context</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.98,583.28,288.97,57.92"><head>Table 1</head><label>1</label><figDesc>Accuracy score on Test Data set of Task 2 (2.2).</figDesc><table coords="7,217.32,614.89,160.63,26.30"><row><cell cols="4">Model total BLEU FKGL SARI</cell></row><row><cell>ST5</cell><cell>245</cell><cell>0.21</cell><cell>12.77 27.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.98,90.49,288.97,57.92"><head>Table 2</head><label>2</label><figDesc>Accuracy score on Test Data set of Task 2 (2.2).</figDesc><table coords="8,217.32,122.10,160.63,26.30"><row><cell cols="4">Model total BLEU FKGL SARI</cell></row><row><cell>ST5</cell><cell>648</cell><cell>0.60</cell><cell>12.30 65.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.98,170.66,432.83,81.83"><head>Table 3</head><label>3</label><figDesc>Accuracy score on Test Data set of Task 2 (2.1).</figDesc><table coords="8,95.27,202.28,426.55,50.21"><row><cell>Model</cell><cell cols="6">total BLEU ROUGE_precision ROUGE_recall ROUGE_fmeasure semantic_match</cell></row><row><cell>AI21</cell><cell>18</cell><cell>0.07</cell><cell>0.31</cell><cell>0.39</cell><cell>0.32</cell><cell>0.79</cell></row><row><cell>BLOOM</cell><cell>9</cell><cell>0.13</cell><cell>0.29</cell><cell>0.28</cell><cell>0.28</cell><cell>0.53</cell></row><row><cell>ST5</cell><cell>197</cell><cell>0.03</cell><cell>0.27</cell><cell>0.21</cell><cell>0.22</cell><cell>0.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.98,274.74,250.91,81.83"><head>Table 4</head><label>4</label><figDesc>Accuracy score on Test Data set of Task 2 (2.1).</figDesc><table coords="8,255.39,306.36,84.50,50.21"><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Ai21</cell><cell>0.43</cell></row><row><cell>BLOOM</cell><cell>0.46</cell></row><row><cell>ST5</cell><cell>0.80</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>of scientific text simplification.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,145.59,394.53,10.91;9,112.66,159.14,255.75,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,284.80,145.59,218.03,10.91">Simplex: a lexical text simplification architecture</title>
		<author>
			<persName coords=""><forename type="first">C.-O</forename><surname>Truică</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-I</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E.-S</forename><surname>Apostol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.66,159.14,161.67,10.91">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6265" to="6280" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,172.69,393.33,10.91;9,112.66,186.24,393.33,10.91;9,112.66,199.79,394.53,10.91;9,112.66,213.34,163.85,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,394.34,172.69,111.65,10.91;9,112.66,186.24,64.27,10.91">Lexical text simplification using wordnet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ballal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dolase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rajmane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,208.22,186.24,297.77,10.91;9,112.66,199.79,114.98,10.91">Advances in Computing and Data Sciences: Third International Conference, ICACDS 2019</title>
		<meeting><address><addrLine>Ghaziabad, India</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">April 12-13, 2019. 2019</date>
			<biblScope unit="page" from="114" to="122" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers. Part II 3</note>
</biblStruct>

<biblStruct coords="9,112.66,226.89,394.52,10.91;9,112.66,240.44,394.83,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,314.67,226.89,188.49,10.91">Lsbert: Lexical simplification based on bert</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.66,240.44,302.12,10.91">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3064" to="3076" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,253.99,393.33,10.91;9,112.66,267.54,323.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,285.27,253.99,220.72,10.91;9,112.66,267.54,85.47,10.91">Improving lexical coverage of text simplification systems for spanish</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,206.60,267.54,150.97,10.91">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,281.08,394.53,10.91;9,112.66,294.63,173.79,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05022</idno>
		<title level="m" coord="9,269.47,281.08,233.50,10.91">Semantic structural evaluation for text simplification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,308.18,394.52,10.91;9,112.66,321.73,231.27,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,207.02,308.18,295.99,10.91">Leveraging event-based semantics for automated text simplification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Glavaš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.66,321.73,147.34,10.91">Expert systems with applications</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="383" to="395" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,335.28,393.33,10.91;9,112.66,348.83,260.79,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,180.24,335.28,325.75,10.91;9,112.66,348.83,78.85,10.91">A semantic relevance based neural network for text summarization and text simplification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02318</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,362.38,393.33,10.91;9,112.66,375.93,322.83,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Andi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bambang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11193</idno>
		<title level="m" coord="9,325.30,362.38,180.68,10.91;9,112.66,375.93,140.89,10.91">Integrating transformer and paraphrase rules for sentence simplification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,389.48,393.33,10.91;9,112.66,403.03,393.33,10.91;9,112.66,416.58,393.32,10.91;9,112.66,430.13,137.66,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,405.01,389.48,100.98,10.91;9,112.66,403.03,184.31,10.91">Sequence-to-sequence models for automated text simplification</title>
		<author>
			<persName coords=""><forename type="first">R.-M</forename><surname>Botarleanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dascalu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,323.02,403.03,182.97,10.91;9,112.66,416.58,161.58,10.91">Artificial Intelligence in Education: 21st International Conference, AIED 2020</title>
		<title level="s" coord="9,428.70,416.58,77.28,10.91;9,112.66,430.13,6.48,10.91">Proceedings, Part II</title>
		<meeting><address><addrLine>Ifrane, Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">July 6-10, 2020. 2020</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,443.67,393.32,10.91;9,112.66,457.22,393.98,10.91;9,112.66,470.77,37.91,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,241.75,443.67,264.23,10.91;9,112.66,457.22,120.81,10.91">Extremely low-resource text simplification with pre-trained transformer language model</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,241.74,457.22,223.90,10.91">International Journal of Asian Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2050001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,484.32,351.59,10.91" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/simplet5/" />
		<imprint>
			<date type="published" when="2022">2022. 2023-06-05</date>
		</imprint>
	</monogr>
	<note>simplet5</note>
</biblStruct>

<biblStruct coords="9,112.66,497.87,395.17,10.91;9,112.66,511.42,394.04,10.91;9,112.66,524.97,357.36,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,320.22,497.87,187.62,10.91;9,112.66,511.42,27.10,10.91">Jurassic-1: Technical Details and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<idno>AI21</idno>
		<ptr target="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,112.66,538.52,395.00,10.91;9,112.66,554.51,97.35,7.90" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Workshop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m" coord="9,175.65,538.52,301.39,10.91">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
