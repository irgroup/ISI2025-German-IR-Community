<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,348.08,15.42;1,89.29,106.66,86.46,15.43">AIIR and LIAAD Labs Systems for CLEF 2023 SimpleText</title>
				<funder>
					<orgName type="full">National Funds</orgName>
				</funder>
				<funder ref="#_SueyBs4">
					<orgName type="full">Portuguese Foundation for Science and Technology)</orgName>
				</funder>
				<funder>
					<orgName type="full">FCT -Fundação para a Ciência e a Tecnologia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,90.05,11.96"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
							<email>behrooz.mansouri@maine.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Maine</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Maine</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.98,134.97,60.90,11.96"><forename type="first">Shea</forename><surname>Durgin</surname></persName>
							<email>shea.durgin@maine.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Maine</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Maine</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.04,134.97,53.82,11.96"><forename type="first">S</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
							<email>sj.franklin@maine.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Maine</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Maine</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.03,134.97,65.63,11.96"><forename type="first">Sean</forename><surname>Fletcher</surname></persName>
							<email>sean.fletcher@maine.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Maine</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>Maine</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,79.86,11.96"><forename type="first">Ricardo</forename><surname>Campos</surname></persName>
							<email>ricardo.campos@ipt.pt</email>
							<affiliation key="aff1">
								<orgName type="department">Ci2 -Smart Cities Research Center</orgName>
								<orgName type="institution">University of Beira Interior</orgName>
								<address>
									<settlement>Covilhã</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Polytechnic Institute of Tomar</orgName>
								<address>
									<settlement>Tomar</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">INESC TEC</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,348.08,15.42;1,89.29,106.66,86.46,15.43">AIIR and LIAAD Labs Systems for CLEF 2023 SimpleText</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">17F860ACCE3C78578810C627220F615D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scientific text simplification</term>
					<term>Keyword Extraction</term>
					<term>Definition Extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Artificial Intelligence and Information Retrieval (AIIR) Lab from the University of Southern Maine and the Laboratory of Artificial Intelligence and Decision Support (LIAAD) lab from INESC TEC in the CLEF 2023 SimpleText lab. There are three tasks defined for SimpleText: (T1) What is in (or out)?, (T2) What is unclear?, and (T3) Rewrite this!. Five runs were submitted for Task 1 using traditional Information Retrieval, and Sentence-BERT models. For Task 2, three runs were submitted, using YAKE! and KBIR keyword extraction models. Finally, for Task 3, two models were deployed, one using OpenAI Davinci embeddings and the other combining two unsupervised simplification models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The SimpleText lab at CLEF 2023 <ref type="bibr" coords="1,231.20,417.44,12.69,10.91" target="#b0">[1]</ref> has three tasks. The first task, What is in (or out)? involves searching a large database of academic abstracts and bibliographic metadata for passages that are relevant to a specific article. The topics for this task are a selection of press articles from two sources: the tech section of The Guardian newspaper (topics G01 to G20) and the Tech Xplore website (topics T01 to T20). Keyword queries are provided with each topic. Participants should find all passages from DBLP abstracts that are relevant to each query, and that could be used as citations in the paper associated with the topic. Each retrieved abstract is associated with a relevance score and a combined score indicating measures such as relevance, readability, and citation measures. For this task, the Artificial Intelligence and Information Retrieval (AIIR) lab from the University of Southern Maine (Maine, USA) and the Laboratory of Artificial Intelligence and Decision Support (LIAAD) lab from INESC TEC (Portugal) proposed five runs. One run, combine results from traditional Information Retrieval (IR) models, TF-IDF and PL2 <ref type="bibr" coords="2,89.29,86.97,11.28,10.91" target="#b1">[2]</ref>. The other runs take advantage of Sentence-BERT <ref type="bibr" coords="2,323.86,86.97,12.68,10.91" target="#b2">[3]</ref> cross-encoders and bi-encoder models, described in the next section.</p><p>The goal of the second task, What is unclear?, is to make the key concepts in a topic easy to understand by providing definitions, examples, and use cases. Participants should identify the most difficult terms in each passage and provide a ranked list of those terms. The difficulty score should be on a scale of 0 to 2, with 2 being the most difficult term and 0 being the easiest term (meaning can be guessed). Definition extraction for the terms is optional (second subtask). We participated in both subtasks, using three different approaches, followed by the same definition extraction pipeline for the second subtask, discussed in Section 3. For subtask 1, we used YAKE! <ref type="bibr" coords="2,89.29,208.91,11.58,10.91" target="#b3">[4]</ref>, YAKE! combined with IDF score, and KBIR <ref type="bibr" coords="2,304.40,208.91,12.99,10.91" target="#b4">[5]</ref> to find difficult terms. Using the detected difficult terms, in subtask 2, we extracted their definitions from the SimpleText corpus, using TF-IDF model to find candidate definitions, followed by a classification model using a fine-tuned ALBERT <ref type="bibr" coords="2,130.39,249.56,12.84,10.91" target="#b5">[6]</ref> model.</p><p>And finally, the goal of the third task, Rewrite this!, is to create a simplified version of sentences that are extracted from scientific abstracts. For this task, our team considered two runs as explained in Section 4. One run chooses between two generated simplified versions of the text using existing techniques for text simplification, while the other uses OpenAI's text-davinci-003 model with a prompt to get the simplified text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 1: What is in (or out)?</head><p>This section introduces our proposed models for task 1, along with the results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Proposed Models</head><p>For the What is in (or out)? task, we proposed five systems: one based on traditional IR models, and the others based on Sentence-BERT <ref type="bibr" coords="2,266.07,438.93,11.28,10.91" target="#b2">[3]</ref>. Here, we provide the descriptions of these systems.</p><p>TF-IDF combined with PL2. For this run, we combine the results from two traditional IR models, TF-IDF and PL2 <ref type="bibr" coords="2,199.62,466.03,11.46,10.91" target="#b1">[2]</ref>. After retrieving the top-1000 results for each of the two systems, the retrieval results are combined using Modified Reciprocal Rank Fusion <ref type="bibr" coords="2,418.44,479.58,12.84,10.91" target="#b6">[7]</ref> as follows:</p><formula xml:id="formula_0" coords="2,207.35,502.15,299.29,29.64">𝑅𝑅𝐹 𝑠𝑐𝑜𝑟𝑒(𝑓 ∈ 𝐹 ) = ∑︁ 𝑚∈𝑀 𝑠 𝑚 (𝑓 ) 60 + 𝑟 𝑚 (𝑓 )<label>(1)</label></formula><p>where 𝑠 𝑚 is the similarity score given by the retrieval model and 𝑟 𝑚 is the rank of the retrieved passage among the top-1000 results. We used MinMax normalization to transform all the relevance scores to a common scale, which ranges from 0 to 1. For both systems, the input query is the given query concatenated with the topic text. For example, for query G01.1 with topic text as "Digital assistants like Siri and Alexa entrench gender biases says UN", and query "Digital assistant", the input query is "Digital assistant Digital assistants like Siri and Alexa entrench gender biases says UN". We considered both the titles and abstracts of the DBLP papers in the collection for retrieval.</p><p>Cross-Encoder. For our first Sentence-BERT model, we use Cross-Encoder architecture <ref type="foot" coords="2,501.96,649.03,3.71,7.97" target="#foot_0">1</ref>with 'ms-marco-electra-base' as the pre-trained model without fine-tuning. This model was trained on the MS Marco Passage Ranking task <ref type="bibr" coords="3,308.60,100.52,11.58,10.91" target="#b7">[8]</ref>. In Cross-Encoder, query and candidate passage is passed to a BERT-based model and the relevance score is predicted. Each input query is represented as the concatenation of query and topic text (with white space), and the relevance is determined based on the abstracts of the papers. We combine the relevance scores from this model with scores from ElasticSearch model, using the Modified Reciprocal Rank Fusion as explained previously.</p><p>Fine-tuned Cross-Encoder. Our next two proposed approaches utilize fine-tuned crossencoder models. For fine-tuning, we used the 29 assessed topics from the 2022 SimpleText lab <ref type="bibr" coords="3,89.29,208.91,11.58,10.91" target="#b8">[9]</ref>. As our pre-trained model, we employed 'ms-marco-MiniLM-L-6-v2'. We considered 200 epochs and selected the best model based on the validation set, using a 90-10 split. Fine-tuning was performed with a maximum sequence length of 512 and a batch size of 4. Our approaches share a similar architecture but differ in the representation of the input query. In the first proposed model (Fine-Tuned Cross-Encoder (1)), we represented each input query as "query text + [QSP] + topic text", while the papers are represented as "title + [TSP] + abstract". Here, [QSP] is a special token used to separate the query text from the topic text, and [TSP] is another special token separating a paper's title from its abstract. In the second approach (Fine-Tuned Cross-Encoder ( <ref type="formula" coords="3,160.86,317.30,3.45,10.91">2</ref>)), we just considered the query text as the input.</p><p>Sentence-BERT Bi-Encoder. Our final proposed approach uses the bi-encoder architecture, where sentences are independently passed through BERT models, and then their corresponding vectors are compared using cosine similarity. We adopted the same representation of query and topic text with [QSP] separator and title and abstract with [TSP] separator as the input for the model. For fine-tuning, we utilized a 'distilroberta-base' model with Triplet loss <ref type="bibr" coords="3,467.74,385.05,16.37,10.91" target="#b9">[10]</ref>. We incorporated all the training samples from the previous lab, with a 90-10 split for the training and validation sets. Positive samples were selected from abstracts with relevance scores of 1 and 2, and the negative samples were chosen from those with a score of 0. We generated all combinations to increase the number of training samples. We fine-tuned for 10 epochs with a batch size of 32 and a maximum sequence length of 512 and used the best model (lowest loss) on the validation set for ranking.</p><p>All the Sentence-BERT models use the top-100 results (for re-ranking or fusion) from Elasticsearch system provided by the organizers of the SimpleText lab. We chose 100 due to the efficiency of the Cross-Encoders models (being slower). Each team was allowed to retrieve up to 100 results per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation</head><p>For evaluation, two aspects are considered: effectiveness and readability. Table <ref type="table" coords="3,451.84,570.27,5.17,10.91" target="#tab_0">1</ref> shows the effectiveness of our proposed approaches, provided by the organizers. As can be seen, our cross-encoder model (with special tokens and topic text), provides the highest MRR and BPref <ref type="bibr" coords="3,89.29,610.92,18.07,10.91" target="#b10">[11]</ref> values, while our proposed cross-encoder (with no fine-tuning) achieved the highest effectiveness considering other measures. To further analyze our proposed approaches, Figure <ref type="figure" coords="3,501.27,624.46,4.97,10.91" target="#fig_0">1</ref> shows the P@10 values per test topic for the Fine-Tuned Cross-Encoder (1). As can be seen, for 11 out of 34 topics, this model has a P@10 &gt; 0.8. For topics such as "T01.2" (light positioning with topic text as "Curve Light: A highly performing indoor positioning system"), this model  retrieves relevant abstracts such as "Visible Light Positioning (VLP) is emerging as a solution for indoor localization. . . " as the top document. Looking at the topics with lower effectiveness, for topic "G19.2" (kaspersky with topic text as International Space Station attacked by 'virus epidemics'), this model has P@10 of 0.4. Documents retrieved as the top-10 results by this model are mostly related to individual terms in the topic text. For instance, documents such as "During the 2009 H1N1 influenza pandemic, there was rising concern about the potential contribution of international travel. . . " is related to the influenza pandemic and its effect on international travel, and not considered related to the topic, specifically International Space Station. Another example is topic G18.1 (Jay Gambetta with topic text as The next giant leap: why Boris Johnson wants to 'go big' on quantum computing), P@10 is 0.1. While the retrieved documents seem to be relevant, in our models we did not make use of meta-data for the abstract and the authors' names are not considered. Abstracts with "Jay Gambetta" as the author (which are topically related to the query) are considered relevant for this topic.</p><formula xml:id="formula_1" coords="4,114.38,280.54,381.13,8.26">G 1 7 .1 G 1 8 .4 T 0 1 .2 T 0 5 .1 T 0 1 .1 T 0 1 .3 T 0 2 .2 T 0 3 .1 T 0 5 .4 G 1 8 .2 T 0 5 .3 G 1 7 .4 T 0 4 .3 T 0 5 .2 G 1 6 .2 G 2 0 .2 T 0 4 .2 G 1 6 .4 G 2 0 .1 T 0 3 .2 T 0 3 .3 G 1 9 .2 G 1 7 .2 G 1 8 .3 T 0 2 .3 T 0 4 .1 G 1 6 .1 G 1 9 .1 G 1 9 .3 T 0 2 .1 T 0 4 .4 G 1 6 .3 G 1 7 .3 G 1 8 .1</formula><p>Comparing the fine-tuned Cross-Encoder (1) with the Cross-Encoder (with no fine-tuning), there were cases that one model outperformed the other. For instance, for topic "G17.1" (quantum computing), the P@10 value for fine-tuned model is 1 while for the other is 0.3. With no finetuning, non-relevant documents discussing other related concepts to quantum computing such as transistors or CNFET cell were retrieved among the top-10 results. On the other hand, for topics such as "G16.2" (NLP application), the P@10 drops from 0.9 to 0.6 when using fine-tuned Cross-Encoder. The fine-tuned model retrieves more specific abstracts about NLP applications discussing tasks such as summarization while the model with no fine-tuning ranks the document on general NLP applications (such as evaluation approaches) higher.</p><p>The same pattern can be seen when comparing TF-IDF+PL2 and fine-tuned Cross-Encoder models. For topic "G18.1" (peer recommendations with topic text: New crowdsourced recruit- ment tool aims to get more women into tech), P@10 value with TF-IDF+PL2 model is 0.8 dropping to 0.1 when using fine-tuned Cross-Encoder. The first model retrieves abstracts that are mainly focused on women while with the Cross-Encoder non-related abstracts discussing minorities, or concepts such as peer-to-peer computing and recommenders are retrieved in the top-10 results. In contrast, for topics such as "T05.4" (empathy with topic text: New 'emotional' robots aim to read human feelings), P@10 value for TF-IDF+PL2 model is 0.2, increasing to 0.9 when fine-tuned Cross-Encoder is used. TF-IDF+PL2 model ranks non-relevant abstracts such as robot emotions where as the Cross-Encoder model focuses on empathy.</p><p>The second aspect of evaluation is readability. Table <ref type="table" coords="5,327.65,343.28,4.97,10.91" target="#tab_0">1</ref> shows the readability analysis provided on the top-10 results by the organizers. In this table, the impact indicates the impact factor based on ACM records, #Refs shows the average number of references per document, and FKGL(Flesch-Kincaid Grade Level) <ref type="bibr" coords="5,241.87,383.93,17.75,10.91" target="#b11">[12]</ref> is the readability score on a scale of 1 to 100. The higher the reading score, the easier a piece of text is to read. While all our proposed models had the highest impact and #Refs, our first fine-tuned cross-encoder model had the highest readability score among the participating teams.</p><p>Overall, our proposed approaches using Cross-Encoder architecture provided strong results for task 1. When fine-tuning the Cross-Encoder, we could achieve better results for more technical queries whereas with no fine-tuning, Cross-Encoder works better for general topics. Comparing traditional IR models with Cross-Encoders, Cross-Encoders could do better focusing on the query term when retrieving the documents than the topic text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 2: What is Unclear?</head><p>Our team participated in both subtasks. For subtask 2.1, we proposed three approaches to detect difficult terms in the given sentences. For subtask 2.2, we used a fine-tuned BERT model to decide if a sentence with a difficult term contains a definition. Next, we elaborate on the specifics of our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Models</head><p>Subtask 2.1: Detecting difficult terms. To detect difficult terms in the given sentences for this subtask, our first proposed model utilizes YAKE! <ref type="bibr" coords="5,328.62,654.87,12.91,10.91" target="#b3">[4]</ref> keyword extractor. With the default parameters and a window size of 3, we select the most relevant keyphrase (the one with the lowest score). Our second approach combines YAKE! scores with IDF (Inverse Document Frequency) scores, similar to Chen et al. <ref type="bibr" coords="6,270.19,100.52,15.41,10.91" target="#b12">[13]</ref>. We first extracted the top-10 important phrases using YAKE!, and then we calculated the average IDF value for each phrase. The final phrase score is defined as 𝑌 𝐴𝐾𝐸!_𝑆𝑐𝑜𝑟𝑒/𝐴𝑉 𝐺_𝐼𝐷𝐹 . We selected the most important keyphrase (with the lowest score). For our last approach, we considered Keyphrase Boundary Infilling with Replacement (KBIR) <ref type="bibr" coords="6,201.32,154.71,12.69,10.91" target="#b4">[5]</ref> to extract difficult terms. KBIR is a pre-trained model that employs a multitask learning framework to optimize a combined loss function comprising Masked Language Modeling (MLM), Keyphrase Boundary Infilling (KBI), and Keyphrase Replacement Classification.</p><p>Subtask 2.2: Providing an explanation for difficult terms. To extract the explanations for each difficult phrase, we first retrieve the top-1000 relevant documents for each phrase. To do this, we use the TF-IDF model. This model is suitable for our task as we are aiming to find the exact matching of candidate phrases. Then, starting from the top retrieved document, we explored the sentences and check if they contain a definition.</p><p>To determine this, we fine-tune AlBERT <ref type="bibr" coords="6,275.85,276.66,12.68,10.91" target="#b5">[6]</ref> model on DEFT <ref type="bibr" coords="6,361.03,276.66,17.76,10.91" target="#b13">[14]</ref> corpus. This corpus contains 16,800 labeled sentences indicating whether the sentence contains a definition. For fine-tuning, we consider 5 epochs, choosing the model with the highest accuracy on the validation set (split of 90-10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation</head><p>For subtask 2.1, two measures were considered for evaluation: the correctness of detected term limits and difficulty scores. The first metric reflects whether the retrieved difficult terms are well-limited or not. The second is a three-scale terms difficulty score, which reflects how difficult the term is in the context for an average user and how necessary it is to provide more context about the term. Table <ref type="table" coords="6,187.48,421.23,5.07,10.91" target="#tab_2">3</ref> shows the results of our proposed approaches to this subtask. Around 10% of our extracted terms were evaluated and among these, YAKE! did better compared to our other two models in extracting terms that should be defined, but KBIR provided better results in detecting the term limits (+Limits). For instance, in the sentence "This paper proposes a new Compressed Video Steganographic scheme. ", KBIR detects "Compressed Video Steganographic scheme" whereas YAKE! considers "Compressed Video Steganographic" as the terms that should be defined. On the other hand, YAKE! is able to extract correct terms in sentences such as "As a consequence, business processes that interact with large amounts of such data may easily cause GDPR violations, due to the typical complexity of such processes. ", extracting "GDPR violations" where KBIR extracts "business processes".</p><p>For subtask 2.2, three measures are considered: 1) BLEU <ref type="bibr" coords="6,357.65,556.72,18.07,10.91" target="#b14">[15]</ref> score between the reference (ground truth definition) and the predicted definitions, 2) ROUGE-L F-measure <ref type="bibr" coords="6,456.33,570.27,18.06,10.91" target="#b15">[16]</ref> which measures the ROUGE F-measure based on the Longest Common Subsequence between the reference and the predicted definitions, and 3) Semantic match between the reference and predicted definitions measured using the "all-mpnet-base-v2" sentence transformer model. Table <ref type="table" coords="6,115.01,624.46,4.97,10.91" target="#tab_3">4</ref> shows the results of our runs for this subtask. Our models have similar effectiveness as a similar pipeline was used for definition extraction. We detected one major issue in our runs after submission; after retrieving a document with the TF-IDF model, the first sentence containing a definition is considered as the extracted term for the definition. The fix for this is to prioritize the sentences that contain definitions but also the target term for which we are looking for its definition. We also believe that based on the results from Task 1, the Cross-Encoder models might be a better suit for the initial retrieval steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 3: Rewrite This!</head><p>For task 3, rewriting scientific text, we propose two systems; one combining results from two systems based on how simplified the outputs are, and the other using text-davinci-003 embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proposed Models</head><p>In our first approach, we combined the simplification results from the models proposed by Laban et al. <ref type="bibr" coords="7,112.32,491.16,17.76,10.91" target="#b16">[17]</ref> (KiS), and Cripwell et al. <ref type="bibr" coords="7,237.99,491.16,16.09,10.91" target="#b17">[18]</ref>. The first model is an unsupervised simplification method, considering the problem as a reward maximization, rewarding simplicity, fluency, salience, and guardrails. The second model, however, considers four operations for simplification: whether to rephrase or copy and whether to split based on syntactic or discourse structure. For each sentence that we are aiming to simplify, we pass them to both models and then calculate the simplicity of the outcomes. Our simplicity score is defined based on the YAKE! and IDF scores introduced in the previous section. For each token in the sentence, we calculated the 𝑌 𝐴𝐾𝐸!/𝐼𝐷𝐹 score and averaged the scores to get the final simplicity score for a sentence. Our intuition is that a sentence with a higher YAKE! score and a lower IDF score is simpler. Therefore, a sentence with the highest score is then selected as the outcome of our proposed approach.</p><p>In the second approach, we used the OpenAI's text-davinci-003, using a simple prompt as:</p><p>Simplify this sentence with simpler wording, explaining difficult terms: followed by the sentence to be simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>For Task 3 evaluation, several measures are considered, including the followings:</p><p>• Flesch-Kincaid Grade Level (FKGL) readability metric • SARI <ref type="bibr" coords="8,141.51,279.81,17.81,10.91" target="#b18">[19]</ref> metric that compares the system's output to multiple simplification references and the original sentence based on the words added, deleted, and kept by a system • BLEU • Compression ratio • Sentence splits • Levenshtein similarity that measures the number of edits (insertions, deletions, or substitutions) needed to transform one sentence into another • Additions proportion • Deletions proportion Table <ref type="table" coords="8,128.27,421.30,5.17,10.91" target="#tab_4">5</ref> shows the results of our proposed approaches for this task. As indicated in this table, the Davinci model provides better effectiveness compared to our combined approach and better compresses the sentence with fewer additions and more deletions. Providing an example, for the source sentence "Abstract Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. ", the simplified version by this model is "Novel tech can be used to help with wildfire confrontation, allowing users to access data, share intelligence, and coordinate personnel and vehicles. ", closely similar to the provided simplified sentence by the organizers as "Novel mobile devices and applications can be used in wildfire confrontation by helping users to access data and information and coordinate personnel and vehicles. ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has described the AIIR and LIAAD labs submissions for SimpleText lab at CLEF 2023. Five runs were submitted for What is in (or out)? task. Our Cross-Encoder models provided better effectiveness compared to the model based on Bi-Encoder and the other model using traditional IR models. We participated in both subtasks of What is unclear?, proposing three approaches. However, our models seem to be less effective for this task. And finally, for the Rewrite This! task, we considered two approaches, one based on the OpenAI Davinci model with a simple prompt for rewriting the text, and the other combining results from two simplification methods. The Davinci model for this task provided better effectiveness and compression. As this was our first attempt at the tasks in SimpleText lab, we aim to explore our current proposed approaches in detail, analyzing the results and removing potential errors as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,315.75,342.91,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision at 10 per test topic for our Fine-Tuned Cross-Encoder (1) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,392.78,208.58"><head>Table 1</head><label>1</label><figDesc>Evaluation of SimpleText Task 1 (Test Qrels).</figDesc><table coords="4,94.10,122.10,387.67,176.96"><row><cell></cell><cell>Run</cell><cell>MRR</cell><cell cols="2">Precision</cell><cell>NDCG</cell><cell>Bpref MAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>10</cell><cell>20</cell></row><row><cell></cell><cell cols="5">Fine-Tuned Cross-Encoder (1) 0.734 0.497 0.400 0.486 0.430 0.344 0.239</cell></row><row><cell></cell><cell>Cross-Encoder</cell><cell cols="4">0.731 0.527 0.450 0.546 0.484 0.334 0.275</cell></row><row><cell></cell><cell cols="5">Fine-Tuned Cross-Encoder (2) 0.708 0.471 0.393 0.462 0.409 0.326 0.225</cell></row><row><cell></cell><cell>Bi-Encoder (TripletLoss)</cell><cell cols="4">0.550 0.338 0.218 0.335 0.256 0.134 0.070</cell></row><row><cell></cell><cell>TF-IDf+PL2</cell><cell cols="4">0.563 0.418 0.281 0.401 0.322 0.216 0.136</cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P@10</cell><cell>0.25 0.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Topic Id</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,389.97,117.69"><head>Table 2</head><label>2</label><figDesc>Text Analysis of SimpleText Task 1 output.</figDesc><table coords="5,116.32,122.10,362.64,86.07"><row><cell>Run</cell><cell cols="2">Impact #Refs</cell><cell cols="2">Length</cell><cell cols="2">FKGL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell cols="3">Median Mean Median</cell></row><row><cell>Fine-Tuned Cross-Encoder (1)</cell><cell>4.41</cell><cell>3.37</cell><cell>1003.75</cell><cell>988.00</cell><cell>15.01</cell><cell>14.80</cell></row><row><cell>Cross-Encoder</cell><cell>4.22</cell><cell>2.86</cell><cell>961.17</cell><cell>923.00</cell><cell>14.64</cell><cell>14.60</cell></row><row><cell>Fine-Tuned Cross-Encoder (2)</cell><cell>3.49</cell><cell>3.04</cell><cell>988.86</cell><cell>951.50</cell><cell>14.95</cell><cell>14.80</cell></row><row><cell>Bi-Encoder (TripletLoss)</cell><cell>4.76</cell><cell>3.29</cell><cell>969.09</cell><cell>973.50</cell><cell>14.69</cell><cell>14.60</cell></row><row><cell>TF-IDf+PL2</cell><cell>3.35</cell><cell>2.58</cell><cell>893.29</cell><cell>894.00</cell><cell>14.03</cell><cell>14.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,313.63,93.78"><head>Table 3</head><label>3</label><figDesc>SimpleText Task 2.1: Results for the official runs.</figDesc><table coords="7,192.66,122.10,209.96,62.16"><row><cell>Run</cell><cell>Total</cell><cell cols="2">Evaluated</cell><cell></cell><cell>Score</cell></row><row><cell></cell><cell></cell><cell></cell><cell>+Limits</cell><cell></cell><cell>+Limits</cell></row><row><cell>YAKE</cell><cell cols="2">4790 486</cell><cell>234</cell><cell>169</cell><cell>78</cell></row><row><cell>KBIR</cell><cell cols="2">4797 498</cell><cell>429</cell><cell>158</cell><cell>135</cell></row><row><cell cols="3">YAKEIDF 4790 465</cell><cell>241</cell><cell>154</cell><cell>75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,206.53,319.88,81.83"><head>Table 4</head><label>4</label><figDesc>SimpleText Task 2.2: Results for the official runs.</figDesc><table coords="7,186.41,238.14,222.47,50.21"><row><cell>Run</cell><cell cols="4">Evaluated BLUE ROUGE Semantic</cell></row><row><cell>KBIR</cell><cell>556</cell><cell>1.62</cell><cell>0.15</cell><cell>0.50</cell></row><row><cell>YAKEIDF</cell><cell>179</cell><cell>1.13</cell><cell>0.14</cell><cell>0.41</cell></row><row><cell>YAKE</cell><cell>165</cell><cell>1.10</cell><cell>0.15</cell><cell>0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,90.49,409.80,73.53"><head>Table 5</head><label>5</label><figDesc>SimpleText Task 3 Results.</figDesc><table coords="8,96.50,121.25,402.28,42.77"><row><cell>Run</cell><cell cols="7">count FKGL SARI BLEU Compression Sentence Levenshtein Additions</cell><cell>Deletions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ratio</cell><cell>splits</cell><cell>similarity</cell><cell cols="2">proportion proportion</cell></row><row><cell>Davinci</cell><cell>243</cell><cell cols="2">11.17 47.10 18.68</cell><cell>0.75</cell><cell>1.00</cell><cell>0.68</cell><cell>0.20</cell><cell>0.45</cell></row><row><cell>Combined</cell><cell>245</cell><cell>9.86</cell><cell>30.07 15.93</cell><cell>1.26</cell><cell>1.67</cell><cell>0.80</cell><cell>0.30</cell><cell>0.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.04,236.79,8.97"><p>https://www.sbert.net/docs/pretrained-models/ce-msmarco.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to the organizers of <rs type="institution">SimpleText lab</rs>. We would like to thank <rs type="person">Liana Ermakova</rs>, for giving a talk at the <rs type="institution">University of Southern Maine</rs> and discussing details of the SimpleText lab. <rs type="person">Ricardo Campos</rs> was financed by <rs type="funder">National Funds</rs> through the <rs type="funder">FCT -Fundação para a Ciência e a Tecnologia</rs>, I.P. (<rs type="funder">Portuguese Foundation for Science and Technology)</rs> within the project <rs type="projectName">StorySense</rs>, with reference 2022.<rs type="grantNumber">09312</rs>.PTDC)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SueyBs4">
					<idno type="grant-number">09312</idno>
					<orgName type="project" subtype="full">StorySense</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,312.61,394.53,10.91;9,112.66,326.16,393.33,10.91;9,112.66,339.71,394.53,10.91;9,112.66,353.26,394.53,10.91;9,112.66,366.81,287.70,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,268.42,326.16,237.57,10.91;9,112.66,339.71,137.65,10.91">Overview of the CLEF 2022 SimpleText Lab: Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.98,339.71,235.21,10.91;9,112.66,353.26,389.90,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 5-8, 2022. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,380.36,393.33,10.91;9,112.66,393.91,393.33,10.91;9,112.33,407.46,61.07,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,257.30,380.36,248.68,10.91;9,112.66,393.91,194.61,10.91">Probabilistic Models of Information Retrieval based on Measuring the Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,316.44,393.91,189.55,10.91;9,112.33,407.46,29.15,10.91">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,421.01,395.17,10.91;9,112.66,434.55,221.81,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="9,231.26,421.01,276.57,10.91;9,112.66,434.55,39.20,10.91">Sentence-BERT: Sentence Embeddings using Siamese BERTnetworks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,448.10,393.32,10.91;9,112.66,461.65,393.32,10.91;9,112.33,475.20,29.19,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,432.41,448.10,73.57,10.91;9,112.66,461.65,289.97,10.91">YAKE! Keyword Extraction from Single Documents using Multiple Local Features</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Information Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,488.75,393.33,10.91;9,112.66,502.30,223.73,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,315.28,488.75,190.70,10.91;9,112.66,502.30,41.65,10.91">Learning Rich Representation of Keyphrases from Text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bhowmik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08547</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,515.85,393.53,10.91;9,112.66,529.40,393.32,10.91;9,112.33,542.95,29.19,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,392.89,515.85,113.30,10.91;9,112.66,529.40,241.83,10.91">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,556.50,394.62,10.91;9,112.66,570.05,393.59,10.91;9,112.66,583.60,52.21,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,286.99,556.50,220.29,10.91;9,112.66,570.05,200.44,10.91">DPRL Systems in the CLEF 2022 ARQMath Lab: Introducing MathAMR for Math-Aware Search</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,320.86,570.05,185.38,10.91;9,112.66,583.60,20.29,10.91">Proceedings of the Working Notes of CLEF 2022</title>
		<meeting>the Working Notes of CLEF 2022</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,597.15,394.61,10.91;9,112.28,610.69,318.99,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Marco</surname></persName>
		</author>
		<title level="m" coord="9,112.28,610.69,287.07,10.91">A Human Generated MAchine Reading COmprehension Dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,624.24,394.53,10.91;9,112.66,637.79,393.33,10.91;9,112.66,651.34,394.53,10.91;10,112.66,86.97,394.53,10.91;10,112.66,100.52,287.70,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,268.42,637.79,237.57,10.91;9,112.66,651.34,137.65,10.91">Overview of the CLEF 2022 SimpleText Lab: Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.98,651.34,235.21,10.91;10,112.66,86.97,389.90,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 5-8, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,393.32,10.91;10,112.66,141.16,79.05,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,280.61,114.06,225.37,10.91;10,112.66,127.61,64.61,10.91">Facenet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,200.49,127.61,305.49,10.91;10,112.66,141.16,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,395.17,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,132.76,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,233.46,154.71,214.02,10.91">Retrieval Evaluation with Incomplete Information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,469.38,154.71,38.44,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,103.37,10.91">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,344.68,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,158.49,195.36,125.41,10.91">A New Readability Yardstick</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,292.17,195.36,133.25,10.91">Journal of applied psychology</title>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.32,10.91;10,112.66,222.46,395.17,10.91;10,112.66,236.01,66.82,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,318.10,208.91,187.89,10.91;10,112.66,222.46,338.96,10.91">An Approach Based on a Cross-Attention Mechanism and Label-Enhancement Algorithm for Legal Judgment Prediction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,459.74,222.46,48.09,10.91;10,112.66,236.01,11.88,10.91">Mathematics</title>
		<imprint>
			<biblScope unit="page">2032</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,395.16,10.91;10,112.66,263.11,393.33,10.91;10,112.28,276.66,127.52,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,387.28,249.56,120.55,10.91;10,112.66,263.11,215.52,10.91">DEFT: A corpus for definition extraction in free-and semi-structured text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Spala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dockhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,351.24,263.11,154.75,10.91;10,112.28,276.66,96.74,10.91">Proceedings of the 13th Linguistic Annotation Workshop</title>
		<meeting>the 13th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.32,10.91;10,112.66,303.75,393.53,10.91;10,112.66,317.30,145.69,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,306.18,290.20,199.80,10.91;10,112.66,303.75,91.33,10.91">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.24,303.75,278.95,10.91;10,112.66,317.30,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.33,10.91;10,112.66,344.40,393.33,10.91;10,112.28,357.95,319.53,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,201.39,330.85,304.60,10.91;10,112.66,344.40,37.52,10.91">Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.50,344.40,334.49,10.91;10,112.28,357.95,290.11,10.91">Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics</title>
		<meeting>the 2003 human language technology conference of the North American chapter of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,393.33,10.91;10,112.66,385.05,287.75,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,315.94,371.50,190.05,10.91;10,112.66,385.05,105.67,10.91">Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03444</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.33,10.91;10,112.66,412.15,394.53,10.91;10,112.66,425.70,22.69,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,275.12,398.60,230.87,10.91;10,112.66,412.15,57.04,10.91">Controllable Sentence Simplification via Operation Classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cripwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.43,412.15,311.31,10.91">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.33,10.91;10,112.33,452.79,393.65,10.91;10,112.66,466.34,80.33,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,370.50,439.25,135.49,10.91;10,112.33,452.79,154.89,10.91">Optimizing Statistical Machine Translation for Text Simplification</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.80,452.79,230.18,10.91;10,112.66,466.34,48.41,10.91">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
