<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,417.16,15.42;1,88.84,106.66,416.97,15.42;1,89.29,128.58,110.95,15.43">UZH_Pandas at SimpleText@CLEF-2023: Alpaca LoRA 7B and LENS Model Selection for Scientific Literature Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,125.82,11.96"><forename type="first">Pascal</forename><surname>Severin Andermatt</surname></persName>
							<email>pascalseverin.andermatt@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich (UZH)</orgName>
								<address>
									<addrLine>Rämistrasse 71</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.26,156.89,91.22,11.96"><forename type="first">Tobias</forename><surname>Fankhauser</surname></persName>
							<email>tobias.fankhauser@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich (UZH)</orgName>
								<address>
									<addrLine>Rämistrasse 71</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,417.16,15.42;1,88.84,106.66,416.97,15.42;1,89.29,128.58,110.95,15.43">UZH_Pandas at SimpleText@CLEF-2023: Alpaca LoRA 7B and LENS Model Selection for Scientific Literature Simplification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">017096A89FFAA1AA440F3986612E3FC7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scientific text simplification</term>
					<term>Generative Language Models</term>
					<term>Automated Simplification Metrics</term>
					<term>Prompt engineering</term>
					<term>Alpaca LoRA 7B</term>
					<term>LENS</term>
					<term>SimpleText@</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we advance the field of scientific text simplification by harnessing the capabilities of Alpaca LoRA 7B [1], a large language model derivative of the 7B LLaMA <ref type="bibr" coords="1,370.09,234.05,9.37,8.97" target="#b1">[2]</ref>. We expand the dataset for Task 3 of SimpleText@CLEF-2023 [3] by integrating data from Task 2, aiming to identify complex terms in need of explanation for better text comprehension. Our methodology involves rigorous fine-tuning, prompt engineering, and the application of the LENS score <ref type="bibr" coords="1,324.67,266.93,10.68,8.97" target="#b3">[4]</ref> as a tool for model reranking and evaluation. Our findings suggest the efficacy of our approach in creating a more effective text simplification system. Our final model demonstrates expertise not only in expanding abbreviations, but also in explaining complex terms present in the input sentence. This ability allows it to create texts that are both easy to understand and simple to comprehend, making the information presented more accessible and opening the door for more efficient communication. However, the study also highlights several challenges and areas of improvement, providing a valuable contribution to future research in text simplification. Our research underscores the potential of large language models like Alpaca LoRA 7B in transforming complex terminologies into more accessible language, ultimately enhancing the public's understanding of scientific literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the rapidly evolving landscape of Natural Language Processing (NLP), the simplification of intricate texts remains a prevalent challenge. To tackle this, we engage Alpaca LoRA 7B <ref type="bibr" coords="1,483.01,491.76,11.44,10.91" target="#b0">[1]</ref>, a state-of-the-art large language model fine-tuned from the LLaMA architecture <ref type="bibr" coords="1,445.75,505.31,12.97,10.91" target="#b1">[2]</ref> using 52K instruction-following data <ref type="bibr" coords="1,212.01,518.86,11.58,10.91" target="#b4">[5]</ref>. This model, further refined using the Low-Rank Adaptation (LoRA) technique <ref type="bibr" coords="1,171.23,532.41,11.56,10.91" target="#b5">[6]</ref>, presents an effective solution for condensing and simplifying complex scientific narratives.</p><p>This paper explores our experiment in using Alpaca LoRA 7B <ref type="bibr" coords="1,382.71,559.51,12.99,10.91" target="#b0">[1]</ref> to address Task 3 of the SimpleText CLEF shared task <ref type="bibr" coords="1,224.32,573.06,11.58,10.91" target="#b2">[3]</ref>, which aims to simplify scientific abstracts given a specific Input snt_id: G06.2_2805209921_6: we have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects. Output extended, simplification, complex words: Transfer learning is a technique used in machine learning which involves taking a pre-trained model and using it as a starting point for a new model. It requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects. Output default, simplification, default: Transfer learning is better than the original method. The examples demonstrate how the extended model with the simplification prompt and complex words evaluation provides a detailed explanation of the concept transfer learning. In contrast, the default model with the simplification prompt and default evaluation provides a much simpler, yet less informative statement.</p><p>query. While previous efforts have made progress in automatic simplification of scientific texts, there remains a gap between these scholarly texts and their accessibility to the public. Our goal is to further diminish this barrier by deploying an automated text simplification system that retains critical information while reducing linguistic complexity.</p><p>Building upon the findings from prior research that demonstrated the complementary nature of Task 2 and Task 3 <ref type="bibr" coords="2,180.72,365.29,11.28,10.91" target="#b6">[7]</ref>, we also explored incorporating data from Task 2 to extend the data for Task 3 of the SimpleText shared tasks. With the data from Task 2, we seek to identify terms or concepts that need explanation for understanding a passage. With this, we aim to provide valuable insight into the key elements of scientific texts that typically impede understanding. By incorporating this data into our work with Alpaca LoRA 7B, we intend to create a more attuned and efficient text simplification system that anticipates and addresses potential comprehension obstacles.</p><p>The key contributions and findings of this study are the following:</p><p>• We illustrate the benefits of dataset augmentation by showing that the integration of the Task 2 dataset (consisting of difficult terms, identified in scientific abstracts, and their corresponding explanations) into the Task 3 dataset (comprising simplified sentences) can enhance the performance and generalization capabilities of the model, improving the text simplification process. • We demonstrate the effectiveness of the Alpaca LoRA 7B model for the text simplification task. By leveraging its instruction following capabilities, Alpaca LoRA 7B was able to effectively simplify complex linguistic constructs. • We highlight the efficacy of the LENS score as a method for model re-ranking and evaluation in the context of text simplification. Compared to traditional metrics such as SARI, the LENS score more accurately captures the nuances of text simplification and aligns more closely with human judgement on the quality of text simplifications.</p><p>Our study marks an exploratory step towards understanding how effectively large language models like Alpaca LoRA 7B <ref type="bibr" coords="2,220.33,669.58,12.97,10.91" target="#b0">[1]</ref> can simplify complex terminologies and concepts into more accessible language. Our findings offer insights into the potential of these models and their applicability to text simplification, making a contribution to improving public understanding of the scientific literature. Figure <ref type="figure" coords="3,239.29,114.06,5.11,10.91" target="#fig_0">1</ref> shows an example of how our model can explain complex terms like transfer learning and simplify the sentence.</p><p>The structure of the paper continues as follows: Section 3 discusses our default and extended datasets used in model training, their origin, the process of their integration, and their effect on the model's performance.</p><p>In Section 4, we provide a comprehensive overview of the various components and steps involved in our approach. We begin by discussing the utilized model architectures in Section 4.1, followed by an exploration of the prompt engineering process in Section 4.2. We then investigate the incorporation of complex terms in Section 4.3, and subsequently explain the fine-tuning procedure in Section 4.4. The evaluation methodology is presented in Section 4.5, and finally, we introduce the LENS Score in Section 4.6 as a metric for assessing the quality of our results.</p><p>Finally, sections 5-7 outline our text simplification findings, highlighting the importance of suitable approaches and strategies, and ending with prospects for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Text-to-Text Transfer Transformer (T5) T5 <ref type="bibr" coords="3,313.11,348.83,11.58,10.91" target="#b7">[8]</ref>, grounded in transformer architecture, operationalized a notable approach in the field of natural language processing (NLP) by recasting all tasks as text-to-text problems. This strategy allowed a single model to address a wide variety of NLP tasks, which was an important step in the development of these technologies. Despite its advancements, T5's effectiveness is intimately tied to the quality and volume of its training data, and the model lacks the ability to genuinely understand the textual content it processes. These factors have shaped its strengths and limitations in practice <ref type="bibr" coords="3,385.98,430.13,11.43,10.91" target="#b7">[8]</ref>.</p><p>Despite these limitations, the success of T5 underscores the potential of transformer-based Large Language Models (LLMs) in tasks like text simplification. LLMs generate contextually appropriate responses, enabling nuanced simplifications. Yet, they require considerable computational resources and can occasionally produce verbose or off-topic outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca 7B</head><p>The Alpaca 7B model <ref type="bibr" coords="3,251.14,513.08,11.59,10.91" target="#b8">[9]</ref>, introduced by Stanford University, is an instructionfollowing language model, fine-tuned from Meta's LLaMA 7B <ref type="bibr" coords="3,368.71,526.63,11.52,10.91" target="#b1">[2]</ref>. This compact and efficient model closely parallels the capabilities of OpenAI's models, notably text-davinci-003, but offers a cost-efficiency alternative for academic research.</p><p>The development of Alpaca 7B addresses key challenges in training high-quality, budgetfriendly instruction-following models. For that, an innovative adaptation of the selfinstruct method was utilized. With 175 initial human-written instruction-output pairs, text-davinci-003 was used to generate an additional 52,000 instruction-following demonstrations <ref type="bibr" coords="3,130.49,621.48,11.36,10.91" target="#b4">[5]</ref>, which were then employed to fine-tune Alpaca 7B using Hugging Face's training framework. text-davinci-003 refers to the third version of a text-based model developed by OpenAI <ref type="bibr" coords="3,140.75,648.57,16.25,10.91" target="#b9">[10]</ref>.</p><p>Preliminary human evaluations demonstrated favorable performance of Alpaca compared to text-davinci-003 <ref type="bibr" coords="4,163.13,100.52,11.33,10.91" target="#b8">[9]</ref>. However, Alpaca does share common language model limitations, such as generating false information and perpetuating social stereotypes. Further, although 7 billion parameters are already small in terms of Large Language Models, Alpaca 7B's size remains a barrier. Hence, the need for more compact models is evident. Parameter-efficient Fine-tuning (PEFT) techniques like Low Rank Adaptation (LoRA) come into play as a potential solution for size constraints without compromising performance.</p><p>Low Rank Adaptation LoRA Emphasizing efficiency in advanced natural language processing, and because of limitations to the available hardware, Low-Rank Adaptation (LoRA) <ref type="bibr" coords="4,493.27,210.57,12.71,10.91" target="#b5">[6]</ref> was applied to the Alpaca 7B model. This technique introduces trainable rank decomposition matrices at each Transformer layer, substantially reducing the number of trainable parameters.</p><p>The impact of LoRA on Alpaca 7B has been transformational, reducing trainable parameters to a mere 16 million, while preserving model performance <ref type="bibr" coords="4,376.12,264.77,11.58,10.91" target="#b0">[1]</ref>. This reduction mitigates computational demands and cost constraints, rendering deployment of the model more feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable Evaluation Metric for Text Simplification (LENS)</head><p>LENS was developed to address limitations in current text simplification evaluation metrics. Leveraging a modern language model, LENS is trained on the SimpEval corpus, a robust dataset featuring human ratings of text simplifications from multiple sources, including GPT-3.5. Through this method, LENS captures nuanced aspects of text simplification that conventional metrics might overlook. The crux of its functionality lies in its adaptivity: LENS adjusts and improves as it encounters more data, increasing the accuracy and relevance of its evaluations. By aligning more closely with human judgment than traditional metrics, LENS offers a promising tool for evaluating and advancing text simplification technologies, as shown by Maddela et al. <ref type="bibr" coords="4,405.08,415.47,11.43,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>During our training procedure, we primarily utilized the Task 3 dataset for fine-tuning our models. Task 3 comprised a parallel corpus of simplified sentences originating from Medicine and Computer Science domains. The simplification was performed either by a master student in Technical Writing and Translation or by an expert duo consisting of a computer scientist and a professional translator <ref type="bibr" coords="4,200.42,528.29,11.45,10.91" target="#b2">[3]</ref>. Despite its quality, the Task 3 dataset posed a challenge in terms of data scarcity, as it consisted of only 648 sentence pairs.</p><p>In an effort to address this challenge and enhance the stability of our model, we incorporated data from Task 2. The Task 2 dataset, also drawn from Medicine and Computer Science domains, consisted of scientific abstracts sourced from the Citation Network Dataset and Google Scholar and PubMed articles focusing on muscle hypertrophy and health. These were annotated by a master student in Technical Writing and Translation, who assigned difficulty scores to extracted terms, resulting in a total of 453 annotated examples <ref type="bibr" coords="4,325.33,623.14,11.43,10.91" target="#b2">[3]</ref>.</p><p>Our approach was to integrate the Task 2 dataset into the training process to assess if a larger amount of data, pointing out difficult terms, could potentially boost the model's performance. By merging Task 2 and Task 3 datasets, we aimed to not only increase the quantity of the training data but also its diversity, thereby enhancing the model's overall performance and generalization capabilities.</p><p>It is important to highlight that our approach did not involve the utilization of any supplementary data or the implementation of other data augmentation techniques. We solely relied on the data from Task 3 as the default dataset, and the integration of Task 2 data resulted in the extended dataset. In the subsequent sections, we will refer to the dataset sourced from Task 3 as the default dataset and the combined dataset from Task 2 and Task 3 as the extended dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we outline our research methodology involving the utilization of the Alpaca 7B model optimized by Low Rank Adaptation (LoRA) for text simplification. We employed strategies such as prompt engineering, identification of complex terms, rigorous fine-tuning, and a unique two-option evaluation process. Furthermore, we used the LENS score for assessment and model fusion to combine the strengths of multiple models. The overall setup of our submission is shown in Figure <ref type="figure" coords="5,164.29,294.63,3.74,10.91" target="#fig_1">2</ref>. The models are then fine-tuned using a wide variety of combinations of the hyperparameters. After fine-tuning, these models are evaluated on two tasks: (1. default) direct text simplification or (2. complex terms) a combination of intermediate outcomes from Task 2 (identification and explanation of challenging terms) with text simplification. Performance assessment is further conducted by computing the LENS score of text simplifications generated on the test set. The output attaining the highest LENS score was selected for the final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Explored Model Architectures</head><p>In this section, we conducted tests on various language models and manually evaluated them to determine the most suitable model for the given task. We analyzed T5 and Alpaca LoRA 7B models.</p><p>Text-to-Text Transfer Transformer (T5) In our initial experimentation, we adopted T5, t5-base from Hugging Face, as a baseline model for the text simplification task. However, we encountered a significant limitation with this approach. The output generated by the T5 model was essentially identical to the input, which rendered it ineffective for our purposes. As a result, we swiftly rejected the idea of using T5 as our primary model.</p><p>Alpaca LoRA 7B Even without fine-tuning, the Alpaca LoRA 7B model demonstrated potential in our preliminary manual evaluations of the text simplification outputs. One example can be seen in Figure <ref type="figure" coords="6,168.17,273.45,3.76,10.91" target="#fig_2">3</ref>. Thus, we pursued further investigation and optimization of this model to potentially extend its usability in the context of text simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prompt Engineering</head><p>We conducted experiments using three distinct prompt templates: default, short, and simplification. The simplification prompt template aimed to tackle the challenge of explaining complex concepts in a way that is understandable to a 5-year-old child. By utilizing this prompt template, we sought to simplify the language and structure of the generated responses, making them more accessible and easier to comprehend. These templates were designed to provide specific instructions to guide the model's generation process and explore different approaches to improve the output quality. For a detailed list of the prompt templates used in our experiments and their corresponding instruction prompts, please refer to Appendix A.</p><p>Input snt_id: G06.2_2805209921_6: we have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects. Output default: Transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects. Output simplification: Transfer learning is applicable in this field, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Complex Terms Identification</head><p>First, we employ the model to identify complex terms within a given input text. We then retrieve definitions for these complex terms, providing a way to make them more understandable. Now we have a set of complex terms (e.g., transfer learning) and their definition. Next, we provide the model with these definitions as well as the original text to create a simplified version of the text. By integrating the explanations of complex concepts into the simplified text, we intend to minimize the need for prior knowledge, thus enhancing the text's understandability for a wider audience. The process is visualized in Figure <ref type="figure" coords="7,334.56,188.83,3.81,10.91" target="#fig_3">4</ref>. The idea of enhancing our model's performance by utilizing intermediate results draws inspiration from the Chain-of-Thought Prompting approach. This strategy significantly improved the outcomes of large language models in tasks like complex reasoning <ref type="bibr" coords="7,268.51,229.48,16.35,10.91" target="#b10">[11]</ref>. In the following section, we refer to this idea of using complex terms and their definitions as complex terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fine-Tuning</head><p>The fine-tuning process was carried out on numerous models, incorporating the two distinct datasets presented in Section 3 (default and extended), three varying prompt templates presented in Section 4.2 (default, short, and simplification), as well as a variety of hyperparameters.</p><p>The training was subjected to a series of hyperparameter tuning experiments. In this context, the number of epochs was varied, ranging from 3 to 10, to investigate the optimal duration for training to balance the trade-off between model performance and computational efficiency.</p><p>Another important aspect of the training was the learning rate scheduler. The use of the learning rate scheduler aimed to optimize the learning rate during the training process, adapting it based on the progress of the training. Two batch sizes, 32 and 64, were considered to observe their influence on the model's learning and performance. It's worth noting that a single training epoch typically took around 8 minutes, highlighting the computationally intensive nature of the procedure. Please refer to Appendix B for detailed information on the hardware used.</p><p>In addition to the aforementioned parameters, we evaluated the performance of various input prompts using the three given prompt templates. This involved experimenting with diverse instructions and task descriptions, as these factors can significantly influence the effectiveness of the model's training and eventual performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation of Fine-Tuned Models</head><p>The fine-tuned models from the previous section were evaluated using two different methods: (1. default) where text simplification was exclusively applied to the source sentence, following the default approach, or (2. complex terms) where the process involved starting with complex terms, providing explanations for those terms, and leveraging all intermediate results to simplify the text, as elaborated in Section 4.3. Evaluating the fine-tuned models on the default dataset should naturally align with the default method, and similarly, the extended dataset with the complex terms approach. Nevertheless, we tried an alternative, which involves blending these approaches, illustrated by the dotted line in Figure <ref type="figure" coords="8,316.53,154.71,3.74,10.91" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model Fusion</head><p>In our approach, we leverage the power of ensemble learning to tackle the task of text simplification. Our goal is to enhance the understandability and readability of text by combining the outputs of multiple models and selecting the best result for each sample. We do this with LENS <ref type="bibr" coords="8,116.77,245.09,11.43,10.91" target="#b3">[4]</ref>, which forms a critical aspect of our evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we elaborate on the findings from our experiments. We noted that some models, like T5 or specific configurations of the Alpaca LoRA model using the short template with the default dataset, showed inadequate performance during testing. The simplifications made by these models were either identical to the input or incomplete, with some instances resulting in the repeated use of the same word. Given these limitations, we chose not to further investigate or utilize these models in our project. One interesting result we observed was that our approach using the Alpaca LoRA 7B resulted in a 10% average reduction in sentence size. However, shorter sentences do not necessarily equate to better readability. The readability improvement rate was found to be within the range of 2%-5%. Like the T5 model, our fine-tuned models also had their share of imperfections. These included instances of incomplete translations, representation of summaries as bullet points, and in some cases, over-complication of the simplification process. The latter was particularly noticeable when the model using complex terms introduced too many definitions, which counterintuitively complicated the text rather than simplifying it. These are areas that require attention for improvement in subsequent iterations of our text simplification model.</p><p>The experimentation also revealed an intriguing trend where simpler methods often outperformed their more complex counterparts. For example, using the default dataset, coupled with fine-tuning via the simplification prompt template and evaluation using the default method, yielded a selection rate of over 50% in the model fusion phase. This showcases the potential effectiveness of simpler training and evaluation processes.</p><p>Fine-tuning with the extended dataset and evaluating using complex terms with the simplification prompt template resulted in a selection rate of over 20%. This further underlines the importance of customizing the training and evaluation processes based on the unique demands of text simplification tasks. This specific model stands out in simplifying and elaborating on complex terms. It provided detailed explanations which aided the simplification process. An example of this can be seen in Figure <ref type="figure" coords="8,252.09,642.44,3.66,10.91" target="#fig_0">1</ref>. Here, the model clarifies the concept of transfer learning before beginning the summarization. This example demonstrates how our model successfully adds new information to help to understand and simplify complex concepts.</p><p>The model also demonstrated proficiency in explaining abbreviations. For instance, it could expand www into World Wide Web (WWW) and p2p into peer-to-peer (P2P). This capability adds another layer of utility in simplifying and clarifying complex text by demystifying unfamiliar abbreviations for the reader.</p><p>However, not all approaches proved successful. The short prompt template, for instance, consistently underperformed across all provided configuration choices. This stresses the necessity to strike a balance between simplicity and effectiveness in text simplification tasks. Figure <ref type="figure" coords="9,500.81,168.26,5.17,10.91" target="#fig_4">5</ref> shows the selection rate of the models during the model fusion phase.</p><p>In addition to our evaluation, the official evaluation results utilizing established metrics such as SARI, BLEU, and FKGL can be found in Appendix C. These results offer a comprehensive assessment of the performance of our text simplification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Our approach, while promising in its outcomes, is subject to certain limitations that are essential to acknowledge. One of the issues is the occasional equivalence of input and output. While this does not increase the complexity of the input, it is not the outcome we desire. Ideally, a simplification model should always render an output that is less complex and more comprehensible than the input. However, our model does not consistently ensure this.</p><p>Furthermore, some cases require additional information to simplify a sentence effectively, such as providing a simpler explanation for a complex term or concept. We attempted this with our model using the extended dataset and complex terms for evaluation. However, these results might not be incorporated into the model fusion phase. This is because our model fusion phase struggles to automatically evaluate scenarios where the simplification significantly deviates from the original input due to the lack of a reliable metric. This limitation presents a challenge in delivering accurate text simplification when necessary.</p><p>Even with the useful LENS score as a tool for evaluating simplification, it carries inherent limitations, particularly when applied to unlabeled data. For example, instances of excessive explanatory information or elaboration can be misinterpreted as hallucinations by the LENS evaluator, consequently leading to inaccurately low scores. Moreover, when employing Large Language Models (LLMs), the issue of hallucinations continues to pose a significant challenge, underscoring an area that requires further exploration and methodological refinement. An additional limitation lies in the common misconception equating text simplification to summarization. While both share the aim of rendering complex information more digestible, they are distinct tasks. Summarization concentrates on shortening the text while retaining its core ideas, whereas simplification is dedicated to reducing complexity, which does not necessarily involve decreasing the text's length. This divergence in objectives introduces unique challenges not fully addressed by our present model.</p><p>Constraints concerning time and computational power further limit the exploration of diverse approaches and models. Identifying suitable evaluation metrics that accurately measure the effectiveness of text simplification also remains a substantial challenge. These factors underline the complexities and challenges involved in the quest for effective text simplification methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>The field of text simplification presents various challenges, but our research has revealed promising pathways. Simple approaches utilizing pretrained large language models, targeted prompts, and adapted training strategies can lead to significant strides towards more effective text simplification. The findings emphasize the importance of using the right techniques and prompts to find a balance between simplicity and effectiveness in text simplification tasks.</p><p>A successful text simplification process should aim to elucidate complex words that may be unfamiliar to the reader and expand abbreviations to ensure clarity. These elements are crucial in reducing textual complexity without compromising on the richness of the information being conveyed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Official Assessment</head><p>This section presents a detailed analysis of the official evaluation of the SimpleText CLEF shared task. All figures in this section show a graphical representation of the ranking of all submissions.</p><p>Figure <ref type="figure" coords="13,131.91,138.38,5.17,10.91" target="#fig_5">6</ref> shows the FKGL scores, a measure of text complexity. Here, our ensemble model outperforms all the models we submitted, achieving the lowest FKGL score. This result is expected given the way our model fusion works, which always selects the simplest version of our models.</p><p>Figure <ref type="figure" coords="13,130.64,192.57,4.97,10.91" target="#fig_6">7</ref> shows the SARI scores. Once again, our ensemble model stands out with the highest SARI score of all our submissions. As SARI is a critical criterion for our model fusion, this high score is not surprising. However, it is worth noting that models that clarify complex terms can deviate significantly from the target sentence, and consequently receive a lower score. This is despite the potential improvement in overall readability and comprehension.</p><p>Figures <ref type="figure" coords="13,135.31,260.32,5.04,10.91" target="#fig_7">8</ref> and<ref type="figure" coords="13,162.12,260.32,10.07,10.91" target="#fig_10">11</ref> show the BLEU scores and Levenshtein similarity scores respectively. Both metrics highlight a correlation between models that produce outputs that are very similar to their inputs. Since this pattern was observed in our models without fine-tuning or prompt engineering, it is expected that these models would also achieve the highest overall BLEU scores. It is clear from these figures that preserving the original sentence structure and content contributes significantly to higher scores.</p><p>In conclusion, our ensemble model, which is designed to incorporate the strengths of all our models, consistently shows superior performance across multiple evaluation metrics. However, it is important to keep in mind that optimal text simplification may involve an acceptable level of deviation from the original sentence, if it results in improved readability and comprehension for the intended audience.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,225.76,416.70,8.93;2,89.29,237.71,416.69,8.96;2,89.29,249.67,416.69,8.96;2,89.29,261.62,414.26,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of text simplification, comparing the different model approaches. The examples demonstrate how the extended model with the simplification prompt and complex words evaluation provides a detailed explanation of the concept transfer learning. In contrast, the default model with the simplification prompt and default evaluation provides a much simpler, yet less informative statement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,548.51,416.94,8.93;5,89.29,560.52,418.23,8.87;5,89.02,572.47,417.14,8.87;5,89.29,584.38,416.70,8.96;5,89.29,596.33,416.69,8.96;5,89.29,608.34,416.69,8.87;5,89.29,620.29,416.69,8.87;5,89.29,632.25,134.41,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic depiction of the training architecture. Input data, originating either from Task 3 (default) or a combination of Task 2 and 3 (extended), is paired with predefined prompt templates.The models are then fine-tuned using a wide variety of combinations of the hyperparameters. After fine-tuning, these models are evaluated on two tasks: (1. default) direct text simplification or (2. complex terms) a combination of intermediate outcomes from Task 2 (identification and explanation of challenging terms) with text simplification. Performance assessment is further conducted by computing the LENS score of text simplifications generated on the test set. The output attaining the highest LENS score was selected for the final submission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,607.63,416.69,8.96;6,89.29,619.64,418.22,8.87;6,89.29,631.55,416.69,8.96;6,89.29,643.55,159.18,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of Alpaca LoRA 7B model's text simplification output without fine-tuning. default prompt template shows the model's output with a default prompt which is nearly identical to the input. simplification prompt template presents a simplified version, showing the model's potential for text simplification without any fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,629.05,416.69,8.93;7,89.29,641.05,416.70,8.87;7,89.29,653.01,29.30,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the complex term identification pipeline. The process involves identifying and explaining difficult terms first and then performing the text simplification task using intermediate results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,89.29,649.99,286.28,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Selection rate of the models during the model fusion phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,89.29,591.68,416.69,8.93;14,89.29,603.68,416.69,8.87;14,89.29,615.64,82.23,8.87;14,89.29,131.46,416.70,452.78"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: FKGL (Flesch-Kincaid Grade Level) scores for the text simplification models. The FKGL metric measures the grade level required to understand the text, with lower scores indicating simpler and more accessible language.</figDesc><graphic coords="14,89.29,131.46,416.70,452.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,89.29,585.70,416.69,8.93;15,89.29,597.70,416.70,8.87;15,89.29,609.66,416.70,8.87;15,89.29,621.61,102.37,8.87;15,89.29,125.49,416.70,452.78"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: SARI (System-level Automatic Evaluation Metric for Text Simplification) scores for text simplification models. Higher values indicate better quality simplifications. SARI is a popular metric that evaluates the overall quality of text simplification by comparing the generated simplified text to reference simplifications.</figDesc><graphic coords="15,89.29,125.49,416.70,452.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,89.29,588.68,416.70,8.93;16,89.29,600.69,416.88,8.87;16,89.29,612.65,138.89,8.87;16,89.29,134.46,416.71,446.80"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: BLEU (Bilingual Evaluation Understudy) scores for the text simplification models. BLEU measures the overlap between the generated simplified text and reference simplifications, with higher scores indicating better similarity.</figDesc><graphic coords="16,89.29,134.46,416.71,446.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="17,89.29,590.57,416.69,8.93;17,89.29,602.57,416.70,8.87;17,89.29,614.53,127.17,8.87;17,89.29,132.57,416.69,450.57"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Compression ratios for the text simplification models. Compression ratio measures the reduction in sentence length achieved by the text simplification models, with higher values indicating more significant simplification.</figDesc><graphic coords="17,89.29,132.57,416.69,450.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="18,89.29,590.57,416.69,8.93;18,89.29,602.57,416.69,8.87;18,89.29,614.53,212.46,8.87;18,89.29,132.57,416.69,450.57"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Number of sentence splits for the text simplification models. Sentence splits measure the extent to which the original sentences were divided during the simplification process, with lower values indicating better preservation of sentence structure.</figDesc><graphic coords="18,89.29,132.57,416.69,450.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="19,89.29,590.57,416.94,8.93;19,89.29,602.57,416.69,8.87;19,89.29,614.53,217.96,8.87;19,89.29,132.57,416.69,450.57"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Levenshtein similarity scores for the text simplification models. Levenshtein similarity measures the similarity between the generated simplified text and the original text, with higher values indicating better preservation of the original content.</figDesc><graphic coords="19,89.29,132.57,416.69,450.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="20,89.29,586.54,416.69,8.93;20,89.29,598.55,416.70,8.87;20,89.29,610.50,203.49,8.87;20,89.29,136.60,416.69,442.51"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Percentage of exact copies for the text simplification models. This metric measures the extent to which the generated simplified text is an exact copy of the original text, with lower values indicating better paraphrasing and simplification.</figDesc><graphic coords="20,89.29,136.60,416.69,442.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="21,89.29,588.68,416.69,8.93;21,89.29,600.69,416.70,8.87;21,89.29,612.65,167.97,8.87;21,89.29,134.46,416.71,446.80"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Proportion of additions for the text simplification models. This metric measures the extent to which additional information was introduced during the simplification process, with lower values indicating better adherence to simplicity.</figDesc><graphic coords="21,89.29,134.46,416.71,446.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="22,89.29,590.57,416.69,8.93;22,89.29,602.57,416.70,8.87;22,89.29,614.53,173.85,8.87;22,89.29,132.57,416.69,450.57"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Proportion of deletions for the text simplification models. This metric measures the extent to which unnecessary or redundant information was removed during the simplification process, with lower values indicating better conciseness.</figDesc><graphic coords="22,89.29,132.57,416.69,450.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="23,89.29,588.68,416.70,8.93;23,89.29,600.69,416.87,8.87;23,89.29,612.65,123.65,8.87;23,89.29,134.46,416.71,446.80"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Lexical complexity scores for the text simplification models. This metric measures the complexity of the vocabulary used in the generated simplified text, with lower scores indicating simpler and more accessible language.</figDesc><graphic coords="23,89.29,134.46,416.71,446.80" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to take this opportunity to express our deepest gratitude and sincere appreciation to <rs type="person">Andrianos Michail</rs>, and <rs type="person">Simon Clematide</rs> for their unwavering support and constructive guidance during the creation of this paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prompt Engineering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Templates</head><p>Default Default prompt template provided by Alpaca-LoRA <ref type="bibr" coords="11,368.43,394.93,11.56,10.91" target="#b0">[1]</ref>:</p><p>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</p><p>Short Short prompt template without any instruction:</p><p>Simplification Modified prompt template specifically for simplification tasks:</p><p>Below is an instruction that describes a simplification task, paired with an input that provides further context. Write a simple response that appropriately completes the request. Write your response as you would talk to a 5-year-old. ### Instruction: {instruction} ### Input: {input} ### Response:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Instructions</head><p>This section shows the prompts for the two datasets used in this paper. The instructions are interpolated in the previously provided template.</p><p>Default For the default evaluation process, we used a simple instruction prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simplify the following sentence</head><p>Complex Terms For the complex terms evaluation, we used chained the model using the following two instruction prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">To identify the difficult terms:</head><p>Decide which terms (up to 5) require explanation and contextualization to help a reader understand a complex scientific text 2. To obtain definitions for the previously identified terms:</p><p>Provide a short (one/two sentence) explanations/definitions for the detected difficult terms: {term} in the context of the following sentence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Environment</head><p>This section provides an overview of the system configuration and software dependencies employed in the development and implementation of our research system.</p><p>To fine-tune and perform inference with the model, 8 cores 32 GB RAM with a 16 GB Tesla T4 GPU was utilized, providing the necessary computational resources. The model we used is chainyo/alpaca-lora-7b from Hugging Face, which is a LLaMA-7B fine-tuned model on the Stanford Alpaca cleaned version dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,533.83,322.22,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,163.70,533.83,49.75,10.91">Alpaca-lora</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/tloen/alpaca-lora" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.38,394.53,10.91;10,112.66,560.93,393.33,10.91;10,112.66,574.48,163.72,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m" coord="10,288.90,560.93,217.08,10.91;10,112.66,574.48,31.72,10.91">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.02,393.33,10.91;10,112.66,601.57,395.17,10.91;10,112.66,615.12,394.53,10.91;10,112.66,628.67,393.98,10.91;10,112.66,642.22,393.33,10.91;10,112.66,655.77,318.59,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,449.73,588.02,56.26,10.91;10,112.66,601.57,325.17,10.91">Overview of simpletext -clef-2023 track on automatic simplification of scientific texts</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,460.47,601.57,47.36,10.91;10,112.66,615.12,24.11,10.91;10,112.66,642.22,393.33,10.91;10,112.66,655.77,288.64,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michalis</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,192.91,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heineman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09739</idno>
		<title level="m" coord="11,307.55,86.97,198.43,10.91;11,112.66,100.52,60.91,10.91">Lens: A learnable evaluation metric for text simplification</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,394.61,10.91;11,112.28,127.61,384.24,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<title level="m" coord="11,450.38,114.06,56.89,10.91;11,112.28,127.61,254.47,10.91">Self-instruct: Aligning language model with self generated instructions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.61,10.91;11,112.66,154.71,293.51,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m" coord="11,438.13,141.16,68.13,10.91;11,112.66,154.71,161.51,10.91">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.33,10.91;11,112.66,181.81,274.49,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,212.82,168.26,293.17,10.91;11,112.66,181.81,43.31,10.91">Hulat-uc3m at simpletext@ clef-2022: Scientific text simplification using bart</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.95,181.81,191.28,10.91">Proceedings of the Working Notes of CLEF</title>
		<meeting>the Working Notes of CLEF</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,394.53,10.91;11,112.66,208.91,393.33,10.91;11,112.48,222.46,264.75,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,112.66,208.91,363.77,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,488.38,208.91,17.60,10.91;11,112.48,222.46,170.67,10.91">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,394.53,10.91;11,112.66,249.56,394.04,10.91;11,112.66,263.11,115.72,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,112.66,249.56,246.92,10.91">Stanford alpaca: An instruction-following llama model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,386.32,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Openai</forename><surname>Openai</surname></persName>
		</author>
		<ptr target="https://platform.openai.com/docs/models/models" />
		<title level="m" coord="11,190.68,276.66,29.71,10.91">Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.33,10.91;11,112.66,303.75,368.60,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m" coord="11,427.04,290.20,78.95,10.91;11,112.66,303.75,236.60,10.91">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
