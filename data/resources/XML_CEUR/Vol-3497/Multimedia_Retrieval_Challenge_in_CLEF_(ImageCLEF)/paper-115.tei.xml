<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.68,15.42;1,89.29,106.66,252.34,15.42">Efficient Fusion Techniques for Result Diversification and Image Interestingness Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,127.97,11.96"><forename type="first">Prabavathy</forename><surname>Balasundaram</surname></persName>
							<email>prabavathyb@ssn.edu.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">UG Student</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.41,134.97,60.74,11.96"><forename type="first">G</forename><surname>Gnana Sai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">UG Student</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,80.55,11.96"><forename type="first">Makesh</forename><surname>Vaibhav</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">UG Student</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.00,148.92,119.88,11.96"><forename type="first">Naren</forename><surname>Srinivasan Murali</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">UG Student</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.39,148.92,109.88,11.96"><forename type="first">Parlapalli</forename><surname>Sai Harshith</surname></persName>
							<email>saiharshith2110036@ssn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">UG Student</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.68,15.42;1,89.29,106.66,252.34,15.42">Efficient Fusion Techniques for Result Diversification and Image Interestingness Tasks</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">2DA8FFA3A7BAA70117B568E8EEE122C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Result diversification</term>
					<term>Image interestingness</term>
					<term>Inducer fusion</term>
					<term>Machine learning techniques</term>
					<term>Ensemble Machine learning techniques</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Result diversification aims to retrieve a set of images that is both relevant and diverse, effectively capturing the essence of a given query. Image interestingness aims to fulfill the need for accurately assessing and predicting the level of interest in images, enabling better user experience and content organization. These two tasks can use inducer fusion, which combines the outputs of multiple inducers to improve the accuracy and robustness of prediction models. In this work, independent and ensemble ML techniques were used to solve the challenges in inducer fusion. Experimental validation was carried out on Result diversification and Image interestingness datasets of ImageCLEF2023-Fusion task. our research contributes to advancing the field of inducer fusion and improving the performance of result diversification and image interestingness tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the exponential growth of digital imagery on the internet, effective image retrieval systems have become indispensable for users seeking visual content. Traditional image search engines primarily rely on content-based features and textual metadata to generate a ranked list of visually similar images. However, this approach often falls short in providing diverse search results, leading to redundancy and limited exploration of the search space. The <ref type="bibr" coords="1,409.23,493.55,12.62,10.91" target="#b0">[1]</ref>- <ref type="bibr" coords="1,426.06,493.55,12.62,10.91" target="#b1">[2]</ref>-diversification task was introduced to address this limitation and encourage the development of techniques that enhance result diversification.</p><p>The proliferation of visual content on various platforms necessitates effective techniques for predicting image interestingness. Accurately determining the level of interestingness associated with images holds immense value in applications such as image search, recommendation systems, and content curation. The ability to automatically rank and retrieve interesting images not only enhances user satisfaction but also streamlines information retrieval processes. In recent years, substantial progress has been made in the development of computational models and techniques for image interestingness prediction. However, the diverse and subjective nature of interestingness poses significant challenges. To address these challenges, Constantin et al <ref type="bibr" coords="2,89.29,154.71,13.00,10.91" target="#b8">[9]</ref> introduced the Interestingness10k dataset, which serves as a standardized benchmark for evaluating image interestingness prediction methods.</p><p>This paper presents a study on result diversification and image interestingness predictions using fusion techniques. Furthermore, the research objective is to investigate the effectiveness of inducer fusion, a technique that combines the outputs of multiple inducers, in enhancing prediction performance. Inducer fusion aims to leverage the strengths of individual inducers and mitigate their weaknesses, ultimately resulting in a more accurate and robust prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The existing work related to Result diversification and Image Interestingness are summarised below.</p><p>Hai-tao, Yu et al <ref type="bibr" coords="2,173.91,335.28,12.74,10.91" target="#b2">[3]</ref> proposes a framework dubbed MO4SRD for search result diversification. While the current methods in use rely on a sequential selection procedure, MO4SRD suggests a score-and-sort approach based on direct metric optimization. It represents the diversity score of each document using probability distributions, enabling the development of different variations of diversity metrics. A probabilistic neural scoring function that takes into account cross-document interaction and permutation equivariance is incorporated into the system. MO4SRD is tested on the four standard test datasets released in the diverse tasks of TREC Web Track from 2009 to 2012 which suggests that it performs better than the current approaches. Shreya Sriram et al <ref type="bibr" coords="2,185.98,443.67,12.68,10.91" target="#b3">[4]</ref> proposed an ensembled approach for web search result diversification using neural network models. The data was obtained from the Retrieving Diverse Social Images Task dataset. Different networks namely, Multilayer Perceptron, Ridge Regressor using Grid Search and Keras Regressor using Sequential model were ranked based on MAE. These ranks and the models were fed as input for the Voting Regressor. The performance of the voting regressor can again be measured with MAE. Among the ten best submissions done to ImageClef 2022, the best F1 score and CR score were 0.5604 and 0.4384 respectively.</p><p>Lekshmi Kalinathan et al <ref type="bibr" coords="2,210.65,538.52,12.69,10.91" target="#b4">[5]</ref> presented a fusion approach for web search result diversification using machine learning algorithms. The data was obtained from the Retrieving Diverse Social Images Task dataset. A voting regressor of three predictor models K Nearest Regressor, Decision Tree Regressor and SVM was used to predict the similarity scores of the models in the validation dataset. Of the 10 best submissions done to ImageClef 2022, the best F1 score and CR score were found to be 0.5634 and 0.4414 respectively.</p><p>Maria, Shoukat et al <ref type="bibr" coords="2,192.62,619.81,12.87,10.91" target="#b5">[6]</ref> presented investigation on predicting media interestingness scores using a novel late fusion framework. The individual inducers' scores are extracted from the Interestingness10k dataset which are provided by the task organizers. The proposed framework combines multiple algorithms and employs two fusion strategies: naive fusion and merit-based fusion. The results revealed that the proposed late fusion framework consistently outperformed alternative approaches, exhibiting superior predictive accuracy and robustness. Overall, this paper offers a comprehensive exploration of media interestingness prediction, providing a valuable contribution to the existing literature.</p><p>Ying, Dai et al <ref type="bibr" coords="3,166.16,141.16,12.87,10.91" target="#b6">[7]</ref> have proposed two image interestingness models with different convolutional neural network architectures and improves on their image aesthetic score (AS) prediction by an ensemble. The models are trained on two datasets, CUHK-PQ and XihAA datasets. One model extracts the subject of the image for predicting the image's aesthetic score, and the other extracts the holistic composition for the prediction. It is found that these models trained on the XiheAA dataset seem to learn the latent photography principles, though it cannot be said that they learn the aesthetic sense. The aggregated model improves the F1 value by 5.4% and 33.1% compared to the first and second model respectively.</p><p>V. Kalakota et al <ref type="bibr" coords="3,176.73,249.56,13.00,10.91" target="#b7">[8]</ref> proposed a model to retrieve diverse images of a particular landmark location that cover different aspects of a query. Images required are obtained from the Flickr Div150Cred dataset. Flickr Baseline Ranking Algorithm and a re-ranking strategy are applied to retrieve the most relevant images out of all the possible set of images using the provided textual metadata. A fusion-based strategy is employed to ensemble several cluster models and a final summary of the query location is produced by selecting images from different clusters. The model is evaluated based on P@10, CR@10, F1@10, P@20, CR@20 and F1@20. The proposed method achieved a start-of-the-art performance on precision scores and F1 Score for images retrieved 30 and above. Cluster Recall scores still need slight improvement for 10 or 20 images being retrieved. Future work will be devoted to improving cluster recall metric without affecting the initial precision scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Dataset Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Result Diversification</head><p>The dataset used for this task is extracted from <ref type="bibr" coords="3,307.60,464.64,11.59,10.91" target="#b1">[2]</ref>. The data corresponds to the Retrieving Diverse Social Images Task dataset <ref type="bibr" coords="3,243.38,478.19,16.09,10.91" target="#b9">[10]</ref>. An inducer is a model which predicts images related to a query. The outputs from 56 inducers, representing a total of 123 queries are split into devset (56 inducers for 60 queries) for training and testset (56 inducers for 63 queries) for testing. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1 Attributes of Inducer File for Result Diversification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field Representation</head><p>query id represents the unique id of the query photo id the unique id of the photo represented by the entry rank rank of the photo sim similarity score of the photo to the query run name a general name for the inducer task is to diversify the results of image search. This fusion task is a retrieval task, where the similarity scores of each image with the query is generated. Each entry or row in these files is of the format as given below in the Table <ref type="table" coords="4,274.65,100.52,3.74,10.91">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Media Interestingness</head><p>The Media interestingness fusion task corresponds to the problem of predicting the interestingness of a particular image. An inducer is responsible to determine the interestingness of the given images. The output of the inducer consists of the relevant images, their interestingness classification and score. However, a single inducer is disadvantageous for application in certain areas due to low precision and lack of performance. To tackle this problem, ensembling, a technique that aggregates the predictions of several inducers, is used. The ensembled system is expected to be superior when compared to the highest-performing individual inducer. The data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodologies used</head><p>Different machine learning algorithms like Elastic net, Gradient Boosting Regressor, Decision Tree were employed for the result diversification task and XGBoost Classifier, k-Nearest Neighbors Classifier and Decision Tree were employed for the image interestingness task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">XGBoost Classifier</head><p>XGBoost is an efficient machine learning algorithm known for its ensembling-based approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Elastic Net</head><p>The Elastic Net is a regression technique that combines L1 (Lasso) and L2 (Ridge) regularisation methods to achieve a balance between feature selection and feature grouping. The model introduces two hyperparameters, alpha and l1 ratio, which control the extent of L1 and L2 regularisation applied during training. By adjusting these hyperparameters, the Elastic Net model can effectively handle both feature selection and grouping, resulting in more accurate and interpretable regression models. After training the model on the provided hyperparameters, predictions are made on the test set. The performance of the model is evaluated using mean absolute error (MAE).This metric provides insights into the accuracy and goodness of fit of the Elastic Net model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Gradient Boosting</head><p>The Gradient Boosting is a powerful machine learning algorithm used for regression tasks. It combines multiple weak predictive models like decision trees in an ensemble to make accurate predictions. The algorithm works by sequentially fitting the models to the residuals of the previous model, allowing it to gradually improve its performance by focusing on the remaining errors. This iterative process effectively captures complex patterns and relationships in the data. The Gradient Boosting Regressor utilises gradient descent optimization to minimise a loss function, such as mean squared error, and find the best fitting model. The algorithm also incorporates regularisation techniques to prevent over-fitting and enhance generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Decision Tree</head><p>The decision tree algorithm is a powerful supervised learning method that constructs a tree-like model as shown in Figure <ref type="figure" coords="5,212.31,554.80,3.81,10.91" target="#fig_0">1</ref>. In this model, internal nodes represent features or attributes, branches represent decision rules, and leaf nodes correspond to predicted values. The algorithm initiates by selecting the best attribute to split the dataset, evaluating various attributes and measuring their impact on reducing the target variable's impurity. This attribute selection process is recursively applied to subsets of data until a predefined stopping criterion is satisfied. Upon constructing the tree, each leaf node is assigned a predicted value based on the average of the target variable. This enables the model to make predictions on new, unseen instances by traversing the tree from the root node to a leaf node, guided by the instance's attribute values.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Result Analysis of Fusion Technique for Result Diversification Task</head><p>This section discusses about the implementation fusion techniques with the analysis of the results using evaluation metrics namely Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>The inducers' data contains information about query_id, inter, photo_id, rank, sim and run_name. The rank and sim are extracted from the inducers data. The missing values of the rank and sim attributes are filled using the SimpleImputer method. The data is then split into 80% training set and 20% testing set. Three regression models M1, M2 and M3 are built to study how the similarity scores are assigned. The model M1 is implemented using the ElasticNet Regressor where the alpha parameter controls the regularization strength. It helps to prevent overfitting by shrinking the coefficients towards zero. The l1_ratio parameter determines the balance between the L1 and L2 penalties. If l1_ratio is 1 it indicates L1 regularization (Lasso) and if it 0 it indicates L2 regularization (Ridge). If the value is in between 0 and 1, it represents a combination of both the penalties. The model is trained on the training data using the fit method, which estimates the coefficients that best fit the data.</p><p>The model M2 is implemented using GradientBoostingRegressor which calculates the gradients of the loss function with respect to the predictions made by the weak learners. In the code, the gradients are implicitly computed during the training process of the GradientBoostingRegressor.</p><p>The model M3 is implemented using Decision Tree Regressor. The fit (X,y) method is used to train the Decision Tree Regressor on the given training data. The predict(X) method is used to make predictions on new data using the trained Decision Tree Regressor.</p><p>The model M4 is implemented using Voting Regressor. The VotingRegressor model is created by passing the base models M1, M2, M3 as estimators to the 'VotingRegressor' class. The voting method used is 'hard', which means the final prediction is based on the majority vote of the base models.</p><p>The ensemble model is created using the StackingRegressor from scikit-learn. The estimators are defined as a list of tuples, where each tuple contains the name of the models M1, M2, and M3. The final estimator, which is the gradient boosting regressor, is used to build a meta model. An ensemble model M5 is obtained from the models M1, M2, and M3 using the final estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and discussion</head><p>The built models M1, M2, M3, M4, and M5 are used to classify the test dataset. The predicted values are compared with the actual values and various evaluation metrics are computed as shown in Table <ref type="table" coords="8,161.65,367.03,5.17,10.91" target="#tab_2">3</ref> to assess the performance of these models. These metrics provide insights into the model's ability to correctly classify and predict the correct results based on rank and similarity scores. After analysing the results for various evaluation metric in the Table <ref type="table" coords="8,481.55,394.13,5.15,10.91" target="#tab_2">3</ref> it is clear that M1 model is the yielding best results among all the model. The M1 model has been tested with the CLEF test data and the F1@20 and CR@20 metrics are used to compare and analyze the performance of the results. F1@20 combines precision and recall into a single score, providing a balanced measure of the system's performance. CR@20 calculates the proportion of relevant items or documents that are retrieved within the top 20 ranked results. A higher CR@20 score indicates a system's ability to retrieve more relevant items within the top-ranked results. An F1@20 of 0.5708 and CR@20 of 0.449 is obtained in the top 10 results. Table <ref type="table" coords="9,206.25,86.97,5.17,10.91" target="#tab_3">4</ref> illustrates the F1@20 and CR@20 evaluated for the 10 best file submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Result Analysis of Fusion Technique for Image Interestingness Task</head><p>This section discusses about the implementation fusion techniques with the analysis of the results using evaluation metrics namely Accuracy, Precision, Recall, F1 score, Mean Absolute Error, Balanced Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation</head><p>The inducers' data, contains information about video and image identifiers, classification labels, and interestingness scores. The interestingness scores and classification labels are extracted from the inducers' data. The interestingness scores are stored in a numpy array, while the classification labels are stored in a separate array. The data is then split into 80% training dataset and 20% testing dataset. Three classifier models M1, M2, and M3 were built to study the nature of classification of the images. The M1 classifier is implemented using the XGBoost algorithm with grid search to find the best combination of hyperparameters such as 𝑚𝑎𝑥𝑑𝑒𝑝𝑡ℎ, 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒, and 𝑛𝑒𝑠𝑡𝑖𝑚𝑎𝑡𝑜𝑟𝑠. The GridSearchCV function from sklearn is used to perform the grid search, with the F1 score as the evaluation metric.</p><p>The M2 classifier is implemented using the decision tree algorithm with grid search to find the optimal combination of hyperparameters such as 𝑚𝑎𝑥𝑑𝑒𝑝𝑡ℎ, 𝑚𝑖𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑠𝑝𝑙𝑖𝑡, and 𝑚𝑖𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑙𝑒𝑎𝑓 .</p><p>The M3 classifier is implemented using the K-nearest neighbours algorithm with grid search to find the optimal combination of hyperparameters such as 𝑛𝑛𝑒𝑖𝑔ℎ𝑏𝑜𝑟𝑠, weights, and p, representing the number of neighbors, the weight function used in prediction, and the power parameter for the Minkowski distance, respectively.</p><p>A Voting Classifier is created with all the models M1, M2, and M3. A grid search is performed to find the optimal combination of the voting scheme and weights. The best Voting Classifier model (M4) is obtained based on the grid search results.</p><p>The ensemble model is created using the StackingClassifier from scikit-learn. The estimators are defined as a list of tuples, where each tuple contains the name of the models M1, M2, and M3 and the corresponding best model instance. The final estimator, which is the decision tree classifier, is used to build a meta model. An ensemble model M5 is obtained from the models M1, M2, and M3 using the final estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results and discussion</head><p>The built models M1, M2, M3, M4, and M5 are used to classify the test dataset. The predicted labels are compared with the actual labels and various evaluation metrics are computed as shown in Table <ref type="table" coords="10,161.65,312.83,5.17,10.91" target="#tab_4">5</ref> to assess the performance of these models. These metrics provide insights into the model's ability to correctly classify and predict the interestingness of media content. After analyzing the results for various evaluation metrics in the Table <ref type="table" coords="10,402.26,339.93,5.08,10.91" target="#tab_4">5</ref> it is clear that the M4 model is the yielding best results among all the models. The M4 model has been tested with the CLEF test data and MAP@10 metric is used to compare and analyze the performance of the results. The Mean Average Precision at 10 ranges from 0 to 1, where a higher value indicates better performance. It considers the order and relevance of the recommended items, giving more weight to relevant items appearing at higher positions in the recommendations. A MAP@10 of 0.1331 is obtained in the top 10 results. Table <ref type="table" coords="10,89.29,598.64,5.07,10.91" target="#tab_5">6</ref> illustrates the MAP@10 score evaluated for the 10 best file submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In order to improve the predictions of the results of the inducers in the result diversification task, three base regressors and two ensemble models were implemented. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,250.34,97.95,8.93;6,151.80,84.19,291.68,153.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decision Tree</figDesc><graphic coords="6,151.80,84.19,291.68,153.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,565.94,111.44,8.93;6,89.29,412.52,416.70,141.08"><head>Figure 2 :Regressor 4 . 7 .</head><label>247</label><figDesc>Figure 2: Voting Regressor</figDesc><graphic coords="6,89.29,412.52,416.70,141.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,362.37,106.59,8.93;7,161.66,136.78,271.95,201.08"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stack Ensemble</figDesc><graphic coords="7,161.66,136.78,271.95,201.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,406.58,349.96,99.40,10.91;11,89.29,363.51,416.69,10.91;11,89.29,377.06,416.69,10.91;11,89.29,390.61,416.69,10.91;11,89.29,404.16,416.69,10.91;11,89.29,417.71,417.90,10.91;11,89.29,431.25,290.14,10.91;11,100.20,444.80,405.78,10.91;11,89.29,458.35,416.69,10.91;11,89.29,471.90,416.69,10.91;11,89.29,485.45,417.89,10.91;11,89.29,499.00,418.37,10.91;11,88.96,512.55,417.02,10.91;11,89.29,526.10,416.69,10.91;11,89.29,539.65,66.10,10.91"><head></head><label></label><figDesc>The model was trained on data from 56 different inducers, containing 134,400 training values and tested on data from 56 inducers, containing 33,600 testing values. The base regressors obtained RMSE values of 0.0070, 0.1274 and 0.0360 each. The ensemble models obtained RMSE scores of 0.0445, and 0.1287 each. The best model is chosen based on the RMSE score. The model is then used to predict the values of 56 inducers containing 176,400 values and among the ten best submissions, the best F1 score and CR score are 0.5708 and 0.4295 respectively. In order to improve the predictions of the results of the inducers in the image interestingness task, three base classifiers and two ensemble models were implemented. The model was trained on data from 29 different inducers, containing 43,546 training values and tested on data from 29 inducers, containing 10,886 testing values. The base classifiers obtained Accuracy of 0.8705, 0.8637 and 0.8691 each. The ensemble models obtained Accuracy of 0.8756, and 0.8461 each. The best model is chosen based on the Accuracy. The model is then used to predict the values of 29 inducers containing 16,182 values and among the ten best submissions, the best MAP@10 score is 0.1331.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.96,259.58,418.70,204.78"><head>Table 2</head><label>2</label><figDesc>Attributes of Inducer File for Media Interestingness</figDesc><table coords="4,164.36,303.10,266.56,74.17"><row><cell>Field</cell><cell>Representation</cell></row><row><cell>video id</cell><cell>the id of the video from which image is extracted</cell></row><row><cell>image id</cell><cell>the id of the image</cell></row><row><cell cols="2">classification classification of the image</cell></row><row><cell>score</cell><cell>the interestingness score of the image</cell></row><row><cell>run name</cell><cell>a general name for the inducer</cell></row></table><note coords="4,89.29,412.81,418.37,10.91;4,88.96,426.35,417.02,10.91;4,89.29,439.90,416.69,10.91;4,89.29,453.45,105.87,10.91"><p><p><p><p><p>for this task is extracted and corresponds to the Interestingness10k dataset</p>[Constantin2021b]</p>. The output data from 29 inducers, representing visual interestingness predictions for 2435 images, is stored in separate text files for each inducer. Each entry of these files is as per the format given in Table</p>2</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,89.29,601.84,418.54,78.65"><head></head><label></label><figDesc>-Nearest Neighbors (k-NN) is a machine learning algorithm used for classification and regression tasks. The k-Nearest Neighbours in the training set are taken into account for predicting the value or class of a new instance. In terms of classification, it chooses the neighbor's majority class, and in terms of regression, it takes the average of those values. The bias-variance trade-off and complexity of the model are influenced by the choice of k. Since it is non-parametric, k-NN can be applied to a variety of situations, although it is sensitive to irrelevant features and distance metrics.</figDesc><table coords="4,89.29,601.84,418.54,78.65"><row><cell>4.2. K-Nearest Neighbors Classifier</cell></row><row><cell>It combines multiple decision tree models sequentially, leveraging gradient boosting to im-</cell></row><row><cell>prove predictions continuously by addressing errors made by previous trees. XGBoost avoids</cell></row><row><cell>overfitting and provides a range of hyperparameters for optimisation through regularisation</cell></row><row><cell>approaches. It has advanced capabilities like handling missing values and parallel processing</cell></row><row><cell>and can handle large-scale datasets effectively. Metrics including accuracy, precision, recall,</cell></row><row><cell>and F1-score are used in evaluation</cell></row></table><note coords="5,89.29,107.54,4.76,10.91"><p>k</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.98,436.68,312.11,117.69"><head>Table 3</head><label>3</label><figDesc>Assessment of Models using Evaluation Metrics</figDesc><table coords="8,194.19,480.26,206.90,74.12"><row><cell>Model</cell><cell>MAE</cell><cell>MSE</cell><cell>RMSE</cell></row><row><cell>ElasticNet(M1)</cell><cell cols="3">0.0039 0.0004 0.0070</cell></row><row><cell cols="4">Gradient Boosting(M2) 0.0314 0.0162 0.1274</cell></row><row><cell>Decision Tree(M3)</cell><cell cols="3">0.0002 0.0013 0.0360</cell></row><row><cell>Voting(M4)</cell><cell cols="3">0.0114 0.0019 0.0445</cell></row><row><cell>Stacking(M5)</cell><cell cols="3">0.0315 0.0165 0.1287</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,127.07,263.10,175.25"><head>Table 4</head><label>4</label><figDesc>Test results for Result Diversification</figDesc><table coords="9,243.19,168.37,108.90,133.95"><row><cell cols="3">Run F1@20 CR@20</cell></row><row><cell>1</cell><cell>0.4993</cell><cell>0.3901</cell></row><row><cell>2</cell><cell>0.5183</cell><cell>0.3992</cell></row><row><cell>3</cell><cell>0.5383</cell><cell>0.428</cell></row><row><cell>4</cell><cell>0.547</cell><cell>0.4137</cell></row><row><cell>5</cell><cell>0.5623</cell><cell>0.439</cell></row><row><cell>6</cell><cell>0.5408</cell><cell>0.4295</cell></row><row><cell>7</cell><cell>0.5708</cell><cell>0.449</cell></row><row><cell>8</cell><cell>0.5364</cell><cell>0.4165</cell></row><row><cell>9</cell><cell>0.5404</cell><cell>0.4215</cell></row><row><cell>10</cell><cell>0.5343</cell><cell>0.414</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.98,382.49,383.98,129.65"><head>Table 5</head><label>5</label><figDesc>Assessment of Models using Evaluation Metrics</figDesc><table coords="10,114.84,414.05,358.13,98.08"><row><cell>Model</cell><cell>Accuracy</cell><cell>Precision</cell><cell>F1 score</cell><cell>Mean per</cell><cell>Balanced</cell></row><row><cell></cell><cell></cell><cell>score</cell><cell></cell><cell>class error</cell><cell>accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>score</cell></row><row><cell>XGBoost(M1)</cell><cell>0.8705</cell><cell>0.8450</cell><cell>0.6826</cell><cell>0.1295</cell><cell>0.3637</cell></row><row><cell>DT (M2)</cell><cell>0.8637</cell><cell>0.8006</cell><cell>0.6763</cell><cell>0.1363</cell><cell>0.3924</cell></row><row><cell>kNN(M3)</cell><cell>0.8691</cell><cell>0.8261</cell><cell>0.6849</cell><cell>0.1309</cell><cell>0.3794</cell></row><row><cell>Voting(M4)</cell><cell>0.8756</cell><cell>0.8801</cell><cell>0.6887</cell><cell>0.1244</cell><cell>0.3428</cell></row><row><cell>Stacking (M5)</cell><cell>0.8461</cell><cell>0.7430</cell><cell>0.6394</cell><cell>0.1539</cell><cell>0.4091</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,88.99,90.49,245.48,177.47"><head>Table 6</head><label>6</label><figDesc>Test results for Image Interestingness</figDesc><table coords="11,260.81,134.01,73.66,133.95"><row><cell cols="2">Run MAP@10</cell></row><row><cell>1</cell><cell>0.0667</cell></row><row><cell>2</cell><cell>0.1331</cell></row><row><cell>3</cell><cell>0.1252</cell></row><row><cell>4</cell><cell>0.1303</cell></row><row><cell>5</cell><cell>0.0977</cell></row><row><cell>6</cell><cell>0.0666</cell></row><row><cell>7</cell><cell>0.0603</cell></row><row><cell>8</cell><cell>0.0666</cell></row><row><cell>9</cell><cell>0.096</cell></row><row><cell>10</cell><cell>0.0969</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,598.27,393.33,10.91;11,112.28,611.82,393.70,10.91;11,112.66,625.37,395.17,10.91;11,111.81,638.92,394.18,10.91;11,112.66,652.47,395.17,10.91;11,112.66,666.02,394.53,10.91;12,112.66,86.97,244.33,10.91;12,353.69,92.12,1.32,5.98;12,356.99,86.97,150.18,10.91;12,112.66,100.52,394.62,10.91;12,112.66,114.06,394.55,10.91;12,112.66,127.61,393.32,10.91;12,112.41,141.16,394.78,10.91;12,112.66,154.71,101.72,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,352.30,100.52,154.98,10.91;12,112.66,114.06,387.25,10.91">Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,127.61,393.32,10.91;12,112.41,141.16,294.34,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF 2023)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,168.26,62.42,10.91;12,171.78,173.41,1.32,5.98;12,175.08,168.26,331.30,10.91;12,112.66,181.81,394.42,10.91;12,112.66,195.36,393.32,10.91;12,112.41,208.91,394.78,10.91;12,112.66,222.46,101.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,464.90,168.26,41.48,10.91;12,112.66,181.81,369.71,10.91">Overview of ImageCLEFfusion 2023 Task -Testing Ensembling Methods in Diverse Scenarios</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liviu-Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,492.41,182.82,14.67,9.72;12,112.66,195.36,393.32,10.91;12,112.41,208.91,294.34,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF 2023)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,236.01,393.33,10.91;12,112.66,249.56,394.52,10.91;12,112.66,263.11,127.49,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,173.09,236.01,332.90,10.91;12,112.66,249.56,87.79,10.91">Optimize What You Evaluate With: Search Result Diversification Based on Metric Optimization</title>
		<author>
			<persName coords=""><forename type="first">Hai-Tao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,210.58,250.57,258.73,9.72">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10399" to="10407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,276.66,393.53,10.91;12,112.14,290.20,393.85,10.91;12,112.14,303.75,265.11,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,396.87,276.66,109.32,10.91;12,112.14,290.20,247.57,10.91">Ensembled Approach for Web Search Result Diversification Using Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Shreya</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramachandran</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,369.65,291.22,103.60,9.72">CLEF2022 Working Notes</title>
		<title level="s" coord="12,479.78,290.20,26.20,10.91;12,112.14,303.75,146.03,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,395.17,10.91;12,112.66,330.85,393.32,10.91;12,112.66,344.40,216.46,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,294.40,317.30,213.42,10.91;12,112.66,330.85,198.94,10.91">A Fusion Approach for Web Search Result Diversification Using Machine Learning Algorithms</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sriram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,321.48,331.87,104.14,9.72">CLEF2022 Working Notes</title>
		<title level="s" coord="12,432.37,330.85,73.62,10.91;12,112.66,344.40,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,357.95,394.52,10.91;12,112.66,371.50,393.33,10.91;12,112.66,385.05,240.36,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,185.63,371.50,320.35,10.91;12,112.66,385.05,65.67,10.91">A Late Fusion Framework with Multiple Optimization Methods for Media Interestingness</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Shoukat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khubaib</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naina</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasir</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammed</forename><surname>Hassanuzaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kashif</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04762</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,398.60,393.33,10.91;12,112.66,412.15,259.35,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,162.75,398.60,343.24,10.91;12,112.66,412.15,40.15,10.91">Building CNN-Based Models for Image Aesthetic Score Prediction Using an Ensemble</title>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,163.31,413.16,80.15,9.72">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2" to="30" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,425.70,395.17,10.91;12,112.66,439.25,393.32,10.91;12,112.66,452.79,187.81,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,263.53,425.70,244.30,10.91;12,112.66,439.25,188.31,10.91">Diversifying Relevant Search Results from Social Media Using Community Contributed Images</title>
		<author>
			<persName coords=""><forename type="first">Vaibhav</forename><surname>Kalakota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,313.03,440.26,192.95,9.72;12,112.66,453.81,156.81,9.72">IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,466.34,182.33,10.91;12,291.69,471.49,1.32,5.98;12,294.99,466.34,212.84,10.91;12,112.66,479.89,393.61,10.91;12,112.66,493.44,394.52,10.91;12,112.66,506.99,22.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,285.67,479.89,220.60,10.91;12,112.66,493.44,139.50,10.91">Visual interestingness prediction: A benchmark framework and literature review</title>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ngoc</forename><forename type="middle">Qk</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claire-Hélène</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mats</forename><surname>Sjöberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,262.01,494.46,172.94,9.72">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1526" to="1550" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,520.54,393.33,10.91;12,112.66,534.09,393.33,10.91;12,112.66,547.64,261.57,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,195.88,534.09,310.10,10.91;12,112.66,547.64,24.97,10.91">Benchmarking Image Retrieval Diversification Techniques for Social Media</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mircea-Radu</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,148.11,548.65,141.64,9.72">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
