<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,407.66,15.42;1,89.29,106.66,367.91,15.42;1,89.29,128.58,219.99,15.43;1,89.29,150.91,300.74,11.96">AIMultimediaLab at ImageCLEFmedical GANs 2023: Determining &quot;Fingerprints&quot; of Training Data in Generated Synthetic Images Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
				<funder ref="#_fMNDrEJ #_9jRAzeT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,176.82,142.00,11.96"><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,249.51,176.82,77.54,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,407.66,15.42;1,89.29,106.66,367.91,15.42;1,89.29,128.58,219.99,15.43;1,89.29,150.91,300.74,11.96">AIMultimediaLab at ImageCLEFmedical GANs 2023: Determining &quot;Fingerprints&quot; of Training Data in Generated Synthetic Images Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1C987E094DDBF147A9455FEB59E42011</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>synthetic medical images</term>
					<term>data augmentation</term>
					<term>Generative Adversarial Networks</term>
					<term>ImageCLEFmedical GANs</term>
					<term>CT images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the AI Multimedia Lab to the 2023 ImageCLEF medical GANs task. The 2023 ImageCLEFmedical GANs task challenges participants to examine the hypothesis that generative models (Generative Adversarial Networks -GAN) are generating medical images that contain "fingerprints" of the real images used for the training of the network. We present our team's approach to tackle this task, consisting of a method that implies generating synthetic images from the development dataset. Subsequently, features were extracted from both sets of generated images (the one provided in the development dataset and the one we generated). A binary Support Vector Machine (SVM) classifier was trained using these features and the labels were predicted for the real images from the test dataset. Experimentation on the testing dataset show promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models are are becoming a powerful tool with immense potential for various applications, including in the medical field. These models can generate synthetic data that resembles real medical images, opening new doors for research, diagnosis, and treatment.</p><p>The integration of generative models offers unprecedented opportunities, enabling researchers to overcome dataset limitations and develop more robust algorithms improving anomaly detection and disease prediction. Image synthesis methods using Generative Adversarial Networks (GAN) have been studied and developed to obtain different types of medical images. Nie et al. <ref type="bibr" coords="1,102.16,537.99,12.75,10.91" target="#b0">[1]</ref> use context-aware GANs to generate computed tomography (CT) images from magnetic resonance images (MRIs), Yang et al. <ref type="bibr" coords="1,249.38,551.54,12.68,10.91" target="#b1">[2]</ref> describe a method to generate MR images using cGANs, Salehinejad et al. <ref type="bibr" coords="1,170.25,565.09,12.99,10.91" target="#b2">[3]</ref> use DCGANs to generate fake chest x-ray images, Frid-Adar et al. <ref type="bibr" coords="1,492.99,565.09,12.99,10.91" target="#b3">[4]</ref> generate synthetic CT images for liver lesion classification and Madani et al. <ref type="bibr" coords="1,442.85,578.64,12.99,10.91" target="#b4">[5]</ref> prove that GAN-based data augmentation achieved higher accuracy than traditional augmentation in chest X-rays. Ho et al. <ref type="bibr" coords="1,163.68,605.73,12.77,10.91" target="#b5">[6]</ref> show that diffusion probabilistic models can generate high-fidelity images comparable to those generated by GANs. For example, Puria Azadi et al. <ref type="bibr" coords="2,430.71,200.42,13.00,10.91" target="#b6">[7]</ref> use diffusion probabilistic models to synthesize high quality histopathology images of brain cancer. However, the use of generative models in healthcare raises important considerations regarding privacy, confidentiality, and ethical implications. Medical data is highly sensitive, and protecting patient privacy is of highly importance. Given these aspects, the ImageCLEFmedical GAN 2023 task which is part of the 2023 ImageCLEF evaluation campaign <ref type="bibr" coords="2,383.02,268.16,11.59,10.91" target="#b7">[8]</ref>, proposes a task related to the need of addressing concerns about the protection of personal information and patient confidentiality and to study if generative models can uphold ethical principles and ensure the privacy of individuals. Participants are given a set of generated images and a set of real images and are tasked to determine which real images were used for obtaining the generated set.</p><p>This paper describes the methods employed by AIMultimedia Lab for this task. We propose an approach that starts from the development dataset from which we generate a set of synthetic images, extract features, train a classifier and use it on the real images. The rest of the paper is structured as follows. Section 2 presents the task and the datasets. The proposed methods are presented in Section 3 and the results are presented in Section 4. Finally, the paper closes with Section 5, where we present the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The 2023 ImageCLEFmedical GANs Task</head><p>ImageCLEFmedical GANs <ref type="bibr" coords="2,210.92,462.28,13.00,10.91" target="#b8">[9]</ref> task is a new challenge in the ImageCLEFmedical track. The objective of this task is to investigate the hypothesis that GANs produce medical images that retain identifiable characteristics ("fingerprints") from the original images utilized during training. . If this hypothesis proves to be true, artificial biomedical images might be subject to similar restrictions and limitations in terms of sharing and usage as actual sensitive medical data. Conversely, if the hypothesis is incorrect, GANs could potentially be utilized to generate extensive datasets of biomedical images that do not require adherence to ethical and privacy regulations.</p><p>Data. The task organizers provide a development set and a training set consisting of axial slices of 3D CT images of about 8,000 lung tuberculosis patients. Fig. <ref type="figure" coords="2,375.14,597.77,5.16,10.91" target="#fig_2">3</ref> depicts examples of images provided with the task.</p><p>‚Ä¢ The development dataset for task includes artificial images, real images which were marked as used or not used for training generative neural networks. ‚Ä¢ The training set was created in similar way, the only difference is that the two subsets of real images are mixed and no proportion of non-used and used ones has been disclosed. More information about the provided sets for the task are described in Table <ref type="table" coords="3,458.90,327.90,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>Two approaches are developed for addressing the task. They are depicted in Figure <ref type="figure" coords="3,472.52,395.49,3.81,10.91" target="#fig_0">1</ref>. Both approaches start by generating synthetic images from the real unused images provided in the development dataset. Subsequently, distinct descriptors/features are extracted and utilized to train a binary SVM classifier that we further used for identifying which of the 200 provided real images were used for generating the 10,000 artificial images from the test dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generate Synthetic Images</head><p>Our research was based on the assumption that both the development and test sets of artificial images were produced utilizing the same Generative model so their characteristics may resemble. Consequently, we used the development dataset to generate synthetic images from the subset of 80 real unused images. We used a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images as the ones presented in Figure <ref type="figure" coords="4,369.22,486.26,14.83,10.91" target="#fig_2">3 (c</ref>). The model is depicted in Figure <ref type="figure" coords="4,120.71,499.81,3.78,10.91" target="#fig_1">2</ref>. The generator consists of 5 deconvolutional layers with a kernel size (4,4), different number of filters for each layer and stride 2. Each deconvolutional layer is followed by a BatchNormalization layer and a LeakyRelu activation function with a slope of 0.2 and the last convolutional layer uses Tanh activation function. The discriminator contains 5 convolutional layer with kernel (4,4) and stride 2. Each convolutional layer is followed by a LeakyRelu activation layer. The last convolution layer of the discriminator is flattened and then fed into a sigmoid output. The model was trained using a batch size of 2 due to the small size of the data available for training. An Adam optimizer was used with learning rate 2e-4 (ùõΩ1 = 0.5). We used the 80 available images for training to generate 802 synthetic images that were further used as described in the diagram. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Features Extraction</head><p>Here is the step where our two approaches differ. Fist, we employed a a handcrafted feature extraction technique called Local Binary Pattern (LBP) to capture the local spatial patterns and the gray scale contrast of the images. Aditionally, we used a deep-learning approach utilizing a pre-trained VGG-16 convolutional network <ref type="bibr" coords="5,280.44,286.34,19.47,10.91" target="#b9">[10]</ref>, which is available in Tensorflow library. We extracted characteristics from both sets of generated images, our set and the one provided in the development dataset, and from the 200 real images released for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prediction</head><p>The training process of the binary SVM classifier is outlined in Figure <ref type="figure" coords="5,402.67,363.17,3.74,10.91" target="#fig_0">1</ref>. It was trained on the features extracted from the two sets of generated images and tested on the features extracted from the 200 real images. To test the linear and non-linear relationship in the data we used both linear and kernel basis functions. The linear kernel was shown to be efficient just for the preliminary tests. While testing our method on the test set, using the linear kernel we only predicted that 9 images were used for training while the other 191 were classified as not used, so this proven that there is a non-linear relationship in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Dataset.Both development and test sets were used. Development set consists of 500 artificial images, 80 real images used for training and 80 real images not used for training while the test set consists of 10,000 artificial images and 200 real images. Different evaluation metrics were used: f1-score, accuracy, precision, specificity, recall. The official evaluation metric for the task is F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preliminary Tests</head><p>To validate the proposed method, we performed two types of preliminary runs on the development dataset before testing it on the test dataset. The first thing that was inspected was to verify that the features extracted from the two generated datasets are unique per class and can be used for classification. To verify this, both generated data sets were split into training and test sets, different data partitioning ratios were tested to train and test a binary SVM classifier and in all cases accuracy values of 100% were obtained for both feature extraction methods. This confirmed that the proposed feature extraction methods can be used for a classification problem.</p><p>The method presented in this paper was first implemented and tested on the development dataset.</p><p>In this case we are talking about a supervised classification method: the features extracted from the two sets of generated images were used to train the classifier, and the testing set consisted of the features extracted from the two types of real images provided in the development dataset.</p><p>The preliminary results obtained are depicted in Table <ref type="table" coords="6,326.88,268.34,3.66,10.91" target="#tab_1">2</ref>. The best preliminary result of F1-score of 0.78 of F1-score was obtained using a deep-learning feature extraction method and a SVM with radial kernel for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Official Results</head><p>We have submitted two runs, one for each method presented previous section, as presented in the following:</p><p>‚Ä¢ run1: uses the handcrafted feature extraction method (LBP).</p><p>‚Ä¢ run2: uses a deep-learning feature extraction method</p><p>Our results are presented in Table <ref type="table" coords="6,250.80,415.97,4.97,10.91" target="#tab_2">3</ref> and the confusion matrices for the two runs are depicted in Figure <ref type="figure" coords="6,132.82,429.52,3.81,10.91" target="#fig_3">4</ref>. Best results were obtained using the handcrafted feature extraction method, the LBP. This can be explained as the fact that texture characteristics from the training images are maintained in the generated synthetic images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper describes the methods developed by AIMultimediaLab team for the ImageCLEFmedical GANs task. The purpose of the task was to determine the images from the real data set that were used to train the models that generated synthetic images. In our system, we have generated a new set of synthetic images from the development dataset, and extracted features that we used to train a classifier. The trained classifier was further used to predict the label (used/not used) for the 200 real images. The best result with an f1-score of 0.62 was obtained using the handcrafted feature extraction technique demonstrating that classical texture descriptors can be involved in analysing synthetic medical images.of our approach in practical applications involving the generation and analysis of artificial medical images. In conclusion, 2023 ImageCLEFmedical GANs aim to bring attention to the possible privacy hazards associated with the utilization of synthetic medical data in practical settings and this paper presents a couple of methods for examining the existing hypothesis that GANs are generating medical images that contain the "fingerprints" of the real images used for generative network training. To enhance the scope of this paper, additional exploration can be conducted by extracting and examining alternative hand-crafted features. This investigation aims to determine whether a combination of these features can provide more insights into the origin of the generated data. Furthermore, an opportunity for further development lies in the method proposed in this paper for generating synthetic images. By testing and implementing alternative methods, it is possible to generate images of superior quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,276.75,256.12,8.93;3,89.29,84.19,416.70,180.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of proposed approach for solving the task.</figDesc><graphic coords="3,89.29,84.19,416.70,180.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,623.72,128.72,8.93;3,89.29,480.37,416.69,130.79"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DCGAN architecture</figDesc><graphic coords="3,89.29,480.37,416.69,130.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,361.94,417.76,8.93;4,89.29,373.94,318.44,8.87;4,89.29,84.19,416.70,271.16"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of images: a) real images from the dataset, b) synthetic images provided in the task' dataset, c) synthetic images generated by the method presented in this paper.</figDesc><graphic coords="4,89.29,84.19,416.70,271.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,651.30,190.68,8.93;6,127.56,478.93,340.17,159.80"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Confusion matrices: a) run1, b) run2</figDesc><graphic coords="6,127.56,478.93,340.17,159.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,323.90,85.58"><head>Table 1</head><label>1</label><figDesc>Description of development and test datasets made available for the task</figDesc><table coords="2,182.39,118.13,230.50,57.94"><row><cell></cell><cell cols="2">Number of images</cell></row><row><cell>Dataset</cell><cell>Generated</cell><cell>Real</cell></row><row><cell>Development</cell><cell>500</cell><cell>80 (used) 80 (not used)</cell></row><row><cell>Test</cell><cell>10.000</cell><cell>200 (used and not used)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,410.53,110.28"><head>Table 2</head><label>2</label><figDesc>Preliminary results</figDesc><table coords="5,95.75,118.53,403.78,82.24"><row><cell></cell><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classifier</cell><cell>extraction</cell><cell cols="5">F1-score Accuracy Precision Specificity Recall</cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SVM, radial kernel hand-crafted</cell><cell>0.54</cell><cell>0.45</cell><cell>0.46</cell><cell>0.25</cell><cell>0.65</cell></row><row><cell cols="2">SVM, linear kernel hand-crafted</cell><cell>0.66</cell><cell>0.5</cell><cell>0.5</cell><cell>0</cell><cell>1</cell></row><row><cell cols="2">SVM, radial kernel deep-learning</cell><cell>0.78</cell><cell>0.58</cell><cell>0.54</cell><cell>0.17</cell><cell>1</cell></row><row><cell cols="2">SVM, linear kernel deep-learning</cell><cell>0.66</cell><cell>0.5</cell><cell>0.5</cell><cell>0</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,433.41,59.45"><head>Table 3</head><label>3</label><figDesc>Results on the test dataset.</figDesc><table coords="6,95.67,116.30,426.74,33.63"><row><cell>Method</cell><cell cols="5">f1_score accuracy precision specificity recall</cell></row><row><cell>run1: handcrafted feature extraction (LBP)</cell><cell>0.626</cell><cell>0.54</cell><cell>0.527</cell><cell>0.31</cell><cell>0.77</cell></row><row><cell>run 2: deep-learning features extraction</cell><cell>0.585</cell><cell>0.47</cell><cell>0.4807</cell><cell>0.19</cell><cell>0.75</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The contribution to this task is supported under project <rs type="projectName">AI4Media</rs>, <rs type="programName">A European Excellence Centre for Media, Society and Democracy</rs>, <rs type="grantNumber">H2020 ICT-48-2020</rs>, grant #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_fMNDrEJ">
					<idno type="grant-number">H2020 ICT-48-2020</idno>
					<orgName type="project" subtype="full">AI4Media</orgName>
					<orgName type="program" subtype="full">A European Excellence Centre for Media, Society and Democracy</orgName>
				</org>
				<org type="funding" xml:id="_9jRAzeT">
					<idno type="grant-number">951911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,499.52,393.33,10.91;7,112.66,513.06,393.33,10.91;7,112.66,526.61,286.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,313.46,499.52,192.53,10.91;7,112.66,513.06,136.71,10.91">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Trullo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,271.53,513.06,234.46,10.91;7,112.66,526.61,156.12,10.91">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,540.16,393.98,10.91;7,112.41,553.71,23.60,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,165.71,540.16,211.41,10.91">Mri cross-modality image-to-image translation</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,386.06,540.16,77.09,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,567.26,393.32,10.91;7,112.66,580.81,394.53,10.91;7,112.66,594.36,394.53,10.91;7,112.66,607.91,107.17,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,371.84,567.26,134.15,10.91;7,112.66,580.81,389.73,10.91">Generalization of deep neural networks for chest pathology classification in x-rays using generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dowdell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,148.72,594.36,353.76,10.91">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="990" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,621.46,395.17,10.91;7,112.66,635.01,324.09,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,177.04,621.46,330.78,10.91;7,112.66,635.01,151.30,10.91">Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,271.89,635.01,75.85,10.91">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,648.56,393.33,10.91;8,112.66,86.97,394.61,10.91;8,112.66,100.52,255.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,382.60,648.56,123.38,10.91;8,112.66,86.97,275.58,10.91">Chest x-ray generation and data augmentation for cardiovascular abnormality classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mandani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,100.52,75.34,10.91">Image processing</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page" from="415" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,393.33,10.91;8,112.66,127.61,395.00,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,221.82,114.06,174.59,10.91">Denoising diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,419.59,114.06,86.39,10.91;8,112.66,127.61,139.72,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,141.16,393.33,10.91;8,112.66,154.71,254.88,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A M</forename></persName>
		</author>
		<idno>arXiv-2209</idno>
		<title level="m" coord="8,188.07,141.16,317.92,10.91;8,112.66,154.71,98.42,10.91">A morphology focused diffusion probabilistic model for synthesis of histopathology images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,168.26,394.53,10.91;8,112.66,181.81,395.17,10.91;8,112.66,195.36,394.53,10.91;8,112.66,208.91,395.17,10.91;8,112.39,222.46,394.80,10.91;8,112.48,236.01,394.70,10.91;8,112.66,249.56,395.17,10.91;8,112.66,263.11,393.32,10.91;8,112.66,276.66,394.52,10.91;8,112.33,290.20,120.27,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,224.93,236.01,282.25,10.91;8,112.66,249.56,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcƒ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.94,249.56,143.89,10.91;8,112.66,263.11,393.32,10.91;8,112.66,276.66,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,280.09,276.66,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,303.75,393.33,10.91;8,112.66,317.30,393.33,10.91;8,112.66,330.85,393.33,10.91;8,112.66,344.40,356.56,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,448.60,303.75,57.38,10.91;8,112.66,317.30,334.73,10.91">Overview of ImageCLEFmedical GANs 2023 task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,464.74,317.30,41.25,10.91;8,112.66,330.85,287.68,10.91;8,421.96,330.85,84.03,10.91;8,112.66,344.40,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="8,143.49,344.40,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="8,112.66,357.95,394.53,10.91;8,112.66,371.50,168.72,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="8,206.70,357.95,296.08,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
