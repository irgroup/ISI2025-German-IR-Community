<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,451.12,17.04;1,72.02,96.20,451.04,17.04;1,72.02,116.96,368.55,17.04;1,72.02,149.86,236.88,10.80">TREDENCE at MEDIQA-Sum 2023: Clinical Note Generation from Doctor Patient Conversation using Utterance Segmentation and Question-Answer Driven Abstractive Summarization Notebook for the ImageCLEF Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,176.38,81.56,10.80"><forename type="first">Vaibhav</forename><surname>Adwani</surname></persName>
							<email>vaibhav.adwani@tredence.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>Tredence</addrLine>
									<settlement>Whitefield</settlement>
									<region>Bengaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.77,176.38,125.76,10.80"><forename type="first">Mohammed</forename><forename type="middle">Sameer</forename><surname>Khan</surname></persName>
							<email>mohammed.sameerkhan@tredence.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>Tredence</addrLine>
									<settlement>Whitefield</settlement>
									<region>Bengaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.31,176.38,75.67,10.80"><forename type="first">Ankush</forename><surname>Chopra</surname></persName>
							<email>ankush.chopra@tredence.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>Tredence</addrLine>
									<settlement>Whitefield</settlement>
									<region>Bengaluru</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,451.12,17.04;1,72.02,96.20,451.04,17.04;1,72.02,116.96,368.55,17.04;1,72.02,149.86,236.88,10.80">TREDENCE at MEDIQA-Sum 2023: Clinical Note Generation from Doctor Patient Conversation using Utterance Segmentation and Question-Answer Driven Abstractive Summarization Notebook for the ImageCLEF Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E089F5D6FD3BDCEC7C42B5ECC481F3F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>ROBERTA</term>
					<term>BART</term>
					<term>PEGASUS</term>
					<term>T5</term>
					<term>MEDIQA-Sum</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our submission to MEDIQA-Sum <ref type="bibr" coords="1,387.79,249.16,12.28,9.50" target="#b1">[2]</ref> shared task for automatic classification and summarization of doctor-patient conversations. These doctor-patient transcripts present many challenges: limited training data, significant domain shifts and noisy transcripts. Here in this paper, we explore the possibility of using pretrained transformer models to automatically summarize and generate clinical note from full doctor-patient transcript. We propose a two-step approach, first step is the use of pretrained encoder models like Biomedical-ROBERTA [3] to get dialogues belonging to similar category (section headers) together from the full transcript, these are known as conversation snippets (parts of full conversation) belonging to some section header like chief-complaint, to achieve this we propose two methods, Fixed window size and Variable window size, both these methods proved to be effective in getting good conversation snippets from the full conversation. In the second step, we summarize these conversation snippets, for this, we propose a QA-based summarization approach using transformer-based summarization models where we pass questions corresponding to the conversation category along with the conversation to make the model focus on important aspects of the conversation and then output the summary in the form of an answer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the widespread adoption of electronic health records (EHRs) has generated an enormous amount of textual data, including doctor-patient conversations. These conversations contain valuable insights and critical information about patients' medical conditions, diagnoses, treatments, and outcomes. MEDIQA-Sum <ref type="bibr" coords="1,192.89,563.16,12.91,9.94" target="#b1">[2]</ref> focuses on the automatic summarization and classification of doctorpatient conversations through three subtasks. Effectively classifying full conversation is the primary step towards achieving good clinical notes, for this, we propose a window size-based approach using transformer-based classification models pretrained on medical data. The next step is the generation of good-quality summaries, for this, we propose the use of abstractive summarization models pretrained on medical data using Question-Answering (QA) approach. By using QA based approach, we frame the summarization task as an answer generation process, where questions correspond to important aspects of the conversation, and the answers are the abstractive summaries themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Summarization is a well-known problem in NLP however recent years have seen big improvements in the field with models like BART <ref type="bibr" coords="2,233.21,118.36,11.79,9.94" target="#b6">[7]</ref>, T5 <ref type="bibr" coords="2,267.29,118.36,18.30,9.94" target="#b16">[17]</ref> etc. <ref type="bibr" coords="2,307.51,118.36,85.38,9.94">Zhang et al. (2021)</ref>  <ref type="bibr" coords="2,395.98,118.36,18.30,9.94" target="#b14">[15]</ref> finetune a BART-Large model to summarize conversations belonging to distinct sections such as History of Present Illness, etc. Similar models can also be used for clinical note generation. <ref type="bibr" coords="2,344.35,143.68,92.03,9.94">Krishna et al. (2020)</ref>  <ref type="bibr" coords="2,439.54,143.68,18.30,9.94" target="#b15">[16]</ref> use multilabel B-LSTM to identify sentences belonging to different subsections like Allergy, Chief complaint etc. and then cluster all sentences belonging to each subsection together and then the use T5-Base <ref type="bibr" coords="2,474.22,169.00,18.30,9.94" target="#b16">[17]</ref> model to summarize the sentence clusters of each subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we describe various tasks, data used, methodologies adopted, and experimentations carried out in ImageCLEF 2023 <ref type="bibr" coords="2,215.45,252.07,12.78,9.94" target="#b0">[1]</ref> MEDIQA-Sum <ref type="bibr" coords="2,301.63,252.07,12.78,9.94" target="#b1">[2]</ref> competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task-A</head><p>This task <ref type="bibr" coords="2,128.90,309.79,12.78,9.94" target="#b1">[2]</ref> aims to identify the topic (section header) of the conversation between the doctor and patient. The section header will be one of the twenty normalized common section labels [E].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Dataset</head><p>We used Dialogue to Topic Classification dataset provided by MEDIQA-Sum organizers <ref type="bibr" coords="2,474.94,380.23,11.70,9.94" target="#b1">[2]</ref>, dataset consisted of conversation ids, conversation snippets and section headers for each snippet. Train, validation, and test set consisted of 1200, 100 and 200 instances respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description &amp; Preprocessing:</head><p>We truncated conversations exceeding the sequence limit of 512 to 512 because of the sequence length limit (512) for base version BERT <ref type="bibr" coords="2,409.54,431.61,18.30,9.94" target="#b10">[11]</ref> and ROBERTA <ref type="bibr" coords="2,502.06,431.61,17.01,9.94" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1:</head><p>Corpus statistics for Task-A dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Method</head><p>To solve this task, we have used the transformer-based models from hugging face <ref type="bibr" coords="2,449.14,600.24,12.80,9.94" target="#b8">[9]</ref> pretrained on biomedical and clinical data like PubMed [D] and MIMIC III <ref type="bibr" coords="2,348.37,612.96,13.63,9.94">[D]</ref>. We further finetuned these models to achieve state-of-the-art performance. To deal with long conversations we have also tried a long sequence encoder model pretrained on clinical data and then further finetuned it on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Experimentations</head><p>We performed three experiments (runs) using three different models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.1">Run -1</head><p>In this run, we used Biomedical-ROBERTA <ref type="bibr" coords="3,285.31,105.76,11.79,9.94" target="#b2">[3]</ref>, base version, pretrained on Biomedical data from Semantic Scholar <ref type="bibr" coords="3,152.30,118.36,13.61,9.94">[D]</ref>. The model was finetuned for conversation classification on train and validation data together by adding a classification head consisting of two linear layers and a 0.1 dropout layer in between two linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.2">Run-2</head><p>In this run, we used Clinical-Longformer <ref type="bibr" coords="3,270.89,201.40,12.80,9.94" target="#b3">[4]</ref> model pretrained using MIMIC-III [D] clinical notes. The Longformer model has a sequence limit of 4096 tokens, this helped us to train the model on full conversations even if the conversation length is large. We then finetuned the model for conversation classification on train and validation data together by adding a classification head consisting of two linear layers and a 0.1 dropout layer in between two linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.3">Run-3</head><p>In this run, we used Bio-Clinical BERT <ref type="bibr" coords="3,264.05,309.79,12.78,9.94" target="#b4">[5]</ref> model pretrained on all notes from MIMIC III <ref type="bibr" coords="3,486.94,309.79,13.59,9.94">[D]</ref>. We further finetuned the models on train and validation data together for final submission by adding two linear layers and a 0.1 dropout layer between the two linear layers on top of Bio-Clinical BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2:</head><p>Hyperparameters for Run-1, Run-2 &amp; Run-3 (Task-A) found after multiple rounds of tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-B</head><p>This task <ref type="bibr" coords="3,129.74,515.61,12.78,9.94" target="#b1">[2]</ref> aims to summarize the conversation snippet between the doctor and the patient. The conversation belongs to a particular topic identified by one of the 20 section headers. The summary contains abstract information related to the specified section header.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Dataset</head><p>Dataset is the same as provided in Task-A but contains one additional column called section text containing summaries for every conversation snippet.</p><p>Dataset Description &amp; Preprocessing: The important step in preprocessing is to prepare inputs for the model to finetune models using Question-Answering based approach as described in section 3.2.2, also because of resource constraints during training, we have truncated the input sequence length to the model to 400 in every experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Method</head><p>To solve this summarization task, we used transformer models from hugging face <ref type="bibr" coords="4,448.90,105.76,12.91,9.94" target="#b8">[9]</ref> pretrained on biomedical or clinical data with pretraining objective of abstractive summarization. We used Question-Answering (QA) based approach to finetune these models on our dataset for abstractive summarization. Although this is fundamentally a summarization task, but we observed that summaries of different sections had different styles. We therefore modelled this as a QA task where the question is used to inculcate the distinct style of a particular section while summarization.</p><p>To train a QA model we require question, context, and an answer. In this use case, answer is the expected output summary, context is the conversation snippet to be summarized and the question is a kind of instruction given to the model to look for specific information in the conversation during summarization.</p><p>Figure <ref type="figure" coords="4,118.57,232.27,5.52,9.94" target="#fig_0">1</ref> explains how models were finetuned for QA-based summarization <ref type="bibr" coords="4,434.98,232.27,18.19,9.94" target="#b11">[12]</ref> by passing the question corresponding to the section header along with the conversation snippet as input to the model. The conversation shown in the below figure belongs to CHIEF COMPLAINT [CC] section header, we picked up question corresponding to CHIEF COMPLAINT and passed it along with the conversation to tell the model what to summarize, inputs to the model are passed in the specific format, as shown in figure <ref type="figure" coords="4,101.18,295.51,4.14,9.94" target="#fig_0">1</ref>, both during finetuning as well as during inference.</p><p>Questions for each section header were empirically selected after multiple experimentations, please refer Appendix section for a full table containing questions for every section header. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONVERSATION MODEL INPUT SUMMARY SECTION HEADER QUESTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Experimentations</head><p>We experimented with multiple summarization models. Two runs have been submitted for evaluation, other experiments which were not submitted are described under Other Experimentations section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.1">Run-1</head><p>In this run, we have used BART-large <ref type="bibr" coords="4,267.17,643.80,12.78,9.94" target="#b6">[7]</ref> model pretrained on biomedical data (PubMed). We finetuned the model using Question-Answering based approach by adding the language modelling head on top of BART-large <ref type="bibr" coords="4,177.53,669.12,11.70,9.94" target="#b6">[7]</ref>. The model was finetuned on train and validation data together for final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.2">Run-2</head><p>In this run, we have used BART-base <ref type="bibr" coords="4,256.37,739.58,12.91,9.94" target="#b7">[8]</ref> model pretrained on biomedical data. To finetune model for the summary generation we added language modelling on top of BART-base <ref type="bibr" coords="4,436.54,752.18,12.91,9.94" target="#b7">[8]</ref> and finetuned it What is the chief complaint of the patient?</p><p>using the Question-Answering approach, as described in section 3.2.2, on train and validation data together for final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4:</head><p>Hyperparameters for Run-1 and Run-2 (Task-B) found after multiple rounds of hyperparameter tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.3">Other Experimentations</head><p>We tried two abstractive summarization models, PEGAUS <ref type="bibr" coords="5,346.87,241.51,18.43,9.94" target="#b13">[14]</ref> and T5 <ref type="bibr" coords="5,401.50,241.51,18.30,9.94" target="#b16">[17]</ref> base version, that were already finetuned for QA-based summarization but not on medical data, we have also tried a T5 <ref type="bibr" coords="5,504.72,254.11,18.30,9.94" target="#b16">[17]</ref> base version that was finetuned on BIOASQ [D] dataset for QA based summarization. We further finetuned each of these models, using the QA approach by adding a language modelling head on top of the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5:</head><p>Hyperparameters for experimented models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-C</head><p>This task <ref type="bibr" coords="5,132.74,483.33,12.78,9.94" target="#b1">[2]</ref> aims to generate a full clinical note summarizing the full encounter conversation between the doctor and patient. Clinical notes should contain abstract information about every topic (section header) being discussed in the full conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dataset</head><p>In our experiments, we used Full-Encounter Dialogue to Note Summarization dataset provided by MEDIQA-Sum organizers <ref type="bibr" coords="5,190.61,579.00,12.80,9.94" target="#b1">[2]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Method</head><p>To create clinical note, we require two things, first is the conversation snippets pertaining to one of the available section headers in the full conversation and second is the summary of these conversation snippets. For summarization we have used best performing summarization model from Task-B. Since we have full conversation with us, we ran classification model from Task-A on parts of full conversation to get conversation snippets belonging to available section headers in the full conversation. After trying various approaches, we found 2 approaches that worked best for us in getting better conversation snippets from the full conversation. These approaches are discussed below.</p><p>Variable Window Size: In the variable window size approach, we require a fixed window size to start with. We keep on extending this window according to different conditions as mentioned in the below steps, but the minimum window size in this approach remains fixed.</p><p>Empirically minimum window size of 2 worked best for us. Once we have this minimum value, we start with the first utterance(dialogue) of the conversation to create starting conversation snippet containing 2(window size) dialogues. Once we have the starting conversation snippet, we followed the below steps to implement the logic.</p><p>1. Classify the conversation snippet and keep a note of the predicted section header and its classification probability. 2. Add another utterance (dialogue) to the present window(snippet) and then classify this new conversation snippet, in this step also keep a note of the predicted section header and its classification probability. 3. If the predicted section header from Step 1 and Step 2 are the same and the classification probability for this section header increase or remains the same in Step 2, keep this utterance (dialogue) added in Step 2 to the present conversation snippet and repeat step no. 2. 4. If the predicted section header from step 1 and step 2 are different or the predicted section header is the same but the classification probability in step 2 decreases after adding the utterance to the present window(snippet), then we drop the utterance(dialogue) added to the window and consider this snippet to belong to section header which was predicted before adding this new utterance(dialogue). After this, we create a new window of fixed size, as decided earlier, from the next utterance of the previous window and then repeat the same steps from 1 to 4. 5. Continue steps 1 to 4 until the full conversation is covered, at the end concatenate conversations belonging to the same section header together in the same order in which they were obtained from different parts of the full conversation. Fixed Window Size: To implement this approach we need to have a fixed window size, after evaluating model performance using different window sizes, we empirically selected a window of size two. We followed the below steps to implement fixed window size logic.</p><p>1. Using a fixed window of a particular size, select the conversation snippet starting from the first utterance(dialogue) containing utterances which are equal to the window size in number and then classify it. 2. Use the same window to select another conversation snippet from the next dialogue of the previous window and then classify it, Repeat the process until the full conversation is covered. 3. Concatenate conversation snippets belonging to the same section header together in the same order in which they were obtained from different parts of the full conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Experimentations</head><p>We performed two experiments implementing variable window size and fixed window size approach as discussed in section 3.3.2. In each experiment window size is the hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.1">Run-1</head><p>In this run, we finetuned the best performing classification model from Task-A, biomed-ROBERTA <ref type="bibr" coords="7,72.02,118.36,11.79,9.94" target="#b2">[3]</ref>, on train, validation, and test set of Task-A and used it to get conversation snippets belonging to different section headers from full conversation using fixed window size approach as discussed in section 3.2.2. To summarize these snippets, we used the best performing summarizing model from Task-B, BART-large <ref type="bibr" coords="7,167.69,156.40,11.79,9.94" target="#b6">[7]</ref>. Summarization is done using QA based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.2">Run-2</head><p>Here we used the Variable window size approach to get conversation snippets belonging to available section headers in the full conversation. Classification and summarization models as well as the approach remain the same as used in run 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We present the results of our experimentations on train, validation, and test sets. We have test set results of only submitted runs since test sets for Task-B and Task-C were not made available. Our submission results and metrics used by the organizers for evaluation are highlighted for each task.</p><p>Note: Test set results are of models trained on and validation set together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task-A</head><p>Dataset provided was highly skewed, 9 classes or categories had less than 15 examples, because of which models performed poorly in identifying these categories and overall accuracy is a bit low. For this task, our best performing runs (run-1 and run-2) were ranked 3 rd among 23 submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7:</head><p>Task-A results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task-B</head><p>QA-based approach significantly improved the performance by making summaries more concise, improving the Rouge-1 score. Models pretrained or finetuned on domain data (Bio-BART) performed better as compared to those pretrained on general data (PEGASIS-QA, Valhalla-T5). For this task, our best performing run (run-1) was ranked 4 th among 16 submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TASK-C</head><p>Performance of fixed window size approach significantly depend on window size as window size remains fixed throughout the experiment, variable window on the other hand does not depend much on window size because window size is dynamic throughout the experiment. To be on the safer side, lower window size can be chosen as the initial value. For this task, our best run (run-2) was the overall best run of the competition and run-2 was second best run of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9: Task-C results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rouge-1</head><p>Rouge- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions &amp; Future Work</head><p>In this paper, we present systems developed by the TREDENCE team for MEDIQA-Sum <ref type="bibr" coords="8,487.06,606.24,12.91,9.94" target="#b1">[2]</ref> task. The task focused on the automatic classification and summarization of doctor-patient conversations. To address this, we used pretrained transformer models for classifying full conversation into conversation snippets using the variable-window side and fixed-window size approaches. Both these approaches performed reasonably well to give good quality snippets. For summarizing the snippets, we used a QAbased approach to instruct the model to give summaries in the form of answers to questions. QA. Our experimentations during summarization reveal that instruction-based learning can significantly improve performance.</p><p>In future, we are looking at incorporating Data-Augmentation to improve classification performance. Secondly, we used the summarization model from Task-B to summarize conversation snippets from Task-C, but we found some differences between expected summaries for both these tasks in terms of elaborateness and presentation, we are working towards finetuning our summarization models on Task-C dataset to further improve the performance.   • Bio-BART base: https://huggingface.co/suryakiran786/5-fold-stratified-cv-biobart-v2base-with-section-description-complete-data-1 (Run-1) • Bio-BART large: https://huggingface.co/GanjinZero/biobart-large (Run-2) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hugging Face Models References for Task-A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Important Links</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.02,501.79,416.83,11.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Figure showing the way models were finetuned for QA Task to generate summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,79.46,357.08,81.49,7.24;4,79.46,366.20,85.05,7.24;4,79.46,375.44,86.22,7.24;4,79.46,384.70,20.06,7.24;4,79.46,393.82,45.41,7.24;4,79.46,403.06,86.12,7.24;4,79.46,414.94,17.38,7.24;4,79.46,424.90,38.98,7.24;4,94.10,461.98,23.05,7.24;4,79.94,471.10,62.76,7.24;4,207.53,460.12,94.13,8.10;4,207.53,470.56,32.55,8.10;4,235.01,391.06,147.62,7.24;4,486.10,389.50,68.26,7.24"><head></head><label></label><figDesc>Doctor: Hi, how are you? Patient: I burned my hand. Doctor: Oh, I am sorry. Wow! Patient: Yeah. Doctor: Is it only right arm? Patient: Yes CHIEF COMPLAINT[CC] Section header -Question Mapping question: {question} context: {conversation} Burn in the right arm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,205.85,579.00,317.19,9.94;5,72.02,591.72,425.64,9.94;5,85.58,603.82,437.40,11.04;5,72.02,617.76,450.98,9.94;5,72.02,630.48,450.73,9.94;5,72.02,643.08,65.14,9.94"><head></head><label></label><figDesc>consisting of full conversation between doctor and patient along with the clinical note. Train, validation, and test dataset consisted of 67, 20 and 40 instances respectively. Dataset Description &amp; Preprocessing: Data was mostly clean and didn't require much preprocessing, apart from removing extra spaces, we formatted [doctor] and [patient] to doctor: and patient: because data on which classification and summarization model was trained, contained this representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,393.04,443.41,364.52"><head>Table 3 :</head><label>3</label><figDesc>Corpus statistics for Task-B dataset</figDesc><table coords="3,78.02,393.04,437.41,72.48"><row><cell>Learning</cell><cell>Train</cell><cell>Epochs</cell><cell>L.R</cell><cell>Optimizer</cell><cell>Weight</cell><cell>Warmup</cell><cell>Gradient</cell></row><row><cell>Rate</cell><cell>Batch Size</cell><cell></cell><cell>schedular</cell><cell></cell><cell>decay</cell><cell>Steps</cell><cell>Accumulat</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ion Steps</cell></row><row><cell>Run-1 5e-5</cell><cell>20</cell><cell>11</cell><cell>Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>100</cell><cell>1</cell></row><row><cell>Run 2 5e-5</cell><cell>2</cell><cell>13</cell><cell>Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>100</cell><cell>5</cell></row><row><cell>Run-3 5e-5</cell><cell>20</cell><cell>5</cell><cell>Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>100</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.02,143.15,445.04,604.57"><head>Table 6 :</head><label>6</label><figDesc>Corpus statistics for Task-C dataset</figDesc><table coords="5,77.42,143.15,439.64,290.09"><row><cell>Learning</cell><cell cols="2">Train Batch</cell><cell>Epochs</cell><cell>L.R</cell><cell cols="2">Optimizer</cell><cell>Weight</cell><cell>Warmup</cell><cell>Gradient</cell></row><row><cell>Rate</cell><cell></cell><cell>Size</cell><cell cols="2">Scheduler</cell><cell></cell><cell></cell><cell>decay</cell><cell>Steps</cell><cell>Accumula</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tion Steps</cell></row><row><cell>5e-5</cell><cell></cell><cell>5</cell><cell>4</cell><cell>Linear</cell><cell></cell><cell>Adam</cell><cell>0.01</cell><cell>60</cell><cell>4</cell></row><row><cell></cell><cell>Learning</cell><cell>Train Batch</cell><cell>Epochs</cell><cell>L.R</cell><cell></cell><cell cols="2">Optimizer Weight</cell><cell>Warmup</cell><cell>Gradient</cell></row><row><cell></cell><cell>Rate</cell><cell>Size</cell><cell></cell><cell cols="2">Scheduler</cell><cell></cell><cell>decay</cell><cell>Steps</cell><cell>Accumul</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Steps</cell></row><row><cell>PEGASUS</cell><cell>5e-5</cell><cell>5</cell><cell>15</cell><cell cols="2">Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>10</cell><cell>4</cell></row><row><cell cols="2">Valhalla T5 5e-5</cell><cell>5</cell><cell>15</cell><cell cols="2">Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>10</cell><cell>4</cell></row><row><cell>Bio-T5</cell><cell>5e-5</cell><cell>5</cell><cell>17</cell><cell cols="2">Linear</cell><cell>Adam</cell><cell>0.01</cell><cell>10</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.02,474.28,457.05,295.01"><head>Table 8 :</head><label>8</label><figDesc>Task-B results</figDesc><table coords="7,77.42,474.28,451.65,295.01"><row><cell></cell><cell cols="2">Train-Accuracy</cell><cell cols="2">Validation-Accuracy</cell><cell></cell><cell cols="2">Test-Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell></cell><cell></cell><cell>(%)</cell><cell></cell></row><row><cell cols="3">Biomedical ROBERTA (Run-1) 99.83</cell><cell>80</cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell cols="2">Clinical Longformer (Run-2)</cell><cell>99.92</cell><cell>80</cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell cols="2">Clinical BERT (Run-3)</cell><cell>95.75</cell><cell>77</cell><cell></cell><cell></cell><cell>75.5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Rouge-1</cell><cell>Bleurt</cell><cell cols="2">BERT-Score</cell><cell cols="2">Aggregate score</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell></cell></row><row><cell>Bio-BART-Large</cell><cell>Train</cell><cell>0.51</cell><cell>0.60</cell><cell>0.78</cell><cell>0.75</cell><cell>0.76</cell><cell>0.62</cell></row><row><cell>(Run-1)</cell><cell>Validation</cell><cell>0.41</cell><cell>0.54</cell><cell>0.74</cell><cell>0.71</cell><cell>0.72</cell><cell>0.56</cell></row><row><cell></cell><cell>Test</cell><cell>0.42</cell><cell>0.53</cell><cell>0.74</cell><cell>0.71</cell><cell>0.72</cell><cell>0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,72.02,108.88,451.10,22.54"><head>Table 10 represent</head><label>10</label><figDesc>Questions corresponding to every possible section headers of the conversation snippet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,72.02,133.70,170.61,11.04"><head>Table 10 :</head><label>10</label><figDesc>Table representing mapping.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,72.02,157.22,450.57,606.82"><head></head><label></label><figDesc>what review is done by the doctor on the patient? what is the past surgical history of the patient? what medications are being discussed in the given conversation? what assessment did doctor do about the patient? what is the result of the exam carried out by the doctor on the patient? doctor diagnosed patient with what disease? what is the disposition status of the patient? what is the plan that patient has to follow? what is the condition of the patient in the emergency department? what is the status of patient's immunization or vaccinations? what is the result of patient's imaging report? what is the gynecologic history of the patient? what procedures or surgeries did patient had? what is the other history of the patient mentioned in the conversation? what are the findings from patient's lab report?</figDesc><table coords="11,72.02,157.22,448.90,551.89"><row><cell>Section Header</cell><cell>Question</cell></row><row><cell>FAM/SOCHX</cell><cell>what all things patient mentions about his/her family or</cell></row><row><cell>[FAMILY/SOCIAL HISORY]</cell><cell>his/her social life?</cell></row><row><cell>GENHX</cell><cell>what all problems did patient mentions to the doctor?</cell></row><row><cell>[HISTORY OF PRESENT ILLNESS]</cell><cell></cell></row><row><cell>PASTMEDICALHX</cell><cell>what is the past medical history of the patient?</cell></row><row><cell>[PAST MEDICAL HISTORY]</cell><cell></cell></row><row><cell>CC</cell><cell>what is the chief complaint of the patient?</cell></row><row><cell>[CHIF COMPLAINT]</cell><cell></cell></row><row><cell>ALLERGY</cell><cell>what allergies does patient mentions?</cell></row><row><cell>ROS</cell><cell></cell></row><row><cell>[REVIEW OF SYSTEMS]</cell><cell></cell></row><row><cell>PASTSURGICAL</cell><cell></cell></row><row><cell>[PAST SURGICAL HISTORY]</cell><cell></cell></row><row><cell>MEDICATIONS</cell><cell></cell></row><row><cell>ASSESSMENT</cell><cell></cell></row><row><cell>EXAM</cell><cell></cell></row><row><cell>DIAGNOSIS</cell><cell></cell></row><row><cell>DISPOSITION</cell><cell></cell></row><row><cell>PLAN</cell><cell></cell></row><row><cell>EDCOURSE</cell><cell></cell></row><row><cell>[EMERGENCY DEPARTMENT COURSE]</cell><cell></cell></row><row><cell>IMMUNIZATIONS</cell><cell></cell></row><row><cell>IMAGING</cell><cell></cell></row><row><cell>GYNHX</cell><cell></cell></row><row><cell>[GYNECOLOGIC HISTORY]</cell><cell></cell></row><row><cell>PROCEDURES</cell><cell></cell></row><row><cell>OTHER_HISTORY</cell><cell></cell></row><row><cell>LABS</cell><cell></cell></row><row><cell cols="2">• Biomedical-ROBERTA: https://huggingface.co/allenai/biomed_roberta_base (Run-1)</cell></row><row><cell cols="2">• Clinical-LongFormer: https://huggingface.co/yikuan8/Clinical-Longformer (Run-2)</cell></row><row><cell cols="2">• Clinical-BERT: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT (Run-3)</cell></row><row><cell cols="2">C. Hugging Face Models References for Task-B</cell></row></table><note coords="11,108.02,725.52,314.91,11.04;11,108.02,740.19,305.89,10.05;11,108.02,753.00,331.32,11.04"><p>• Valhalla T5 model: https://huggingface.co/valhalla/t5-base-qa-qg-hl • Pegasus-QA model: https://huggingface.co/tuner007/pegasus_qa • Bio-T5 model: https://huggingface.co/ozcangundes/T5-base-for-BioQA</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,72.02,163.13,348.28,350.18"><head>•</head><label></label><figDesc>BIOASQ website: http://participants-area.bioasq.org/datasets/ • PubMed website: https://pubmed.ncbi.nlm.nih.gov/ • MIMIC III database: https://www.nature.com/articles/sdata201635 • Semantic scholar: https://www.semanticscholar.org/</figDesc><table coords="12,72.02,228.92,283.92,284.39"><row><cell>E. SECTION LABELS FOR TASK A &amp; B</cell></row><row><cell>1. fam/sochx [FAMILY HISTORY/SOCIAL HISTORY]</cell></row><row><cell>2. genhx [HISTORY of PRESENT ILLNESS]</cell></row><row><cell>3. pastmedicalhx [PAST MEDICAL HISTORY]</cell></row><row><cell>4. cc [CHIEF COMPLAINT]</cell></row><row><cell>5. pastsurgical [PAST SURGICAL HISTORY]</cell></row><row><cell>6. allergy</cell></row><row><cell>7. ros [REVIEW OF SYSTEMS]</cell></row><row><cell>8. medications</cell></row><row><cell>9. assessment</cell></row><row><cell>10. exam</cell></row><row><cell>11. diagnosis</cell></row><row><cell>12. disposition</cell></row><row><cell>13. plan</cell></row><row><cell>14. edcourse [EMERGENCY DEPARTMENT COURSE]</cell></row><row><cell>15. immunizations</cell></row><row><cell>16. imaging</cell></row><row><cell>17. gynhx [GYNECOLOGIC HISTORY]</cell></row><row><cell>18. procedures</cell></row><row><cell>19. other_history</cell></row><row><cell>20. labs</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,108.02,105.76,415.10,9.94;9,108.02,118.36,415.10,9.94;9,108.02,131.08,415.11,9.94;9,108.02,143.68,415.06,9.94;9,108.02,156.40,415.38,9.94;9,108.02,169.00,414.81,9.94;9,108.02,181.72,414.95,9.94;9,108.02,194.32,415.06,9.94;9,108.02,206.92,415.13,9.94;9,108.02,219.64,415.10,9.94;9,108.02,232.27,415.09,9.94;9,108.02,244.99,333.81,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,464.61,118.36,58.51,9.94;9,108.02,131.08,22.63,9.94;9,319.39,194.32,117.13,9.94;9,471.74,194.32,51.34,9.94;9,108.02,206.92,415.13,9.94;9,108.02,219.64,311.95,9.94">Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications in use: Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M\"</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Uller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen{-}wai</forename><surname>Dr\u{a}gulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Asma {ben Abacha}</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R\ ;</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">"</forename><surname>Br\</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Ungel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Sch\"afer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ihar</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Filipovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Liviu-Daniel \c{s}tefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>J\'er\^ome Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,441.34,219.64,81.78,9.94;9,108.02,232.27,305.47,9.94">Proceedings of the 14th International Conference of the CLEF Association (CLEF 2023)</title>
		<title level="s" coord="9,448.78,232.27,74.33,9.94;9,108.02,244.99,148.09,9.94">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 14th International Conference of the CLEF Association (CLEF 2023)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09-18">2023. 2023. September 18-21</date>
		</imprint>
	</monogr>
	<note>{Overview of ImageCLEF</note>
</biblStruct>

<biblStruct coords="9,108.02,257.59,415.12,9.94;9,108.02,270.19,415.00,9.94;9,108.02,282.91,415.17,9.94;9,108.02,295.51,384.56,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,154.70,270.19,368.32,9.94;9,108.02,282.91,354.63,9.94">Overview of the MEDIQA-Sum Task at ImageCLEF 2023: Summarization and Classification of Doctor-Patient Conversations in use: CLEF 2023 Working Notes</title>
		<author>
			<persName coords=""><forename type="first">Wen{-}wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>{ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abacha}</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,483.34,282.91,39.85,9.94;9,108.02,295.51,100.53,9.94">{CEUR} Workshop Proceedings</title>
		<title level="s" coord="9,243.41,295.51,23.24,9.94">CEUR</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09-18">2023. September 18-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,308.36,414.91,10.05;9,108.26,321.67,414.82,9.94;9,108.26,334.75,213.05,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,292.15,321.67,230.93,9.94;9,108.26,334.75,84.91,9.94">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName coords=""><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.05,334.75,87.08,9.94">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,347.59,415.23,9.94;9,108.02,360.31,415.12,9.94;9,108.02,372.91,344.84,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,515.28,347.59,7.97,9.94;9,108.02,360.31,303.86,9.94">A comparative study of pretrained language models for long clinical text</title>
		<author>
			<persName coords=""><forename type="first">Yikuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wehbe</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramsey</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faraz</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,418.06,360.31,105.08,9.94;9,108.02,372.91,144.17,9.94">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="340" to="347" />
			<date type="published" when="2023">2023</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,385.65,414.99,9.94;9,108.02,398.25,415.12,9.94;9,108.02,410.85,189.53,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,255.77,398.25,228.67,9.94">Publicly Available Clinical BERT Embeddings</title>
		<author>
			<persName coords=""><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1904.03323</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1904.03323" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,423.57,414.89,9.94;9,108.02,436.17,415.05,9.94;9,108.02,448.89,324.81,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,181.97,436.17,341.10,9.94;9,108.02,448.89,100.05,9.94">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1901.08746</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1901.08746" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,461.49,415.06,9.94;9,108.02,474.09,415.18,9.94;9,108.02,486.81,220.13,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,108.02,474.09,381.30,9.94">BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model</title>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruyi</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.03905</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.03905" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,499.41,415.10,9.94;9,108.02,512.13,415.08,9.94;9,108.02,524.73,189.53,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,108.02,512.13,347.81,9.94">Pretraining and Evaluation of A Biomedical Generative Language Model</title>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruyi</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Biobart</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.03905</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.03905" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,537.33,415.13,9.94;9,108.02,550.08,414.77,9.94;9,108.02,562.68,414.84,9.94;9,108.02,575.40,415.02,9.94;9,108.02,588.00,400.06,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,363.73,575.40,159.31,9.94;9,108.02,588.00,197.71,9.94">Rush, HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.03771</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1910.03771" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,600.60,415.14,9.94;9,108.02,613.32,415.05,9.94;9,108.02,625.92,320.72,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,381.77,613.32,141.30,9.94;9,108.02,625.92,92.81,9.94">A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberta</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1907.11692" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,638.64,415.13,9.94;9,108.02,651.24,57.50,9.94;9,195.45,651.24,59.21,9.94;9,284.55,651.24,12.80,9.94;9,327.25,651.24,43.40,9.94;9,400.55,651.24,67.79,9.94;9,498.09,651.24,25.05,9.94;9,108.02,663.96,192.41,9.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,433.55,638.64,89.61,9.94;9,108.02,651.24,57.50,9.94;9,195.45,651.24,59.21,9.94;9,284.55,651.24,12.80,9.94;9,327.25,651.24,43.40,9.94;9,400.55,651.24,62.95,9.94">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1810.04805" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,676.56,415.00,9.94;9,108.02,689.16,395.00,9.94" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,324.91,676.56,198.11,9.94;9,108.02,689.16,115.92,9.94">A Semantic QA-Based Approach for Text Summarization Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
		<ptr target="https://arxiv.org/ftp/arxiv/papers/1704/1704.06259.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,701.90,415.18,9.94;9,108.02,714.50,220.13,9.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,311.71,701.90,206.67,9.94">Longformer: The Long-Document Transformer</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2004.05150</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2004.05150" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,727.22,415.12,9.94;9,108.02,739.82,42.21,9.94;9,180.23,739.82,64.20,9.94;9,274.45,739.82,12.78,9.94;9,317.25,739.82,50.71,9.94;9,397.97,739.82,70.25,9.94;9,498.08,739.82,25.05,9.94;9,108.02,752.42,192.41,9.94" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,389.95,727.22,133.19,9.94;9,108.02,739.82,42.21,9.94;9,180.23,739.82,64.20,9.94;9,274.45,739.82,12.78,9.94;9,317.25,739.82,50.71,9.94;9,397.97,739.82,65.23,9.94">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
		<author>
			<persName coords=""><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1912.08777</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1912.08777" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.02,74.68,415.10,9.94;10,108.02,87.28,414.99,9.94;10,108.02,100.00,46.52,9.94;10,182.45,100.00,67.18,9.94;10,277.58,100.00,9.07,9.94;10,314.64,100.00,64.79,9.94;10,407.34,100.00,62.95,9.94;10,498.22,100.00,24.91,9.94;10,108.02,112.60,143.66,9.94" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,360.91,87.28,162.10,9.94;10,108.02,100.00,46.52,9.94;10,182.45,100.00,67.18,9.94;10,277.58,100.00,9.07,9.94;10,314.64,100.00,64.79,9.94;10,407.34,100.00,62.95,9.94">Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations</title>
		<author>
			<persName coords=""><forename type="first">Longxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arindam</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasudevan</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reza</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gormley</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.12174" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.02,125.32,415.07,9.94;10,108.02,137.92,415.11,9.94;10,108.02,150.52,146.43,9.94" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,417.70,125.32,105.39,9.94;10,108.02,137.92,381.62,9.94">Generating SOAP notes from Doctor-Patient Conversations Using Modular Summarization Techniques</title>
		<author>
			<persName coords=""><forename type="first">Kundan</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sopan</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.01795" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.02,163.24,414.83,9.94;10,108.02,175.84,414.89,9.94;10,108.02,188.56,339.56,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,264.41,175.84,258.50,9.94;10,108.02,188.56,111.98,9.94">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.10683</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1910.10683" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
