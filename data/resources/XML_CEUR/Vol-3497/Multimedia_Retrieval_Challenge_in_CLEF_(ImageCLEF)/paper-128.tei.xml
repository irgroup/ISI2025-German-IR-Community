<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.70,15.42;1,89.29,106.66,35.39,15.43;1,89.29,129.00,230.34,11.96">IUST_NLPLAB at ImageCLEFmedical Caption Tasks 2023 Notebook for the ImageCLEF Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">School of Computer Engineering of Iran University of Science and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,154.90,93.42,11.96"><forename type="first">Yasaman</forename><surname>Lotfollahi</surname></persName>
							<email>y_lotfollahi@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.52,154.90,93.60,11.96"><forename type="first">Melika</forename><surname>Nobakhtian</surname></persName>
							<email>m_nobakhtian@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.28,154.90,97.42,11.96"><forename type="first">Malihe</forename><surname>Hajihosseini</surname></persName>
							<email>m_hajihosseini@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,80.85,11.96"><forename type="first">Sauleh</forename><surname>Eetemadi</surname></persName>
							<email>sauleh@iust.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Assistant Professor of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.70,15.42;1,89.29,106.66,35.39,15.43;1,89.29,129.00,230.34,11.96">IUST_NLPLAB at ImageCLEFmedical Caption Tasks 2023 Notebook for the ImageCLEF Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D420B8658E5B4D8B207F135D61FDD000</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Image Captioning</term>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present models implemented by the IUST_NLPLAB group for ImageCLEFmedical Caption Task 2023. This task contains two subtasks: Concept Detection and Caption Prediction. Under the first subtask, the model should extract medical concepts contained in radiology images. These concepts can be used for context-based image and information retrieval. Under the second subtask, the model predicts the caption for a medical image. This can be used for improving the diagnosis and treatment of diseases by saving time, money and helping physicians. This was our second experience to participate in this competition. We used diffrent models for both subtasks. We were able to get the 4th rank in the concepts detection subtask with a score of 0.49. Also, in the caption prediction subtask, we were able to get the 12th rank based on the BERTScore evaluation metric. This is despite the fact that our model has won the first rank based on ROUGE, BLEU and METEOR. From this, it can be concluded that the type of evaluation metric determined has an important effect on the results of this subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,138.32,470.24,16.34,10.91" target="#b0">[1]</ref> is part of CLEF 1 . ImageCLEF was launched in 2003 and added a medical task in 2004. Although it started with four participants, in 2020 was able to attract more than one hundred and ten participants from all around the world to participate in the competition. ImageCLEF includes various sections that retrieve and classify visual information using textual and visual data and their combinations.</p><p>In 2022, imageclef used the AIcrowd<ref type="foot" coords="2,255.85,85.21,3.71,7.97" target="#foot_0">2</ref> platform to publish contest data and receive submissions from participating groups <ref type="bibr" coords="2,203.37,100.52,14.01,10.91" target="#b1">[2]</ref>. In that platform, groups could see the score earned after each submission and plan to improve their models'score. However, the score obtained by other groups could not be seen.</p><p>In ImageCLEF 2023 <ref type="bibr" coords="2,186.37,141.16,13.55,10.91" target="#b2">[3]</ref>, the contest data was made available to participating groups via a private GitHub link. Also, sciebo <ref type="foot" coords="2,238.17,152.96,3.71,7.97" target="#foot_1">3</ref> system was used to receive the results sent by the groups. Participating groups could have a maximum of 10 successful submissions in each subtask. In each run, in addition to the test data results in csv format, a txt file containing a brief description of the method should be attached. Unfortunately, unlike the AIcrowd platform, in the sciebo system, the scores obtained after each submission were not presented, and it was not possible to improve the models and analyze them by comparing the obtained results.</p><p>In ImageCLEFmedical 2023, four tasks were proposed We selected the Image Captioning task from the ImageCLEFmedical section to participate in the competition. ImageCLEF medical Image Captioning task in 2023 <ref type="bibr" coords="2,383.12,331.04,13.10,10.91" target="#b3">[4]</ref>, like last year, contained two subtasks: Concepts Detection and Caption Prediction. Each group could participate in one or both subtasks. In this paper, we present the methods our group, IUST_NLPLAB, from the Iran University of Science and Technology<ref type="foot" coords="2,301.52,369.93,3.71,7.97" target="#foot_2">4</ref> , School of Computer Engineering<ref type="foot" coords="2,461.67,369.93,3.71,7.97" target="#foot_3">5</ref> , Natural Language Pocessing Laboratory<ref type="foot" coords="2,232.33,383.48,3.71,7.97" target="#foot_4">6</ref> used in both subtasks. This is our second time participating in the ImageCLEF competition. We participated in both subtasks and registered 7 successful submissions in the concept detection subtask and 10 successful submissions in the caption prediction subtasks.</p><p>In the concept detection subtask, we were able to win the fourth place in the competition with a gap of about 2 percent in F1-score from the first ranked group. Also, in the subtask of caption prediction, we were able to get the 12th rank of the competition based on BERTScore <ref type="bibr" coords="2,487.57,466.53,14.71,10.91" target="#b4">[5]</ref>, which was the main evaluation metric of the competition. But based on other evaluation metrics such as ROUGE <ref type="bibr" coords="2,152.05,493.63,16.60,10.91" target="#b5">[6]</ref>, BLEU <ref type="bibr" coords="2,198.67,493.63,16.33,10.91" target="#b6">[7]</ref> and METEOR <ref type="bibr" coords="2,270.86,493.63,17.04,10.91" target="#b7">[8]</ref>, our group was able to win the first rank among other participating groups.</p><p>In the following sections, we will describe the task, datasets, models developed and the results we achieved in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task description</head><p>This year the ImageCLEF evaluation campaign hosted the 7th edition of the medical image caption task. Unlike some of the previous editions which only contained the caption prediction task (e.g., 2016 <ref type="bibr" coords="3,151.49,342.48,13.59,10.91" target="#b8">[9]</ref>) or only the concept detection task (e.g., 2019 <ref type="bibr" coords="3,366.75,342.48,17.49,10.91" target="#b9">[10]</ref>), the 7th edition, as like as last year, contained both subtasks as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>In this subtask, the goal is to extract medical concepts in medical images. These concepts are selected from UMLS 7 <ref type="bibr" coords="3,201.84,419.31,18.06,10.91" target="#b10">[11]</ref> Concept Unique Identifiers (CUIs) specified in the dataset. The extraction of these concepts can be used for image retrieval and context-based information purposes.</p><p>The 2023 dataset contains 2,125 medical concepts, which has decreased compared to last year's dataset. Table <ref type="table" coords="3,183.15,473.51,5.11,10.91" target="#tab_1">1</ref> shows a list of the 15 most frequent concepts in the training collection based on their frequency. According to the table published in the <ref type="bibr" coords="3,375.16,487.05,11.28,10.91" target="#b1">[2]</ref>, most of the most frequent concepts of the 2023 dataset are in common with the most frequent concepts of the 2022 dataset, but their frequency has decreased compared to last year. The lowest rate frequency of concepts in the training set is related to six concepts, each of them was repeated only 2 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>In this subtask, the goal is to generate a suitable caption for the input medical images. Extracting medical concepts can help in producing a more appropriate captions. This subtask consists of a combination of text and image processing and is more complicated than the previous subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>The dataset introduced for the ImageCLEFmedical Caption 2023 is a subset of the Radiology Objects in COntext (ROCO) <ref type="bibr" coords="4,207.97,124.83,21.04,10.91" target="#b11">[12]</ref> dataset. The dataset published in 2023 was structurally similar to the dataset of 2022. In this year's dataset, there were 60,918 training data, which was reduced compared to last year's dataset, but the validation and testing datasets included 10,437 and 10,473 data, respectively, which increased compared to last year. For each image in the training and validation dataset, the concepts in the image and a suitable caption of it were provided.</p><p>In the following, more details of the data of each subtask are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Concepts</head><p>In this subtask, each image in the dataset has several related concepts. These concepts have originated from the Unified Medical Language System (UMLS) <ref type="bibr" coords="4,362.98,255.85,20.72,10.91" target="#b10">[11]</ref> Concept Unique Identifiers (CUIs). The generated concepts are based on a reduced subset of the UMLS 2022 AB release<ref type="foot" coords="4,501.78,267.65,3.71,7.97" target="#foot_5">8</ref> this year. Filtering images according to their semantic type was performed to reach a higher possibility of recognizing concepts in images. Concepts with a low occurrence were removed based on recommendations from previous years. Each image has a different number of concepts. The overall number of concepts is 2125. An image has at least one related concept and at most 24 concepts. Most images in the dataset have three concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Captions</head><p>A caption is provided for each image in the training and validation sets in this subtask. Last year, the provided captions were pre-processed in four stages, but according to the explanations provided by the organizers, in this year only one pre-processing step, removal of links from the captions, was done on the captions.</p><p>Based on the analysis performed on the training dataset, 63 images have one-word captions, which is the shortest caption length in the dataset. The maximum length of the caption is 410 words, which is related to one image. Also, the average number of words in captions is 20 words. We also calculated the TTR 9 for this annotation dataset. TTR is obtained by dividing the number of unique words by the text size and is a simple measure of lexical diversity <ref type="bibr" coords="4,486.12,508.82,17.23,10.91" target="#b12">[13]</ref>. Considering the stop words, the TTR value in this dataset is 0.07 and without considering the stop words, it is 0.05, both of them have increased compared to last year. Figure <ref type="figure" coords="4,460.74,535.91,5.17,10.91" target="#fig_1">1</ref> displays the frequently recurring words in the captions of the training set, along with their frequency including and excluding stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Sample images from the training set along with their concepts and captions <ref type="bibr" coords="5,398.30,101.55,16.53,9.96" target="#b13">[14,</ref><ref type="bibr" coords="5,417.33,101.55,11.46,9.96" target="#b14">15,</ref><ref type="bibr" coords="5,431.28,101.55,11.46,9.96" target="#b15">16,</ref><ref type="bibr" coords="5,445.22,101.55,11.29,9.96" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Concepts Caption</p><p>‚Ä¢ C0040405 (X-Ray Computed Tomography)</p><formula xml:id="formula_0" coords="5,217.14,191.61,125.07,35.23">‚Ä¢ C0817096 (Chest) ‚Ä¢ C0497156 (Lymphadenopa- thy)</formula><p>‚Ä¢ Thoracic CT scan showing perihilar pulmonary lymphadenomegaly.</p><p>‚Ä¢ C1306645 (Plain x-ray)</p><p>‚Ä¢ C0037303 (Bone structure of cranium) ‚Ä¢ C0001168 (Complete obstruction)</p><p>‚Ä¢ Obturation.</p><formula xml:id="formula_1" coords="5,215.90,362.58,123.98,76.51">‚Ä¢ C1306645 (Plain x-ray) ‚Ä¢ C0023216 (Lower Extremity) ‚Ä¢ C0205129 (Sagittal) ‚Ä¢ C0449900 (Contrast used) ‚Ä¢ C0017067 (Ganglia) ‚Ä¢ C0206207 (Joint Capsule)</formula><p>‚Ä¢ Contrast X-ray of right knee. Lateral view showing the stalk of the ganglion not communicating with the joint capsule.</p><p>‚Ä¢ C0040405 (X-Ray Computed Tomography)  </p><formula xml:id="formula_2" coords="5,215.90,516.01,125.08,139.00">‚Ä¢ C0449900 (Contrast used) ‚Ä¢ C0000726 (Abdomen) ‚Ä¢ C0030797 (Pelvis ) ‚Ä¢ C1510412 (Pseudoa- neurysm) ‚Ä¢ C0205417 (Lobular) ‚Ä¢ C0278403 (Subcutaneous Tis- sue) ‚Ä¢ C0223651 (Iliac crest struc- ture) ‚Ä¢ C0018944 (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>In this section, we present our methods for concept detection and caption prediction subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>In concept detection subtask, we used different image preprocessing methods. When we used CLIP <ref type="bibr" coords="6,109.52,428.24,20.23,10.91" target="#b17">[18]</ref> and PubMedCLIP <ref type="bibr" coords="6,207.93,428.24,22.25,10.91" target="#b18">[19]</ref> models, we used their preprocess method. When used other pretrained models such as Resnet <ref type="bibr" coords="6,235.22,441.79,18.91,10.91" target="#b19">[20]</ref> and Efficientnet <ref type="bibr" coords="6,325.82,441.79,16.67,10.91" target="#b20">[21]</ref>, we used CLAHE <ref type="bibr" coords="6,415.03,441.79,21.70,10.91" target="#b21">[22]</ref>. CLAHE is the one of ways to increase quality of image.</p><p>In the following, we explain our developed models for concept detection subtask. We used two methods: Ensemble Models as v1 and Multi-Label Classification Method as v2. Table <ref type="table" coords="6,500.81,482.44,5.17,10.91" target="#tab_2">3</ref> shows the details of the all developed models in concept detection subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Ensemble Models</head><p>One of the systems that we designed was based on ensemble systems. We adopted this method according to the winner of last year, the AUEB-NLP group <ref type="bibr" coords="6,358.07,558.87,18.97,10.91" target="#b22">[23]</ref>. We utilized two instances of EfficientNetV2B0 <ref type="bibr" coords="6,178.50,572.42,19.24,10.91" target="#b23">[24]</ref> for this model. All layers of base models were frozen until the last convolutional layer during the training process. A dense layer was added to each model to predict concepts. Models were trained for 50 epochs with a batch size of 256. We considered different thresholds for each concept to find out if a concept is related to an image or not. We tried certain thresholds on validation data and found the best one regarding F1-score. Every five-epoch model weights and best thresholds were saved. After training, the best weights and thresholds were chosen for each model. To predict concepts, if both models assigned a concept to an image, we concluded that this image has this concept, in other words, we used an intersection of concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Multi-Label Classification Method</head><p>In this approach, we built a multi-label classification model to predict the correct concepts for each input image. We used CNNs with pre-trained weights from ImageNet <ref type="bibr" coords="7,409.67,323.27,19.19,10.91" target="#b24">[25]</ref>. These networks were modified by removing their final layer and adding a classification layer. Then, they were fine-tuned on the target dataset. We tried fine-tuning different pre-trained models and applied various thresholds to find the best results. In this year, we also used the vision-language models of CLIP <ref type="bibr" coords="7,120.02,377.47,19.63,10.91" target="#b17">[18]</ref> and its medical version, PubMedCLIP <ref type="bibr" coords="7,298.40,377.47,20.69,10.91" target="#b18">[19]</ref>, which had achieved good results in many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>In the caption prediction subtask, we utilized the approach we developed for the previous year's challenge, where we achieved first place. This methodology treats each word in a caption as a label corresponding to the associated image. We trained a multi-label classification model to predict the words that will ultimately form a caption for the given image.</p><p>To extract image features for the subtask, we used a pre-trained CNN on ImageNet <ref type="bibr" coords="7,485.30,494.94,20.68,10.91" target="#b24">[25]</ref> and fine-tuned it on the training set. During fine-tuning, we excluded the last layer of the CNN and added a dropout layer and a dense layer. We tried various CNN models to explore different possibilities. Similar to the concept detection subtask, in this subtask, we used the vision-language models of CLIP <ref type="bibr" coords="7,228.76,549.14,19.93,10.91" target="#b17">[18]</ref> and its medical version, PubMedCLIP <ref type="bibr" coords="7,411.92,549.14,21.06,10.91" target="#b18">[19]</ref>.</p><p>The model generates captions for images by predicting the corresponding words. The probability of each word is computed in the output layer using the sigmoid activation function. Two methods are used to select the candidate words:</p><p>1. The top ùëÅ words with the highest probability are chosen. ùëÅ is a hyper-parameter that will define the length of captions. 2. A threshold is applied to the model output. Words with probabilities higher than the threshold are chosen to create the caption. After extracting the correct words, we need to sort them to create the full caption. Two methods are used to arrange the words:</p><p>1. Words are arranged from highest to lowest probability. 2. Words are ordered based on their statistical occurrence within the training set. Each word is assigned to its most common position in the caption.</p><p>Different values of ùëÅ and threshold were applied to the output. Although BERTScore <ref type="bibr" coords="8,480.26,543.49,15.74,10.91" target="#b4">[5]</ref> is the primary score in this year's competition, we were not able to use this metric to evaluate our models because of our resource limits. Therefore, we used the BLEU <ref type="bibr" coords="8,410.88,570.59,16.60,10.91" target="#b6">[7]</ref> score to evaluate our models and find the best hyperparameters. Details of each submission and their results are described in table 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the previous parts, the details of the models implemented by our group and the results obtained by each one were explained.</p><p>In the concept detection subtask, two metrics, F1-Score and F1-Score Manual, which was calculated using a subset of manually validated concepts, were used to evaluate the models, but the results of the competition was based on the F1-Score metric. In 2023, 9 groups from all over the world participated in the concept detection subtask and managed to register successful submissions. The details of the results announced by the organizers of the competition in this subtask are presented in Table <ref type="table" coords="9,227.68,154.71,3.76,10.91" target="#tab_5">6</ref>. Among the submissions of our group, Run ID 7, which used the basic model of PubMedCLIP ViT-B/32 <ref type="bibr" coords="9,276.26,168.26,19.53,10.91" target="#b18">[19]</ref> to extract the features of the images, was able to get the best result with a difference of about 2 percent from the first group and achived 4th rank in this competition, which has increased four ranks compared to last year's results.</p><p>In the caption prediction subtask, seven metrics: BERTScore <ref type="bibr" coords="9,370.18,208.91,15.27,10.91" target="#b4">[5]</ref>, ROUGE <ref type="bibr" coords="9,421.86,208.91,16.99,10.91" target="#b5">[6]</ref>, BLEURT <ref type="bibr" coords="9,480.06,208.91,21.70,10.91" target="#b25">[26]</ref>, BLEU <ref type="bibr" coords="9,109.29,222.46,15.00,10.91" target="#b6">[7]</ref>, METEOR <ref type="bibr" coords="9,165.57,222.46,16.82,10.91" target="#b7">[8]</ref>, CIDEr <ref type="bibr" coords="9,215.79,222.46,20.12,10.91" target="#b26">[27]</ref> and CLIPScore <ref type="bibr" coords="9,299.32,222.46,18.70,10.91" target="#b27">[28]</ref>, were used to evaluate the models, but the results of the competition was based on the BERTScore metric. In 2023, 13 groups from all over the world participated in the caption prediction subtask and managed to register successful submissions. The details of the results announced by the organizers of the competition in this subtask are presented in Table <ref type="table" coords="9,227.68,276.66,3.76,10.91" target="#tab_6">7</ref>. Among the submissions of our group, Run ID 6, which used the basic model of PubMedCLIP RN50x4 <ref type="bibr" coords="9,266.44,290.20,21.07,10.91" target="#b18">[19]</ref> to extract the features of the images, was able to get the best result with a difference of about 7 Percent from the first group and achived 12th rank in this competition.</p><p>The noteworthy point is that based on three metrics ROUGE, BLEU and METEOR, our group has been able to get the first rank among the participating groups.</p><p>Differences in rankings based on different metrics can show challenges in evaluating generated captions. This is due to the differences in how these metrics evaluate the quality of generated captions. For example, the BLEU score measures n-gram overlap between the generated and reference captions, rewarding precision, and the presence of matching n-grams. In contrast, BERTScore used contextualized embeddings from BERT to capture semantic similarity, taking into account both word order and correctness. Consequently, a result with higher n-gram overlap but potential issues in word order or overall fluency could receive a better BLEU score and a lower BERTScore.</p><p>While our models can generate relevant words to describe the input image, they struggle to shape them into actual sentences which are semantically similar to the original caption. This issue can be one of the reasons why our results have low BERTScores, while having high BLEU scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper describes the participation of IUST_NLPLAB at Iran University of Science and Technology at ImageCLEFmedical caption 2023 task.</p><p>In the concept detection subtask, we ranked 4 among 9 participating teams. We used MLC and ensemble models in this subtask. Our MLC methods with PubMedCLIP ViT-B/32 as a base model had better overall score.</p><p>In the caption prediction subtask, last year, our group won the first place in the competition based on the BLEU evaluation metric, but this year it won the 12th place in the competition based on the BERTScore metric. Based on the published results, our group was able to win first place in all 10 of its submissions based on the three metric ROUGE, BLEU and METEOR.</p><p>Based on these results, it can be concluded that the selection of evaluation metric in the analysis of the models presented for this subtask is very important and the results based on different evaluation metric can have significant differences from each other. This year was our second experience of participating in this competition and we hope to be able to participate in these competitions in the coming years and gain new experiences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,271.69,645.05,46.58,9.96;5,369.58,490.75,123.41,9.96;5,378.53,502.70,114.46,9.96;5,378.53,514.66,114.46,9.96;5,378.53,526.61,116.12,9.96;5,378.53,538.57,114.46,9.96;5,378.53,550.52,116.12,9.96;5,378.53,562.48,116.12,9.96;5,378.53,574.43,114.46,9.96;5,378.53,586.39,115.56,9.96;5,378.53,598.34,116.12,9.96;5,378.53,610.30,114.46,9.96;5,378.53,622.25,116.12,9.96;5,378.53,634.21,115.03,9.96;6,99.87,252.09,187.81,9.96;6,307.30,252.09,187.81,9.96;6,320.69,264.04,81.44,9.96"><head></head><label></label><figDesc>Hematoma) ‚Ä¢ Contrast-enhanced CT scan of the lower abdomen and pelvis showing a single lobe of a presumed, bilobed pseudoaneurysm (a) as well as a 3.5 √ó 5.5 √ó 6 cm rimenhancing, lobular collection of the superior right gluteal subcutaneous tissues, just superior to the right iliac crest and lateral to the paraspinal musculature, consistent with a hematoma (b) (a) Ten most frequent words in the training set. (b) Ten most frequent words in the training set without stop words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,286.78,220.29,9.96;6,100.17,84.19,187.51,161.03"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ten most frequent words in the training set</figDesc><graphic coords="6,100.17,84.19,187.51,161.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,103.64,254.86,329.09,54.06"><head></head><label></label><figDesc>1. Image Captioning. 2. Controlling the Quality of Synthetic Medical Images created via GANs. 3. Visual Question Answering for Colonoscopy Images. 4. Medical Dialogue Summarization.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,90.49,324.79,225.45"><head>Table 1</head><label>1</label><figDesc>Most frequent concepts in the training data</figDesc><table coords="3,179.00,121.17,234.79,194.77"><row><cell>UMLS CUI</cell><cell>UMLS Meaning</cell><cell>frequency</cell></row><row><cell cols="2">C0040405 X-Ray Computed Tomography</cell><cell>20955</cell></row><row><cell>C1306645</cell><cell>Plain x-ray</cell><cell>17108</cell></row><row><cell>C0024485</cell><cell>Magnetic Resonance Imaging</cell><cell>10062</cell></row><row><cell>C0041618</cell><cell>Ultrasonography</cell><cell>8390</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>6805</cell></row><row><cell>C1999039</cell><cell>Anterior-Posterior</cell><cell>5907</cell></row><row><cell>C0449900</cell><cell>Contrast used</cell><cell>4945</cell></row><row><cell>C0002978</cell><cell>angiogram</cell><cell>4194</cell></row><row><cell>C0037303</cell><cell>Bone structure of cranium</cell><cell>3058</cell></row><row><cell>C1996865</cell><cell>Postero-Anterior</cell><cell>2911</cell></row><row><cell>C0039985</cell><cell>Plain chest X-ray</cell><cell>2884</cell></row><row><cell>C0000726</cell><cell>Abdomen</cell><cell>2824</cell></row><row><cell>C0030797</cell><cell>Pelvis</cell><cell>2590</cell></row><row><cell>C0023216</cell><cell>Lower Extremity</cell><cell>1989</cell></row><row><cell>C0205129</cell><cell>Sagittal</cell><cell>1930</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,409.13,129.81"><head>Table 3</head><label>3</label><figDesc>Concept detection submissions' details of IUST_NLPLAB group.</figDesc><table coords="7,97.16,121.17,400.96,99.13"><row><cell cols="2">Run ID Type</cell><cell>Base model</cell><cell cols="4">Best Epochs Batch size Best Threshold F1-score</cell></row><row><cell>7</cell><cell>v2</cell><cell>PubMedCLIP ViT-B/32</cell><cell>20</cell><cell>256</cell><cell>0.2</cell><cell>0.495</cell></row><row><cell>5</cell><cell>v2</cell><cell>Resnet50</cell><cell>5</cell><cell>256</cell><cell>0.3</cell><cell>0.485</cell></row><row><cell>6</cell><cell>v2</cell><cell>CLIP ViT-B/32</cell><cell>30</cell><cell>256</cell><cell>0.3</cell><cell>0.440</cell></row><row><cell>3</cell><cell>v1</cell><cell>EfficientnetB0</cell><cell>50</cell><cell>256</cell><cell>-</cell><cell>0.435</cell></row><row><cell>4</cell><cell>v2</cell><cell>Resnet50</cell><cell>5</cell><cell>128</cell><cell>0.3</cell><cell>0.433</cell></row><row><cell>8</cell><cell>v2</cell><cell>PubMedCLIP RN50</cell><cell>30</cell><cell>256</cell><cell>0.2</cell><cell>0.421</cell></row><row><cell>2</cell><cell>v2</cell><cell>PubMedCLIP RN50x4</cell><cell>20</cell><cell>256</cell><cell>0.3</cell><cell>0.380</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,378.49,165.67"><head>Table 4</head><label>4</label><figDesc>Caption prediction submissions' details of IUST_NLPLAB group.</figDesc><table coords="8,127.79,121.17,339.69,134.99"><row><cell>Run ID</cell><cell>Base model</cell><cell cols="4">Best Epochs Word limit Threshold Sorted</cell></row><row><cell>6</cell><cell>PubMedCLIP RN50x4</cell><cell>90</cell><cell>20</cell><cell>-</cell><cell>Yes</cell></row><row><cell>2</cell><cell>ResNet50</cell><cell>20</cell><cell>19</cell><cell>-</cell><cell>Yes</cell></row><row><cell>4</cell><cell>CLIP RN50x4</cell><cell>90</cell><cell>20</cell><cell>-</cell><cell>Yes</cell></row><row><cell>10</cell><cell>PubMedCLIP RN50x4</cell><cell>95</cell><cell>-</cell><cell>0.1</cell><cell>Yes</cell></row><row><cell>8</cell><cell>CLIP RN50x4</cell><cell>95</cell><cell>-</cell><cell>0.1</cell><cell>Yes</cell></row><row><cell>5</cell><cell>PubMedCLIP RN50x4</cell><cell>90</cell><cell>20</cell><cell>-</cell><cell>No</cell></row><row><cell>1</cell><cell>ResNet50</cell><cell>20</cell><cell>19</cell><cell>-</cell><cell>No</cell></row><row><cell>3</cell><cell>CLIP RN50x4</cell><cell>90</cell><cell>20</cell><cell>-</cell><cell>No</cell></row><row><cell>9</cell><cell>PubMedCLIP RN50x4</cell><cell>95</cell><cell>-</cell><cell>0.1</cell><cell>No</cell></row><row><cell>7</cell><cell>CLIP RN50x4</cell><cell>95</cell><cell>-</cell><cell>0.1</cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,277.65,386.51,160.59"><head>Table 5</head><label>5</label><figDesc>Caption prediction submissions' results of IUST_NLPLAB group.</figDesc><table coords="8,119.78,308.33,355.72,129.91"><row><cell cols="8">Run ID BERTScore ROUGE BLEURT BLEU METEOR CIDEr CLIPScore</cell></row><row><cell>6</cell><cell>0.567</cell><cell>0.290</cell><cell>0.223</cell><cell>0.268</cell><cell>0.104</cell><cell>0.177</cell><cell>0.807</cell></row><row><cell>2</cell><cell>0.565</cell><cell>0.271</cell><cell>0.209</cell><cell>0.241</cell><cell>0.089</cell><cell>0.159</cell><cell>0.805</cell></row><row><cell>4</cell><cell>0.561</cell><cell>0.280</cell><cell>0.210</cell><cell>0.259</cell><cell>0.095</cell><cell>0.162</cell><cell>0.806</cell></row><row><cell>10</cell><cell>0.556</cell><cell>0.275</cell><cell>0.212</cell><cell>0.264</cell><cell>0.096</cell><cell>0.142</cell><cell>0.801</cell></row><row><cell>8</cell><cell>0.553</cell><cell>0.269</cell><cell>0.203</cell><cell>0.264</cell><cell>0.096</cell><cell>0.134</cell><cell>0.803</cell></row><row><cell>5</cell><cell>0.549</cell><cell>0.290</cell><cell>0.201</cell><cell>0.268</cell><cell>0.100</cell><cell>0.174</cell><cell>0.804</cell></row><row><cell>1</cell><cell>0.546</cell><cell>0.271</cell><cell>0.189</cell><cell>0.241</cell><cell>0.089</cell><cell>0.156</cell><cell>0.803</cell></row><row><cell>3</cell><cell>0.544</cell><cell>0.280</cell><cell>0.186</cell><cell>0.259</cell><cell>0.094</cell><cell>0.158</cell><cell>0.803</cell></row><row><cell>9</cell><cell>0.539</cell><cell>0.275</cell><cell>0.177</cell><cell>0.264</cell><cell>0.096</cell><cell>0.142</cell><cell>0.797</cell></row><row><cell>7</cell><cell>0.537</cell><cell>0.269</cell><cell>0.168</cell><cell>0.264</cell><cell>0.095</cell><cell>0.133</cell><cell>0.797</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,88.99,90.49,346.80,153.72"><head>Table 6</head><label>6</label><figDesc>Results of ImageCLEFmedical 2023 concept detection subtask.</figDesc><table coords="10,159.49,121.17,276.30,123.04"><row><cell>Team Name</cell><cell cols="3">Best Run ID F1-Score F1-Score Manual</cell></row><row><cell>AUEB-NLP-Group</cell><cell>4</cell><cell>0.522272</cell><cell>0.925842</cell></row><row><cell>KDE-Lab_Med</cell><cell>10</cell><cell>0.507414</cell><cell>0.932091</cell></row><row><cell>VCMI</cell><cell>8</cell><cell>0.499812</cell><cell>0.916184</cell></row><row><cell>IUST_NLPLAB</cell><cell>7</cell><cell>0.495863</cell><cell>0.880381</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>1</cell><cell>0.495730</cell><cell>0.910585</cell></row><row><cell>CS_Morgan</cell><cell>2</cell><cell>0.483401</cell><cell>0.890151</cell></row><row><cell>SSNSheerinKavitha</cell><cell>1</cell><cell>0.464894</cell><cell>0.860296</cell></row><row><cell>closeAI2023</cell><cell>5</cell><cell>0.448105</cell><cell>0.856928</cell></row><row><cell>SSN_MLRG</cell><cell>3</cell><cell>0.017250</cell><cell>0.112211</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,88.99,266.30,440.91,184.50"><head>Table 7</head><label>7</label><figDesc>Results of ImageCLEFmedical 2023 Caption prediction subtask.</figDesc><table coords="10,95.27,296.98,434.64,153.82"><row><cell>Team Name</cell><cell cols="2">BERTScore ROUGE BLEURT</cell><cell>BLEU</cell><cell>METEOR</cell><cell>CIDEr</cell><cell>CLIPScore</cell></row><row><cell>closeAI2023</cell><cell>0.628106</cell><cell cols="4">0.240061 0.320915 0.184624 0.087254 0.237704</cell><cell>0.807454</cell></row><row><cell>AUEB-NLP-Group</cell><cell>0.617034</cell><cell cols="4">0.213014 0.295011 0.169212 0.071982 0.146601</cell><cell>0.803888</cell></row><row><cell>PCLmed</cell><cell>0.615190</cell><cell cols="4">0.252756 0.316561 0.217150 0.092063 0.231535</cell><cell>0.802123</cell></row><row><cell>VCMI</cell><cell>0.614736</cell><cell cols="4">0.217545 0.308386 0.165322 0.073449 0.172042</cell><cell>0.808184</cell></row><row><cell>KDE-Lab_Med</cell><cell>0.614538</cell><cell cols="4">0.222341 0.301391 0.156465 0.072441 0.181853</cell><cell>0.806207</cell></row><row><cell>SSN_MLRG</cell><cell>0.601933</cell><cell cols="4">0.211177 0.277434 0.141797 0.061514 0.128443</cell><cell>0.775915</cell></row><row><cell>DLNU_CCSE</cell><cell>0.600546</cell><cell cols="4">0.202888 0.262998 0.105948 0.055716 0.133207</cell><cell>0.772518</cell></row><row><cell>CS_Morgan</cell><cell>0.581949</cell><cell cols="4">0.156419 0.224238 0.056632 0.043649 0.083982</cell><cell>0.759258</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>0.581625</cell><cell cols="4">0.218103 0.269043 0.145035 0.070155 0.173664</cell><cell>0.789327</cell></row><row><cell>Bluefield-2023</cell><cell>0.577966</cell><cell cols="4">0.153448 0.271642 0.154316 0.060069 0.100910</cell><cell>0.783725</cell></row><row><cell>IUST_NLPLAB</cell><cell>0.566886</cell><cell cols="4">0.289774 0.222957 0.268452 0.100354 0.177266</cell><cell>0.806763</cell></row><row><cell>SSNSheerinKavitha</cell><cell>0.544106</cell><cell cols="4">0.086648 0.215170 0.074905 0.025768 0.014313</cell><cell>0.687312</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,92.57,627.20,196.18,8.97"><p>https://www.aicrowd.com/ (last accessed: 2023-07-05)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,92.57,638.16,213.85,8.97"><p>https://hochschulcloud.nrw/en/ (last accessed: 2023-07-05)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,92.57,649.12,186.05,8.97"><p>http://www.iust.ac.ir/en (last accessed: 2023-07-05)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="2,92.57,660.08,185.66,8.97"><p>http://ce-inter.iust.ac.ir/ (last accessed: 2023-07-05)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="2,92.57,671.04,180.54,8.97"><p>https://nlplab.iust.ac.ir (last accessed: 2023-07-05)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="4,92.57,660.07,413.29,8.97"><p>https://www.nlm.nih.gov/pubs/techbull/nd22/nd22_umls_2022ab_release_available.html (last accessed: 2023-07-05)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been supported by <rs type="funder">School of Computer Engineering of Iran University of Science and Technology</rs> and we are very grateful for their support.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,111.28,395.01,10.91;11,112.66,124.83,395.17,10.91;11,112.39,138.38,394.80,10.91;11,112.66,151.93,394.62,10.91;11,112.66,165.48,393.33,10.91;11,112.66,179.03,395.17,10.91;11,112.66,192.57,393.54,10.91;11,112.66,206.12,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,352.50,151.93,154.78,10.91;11,112.66,165.48,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.26,165.48,70.73,10.91;11,112.66,179.03,395.17,10.91;11,112.66,192.57,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="11,359.20,192.57,147.00,10.91;11,112.66,206.12,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,219.67,394.53,10.91;11,112.66,233.22,234.01,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajihosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lotfollahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nobakhtian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Javid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Omidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eetemadi</surname></persName>
		</author>
		<title level="m" coord="11,112.66,233.22,202.09,10.91">Iust_nlplab at imageclefmedical caption tasks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,246.77,394.53,10.91;11,112.66,260.32,395.17,10.91;11,112.66,273.87,394.53,10.91;11,112.66,287.42,395.17,10.91;11,112.39,300.97,394.80,10.91;11,112.48,314.52,394.70,10.91;11,112.66,328.07,395.17,10.91;11,112.66,341.62,393.32,10.91;11,112.66,355.17,394.52,10.91;11,112.33,368.71,120.27,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,224.93,314.52,282.25,10.91;11,112.66,328.07,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcƒ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,363.94,328.07,143.89,10.91;11,112.66,341.62,393.32,10.91;11,112.66,355.17,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,280.09,355.17,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,382.26,394.52,10.91;11,112.66,395.81,395.17,10.91;11,112.66,409.36,393.33,10.91;11,112.66,422.91,247.61,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,298.72,395.81,209.11,10.91;11,112.66,409.36,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,307.89,409.36,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="11,429.41,409.36,76.58,10.91;11,112.66,422.91,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,436.46,393.33,10.91;11,112.66,450.01,271.84,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m" coord="11,384.35,436.46,121.63,10.91;11,112.66,450.01,90.07,10.91">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,463.56,393.33,10.91;11,112.66,477.11,133.03,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,154.48,463.56,243.31,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,419.57,463.56,86.42,10.91;11,112.66,477.11,55.58,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,490.66,393.33,10.91;11,112.66,504.21,393.53,10.91;11,112.66,517.76,203.57,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,310.87,490.66,195.12,10.91;11,112.66,504.21,88.86,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,224.89,504.21,281.30,10.91;11,112.66,517.76,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,531.30,393.33,10.91;11,112.66,544.85,393.33,10.91;11,112.66,558.40,134.04,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,234.11,531.30,271.87,10.91;11,112.66,544.85,104.59,10.91">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.66,544.85,263.33,10.91;11,112.66,558.40,46.51,10.91">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,571.95,393.59,10.91;11,112.66,585.50,394.53,10.91;11,112.66,599.05,22.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,380.21,571.95,126.04,10.91;11,112.66,585.50,77.03,10.91">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,212.47,585.50,289.72,10.91">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,612.60,394.53,10.91;11,112.28,626.15,395.55,10.91;11,112.66,639.70,394.53,10.91;11,112.66,653.25,395.17,10.91;11,112.66,666.80,393.33,10.91;12,112.66,86.97,394.53,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,394.52,10.91;12,112.66,127.61,118.94,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,318.99,666.80,186.99,10.91;12,112.66,86.97,189.06,10.91">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodr√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,328.82,86.97,178.37,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,127.98,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,272.53,114.06,186.62,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,393.33,10.91;12,112.66,154.71,258.51,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,190.92,141.16,315.07,10.91;12,112.66,154.71,51.86,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,173.16,154.71,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.33,181.81,393.65,10.91;12,112.66,195.36,394.62,10.91;12,112.41,208.91,394.78,10.91;12,112.66,222.46,394.53,10.91;12,112.66,236.01,215.79,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,375.93,168.26,130.06,10.91;12,112.33,181.81,157.87,10.91">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,295.73,181.81,210.25,10.91;12,112.66,195.36,394.62,10.91;12,112.41,208.91,389.37,10.91;12,179.72,222.46,180.34,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="12,112.66,249.56,394.53,10.91;12,112.48,263.11,237.85,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,168.95,249.56,333.67,10.91">Can type-token ratio be used to show morphological complexity of languages?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kettunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,112.48,263.11,153.92,10.91">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="223" to="245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,276.66,393.32,10.91;12,112.66,290.20,393.98,10.91;12,112.41,303.75,38.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,465.58,276.66,40.41,10.91;12,112.66,290.20,258.99,10.91">Systemic sarcoidosis induced by etanercept: first brazilian case report</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Unterstell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Serpa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">P D</forename><surname>Fonseca E Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Gripp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,379.48,290.20,85.86,10.91">An. Bras. Dermatol</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="197" to="199" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,393.33,10.91;12,112.66,330.85,394.53,10.91;12,112.28,344.40,220.34,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,234.97,317.30,271.02,10.91;12,112.66,330.85,389.63,10.91">A new extraordinary means of appeal in the polish criminal procedure: the basic principles of a fair trial and a complaint against a cassatory judgment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zbiciak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Markiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,112.28,344.40,156.70,10.91">Access to Justice in Eastern Europe</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,357.95,395.17,10.91;12,112.66,371.50,385.35,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,193.65,371.50,144.81,10.91">Semimembranosus ganglion cyst</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumarasamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Kannadath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soundamourthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Sinhasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">V</forename><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,346.46,371.50,67.62,10.91">Anat. Cell Biol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="207" to="209" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,385.05,393.33,10.91;12,112.66,398.60,393.33,10.91;12,112.66,412.15,104.56,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,390.56,385.05,115.43,10.91;12,112.66,398.60,329.20,10.91">Delayed presentation of a lumbar artery pseudoaneurysm resulting from isolated penetrating trauma</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Counihan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Pontell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Selvan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trebelev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nunez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,450.66,398.60,55.33,10.91;12,112.66,412.15,19.23,10.91">J. Surg. Case Rep</title>
		<imprint>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,425.70,394.53,10.91;12,112.66,439.25,393.33,10.91;12,112.66,452.79,395.01,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,237.25,439.25,268.73,10.91;12,112.66,452.79,48.76,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.64,452.79,197.17,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,466.34,393.33,10.91;12,112.66,479.89,395.00,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13906</idno>
		<title level="m" coord="12,254.56,466.34,251.43,10.91;12,112.66,479.89,215.55,10.91">Does clip benefit visual question answering in the medical domain as much as it does in the general domain?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,493.44,394.53,10.91;12,112.28,506.99,128.06,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="12,150.25,493.44,353.02,10.91">Convolutional neural networks for multi-class histopathology image classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Talo</surname></persName>
		</author>
		<idno>ArXiv abs/1903.10035</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,520.54,394.53,10.91;12,112.66,534.09,346.82,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,178.42,520.54,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.29,534.09,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,547.64,393.98,10.91;12,112.41,561.19,38.81,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,180.79,547.64,220.34,10.91">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,409.50,547.64,67.10,10.91">Graphics gems</title>
		<imprint>
			<biblScope unit="page" from="474" to="485" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,574.74,393.32,10.91;12,112.66,588.29,273.53,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,322.78,574.74,183.20,10.91">Aueb nlp group at imageclefmed caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,153.91,588.29,201.59,10.91">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2019">2022. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,601.84,395.01,10.91;12,112.66,617.83,97.35,7.90" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m" coord="12,217.58,601.84,253.55,10.91">Efficientnetv2: Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,628.93,393.33,10.91;12,112.66,642.48,394.53,10.91;12,112.66,656.03,103.61,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,346.64,628.93,159.35,10.91;12,112.66,642.48,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.08,642.48,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,669.58,393.60,10.91;13,112.66,86.97,146.44,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="12,251.04,669.58,222.30,10.91">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04696</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,100.52,393.33,10.91;13,112.66,114.06,393.33,10.91;13,112.66,127.61,147.08,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,317.23,100.52,188.76,10.91;13,112.66,114.06,45.51,10.91">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,188.32,114.06,317.66,10.91;13,112.66,127.61,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,141.16,393.33,10.91;13,112.66,154.71,304.21,10.91" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="13,341.19,141.16,164.80,10.91;13,112.66,154.71,121.96,10.91">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
