<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,400.10,15.42;1,89.29,106.66,412.65,15.42;1,89.29,128.58,402.44,15.43;1,89.29,150.49,117.12,15.43;1,89.29,172.83,230.34,11.96">Overview of ImageCLEFmedical GANs 2023 Task -Identifying Training Data &quot;Fingerprints&quot; in Synthetic Biomedical Images Generated by GANs for Medical Image Security Notebook for the ImageCLEF Lab at CLEF 2023</title>
				<funder ref="#_RgrajUK #_7eGVdPS">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,198.73,142.00,11.96"><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.51,198.73,116.54,11.96"><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Belarusian Academy of Sciences</orgName>
								<address>
									<settlement>Minsk</settlement>
									<country key="BY">Belarus</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.69,198.73,59.64,11.96"><forename type="first">Ioan</forename><surname>Coman</surname></persName>
							<email>coman.ioan95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.90,212.68,73.57,11.96"><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Belarusian Academy of Sciences</orgName>
								<address>
									<settlement>Minsk</settlement>
									<country key="BY">Belarus</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.11,212.68,77.54,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.65,212.68,78.20,11.96"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Applied Sciences Western Switzerland</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,400.10,15.42;1,89.29,106.66,412.65,15.42;1,89.29,128.58,402.44,15.43;1,89.29,150.49,117.12,15.43;1,89.29,172.83,230.34,11.96">Overview of ImageCLEFmedical GANs 2023 Task -Identifying Training Data &quot;Fingerprints&quot; in Synthetic Biomedical Images Generated by GANs for Medical Image Security Notebook for the ImageCLEF Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B3424BD259E4AA4A97AB87BDF96C9BA3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence and deep learning</term>
					<term>generative models</term>
					<term>medical synthetic data</term>
					<term>medical imaging</term>
					<term>ImageCLEF benchmarking lab</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2023 ImageCLEFmedical GANs task is the first edition of this task, examining the existing hypothesis that GANs (Generative Adversarial Networks) are generating medical images that contain the "fingerprints" of the real images used for generative network training. The objective proposed to the participants is to identify the real images that were used to obtain some synthetic images using Generative Models. Overall, 23 teams registered to the task, 8 of them finalizing the task and submitting runs. A total of 40 runs were received. An analysis of the proposed methods shows a great diversity among them, ranging from texture analysis, similarity-based approaches that join inducer predictions like SVM or KNN, to deep learning approaches and even multi-stage transfer learning. This paper presents the overview of 2023 ImageCLEFmedical GANs task by describing its datasets, evaluation metrics as well as a discussion of the participants runs and results, and the future challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,144.86,537.99,12.99,10.91" target="#b0">[1]</ref> is part of the CLEF initiative 1 and presents a set of multimedia information retrieval tasks. Medical tasks were included in the 2nd edition of ImageCLEF in 2004 and have been held every year since then. The 2023 ImageCLEFmedical GANs task is the first edition of this task, examining the existing hypothesis that GANs (Generative Adversarial Networks) are generating medical images that contain the "fingerprints" of the real images used for generative network training. If this hypothesis is true, it may question the very nature of the synthetic images in term of copyright issues. So far, synthetic images are considered to be totally artificial data, thus no copyright issues can occur with respect to the real images.</p><p>In recent years, the emergence of generative models in the field of Artificial Intelligence (AI) has sparked significant interest and innovation, transforming various fields and revolutionizing the way we solve complex problems. The 2023 ImageCLEFmedical GANs Task offered an environment for investigating GANs' effects on the creation of synthetic medical images by providing a benchmark to explore the impact of GANs on artificial biomedical image generation. Medical image generation plays a critical role in medical research, training healthcare professionals, and improving patient care. While real patient data can be expensive, insufficient, or ethically challenging to acquire, the ability to generate synthetic yet realistic biomedical images can bridge these gaps and empower researchers, clinicians, and educators. Thus, Generative Models have demonstrated remarkable capabilities in generating high-quality images that mimic the characteristics and patterns of real data.</p><p>In this article, we present an overview of the 2023 ImageCLEFmedical GANs Task, describing the objective, data sets, evaluation metrics and participants' solutions. The reminder of the article is organized as follows. Section 2 introduces the scope and objectives of the task. Section 3 presents the evaluation metrics and Section 4 describes and presents the methods and results obtained by each participant team. Finally, Section 5 concludes the paper and presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task description</head><p>ImageCLEFmedical GANs task is a new challenge of the 2023 ImageCLEF lab <ref type="bibr" coords="2,419.19,416.58,11.28,10.91" target="#b0">[1]</ref>. The objective of the first edition of ImageCLEFmedical GANs task is to investigate the hypothesis that generative models generate medical images that exhibit resemblances to the images employed during their training. This addresses concerns surrounding the privacy and security of personal medical image data in the context of generating and utilizing artificial images in various real-world scenarios.</p><p>The task aims to identify distinctive features or "fingerprints" within synthetic biomedical image data, allowing us to determine which real images were used during the training process to generate the synthetic images. The task is formulated as following:</p><p>â¢ given a set that contains generated and real images, the participants are requested to employ machine learning and/or deep learning models to determine which of the real images were used to train the models to generate the provided synthetic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Description</head><p>For the ImageCLEFmedical GANs task, we provided a data set containing axial chest CT scans of lung tuberculosis patients. This means that some of them may appear pretty "normal" whereas the others may contain certain lung lesions including the severe ones. These images are stored </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation Methodology</head><p>The task was evaluated as a binary-class classification problem and the evaluation was carried out by measuring the F1-score, accuracy, precision, recall and specificity metrics. The official evaluation metric of this year's edition is the F1-score. The metrics are defined as follows: </p><formula xml:id="formula_0" coords="3,261.96,578.38,244.68,24.43">ð ððððð ððð = ð ð ð ð + ð¹ ð<label>(1)</label></formula><formula xml:id="formula_1" coords="3,277.66,605.60,228.98,24.43">ðððððð = ð ð ð ð + ð¹ ð<label>(2)</label></formula><formula xml:id="formula_2" coords="3,253.74,632.81,252.90,24.43">ðððððð ðððð¡ð¦ = ð ð ð ð + ð¹ ð<label>(3)</label></formula><formula xml:id="formula_3" coords="3,201.65,660.02,304.99,24.43">ð´ððð¢ðððð¦ = ð ð + ð ð ð ð + ð ð + ð¹ ð + ð¹ ð (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Participant Runs</head><p>Each participating team could submit up to 10 runs in total. Table <ref type="table" coords="4,408.97,310.40,5.17,10.91" target="#tab_0">1</ref> presents the list of participants and their institutions. The ranking is presented in Table <ref type="table" coords="4,395.04,323.95,5.04,10.91" target="#tab_1">2</ref> and it was according to the F1-score. A total of 40 runs were received from eight teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEF-CSE-GAN.</head><p>The best performing run from the CLEF-CSE-GAN team achieved an F1-score of 0.614. This team proposed three different workflows based on ResNet feature extractors <ref type="bibr" coords="4,136.27,391.69,11.38,10.91" target="#b1">[2]</ref>. First, aglomerative clustering is used to group similar images together based on generated features. By identifying clusters predominantly composed of real images, the authors enhance the ability to distinguish between real and artificial images effectively. Second, an SVM is implemented as a classifier that discerns real from artificial images, this time based on a one-dimensional flattened concatenation of the features from corresponding images pairs. The SVM model is trained using the combined feature representations obtained from the real and artificial images. And finally, a relation network based out of few-shot learning is used to fine-tune the backbone to learn fingerprints, learn a custom similarity comparison metric, and preserve spatial context by concatenating features as two-dimensional representations. All results obtained by the team are shown in Table <ref type="table" coords="4,305.49,513.63,5.07,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: relational model that used ResNet-101 as the backbone model for feature extraction. â¢ Submission #2: Hierarchical clustering approach.</p><p>â¢ Submission #3: SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMK-SSM.</head><p>The best performing run from DMK-SSM team achieved an F1-score of 0.480. They used Perceptual Hashing (pHash) algorithm <ref type="bibr" coords="4,284.08,615.39,11.32,10.91" target="#b2">[3]</ref>. It was applied on the generated images, which produces a hash, a fingerprint of the image. Similarly, hashes for the used and unused real images were generated and Hamming distance was computed between the used and the generated images. By doing this, pair of source image and generated images were identified and a threshold value of 50 was selected for the hamming distance. Two models were used: Convolutional Neural Network (CNN) and Scale-Invariant Feature Transformation (SIFT) algorithm with the K-Nearest Neighbors (KNN) classifier. All results obtained by the team are shown in Table <ref type="table" coords="5,500.81,659.49,5.17,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: 3 layers CNN model.</p><p>â¢ Submission #2: SIFT-KNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-ISI.</head><p>The best performing run from GAN-ISI team achieved an F1-score of 0.489. The authors used texture analysis to study characteristics of real and synthetic images <ref type="bibr" coords="6,476.52,158.18,11.59,10.91" target="#b3">[4]</ref>. A range of texture descriptors and analysis methods were used to identify discernible patterns within the synthetic image data and determine the source images employed for training.</p><p>The cumulative distribution function (CDF) of texture feature maps was calculated and the Wasserstein distance was applied to compare the CDFs of the query and generated images. A binary classifier was trained to predict the utilization of the query image in generating each GAN image. Five different runs using the same method were submitted and the best results were obtained for submission #5 with an F1-score of 0.502.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KDE-lab.</head><p>The best performing run from the KDE-lab team achieved an F1-score of 0.548. The team proposed a fine-tuning deep neural network model that uses multi-stage transfer learning <ref type="bibr" coords="6,129.57,307.22,11.54,10.91" target="#b4">[5]</ref>. The first stage transfer learning uses casia dataset <ref type="bibr" coords="6,375.47,307.22,11.54,10.91" target="#b5">[6]</ref>, the second stage transfer learning uses COVID-19 dataset <ref type="bibr" coords="6,239.18,320.77,11.58,10.91" target="#b6">[7]</ref>, the third stage transfer learning uses the development dataset provided with the task, while the fourth stage transfer learning uses test dataset. Several methods were used for predicting the used/not used label of the real images. The best results of an F1-score of 0.548 was obtained using ViT B/32 using multi-stage transfer learning. The results obtained by the team are shown in Table <ref type="table" coords="6,307.74,374.96,5.13,10.91" target="#tab_1">2</ref> (there was no reference in team's working notes to the method used to obtain the results provided in the Submission file #2) and consists of the following methods:</p><p>â¢ Submission 1 -Conv model using multi-stage transfer learning.</p><p>â¢ Submission 3 -ResNet18 using multi-stage transfer learning.</p><p>â¢ Submission 4 -VGG11 using multi-stage transfer learning.</p><p>â¢ Submission 5-ViT B/32 using multi-stage transfer learning.</p><p>One five one zero. The best performing run from the one five one zero team achieved an F1-score of 0.507. The authors used a contrastive learning architecture, combined with transfer learning <ref type="bibr" coords="6,129.11,516.02,11.45,10.91" target="#b7">[8]</ref>. They used different pre-trained feature extraction modules such as Inception V3, ResNet and EfficientNet to find the target with large response value through the similarity calculation method in order to find the original image. Euclidian distance was used to determine the distance between the target, positive and negative examples, and use it was used an input to the loss function. All results obtained by the team are shown in Table <ref type="table" coords="6,414.76,570.22,5.10,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: Inception V3.</p><p>â¢ Submission #2: ResNet50.</p><p>â¢ Submission #3: EfficientNet.</p><p>PicusLabMed. The best performing run from the PicusLabMed team achieved an F1-score of 0.666. The team studied the ability of Deep-Learning models to provide a representation of the input data, relying on CNN, to extract the features from the real and generated images <ref type="bibr" coords="7,89.29,100.52,11.43,10.91" target="#b8">[9]</ref>. These features were analysed using a ML model for the identification of the samples used during the development of the generative model among all the real instances. The authors proposed two variants for the features extraction step, introducing Vector-Net, a convolutional network that learns how to map in the input image in an efficient representation, and leveraging a Deforming Autoencoder (DAE), that provides a latent vector in an unsupervised manner. All results obtained by the team are shown in Table <ref type="table" coords="7,305.49,168.26,5.07,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: Vector-Net applied to the images that were not used for training and to the images that were used for training <ref type="bibr" coords="7,289.67,203.63,11.57,10.91" target="#b0">(1)</ref> and Linear-SVM classifier. â¢ Submission #2: Vector-Net (1) and SVM-2 classifier.</p><p>â¢ Submission #3: Vector-Net (1,2) and Linear-SVM classifier.</p><p>â¢ Submission #4: Vector-Net (1,2) and SVM-2 classifier.</p><p>â¢ Submission #5: Vector-Net (1,2,3) and Linear-SVM classifier.</p><p>â¢ Submission #6: Vector-Net (1,2,3) and SVM-2 classifier.</p><p>â¢ Submission #7: DAE applied to the generated images and the images used for training and SVM-Linear Classifier. â¢ Submission #8: DAE applied to the generated images and the images that were not used for training and SVM-Linear Classifier. â¢ Submission #9: DAE applied to the generated images, the images that were not used for training and to the images that were used for training and SVM-Linear Classifier. â¢ Submission #10: voting strategy among the other results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VCMI.</head><p>The best performing run from the VCMI team achieved an F1-score of 0.802. The authors used similarity-based approaches such as: auto-encoders (AE) to classify the images through outlier detection techniques and patch-based methods that operate on patches extracted from real and generated images to measure their similarity <ref type="bibr" coords="7,329.73,439.25,16.25,10.91" target="#b9">[10]</ref>.</p><p>Structural Similarity Index Measure (SSIM) between real and generated images was studied and different methods were applied as described in the following: (i) Threshold approach was used to find and classify as "used" real images whose similarity to their most similar generated image is higher than a threshold. The threshold was calculated based on the similarity between real images; (ii) Retrieval approach was used to find a set of real images that are the most similar to at least one generated image, and those images were classified as "used". All retrieved images that are, therefore, the most similar to at least one of the generated images, were classified as "used". Real images that are not retrieved were classified as "not used"; (iii) Ranking approach was used to classify real images based on a ranking that defines how similar they are to the generated images. The method starts by calculating a threshold that represents the average rank of similarity of a real image when compared with other real images. Finally, if this average rank is higher than the threshold, then the image is classified as "used", as it shows high similarity with respect to the generated images. Otherwise, the image is classified as "not used"; (iv)(4) Clustering approach was used to find outliers in the data, to classify them as "not used". First, the method maps both generated and real images into a common space. Then, it uses the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to form clusters, and to find outliers. Outliers identified in the subset of real images were classified as "not used", while the remaining images were classified as "used"; (v) Ensemble method was used to merge the results of different methods presented by the team; (vi) AE based methods using two types of auto-encoders (Basic AE and ResNet AE) were used to classify images by two ways: Computing the similarity between the images based on their latent representations, enabling the direct application of the techniques defined in the previous section; Applying outlier detection techniques to identify data points from the real data that do not follow the probability distribution of the generated data; (vii) Patch-based method were used to extract patches from images and perform different operations such as : matching patches using Triplet loss and replacing patches from real images with patches extracted from the generated images.</p><p>The best results were obtained with a similarity-based approach that uses Structural Similarity SSIM to compute the similarity between real and generated images, achieving an F1-score of 0.802. All results obtained by the team are shown in Table <ref type="table" coords="8,363.92,236.01,5.17,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: Ranking/ Ensemble method using as a similarity SSIM metric.</p><p>â¢ Submission #2: Threshold (MAX) method using SSIM as a similarity metric.</p><p>â¢ Submission #3: Retrieval method using SSIM as a similarity metric. AIMultimediaLab. The best performing run from the AIMultimediaLab achieved an F1-score of 0.626. The team proposed two approaches for addressing the task <ref type="bibr" coords="8,404.95,469.37,16.41,10.91" target="#b10">[11]</ref>. Both approaches start by generating synthetic images from the real unused images provided in the development dataset. Subsequently, distinct descriptors/features are extracted and utilized to train a binary SVM classifier that was further used for identifying which of the 200 provided real images were used for generating the 10,000 artificial images from the test dataset. The analyzed features were extracted using two methods: a hand-crafted feature extraction technique called Local Binary Pattern (LBP) to capture the local spatial patterns and the gray scale contrast of the images and a deep-learning approach utilizing a pre-trained VGG-16 convolutional network. All results obtained by the team are shown in Table <ref type="table" coords="8,321.18,577.77,5.07,10.91" target="#tab_1">2</ref> and consists in the following methods:</p><p>â¢ Submission #1: Hand-crafted feature extraction method and an radial SVM classifier. â¢ Submission #2: Deep-learning feature extraction method and an radial SVM classifier. Fig. <ref type="figure" coords="8,119.03,637.70,5.07,10.91" target="#fig_1">2</ref> shows the corresponding confusion matrix for each team's best run. VCMI team achieved the best F1-score of 0.802 for the experiment in which they used Threshold (MAX) method using SSIM as a similarity metric to check whether the distance between each real image and its closest generated image is higher than a threshold. The threshold approach finds real images whose similarity to their most similar generated image is higher than a threshold, classifying them as "used". The threshold is calculated based on the similarity between real images. The MAX threshold was considered the one with the maximum similarity between two images from the real data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The first edition of the ImageCLEF medical GANs task attracted a total of 8 teams that submitted runs, with all of them completing their submissions by creating a working notes paper. One task was proposed to the participants, a prediction-based task that uses real and generated CT images. All the participant teams show interesting methods and results. The best result for the task is an F1-score of 0.802 obtained by VCMI team followed by PicusLabMed with an F1-score of 0.666 and AiMultimediaLab with an F1-score of 0.626. Regarding the identification methods proposed by the participants, we are happy to report a high degree of diversity among them. Proposed methods include multi-stage transfer learning, analysis of similarity of different features, patch extraction methods, threshold methods, Perceptual Hashing algorithm and different deeplearning feature extraction methods that were further classified using both traditional (SVM, kNN) and deep learning models for prediction. We find this to be truly motivating, and we are looking forward to development of this tas in the future editions of ImageCLEF.</p><p>Future editions of this task will expand the study areas of synthetic medical data, varying different aspects such as datasets and generation methods. Also, we plan to add other tasks based on different aspects of the privacy and security of the generated data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,291.60,416.69,8.93;3,89.29,303.61,151.18,8.87;3,95.27,184.65,416.69,83.99"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of images from the provided test dataset: (a) real images, (b) synthetic images generated using a Generative Model.</figDesc><graphic coords="3,95.27,184.65,416.69,83.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,89.29,488.87,418.35,8.93;9,89.29,500.87,332.21,8.87;9,155.91,163.32,283.47,318.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confusion matrices for each team's best run. Vertical axes --true label, horizontal axes -predicted label, "1" -images used for training, "0"-images not used for training.</figDesc><graphic coords="9,155.91,163.32,283.47,318.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,417.65,179.08"><head>Table 1</head><label>1</label><figDesc>List of participating teams that submitted at least one run (* task organizing team).</figDesc><table coords="4,113.15,118.53,393.49,151.04"><row><cell>Group name</cell><cell></cell><cell>Main institution</cell><cell>Country</cell></row><row><cell cols="4">Clef-CSE-GAN-Team Sri Sivasubramaniya Nadar College of Engineering India</cell></row><row><cell>DMKS-SSN</cell><cell cols="2">Sri Sivasubramiya Nadar College of Engineering</cell><cell>India</cell></row><row><cell>GAN-ISI</cell><cell cols="2">University of Copenhagen</cell><cell>Denmark</cell></row><row><cell>KDE-lab</cell><cell cols="2">Toyohashi University of Technology</cell><cell>Japan</cell></row><row><cell>one five one zero</cell><cell cols="2">Yunnan University in Kunming</cell><cell>China</cell></row><row><cell>PicusLabMed</cell><cell cols="2">University of Naples Federico II</cell><cell>Italy</cell></row><row><cell>VCMI</cell><cell>INESC TEC</cell><cell></cell><cell>Portugal</cell></row><row><cell>AIMultimediaLab*</cell><cell cols="2">University Politehnica of Bucharest</cell><cell>Romania</cell></row><row><cell></cell><cell>ð¹ 1 -ð ðððð =</cell><cell>ð ððððð ððð â¢ ðððððð ð ððððð ððð + ðððððð</cell><cell>(5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,345.33,531.10"><head>Table 2</head><label>2</label><figDesc>Summary on the participant submissions and their results.</figDesc><table coords="5,160.96,118.53,273.36,503.06"><row><cell>Group rank</cell><cell>Group name</cell><cell cols="2">Submission # F1-score</cell></row><row><cell>#1</cell><cell>VCMI</cell><cell>submission 2</cell><cell>0.802</cell></row><row><cell>#2</cell><cell>VCMI</cell><cell>submission 1</cell><cell>0.731</cell></row><row><cell>#3</cell><cell>VCMI</cell><cell>submission 3</cell><cell>0.707</cell></row><row><cell>#4</cell><cell>PicusLabMed</cell><cell>submission 8</cell><cell>0.666</cell></row><row><cell>#5</cell><cell>VCMI</cell><cell>submission 4</cell><cell>0.654</cell></row><row><cell>#6</cell><cell>AIMultimediaLab</cell><cell>submission 1</cell><cell>0.626</cell></row><row><cell>#7</cell><cell>PicusLabMed</cell><cell>submission 6</cell><cell>0.624</cell></row><row><cell>#8</cell><cell>VCMI</cell><cell>submission 5</cell><cell>0.621</cell></row><row><cell>#9</cell><cell>Clef-CSE-GAN-Team</cell><cell>submission 1</cell><cell>0.614</cell></row><row><cell>#10</cell><cell>VCMI</cell><cell>submission 7</cell><cell>0.613</cell></row><row><cell>#11</cell><cell>VCMI</cell><cell>submission 6</cell><cell>0.605</cell></row><row><cell>#12</cell><cell>VCMI</cell><cell>submission 10</cell><cell>0.594</cell></row><row><cell>#13</cell><cell>AIMultimediaLab</cell><cell>submission 2</cell><cell>0.585</cell></row><row><cell>#14</cell><cell>one five one zero</cell><cell>submission 2</cell><cell>0.563</cell></row><row><cell>#15</cell><cell>PicusLabMed</cell><cell>submission 9</cell><cell>0.562</cell></row><row><cell>#16</cell><cell>PicusLabMed</cell><cell>submission 4</cell><cell>0.552</cell></row><row><cell>#17</cell><cell>KDE lab</cell><cell>submission 5</cell><cell>0.548</cell></row><row><cell>#18</cell><cell>one five one zero</cell><cell>submission 3</cell><cell>0.522</cell></row><row><cell>#19</cell><cell>Clef-CSE-GAN-Team</cell><cell>submission 2</cell><cell>0.521</cell></row><row><cell>#20</cell><cell>VCMI</cell><cell>submission 9</cell><cell>0.514</cell></row><row><cell>#21</cell><cell>one five one zero</cell><cell>submission 1</cell><cell>0.507</cell></row><row><cell>#22</cell><cell>GAN-ISI</cell><cell>submission 5</cell><cell>0.502</cell></row><row><cell>#23</cell><cell>GAN-ISI</cell><cell>submission 2</cell><cell>0.489</cell></row><row><cell>#24</cell><cell>PicusLabMed</cell><cell>submission 10</cell><cell>0.487</cell></row><row><cell>#25</cell><cell>GAN-ISI</cell><cell>submission 3</cell><cell>0.486</cell></row><row><cell>#26</cell><cell>GAN-ISI</cell><cell>submission 4</cell><cell>0.483</cell></row><row><cell>#27</cell><cell>DMK</cell><cell>submission 1</cell><cell>0.480</cell></row><row><cell>#28</cell><cell>PicusLabMed</cell><cell>submission 2</cell><cell>0.470</cell></row><row><cell>#29</cell><cell>KDE lab</cell><cell>submission 2</cell><cell>0.469</cell></row><row><cell>#30</cell><cell>GAN-ISI</cell><cell>submission 1</cell><cell>0.469</cell></row><row><cell>#31</cell><cell>KDE lab</cell><cell>submission 1</cell><cell>0.465</cell></row><row><cell>#32</cell><cell>KDE lab</cell><cell>submission 4</cell><cell>0.457</cell></row><row><cell>#33</cell><cell>DMK</cell><cell>submission 2</cell><cell>0.449</cell></row><row><cell>#34</cell><cell>VCMI</cell><cell>submission 8</cell><cell>0.448</cell></row><row><cell>#35</cell><cell>PicusLabMed</cell><cell>submission 1</cell><cell>0.434</cell></row><row><cell>#36</cell><cell>Clef-CSE-GAN-Team</cell><cell>submission 3</cell><cell>0.431</cell></row><row><cell>#37</cell><cell>PicusLabMed</cell><cell>submission 3</cell><cell>0.419</cell></row><row><cell>#38</cell><cell>PicusLabMed</cell><cell>submission 5</cell><cell>0.417</cell></row><row><cell>#39</cell><cell>KDE lab</cell><cell>submission 3</cell><cell>0.407</cell></row><row><cell>#40</cell><cell>PicusLabMed</cell><cell>submission 7</cell><cell>0.093</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The contribution of <rs type="person">Alexandra Andrei</rs>, <rs type="person">Bogdan Ionescu</rs> and <rs type="person">Henning MÃ¼ller</rs> to this task is supported under project <rs type="projectName">AI4Media</rs>, <rs type="programName">A European Excellence Centre for Media, Society and Democracy</rs>, <rs type="grantNumber">H2020 ICT-48-2020</rs>, grant #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RgrajUK">
					<idno type="grant-number">H2020 ICT-48-2020</idno>
					<orgName type="project" subtype="full">AI4Media</orgName>
					<orgName type="program" subtype="full">A European Excellence Centre for Media, Society and Democracy</orgName>
				</org>
				<org type="funding" xml:id="_7eGVdPS">
					<idno type="grant-number">951911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,312.61,394.53,10.91;10,112.66,326.16,395.17,10.91;10,112.66,339.71,394.53,10.91;10,112.66,353.26,395.17,10.91;10,112.39,366.81,394.80,10.91;10,112.48,380.36,394.70,10.91;10,112.66,393.91,395.17,10.91;10,112.66,407.46,393.32,10.91;10,112.66,421.01,394.52,10.91;10,112.33,434.55,120.27,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,224.93,380.36,282.25,10.91;10,112.66,393.91,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrÄgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÄ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>StorÃ¥s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>SchÃ¶ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.94,393.91,143.89,10.91;10,112.66,407.46,393.32,10.91;10,112.66,421.01,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,280.09,421.01,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,448.10,393.33,10.91;10,112.66,461.65,393.33,10.91;10,112.66,475.20,394.62,10.91;10,112.66,488.75,377.36,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,426.96,448.10,79.03,10.91;10,112.66,461.65,393.33,10.91;10,112.66,475.20,350.28,10.91">CLEF-Correlating Biomedical Image Fingerprints between Real and GAN-generated Images using a ResNet Backbone with ML-based Downstream Comparators: ImageCLEFmed GANs</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bharathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,488.75,249.65,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,502.30,393.61,10.91;10,112.66,515.85,393.33,10.91;10,112.66,529.40,394.53,10.91;10,112.66,542.95,58.60,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,250.67,502.30,255.60,10.91;10,112.66,515.85,393.33,10.91;10,112.66,529.40,46.94,10.91">Dmk-ssn at imageclef 2023 medical: Controlling the quality of synthetic medical images created via gans using machine learning and image hashing techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Kavitha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,184.05,529.40,255.27,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,556.50,393.33,10.91;10,112.66,570.05,393.32,10.91;10,112.14,583.60,228.17,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,317.82,556.50,188.16,10.91;10,112.66,570.05,221.81,10.91">Gan-isi: Generative adversarial networks image source identification using texture analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mehdipour-Ghazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mehdipour-Ghazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.55,570.05,148.44,10.91;10,112.14,583.60,100.46,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,597.15,393.33,10.91;10,112.66,610.69,393.33,10.91;10,112.14,624.24,228.17,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,388.87,597.15,117.11,10.91;10,112.66,610.69,218.40,10.91">Real and generated image classification using multi-stage transfer learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shinoda1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Togawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,355.88,610.69,150.11,10.91;10,112.14,624.24,100.46,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,637.79,394.04,10.91;10,112.66,651.34,128.80,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sovathna</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/sophatvathana/casia-dataset" />
		<title level="m" coord="10,180.38,637.79,99.35,10.91">casia dataset, kaggle</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.61,10.91;11,112.66,100.52,393.33,10.91;11,112.66,114.06,181.91,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,293.02,86.97,213.25,10.91;11,112.66,100.52,299.22,10.91">A fully automated deep learning-based network for detecting covid-19 from a new and large lung ct scan dataset</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahimzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Attar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Sakhaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,423.83,100.52,82.15,10.91;11,112.66,114.06,103.95,10.91">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">102588</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.32,10.91;11,112.66,141.16,395.17,10.91;11,112.66,154.71,89.52,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,188.80,127.61,317.18,10.91;11,112.66,141.16,76.20,10.91">Finding the source images from the generated images with contrastive learning methods</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,212.70,141.16,252.74,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.33,10.91;11,112.66,181.81,394.53,10.91;11,112.66,195.36,257.64,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,269.17,168.26,236.81,10.91;11,112.66,181.81,258.06,10.91">Analyzing the similarity between artificial and training images in generative models: The picuslabmed contribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marrone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,392.91,181.81,114.28,10.91;11,112.66,195.36,129.93,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,393.33,10.91;11,112.66,236.01,394.52,10.91;11,112.33,249.56,120.27,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,459.03,208.91,46.95,10.91;11,112.66,222.46,393.33,10.91;11,112.66,236.01,120.18,10.91">Evaluating privacy on synthetic images generated using gans: Contributions of the vcmi team to imageclefmedical gans 2023</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>PatrÃ­cio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rio-Torto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>GonÃ§alves1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">F</forename><surname>Teixeira1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,255.74,236.01,246.79,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,263.11,393.33,10.91;11,111.44,276.66,395.74,10.91;11,112.66,290.20,257.64,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,225.61,263.11,280.38,10.91;11,111.44,276.66,259.22,10.91">Aimultimedialab at ImageCLEFmedical GANs 2023:determining &quot;fingerprints&quot; of training data in generated synthetic images</title>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,393.04,276.66,114.15,10.91;11,112.66,290.20,129.93,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
