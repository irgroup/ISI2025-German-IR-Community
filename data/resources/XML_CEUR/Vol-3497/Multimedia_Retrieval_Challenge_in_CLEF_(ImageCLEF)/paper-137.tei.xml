<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,396.50,15.42;1,89.29,106.66,407.30,15.42;1,89.29,128.58,259.02,15.43;1,89.29,150.91,310.14,11.96">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmedical Caption 2023 Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
				<funder ref="#_BCyxqFv">
					<orgName type="full">ERDF -European Regional Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">CMU -Portugal International Partnership</orgName>
				</funder>
				<funder ref="#_Kt89YhX">
					<orgName type="full">Portuguese Foundation for Science and Technology -FCT</orgName>
				</funder>
				<funder ref="#_yp5EarX">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_SJDvMxB #_CfF2Mhf #_kcYPf5N">
					<orgName type="full">Portuguese Foundation for Science and Technology (FCT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,77.58,11.96"><forename type="first">Isabel</forename><surname>Rio-Torto</surname></persName>
							<email>isabel.riotorto@inesctec.pt</email>
							<affiliation key="aff0">
								<orgName type="department">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Departamento de Ci√™ncia de Computadores</orgName>
								<orgName type="department" key="dep2">Faculdade de Ci√™ncias</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua do Campo Alegre s/n</addrLine>
									<postCode>4169-007</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.78,176.82,85.83,11.96"><forename type="first">Cristiano</forename><surname>Patr√≠cio</surname></persName>
							<email>cristiano.p.patricio@inesctec.pt</email>
							<affiliation key="aff0">
								<orgName type="department">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Departamento de Inform√°tica</orgName>
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<addrLine>Rua Marqu√™s de √Åvila e Bolama</addrLine>
									<postCode>6201-001</postCode>
									<settlement>Covilh√£</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.55,176.82,97.34,11.96"><forename type="first">Helena</forename><surname>Montenegro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.81,176.82,81.12,11.96"><forename type="first">Tiago</forename><surname>Gon√ßalves</surname></persName>
							<email>tiago.f.goncalves@inesctec.pt</email>
							<affiliation key="aff0">
								<orgName type="department">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,190.76,82.05,11.96"><forename type="first">Jaime</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
							<email>jaime.cardoso@fe.up.pt</email>
							<affiliation key="aff0">
								<orgName type="department">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,396.50,15.42;1,89.29,106.66,407.30,15.42;1,89.29,128.58,259.02,15.43;1,89.29,150.91,310.14,11.96">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmedical Caption 2023 Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B23E1BB909D8572651CC02A58ED0D5ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Retrieval</term>
					<term>Image Captioning</term>
					<term>Medical Concept Detection</term>
					<term>Multi-label Classification</term>
					<term>Natural Language Generation</term>
					<term>Vision Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the main contributions of the VCMI Team to the ImageCLEFmedical Caption 2023 task. We addressed both the concept detection and caption prediction tasks. Regarding concept detection, our team employed different approaches to assign concepts to medical images: multi-label classification, adversarial training, autoregressive modelling, image retrieval, and concept retrieval. We also developed three model ensembles merging the results of some of the proposed methods. Our best submission obtained an F1-score of 0.4998, ranking 3rd among nine teams. Regarding the caption prediction task, our team explored two main approaches based on image retrieval and language generation. The language generation approaches, based on a vision model as the encoder and a language model as the decoder, yielded the best results, allowing us to rank 5th among thirteen teams, with a BERTScore of 0.6147.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF 2023 <ref type="bibr" coords="2,166.73,111.28,12.88,10.91" target="#b0">[1]</ref> is a multi-modal challenge organised as part of the CLEF Initiative Labs<ref type="foot" coords="2,501.96,109.52,3.71,7.97" target="#foot_0">1</ref> (Conference and Labs of the Evaluation Forum, formerly known as Cross-Language Evaluation Forum) set to promote the evaluation of technologies for annotation, indexing, classification and retrieval of multi-modal data. The 2023 edition included four challenges from diverse applications (i.e. medical, social media and Internet, and content recommendation).</p><p>Similarly to last year <ref type="bibr" coords="2,199.57,179.03,11.58,10.91" target="#b1">[2]</ref>, our team, composed of members of the Visual Computing and Machine Intelligence (VCMI) Research Group of the Institute for Systems and Computer Engineering, Technology and Science (INESC TEC) from Porto, Portugal, participated in the ImageCLEFmedical Caption 2023 task <ref type="bibr" coords="2,260.20,219.67,12.84,10.91" target="#b2">[3]</ref> where the goal is to challenge the scientific community to design and train automatic algorithms capable of interpreting and summarising the insights gained from medical images. Once again, this challenge consisted of two independent, but complementary, tasks: concept detection, which aims to identify the presence of relevant concepts in a large corpus of medical images; and caption prediction, which aims to generate coherent textual descriptions describing a medical image. We addressed both the concept detection and caption prediction tasks.</p><p>For the concept detection task, we developed five different approaches: (i) baseline multi-label classification, in which a convolutional neural network (CNN) simultaneously predicts all the concepts from an image; (ii) adversarial approach, in which a multi-label classifier and a concept discriminator are trained in an adversarial manner to promote the learning of admissible concept combinations by the multi-label classifier; (iii) autoregressive approach, that aims to model dependencies between concepts using autoregressive learning; (iv) image retrieval, in which a model assigns concepts to an image based on its most similar images from the training data; and (v) concept retrieval, in which a model learns to map concepts and images into a common latent space where images are closer to the concepts they contain. We also developed three model ensembles using the aforementioned approaches: (i) multi-label classification and concept retrieval, (ii) autoregressive model and image retrieval using autoregressive model, (iii) adversarial model and image retrieval using autoregressive model. Our best submission (i.e.ensemble with autoregressive model and image retrieval using autoregressive model) obtained an F1-score of 0.4998, ranking 3rd among nine teams.</p><p>For the caption prediction task, we relied on Vision Encoder-Decoder Transformer-based architectures, since they worked well on last year's competition <ref type="bibr" coords="2,372.91,517.76,11.34,10.91" target="#b1">[2]</ref>. We explored two different categories of image feature extractors for the Encoder, namely a Vision Transformer and a CNN. Furthermore, we introduced a caption-to-concepts classification branch as an additional supervisory signal for the model, since the caption needs to contain enough information to allow, to some extent, for the prediction of the concepts. Our best submission (i.e. the Vision Transformer encoder model trained on both training and validation sets) achieved a BERTScore of 0.6147, ranking 5th among thirteen participating teams.</p><p>The remainder of this paper is organised as follows: Section 2 provides an overview of the data provided by the organisation to address the tasks and describes our exploratory data analysis; Section 3 details the different proposals developed to solve the aforementioned tasks; Section 4 presents the results and their discussion; and Section 5 concludes this paper and recommends future work directions. The code related to this paper is publicly available in a GitHub repository<ref type="foot" coords="3,171.11,112.31,3.71,7.97" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The dataset provided in this competition is an extended version of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="3,220.35,186.09,11.58,10.91" target="#b3">[4]</ref>. The data originates from biomedical articles of the PMC OpenAccess subset <ref type="bibr" coords="3,174.78,199.63,11.28,10.91" target="#b2">[3]</ref>. The images provided to the participants are divided into training (60,918 images), validation (10,437 images) and test (10,473 images) sets.</p><p>The concepts provided in the training and validation data were annotated according to the Unified Medical Language System (UMLS) <ref type="bibr" coords="3,274.10,240.28,12.69,10.91" target="#b4">[5]</ref> 2022 AB release, wherein each concept is uniquely identified through a Concept Unique Identifier (CUI). For additional details, please refer to <ref type="bibr" coords="3,492.25,253.83,11.42,10.91" target="#b2">[3]</ref>.</p><p>Table <ref type="table" coords="3,128.71,267.38,5.17,10.91" target="#tab_0">1</ref> presents an analysis of the number of concepts contained in each training and validation image. On average, each image has 3.7 concepts, and while there are 4716 images with only 1 concept, there are also 233 images with more than 10 concepts, the maximum number of concepts per image being 24 in the training set. Table <ref type="table" coords="3,126.21,448.91,4.97,10.91">2</ref> presents an analysis of the concepts and their frequency in the training and validation data. While the training set contains a total of 2125 concepts, only 1945 of these appear in the validation data. Furthermore, each concept appears on average on 106.9 images in the training set and on 20.6 images in the validation set. While there are concepts that appear in 1 or 2 images only, there are other concepts that appear in 20955 images, making this task highly imbalanced. Moreover, the validation data contains a significant amount of concepts that appear in 10 or less images. Since Deep Learning models require large amounts of training data to achieve good performance, the existence of these less-frequent concepts raises the difficulty of the concept detection task.</p><p>Regarding the data for the caption prediction task, which was obtained from the aforementioned biomedical articles, we present our analysis in Table <ref type="table" coords="3,366.05,584.41,3.81,10.91">3</ref>. While the smallest captions contain only 2 tokens, the biggest ones can have up to 995 tokens. The average caption length is 33.6 and 36.6 for the training and validation sets, respectively. Looking at the 98% percentile, we conclude that only 2% of the dataset has captions with more than 100 tokens. This observation led us to choose a maximum length of 100 for all our developed methods described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Data analysis for the concept detection task, from the concept-based perspective. "Avg.", "Min." and "Max." stand for "Average", "Minimum" and "Maximum" number of images per concept, respectively. The last two columns refer to the number of concepts that appear in 10 or less images and that appear in over 100 images, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The following sections describe the methods developed to fulfill the concept detection and caption prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>The concept detection task was solved using two main approaches: modelling the concept detection task as a multi-label classification problem and as an information retrieval problem.</p><p>We developed three models based on multi-label classification: a baseline model, a model trained in an adversarial manner and a model trained using autoregressive learning. Furthermore, we developed models to perform concept retrieval and image retrieval. The following subsections describe, in detail, each of the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Baseline Multi-Label Approach</head><p>A conventional approach to address the concept detection task involves employing a multilabel classification model, considering the inherent nature of images to encompass multiple non-mutually exclusive concepts. Specifically, we adapted the DenseNet-121 <ref type="bibr" coords="4,293.94,584.28,12.98,10.91" target="#b5">[6]</ref> architecture by modifying the classification layer to have ùëÅ outputs, where ùëÅ is the number of concepts, i.e. 2125.</p><p>In the training phase, the model was trained using the binary cross-entropy loss function and the adaptive moment estimation (Adam) optimiser <ref type="bibr" coords="4,340.52,624.93,13.00,10.91" target="#b6">[7]</ref> with its default hyperparameters. The model was trained during 100 epochs with a learning rate of 1e-4. Concretely, we trained the classification layer of the model while keeping the remaining layers frozen. Subsequently, the model with the best validation loss was selected for the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Adversarial Approach</head><p>Ensuring that the multi-label baseline approach learns the correct combination of concepts is not trivial (e.g. concepts related to different body parts should not be combined). Hence, we propose adversarial training to learn a realistic combination of concepts per image, according to the distribution of the training data. This model is composed of two blocks (see Figure <ref type="figure" coords="5,487.59,148.18,3.63,10.91" target="#fig_1">1</ref>):</p><p>‚Ä¢ A multi-label classifier trained to predict the top-K most frequent concepts (ùêæ = 100) in the database. This block uses ResNet50 <ref type="bibr" coords="5,289.65,181.58,12.68,10.91" target="#b7">[8]</ref> as a feature extractor along with a multi-layer perceptron (MLP) with a sigmoid activation. ‚Ä¢ A concept discriminator trained to distinguish between real (i.e. admissible) and fake (i.e. inadmissible) combinations of concepts. This block is an MLP with two fully-connected layers followed by a ReLU activation and a fully-connected layer with a sigmoid activation.</p><p>We trained this model for 20 epochs using binary cross-entropy as the loss function for both the multi-label classifier and the concept discriminator, and Adam <ref type="bibr" coords="5,378.98,270.23,12.69,10.91" target="#b6">[7]</ref> as the optimiser. The best model is saved according to the lowest validation loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Autoregressive Approach</head><p>The main limitation of the baseline multi-label classification approach is that it assumes that the concepts are independent of each other. However, there may be dependencies between concepts, as there are concepts that never appear together in the training data, or concepts that only exist in the presence of other concepts. To overcome this limitation, we devised an approach to model dependencies between concepts based on autoregressive learning. The proposed model is a multi-label classification network that, instead of having a final classification layer with 2125 units to predict all the concepts, contains several classification layers, each predicting a subset of concepts. To model dependencies, each layer is conditioned on the output of the previous layers. An overview of the autoregressive model is depicted in Figure <ref type="figure" coords="5,121.00,615.39,3.81,10.91" target="#fig_2">2</ref>. As the feature extractor of the network, we used a VGG16 <ref type="bibr" coords="5,398.17,615.39,12.99,10.91" target="#b8">[9]</ref> network pre-trained on ImageNet <ref type="bibr" coords="5,151.29,628.93,16.41,10.91" target="#b9">[10]</ref>, followed by two fully-connected layers with LeakyReLU activations and Dropout. All of the classification layers are fully-connected layers with sigmoid activation.</p><p>Since it is easier for a network to predict concepts that exist in more images, we organised the concepts in the layers according to how frequent they are among the training images. The most common concepts are predicted by the first while the rarest ones are predicted by the last classification layers. Since there is a total of 2125 concepts, we used 17 classification layers, each responsible for predicting 125 concepts.</p><p>In the training phase, the model was trained using binary cross-entropy as the loss function and the Adam optimiser with a learning rate of 1e-5. We trained the model in two phases. First, we trained the classification layers of the model for 50 epochs, with the feature extractor frozen. Then, we fine-tuned the entire network by training it for 20 epochs. We selected the best instance of the model by monitoring its loss on the validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Retrieval Approaches</head><p>We implemented two main approaches to predict the concepts of an image based on information retrieval techniques: concept retrieval and image retrieval. In concept retrieval, the method maps images and concepts into a common latent space, retrieving the closest concepts to an image. In image retrieval, the method assigns concepts to an image based on its most similar images from the training data. Both these methods will be described in detail below.</p><p>In the concept retrieval approach, we use an image encoder and a concept encoder to map images and concepts into a common latent space. Then, we compute the Euclidean distance between the latent representations of the images and concepts, as depicted in Figure <ref type="figure" coords="6,463.87,471.32,3.70,10.91" target="#fig_3">3</ref>. During training, we minimise the Euclidean distance between an image and the concepts it contains, and we maximise the distance between the image and the concepts it does not contain.</p><p>In our implementation, the image encoder is a CNN with four blocks of convolutional layers with Batch Normalisation and Max Pooling, followed by a layer that performs Global Average Pooling and a fully-connected layer. The concept encoder is a Multi-Layer Perceptron (MLP) with one fully-connected layer with Dropout and LeakyReLU as the activation function, followed by a second fully-connected layer.</p><p>In addition to the Image-to-Concept (ITC) loss, we also performed some experiments where we added the following loss functions to the training of the networks:</p><p>‚Ä¢ Concept-to-Concept (CTC) loss: Minimises the distance between two different concepts that exist in the same images, and maximises the distance between concepts that do not appear together in any image. We apply a weight to the loss function by multiplying it by the percentage of images that two concepts share (intersection over union). ‚Ä¢ Image-to-Image (ITI) loss: Minimises the distance between images that have some concepts in common, and maximises the distance between images that do not share any concepts. We apply a weight to the loss function by multiplying it by the percentage of concepts that two images have in common (intersection over union).</p><p>We performed three experiments: (i) training the concept retrieval networks only with the ITC loss for 2600 epochs, (ii) fine-tuning the network trained with the ITC loss using the CTC loss for 100 epochs, and (iii) fine-tuning the network trained with the ITC loss simultaneously using the CTC and ITI losses for 100 epochs. The networks were trained using the Adam optimiser with a learning rate of 1e-5. We monitored the loss on the validation data to obtain the best model.</p><p>In the image retrieval approach, we use pre-trained models to obtain latent representations of the images, which are then used to measure the distance between the target image whose concepts we want to predict and the images of the training data. We devised three strategies to assign concepts to the target image, based on its most similar images:</p><p>‚Ä¢ Strategy 1 (S1): Retrieve the closest image and assign its concepts to the target image. We empirically chose to retrieve the Top-4 closest images in strategies 2 and 3.</p><p>As the pre-trained models to obtain a latent representation of the images we used a ResNet50 <ref type="bibr" coords="7,89.29,656.03,13.00,10.91" target="#b7">[8]</ref> trained on ImageNet <ref type="bibr" coords="7,204.48,656.03,16.41,10.91" target="#b9">[10]</ref>, and the image encoders of the previously described concept retrieval and autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Ensemble</head><p>The multi-label classification-based approaches (baseline, adversarial and autoregressive) often fail to predict any concepts for a given test image, leading to many images in the test dataset with no predicted concepts. As such, we devise an ensemble strategy where, for each image where the multi-label approaches fail to predict any concepts, we assign the concepts predicted by one of the retrieval approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>The caption prediction task involves generating text that describes an image. To tackle this task we considered two categories of approaches, retrieval and language generation, which we describe in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Retrieval Approach</head><p>We applied the image retrieval approach developed for the concept detection task to obtain captions for the test images. We used the pre-trained ResNet <ref type="bibr" coords="8,358.24,301.44,11.32,10.91" target="#b7">[8]</ref>, the concept retrieval network trained using the ITC loss and the autoregressive network to obtain latent representations of the images. These representations were then used to obtain the closest images from the training and validation data whose captions were assigned to the test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Language Generation Approaches</head><p>The language generation-based strategies used to tackle this task employ an Encoder-Decoder framework, since it was our best performing approach in last year's competition <ref type="bibr" coords="8,467.14,404.96,11.58,10.91" target="#b1">[2]</ref>. The Encoder, typically a CNN or a Vision Transformer, is responsible for analysing the image and extracting relevant features. The Decoder then receives the encoded image features and generates the caption. Thus, it is usually an autoregressive model, such as GPT-2 <ref type="bibr" coords="8,451.46,445.61,16.25,10.91" target="#b10">[11]</ref>.</p><p>We experimented with two different encoders: the small distilled version of the Data-efficient image Transformer (DeiT) <ref type="bibr" coords="8,208.60,472.71,17.90,10.91" target="#b11">[12]</ref> from the Huggingface Transformers library <ref type="bibr" coords="8,425.36,472.71,16.23,10.91" target="#b12">[13]</ref>, and, inspired by the work of Hou et al. <ref type="bibr" coords="8,207.57,486.26,16.41,10.91" target="#b13">[14]</ref>, DenseNet121 <ref type="bibr" coords="8,294.15,486.26,12.99,10.91" target="#b5">[6]</ref> from TorchXRayVision <ref type="bibr" coords="8,418.22,486.26,16.55,10.91" target="#b14">[15,</ref><ref type="bibr" coords="8,437.81,486.26,14.11,10.91" target="#b15">16]</ref> pre-trained on all available datasets (densenet121-res224-all). The decoder consisted of the distilled version of GPT-2 <ref type="bibr" coords="8,166.63,513.35,16.21,10.91" target="#b10">[11]</ref>. Both models were trained with an initial learning rate of 1e-4 using the AdamW optimiser <ref type="bibr" coords="8,174.09,526.90,17.96,10.91" target="#b16">[17]</ref> for 25 epochs. We monitored the BERTScore on the validation data to obtain the best model.</p><p>Since the UMLS concepts of the concept detection task are tightly related to the captions in the caption prediction task, we hypothesise that it should be possible to predict the concepts from the captions to some extent. Furthermore, predicting the concepts from the captions might prove a good additional supervisory signal for training the caption prediction model. Therefore, we explored the inclusion of a text classifier that takes the caption of a given image and predicts its concepts (see Figure <ref type="figure" coords="8,195.39,621.75,3.57,10.91" target="#fig_6">4</ref>).</p><p>To accomplish this we originally trained a DistilBERT <ref type="bibr" coords="8,351.08,635.30,18.06,10.91" target="#b17">[18]</ref> model for caption-to-concept multi-label classification. The model was trained with the binary cross-entropy loss on the CLS token for 20 epochs with an initial learning rate of 2e-5 and the AdamW optimiser. This captionto-concept classifier was then used (but kept frozen) on top of the DenseNet-DistilGPT2 model to provide an extra loss function for training. However, since the output of the Encoder-Decoder module and the input of the caption-to-concept classifier (i.e. the generated text) is discrete, Reinforcement Learning (RL) is needed, similarly to what is done in Self-Critical Sequence Training <ref type="bibr" coords="9,130.74,154.71,16.50,10.91" target="#b18">[19]</ref>; thus, the whole sentence needs to be generated before classification can occur, making this approach much slower compared to teacher forcing-only training.</p><p>We also experimented with simply adding a fully connected layer directly on top of the latent representation of the Decoder's last token and training the whole Encoder-Decoder plus classification layer together. This approach has the advantage of not needing RL, thus making it faster and easier to train.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>This section details the results obtained by the methods developed for the concept detection and caption prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>The concept detection task is evaluated using the example-based F1-score between the predicted and ground-truth concepts. Table <ref type="table" coords="9,241.13,568.35,5.08,10.91" target="#tab_2">4</ref> presents the results in terms of F1-Score obtained by each proposed method on the validation and test data. Furthermore, it presents a secondary F1-score metric (F1-Score Manual) that compares the concepts predicted on the test data with a subset of manually validated concepts.</p><p>The baseline multi-label classification approach obtained an F1-score of 0.4469 on the test set. Contrary to our expectations, the adversarial approach did not improve upon the baseline. This might be explained by the fact that this adversarial model was only trained on the top-100 concepts. Thus, we leave as future work a more in-depth exploration of this approach. The autoregressive approach achieved the highest performance among the multi-label-based models.</p><p>In the concept retrieval approach, we verify that adding the CTC and ITI loss functions to the network trained only with the ITC loss leads to a lower F1-score.</p><p>Regarding the image retrieval method, we empirically found that Strategy 3 (S3) produced the best results. This ablation study can be found in Table <ref type="table" coords="10,346.97,456.05,3.70,10.91" target="#tab_3">5</ref>, that compares the different image retrieval strategies on the validation data, using the concept retrieval model trained with ITC loss as the base. We verify that assigning concepts that exist in at least two of the Top-4 most similar images (Strategy 3) leads to the highest F1-Score. Among the three different base models used (ResNet, autoregressive and concept retrieval), the best results were obtained by using the autoregressive model. Nevertheless, these results do not surpass the values obtained by the multi-label classification-based autoregressive model.</p><p>However, the retrieval-based approaches proved very useful as complements to the classification-based methods. As expected, the ensemble methods, which combine both techniques, improved the results of all three multi-label classification networks (baseline, adversarial and autoregressive). We obtained the best results by merging our two best models from each category, the autoregressive multi-label classification network and the image retrieval approach using the autoregressive model, achieving an F1-Score of 0.4998 and a Manual F1-Score of 0.9162. This allowed us to rank 3rd in the competition among nine teams. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>The caption prediction task is evaluated in terms of BERTScore <ref type="bibr" coords="11,360.61,314.23,17.76,10.91" target="#b19">[20]</ref> and ROUGE <ref type="bibr" coords="11,434.54,314.23,16.09,10.91" target="#b20">[21]</ref>. We present the obtained results in Table <ref type="table" coords="11,218.21,327.78,5.07,10.91" target="#tab_4">6</ref> for both retrieval and language generation-based approaches.</p><p>All retrieval approaches ranked below the language generation-based approaches, which confirms that simply using the captions from similar images is not enough to accurately describe a different image.</p><p>Regarding the generation-based approaches, using the DeiT encoder yielded slightly improved results when compared to using DenseNet-121. As expected, adding the classification loss improved the corresponding base architecture, but it was not enough to surpass the DeiT + DistilGPT2 model. This suggests that, had time permitted, adding the classification loss to the DeiT instead of the DenseNet-based model would have further improved our results. We would like to point out that we do not report the results obtained by our model with the RL concept-to-caption classifier because we were not able to train it in a reasonable amount of time given the computational resources available.</p><p>Thus, our best results were obtained by the DeiT + DistilGPT2 model trained on both training and validation sets. This also suggests that our other developed methods could have better results if trained on both sets, something we leave as future work. In the end, these results awarded us the 5th place in the competition among thirteen participating teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This work described the methods developed by the VCMI team in the ImageCLEFmedical Caption 2023 task. We developed approaches based on multi-label classification and retrieval to assign concepts to medical images, obtaining an F1-Score of 0.4998 that granted us 3rd place among the nine teams that participated in the challenge. For caption generation, we focused on encoder-decoder approaches with Transformers, obtaining a 5th place among thirteen teams, with a BERTScore of 0.6147. In the concept detection task, the experiments show that training an autoregressive multilabel classification network to model dependencies between concepts is a promising approach capable of achieving high performance. As such, future work includes the further development of autoregressive models, potentially with the integration of more advanced autoregressive networks from the literature, such as Transformers <ref type="bibr" coords="12,314.90,371.77,16.09,10.91" target="#b21">[22]</ref>. We also intend to continue developing the concept retrieval approach by pre-training the concept encoder using the concept-to-concept loss before training the whole model. Finally, we consider the application of the adversarial approach to predict all concepts, rather than only the Top-100 most frequent concepts, and the potential integration between the adversarial and the autoregressive approaches into one model.</p><p>In the caption prediction task, future work involves exploring different and more powerful image encoders, as well as more recent language models. We also intend to explore more in-depth the inclusion of the concept classification loss into our base encoder-decoder approach, not only by applying it to all our model configurations, but also by investigating the best way of integrating it during training, e.g. only after the captioning module is sufficiently trained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,198.80,355.78,39.83,7.54;5,201.97,365.56,33.49,7.54;5,281.15,361.07,71.07,7.54;5,275.06,313.22,85.24,7.54;5,401.48,355.78,30.33,7.54;5,392.89,365.56,47.52,7.54;5,462.63,360.71,41.63,7.54;5,98.11,401.56,59.67,5.39;5,98.11,408.55,32.64,5.39"><head></head><label></label><figDesc>CC BY-NC [Al Mulhim et al. (2022)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,429.32,232.33,8.93;5,95.75,331.56,64.94,64.66"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the multi-label adversarial model.</figDesc><graphic coords="5,95.75,331.56,64.94,64.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,194.70,246.45,8.93;6,95.73,96.13,64.67,64.38"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the multi-label autoregressive model.</figDesc><graphic coords="6,95.73,96.13,64.67,64.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,263.43,208.71,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the concept retrieval model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,107.28,514.03,398.71,10.91;7,116.56,527.58,389.42,10.91;7,116.56,541.13,389.42,10.91;7,116.24,554.68,363.47,10.91;7,107.28,568.63,398.70,10.91;7,116.56,582.18,390.62,10.91;7,116.56,595.72,389.42,10.91;7,116.56,609.27,206.57,10.91"><head>‚Ä¢ Strategy 2 (</head><label>2</label><figDesc>S2): Retrieve the Top-N closest images and assign to the target image the concepts of the closest image that also exist in at least one other image from the Top-N retrieved images. If no concept of the closest image appears in another image of the Top-N, then all the concepts of the closest image are assigned to the target image. ‚Ä¢ Strategy 3 (S3): Retrieve the Top-N closest images and assign the concepts that exist in at least two of the Top-N retrieved images to the target image. Similarly to Strategy 2, if no concept appears in at least two images of the Top-N, then all the concepts of the closest image are assigned to the target image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,214.20,314.15,35.43,8.81;9,324.50,379.01,50.59,7.55;9,331.84,314.15,35.96,8.81;9,313.14,254.40,103.77,7.55;9,429.32,314.67,59.20,8.81;9,441.69,369.12,34.44,7.55;9,441.69,378.91,34.45,7.55;9,101.25,361.38,69.70,6.29;9,101.25,369.54,38.13,6.29"><head></head><label></label><figDesc>NC [Al Mulhim et al. (2022)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,89.29,414.35,183.92,8.93;9,98.50,280.19,75.86,75.52"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of the captioning model.</figDesc><graphic coords="9,98.50,280.19,75.86,75.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,336.71,418.14,93.78"><head>Table 1</head><label>1</label><figDesc>Data analysis for the concept detection task, from the image-based perspective. "Avg. ", "Min. " and "Max. " stand for "Average", "Minimum" and "Maximum" number of concepts per image, respectively. The last two columns refer to the number of images with only 1 concept and with over 10 concepts.</figDesc><table coords="3,109.55,392.23,376.17,38.25"><row><cell>Subset</cell><cell cols="6">Total Images Avg. Min. Max. With 1 concept With over 10 concepts</cell></row><row><cell>Training</cell><cell>60918</cell><cell>3.7</cell><cell>1</cell><cell>24</cell><cell>4716</cell><cell>233</cell></row><row><cell>Validation</cell><cell>10437</cell><cell>3.8</cell><cell>1</cell><cell>33</cell><cell>771</cell><cell>65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,417.00,271.11"><head>Table 4</head><label>4</label><figDesc>Results of the concept detection task in terms of F1-score and Secondary F1-score computed on a subset of manually validated concepts. The models identified with * were trained on the training and validation data before being applied to the test data. The results on the validation dataset were obtained using models trained only on the training set.</figDesc><table coords="10,108.31,157.97,378.64,203.63"><row><cell cols="2">Run Model</cell><cell>F1-score (Validation)</cell><cell cols="2">F1-score F1-score Manual (Test) (Test)</cell></row><row><cell>1</cell><cell>Baseline Multi-label*</cell><cell>0.4364</cell><cell>0.4469</cell><cell>0.8305</cell></row><row><cell>4</cell><cell>Adversarial* (Top-100)</cell><cell>0.2816</cell><cell>0.2803</cell><cell>0.5999</cell></row><row><cell>5</cell><cell>Autoregressive*</cell><cell>0.4905</cell><cell>0.4928</cell><cell>0.9062</cell></row><row><cell>2</cell><cell>Concept Retrieval (ITC)</cell><cell>0.4523</cell><cell>0.4360</cell><cell>0.7582</cell></row><row><cell>-</cell><cell>Concept Retrieval (ITC + CTC)</cell><cell>0.4404</cell><cell>-</cell><cell>-</cell></row><row><cell>-</cell><cell>Concept Retrieval (ITC + CTC + ITI)</cell><cell>0.2446</cell><cell>-</cell><cell>-</cell></row><row><cell>3</cell><cell>Image Retrieval -S3 (ResNet)</cell><cell>0.4693</cell><cell>0.4676</cell><cell>0.8305</cell></row><row><cell>7</cell><cell>Image Retrieval -S3 (Autoregressive)</cell><cell>0.4793</cell><cell>0.4793</cell><cell>0.9014</cell></row><row><cell>10</cell><cell>Image Retrieval -S3 (Concept Retrieval)</cell><cell>0.4379</cell><cell>0.4387</cell><cell>0.8394</cell></row><row><cell>6</cell><cell>Ensemble (Runs 1 and 2)</cell><cell>-</cell><cell>0.4728</cell><cell>0.8738</cell></row><row><cell>8</cell><cell>Ensemble (Runs 5 and 7)</cell><cell>-</cell><cell>0.4998</cell><cell>0.9162</cell></row><row><cell>9</cell><cell>Ensemble (Runs 4 and 7)</cell><cell>-</cell><cell>0.3327</cell><cell>0.7049</cell></row><row><cell>-</cell><cell>Task Winners</cell><cell>-</cell><cell>0.5223</cell><cell>0.9258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.99,90.49,416.99,176.47"><head>Table 5</head><label>5</label><figDesc>Comparison (on the validation set) between the different strategies of image retrieval using the concept retrieval model with the Image-To-Concept loss.</figDesc><table coords="11,216.02,134.06,163.24,132.89"><row><cell cols="3">Strategy Retrieved Images F1-score</cell></row><row><cell>S1</cell><cell>1</cell><cell>0.3314</cell></row><row><cell>S2</cell><cell>3</cell><cell>0.4030</cell></row><row><cell>S2</cell><cell>4</cell><cell>0.4232</cell></row><row><cell>S2</cell><cell>5</cell><cell>0.4234</cell></row><row><cell>S2</cell><cell>10</cell><cell>0.4176</cell></row><row><cell>S3</cell><cell>3</cell><cell>0.4354</cell></row><row><cell>S3</cell><cell>4</cell><cell>0.4379</cell></row><row><cell>S3</cell><cell>5</cell><cell>0.4345</cell></row><row><cell>S3</cell><cell>10</cell><cell>0.3887</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.99,90.49,418.52,200.38"><head>Table 6</head><label>6</label><figDesc>Results of the caption prediction task on the validation and test sets in terms of BERTScore and ROUGE. The models identified with * were trained on the training and validation data before being applied to the test data. The results on the validation dataset were obtained using models trained only on the training set.</figDesc><table coords="12,95.89,157.97,403.50,132.90"><row><cell cols="2">Run Model</cell><cell>BERTScore</cell><cell>ROUGE</cell><cell cols="2">BERTScore ROUGE</cell></row><row><cell></cell><cell></cell><cell cols="2">(Validation) (Validation)</cell><cell>(Test)</cell><cell>(Test)</cell></row><row><cell>1</cell><cell>Image Retrieval (ResNet)</cell><cell>0.5738</cell><cell>0.1417</cell><cell>0.5734</cell><cell>0.1427</cell></row><row><cell>2</cell><cell>Image Retrieval (Concept Retrieval)</cell><cell>0.5653</cell><cell>0.1268</cell><cell>0.5647</cell><cell>0.1284</cell></row><row><cell>8</cell><cell>Image Retrieval (Autoregressive)</cell><cell>0.5756</cell><cell>0.1464</cell><cell>0.5750</cell><cell>0.1464</cell></row><row><cell>3</cell><cell>DeiT + DistilGPT2</cell><cell>0.6133</cell><cell>0.2167</cell><cell>0.6138</cell><cell>0.2181</cell></row><row><cell>5</cell><cell>DeiT + DistilGPT2*</cell><cell>0.6133</cell><cell>0.2167</cell><cell>0.6147</cell><cell>0.2175</cell></row><row><cell>4</cell><cell>DenseNet-121 + DistilGPT2</cell><cell>0.6108</cell><cell>0.1935</cell><cell>0.6096</cell><cell>0.1938</cell></row><row><cell>6</cell><cell>DenseNet-121 + DistilGPT2 + Clf loss</cell><cell>0.6113</cell><cell>0.1947</cell><cell>0.6103</cell><cell>0.1948</cell></row><row><cell>-</cell><cell>Task Winners</cell><cell>-</cell><cell>-</cell><cell>0.6425</cell><cell>0.2446</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.02,199.39,8.97"><p>http://www.clef-initiative.eu (accessed on: 02-06-2023)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,92.57,671.04,282.27,8.97"><p>https://github.com/TiagoFilipeSousaGoncalves/ImageCLEFmedical2023VCMI</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank our colleague <rs type="person">Pedro Neto</rs> for his valuable feedback and suggestions.</p><p>This work was partially funded by the Project <rs type="projectName">TAMI -Transparent Artificial Medical Intelligence</rs> (<rs type="grantNumber">NORTE-01-0247-FEDER-045905</rs>) financed by <rs type="funder">ERDF -European Regional Fund</rs> through the <rs type="programName">North Portugal Regional Operational Program</rs> -<rs type="grantNumber">NORTE 2020</rs> and by the <rs type="funder">Portuguese Foundation for Science and Technology -FCT</rs> under the <rs type="funder">CMU -Portugal International Partnership</rs>, and also by the <rs type="funder">Portuguese Foundation for Science and Technology (FCT)</rs> within PhD grants <rs type="grantNumber">2022.14516.BD</rs>, <rs type="grantNumber">2022.11566.BD</rs>, <rs type="grantNumber">2020.06434.BD</rs> and <rs type="grantNumber">2020.07034</rs>.BD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_BCyxqFv">
					<idno type="grant-number">NORTE-01-0247-FEDER-045905</idno>
					<orgName type="project" subtype="full">TAMI -Transparent Artificial Medical Intelligence</orgName>
					<orgName type="program" subtype="full">North Portugal Regional Operational Program</orgName>
				</org>
				<org type="funding" xml:id="_Kt89YhX">
					<idno type="grant-number">NORTE 2020</idno>
				</org>
				<org type="funding" xml:id="_SJDvMxB">
					<idno type="grant-number">2022.14516.BD</idno>
				</org>
				<org type="funding" xml:id="_CfF2Mhf">
					<idno type="grant-number">2022.11566.BD</idno>
				</org>
				<org type="funding" xml:id="_kcYPf5N">
					<idno type="grant-number">2020.06434.BD</idno>
				</org>
				<org type="funding" xml:id="_yp5EarX">
					<idno type="grant-number">2020.07034</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,111.28,394.53,10.91;13,112.66,124.83,395.17,10.91;13,112.66,138.38,394.53,10.91;13,112.66,151.93,395.17,10.91;13,112.39,165.48,394.80,10.91;13,112.48,179.03,394.70,10.91;13,112.66,192.57,395.17,10.91;13,112.66,206.12,393.32,10.91;13,112.66,219.67,394.52,10.91;13,112.33,233.22,120.27,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,224.93,179.03,282.25,10.91;13,112.66,192.57,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,363.94,192.57,143.89,10.91;13,112.66,206.12,393.32,10.91;13,112.66,219.67,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,280.09,219.67,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,246.77,393.33,10.91;13,112.66,260.32,393.32,10.91;13,112.66,273.87,393.33,10.91;13,112.66,287.42,394.53,10.91;13,112.66,300.97,90.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,350.20,246.77,155.79,10.91;13,112.66,260.32,393.32,10.91;13,112.66,273.87,55.76,10.91">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmedical 2022 Caption</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rio-Torto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Patr√≠cio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gon√ßalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,191.58,273.87,314.41,10.91;13,112.66,287.42,320.17,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings, CEUR-WS.org<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1535" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,314.52,394.52,10.91;13,112.66,328.07,395.17,10.91;13,112.66,341.62,393.33,10.91;13,112.66,355.17,247.61,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,298.72,328.07,209.11,10.91;13,112.66,341.62,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.89,341.62,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="13,429.41,341.62,76.58,10.91;13,112.66,355.17,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,368.71,393.33,10.91;13,112.33,382.26,393.65,10.91;13,112.66,395.81,394.53,10.91;13,112.66,409.36,266.54,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,370.24,368.71,135.74,10.91;13,112.33,382.26,166.38,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,301.70,382.26,204.28,10.91;13,112.66,395.81,389.95,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,422.91,393.32,10.91;13,112.66,436.46,397.48,10.91;13,112.36,452.45,91.42,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,186.64,422.91,319.34,10.91;13,112.66,436.46,52.71,10.91">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
		<ptr target="https://doi.org/10.1093/nar/gkh061" />
	</analytic>
	<monogr>
		<title level="j" coord="13,174.12,436.46,104.19,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,463.56,393.33,10.91;13,112.66,477.11,394.53,10.91;13,112.66,490.66,397.48,10.91;13,112.66,506.65,49.88,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,354.40,463.56,151.59,10.91;13,112.66,477.11,38.41,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,196.50,477.11,305.75,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,517.76,393.33,10.91;13,112.66,531.30,336.70,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,236.18,517.76,169.80,10.91">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,429.48,517.76,76.50,10.91;13,112.66,531.30,211.47,10.91">3rd International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,544.85,393.33,10.91;13,112.66,558.40,394.53,10.91;13,112.66,571.95,265.32,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,258.15,544.85,202.68,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,558.40,313.85,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,585.50,393.33,10.91;13,112.66,599.05,393.33,10.91;13,112.66,612.60,99.11,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,247.99,585.50,258.00,10.91;13,112.66,599.05,50.14,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,186.59,599.05,295.51,10.91">3rd International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,626.15,393.33,10.91;13,112.66,639.70,393.32,10.91;13,112.33,653.25,390.38,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,345.70,626.15,160.29,10.91;13,112.66,639.70,66.23,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,225.23,639.70,280.75,10.91;13,112.33,653.25,30.22,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,666.80,393.33,10.91;14,112.66,86.97,259.68,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,407.45,666.80,98.53,10.91;14,112.66,86.97,145.68,10.91">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,266.84,86.97,57.98,10.91">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,100.52,395.17,10.91;14,112.66,114.06,393.33,10.91;14,112.66,127.61,393.32,10.91;14,112.66,141.16,395.01,10.91;14,112.66,154.71,144.31,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,441.53,100.52,66.31,10.91;14,112.66,114.06,277.66,10.91">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/touvron21a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,419.68,114.06,86.30,10.91;14,112.66,127.61,260.65,10.91;14,445.33,128.63,60.65,9.72;14,112.66,142.18,113.24,9.72">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)<address><addrLine>PMLR, Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="14,112.66,168.26,394.53,10.91;14,112.66,181.81,394.53,10.91;14,112.66,195.36,393.33,10.91;14,112.66,208.91,393.33,10.91;14,112.66,222.46,393.33,10.91;14,112.66,236.01,359.40,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,292.01,195.36,213.98,10.91;14,112.66,208.91,46.57,10.91">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,186.22,208.91,319.77,10.91;14,112.66,222.46,393.33,10.91;14,112.66,236.01,46.58,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,249.56,395.17,10.91;14,112.66,263.11,393.33,10.91;14,112.66,276.66,395.01,10.91;14,112.66,290.20,282.06,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,308.25,249.56,199.58,10.91;14,112.66,263.11,130.43,10.91">RATCHET: Medical Transformer for Chest Xray Diagnosis and Reporting</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-2_28" />
	</analytic>
	<monogr>
		<title level="m" coord="14,272.21,263.11,233.77,10.91;14,112.66,276.66,102.99,10.91">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,303.75,393.33,10.91;14,112.66,317.30,393.33,10.91;14,112.26,330.85,394.93,10.91;14,112.66,344.40,367.36,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,313.59,303.75,192.39,10.91;14,112.66,317.30,128.07,10.91">On the limits of cross-domain generalization in automated X-ray prediction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v121/cohen20a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,261.48,317.30,244.51,10.91;14,112.26,330.85,116.69,10.91;14,297.40,331.87,174.32,9.72">Proceedings of the Third Conference on Medical Imaging with Deep Learning (MIDL)</title>
		<meeting>the Third Conference on Medical Imaging with Deep Learning (MIDL)<address><addrLine>PMLR, Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="136" to="155" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="14,112.66,357.95,394.52,10.91;14,112.28,371.50,393.70,10.91;14,112.30,385.05,393.68,10.91;14,112.66,398.60,393.33,10.91;14,112.66,412.15,394.03,10.91;14,112.39,425.70,91.73,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,339.61,371.50,166.37,10.91;14,112.30,385.05,120.48,10.91">TorchXRayVision: A library of chest X-ray datasets and models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Viviano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Torabian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guarrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v172/cohen22a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,263.26,385.05,242.72,10.91;14,112.66,398.60,208.54,10.91;14,400.40,399.61,105.59,9.72;14,112.66,413.16,77.45,9.72">Proceedings of The 5th International Conference on Medical Imaging with Deep Learning (MIDL)</title>
		<meeting>The 5th International Conference on Medical Imaging with Deep Learning (MIDL)<address><addrLine>PMLR, Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="231" to="249" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="14,112.66,439.25,393.33,10.91;14,112.66,452.79,348.10,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,225.12,439.25,177.47,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,428.80,439.25,77.19,10.91;14,112.66,452.79,211.47,10.91">7th International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,466.34,394.52,10.91;14,112.66,479.89,393.33,10.91;14,112.66,493.44,394.53,10.91;14,112.66,506.99,22.69,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,295.53,466.34,211.65,10.91;14,112.66,479.89,115.57,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,315.41,479.89,190.58,10.91;14,112.66,493.44,390.05,10.91">EMC2: Energy Efficient Machine Learning and Cognitive Computing Workshop at Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>5th Edition of</note>
</biblStruct>

<biblStruct coords="14,112.66,520.54,393.54,10.91;14,112.66,534.09,393.33,10.91;14,112.33,547.64,397.81,10.91;14,112.36,563.63,109.22,7.90" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,352.43,520.54,153.76,10.91;14,112.66,534.09,75.68,10.91">Self-Critical Sequence Training for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.131</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,233.27,534.09,272.71,10.91;14,112.33,547.64,30.83,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,574.74,393.33,10.91;14,112.66,588.29,394.52,10.91;14,112.28,601.84,127.13,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,375.83,574.74,130.16,10.91;14,112.66,588.29,97.01,10.91">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,233.57,588.29,269.19,10.91">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,615.39,395.17,10.91;14,112.66,628.93,394.53,10.91;14,112.66,642.48,255.52,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,156.31,615.39,253.99,10.91">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m" coord="14,433.56,615.39,74.27,10.91;14,112.66,628.93,308.43,10.91">Text Summarization Branches Out, Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,656.03,393.33,10.91;14,112.26,669.58,393.73,10.91;15,112.66,86.97,394.53,10.91;15,112.41,100.52,215.61,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,319.38,656.03,186.60,10.91;14,112.26,669.58,83.52,10.91">General Multi-label Image Classification with Transformers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.01621</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,249.59,669.58,256.40,10.91;15,112.66,86.97,88.51,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="16473" to="16483" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
