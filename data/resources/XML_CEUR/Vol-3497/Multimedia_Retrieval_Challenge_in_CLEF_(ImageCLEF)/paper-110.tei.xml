<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,391.94,15.42;1,89.29,106.66,328.59,15.42">Overview of ImageCLEFfusion 2023 Task -Testing Ensembling Methods in Diverse Scenarios</title>
				<funder ref="#_kgm43kA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,92.56,11.96"><forename type="first">Liviu-Daniel</forename><surname>Åžtefan</surname></persName>
							<email>liviu-daniel.stefan@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.89,134.97,121.16,11.96"><forename type="first">Mihai</forename><forename type="middle">Gabriel</forename><surname>Constantin</surname></persName>
							<email>mihai.constantin84@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.09,134.97,69.56,11.96"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mihai.dogariu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.25,134.97,75.70,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,391.94,15.42;1,89.29,106.66,328.59,15.42">Overview of ImageCLEFfusion 2023 Task -Testing Ensembling Methods in Diverse Scenarios</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8F46A2ABB7AEF557B2A6ED97EDA66BAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Late fusion</term>
					<term>Ensembling</term>
					<term>Fusion benchmarking</term>
					<term>Visual interestingness prediction</term>
					<term>Image search results diversification</term>
					<term>Caption detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a comprehensive overview of the second edition of the ImageCLEFfusion task, held in 2023. The primary goal of this endeavor is to facilitate the advancement of late fusion or ensembling methodologies, which possess the capability to leverage prediction outcomes derived from pre-computed inducers to generate superior and enhanced prediction outputs. The present iteration of this task encompasses three distinct challenges: the continuation of the previous year's regression challenge utilizing media interestingness data, where performance is measured via the mAP at 10 metric; the continuation of the retrieval challenge involving image search result diversification data, where performance is measured via the F1-score and Cluster Recall at 20; and the addition of a new multi-label classification task focused on concepts detection in medical data, where performance is measured via the F1-score. Participants were provided with a predetermined set of pre-computed inducers and were strictly prohibited from incorporating external inducers during the competition. This ensured a fair and standardized playing field for all participants. A total of 23 runs were received and the analysis of the proposed methods shows diversity among them ranging from machine learning approaches that join the inducer predictions to ensemble schemes that learn the results of other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The fusion task, part of ImageCLEF <ref type="bibr" coords="1,254.35,456.29,11.48,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,268.89,456.29,7.65,10.91" target="#b1">2]</ref>, was first proposed in 2022 <ref type="bibr" coords="1,406.38,456.29,13.00,10.91" target="#b2">[3]</ref> comprising of two subtasks: a regression challenge utilizing media interestingness data (ImageCLEFfusion-int) and a retrieval challenge involving image search result diversification data (ImageCLEFfusion-div).</p><p>In 2023 <ref type="bibr" coords="1,123.56,496.94,11.28,10.91" target="#b1">[2]</ref>, both subtasks, ImageCLEFfusion-int and ImageCLEFfusion-div, were running again with the addition of a multi-label classification task focused on concepts detection in medical data (ImageCLEFfusion-cap). These type of tasks typically exhibit inferior performance in endto-end systems when juxtaposed with conventional computer vision tasks. This phenomenon is frequently ascribed to their intrinsic subjectivity and multi-modality, compounded by challenges associated with establishing dependable ground-truth annotations <ref type="bibr" coords="2,397.29,86.97,11.49,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,412.33,86.97,7.65,10.91" target="#b4">5]</ref>. To address these limitations, researchers have turned to late fusion or ensembling systems as a primary approach to enhance model performance. These systems involve the integration of multiple individual prediction systems, referred to as inducers, through fusion schemes.</p><p>Given these factors, the participants in this task are faced with several challenges that necessitate exploration. These challenges include diversity, which pertains to a collection of classifiers that generate varying predictions for the same instance; voting mechanism, which governs the utilization of individual outputs from the base models during prediction; dependency, which refers to the influence of a base model on the construction of the subsequent model in the fusion chain; cardinality, which denotes the number of individual base models composing the ensemble-a delicate balance must be struck, as incorporating too many models may diminish diversity within the fusion; and finally, the learning mode of the base models, which represents the characteristic that enables the classifiers to effectively adapt to new, previously unseen data while retaining previously acquired knowledge.</p><p>This paper presents an overview of the 2023 ImageCLEFfusion task including the data creation in Section 2, the evaluation methodology in Section 3, and the task and participation in Section 4. The results are described in Section 5, followed by conclusion in Sections 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data description</head><p>The ImageCLEFfusion framework encompasses three distinct tasks, each utilizing different datasets and associated challenges: ImageCLEFfusion-int: This task focuses on the Interestingness10k dataset <ref type="bibr" coords="2,456.62,393.22,11.32,10.91" target="#b4">[5]</ref>. Specifically, it utilizes image-based prediction data derived from the 2017 MediaEval Predicting Media Interestingness task <ref type="bibr" coords="2,235.17,420.32,11.28,10.91" target="#b5">[6]</ref>. The task provides prediction outputs from 29 systems that participated in the benchmarking task. To facilitate the training and evaluation of fusion systems, the available data is divided into 1,877 samples for training and 558 samples for testing. ImageCLEFfusion-div: This task relies on the Retrieving Diverse Social Images dataset <ref type="bibr" coords="2,152.65,488.15,11.58,10.91" target="#b6">[7]</ref>, specifically targeting the DIV150Multi challenge <ref type="bibr" coords="2,400.74,488.15,11.58,10.91" target="#b7">[8]</ref>. The task provides retrieval outputs from 56 systems, which are further divided into 60 queries for the training data and 63 queries for the testing data. ImageCLEFfusion-cap: This task is derived from the ImageCLEF Medical Caption Task <ref type="bibr" coords="2,139.23,542.44,11.28,10.91" target="#b8">[9]</ref>. It involves the extraction of multi-label outputs from 84 inducers. The data used for this task consists of 6,101 images for the development set and 1,500 images for the testing set.</p><p>For the training sets, we provide a comprehensive package comprising ground truth data, inducer prediction outputs, detailed inducer performance metrics, and the requisite scripts for metric computation. Conversely, the testing sets solely include the inducer prediction outputs. The characteristics of the datasets used in these tasks are presented in Table <ref type="table" coords="2,420.32,628.93,3.66,10.91" target="#tab_0">1</ref>. Participants have the freedom to generate their own validation sets by partitioning the training set according to their specific needs. However, to ensure a fair and reasonable selection of proposed fusion methods, participants are limited to a maximum of 10 runs for each of the three tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation Methodology</head><p>Participants were required to devise late fusion learning strategies based on the outputs of the inducers associated with the media samples for each of the subtasks. The evaluation of the participants' submissions was conducted using the Mean Average Precision at 10 (mAP@10) metric for the ImageCLEFfusion-int task, F1 at 20 (F1@20) and Cluster Recall at 20 (Cluster Recall@20) metrics for the ImageCLEFfusion-div task, and the F1 metric for the ImageCLEFfusion-cap task.</p><p>The aforementioned metrics align with the evaluation measures employed for the individual datasets pertaining to each of the three tasks. Participants were encouraged to submit their solutions for all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Participation</head><p>A total of 12 teams completed their registration for ImageCLEFfusion, demonstrating a strong interest in the competition. Among these teams, two successfully submitted their runs and completed the competition by submitting detailed working notes describing their methods.</p><p>In terms of the interestingness task, both teams collectively submitted 13 runs, while one team submitted a total of 10 runs for the diversification task. No runs were recorded for the ImageCLEFfusion-cap task. For a comprehensive overview of the participating teams, please refer to Table <ref type="table" coords="3,151.68,446.44,3.74,10.91">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Groups that participated with runs in the ImageCLEFfusion tasks. We present the institutions represented by these teams, the number of runs for ImageCLEFfusion-int, ImageCLEFfusion-div, and ImageCLEFfusion-cap, as well as references to their papers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageCLEFfusion-int task</head><p>A total of 13 runs were submitted by two teams for the ImageCLEFfusion-int task. The highest achieved performance in terms of MAP@10 value was 0.1331, indicating a significant improve-ment of 40.65% compared to the baseline value of 0.0946. Despite the reduced number of participants compared to the previous year, the participating team achieved a performance that surpassed the majority of the participants in the previous year, but still under the state-ofthe-art result of the last year achieved by <ref type="bibr" coords="4,276.78,127.61,16.35,10.91" target="#b11">[12]</ref>. The results for the participating teams for the ImageCLEFfusion-int task are presented in Tables <ref type="table" coords="4,313.60,141.16,3.74,10.91">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSN CSE-ML:</head><p>The SSN CSE-ML team's most successful run attained a mAP@10 score of 0.1331, establishing itself as the highest-scoring submission among the participating teams in this year's competition for this particular subtask. Balasundaram et al. <ref type="bibr" coords="4,439.35,188.82,17.80,10.91" target="#b10">[11]</ref> utilized an ensemble learning model approach based on a Voting Classifier that leverages XGBoost, decision trees, and K-nearest neighbors algorithms, and uses grid search to find the best hyperparameters for each classifier, and the optimal voting scheme and weights for the Voting Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS_Morgan:</head><p>The best performing run from the CS_Morgan team achieved a mAP@10 of 0.1287. For this approach, Emon and Rahman <ref type="bibr" coords="4,334.74,278.30,18.07,10.91" target="#b9">[10]</ref> utilized an ensemble of decision trees trained sequentially, with each tree using the predictions from the previous tree to calculate residual errors. A shrinkage technique is applied to reduce the ensemble's impact after each tree's prediction. The ensemble's final predictions are obtained by averaging the regression results. Additionally, the predictions undergo scaling through min-max normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Participation in the ImageCLEF-int 2023 task: the best score from all runs for each team. We also included a baseline that consists of the average performance of all the provided inducers.</p><p>Team #Runs mAP@10 SSN CSE-ML 10 0.1331 CS_Morgan 3 0,1287 baseline -0.0946</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageCLEFfusion-div task</head><p>A single team submitted a total of 10 runs for the ImageCLEFfusion-div task. The highest performance achieved by the team resulted in an F1@20 score of 0.5708, indicating a 7.4% improvement compared to the baseline value of 0.5313. Additionally, for the secondary metric CR@20, the corresponding system exhibited an improvement of 8.45%. The results for the participating teams in the ImageCLEFfusion-div task can be found in Tables <ref type="table" coords="4,429.95,571.69,3.74,10.91">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSN CSE-ML:</head><p>The best performing run from the SSN CSE-ML team achieved an F1@20 score of 0.5708 and an CR@20 score of 0.449 using the same model construction as for the ImageCLEFfusion-int task, i.e., bulding an ensemble model based on three classifier models (XGBoost, decision tree, and K-nearest neighbors), and finally creating a Voting Classifier based on the best combination of voting scheme and weights obtained through a grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Participation in the ImageCLEF-div 2023 task: the best score from all runs for each team. We also included a baseline that consists of the average performance of all the provided inducers.</p><p>Team #Runs F1@20 CR@20 SSN CSE-ML 10 0.5708 0.449 baseline -0.5313 0.414</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The second edition of the ImageCLEFfusion task garnered submissions from a total of two teams. The participants were presented with three tasks: the continuation of the previous year's regression challenge, which involved media interestingness data; the continuation of the retrieval challenge, which focused on image search result diversification data; and the addition of a new multi-label classification task centered around concepts detection in medical data. In total, the teams submitted 23 runs, with 13 runs for media interestingness and 10 runs for diversification. Unfortunately, no runs were recorded for the concept detection task. Despite the reduced number of participants compared to the previous year, with only two teams submitting runs for two out of the three presented tasks, the participating teams achieved commendable performance that surpassed the majority of the participants from the previous year. However, their performance still fell short of the state-of-the-art result achieved in the previous year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,418.32,73.22"><head>Table 1</head><label>1</label><figDesc>Data composition for the ImageCLEFfusion-int, ImageCLEFfusion-div tasks and ImageCLEFfusion-cap.</figDesc><table coords="3,156.41,118.58,282.45,45.13"><row><cell>Task</cell><cell>Training set</cell><cell>Testing set</cell><cell>No. inducers</cell></row><row><cell cols="2">ImageCLEFfusion-int 1,877 images</cell><cell>558 images</cell><cell>29</cell></row><row><cell>ImageCLEFfusion-div</cell><cell>60 queries</cell><cell>63 queries</cell><cell>56</cell></row><row><cell cols="3">ImageCLEFfusion-cap 6,101 images 1,500 images</cell><cell>84</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgments</head><p>This work is supported under the <rs type="programName">H2020 AI4Media "A European Excellence Centre for Media, Society and Democracy"</rs> project, contract #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kgm43kA">
					<idno type="grant-number">951911</idno>
					<orgName type="program" subtype="full">H2020 AI4Media &quot;A European Excellence Centre for Media, Society and Democracy&quot;</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,112.66,478.27,395.01,10.91;5,112.66,491.82,395.17,10.91;5,112.39,505.37,394.80,10.91;5,112.66,518.92,394.62,10.91;5,112.66,532.47,393.33,10.91;5,112.66,546.02,395.17,10.91;5,112.66,559.57,393.54,10.91;5,112.66,573.11,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,352.50,518.92,154.78,10.91;5,112.66,532.47,311.34,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PÃ©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,446.87,532.47,59.11,10.91;5,112.66,546.02,395.17,10.91;5,112.66,559.57,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="5,359.20,559.57,147.00,10.91;5,112.66,573.11,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,112.66,586.66,394.52,10.91;5,112.66,600.21,395.17,10.91;5,111.81,613.76,395.37,10.91;5,112.66,627.31,394.53,10.91;5,112.28,640.86,395.55,10.91;5,112.66,654.41,393.32,10.91;6,112.66,86.97,395.17,10.91;6,112.66,100.52,393.32,10.91;6,112.66,114.06,394.53,10.91;6,112.33,127.61,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,201.96,654.41,304.02,10.91;6,112.66,86.97,207.53,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, social media, and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-M</forename><surname>DrÄƒgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>StorÃ¥s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>SchÃ¶ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,343.23,86.97,164.60,10.91;6,112.66,100.52,393.32,10.91;6,112.66,114.06,126.40,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,270.57,114.06,226.68,10.91">Springer Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,141.16,393.33,10.91;6,112.66,154.71,394.53,10.91;6,112.66,168.26,394.53,10.91;6,112.66,181.81,47.34,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,356.66,141.16,149.33,10.91;6,112.66,154.71,390.40,10.91">Overview of imagecleffusion 2022 task-ensembling methods for media interestingness prediction and result diversification</title>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,128.59,168.26,115.53,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="6,252.30,168.26,184.53,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,195.36,394.53,10.91;6,112.28,208.91,393.71,10.91;6,112.28,222.46,125.20,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,112.28,208.91,281.84,10.91">Affect in multimedia: benchmarking violent scenes detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sjoberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,406.54,208.91,99.44,10.91;6,112.28,222.46,93.28,10.91">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,236.01,393.33,10.91;6,112.66,249.56,393.32,10.91;6,112.48,263.11,222.55,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,478.57,236.01,27.42,10.91;6,112.66,249.56,326.29,10.91">Visual interestingness prediction: a benchmark framework and literature review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SjÃ¶berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,447.46,249.56,58.52,10.91;6,112.48,263.11,123.39,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1526" to="1550" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,276.66,393.57,10.91;6,112.66,290.20,309.16,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SjÃ¶berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<title level="m" coord="6,435.63,276.66,70.60,10.91;6,112.66,290.20,164.62,10.91;6,299.97,290.20,91.24,10.91">Mediaeval 2017 predicting media interestingness task</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>MediaEval workshop</note>
</biblStruct>

<biblStruct coords="6,112.66,303.75,393.33,10.91;6,112.66,317.30,393.33,10.91;6,112.33,330.85,68.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,411.51,303.75,94.48,10.91;6,112.66,317.30,223.52,10.91">Benchmarking image retrieval diversification techniques for social media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>GÃ®nscÄƒ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,345.02,317.30,148.18,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,344.40,393.33,10.91;6,112.66,357.95,393.33,10.91;6,112.66,371.50,314.29,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,414.23,344.40,91.76,10.91;6,112.66,357.95,304.57,10.91">Div150multi: a social image retrieval result diversification dataset with multi-topic queries</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>GÃ®nscÄƒ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.14,357.95,65.85,10.91;6,112.66,371.50,246.56,10.91">Proceedings of the 7th international conference on multimedia systems</title>
		<meeting>the 7th international conference on multimedia systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,385.05,395.17,10.91;6,111.81,398.60,395.37,10.91;6,112.66,412.15,393.33,10.91;6,112.66,425.70,216.46,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,326.22,398.60,180.96,10.91;6,112.66,412.15,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,316.25,412.15,189.74,10.91;6,112.66,425.70,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,439.25,393.33,10.91;6,112.66,452.79,393.33,10.91;6,112.66,466.34,394.53,10.91;6,112.66,479.89,188.37,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,213.28,439.25,292.71,10.91;6,112.66,452.79,312.12,10.91">Media interestingness prediction in imagecleffusion 2023 with dense architecture-based ensemble &amp; scaled gradient boosting regressor model</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Emon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,447.41,452.79,58.58,10.91;6,112.66,466.34,394.53,10.91;6,112.66,479.89,59.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,493.44,394.52,10.91;6,112.66,506.99,394.61,10.91;6,112.66,520.54,393.33,10.91;6,112.66,534.09,247.61,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,112.66,506.99,374.88,10.91">Efficient fusion techniques for result diversification and image interestingness tasks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Prabavathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olirva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Vaibhav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Harshith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,112.66,520.54,393.33,10.91;6,112.66,534.09,119.17,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,547.64,393.33,10.91;6,112.66,561.19,393.33,10.91;6,112.66,574.74,328.13,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,348.05,547.64,157.94,10.91;6,139.34,561.19,254.94,10.91">Deepfusion methods for ensembling in diverse scenarios</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åžtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,418.05,561.19,87.94,10.91;6,112.66,574.74,210.16,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Ai multimedia lab at imagecleffusion</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
