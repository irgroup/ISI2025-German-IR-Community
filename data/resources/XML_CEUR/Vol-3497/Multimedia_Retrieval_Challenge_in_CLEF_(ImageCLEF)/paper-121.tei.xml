<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,75.47,450.98,16.98;1,72.00,96.23,451.04,16.98;1,72.00,117.01,119.23,16.98;1,72.00,147.28,236.22,10.80">Media Interestingness Prediction in ImageCLEFfusion 2023 with Dense Architecture-based Ensemble &amp; Scaled Gradient Boosting Regressor Model Notebook for the CS_Morgan Lab at CLEF 2023</title>
				<funder ref="#_ZkA9nHq">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_pEZADxD #_UdSjZGK">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,173.74,52.69,10.80"><forename type="first">Md</forename><surname>Ismail</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,127.69,173.74,66.37,10.80"><forename type="first">Siddiqi</forename><surname>Emon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,224.42,173.74,115.96,10.80"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,75.47,450.98,16.98;1,72.00,96.23,451.04,16.98;1,72.00,117.01,119.23,16.98;1,72.00,147.28,236.22,10.80">Media Interestingness Prediction in ImageCLEFfusion 2023 with Dense Architecture-based Ensemble &amp; Scaled Gradient Boosting Regressor Model Notebook for the CS_Morgan Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB1AB3243F97901F575FC187A3D36598</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ensemble</term>
					<term>Fusion</term>
					<term>Regression</term>
					<term>Gradient boost</term>
					<term>Deep fusion</term>
					<term>Dense Architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of computer vision plays a key role in managing, processing, analyzing, and interpreting multimedia data in diverse applications. Visual interestingness in multimedia contents is crucial for many practical applications, such as search and recommendation. Determining the interestingness of a particular piece of media content and selecting the highest-value item in terms of content analysis, viewers' perspective, content classification, and scoring media are sophisticated tasks to perform due to the heavily subjective nature. This work presents the approaches of the CS_Morgan team by participating in the media interestingness prediction task under ImageCLEFfusion 2023 benchmark evaluation. We experimented with two ensemble methods which contain a dense architecture and a gradient boosting scaled architecture. For the dense architecture, several hyperparameters tunings are performed and the output scores of all the inducers after the dense layers are combined using min-max rule. The gradient boost estimator provides an additive model in staged forward propagation, which allows an optimized loss function. For every step in the ensemble gradient boosting scaled (EGBS) architecture, a regression tree is fitted to the negative gradient of the loss function. We achieved the best accuracy with a MAP@10 score of 0.1287 by using the ensemble EGBS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This work presents the CS_Morgan team's participation in the ImageCLEFfusion 2023 <ref type="bibr" coords="1,481.72,593.68,12.82,9.88" target="#b0">[1]</ref> under the ImageCLEF 2023 benchmark evaluation campaign <ref type="bibr" coords="1,332.14,606.28,11.68,9.88" target="#b1">[2]</ref>. We participated solely in the media interestingness (ImageCLEFfusion-int) task, which is mainly an image interestingness score prediction regression task applied to the media interestingness data associated with the Interestingness10k dataset <ref type="bibr" coords="1,72.00,644.26,11.68,9.88" target="#b2">[3]</ref>. Generally, the concept visual or media interestingness attempts to measure the ability of multimedia (e.g., image, audio, video, etc.) content to capture and keep the viewer's attention for longer periods of time <ref type="bibr" coords="1,144.85,669.60,11.97,9.88" target="#b3">[4,</ref><ref type="bibr" coords="1,156.82,669.60,7.98,9.88" target="#b4">5]</ref>. A growing number of media contents makes it more difficult to calculate or quantify the interestingness, which is a high-level semantic concept and highly subjective <ref type="bibr" coords="2,486.10,74.76,11.68,9.88" target="#b2">[3]</ref>. The review of the literature in the domain of interestingness prediction, overview of the traditional fusion mechanisms, and investigation of several types of deep networks for creating the fusion systems are presented in <ref type="bibr" coords="2,128.16,112.68,11.68,9.88" target="#b4">[5]</ref>.</p><p>To create better, stronger image interestingness prediction results, the goal of this task is to use ensemble learning techniques to enhance the performance of individual prediction systems, called inducers or weak learner algorithms. It has been demonstrated many times since the early days of machine learning (ML) research that ensembles of classifiers can be more accurate than individual models <ref type="bibr" coords="2,107.00,188.60,11.93,9.88" target="#b5">[6,</ref><ref type="bibr" coords="2,118.93,188.60,7.95,9.88" target="#b6">7]</ref>. The ensemble methods use multiple learning algorithms as weak learners (inducers) to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. The algorithms generally add all the weak learners such that errors from each single learner or inducer are remunerated by other inducers, which eventually provides a robust performance by averaging out. The remainder of this paper describes the proposed methodology, result analysis, and conclusion with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>This media interestingness prediction task specifically concentrates on the fusion method Where the inducer's outputs are given. Participants' task is to combine these scores and predict the interestingness score of those visual contents. To do so, we proposed to use two different architectures, a Dense architecture, and a scaled Gradient Boosting Regressor and finally submitted three different runs based on those architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dense Architecture Figure 1: Dense Architecture</head><p>For the dense architecture, we put together a stack of dense layers (Fig. <ref type="figure" coords="2,423.90,585.34,4.70,9.88">1</ref>) and set up a dense ensemble with a predefined set of hyperparameters, such as a blend of different numbers of dense layers, neurons for each of these dense layers, batch norms, etc. A dense architecture contains all the neurons that are connected internally in a deep manner, which refers to the fact that every node or neuron in a dense architecture receives output from previous layers of neurons as input. For ensembling, we applied a combination of min-max (LFMinMax) of the output scores of all the inducers after the dense layers.</p><p>To do this, we can implement as many dense layers as we need. For this task, we proposed two different variations, where the second architecture is basically an extended version of the first (base) architecture. The first fusion architecture includes several dense layers with a normal kernel initializer along with rectified linear unit (ReLU) activation functions. Then we compiled our architecture with the mean squared error (MSE) loss function and fitted with Adam as the optimizer <ref type="bibr" coords="2,409.65,711.84,11.78,9.88" target="#b7">[8]</ref>. Additionally, in our second architecture, we increased the number of dense layers. In addition to that, the number of neurons in each of those dense layer's increases, respectively. Later, we scaled our results to conform to the final submission format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensemble Gradient Boosting Regressor (EGBS)</head><p>Boosting refers to the way an ensemble can 'boost' a weak learner into an arbitrarily accurate strong learner where the weak learner (inducer) is typically a decision tree with just one decision node. In boosting the estimators are trained in series with the training of a new member being influenced by overall performance so far <ref type="bibr" coords="3,196.46,167.78,11.69,9.88" target="#b7">[8]</ref>. The estimator performance also determines their contribution in the aggregation process. Gradient boosting seeks to optimize the training of new estimators in tandem with the aggregation process. The gradient-boosting regression trees resemble the idea of fusion in ML, more specifically late fusion, which derives from decision trees. This estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function <ref type="bibr" coords="3,437.44,231.02,11.69,9.88" target="#b8">[9]</ref>. When it is used for predicting continuous target variable (as a Regressor) as a regressor, the cost function is Mean Square Error (MSE).</p><p>Mainly, this method creates several decision trees or more specifically random forests. To prevent overfitting, the main idea is to use the ensemble method used for decision trees and then average the regression results as shown in Fig. <ref type="figure" coords="3,226.31,306.92,4.15,9.88">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Ensemble of the Gradient Boosting Regressor</head><p>Here, the key idea of late fusion is working in a sequential way such that out of the M trees, the training of the first tree is performed by feature matrix X (e.g., inducer score) and labels y. Using the predictions from the first tree, the residual error r0 was calculated. Then the second tree completes training using feature matrix X and the residual error r0 from the first tree as labels. From the prediction of the second tree, we calculate the residual error r1, and so on. It is important to mention that a shrunk technique Shrinkage is applied, which shrinks the ensemble after the prediction of each tree by multiplying the learning rate ranges from 0 to 1. To meet a standard of model performance, there is a tradeoff between learning rate and number of regression trees, where a declining learning rate must be compensated with upward estimators or a higher number of regression trees. Eventually, all the trees are trained completely, and prediction is performed using the following equation, where lr = learning rate:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y (pred) = y1 + (lr * r0) + (lr * r1) + …………... + (lr * rN) (1)</head><p>Then our prediction had to go through the final scaling pipeline to meet the actual prediction label using the following formula:</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Result Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset description</head><p>The data for this task is extracted and corresponds to the Interestingness10k dataset <ref type="bibr" coords="4,485.13,227.60,11.83,9.88" target="#b2">[3]</ref>. The organizer provided the output data (media interestingness prediction scores) from 29 inducers, which were gathered from the prediction outputs of the previous MediaEval Predicting Media Interestingness task under the benchmarking Initiative for Multimedia Evaluation <ref type="bibr" coords="4,373.06,265.52,16.88,9.88" target="#b9">[10]</ref>. The dataset splits into 1877 samples in the development set and 558 samples in testing set. The outputs from the 29 inducers for all the images in the development and test set are provided in .txt format, where each entry in this file contains the fields, video id, image id, classification (0 represents non-interesting and 1 represents interesting), and interestingness score by that respective inducer. Some inducers have a big range of results in the development set, which could result in unrealistic output due to the outliers. Therefore, it was necessary to convert them to the same range to meet our model consistency and get a higher accuracy. We submitted three different runs based on two architectures described in Section 2. Table <ref type="table" coords="4,487.37,516.10,5.49,9.88" target="#tab_0">1</ref> shows the three different runs. The first run (Dense Architecture Fusion 1) is based on the dense architecture fusion, which includes several dense layers with a normal kernel initializer along with ReLU activation function. For the second run (Dense Architecture Fusion 2), we increased the number of dense layers to the range of 10 to 25, which was previously in the range of 5 to 10. Further, the number of neurons in each of those dense layer's increases, respectively, in the range of 25 to 1000. Then the train data was fitted, and output prediction was carried out. Finally, our data was scaled, which was incorporated into the system to show the ultimate results. The third run (Ensembled Gradient Booster Scaled (EGBS)) is based on the Gradient Boosting Regressor method described in Section 2.2. From Table <ref type="table" coords="4,514.42,617.32,4.12,9.88" target="#tab_0">1</ref>, we can observe that the EGBS yields the best MAP@10 score of 0.1287.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results of the Submission</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>In our study, we implemented several ensemble methodologies for the given media interestingness regression task under ImageCLEFFusion 2023 benchmark evaluation <ref type="bibr" coords="4,377.80,710.28,11.69,9.88" target="#b0">[1]</ref>. Here we carry out both dense architecture and a gradient-boosting regressor for our target task. While training and experimenting, the hyperparameter tuning helped us achieve our objective. It was identified that using weight optimization and tuning the hyperparameter of the gradient-boosting regressor provides the best score for our regression task. In our future work, we plan to investigate a few diversified deep learning-based architectures, including self-attention mechanisms where one of the potential methods would be using attention with optimized gradient regression residuals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,144.80,408.63,300.07,66.72"><head>Table 1 :</head><label>1</label><figDesc>Results of the submitted runs in terms of MAP@10 score.</figDesc><table coords="4,144.80,422.55,290.79,52.80"><row><cell>Run ID</cell><cell>MAP@10</cell></row><row><cell>Dense Architecture Fusion 1</cell><cell>0.0595</cell></row><row><cell>Dense Architecture Fusion 2</cell><cell>0.0595</cell></row><row><cell>Ensembled Gradient Booster Scaled (EGBS)</cell><cell>0.1287</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">National Science Foundation (NSF)</rs> grant (<rs type="grantNumber">ID. 2131307</rs>) "<rs type="projectName">CISE-MSI</rs>: <rs type="projectName">DP</rs>: <rs type="projectName">IIS: III: Deep Learning-Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZkA9nHq">
					<idno type="grant-number">ID. 2131307</idno>
					<orgName type="project" subtype="full">CISE-MSI</orgName>
				</org>
				<org type="funded-project" xml:id="_pEZADxD">
					<orgName type="project" subtype="full">DP</orgName>
				</org>
				<org type="funded-project" xml:id="_UdSjZGK">
					<orgName type="project" subtype="full">IIS: III: Deep Learning-Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,108.02,239.73,414.76,9.99;5,108.02,252.39,414.94,9.99;5,108.02,265.05,414.75,9.98;5,108.02,277.76,388.92,9.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,113.24,252.39,409.72,9.99;5,108.02,265.05,40.19,9.98">Overview of ImageCLEFfusion 2023 Task -Testing Ensembling Methods in Diverse Scenarios</title>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,172.60,265.05,350.16,9.98;5,108.02,277.76,171.05,9.88">Experimental IR Meets Multilinguality, Multimodality, and Interaction. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,290.31,415.00,9.99;5,108.02,303.08,414.85,9.88;5,108.02,315.74,415.00,9.88;5,108.02,328.40,414.83,9.88;5,108.02,341.00,414.99,9.88;5,108.02,353.66,414.74,9.88;5,108.02,366.21,414.82,9.99;5,108.02,378.98,414.73,9.88;5,108.02,391.66,414.80,9.88;5,108.02,404.26,414.85,9.88;5,108.02,416.92,414.74,9.88;5,108.02,429.58,274.68,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,239.20,378.98,283.55,9.88;5,108.02,391.66,288.92,9.88">Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Alexander Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,417.90,391.66,104.92,9.88;5,108.02,404.26,414.85,9.88;5,108.02,416.92,207.20,9.88">Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="5,353.47,416.92,169.29,9.88;5,108.02,429.58,61.62,9.88">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 14th International Conference of the CLEF Association (CLEF<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09-18">2023. September 18-21, 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="5,108.02,442.13,414.94,9.99;5,108.02,454.90,414.74,9.88;5,108.02,467.56,194.37,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,494.23,442.24,28.72,9.88;5,108.02,454.90,317.07,9.88">Visual interestingness prediction: A benchmark framework and literature review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sjöberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,431.96,454.90,90.79,9.88;5,108.02,467.56,87.94,9.88">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1526" to="1550" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,480.16,414.91,9.88;5,108.02,492.82,414.80,9.88;5,108.02,505.48,381.34,9.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,328.71,480.16,194.22,9.88;5,108.02,492.82,114.03,9.88">Deep learning for multimodal-based video interestingness prediction</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Q K</forename><surname>Duong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2017.8019300</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,256.85,492.82,265.97,9.88;5,108.02,505.48,32.17,9.88">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1003" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,518.03,414.87,9.99;5,108.02,530.80,414.74,9.88;5,108.02,543.40,415.06,9.88;5,108.02,556.06,47.67,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,301.28,518.14,221.61,9.88;5,108.02,530.80,143.94,9.88">Exploring Deep Fusion Ensembling for Automatic Visual Interestingness Prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-81465-6_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-81465-6_2" />
	</analytic>
	<monogr>
		<title level="m" coord="5,490.43,530.80,32.33,9.88;5,108.02,543.40,145.10,9.88">Human Perception of Visual Information</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,568.72,333.57,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,163.04,568.72,99.74,9.88">Stacked generalization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,300.42,568.72,70.91,9.88">Neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,581.38,328.17,9.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,163.64,581.38,83.95,9.88">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00058655</idno>
	</analytic>
	<monogr>
		<title level="j" coord="5,285.21,581.38,75.72,9.88">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.02,594.04,414.79,9.88;5,108.02,606.70,125.01,9.88" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="5,266.82,594.04,171.54,9.88">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ICLR (Poster) 2015</note>
</biblStruct>

<biblStruct coords="5,108.02,619.30,415.06,9.88;5,108.02,631.96,163.75,9.88" xml:id="b8">
	<monogr>
		<ptr target="https://blog.paperspace.com/implementing-gradient-boosting-regression-python/" />
		<title level="m" coord="5,108.02,619.30,188.51,9.88">Implementing Gradient Boosting in Python</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,144.02,644.62,378.96,9.88;5,108.02,657.30,211.94,9.88" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,144.02,644.62,334.83,9.88">MediaEval Benchmarking Initiative for Multimedia Evaluation</title>
		<ptr target="http://www.multimediaeval.org/mediaeval2019/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
