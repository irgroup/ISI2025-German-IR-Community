<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,348.04,15.42;1,89.29,106.66,182.53,15.42">Multi-stage Medical Image Captioning using Classification and CLIP</title>
				<funder ref="#_7ND23vY #_79CFmd3 #_XzjK69h">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,64.67,11.96"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>masaki.aono.ss@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaokam Tempakucho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.60,134.97,74.27,11.96"><forename type="first">Hiroki</forename><surname>Shinoda</surname></persName>
							<email>shinoda.hiroki.vo@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaokam Tempakucho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.51,134.97,85.40,11.96"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa.tetsuya.um@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaokam Tempakucho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.55,134.97,76.24,11.96"><forename type="first">Kazuki</forename><surname>Shimizu</surname></persName>
							<email>shimizu@heart-center.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1Gobutori, Ohyamacho</addrLine>
									<postCode>441-8071</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.93,148.92,77.34,11.96"><forename type="first">Takuya</forename><surname>Togawa</surname></persName>
							<email>togawa@heart-center.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1Gobutori, Ohyamacho</addrLine>
									<postCode>441-8071</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.27,148.92,89.81,11.96"><forename type="first">Takuyuki</forename><surname>Komoda</surname></persName>
							<email>komoda@heart-center.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1Gobutori, Ohyamacho</addrLine>
									<postCode>441-8071</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,348.04,15.42;1,89.29,106.66,182.53,15.42">Multi-stage Medical Image Captioning using Classification and CLIP</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">DD8FEE111C442E08FB6E862459808ECB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image classification</term>
					<term>CLIP</term>
					<term>image captioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have participated in ImageCLEFmedical2023 caption prediction task alone as team "Bluefield-2023". In this paper, we propose a multi-stage medical image captioning, based on image classification as the early stage and CLIP (Contrastive Language-Image Pre-Training) as the final stage. In order to take advantage of the image classification problem, we have done automatic and semi-automatic grouping of both training and validation images into 7 groups (CT, MRI, Echo, Chest X-ray, X-ray Misc, Special, and Misc groups) analogous to ROCO dataset's predefined classes. The idea is to attempt to utilize CLIP model's image-text matching ability by separating given medical images into similar groups so that we try not to miss the fundamental terms that appear often inside each group. For instance, "MRI" group often includes terms such as "magnetic resonance" and "t1". We avail ourselves of these specific terms in the captions of training dataset and divide images into 7 groups in the first two stages. Then, we apply image classification for unknown (test) images into 7 groups. At the same time, we produce 7 different best CLIP models. In the fourth stage, we load the best CLIP model for each group associated with the captions, which are generated during the third (i.e., classification) stage. Although the final result with test dataset does not end up with what we have anticipated, we believe our approach would shed some light in this type of research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we focus on one of the tasks in Image Captioning; i.e., Caption Prediction Task <ref type="bibr" coords="1,492.63,483.19,11.28,10.91" target="#b0">[1]</ref>. There are several other tasks in ImageCLEFmedical2023 <ref type="bibr" coords="1,340.55,496.74,11.43,10.91" target="#b1">[2]</ref>.</p><p>Image captioning has been studied almost for decades. It is interpreted as describing the content of a given image in words through natural language processing. Popular benchmark datasets for image caption include COCO <ref type="bibr" coords="1,272.06,537.39,12.69,10.91" target="#b2">[3]</ref> and Flickr30k <ref type="bibr" coords="1,349.03,537.39,11.28,10.91" target="#b3">[4]</ref>. Show and Tell <ref type="bibr" coords="1,431.36,537.39,11.28,10.91" target="#b4">[5]</ref>, Show Attend and Tell <ref type="bibr" coords="1,127.26,550.94,12.77,10.91" target="#b5">[6]</ref> might belong to earlier approaches to image captioning where in most cases, CNN (Convolutional Neural Network) models are used as image feature extraction (a.k.a. image encoder), while RNN models such as LSTM are used for output text decoder. Later, CNN encoder has occasionally been replaced by Transformer based models (e.g., Vision Transformer ViT-32 <ref type="bibr" coords="2,102.05,127.61,10.99,10.91" target="#b6">[7]</ref>), and LSTM decoder has sometimes replaced by Transformer decoder such as Transform and Tell <ref type="bibr" coords="2,127.31,141.16,11.35,10.91" target="#b7">[8]</ref>. For the research related to "Show Attend and Tell" approach, it is noted that Ke et al <ref type="bibr" coords="2,99.98,154.71,12.94,10.91" target="#b8">[9]</ref> focused on vocabulary coherence to propose a "Reflective Decoding Network" to boost captioning performance.</p><p>On the other hand, image captioning for medical images where gray-scale images is dominant, has shorter history compared with the same task for general color images such as COCO and Flickr30k. As far as the authors can tell, medical image captioning in ImageCLEF has begun sometime around 2017, and has been one of main tasks in ImageCLEFmedical <ref type="bibr" coords="2,437.26,222.46,11.43,10.91" target="#b0">[1]</ref>.</p><p>It should be noted that medical image dataset such as ROCO (Radiology Objects in COntext) <ref type="bibr" coords="2,112.31,249.56,17.91,10.91" target="#b9">[10]</ref> has been used in PCM-CLIP <ref type="bibr" coords="2,260.05,249.56,16.25,10.91" target="#b10">[11]</ref>.</p><p>Examples of recent approaches to image-text matching are as follows: CLIP or Contrastive Language-Image Pre-training <ref type="bibr" coords="2,222.67,276.66,17.90,10.91" target="#b11">[12]</ref> was proposed for matching images and texts. BLIP or Bootstrapping Language-Image Pre-training <ref type="bibr" coords="2,271.12,290.20,18.06,10.91" target="#b12">[13]</ref> was introduced to outperform CLIP by filtering the noise generated by CLIP. BLIP was also used for image captioning as well as visual question answering. BLIP-2 was proposed <ref type="bibr" coords="2,238.09,317.30,17.91,10.91" target="#b13">[14]</ref> to generate a descriptive text given an image.</p><p>Popular evaluation criteria for medical image captioning include BLEU <ref type="bibr" coords="2,419.03,330.85,16.28,10.91" target="#b14">[15]</ref>, METEOR <ref type="bibr" coords="2,486.84,330.85,16.27,10.91" target="#b15">[16]</ref>, and CIDEr <ref type="bibr" coords="2,137.49,344.40,16.09,10.91" target="#b16">[17]</ref>, typically has been used in machine translation. From year 2023, BERTScore <ref type="bibr" coords="2,488.22,344.40,17.76,10.91" target="#b17">[18]</ref> is added as one of the evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Approach</head><p>Our proposed approach consists of classification of medical images, per-class CLIP model application, per-class search for most similar image, and the extraction of the caption associated with the image. Detailed processing is elaborated in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">First stage: automatically classifying data into 6 groups</head><p>In the first stage toward medical image captioning, we have adopted classification approach. Initially, we observed ImageCLEF2023 caption data (both image and caption) carefully. Then we have decided to partition data into 6 groups; CT, MRI, Echo, Chest X-ray, X-ray Misc, and Misc groups, respectively, as shown in Figure <ref type="figure" coords="2,300.58,534.05,5.17,10.91" target="#fig_1">1</ref> (note that "Misc" stands for Miscellaneous). The reason behind this is our strategy to "divide-and-conquer". To do this, we have looked for unique terms appearing in each group. Examples are as follows: CT group has terms such as "CT" and "computed tomography", MRI group has terms such as "magnetic resonance" and "T1", Echo group has terms such as "ultrasound" and "echocardiogram", Chest X-ray group has terms such as "chest X-ray" and "chest radiograph", X-ray Misc group has terms such as "skull X-ray" and "pelvis", while Misc group has terms such as "angiography" and "LAD". It should be noted that these 6 groups are not at all perfect. Indeed, even the sample terms described above might occur in two or more groups. Nevertheless, we believe that it is preferable to start with the above 6 groups as a rough clue to come closer to the appropriate caption, given unknown medical image with our classification strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Based on captions, we first automatically classify data into 6 groups</head><p>CT group (inc. CT, computer tomography)</p><p>MRI group (inc. MRI, magnetic resonance)</p><p>Echo group</p><formula xml:id="formula_0" coords="3,249.38,212.97,45.44,11.87">(inc. echo, ultrasound)</formula><p>Chest X-ray group (inc.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Second stage: adjusting data and re-classifying them into 7 groups</head><p>In the first stage, we have obtained 6 groups of medical images with their captions as the first order approximation. During the first stage, we have realized that the initial crude grouping worked out to some extent, so that it makes it possible to construct answer dataset for deep learning classification. Careful observation of the crude grouping result makes us feel that we need further elaboration on the "Misc" group. In particular, we have found that there are a small group of images that are not falling into any of the 6 groups. Indeed, our initial attempt to make a "Misc" group was to classify medical diagnostic images of real human subjects which are not belonging to other groups. Typical examples of "Misc" group include diagnostic images such as "angiography". However, we have found that there are certain amount of non-diagnostic images including illustrations, graphs, plottings, animals being operated, scenes of an operation room, and photos of medical apparatuses. A couple of examples of such images are shown in Figure <ref type="figure" coords="4,121.42,341.69,3.81,10.91" target="#fig_2">2</ref>. Consequently, we thus have constructed a new "Special" group for keeping these non-diagnostic images. well. This ends up the construction of 7 groups from given data for both training and validation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Third stage: deep neural network to classify given medical images</head><p>Based on the 7 groups for both training and validation datasets, we formulated a deep neural network to classify images from ImageCLEF2023 caption prediction data. Specifically, we have tested EfficientNetB0 and EfficientNetB1 for our 7-group classification <ref type="bibr" coords="4,392.77,445.61,17.75,10.91" target="#b18">[19]</ref> after trial and error of other EfficientNet group CNNs. The reason behind the adoption of these EfficientNet DNNs lies in our relevant research toward the stenosis detection for cardiac CT images where EfficientNet turned out to perform very well among several CNNs and Transformer-based DNNs <ref type="bibr" coords="4,466.92,486.26,11.43,10.91" target="#b6">[7]</ref>. Figure <ref type="figure" coords="4,131.00,499.81,5.03,10.91">3</ref> (a) shows the Epoch-Accuracy graph of EfficientNetB0, while (b) depicts the Epoch-Accuracy graph of EfficientNetB1 with the data tagged in Table <ref type="table" coords="4,363.03,513.36,3.66,10.91" target="#tab_0">1</ref>. Figure <ref type="figure" coords="4,403.29,513.36,4.97,10.91">4</ref> exhibits the confusion matrices of EfficientNetB0 (a) and EfficientNetB1 (b).</p><p>The best validation accuracy for the classification with EfficientNetB0 and EfficientNetB1 are 0.9333 and 0.9365, respectively. Hyperparameters for the classifications are the same and they are as follows: 25 epochs, cross entropy loss for the loss function, AdamW optimizer <ref type="bibr" coords="4,488.12,567.55,17.86,10.91" target="#b19">[20]</ref> with learning rate 0.001. Our run2 and run3 correspond to these CNNs. Table <ref type="table" coords="4,438.04,581.10,5.10,10.91" target="#tab_1">2</ref> demonstrates accuracy for each class, where both validation (open) and training (closed) accuracies are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fourth stage: Application of CLIP to each class</head><p>Once a given unknown image is classified into one of the 7 groups during the third stage, we proceed to the fourth stage. In this stage, we have adopted CLIP, as a multi-modal model, to predict caption from image. According to the data shown in Table <ref type="table" coords="5,384.73,562.06,3.73,10.91" target="#tab_0">1</ref>, we applied CLIP training process 7 times for each group. As far as we know, we can choose either Vision Transformer (ViT-32) <ref type="bibr" coords="5,127.41,589.15,12.73,10.91" target="#b6">[7]</ref> or ResNet50 as the backbone of the CLIP model <ref type="bibr" coords="5,354.14,589.15,16.13,10.91" target="#b11">[12]</ref>. Based on our past experience of the success of ResNet CNN family <ref type="bibr" coords="5,252.60,602.70,17.77,10.91" target="#b20">[21]</ref> on medical images where black and white images are dominant, we have chosen ResNet50 for our backbone inside CLIP.</p><p>The overall process in the fourth stage is illustrated in Figure <ref type="figure" coords="5,365.82,629.80,3.66,10.91" target="#fig_4">5</ref>. Best CLIP model can be found independently for each group as enclosed in dotted rectangle in Figure <ref type="figure" coords="5,408.96,643.35,3.77,10.91" target="#fig_4">5</ref>. Given an unknown image from test dataset, we apply our trained classifier mentioned in the third stage to predict   <ref type="table" coords="6,325.22,468.61,3.41,8.87" target="#tab_2">3</ref>.</p><p>the group most likely fitting. Then, we proceed to do similarity computation using group-best CLIP model, followed by sorting the confidence values from CLIP model, getting the most similar image, and yielding the caption corresponding to the image. Thus, the caption for the unknown input image can be retrieved from the captions belonging to the predicted group. Our evaluation results (run2 and run3) with test data received from organizers are shown in Table <ref type="table" coords="6,116.06,573.91,3.78,10.91" target="#tab_2">3</ref>. Although our results may not yield good performance, we believe that our proposed classification-based method shed some light in the future research. For example, it might be an idea to apply "Show Attend and Tell" to each group after classification into 7 groups to see what the results would look like. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusion</head><p>In this paper, we proposed a multi-stage image captioning, based on image classification as the early stage and CLIP (Contrastive Language-Image Pre-Training) as the final stage. For classification into 7 groups, we combined automatic grouping by means of terms specific to each group, yielding 7 groups of training and validation dataset. Our implementation with EfficientNet for the classification resulted in more than 93 % on the average. However, CLIP of the last stage needs more improvement, judging from the result from organizers. Nevertheless, we believe that our approach sheds some light on the applications with medical images and their captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>As we have demonstrated, the accuracy of classification in the third stage turned out to be quite good in terms of Figures <ref type="figure" coords="7,203.05,390.35,5.17,10.91">3</ref> and<ref type="figure" coords="7,230.60,390.35,3.81,10.91">4</ref>, as well as Table <ref type="table" coords="7,314.82,390.35,5.17,10.91" target="#tab_1">2</ref> except group "Special" introduced in the second stage. It might be controversial to introduce such a separate group with non-diagnostic images including illustrations and medical apparatuses as "Special" group, apart from "Misc" (i.e., Miscellaneous) group. An improvement to identify "Special" group in the second stateg might be to classify given images of this group automatically based on specific vocabularies by taking close look at the associated captions, which should allow us to reproduce our approach better.</p><p>Using CLIP and the subsequent retrieval of captions in the training data could be improved. For instance, during similarity computation, we have trimmed the length of captions into somewhere from 70 to 80 words, while when we retrieve the real caption in the given dataset, we return the caption with the original whole length. We have examined, with the validation data, the trimming effect of the caption length after submission, turned out to improve a few percentages in terms of BERT Score. Other evaluation criteria might yield similar improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,311.50,223.46,47.33,1.00;3,375.24,193.99,48.30,10.14;3,376.71,204.86,25.70,10.14;3,404.91,213.08,17.15,1.00;3,373.66,223.95,51.49,1.00;3,445.29,193.88,48.31,10.14;3,460.88,212.97,17.15,1.00;3,446.48,223.84,45.95,1.00;3,96.74,304.04,65.09,1.00;3,119.18,311.65,20.17,1.00;3,185.86,303.88,39.13,1.00;3,187.60,311.49,35.72,1.00;3,243.84,305.19,50.42,1.00;3,251.18,312.79,35.72,1.00;3,327.25,303.39,19.44,1.00;3,315.46,311.00,44.06,1.00;3,326.77,318.61,20.17,1.00;3,375.51,305.89,51.62,1.00;3,387.03,313.50,28.65,1.00;3,447.35,305.19,44.22,1.00;3,451.70,312.79,35.72,1.00"><head></head><label></label><figDesc>BY [Muacevic et al. (2020)] CC BY [Singh et al. (2010)] CC BY [Mudaliyar et al. (2017)] CC BY [Muacevic et al. (2020)] CC BY-NC [Kim et al. (2014)] CC BY-NC [Lee et al. (2012)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,330.79,416.69,8.93;3,89.29,342.80,146.65,8.87;3,173.12,370.01,93.59,127.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 6 predefined groups; both training and validation data are classified into 6 groups. Based on the terms appearing inside captions</figDesc><graphic coords="3,173.12,370.01,93.59,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,553.81,417.78,8.93;3,88.93,565.82,149.63,8.87;3,275.24,372.39,146.99,121.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of provided images in the training dataset that never fall into predefined 6 groups, which we put a new "Special" group.</figDesc><graphic coords="3,275.24,372.39,146.99,121.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,269.75,307.16,8.93;5,78.04,69.80,450.92,193.07"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Epoch-Accuracy graphs for EfficientB0 (a) and EfficientNetB1 (b)</figDesc><graphic coords="5,78.04,69.80,450.92,193.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,456.60,416.70,8.93;6,89.29,468.61,242.75,8.87;6,89.29,245.15,416.69,201.20"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overall image captioning flow using group-based CLIP model with trained classifier The evaluation result provided by organizer is shown in Table3.</figDesc><graphic coords="6,89.29,245.15,416.69,201.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,284.47,129.65"><head>Table 1</head><label>1</label><figDesc>Training and validation support for 7 classes</figDesc><table coords="4,221.82,122.10,151.64,98.03"><row><cell cols="3">Group name Training Validation</cell></row><row><cell>CT</cell><cell>21,609</cell><cell>3.924</cell></row><row><cell>Echo</cell><cell>8,292</cell><cell>1,554</cell></row><row><cell>MRI</cell><cell>8,563</cell><cell>1,420</cell></row><row><cell>Misc</cell><cell>6,349</cell><cell>1,047</cell></row><row><cell>Special</cell><cell>551</cell><cell>115</cell></row><row><cell>Chest X-ray</cell><cell>4,367</cell><cell>815</cell></row><row><cell>X-ray Misc</cell><cell>11,187</cell><cell>1,562</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,296.89,129.65"><head>Table 2</head><label>2</label><figDesc>Training and validation accuracies (%) for each class with EfficientNetB1</figDesc><table coords="6,222.36,122.10,150.55,98.03"><row><cell cols="3">Group name Training validation</cell></row><row><cell>CT</cell><cell>96.8</cell><cell>95.4</cell></row><row><cell>Echo</cell><cell>98.6</cell><cell>97.8</cell></row><row><cell>MRI</cell><cell>95.3</cell><cell>95.3</cell></row><row><cell>Misc</cell><cell>85.1</cell><cell>83.1</cell></row><row><cell>Special</cell><cell>62.6</cell><cell>53.6</cell></row><row><cell>Chest X-ray</cell><cell>94.5</cell><cell>94.0</cell></row><row><cell>X0ray Misc</cell><cell>91.6</cell><cell>92.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,416.99,81.83"><head>Table 3</head><label>3</label><figDesc>Test data accuracies (%) of our runs for test data. Run2 was based on EfficientNetB0, while Run3 was based on EfficientNetB1 as pretrained classifiers in the third stage.</figDesc><table coords="7,120.81,134.06,353.66,38.25"><row><cell cols="7">Bluefield run BERT Score ROUGE BLUERT METEOR CIDEr CLIP Score</cell></row><row><cell>Run2</cell><cell>0.5779</cell><cell>0.15344</cell><cell>0.27164</cell><cell>0.15431</cell><cell>0.06006</cell><cell>0.10091</cell></row><row><cell>Run3</cell><cell>0.5776</cell><cell>0.15392</cell><cell>0.27136</cell><cell>0.15404</cell><cell>0.06970</cell><cell>0.10482</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>A part of this research was carried out with the support of the <rs type="grantName">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</rs> and the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (C) (issue numbers <rs type="grantNumber">22K12149</rs> and <rs type="grantNumber">22K12040</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7ND23vY">
					<orgName type="grant-name">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</orgName>
				</org>
				<org type="funding" xml:id="_79CFmd3">
					<idno type="grant-number">22K12149</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_XzjK69h">
					<idno type="grant-number">22K12040</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,394.52,10.91;8,112.66,124.83,395.17,10.91;8,112.66,138.38,393.33,10.91;8,112.66,151.93,247.61,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,298.72,124.83,209.11,10.91;8,112.66,138.38,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.89,138.38,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="8,429.41,138.38,76.58,10.91;8,112.66,151.93,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,165.48,394.53,10.91;8,112.66,179.03,395.17,10.91;8,112.66,192.57,394.53,10.91;8,112.66,206.12,395.17,10.91;8,112.39,219.67,394.80,10.91;8,112.48,233.22,394.70,10.91;8,112.66,246.77,395.17,10.91;8,112.66,260.32,393.32,10.91;8,112.66,273.87,394.52,10.91;8,112.33,287.42,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,224.93,233.22,282.25,10.91;8,112.66,246.77,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.94,246.77,143.89,10.91;8,112.66,260.32,393.32,10.91;8,112.66,273.87,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,280.09,273.87,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,300.97,394.53,10.91;8,112.66,314.52,393.33,10.91;8,112.33,328.07,395.33,10.91;8,112.41,341.62,38.81,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,112.66,314.52,187.63,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.00,328.07,135.06,10.91">Computer Vision -ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,355.17,394.53,10.91;8,112.66,368.71,395.17,10.91;8,112.66,382.26,394.53,10.91;8,112.66,395.81,237.02,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,112.66,368.71,395.17,10.91;8,112.66,382.26,72.51,10.91">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.303</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,210.95,382.26,291.38,10.91">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,409.36,394.53,10.91;8,112.66,422.91,393.33,10.91;8,112.33,436.46,60.68,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,296.75,409.36,206.04,10.91">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,128.67,422.91,377.32,10.91;8,112.33,436.46,30.22,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,450.01,394.53,10.91;8,112.66,463.56,393.32,10.91;8,112.33,477.11,393.90,10.91;8,112.66,490.66,395.00,10.91;8,112.66,504.21,227.77,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,480.15,450.01,27.04,10.91;8,112.66,463.56,304.95,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/xuc15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,141.74,477.11,312.04,10.91;8,124.94,491.67,186.37,9.72">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>PMLR, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="8,112.66,517.76,395.16,10.91;8,112.66,531.30,393.33,10.91;8,112.41,544.85,393.58,10.91;8,112.66,558.40,393.08,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,421.13,531.30,84.86,10.91;8,112.41,544.85,257.64,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="8,392.80,544.85,113.18,10.91;8,112.66,558.40,125.91,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,571.95,394.62,10.91;8,112.66,585.50,365.77,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,240.70,571.95,246.90,10.91">Transform and tell: Entity-aware news image captioning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,585.50,335.32,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,599.05,394.52,10.91;8,112.66,612.60,394.52,10.91;8,112.66,626.15,22.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,283.70,599.05,218.98,10.91">Reflective decoding network for image captioning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,127.48,612.60,374.85,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,639.70,394.61,10.91;8,112.28,653.25,394.91,10.91;8,112.66,666.80,394.53,10.91;9,112.30,86.97,393.69,10.91;9,112.66,100.52,393.33,10.91;9,112.66,114.06,373.13,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,354.40,639.70,152.87,10.91;8,112.28,653.25,121.09,10.91;9,406.43,86.97,99.56,10.91;9,112.66,100.52,393.33,10.91;9,112.66,114.06,99.38,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<editor>D. Stoyanov, Z. Taylor, S. Balocco, R. Sznitman, A. Martel, L. Maier-Hein, L. Duong, G. Zahnd, S. Demirci, S. Albarqouni, S.-L. Lee, S. Moriconi, V. Cheplygina, D. Mateus, E. Trucco, E. Granger, P. Jannin</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="180" to="189" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology objects in context (roco): A multimodal image dataset</note>
</biblStruct>

<biblStruct coords="9,112.66,127.61,393.33,10.91;9,112.66,141.16,385.06,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07240</idno>
		<title level="m" coord="9,405.81,127.61,100.18,10.91;9,112.66,141.16,254.45,10.91">Pmc-clip: Contrastive language-image pre-training using biomedical documents</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,154.71,395.17,10.91;9,112.66,168.26,393.33,10.91;9,112.14,181.81,385.77,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,233.80,154.71,274.03,10.91;9,112.66,168.26,32.62,10.91">Clip-art: Contrastive pre-training for fine-grained art classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Turgutlu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW53098.2021.00444</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,170.84,168.26,335.14,10.91;9,112.14,181.81,92.84,10.91">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3951" to="3955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.39,208.91,394.80,10.91;9,112.66,222.46,393.33,10.91;9,112.66,236.01,394.53,10.91;9,112.66,249.56,332.38,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,238.17,195.36,267.82,10.91;9,112.39,208.91,212.15,10.91">BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/li22n.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,284.94,222.46,221.05,10.91;9,112.66,236.01,95.36,10.91;9,283.46,237.02,185.80,9.72">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="9,112.66,263.11,393.53,10.91;9,112.39,276.66,393.80,10.91;9,112.30,290.20,281.16,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,292.08,263.11,214.11,10.91;9,112.39,276.66,118.75,10.91">Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tewel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,255.27,276.66,250.92,10.91;9,112.30,290.20,172.53,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17918" to="17928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,393.33,10.91;9,112.66,317.30,393.33,10.91;9,112.66,330.85,394.53,10.91;9,112.66,344.40,397.48,10.91;9,112.66,360.39,121.09,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,318.69,303.75,187.29,10.91;9,112.66,317.30,100.78,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="9,237.08,317.30,268.90,10.91;9,112.66,330.85,133.28,10.91">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,371.50,393.33,10.91;9,112.66,385.05,393.33,10.91;9,112.66,398.60,394.52,10.91;9,112.66,412.15,265.67,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,207.90,371.50,298.08,10.91;9,112.66,385.05,149.52,10.91">METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W07-0734" />
	</analytic>
	<monogr>
		<title level="m" coord="9,285.02,385.05,220.97,10.91;9,112.66,398.60,281.48,10.91">Proceedings of the Second Workshop on Statistical Machine Translation, Association for Computational Linguistics</title>
		<meeting>the Second Workshop on Statistical Machine Translation, Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,425.70,393.33,10.91;9,112.66,439.25,393.33,10.91;9,112.33,452.79,395.33,10.91;9,112.66,466.34,381.64,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,327.47,425.70,178.52,10.91;9,112.66,439.25,95.01,10.91">CIDEr-R: Robust consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Oliveira Dos Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Colombini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.wnut-1.39</idno>
		<ptr target="https://aclanthology.org/2021.wnut-1.39.doi:10.18653/v1/2021.wnut-1.39" />
	</analytic>
	<monogr>
		<title level="m" coord="9,229.81,439.25,276.18,10.91;9,112.33,452.79,83.96,10.91">Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</title>
		<meeting>the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,479.89,393.33,10.91;9,112.66,493.44,395.01,10.91;9,112.66,506.99,238.25,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,386.12,479.89,119.86,10.91;9,112.66,493.44,92.47,10.91">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
	</analytic>
	<monogr>
		<title level="m" coord="9,230.14,493.44,246.51,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,394.52,10.91;9,112.66,534.09,393.33,10.91;9,112.66,547.64,394.52,10.91;9,112.66,561.19,299.32,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,178.09,520.54,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,293.89,534.09,212.10,10.91;9,112.66,547.64,90.84,10.91;9,269.41,548.65,176.97,9.72">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="9,112.66,574.74,393.33,10.91;9,112.66,588.29,393.33,10.91;9,112.66,601.84,58.17,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,233.10,574.74,180.82,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m" coord="9,446.71,574.74,59.28,10.91;9,112.66,588.29,182.30,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.39,395.17,10.91;9,112.66,628.93,394.53,10.91;9,112.66,642.48,22.69,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,259.74,615.39,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.38,615.39,19.45,10.91;9,112.66,628.93,389.39,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
