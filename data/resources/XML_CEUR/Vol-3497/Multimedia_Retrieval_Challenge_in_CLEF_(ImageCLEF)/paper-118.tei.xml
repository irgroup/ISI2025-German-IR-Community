<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,369.70,15.42;1,89.29,106.66,338.88,15.42;1,89.29,129.00,300.74,11.96">Finding the Source Images From the Generated Images with Contrastive Learning Methods Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,59.93,11.96"><forename type="first">Shitong</forename><surname>Cao</surname></persName>
							<email>stcao@mail.ynu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650504</postCode>
									<settlement>Kunming, Yunnan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.87,154.90,73.23,11.96"><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
							<email>zhouxb@ynu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<postCode>650504</postCode>
									<settlement>Kunming, Yunnan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,369.70,15.42;1,89.29,106.66,338.88,15.42;1,89.29,129.00,300.74,11.96">Finding the Source Images From the Generated Images with Contrastive Learning Methods Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A379815B5465FE789A9FA5865C13277B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GANs</term>
					<term>Contrastive Learning</term>
					<term>Pre-trained Model</term>
					<term>Triplet Loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an notebook for the ImageCLEFmedical GANs lab at CLEF 2023. Using generative adversarial networks for medical image generation is a standard data-expanding method. However, the quality of the generated data is not high, and finding the source based on the generated images is a new task worth investigating, as finding the source of the real data can ensure the reliability of the generated data. The GANs task is a completely new challenge in the ImageCLEFmedical track. The task is focused on examining the existing hypothesis that GANs are generating medical images that contain the "fingerprints" of the real images used for generative network training. In this paper, our team(one five one zero) use contrastive learning to find real images with high response values through similarity calculations based on the natural similarity between real images and generated images. We use a triplet loss function for optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generating medical images is a fundamental problem in medical imaging and can be used for applications such as data enhancement and model training <ref type="bibr" coords="1,372.49,434.97,12.96,10.91" target="#b0">[1]</ref>. However, the commonly used methods for medical image generation are all deep learning-based Generative Adversarial Networks (GANs), which require a large amount of real image data for training. In practical applications, the acquisition of real image data is often restricted by many aspects, such as data privacy and data protection. Therefore, false data generation has become a hot research direction in the field of medical imaging.</p><p>However, due to the unclear data sources in the process of false data generation, the quality and reliability of incorrect data have also been widely questioned <ref type="bibr" coords="1,376.33,529.82,13.69,10.91" target="#b1">[2]</ref>. Therefore, in the field of medical image generation, it is important to find the provenance of the real data to ensure the quality and reliability of the generated data. In addition, finding the provenance of real data can also help us scale up the dataset's size, improve the generalization ability of the model, and protect the privacy of the data.</p><p>The GANs task is an entirely new task in the ImageCLEFmedical track <ref type="bibr" coords="2,403.67,86.97,12.44,10.91" target="#b2">[3]</ref>. The task is focused on examining the existing hypothesis that GANs are generating medical images that contain the "fingerprints" of the real images used for generative network training <ref type="bibr" coords="2,407.31,114.06,12.50,10.91" target="#b3">[4]</ref>. The results of this task, for better or worse, can tell us something valuable. If the hypothesis is correct, artificial biomedical images may be subject to the same sharing and usage limitations as real sensitive medical data. On the other hand, if the hypothesis is wrong, GANs may be potentially used to create rich datasets of biomedical images that are free of ethical and privacy regulations.</p><p>To address the requirements of this task, this paper uses similarity calculations with the generated data as the target, the images used for a generation as positive examples, and the real images unused for a generation as negative examples. The model learns the data distributions of the three by learning them through contrastive learning, and since the generated images are output through the positive examples, the nature of generative adversarial networks is to learn the data distribution, so the generated data has a higher similarity to the data distribution than the positive examples, so contrastive learning is used as a way to distinguish the distribution of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Description and Task Analysis</head><p>Investigate the hypothesis that GANs are generating medical images that are in some way similar to the ones used for the GAN training. The task is related to the problem of the security of personal medical image data in the context of generating and using artificial images in different real-life scenarios.</p><p>The objective of the task is to detect "fingerprints" within the synthetic biomedical image data to determine which real images were used in training to produce the generated images. The task is to analyze test image datasets and assess the probability with which certain images of real patients were used for training image generators and which were not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Description</head><p>The benchmarking image data are the axial slices of 3D CT images of about 8000 lung tuberculosis patients. This particularly means that some of them may appear pretty "normal, " whereas the others may contain certain lung lesions, including severe ones. These images are stored in the form of 8-bit/pixel PNG images with dimensions of 256x256 pixels.</p><p>The published development dataset for the task includes 500 artificial images, 80 real images which were unused for training generative neural networks as well as 80 real images taken from the image set which has been used for training the corresponding generative model. The test dataset was created in a similar way. The only difference is that the two subsets of real images are mixed, and no proportion of non-used and used ones has been disclosed. Thus, a total of 10,000 were generated, and 200 real images were provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Task Analysis</head><p>The process of generating a model is essentially learning the distribution of accurate data. By analyzing the real training data, you can learn the distribution of the data, and having learned the distribution of the data, you can generate a lot of data that fits this data distribution based on the distribution. To find out the real images based on the generated fake images, the good idea is to fit the model, that is, to fit the generative ability of the model, because different models learn to learn different features; if you can know the network model and the parameters, you can achieve the inverse process, through the generated fake images reverse inference to the real images.</p><p>However, as the task did not tell us strictly what network was used, and the number of images given in the training set was very small, with only 80 of them used, it was difficult to train the generative adversarial network to achieve a simulation of the network model. Therefore, we can only focus on the distribution of the data, which is close to the distribution of the used images and differs significantly from the distribution of the unused data, so we can see the use of contrastive learning for the model's training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive learning</head><p>Contrastive learning uses data that has been augmented as a positive sample and data that is not of the same category as a negative sample for comparison training <ref type="bibr" coords="3,399.38,580.81,12.75,10.91" target="#b4">[5,</ref><ref type="bibr" coords="3,414.86,580.81,7.54,10.91" target="#b5">6]</ref>. At the same time, the images generated by the generative network are not the data obtained by cropping, scaling, rotating, etc., in the traditional sense, but are also obtained from used images, so they can be understood as the same class of data with similar to the data distribution is identical. Hence, the features implied by both are similar. The unused image is not involved in generating the image, so the difference between its features and the data distribution of the generated image will be more pronounced, so a comparative learning approach is used in this paper to build it <ref type="bibr" coords="3,471.53,662.11,10.82,10.91" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training Models</head><p>This paper adopts a pre-trained model, which can be pre-trained on large-scale data to learn universal and robust feature representations <ref type="bibr" coords="4,289.77,258.95,13.54,10.91" target="#b7">[8]</ref>. These features can be migrated to various specific tasks, thereby improving the generalization ability and performance of the model. By utilizing pre-trained models, knowledge from large-scale data can be transferred to specific tasks, thereby reducing training time and data volume. This can greatly improve the efficiency and accuracy of the model. In some application scenarios, insufficient or incomplete data annotation makes it difficult to train models directly.</p><p>Pre-trained models can utilize large-scale unlabeled data to learn the potential structure and patterns of the data, thus addressing the problem of insufficient or incomplete data annotation. This paper uses various pre-training models, including Inception V3, ResNet, and EfficientNet. <ref type="bibr" coords="4,143.26,380.89,12.45,10.91" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Triplet Loss Function</head><p>The triplet loss function describes the requirements of this task well <ref type="bibr" coords="4,393.75,430.62,17.49,10.91" target="#b9">[10]</ref>. In the design of the above model, the original image of the target is found by similarity by calculating the distance between the target and the positive and negative examples. This is consistent with the ternary loss function, as shown in Figure <ref type="figure" coords="4,238.57,471.27,3.74,10.91" target="#fig_1">2</ref>.</p><p>In this paper, our goal is to make the distance between the target and the positive example smaller than the distance between the target and the negative examples <ref type="bibr" coords="4,395.88,498.37,18.68,10.91" target="#b10">[11]</ref>. Therefore, we need to calculate the distance between the target, positive and negative examples, and use it as an input to the loss function. Specifically, we use the Euclidean distance to calculate the distance, as shown in Equation <ref type="bibr" coords="4,188.07,539.02,10.48,10.91" target="#b0">(1)</ref>.</p><formula xml:id="formula_0" coords="4,151.07,564.35,355.57,12.68">loss = max(0, ||洧녭 (洧논) -洧녭 (洧논 + )|| 2 -||洧녭 (洧논) -洧녭 (洧논 -)|| 2 + 洧녴洧녩洧洧녮洧녰洧녵)<label>(1)</label></formula><p>Where 洧녭 (洧논) denotes the generated image, 洧녭 (洧논 + ) denotes the positive example, the used image, and 洧녭 (洧논 -) denotes the negative example, the unused image, and 洧녴洧녩洧洧녮洧녰洧녵 is the hyperparameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Design</head><p>In existing experiments, training data was used for training and reasoning was done in test data. However, by analyzing the data, it can be seen that 500 images were generated in the training data, which were generated from 80 images. The data generated in the test data provided 10000 images. At this time, using training data for training and testing data for reasoning seemed inappropriate <ref type="bibr" coords="5,148.78,274.59,18.30,10.91" target="#b11">[12]</ref>. Due to the connection between the training data and the test data, it can be inferred that the 10000 images provided in the test have the same data distribution as the images in the training. Therefore, the generated images from the test data are also included in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Data Processing</head><p>If the test data is placed in training, there will be imbalanced data. The generated data in the given test data has 10000 images, while the test data in the training data only has 500 pieces. Therefore, copying 500 pieces of the training data is equivalent to giving greater weight <ref type="bibr" coords="5,484.12,392.06,18.84,10.91" target="#b12">[13]</ref>.</p><p>Thus achieving a balance between training and testing data. Secondly, there are only 80 used and unused images in the training, which means there are only 80 positive and negative images, respectively, which is a significant difference from the target's data volume. In comparative learning, it is necessary to take a target image, a positive image, and a negative image separately. After pre-processing, the number of data sets is shown in Table <ref type="table" coords="5,373.81,459.81,3.74,10.91" target="#tab_0">1</ref>.</p><p>Due to the large size of the target data, the positive data was enhanced to 1000 images and replicated 10 times to maintain consistency with the target data. In this way, data can be taken from the target and then from positive and negative, respectively. The process of contrastive learning can be completed. The reverse update is completed through the triplet loss function, making the distance between the target and the positive closer and closer and the distance between the target and the negative farther and farther.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results and Analysis</head><p>This task has three evaluation metrics: Accuracy, Recall, and F1 score. In this experiment, a total of three results were submitted, and three pre-trained models were used, i.e., Inception V3, ResNet and EfficientNet. The experimental equipment used in this experiment is RTX 3070, with a learning rate of 1e-4. The final submitted result scores are shown in Table <ref type="table" coords="5,450.72,631.48,3.74,10.91" target="#tab_1">2</ref>.</p><p>As can be seen from Table <ref type="table" coords="5,218.99,645.03,3.71,10.91" target="#tab_1">2</ref>, the best results were achieved by pre-training the model based on EfficientNet, which optimises the depth, width and resolution of the network at the same time and is highly versatile and scalable, and therefore performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, to complete the task of finding the original image according to the generated image, from the perspective of the similar data distribution between the generated image and the original image, we use the contrastive learning architecture, combined with transfer learning, and use the pre-trained feature extraction module to find the target with large response value through the similarity calculation method, so as to find the original image. Future research can attempt more fine-grained networks, such as those in face detection, as the generated medical images are highly similar and differ mainly in some fine-grained features. This is in line with the networks in face detection, and further research can be considered from this perspective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,277.12,416.69,8.93;3,89.29,289.12,416.69,8.87;3,89.29,301.08,138.79,8.87;3,154.89,84.19,285.50,185.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparative Learning Diagram. The generated image is obtained from the used image, so the two are similar in comparative learning. When comparing the generated image with the not used image for learning, it is dissimilar.</figDesc><graphic coords="3,154.89,84.19,285.50,185.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,175.28,416.70,8.93;4,89.29,187.28,312.91,8.87;4,222.39,84.19,150.50,84.50"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic diagram of triplet loss. The generated image serves as the anchor, the used image serves as positive sample, and the not used image serves as negative sample.</figDesc><graphic coords="4,222.39,84.19,150.50,84.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,290.66,57.92"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,89.29,102.49,290.36,45.91"><row><cell>Quantity after data preprocessing</cell><cell></cell></row><row><cell cols="2">generated(train+test) used unused</cell></row><row><cell>11000</cell><cell>11000 11000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,311.54,81.83"><head>Table 2</head><label>2</label><figDesc>Presentation of experimental resultsPre-trained model Accuracy Recall F1 score</figDesc><table coords="6,205.86,139.53,182.09,32.78"><row><cell>Inception V3</cell><cell>0.48</cell><cell>0.57</cell><cell>0.522</cell></row><row><cell>ResNet50</cell><cell>0.515</cell><cell>0.5</cell><cell>0.507</cell></row><row><cell>EfficientNet</cell><cell>0.52</cell><cell>0.62</cell><cell>0.563</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,411.12,395.17,10.91;6,112.66,424.67,150.97,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,207.48,411.12,166.19,10.91">Improved techniques for training gans</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,382.11,411.12,125.71,10.91;6,112.66,424.67,106.18,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,438.22,393.33,10.91;6,112.66,451.77,107.17,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07397</idno>
		<title level="m" coord="6,218.49,438.22,213.77,10.91">The deepfake detection challenge (dfdc) dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,112.66,465.31,394.53,10.91;6,112.66,478.86,395.17,10.91;6,112.66,492.41,394.53,10.91;6,112.66,505.96,394.52,10.91;6,112.66,519.51,393.72,10.91;6,112.66,533.06,395.17,10.91;6,112.66,546.61,395.17,10.91;6,112.66,560.16,393.59,10.91;6,112.66,573.71,380.72,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,463.73,519.51,42.66,10.91;6,112.66,533.06,395.17,10.91;6,112.66,546.61,77.37,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M칲ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dr캒gulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R칲ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc캼a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br칲ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch칛fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor친s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J A A A R I C V K A S G I</forename><surname>Nikolaos Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Sch칬ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>룞efan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,213.51,546.61,294.33,10.91;6,112.66,560.16,393.59,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,141.33,573.71,223.42,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,587.26,393.33,10.91;6,112.66,600.81,393.33,10.91;6,112.66,614.36,393.33,10.91;6,112.66,627.90,356.56,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,448.60,587.26,57.38,10.91;6,112.66,600.81,334.73,10.91">Overview of ImageCLEFmedical GANs 2023 task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M칲ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,464.74,600.81,41.25,10.91;6,112.66,614.36,287.68,10.91;6,421.96,614.36,84.03,10.91;6,112.66,627.90,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="6,143.49,627.90,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="6,112.66,641.45,394.53,10.91;6,112.66,655.00,346.82,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,192.43,641.45,310.29,10.91">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,127.29,655.00,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,86.97,393.33,10.91;7,112.66,100.52,107.17,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Der Pol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12247</idno>
		<title level="m" coord="7,220.77,86.97,212.53,10.91">Contrastive learning of structured world models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,114.06,393.33,10.91;7,112.66,127.61,179.72,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,218.61,114.06,133.33,10.91">Debiased contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,360.96,114.06,145.02,10.91;7,112.66,127.61,85.64,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8765" to="8775" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,141.16,393.33,10.91;7,112.66,154.71,225.86,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,186.08,141.16,277.27,10.91">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,472.07,141.16,33.92,10.91;7,112.66,154.71,131.78,10.91">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,168.26,393.33,10.91;7,112.66,181.81,394.53,10.91;7,112.41,195.36,27.76,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,203.90,168.26,188.89,10.91">Pre-trained image processing transformer</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,420.38,168.26,85.60,10.91;7,112.66,181.81,310.29,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,208.91,393.33,10.91;7,112.66,222.46,266.45,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,197.16,208.91,202.76,10.91">Rethinking triplet loss for domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,408.57,208.91,97.42,10.91;7,112.66,222.46,192.67,10.91">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,236.01,393.33,10.91;7,112.66,249.56,301.95,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,179.54,236.01,221.34,10.91">Correcting the triplet selection bias for triplet loss</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,423.33,236.01,82.66,10.91;7,112.66,249.56,223.63,10.91">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,263.11,393.33,10.91;7,112.66,276.66,297.80,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,218.59,263.11,183.14,10.91">Handling imbalanced datasets: A review</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,412.46,263.11,93.52,10.91;7,112.66,276.66,224.01,10.91">GESTS international transactions on computer science and engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,290.20,393.33,10.91;7,112.66,303.75,212.57,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,182.87,290.20,233.37,10.91">Data mining for imbalanced datasets: An overview</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.22,290.20,77.77,10.91;7,112.66,303.75,141.51,10.91">Data mining and knowledge discovery handbook</title>
		<imprint>
			<biblScope unit="page" from="875" to="886" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
