<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,378.98,15.42;1,89.29,106.66,289.22,15.42;1,89.29,129.00,244.49,11.96">Transferring Pre-Trained Large Language-Image Model for Medical Image Captioning Notebook for the Baidu Intelligent Health Unit and</title>
				<funder>
					<orgName type="full">OpenI Community</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,336.76,129.00,58.08,11.96"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName coords="1,88.72,168.85,74.72,11.96"><forename type="first">Wenshuo</forename><surname>Zhou</surname></persName>
							<email>ws.zhou@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.60,168.85,43.76,11.96"><forename type="first">Zhiyu</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.52,168.85,54.46,11.96"><forename type="first">Yehui</forename><surname>Yang</surname></persName>
							<email>yangyehuisw@126.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.62,168.85,49.85,11.96"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
							<email>wangsiqi06@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.11,168.85,74.92,11.96"><forename type="first">Haifeng</forename><surname>Huang</surname></persName>
							<email>huanghaifeng@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,182.79,68.90,11.96"><forename type="first">Rongjie</forename><surname>Wang</surname></persName>
							<email>wangrj@pcl.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.17,182.79,50.41,11.96"><forename type="first">Dalu</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory Joint Team at CLEF 2023</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,378.98,15.42;1,89.29,106.66,289.22,15.42;1,89.29,129.00,244.49,11.96">Transferring Pre-Trained Large Language-Image Model for Medical Image Captioning Notebook for the Baidu Intelligent Health Unit and</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D8F483051E0A1374FE5503F2E89B76A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Medical Image Caption</term>
					<term>Caption Prediction</term>
					<term>BLIP-2</term>
					<term>Domain Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the work conducted by the team "closeAI2023" in the ImageCLEFmedical Caption 2023 Image Caption sub-task. Medical image captioning poses unique difficulties due to the specialized nature of the medical domain. It requires the generation of accurate and coherent captions that not only describe the visual content but also capture the essential medical information conveyed by the images. To leverage the abilities of pre-trained Large-Image Models, we utilise the state-of-the-art BLIP-2 with a giant vision transformer (ViT-g) and Open Pre-trained Transformer Language Models (OPT2.7ùêµ) as the foundation of our caption prediction sub-task. To adapt the model to the medical domain, we employed a two-stage fine-tuning process. The pre-trained OPT2.7ùêµ was fixed during the whole training process. A step-wise fine-tuning of the ViT-g and the Q-Former modules was conducted to better align with the characteristics of medical data. Our team's approach yielded promising results, as we achieved a second-place ranking among all participating teams with a BERTScore of 0.6281. Additionally, our model performed well across various evaluation metrics: ROUGE of 0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF 2023 is a Multimodal Challenge organized as part of CLEF Initiative Labs <ref type="bibr" coords="1,461.64,495.15,13.26,10.91" target="#b0">[1]</ref>. Since 2003, ImageCLEF has been dedicated to investigating solutions for challenges involving multimodal data across diverse domains. Medical image captioning, recognized as a significant and demanding task within the medical field, has been featured in ImageCLEF for the seventh consecutive year. ImageCLEFmedical Caption 2023 <ref type="bibr" coords="1,319.64,549.34,14.44,10.91" target="#b1">[2]</ref> encompasses two subtasks, including Concept Detection and Caption Prediction Task. Our team mainly focused on the latter task. The Caption Prediction Task necessitates the generation of accurate and coherent descriptions based on medical images. To accomplish this, the model must effectively recognize and extract the semantic information embedded within the medical images, capture the inherent relationships between these semantics, and proficiently express them using medical terms. BERTScore <ref type="bibr" coords="2,136.21,224.90,15.64,10.91" target="#b2">[3]</ref> and ROUGE <ref type="bibr" coords="2,203.50,224.90,17.87,10.91" target="#b3">[4]</ref> are the primary and secondary evaluation metrics for this task.</p><p>The dataset used for ImageCLEFmedical Caption 2023 is an updated and extended version of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,325.76,252.00,12.98,10.91" target="#b4">[5]</ref>. ROCO dataset contains over 80,000 radiology images with various modalities including ultrasound, X-Ray, Computer Tomography (CT), Magnetic Resonance Imaging (MRI) and so on. The statistics of images modalities in the training set is shown in Table <ref type="table" coords="2,225.22,292.65,3.78,10.91" target="#tab_0">1</ref>. All images in ROCO have corresponding caption, keywords, Unified Medical Language Systems (UMLS) Concept Unique Identifiers (CUIs) and Semantic Type. For caption prediction task, the ground truth captions were pre-processed by removing all the links contained in original captions.</p><p>We employed BLIP-2 <ref type="bibr" coords="2,190.11,346.84,13.51,10.91" target="#b5">[6]</ref>, a vision-language pre-training method that bootstraps from frozen pre-trained unimodal models, for the caption prediction task. BLIP-2 is a recently proposed vision-language pre-training method by Li et al. building upon their previous work of BLIP <ref type="bibr" coords="2,491.39,373.94,14.59,10.91" target="#b6">[7]</ref> and it has demonstrated superior performance compared to various other vision-language pre-training methods, including Flamingo <ref type="bibr" coords="2,273.32,401.04,14.35,10.91" target="#b7">[8]</ref>, across a range of vision-language tasks such as visual question answering, image captioning, and image-text retrieval.</p><p>In this paper, our method is specifically introduced in Section 2, the experiments and results are demonstrated in Section 3 and a brief summary is given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>The pipeline of our method is shown in Figure <ref type="figure" coords="2,301.49,500.31,3.79,10.91" target="#fig_1">1</ref>. Our method adopted the fine-tuned BLIP-2 ViT-g OPT 2.7ùêµ model in <ref type="bibr" coords="2,202.43,513.86,12.99,10.91" target="#b5">[6]</ref> and a two-stage fine-tuning, i.e. concept-based fine-tuning and overall fine-tuning, was performed to the model on the competition dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>The framework of BLIP-2 consists of three main components: an image encoder, a lightweight Querying Transformer (Q-Former), and a large language model (LLM). The pre-training process of BLIP-2 comprises two stages: vision-language representation learning from a frozen image encoder stage and vision-to-language generative learning from a frozen LLM stage. In deep learning, the term "frozen" refers to a state where specific layers or parameters of a neural network are set to be untrainable or unmodifiable. This implies that, during the training process, the weights associated with these frozen layers or parameters remain constant and do not get  The training pipeline of our method. The image encoder and Q-Former are in yellow boxes, indicating that their parameters were updated during training, while the LLM in the green box had its parameters frozen. In concept-based fine-tuning stage, both concept loss and caption loss were utilized, but only caption loss was employed in overall fine-tuning stage.</p><p>updated. During the two pre-training stages of BLIP-2, image models and language models were frozen, preserving their initial image understanding and text generation capabilities. In contrast, Q-Former was trained exclusively in the pre-training to extract visual representations that effectively corresponded to textual information and provided this information to the LLM. When applying BLIP-2 models to downstream tasks such as image captioning, the LLM remained frozen during the fine-tuning process, while the parameters of the image encoder and Q-Former were updated <ref type="bibr" coords="3,145.98,419.58,13.96,10.91" target="#b5">[6]</ref>.</p><p>For our specific task, we adopted the BLIP-2 model for image captioning in <ref type="bibr" coords="3,444.75,433.13,13.00,10.91" target="#b5">[6]</ref> and chose the ViT-g/14 from EVA-CLIP <ref type="bibr" coords="3,213.79,446.68,15.72,10.91" target="#b8">[9]</ref> as the image encoder. Regarding the LLM selection, while the encoder-decoder-based FlanT5XL <ref type="bibr" coords="3,239.19,460.23,15.71,10.91" target="#b7">[8]</ref> exhibited superior performance in the zero-shot image captioning task compared to decoder-based OPT models, the OPT 2.7ùêµ <ref type="bibr" coords="3,413.22,473.78,18.07,10.91" target="#b9">[10]</ref> demonstrated a slightly stronger ability to generate normal captions <ref type="bibr" coords="3,314.88,487.33,12.92,10.91" target="#b5">[6]</ref>. As a result, we adopted the OPT 2.7ùêµ as the LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training Strategy</head><p>In the pre-training and fine-tuning phase of ViT-g and BLIP-2 ViT-g OPT 2.7ùêµ , standard natural image datasets such as ImageNet <ref type="bibr" coords="3,238.30,564.15,20.69,10.91" target="#b10">[11]</ref> and COCO <ref type="bibr" coords="3,306.41,564.15,24.03,10.91" target="#b11">[12]</ref> were employed. However, our task is based on medical images, which exhibit a substantial domain shift from natural images, thus we performed a two-stage fine-tuning process on the competition dataset with the LLM OPT 2.7ùêµ frozen and simultaneously the Q-Former together with the image encoder ViT-g were updated. The language modeling loss <ref type="bibr" coords="3,219.74,618.35,12.95,10.91" target="#b6">[7]</ref> was utilized during the fine-tuning, which acts on imagegrounded text to optimize the model's ability to generate coherent captions according to visual information.</p><p>Stage 1 -Concept-based Fine-tuning. In this stage, we used the concepts and captions of images to jointly optimize the model. We designated the loss between the output of Q-Former and image concepts as the concept loss, while the loss between the final output of OPT 2.7ùêµ and the image caption was termed the caption loss. Both of these losses were essentially language modeling losses, albeit with distinct optimization objectives.</p><p>‚Ä¢ Concept loss aims to encourage Q-Former to generate expressions that align closely with professional medical terminology, leveraging the representative features extracted from medical images. Since each image corresponds to specific concepts represented by Concept Unique Identifiers (CUIs), we initially map these CUIs to English and then organize them into language descriptions using the sentence structure: "The image shows</p><formula xml:id="formula_0" coords="4,116.56,220.86,215.84,10.91">[concept 1], [concept 2], ..., [concept n]</formula><p>." These descriptions contain valuable and accurate information. Our intention is to make the output of Q-Former as close as possible to these descriptions. ‚Ä¢ Caption loss calculates the difference between the final outputs of OPT 2.7ùêµ and the ground truth image caption, ensuring that Q-Former can generate informative and professional prompts while also allowing OPT 2.7ùêµ to generate accurate captions.</p><p>Stage 2 -Overall Fine-tuning. In this stage, the model was exclusively trained by minimizing the caption loss, prioritizing overall optimization during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Our framework was developed using PaddlePaddle<ref type="foot" coords="4,315.02,406.86,3.71,7.97" target="#foot_0">1</ref> version 2.4.2 and trained on 8 Ascend 910 NPUs. The adapter plug-in PaddleCustomDevice<ref type="foot" coords="4,307.26,420.41,3.71,7.97" target="#foot_1">2</ref> was utilized in order to be compatible with the Ascend NPU. The size of image input were resized to 364 √ó 364 and the batch size was 16 in both fine-tuning stage. The model was fine-tuned for 50 epochs and 20 epochs in the first and second stage respectively. We use the AdamW optimizer with the weight decay of 0.05. The initial learning rate was set to 10 -5 with a warm-up of 1000 steps to gradually adjust the learning rate.</p><p>In addition, based on the statistics depicted in Figure <ref type="figure" coords="4,339.98,503.46,3.78,10.91" target="#fig_2">2</ref>, the maximum output length of the model was set to 128. It is evident from the data that the majority of sentence lengths fall below this threshold. Our aim is to ensure the conciseness and adherence of the generated captions to the sentence length distribution in the dataset by setting an appropriate maximum output length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Post-processing</head><p>Generative language models frequently face the problem of degradation, specifically, the quality of the generated text gradually declines as its length increases. This degradation often leads </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Comparisons of some predicted captions before and after the post-processing. to the emergence of repetitive patterns at various levels, including characters, phrases, and sentences.</p><p>To address the problem of duplicate vocabularies in our model's output, we implemented a post-processing. Specifically, we provide a predicted caption which contains repeated words as a prompt for ChatGPT<ref type="foot" coords="5,192.36,581.19,3.71,7.97" target="#foot_2">3</ref> , requesting it to generate a Python code snippet capable of removing repetitive content. This generated code was then employed to perform text deduplication on all predicted captions. Following the post-processing step, the BERTScore of our results on validation data increased from 0.608 to 0.628. Table <ref type="table" coords="5,331.27,623.59,5.17,10.91">2</ref> demonstrates a comparison of some predicted captions before and after deduplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Ablation studies on validation data of caption prediction task. BLIP-2 ViT-g Q-Former in the first row indicates that this model is only composed of ViT-g and Q-Former, without LLM. ‚ãÜ represents the results of the model have been post-processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Image Size Trainable Params Total Params BERTScore ROUGE-1   </p><formula xml:id="formula_1" coords="6,97.13,161.71,59.90,8.20">BLIP-2 ViT-g Q-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Ablation Studies on Validation Set</head><p>Table <ref type="table" coords="6,116.12,498.40,5.14,10.91">3</ref> displays the results of ablation studies during our model selection process. We tested different model compositions, input image sizes, and the effect of post-processing via these studies. The result of BLIP-2 ViT-g OPT 2.7ùêµ after post-processing (the last row of Table <ref type="table" coords="6,477.77,525.49,4.12,10.91">3</ref>) was ultimately chosen and submitted.</p><p>Figure <ref type="figure" coords="6,130.05,552.59,4.97,10.91" target="#fig_5">3</ref> provides two examples of the validation data to visually demonstrate the performance of our chosen model. Both examples showcase the input medical images alongside their corresponding predicted captions provided by our model. In the two presented examples, our prediction results successfully recognized the modality of the medical images, but fell short in accurately detecting anomalies. In the first example, the description of "bilateral infiltrates" relatively matched the presence of "pulmonary congestion"; however, it failed to identify cardiac enlargement. The second example involved an incorrect diagnosis, misclassifying an abdominal CT as a chest CT. Although a sizable lesion was identified, its localization was inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The results of the top five teams for the caption prediction task in ImageCLEF 2023 <ref type="bibr" coords="7,427.07,102.49,12.21,8.87" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Results on Test Set</head><p>The ranking for the caption prediction task is determined based on the BERTScore. A complete list of all runs for the caption prediction are now available in the results folder <ref type="bibr" coords="7,442.01,238.43,13.18,10.91" target="#b1">[2]</ref> and on the official website <ref type="foot" coords="7,157.67,250.23,3.71,7.97" target="#foot_3">4</ref> . Table <ref type="table" coords="7,196.75,251.98,5.17,10.91">4</ref> displays the best runs' results of the top five teams. It shows that our team "closeAI2023" achieved a second-place ranking with a BERTScore of 0.6281, which is only 0.0144 lower than that of the first-ranked team. Among the seven listed metrics, we have surpassed the first-ranked team in four of them, i.e. BLEURT, BLEU, METEOR and CIDEr. Furthermore, our BLEURT and CIDEr metrics achieved top positions with scores of 0.3209 and 0.2377, respectively. These results demonstrate the consistent and strong performance of our method across various evaluation criteria in the competition. However, despite the commendable results attained by our model, Figure <ref type="figure" coords="7,317.36,346.82,5.00,10.91" target="#fig_5">3</ref> suggests that there still remains room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>This paper introduces the work of team "closeAI2023" in Caption Prediction Task of Image-CLEFmedical Caption 2023. The model we used was obtained through a two-stages fine-tuning based on BLIP-2 ViT-g OPT 2.7ùêµ . To eliminate the impact of duplicate statements, we also performed post-processing on the outputs of the model. Our team ultimately achieved second place in this task, with a BERTScore of 0.6281. This points out the effectiveness of our approach in generating high-quality captions for medical images. Codes and models will be open-sourced at OpenMedIA<ref type="foot" coords="7,155.08,498.54,3.71,7.97" target="#foot_4">5</ref>  <ref type="bibr" coords="7,159.29,500.30,16.25,10.91" target="#b12">[13]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,230.50,116.36,50.64,6.93;3,244.41,125.32,22.81,6.93;3,369.61,116.36,14.54,6.93;3,362.19,125.32,29.37,8.04;3,215.06,91.97,194.93,7.68;3,303.94,120.71,32.34,6.93;3,129.41,89.49,21.70,7.82;3,123.89,168.05,32.72,7.82;3,99.21,181.72,82.07,5.23;3,99.98,189.80,80.52,5.23;3,126.53,205.50,27.42,7.82;3,92.93,219.58,94.62,5.23;3,92.93,226.30,94.62,5.23;3,92.93,233.01,93.30,5.23;3,225.31,177.58,68.50,5.23;3,227.95,185.66,63.21,5.23;3,231.57,193.74,55.99,5.22;3,197.25,180.49,13.53,6.72;3,190.54,189.45,26.96,6.72;3,181.75,113.87,16.14,6.72;3,314.06,180.83,25.13,6.72;3,320.16,189.79,12.91,6.72;3,461.53,180.83,23.51,6.72;3,466.83,189.79,12.91,6.72;3,420.24,113.80,21.71,6.72;3,457.50,115.48,31.55,6.93;3,460.40,126.13,25.75,6.93;3,373.71,204.78,52.46,6.72;3,372.80,213.74,54.32,6.72;3,449.92,237.21,46.72,6.72;3,456.01,246.17,34.58,6.72;3,131.25,149.63,18.01,3.46;3,120.58,155.06,39.32,3.46"><head></head><label></label><figDesc>!.#$ ) BLIP-2 ViT-g OPT !.#$ Model for Image Captioning (Li et al., 2023) ray showing enlarged cardiac silhouette with cardiothoracic ratio of 70%, and mild pulmonary congestion. Plain x-ray; Chest; Postero-Anterior; Enlarged; Heart; et al. (2022)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,264.82,417.79,8.93;3,89.29,276.82,416.70,8.87;3,89.29,288.78,417.79,8.87;3,89.29,300.74,260.92,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:The training pipeline of our method. The image encoder and Q-Former are in yellow boxes, indicating that their parameters were updated during training, while the LLM in the green box had its parameters frozen. In concept-based fine-tuning stage, both concept loss and caption loss were utilized, but only caption loss was employed in overall fine-tuning stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,283.86,416.69,8.93;5,89.29,295.86,314.24,8.87;5,172.63,84.19,250.01,193.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The frequency histogram regarding the sentence length of captions in the ImageCLEFmedical Caption 2023 dataset. The horizontal axis represents the length of sentences.</figDesc><graphic coords="5,172.63,84.19,250.01,193.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,98.36,360.72,110.72,6.50;5,376.46,360.72,65.68,6.50;5,100.90,375.87,20.25,6.46;5,138.20,375.87,224.95,6.46;5,138.20,384.34,223.74,6.46;5,138.20,392.81,223.74,6.46;5,138.20,401.28,142.38,6.46;5,376.46,375.87,120.47,6.46;5,376.46,384.34,120.47,6.46;5,100.90,410.23,20.25,6.46;5,138.20,410.23,223.74,6.46;5,138.20,418.70,223.74,6.46;5,138.20,427.17,223.74,6.46;5,138.20,435.64,223.74,6.46;5,138.20,444.10,223.74,6.46;5,138.20,452.57,66.17,6.46;5,376.46,410.23,121.69,6.46;5,376.46,418.70,121.69,6.46;5,376.46,427.17,120.47,6.46;5,376.46,435.64,120.48,6.46;5,376.46,444.10,120.48,6.46;5,376.46,452.57,18.64,6.46;5,100.90,461.52,20.25,6.46;5,138.20,461.52,223.74,6.46;5,138.20,469.99,223.74,6.46;5,138.20,478.46,223.74,6.46;5,138.20,486.93,223.74,6.46;5,138.20,495.40,51.30,6.46;5,376.46,461.52,121.69,6.46;5,376.46,469.99,121.69,6.46;5,376.46,478.46,51.00,6.46"><head></head><label></label><figDesc>of a small aneurysm in the left atrial septum ao aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa aneurysm aoa Transthoracic echocardiography view of a small aneurysm in the left atrial septum.000047anteroposterior radiograph of the left ankle at the end of the first year of treatment showing the thickening of the tibia and fibula and the growth of the distal end of the tibia into the distal end of the fibula note the thickening of the distal end of the tibia into the distal end of the fibula and the growth of the distal end of the tibia into the distal end of the fibula at the end of the first year of treatment Anteroposterior radiograph of the left ankle at the end of the first year of treatment showing the thickening of the tibia and fibula and the growth of the distal end of the tibia into the distal end of the fibula. 000080 ultrasonographic view of the patellar tendon of the patellofemoral joint of a 4-year-old domestic dog the patella patellar tendon lv patellar ligament lp patellar tendon ap patella ap patellar ligament ap patella ap patella ap patella ap patella ap patella ap patella ap patella ap patella ap patella ap patella ap patella Ultrasonographic view of the patellar tendon of the patellofemoral joint of a 4-yearold domestic dog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,220.11,266.77,201.79,8.40;6,220.11,335.51,180.17,8.40;6,220.11,356.27,259.31,8.40;6,220.11,367.08,135.19,8.40;6,156.90,292.09,26.11,5.02;6,141.44,299.96,57.00,5.02;6,123.45,393.59,68.78,5.02"><head>Prediction:</head><label></label><figDesc>chest x-ray showing bilateral infiltrates Ground Truth: A giant retroperitoneal tumor. Prediction: computed tomography ct scan of the chest showing a large lesion in the thoracic aorta CC BY-NC [Al Mulhim et al. (2022)] CC BY [Tsutsui et al. (2021)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,89.29,409.92,416.70,8.93;6,89.29,421.93,19.33,8.87;6,110.64,319.39,94.51,70.32"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two examples of predicted results and ground truths in the validation set of caption prediction task.</figDesc><graphic coords="6,110.64,319.39,94.51,70.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,411.40,54.87"><head>Table 1</head><label>1</label><figDesc>Number of images per modality in ImageCLEFmedical Caption 2023 training dataset.</figDesc><table coords="2,94.88,120.11,405.52,25.25"><row><cell>Overall</cell><cell cols="7">Modalities X-Ray Plain X-Ray MRI Ultrasonography Angiogram PET PET/CT Radionuclide Imaging Radiographic Imaging Other</cell></row><row><cell>60,918</cell><cell>20,955 16,838</cell><cell>9,482 8,355</cell><cell>3,954</cell><cell>472 136</cell><cell>37</cell><cell>21</cell><cell>668</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,97.13,161.71,382.30,92.60"><head></head><label></label><figDesc>Chest X-ray showing enlarged cardiac silhouette with cardiothoracic ratio of 70%, and mild pulmonary congestion.</figDesc><table coords="6,97.13,161.71,381.85,81.79"><row><cell cols="3">Former 224</cell><cell>1.1B</cell><cell>1.1B</cell><cell>0.541</cell><cell>0.160</cell></row><row><cell>BLIP-2 ViT-g OPT 2.7ùêµ</cell><cell></cell><cell>224</cell><cell>1.1B</cell><cell>3.8B</cell><cell>0.593</cell><cell>0.249</cell></row><row><cell>BLIP-2 ViT-g OPT 2.7ùêµ</cell><cell></cell><cell>364</cell><cell>1.1B</cell><cell>3.8B</cell><cell>0.608</cell><cell>0.255</cell></row><row><cell>BLIP-2 ViT-g OPT 2.7ùêµ</cell><cell>‚ãÜ</cell><cell>364</cell><cell>1.1B</cell><cell>3.8B</cell><cell>0.628</cell><cell>0.253</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ground Truth:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,660.04,146.52,8.97"><p>https://github.com/PaddlePaddle/Paddle</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,92.57,671.00,199.97,8.97"><p>https://github.com/PaddlePaddle/PaddleCustomDevice</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,92.57,671.01,117.74,8.97"><p>https://openai.com/blog/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,92.57,659.94,178.84,8.97"><p>https://www.imageclef.org/2023/medical/caption</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,92.57,670.90,128.35,8.97"><p>https://openi.pcl.ac.cn/OpenMedIA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The computing resources of <rs type="institution">Pengcheng Laboratory Cloudbrain</rs> II are used in this research. We acknowledge the support provided by <rs type="funder">OpenI Community</rs> (https://git.openi.org.cn).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,394.53,10.91;8,112.66,124.83,395.17,10.91;8,112.66,138.38,394.53,10.91;8,112.66,151.93,395.17,10.91;8,112.39,165.48,394.80,10.91;8,112.48,179.03,394.70,10.91;8,112.66,192.57,395.17,10.91;8,112.66,206.12,393.32,10.91;8,112.66,219.67,394.52,10.91;8,112.33,233.22,120.27,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,224.93,179.03,282.25,10.91;8,112.66,192.57,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.94,192.57,143.89,10.91;8,112.66,206.12,393.32,10.91;8,112.66,219.67,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,280.09,219.67,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,246.77,394.52,10.91;8,112.66,260.32,395.17,10.91;8,112.66,273.87,393.33,10.91;8,112.66,287.42,247.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,298.72,260.32,209.11,10.91;8,112.66,273.87,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.89,273.87,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="8,429.41,273.87,76.58,10.91;8,112.66,287.42,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,300.97,393.33,10.91;8,112.66,314.52,271.84,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m" coord="8,384.35,300.97,121.63,10.91;8,112.66,314.52,90.07,10.91">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,328.07,393.33,10.91;8,112.66,341.62,133.03,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,154.48,328.07,243.31,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,419.57,328.07,86.42,10.91;8,112.66,341.62,55.58,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,355.17,393.33,10.91;8,112.33,368.71,393.65,10.91;8,112.66,382.26,394.62,10.91;8,112.41,395.81,394.78,10.91;8,112.66,409.36,394.53,10.91;8,112.66,422.91,215.79,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,375.93,355.17,130.06,10.91;8,112.33,368.71,157.87,10.91">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,295.73,368.71,210.25,10.91;8,112.66,382.26,394.62,10.91;8,112.41,395.81,389.37,10.91;8,179.72,409.36,180.34,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="8,112.66,436.46,393.33,10.91;8,112.66,450.01,395.00,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<title level="m" coord="8,251.74,436.46,254.25,10.91;8,112.66,450.01,215.65,10.91">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,463.56,393.33,10.91;8,112.39,477.11,393.60,10.91;8,112.66,490.66,178.25,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,239.04,463.56,266.95,10.91;8,112.39,477.11,204.61,10.91">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,339.99,477.11,165.99,10.91;8,112.66,490.66,37.61,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,504.21,395.17,10.91;8,112.66,517.76,393.32,10.91;8,112.66,531.30,107.17,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m" coord="8,227.28,517.76,205.78,10.91">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,544.85,394.62,10.91;8,112.66,558.40,393.33,10.91;8,112.66,571.95,395.01,10.91;8,112.41,585.50,59.11,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,112.66,558.40,314.18,10.91">Exploring the limits of masked visual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eva</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,450.78,558.40,55.20,10.91;8,112.66,571.95,344.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19358" to="19369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,599.05,394.53,10.91;8,112.30,612.60,393.68,10.91;8,112.66,626.15,107.17,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m" coord="8,189.92,612.60,238.75,10.91">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,639.70,393.33,10.91;8,112.66,653.25,394.53,10.91;8,112.66,666.80,103.61,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,346.64,639.70,159.35,10.91;8,112.66,653.25,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.08,653.25,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,394.53,10.91;9,112.66,100.52,393.33,10.91;9,112.66,114.06,394.52,10.91;9,112.66,127.61,123.33,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,112.66,100.52,200.81,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,341.73,100.52,164.26,10.91;9,112.66,114.06,93.50,10.91">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,395.17,10.91;9,112.66,154.71,393.32,10.91;9,112.66,168.26,395.16,10.91;9,112.66,181.81,394.52,10.91;9,112.66,195.36,80.57,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,479.40,141.16,28.43,10.91;9,112.66,154.71,393.32,10.91;9,112.66,168.26,101.86,10.91">Openmedia: Open-source medical image analysis toolbox and benchmark under heterogeneous ai computing platforms</title>
		<author>
			<persName coords=""><forename type="first">J.-X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,237.55,168.26,270.28,10.91;9,112.66,181.81,84.42,10.91">Pattern Recognition and Computer Vision: 5th Chinese Conference, PRCV 2022</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">November 4-7, 2022. 2022</date>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
	<note>Part I</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
