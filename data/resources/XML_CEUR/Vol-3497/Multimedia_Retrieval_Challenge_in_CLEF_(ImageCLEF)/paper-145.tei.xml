<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,417.65,15.42;1,89.29,106.66,279.96,15.42;1,89.29,129.00,244.49,11.96">Adapting Pre-Trained Visual and Language Models for Medical Image Question Answering Notebook for the Baidu Intelligent Health Unit and</title>
				<funder>
					<orgName type="full">OpenI Community</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,336.76,129.00,58.08,11.96"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName coords="1,89.29,168.85,49.85,11.96"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,151.79,168.85,74.72,11.96"><forename type="first">Wenshuo</forename><surname>Zhou</surname></persName>
							<email>ws.zhou@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.15,168.85,54.46,11.96"><forename type="first">Yehui</forename><surname>Yang</surname></persName>
							<email>yangyehuisw@126.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.25,168.85,74.92,11.96"><forename type="first">Haifeng</forename><surname>Huang</surname></persName>
							<email>huanghaifeng@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.82,168.85,43.76,11.96"><forename type="first">Zhiyu</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.93,182.79,59.43,11.96"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>zhangt02@pcl.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.35,182.79,50.41,11.96"><forename type="first">Dalu</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Intelligent Health Unit</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory Joint Team at CLEF 2023</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,417.65,15.42;1,89.29,106.66,279.96,15.42;1,89.29,129.00,244.49,11.96">Adapting Pre-Trained Visual and Language Models for Medical Image Question Answering Notebook for the Baidu Intelligent Health Unit and</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">ED5C0E9761346146038DFDAE6995F83C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Visual Question Answering</term>
					<term>Blip-2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work carried out by the "wsq4747" team in the ImageCLEFmedical2023 title for the visual Question Answering subtask. Medical image question answering presents unique challenges due to the specialized nature of the medical field. Not only does it require the model to generate accurate and coherent answers through the image and the question, but it also needs to capture the basic medical information conveyed by the image. In order to leverage the capabilities of pre-trained large image models, we utilized the state-of-the-art BLIP-2 combined with a giant visual transformer (vit-g) and an open pre-training transformer language model (GLM-6B) as the foundation for our title prediction subtask. To adapt this model to the medical field, we employed a two-stage fine-tuning process. During the entire training process, the pre-trained GLM-6B was kept fixed, and step-by-step fine-tuning was applied to the vit-g and Q-Former modules to better align with the features of medical data. Our team's approach produced promising results with an accuracy(ACC) of 0.7396, as our method achieved an ACC of over 0.8 on 6 questions, an ACC of over 0.7 on 10 questions, and an ACC of around 0.1 on two questions (due to our oversight)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,134.62,495.15,15.11,10.91" target="#b0">[1]</ref>, short for Image Retrieval Evaluation Campaign, is a component of CLEF (Cross-Language Evaluation Forum), a European project in the fields of computer science and information retrieval that organizes various research challenges annually to evaluate the performance of multilingual information retrieval. The objective of ImageCLEF is to propel the advancement of computer vision and multimedia information retrieval technologies.The task encompasses both natural language processing and image recognition. It provides a query, which could be a question, a statement, or a description in another form, and then requires the system to find images relevant to the query from a vast image library.</p><p>The challenges presented in ImageCLEF comprise of multiple subtasks, including Visual Question Answering (VQA) <ref type="bibr" coords="2,212.49,141.16,15.60,10.91" target="#b1">[2]</ref> and medical image description. VQA is a multimodal task in the field of artificial intelligence, with the objective of developing models that can interpret visual content such as images or videos, and provide responses to corresponding natural language queries. This task becomes significantly more complex when applied to intricate scenarios in medical imaging. In the medical image question-answering task, we focused on colonoscopy images. Colonoscopy images are inherently complex and high-dimensional, with intricate relationships between visual features and medical semantics. Effectively modeling these relationships presents a significant challenge. Additionally, the language used in medical queries is often laden with complex medical jargon, necessitating a deep understanding of medical concepts that may not be encapsulated in general language models. Furthermore, procuring large-scale annotated data for training such models poses a difficulty due to privacy concerns and the requirement for expert annotations. Nevertheless, despite these challenges, the potential of medical VQA is considerable. Ongoing research continues to push the boundaries of our capabilities in this crucial area. In this work, our team primarily focuses on the task of medical image question-answering.</p><p>Some methods freeze the image encoder, including the early work which adopts a frozen object detector to extract visual features <ref type="bibr" coords="2,274.57,357.95,11.48,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,288.89,357.95,7.52,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,299.25,357.95,7.65,10.91" target="#b4">5]</ref>, and the recent LiT <ref type="bibr" coords="2,400.10,357.95,12.99,10.91" target="#b5">[6]</ref> which uses a frozen pre-trained image encoder for CLIP <ref type="bibr" coords="2,240.76,371.50,14.66,10.91" target="#b6">[7]</ref> pre-training. Some methods freeze the language model to use the knowledge from LLMs for vision-to-language generation tasks <ref type="bibr" coords="2,398.38,385.05,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,412.23,385.05,7.49,10.91" target="#b8">9]</ref>. The key challenge in using a frozen LLM is to align visual features to the text space. To achieve this, Frozen <ref type="bibr" coords="2,491.39,398.60,14.59,10.91" target="#b7">[8]</ref> finetunes an image encoder whose outputs are directly used as soft prompts for the LLM In this task, we employed BLIP-2 <ref type="bibr" coords="2,248.54,425.70,18.61,10.91" target="#b9">[10]</ref>. BLIP-2 is a recently proposed vision-language pretraining method by Li et al <ref type="bibr" coords="2,216.11,439.25,16.31,10.91" target="#b9">[10]</ref>. Blip-2 is a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models(LLM). BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages.building upon their previous work of BLIP <ref type="bibr" coords="2,119.53,493.44,18.59,10.91" target="#b9">[10]</ref>, and it has demonstrated superior performance compared to various other visionlanguage pre-training methods, including Flamingo <ref type="bibr" coords="2,278.27,520.54,18.77,10.91" target="#b10">[11]</ref>, across a range of vision-language tasks such as visual question answering, image captioning, and image-text retrieval. In this paper, our method is specifically introduced in Section 2, the experiments, data, and results are demonstrated in Section 3 and a brief summary is given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architechture</head><p>BLIP-2 <ref type="bibr" coords="2,118.43,640.78,19.42,10.91" target="#b9">[10]</ref> is a sophisticated framework designed for vision-to-language tasks, comprised of three main components: an image encoder, a Query Transformer (Q-Former), and a LLM.As shown in Figure <ref type="figure" coords="3,164.79,485.85,3.77,10.91" target="#fig_0">1</ref>. The Q-Former as the trainable module to bridge the gap between a frozen image encoder and a frozen LLM.</p><p>During the pre-training generation phase, we connected the Q-Former, equipped with a frozen image encoder, to the frozen Language Model (LLM) to leverage its language generation capacity. As depicted in Figure <ref type="figure" coords="3,184.67,540.04,3.67,10.91" target="#fig_0">1</ref>, we employed a fully connected (FC) layer to linearly project the output query embedding into the same dimensionality as the LLM's text embeddings. This projected query embedding was then added prior to the input text embeddings. Acting as soft visual prompts, they placed the LLM upon the visual representation extracted by the Q-Former. Given that the Q-Former had been pre-trained to extract visual representations carrying language information, it effectively played the role of an information bottleneck, offering the most useful details to the LLM while discarding irrelevant visual data. This mitigated the burden of learning visual-language alignment on the LLM, thereby alleviating the issue of catastrophic forgetting.</p><p>In response to our task, we employed the BLIP-2 model for visual question answering and selected vite-g/14 from EVA-CLIP <ref type="bibr" coords="3,238.59,661.99,21.26,10.91" target="#b11">[12]</ref> as our image encoder. For the LLM, we selected GLM- 6b <ref type="bibr" coords="4,98.20,210.98,17.82,10.91" target="#b12">[13]</ref>, a prefix-based, decoder-only model, to serve as our language language model (LLM). .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset</head><p>The dataset encompasses images spanning the entirety of the gastrointestinal tract, from the mouth to the anus. It encapsulates various instances, including abnormalities, surgical instruments, and normal findings. The images are procured from different procedures such as gastroscopy, colonoscopy, and capsule endoscopy. The distribution of image sizes within our dataset is illustrated in Table <ref type="table" coords="4,220.27,328.45,3.75,10.91" target="#tab_0">1</ref>, exhibiting a broad array of dimensions. A minimal fraction of the images, exactly 21, have dimensions less than 500 pixels. The bulk of our images, amounting to 1441, belong to the medium size category with pixel dimensions ranging from 500 to 1000. Lastly, a significant subset of our data, constituting 538 images, features dimensions exceeding 1000 pixels. This heterogeneity in image size amplifies the diversity and intricacy of our dataset, thereby increasing the challenge and comprehensiveness of the Visual Question Answering task at hand. For both Task 1 (VQA) and Task 2 (Visual Question Generation, VQG), a minimum of 2000 image samples have been provided, each accompanied by eighteen question-and-answer pairs. It should be noted that not all questions are pertinent to the corresponding image. In Task1, since the data did not divide the training set and the validation set, we randomly selected 10% of the image-text question-answer pairs as the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Strategy</head><p>The training protocol for BLIP-2 is carried out in two distinct stages. In the first stage, a process known as vision-language representation learning, the image encoder and language models are frozen, allowing the model to tap into its inherent image understanding capabilities. The second stage involves vision-to-language generative learning, where the LLM is frozen, maintaining its existing text generation capabilities. When applying the BLIP-2 model to downstream tasks, such as visual question answering, the LLM is kept frozen during the fine-tuning phase. Meanwhile, the parameters of the image encoder and Q-Former are updated.</p><p>Throughout these two stages, the language models are kept frozen to preserve their initial functionalities. In contrast, the Q-Former is exclusively trained during this pre-training phase. The role of the Q-Former is to effectively extract visual representations that align with the corresponding textual information and to relay this information to the LLM. This focused training approach allows BLIP-2 to achieve a higher level of correspondence between visual and textual data.</p><p>In our pre-training phase, we initialized our large-scale visual transformer (vit-g) and Query Transformer (Q-Former) with weights from BLIP-2, which had been previously pre-trained on the ImageNet <ref type="bibr" coords="5,161.97,323.36,20.62,10.91" target="#b13">[14]</ref> and COCO <ref type="bibr" coords="5,228.65,323.36,23.96,10.91" target="#b14">[15]</ref> datasets. However, our specific task focused on medical imaging (endoscopic images) for the visual question answering task. It's worth noting that there is a significant domain shift between natural images and medical imaging data.</p><p>In order to address this issue and to allow the visual encoder to extract image features more delicately, we adopted a fine-tuning strategy. During this fine-tuning process, the parameters of the LLM(GLM-6B) were kept frozen, while the vit-g and Q-Former were trained concurrently. This strategy was designed to leverage the powerful visual representation capabilities of vit-g and Q-Former, while also accommodating the specific characteristics and challenges of the medical imaging domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Our framework was developed using PaddlePaddle1 version 2.4.2 and trained on 8 Ascend 910 NPUs. The adapter plug-in PaddleCustomDevice2 was utilized in order to be compatible with the Ascend NPU. The entire process, encompassing two training stages, spanned a total duration of four days. The input image size was set to 224 × 224, and the batch size was fixed at 16 for both fine-tuning stages. The model underwent fine-tuning for 100 epochs in the first stage, and 50 epochs in the second stage. Optimization was performed using an AdamW optimizer with a weight decay of 10 -4 . The initial learning rate was set to 10 -4 and was incrementally adjusted through a 1000-step warm-up phase. Additionally, we set the maximum output length of our model to 10, as the majority of answers from the data clearly fell below this threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Settings</head><p>In the endoscopic dataset, since there is no predefined division between training and validation sets, we conducted a manual split to validate the performance of our model. Specifically, 10% of the data, equating to 200 images with corresponding 3200 questions, was earmarked as the validation set. Beyond this, we employed accuracy as our primary evaluation metric to gauge the model's effectiveness in predicting the correct responses.As demonstrated in Table <ref type="table" coords="6,482.50,477.03,3.80,10.91">3</ref>, we have evaluated our model performance on the validation set that was manually partitioned by us. For the final submission, however, we leveraged the entire dataset for fine-tuning the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on Validation Data</head><p>Table <ref type="table" coords="6,116.11,553.85,5.14,10.91" target="#tab_1">2</ref> presents a comparative overview of the results from the ablation study conducted on BLIP-2. It is evident that as the model parameters progressively increase, the performance also improves. Moreover, fine-tuning only the LLM(GLM-6b) yields better results compared to freezing both the Image Encoder (vit-g) and the LLM simultaneously. For the LLM, we compared FlanT5-3B <ref type="bibr" coords="6,132.62,608.05,19.26,10.91" target="#b15">[16]</ref>, based on an encoder-decoder structure, and GLM-6b <ref type="bibr" coords="6,397.16,608.05,21.01,10.91" target="#b12">[13]</ref>, based on a prefix decoder-only structure. The results showed that GLM-6b demonstrated superior performance in zero-shot images, thus we adopted GLM-6B as our LLM. However, due to time constraints, we ultimately opted for the model with an accuracy of 0.9105 as the final choice for our backbone model. The plot compares the accuracy of models with different parameter sizes, under conditions where the models were kept frozen (blue line) and where the models were allowed to unfreeze (orange line). The trend illustrates that larger models tend to perform better, and unfreezing the models generally leads to higher accuracy.</p><p>In Figure <ref type="figure" coords="7,141.79,509.66,3.66,10.91" target="#fig_1">2</ref>, we provide an illustrative comparison between the performances of models under different parameter sizes, and the conditions whether the models were kept frozen or allowed to unfreeze. As shown in figure <ref type="figure" coords="7,236.92,536.76,3.81,10.91" target="#fig_1">2</ref>, there is an observable trend that larger models generally outperform the smaller ones, suggesting that the number of parameters plays a significant role in the accuracy of the VQA task. Furthermore, it is also evident that when the models are unfrozen, allowing the parameters to adjust during training, the accuracy increases across all model sizes. This underlines the importance of parameter fine-tuning in optimizing model performance in the context of medical VQA tasks.</p><p>Additionally, we analyzed the accuracy rates for individual questions, as shown in Table <ref type="table" coords="7,500.10,618.05,3.78,10.91">3</ref>. From the validation set, it was apparent that the model had poor performance in predicting the size and type of polyps. We summarized the answers to these two questions in Table <ref type="table" coords="7,500.04,645.15,3.81,10.91">4</ref>. To address these shortcomings, for the question What is the size of the polyp?, we trained a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results on Test Data</head><p>Our submission results, as depicted in Table <ref type="table" coords="8,292.90,487.70,3.81,10.91">5</ref>, were not as impressive as anticipated on the test set, yielding an accuracy of 0.7396. On six questions, the accuracy exceeded 0.8, while on ten questions, it surpassed 0.7. However, the accuracy was around 0.1 for the questions What color is the abnormality? and Where in the image is the abnormality?, significantly dragging down the overall average. We speculate that this may be due to the visual encoder's inability to extract detailed features related to color and position. As illustrated in Figure <ref type="figure" coords="8,447.10,555.45,3.74,10.91">3</ref>, we present one example of the predicted results obtained from the validation set for the answer prediction task. This figure visually demonstrates how our model performs in terms of predicting answers, offering an insight into the capabilities of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>This paper has presented the work of the "wsq4747" team in the Visual Question Answering task for imageCLEFmedical VQA 2023. The model we utilized, which is a variant of BLIP-2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,424.34,416.69,8.93;3,89.29,436.34,416.70,8.87;3,89.29,448.30,117.31,8.87;3,89.29,84.19,416.69,332.72"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: In our method, the first stage of BLIP-2 is a bootstrapping process that involves freezing the image encoder and LLM and BLIP-2's second-stage vision-to-language generative pre-training, which bootstraps from frozen LLM.</figDesc><graphic coords="3,89.29,84.19,416.69,332.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,424.24,416.69,8.93;7,89.29,436.24,416.69,8.87;7,89.29,448.20,416.69,8.87;7,88.99,460.15,417.17,8.87;7,89.29,472.11,381.00,8.87;7,89.29,84.19,416.70,332.62"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy vs Model Parameters for both frozen and unfrozen models. This figure demonstrates the impact of the number of model parameters on the accuracy of the VQA task.The plot compares the accuracy of models with different parameter sizes, under conditions where the models were kept frozen (blue line) and where the models were allowed to unfreeze (orange line). The trend illustrates that larger models tend to perform better, and unfreezing the models generally leads to higher accuracy.</figDesc><graphic coords="7,89.29,84.19,416.70,332.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,254.05,369.88,8.93;8,89.29,84.19,416.70,157.29"><head>Figure 3 : 4</head><label>34</label><figDesc>Figure 3: One example of predicted results in the validation set of answer prediction task.</figDesc><graphic coords="8,89.29,84.19,416.70,157.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,416.99,93.78"><head>Table 1</head><label>1</label><figDesc>Distribution of image sizes in our dataset. The table shows the number of images for three different size categories: less than 500, between 500 and 1000, and greater than 1000.</figDesc><table coords="4,242.18,134.06,110.91,50.21"><row><cell>image size</cell><cell>amount</cell></row><row><cell>&lt;500</cell><cell>21</cell></row><row><cell>500&lt;image&lt;1000</cell><cell>1441</cell></row><row><cell>&gt;1000</cell><cell>538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.93,90.49,417.05,165.51"><head>Table 2</head><label>2</label><figDesc>In our model ablation study on the validation set, due to time constraints, we only employed the version with an accuracy of 0.9105 for comparison. * denote instances where model parameters were frozen during training.</figDesc><table coords="5,151.27,146.01,292.73,109.99"><row><cell>Models</cell><cell cols="2">model parameters accuracy</cell></row><row><cell>vit-l * +Q-former+t5-base *</cell><cell>715M</cell><cell>0.5227</cell></row><row><cell>vit-l+Q-former+t5-base *</cell><cell>715M</cell><cell>0.6048</cell></row><row><cell>vit-g * +Q-former+t5-base *</cell><cell>1.4B</cell><cell>0.6017</cell></row><row><cell>vit-g+Q-former+t5-base *</cell><cell>1.4B</cell><cell>0.7577</cell></row><row><cell>vit-g * +Q-former+t5-3b *</cell><cell>4.2B</cell><cell>0.7427</cell></row><row><cell>vit-g+Q-former+t5-3b *</cell><cell>4.2B</cell><cell>0.8324</cell></row><row><cell>vit-g * +Q-former+GLM-6b *</cell><cell>7.2B</cell><cell>0.9105(submitted)</cell></row><row><cell>vit-g+Q-former+GLM-6b *</cell><cell>7.2B</cell><cell>0.9317(unsubmitted)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The computing resources of <rs type="institution">Pengcheng Laboratory Cloudbrain</rs> II are used in this research. We acknowledge the support provided by <rs type="funder">OpenI Community</rs> (https://git.openi.org.cn).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>vit-g GLM-6b, underwent two stages of fine-tuning. Our team's final accuracy was 0.7396, demonstrating the effectiveness of our approach in generating high-quality question-answering for medical images.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,560.56,394.53,10.91;9,112.66,574.11,395.17,10.91;9,112.66,587.66,394.53,10.91;9,112.66,601.20,394.52,10.91;9,112.66,614.75,393.72,10.91;9,112.66,628.30,395.17,10.91;9,112.66,641.85,109.71,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcıa Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J A A A R I C V K A S G I</forename><surname>Nikolaos Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<title level="m" coord="9,463.73,614.75,42.66,10.91;9,112.66,628.30,395.17,10.91;9,112.66,641.85,77.79,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,655.40,393.33,10.91;9,112.66,668.95,328.36,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H T D L M A R V T</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<title level="m" coord="9,372.31,655.40,133.68,10.91;9,112.66,668.95,296.44,10.91">Overview of imageclefmedical 2023 -medical visual question answering for gastrointestinal tract</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,394.62,10.91;10,112.66,100.52,394.04,10.91;10,112.66,114.06,279.36,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,475.62,86.97,31.66,10.91;10,112.66,100.52,211.33,10.91">Uniter: Universal image-text representation learning</title>
		<author>
			<persName coords=""><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58577-8_7</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-58577-8_7.doi:10.1007/978-3-030-58577-8_7" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.53,10.91;10,112.66,141.16,395.17,10.91;10,112.66,154.71,97.52,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corporation</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Embeddings</surname></persName>
		</author>
		<title level="m" coord="10,261.00,141.16,246.83,10.91;10,112.66,154.71,65.60,10.91">Oscar: Object-semantics aligned pre-training for visionlanguage tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.32,10.91;10,112.66,181.81,252.78,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m" coord="10,411.50,168.26,94.49,10.91;10,112.66,181.81,220.87,10.91">Vinvl: Making visual representations matter in vision-language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,395.00,10.91;10,112.66,222.46,238.30,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,445.71,195.36,60.28,10.91;10,112.66,208.91,173.85,10.91">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01759</idno>
		<ptr target="http://dx.doi.org/10.1109/cvpr52688.2022.01759.doi:10.1109/cvpr52688.2022.01759" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.53,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,284.26,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,318.79,249.56,187.19,10.91;10,112.66,263.11,127.62,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Cornell University -arXiv</note>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.33,10.91;10,112.66,290.20,383.04,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,410.39,276.66,95.60,10.91;10,112.66,290.20,166.22,10.91">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,287.68,290.20,176.09,10.91">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,220.95,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<title level="m" coord="10,292.02,303.75,213.97,10.91;10,112.66,317.30,189.03,10.91">Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,393.33,10.91;10,112.66,344.40,253.92,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<title level="m" coord="10,251.74,330.85,254.25,10.91;10,112.66,344.40,222.00,10.91">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,395.17,10.91;10,112.66,371.50,395.17,10.91;10,112.66,385.05,394.53,10.91;10,112.66,398.60,393.53,10.91;10,112.66,412.15,110.77,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<title level="m" coord="10,339.39,398.60,166.80,10.91;10,112.66,412.15,78.85,10.91">Flamingo: a visual language model for few-shot learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,425.70,394.62,10.91;10,112.66,439.25,340.48,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<title level="m" coord="10,487.84,425.70,19.44,10.91;10,112.66,439.25,308.56,10.91">Eva: Exploring the limits of masked visual representation learning at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,394.52,10.91;10,112.66,466.34,392.26,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m" coord="10,112.66,466.34,209.56,10.91">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.33,10.91;10,112.66,493.44,394.51,10.91;10,112.66,509.43,103.29,7.90" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,346.64,479.89,159.35,10.91;10,112.66,493.44,69.17,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
		<idno>doi:</idno>
		<ptr target="10.1109/cvpr.2009.5206848" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,520.54,394.53,10.91;10,112.66,534.09,394.03,10.91;10,112.66,547.64,290.37,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,112.66,534.09,188.72,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-10602-1_48.doi:10.1007/978-3-319-10602-1_48" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,561.19,394.53,10.91;10,112.66,574.74,394.53,10.91;10,112.66,588.29,394.53,10.91;10,112.28,601.84,393.05,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<title level="m" coord="10,265.90,601.84,207.51,10.91">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
