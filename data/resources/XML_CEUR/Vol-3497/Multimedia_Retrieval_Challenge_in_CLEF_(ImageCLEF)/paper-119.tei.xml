<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,363.06,15.42;1,88.78,106.66,366.15,15.42;1,89.29,128.58,106.29,15.43">StellEllaStars at MEDIQA-Sum 2023: Exploring Transformer-Based models for Dialogue2Topic Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,75.15,11.96"><forename type="first">Chi-Yun</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.07,156.89,34.35,11.96"><forename type="first">Jiaqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.07,156.89,78.88,11.96"><forename type="first">Shivangi</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.94,156.89,113.73,11.96"><forename type="first">V</forename><forename type="middle">G Vinod</forename><surname>Vydiswaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,363.06,15.42;1,88.78,106.66,366.15,15.42;1,89.29,128.58,106.29,15.43">StellEllaStars at MEDIQA-Sum 2023: Exploring Transformer-Based models for Dialogue2Topic Classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6092461F0E8C8BC6D5F4AA7BE2C4F66C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural language processing</term>
					<term>Text classification</term>
					<term>Deep learning</term>
					<term>Clinical notes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic labeling of clinical notes is an essential task in Clinical Natural Language Processing. In this study, we explore deep neural network-based classification approaches to assign predetermined topic categories to conversation snippets between doctors and patients. Our proposed models 1 include transformer-based models like BERT, traditional machine learning models like Logistic Regression, and deep learning models such as Continuous Bag-of-Words (CBoW). To address the issue of the lack of sufficient data due to a small training dataset, we incorporate oversampling techniques into our methods. Our proposed approaches perform fairly well on the MEDIQA-Sum 2023 shared task on classifying doctor-patient dialogues into topic categories, with our best run which leverages the ClinicalBERT model achieving an accuracy score of 0.765 on the MEDIQA-Sum test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Medical conversations between doctors and patients contain critical clinical information. Developing natural language processing (NLP) approaches that understand these interactions could facilitate numerous applications, such as clinical decision support and patient-centered health information systems. However, accurately identifying the conversation topics is challenging due to the complex and domain-specific nature of medical dialogues. As part of our participation in the Dialogue2Topic Classification subtask (Subtask A) of MEDIQA-Sum 2023 <ref type="bibr" coords="1,439.20,480.20,11.30,10.91" target="#b0">[1]</ref>, we focused on exploring deep neural network models to identify the most appropriate clinical note section headers for the dialogue snippets, classifying dialogue snippets into one of twenty predefined section labels such as Assessment, Diagnosis, Exam, Medications, and Past Medical History.</p><p>Previous studies have explored various machine learning models, especially deep neuralnetwork based models, for similar classification tasks. Li et al. <ref type="bibr" coords="1,375.94,547.95,13.00,10.91" target="#b1">[2]</ref> focused on automatically classifying different sections within clinical notes using a supervised Hidden Markov Model (HMM) based on section content and structure. The evaluation on clinical notes from MIMIC-II showed promising results outperforming the other methods with 93% accuracy for identifying individual section headers and 70% accuracy for identifying all section headers in a note. Although this work made significant contributions in benchmarking "traditional" machine learning approaches, it highlighted the generalizability concerns of training models on a relatively small dataset derived over ICU notes from the MIMIC-II.</p><p>In recent years, the focus of evaluation has been on deep neural network models. Nair et al. <ref type="bibr" coords="2,493.31,154.71,12.68,10.91" target="#b2">[3]</ref> identified different sections within clinical notes using transfer learning techniques to leverage pre-trained language models. The proposed methodology involved fine-tuning a pre-trained transformer-based language model, Bidirectional Encoder Representations from Transformers (BERT). Experiments were conducted on two publicly available clinical note datasets, MIMIC-III and i2b2-2010, and compared BERT models against rule-based and "traditional" machine learning models. BERT models achieved the best accuracy of 87% on this task. Similarly, Qing et al. <ref type="bibr" coords="2,113.69,249.56,12.80,10.91" target="#b3">[4]</ref> presented a novel neural network-based approach focused on capturing the semantic representations and contextual data included in medical text. Their approach combined a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) network. The LSTM network collected long-range dependencies and contextual data, whereas CNN extracted local features from the input medical text. The authors tested their methodology using a medical text dataset with a variety of categories, including disease diagnosis and treatment, and achieved a relative improvement of 12.47% on one of the datasets with CNN over other approaches. Aiming to make use of a larger amount of data, Wang et al. <ref type="bibr" coords="2,323.69,344.40,13.00,10.91" target="#b4">[5]</ref> proposed a clinical text classification model that combines weak supervision with deep representation. Their methodology fed weakly labeled data in to a Hierarchical Attention Network (HAN) that captured the hierarchical structure and semantic representations of clinical writing. The evaluation on a large collection of clinical notes from Mayo Clinic showed that the weakly supervised HAN model was able to achieve competitive performance in clinical text categorization.</p><p>In other related work, Schloss and Konam <ref type="bibr" coords="2,290.02,425.70,12.87,10.91" target="#b5">[6]</ref> proposed medical dialogue modeling to automate the creation of clinical notes by using automatic speech recognition (ASR) to transcribe medical conversations and then implementing deep learning architectures for SOAP (Subjective, Objective, Assessment, Plan) section classification. This study shows the potential for streamlining clinical documentation procedures in healthcare settings and further underlines the significance of context-aware dialogue systems.</p><p>Building on these prior work, we investigated numerous machine learning models for Dia-logue2Topic classification. Our ensemble of models included ClinicalBERT, a transformer-based model pre-trained on clinical text, traditional machine learning models, and deep learning models such as CNN, LSTM, and Continuous Bag-of-Words. We addressed the inherent class imbalance in the training dataset by oversampling the minority class to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data preprocessing</head><p>As part of the shared task, the MEDIQA-Sum task organizers released two datasets, a training dataset consisting of 1,201 rows and a validation dataset consisting of 100 rows <ref type="bibr" coords="2,444.21,654.33,11.44,10.91" target="#b0">[1]</ref>. Each data instance is composed of three columns: ID, section header (the "topic"), and the dialogue snippet. As shown in Figure <ref type="figure" coords="3,183.96,337.71,3.81,10.91" target="#fig_0">1</ref>, the training dataset is imbalanced, with FAM/SOCHX and GENHX sections being the most frequent, with more than 250 instances each, while the smallest ten classes has less than 20 instances each. During the test phase, the organizers released a test set consisting of 200 instances with the section header column absent.</p><p>We took the following preprocessing steps to clean the dataset for the classification task:</p><p>1. Removing punctuation, digits, and special characters: We eliminated all nonalphabetic characters. This included punctuation marks such as periods, commas, semicolons, question marks, and exclamation points, among others as listed next: "@", "#", "$", "%", "ˆ", "&amp;", "*", "(", ")", "-", "+", "=", "[", "]", "{", "}", "|", ":", "'", """, "&lt;", "&gt;", "/", and "?". Numerical digits ranging from zero to nine were also excluded. 2. Removing dialogue markers: Specific dialogue markers such as "Doctor:", "Patient:", and timestamps were removed. This step was taken to ensure the data's uniformity and to prevent these recurring strings from skewing the analysis. 3. Removing stop words: Stop words, which typically include commonly used English words, such as "and", "the", "is", etc., were removed since they do not add semantic value to the text. We utilized the stop words list from the nltk.corpus package. 4. Converting text to lowercase: All text data was converted to lowercase. This step is essential to guarantee consistency and to prevent different token representations for the same word based on case differences, as "Allergy" and "allergy" would be treated as distinct entities without this conversion. 5. Tokenizing text: The text data was split into individual words or "tokens" by identifying word boundaries such as spaces, commas and full stops between adjacent words. 6. Lemmatization: Finally, words were reduced to their base or root form (e.g., "cancers" to "cancer") using the WordNetLemmatizer from the nltk.stem package. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Machine learning models for text classification</head><p>We experimented with several machine learning models frequently employed for classification tasks. Prior to model training, we ran a feature extraction step by utilizing TfidfVectorizer from the Natural Language Toolkit (NLTK) package <ref type="bibr" coords="4,321.91,462.88,11.42,10.91" target="#b6">[7]</ref>, removing stop words, and generating unigram and bigram features with the maximum document frequency parameter max_df set to 0.9. This resulted in 35,290 features. Next, we ran Grid Search <ref type="bibr" coords="4,386.40,489.98,12.99,10.91" target="#b7">[8]</ref> to tune the parameters for the following machine learning models: Support Vector Machines (SVM), Multinomial Naive Bayes, Logistic Regression, Multilayer Perceptron (MLP), Decision Trees, and Random Forest models <ref type="bibr" coords="4,152.81,530.62,11.30,10.91" target="#b8">[9]</ref>. We set the Logistic Regression model to run with L2 regularization, with the regularization strength parameter C set to 1.0. We undertook the following measures to further improve our models: Oversampling : To address the inherent imbalance in our dataset, we oversampled our data using the RandomOverSampler method <ref type="bibr" coords="4,264.22,584.82,17.76,10.91" target="#b9">[10]</ref> to generate a more balanced dataset. RandomOver-Sampler duplicates random instances of the minority class until its count equals that of the majority class.</p><p>Creation of Hand-Coded Keyword Sets : We enhanced our features by creating a handcoded keyword set, shown in Table <ref type="table" coords="4,247.81,639.02,3.71,10.91" target="#tab_0">1</ref>. If a dialogue contained these keywords, we would add a binary feature to the corresponding section header to boost prediction accuracy.</p><p>Feature Selection : We also explored mutual information-based feature selection approaches, but did not find it to be highly significant for improving model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep learning models for text classification</head><p>In addition to the machine learning models described above, we explored several deep learning models using Keras <ref type="bibr" coords="5,182.56,150.24,16.41,10.91" target="#b10">[11]</ref>. We preprocessed the data as follows: we tokenized the text into sequences of indices representing the 20,000 most frequent words, performed padding so that all sequences have the same length, and represented the output classes as a one-hot encoding. The average sequence length was 257, and we set the maximum sequence length of 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Bag-of-Words (CBoW) :</head><p>A Continuous Bag-of-Words (CBoW) model <ref type="bibr" coords="5,465.09,219.65,17.75,10.91" target="#b11">[12]</ref> averages the word embeddings of all words in a sentence to capture the semantics and context of the collection of words to make predictions. We inputted embedded text sequences into a GlobalAv-eragePooling1D layer, then generated predictions through a Dense layer using softmax as the activation type. We compiled the CBoW model with the loss set to be categorical_crossentropy with adam optimizer, and accuracy metric as the optimization criterion. The model was fitted with a validation split size of 0.1, batch size of 128, and 150 epochs.</p><p>Long Short-Term Memory (LSTM) : Next, we initiated a Long Short-Term Memory (LSTM) model <ref type="bibr" coords="5,119.21,343.25,16.17,10.91" target="#b12">[13]</ref>, which is a Recurrent Neural Network shown to handle sequential data well. LSTM models capture dependencies between words in a sentence, allowing the model to understand the context in which a word appears. These features make it suitable for text classification tasks.</p><p>Our model consists of an LSTM layer with an output dimension of 128, a dropout rate of 0.2, a recurrent state dropout rate of 0.2, and a Dense layer using softmax as the activation type. We compiled the LSTM model with the loss set to be categorical_crossentropy with adam optimizer, and accuracy metric as the optimization criterion. The model was fitted with a validation split size of 0.1, batch size of 128, and 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional Long Short-Term Memory (BiLSTM) :</head><p>We also investigated the Bidirectional LSTM model <ref type="bibr" coords="5,181.04,480.41,16.42,10.91" target="#b13">[14]</ref>, which is similar to LSTM, but consists of two layers taking input in both the forward and backward directions. Given its two layers, it is good at capturing both past and future context for each word in a sentence and also long-term dependencies within text. Our BiLSTM model consists of a SpatialDropout1D layer with a dropout rate of 0.2, a Bidirectional layer, a Conv1D layer with a filter size of 64, a GlobalAveragePooling1D layer, a GlobalMaxPooling1D layer, and a Dense layer using sigmoid as the activation type <ref type="bibr" coords="5,487.55,548.15,16.09,10.91" target="#b14">[15]</ref>. Similar to the CBoW and LSTM models, the BiLSTM model was compiled with the loss set to be categorical_crossentropy with adam optimizer and accuracy metric as the optimization criterion, and fitted over a validation split size of 0.1, batch size of 128, and 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Network (CNN) -Long Short-Term Memory (LSTM) :</head><p>Next, we combined the Convolutional Neural Network (CNN) and LSTM models, similar to the one explored by Qing et al. <ref type="bibr" coords="5,195.88,644.66,12.99,10.91" target="#b3">[4]</ref> for text classification. CNN is set to extract higher levels of word representations, which can then be inputted into LSTM to obtain sentence representations.</p><p>We used 1D Convolutional layers to extract local features such as n-grams or character-level features from the input text sequence, and then added pooling layers for dimension reduction.</p><p>To prevent overfitting, we also added dropout layers for regularization. Our CNN-LSTM models consists of the following layers stacked in this order: a Conv1D layer with a filter size of 64 and a window size of 5, a MaxPooling1D layer with a pool size of 5, a Dropout layer with a dropout rate of 0.2, a Conv1D layer with a filter size of 64 and a window size of 5, a MaxPooling1D layer with a pool size of 5, a Dropout layer with a dropout rate of 0.2, an LSTM layer with an output dimension of 64, and a Dense layer using softmax as the activation type. Similar to the other neural network models above, we compiled the CNN-LSTM model with the loss set to be categorical_crossentropy with adam optimizer and accuracy metric as the optimization criterion, and fitted over the validation split size of 0.1, batch size of 128, and 40 epochs.</p><p>While training these deep learning models, we also used some functions in the Callbacks module of Keras to improve overall training performance and efficiency. We used EarlyStopping to drop out from epochs when the validation loss ceases to improve, so as to reduce overfitting. We used ModelCheckpoint to save optimal model weights during the training process.</p><p>ClinicalBERT: Last, we instantiated a ClinicalBERT model <ref type="bibr" coords="6,359.25,305.41,16.09,10.91" target="#b15">[16]</ref>, a variant of the BERT model trained on clinical text, to classify clinical note sections. The ClinicalBERT model's design aims to capture the intricacies of medical language, offering domain-specific knowledge and contextual understanding to enhance performance in healthcare-related natural language processing tasks. Since traditional feature extractors like TfidfVectorizer are not necessary for BERT models, we tokenized the input data and set up attention masks and label encoding. We set the number of epochs at 3, the training and evaluation batch sizes at 8, and the learning rate of 2e-5 during training to dictate the degree of adjustment to the model weights in response to each estimated error update. To prevent overfitting, we implemented a weight decay of 0.01, a regularization technique that adds a small penalty proportional to the L2-norm of the weights to the loss function <ref type="bibr" coords="6,129.59,440.91,16.25,10.91" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Selecting models for test submission</head><p>We chose the three best models based on their performances on the validation data -Logistic Regression, CBoW, and ClinicalBERT models. We chose CBoW over CNN-LSTM models because of higher consistency in results across classes. All models were implemented with the oversampling technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Performance on Validation Data</head><p>Our baseline model, an untuned Support Vector Classifier (SVC) performed at an accuracy of 0.56 on the validation data. The deployment of the RandomOverSampler method resulted in a considerable enhancement of our overall performance metrics. This was clearly reflected in the ClinicalBERT model, whose accuracy improved from 0.66 to 0.75 as a result of the implementation of RandomOverSampler. More importantly, we observed a substantial increase in the prediction accuracy for the underrepresented classes within our data. For instance, the F1-score for the section header EDCOURSE improved from 0.00 to 0.50. This improvement indicates that our approach provided for a more equitable representation, thereby improving the comprehensiveness and reliability of our model's predictive capabilities.</p><p>Among other measures we took to enhance the performances of our models, the creation of hand-coded keyword sets did not result in improvement in model performances, so we discontinued this approach. We conclude that more work is needed to incorporate domain knowledge to improve these hand-coded keywords or to capture them directly in the model.</p><p>The best performing model is ClinicalBERT combined with the oversampling technique, which achieves an accuracy of 0.75. The next-best performing model is Logistic Regression, with an accuracy of 0.71. Among the deep learning models, CBoW trained with un-resampled data obtained an accuracy of 0.66, while CBoW trained with resampled data had a slightly improved performance of 0.68. LSTM models and CNN-LSTM models performed slightly better with un-resampled data (0.62 and 0.68, respectively) compared to with resampled data (0.60 and 0.64, respectively); while BiLSTM's performance was better with resampled data (0.64) than with un-resampled data (0.61).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Qualitative analysis of best performing models</head><p>Logistic Regression : This model achieved an accuracy of 0.71 and a weighted average F1-score of 0.66, suggesting balanced performance in precision and recall. High F1-scores, with near perfect precision and recall were observed for FAM/SOCHX and PASTSURGICAL, while perfect scores were observed for DIAGNOSIS, DISPOSITION, and IMMUNIZATIONS. Conversely, precision and recall were zero for ASSESSMENT and EDCOURSE. The class EXAM was over-predicted, thus generating more false positives.</p><p>CBoW : The overall prediction accuracy of the model is 0.68, with perfect classification results for IMMUNICATIONS and near-perfect F1-scores for FAM/SOCHX, PASTSURGICAL and ROS. On the other hand, the model showed poor performance with zero precision, recall and F1 scores for ASSESSMENT, EDCOURSE, IMAGING, OTHER_HISTORY, and PROCEDURES.</p><p>Clinical BERT : This model achieved the best of the three submitted runs, with an overall accuracy of 0.75 and a weighted average F1-score of 0.734. High F1-scores were achieved for ALLERGY and FAM/SOCHX classes, while F1-scores for ASSESSMENT and DISPOSITION classes were low, and zero for DIAGNOSIS and GYNHX classes. Classes such as EDCOURSE and PLAN demonstrated high precision, but low recall, indicating that while the model does not tend to misclassify instances, it is prone to omitting instances from these classes.</p><p>All three models benefited from oversampling techniques and had improved performance on infrequent classes. Even so, the models performed significantly better on larger classes and less so on less frequent ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance on Test Data</head><p>As summarized in the last column of Table <ref type="table" coords="7,287.80,633.54,3.81,10.91" target="#tab_1">2</ref>, among the three runs we submitted, the Clin-icalBERT model performed the best, with an accuracy of 0.765 on the test data. CBoW and Logistic Regression models achieved an accuracy of 0.695 and 0.675, respectively, on the test data. We think that the higher performance of the ClinicalBERT model can be attributed to its pre-training on clinical text data, which allows for transfer of domain-specific knowledge and contextual understanding learned from a more extensive corpus of medical language, resulting in enhanced performance on the target task despite limited training data <ref type="bibr" coords="8,418.50,359.22,16.34,10.91" target="#b17">[18]</ref>. CBoW models performed similarly over the test set (accuracy of 0.695) as over the validation set (accuracy of 0.68), mirroring the stability of results across classes we noticed on validation data. When compared to the deep learning models, the test performance of the Logistic Regression model was significantly poorer. We think that some of the poor performance can be attributed to potentially missing informative features. While it is known that feature selection plays a critical part in machine learning model training <ref type="bibr" coords="8,271.14,440.52,16.27,10.91" target="#b18">[19]</ref>, our attempt to optimize our selected feature set for the Logistic Regression model, including the hand-coded features, did not appear to work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Limitations</head><p>For the task of Dialogue2Topic classification, our experiments showed the ClinicalBERT model to perform the best among the models tested, with our implementation achieving the best performance of 0.765 on the MEDIQA-Sum 2023 test set. One of the main limitations of our approach was the limited training dataset size available to train traditional machine learning models. While we addressed some of these concerns through oversampling, there were still some classes with only one instance that did not benefit much from the approach. We also encountered the issue of overfitting, which possibly also stems from the limited dataset size. We addressed this problem partially through feature selection and using EarlyStopping <ref type="bibr" coords="8,467.71,621.08,16.38,10.91" target="#b19">[20]</ref>. We plan to continue exploring alternate approaches including transfer learning <ref type="bibr" coords="8,421.08,634.63,16.08,10.91" target="#b20">[21]</ref>, to incorporate domain knowledge into models through better feature and domain representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,300.78,355.41,8.93;3,130.96,84.19,333.37,204.02"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training Data Distribution Highlighting the Skewed Label Data Distribution</figDesc><graphic coords="3,130.96,84.19,333.37,204.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,418.03,297.02"><head>Table 1</head><label>1</label><figDesc>Hand-Coded Keywords</figDesc><table coords="4,95.27,122.10,411.75,265.40"><row><cell>Section Header</cell><cell>Keywords</cell></row><row><cell>ALLERGY</cell><cell>allergy, allergies, allergic, reaction, hypersensitivity, anaphylaxis</cell></row><row><cell>ASSESSMENT</cell><cell>assessment, evaluate, evaluation, examine, analysis, appraisal</cell></row><row><cell>CC</cell><cell>chief complaint, complaint, symptom, presenting, concern, Issue</cell></row><row><cell>DIAGNOSIS</cell><cell>diagnosis, diagnose, condition, finding, disease, disorder</cell></row><row><cell>DISPOSITION</cell><cell>disposition, discharge, admit, transfer, status, outcome</cell></row><row><cell>EDCOURSE</cell><cell>ed course, emergency department, treatment, management, care, therapy</cell></row><row><cell>EXAM</cell><cell>physical exam, examination, inspection, auscultation, palpation, assessment</cell></row><row><cell>FAM/SOCHX</cell><cell>family history, social history, lifestyle, habits, relationships, environment</cell></row><row><cell>GENHX</cell><cell>general history, medical history, background, chronic illness, disease</cell></row><row><cell>GYNHX</cell><cell>gynecological history, gynecology, reproductive, menstruation, pregnancy, con-</cell></row><row><cell></cell><cell>traception</cell></row><row><cell>IMAGING</cell><cell>imaging, x-ray, ultrasound, CT, MRI, radiology, scan, radiograph</cell></row><row><cell cols="2">IMMUNIZATIONS Immunizations, vaccinations, vaccine, shot, immunity, inoculation</cell></row><row><cell>LABS</cell><cell>labs, laboratory, blood work, test results, analysis, diagnostics</cell></row><row><cell>MEDICATIONS</cell><cell>medications, drugs, prescriptions, meds, pharmacotherapy, pharmaceuticals</cell></row><row><cell>OTHER HISTORY</cell><cell>other history, additional history, miscellaneous, unrelated, extra, supplementary</cell></row><row><cell cols="2">PASTMEDICALHX past medical history, pmh, previous conditions, comorbidities, illnesses, disorders</cell></row><row><cell>PASTSURGICAL</cell><cell>past surgical, surgical history, operations, procedures, interventions, surgeries</cell></row><row><cell>PLAN</cell><cell>plan, treatment plan, interventions, actions, approach, strategy</cell></row><row><cell>PROCEDURES</cell><cell>procedures, interventions, techniques, operations, methods, practices</cell></row><row><cell>ROS</cell><cell>review of systems, ros, systematic clinical review, organ systems, body systems</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,90.49,399.40,201.38"><head>Table 2</head><label>2</label><figDesc>Best validation performance of all models, and performance of the selected models on test set</figDesc><table coords="8,104.39,122.10,384.01,169.76"><row><cell>Model</cell><cell>Accuracy with</cell><cell>Accuracy with</cell><cell>Best</cell><cell>Test</cell></row><row><cell></cell><cell cols="4">unresampled data resampled data accuracy accuracy</cell></row><row><cell>Baseline Model -untuned SVC</cell><cell>0.54</cell><cell>0.56</cell><cell>0.56</cell><cell></cell></row><row><cell>SVC</cell><cell>0.65</cell><cell>0.65</cell><cell>0.65</cell><cell></cell></row><row><cell>Multinomial NB</cell><cell>0.56</cell><cell>0.69</cell><cell>0.69</cell><cell></cell></row><row><cell>Logistic Regression</cell><cell>0.66</cell><cell>0.71</cell><cell>0.71</cell><cell>0.675</cell></row><row><cell>MLP</cell><cell>0.41</cell><cell>0.65</cell><cell>0.65</cell><cell></cell></row><row><cell>Decision Tree</cell><cell>0.52</cell><cell>0.58</cell><cell>0.58</cell><cell></cell></row><row><cell>Random Forest</cell><cell>0.64</cell><cell>0.67</cell><cell>0.67</cell><cell></cell></row><row><cell>CBoW</cell><cell>0.66</cell><cell>0.68</cell><cell>0.68</cell><cell>0.695</cell></row><row><cell>LSTM</cell><cell>0.62</cell><cell>0.60</cell><cell>0.62</cell><cell></cell></row><row><cell>BiLSTM</cell><cell>0.61</cell><cell>0.64</cell><cell>0.64</cell><cell></cell></row><row><cell>CNN -LSTM</cell><cell>0.68</cell><cell>0.64</cell><cell>0.68</cell><cell></cell></row><row><cell>Clinical BERT</cell><cell>0.66</cell><cell>0.75</cell><cell>0.75</cell><cell>0.765</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,111.28,393.32,10.91;9,112.66,124.83,394.53,10.91;9,112.66,138.38,394.53,10.91;9,112.66,151.93,58.60,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,375.45,111.28,130.53,10.91;9,112.66,124.83,389.97,10.91">Overview of the mediqa-sum task at imageclef 2023: Summarization and classification of doctor-patient conversations</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.07,138.38,113.05,10.91">CLEF 2023 Working Notes</title>
		<title level="s" coord="9,247.43,138.38,172.42,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,165.48,393.33,10.91;9,112.66,179.03,393.32,10.91;9,112.66,192.57,138.09,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,270.07,165.48,235.92,10.91;9,112.66,179.03,95.87,10.91">Section classification in clinical notes using supervised hidden markov model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lipsky Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,232.18,179.03,273.81,10.91;9,112.66,192.57,49.31,10.91">Proceedings of the 1st ACM International Health Informatics Symposium</title>
		<meeting>the 1st ACM International Health Informatics Symposium</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="744" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,206.12,393.33,10.91;9,112.66,219.67,395.17,10.91;9,112.66,233.22,394.52,10.91;9,112.66,246.77,80.57,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,322.27,206.12,183.71,10.91;9,112.66,219.67,70.79,10.91">Clinical note section identification using transfer learning</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Achan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">P</forename><surname>Soman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,204.87,219.67,302.96,10.91;9,112.66,233.22,161.08,10.91">Proceedings of Sixth International Congress on Information and Communication Technology: ICICT 2021</title>
		<meeting>Sixth International Congress on Information and Communication Technology: ICICT 2021<address><addrLine>London; Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,260.32,393.33,10.91;9,112.66,273.87,194.31,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,259.44,260.32,246.54,10.91;9,112.66,273.87,55.98,10.91">A novel neural network-based method for medical text classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Linhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xuehai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,176.56,273.87,67.67,10.91">Future Internet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">255</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,287.42,393.33,10.91;9,112.66,300.97,393.33,10.91;9,112.66,314.52,217.23,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,440.84,287.42,65.15,10.91;9,112.66,300.97,323.48,10.91">A clinical text classification paradigm using weak supervision and deep representation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,444.60,300.97,61.38,10.91;9,112.66,314.52,148.52,10.91">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,328.07,393.33,10.91;9,112.66,341.62,395.01,10.91;9,112.66,355.17,38.81,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,215.43,328.07,290.56,10.91;9,112.66,341.62,96.03,10.91">Towards an automated SOAP note: Classifying utterances from medical conversations</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Konam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,231.67,341.62,197.64,10.91">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="610" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,368.71,393.33,10.91;9,112.66,382.26,257.39,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,226.29,368.71,279.70,10.91;9,112.66,382.26,129.27,10.91">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,395.81,393.33,10.91;9,112.66,409.36,207.63,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,220.65,395.81,227.14,10.91">Random search for hyper-parameter optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,459.97,395.81,46.01,10.91;9,112.66,409.36,123.70,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,422.91,393.33,10.91;9,112.66,436.46,393.33,10.91;9,112.66,450.01,85.93,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,222.87,422.91,140.12,10.91">Speech and language processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,370.21,422.91,135.78,10.91;9,112.66,436.46,50.38,10.91">Prentice Hall series in artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>Prentice Hall, Pearson Education International</publisher>
		</imprint>
	</monogr>
	<note>pearson international edition</note>
</biblStruct>

<biblStruct coords="9,112.66,463.56,393.33,10.91;9,112.66,477.11,393.33,10.91;9,112.66,490.66,394.52,10.91;9,112.66,504.21,80.57,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,264.49,463.56,241.50,10.91;9,112.66,477.11,135.17,10.91">Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,276.73,477.11,229.26,10.91;9,112.66,490.66,166.48,10.91">Advances in Intelligent Computing: International Conference on Intelligent Computing</title>
		<meeting><address><addrLine>ICIC; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,517.76,228.35,10.91" xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keras</forename></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,531.30,393.33,10.91;9,112.66,544.85,393.33,10.91;9,112.66,558.40,395.01,10.91;9,112.66,571.95,138.14,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,150.50,531.30,280.30,10.91">Context encoders as a simple but powerful extension of word2vec</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Horn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2602</idno>
		<ptr target="https://aclanthology.org/W17-2602.doi:10.18653/v1/W17-2602" />
	</analytic>
	<monogr>
		<title level="m" coord="9,452.94,531.30,53.05,10.91;9,112.66,544.85,393.33,10.91;9,112.66,558.40,46.02,10.91">Proceedings of the 2nd Workshop on Representation Learning for NLP, Association for Computational Linguistics</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP, Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,585.50,393.33,10.91;9,112.66,599.05,393.97,10.91;9,112.66,612.60,48.96,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,434.53,585.50,71.45,10.91;9,112.66,599.05,61.56,10.91">LSTM: A search space odyssey</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,183.05,599.05,280.48,10.91">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,626.15,393.32,10.91;9,112.66,639.70,172.11,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,231.17,626.15,171.97,10.91">Bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,411.87,626.15,94.11,10.91;9,112.66,639.70,78.03,10.91">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,653.25,395.17,10.91;9,112.66,666.80,395.17,10.91;10,112.66,86.97,394.53,10.91;10,112.28,100.52,394.99,10.91;10,112.66,114.06,150.56,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,219.18,653.25,288.65,10.91;9,112.66,666.80,223.13,10.91">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/I17-1026" />
	</analytic>
	<monogr>
		<title level="m" coord="9,360.76,666.80,147.07,10.91;10,112.66,86.97,272.84,10.91;10,434.91,86.97,72.28,10.91;10,112.28,100.52,214.67,10.91">Proceedings of the Eighth Inter-national Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth Inter-national Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="253" to="263" />
		</imprint>
	</monogr>
	<note>: Long Papers), Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.04,10.91;10,112.66,141.16,176.07,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<ptr target="https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT" />
		<title level="m" coord="10,179.80,127.61,209.56,10.91">ClinicalBERT -Bio + Clinical BERT model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,393.33,10.91;10,112.66,168.26,394.61,10.91;10,112.66,181.81,210.48,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,271.96,154.71,193.44,10.91">Neural pruning via growing regularization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=o966_Is_nPA" />
	</analytic>
	<monogr>
		<title level="m" coord="10,491.31,154.71,14.68,10.91;10,112.66,168.26,295.84,10.91">9th International Conference on Learning Representations, ICLR 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,389.65,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,449.56,195.36,56.43,10.91;10,112.66,208.91,234.91,10.91">Scalable and accurate deep learning with electronic health records</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,355.90,208.91,93.82,10.91">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,171.56,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,207.88,222.46,208.22,10.91">An introduction to variable and feature selection</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,423.25,222.46,82.74,10.91;10,112.66,236.01,82.55,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.71,10.91;10,112.66,263.11,393.33,10.91;10,112.66,276.66,133.99,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,452.22,249.56,54.15,10.91;10,112.66,263.11,241.13,10.91">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,361.45,263.11,144.54,10.91;10,112.66,276.66,39.91,10.91">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.32,10.91;10,112.66,303.75,60.92,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,287.92,290.20,125.48,10.91">A survey of transfer learning</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,421.67,290.20,84.31,10.91">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
