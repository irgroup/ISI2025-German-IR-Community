<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,371.58,15.42;1,89.29,107.08,310.14,11.96">A Concise Model for Medical Image Captioning Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">CSIRO&apos;s Machine Learning and Artificial Intelligence Future Science Platform</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,132.98,76.54,11.96"><forename type="first">Aaron</forename><surname>Nicolson</surname></persName>
							<email>aaron.nicolson@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<addrLine>Herston 4006</addrLine>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.04,132.98,71.58,11.96"><forename type="first">Jason</forename><surname>Dowling</surname></persName>
							<email>jason.dowling@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<addrLine>Herston 4006</addrLine>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.61,132.98,80.27,11.96"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
							<email>bevan.koopman@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<addrLine>Herston 4006</addrLine>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,371.58,15.42;1,89.29,107.08,310.14,11.96">A Concise Model for Medical Image Captioning Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6A84AD117A0F6DCE2F67DC1B82F12D35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image captioning</term>
					<term>Multimodal learning</term>
					<term>Encoder-to-decoder model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the ImageCLEFmedical Caption task of 2023. The task required participants to automatically compose coherent captions for a set of medical images. To this end, we employed a concise encoder-to-decoder model for caption generation. In addition, we leveraged Self-Critical Sequence Training (SCST) to optimise our model on the primary metric of the competition, BERTScore. CSIRO placed first amongst the participating teams-with a BERTScore of 0.643. The decoder of our best-performing submission was conditioned on the visual features of the medical image via the self-attention rather than the cross-attention. Here, the visual features were mapped to the token embedding space and used to prompt the decoder. Code and model checkpoints are available at https://github.com/aehrc/imageclefmedical_caption_23.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We detail our participation in the ImageCLEFmedical Caption task of 2023, the 7 ùë°‚Ñé edition of the task <ref type="bibr" coords="1,109.20,413.06,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,122.42,413.06,7.49,10.91" target="#b1">2]</ref>. Specifically, we participated in the caption prediction subtask. Here, participants were tasked with automatically generating captions for given medical images, where the image could be one of many modalities, e.g., radiography, ultrasonography, computed tomography, magnetic resonance, etc. The development of medical image captioning methods lays the groundwork for potential multimodal medical image analysis tools that could assist with clinical documentation, maintain and improve the consistency, quality, and efficiency of clinical reporting, produce rich textual descriptions from medical images, provide fast and inexpensive second readers, and help reduce teaching time.</p><p>For the 7 ùë°‚Ñé edition, several issues with the dataset (lemmatization errors and duplicate captions) were amended from the previous edition. The primary evaluation metric for the caption prediction subtask was also changed to a metric that captures the semantic similarity between generated and label captions, namely, BERTScore <ref type="bibr" coords="1,351.75,562.10,11.43,10.91" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ Head</head><p>Hyperintense areas are seen... ùëÅ is the number of Transformer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A: Condition via Cross-attention B: Condition via Selfattention</head><p>[BOS] is the beginning-of-sentence special token.</p><p>Our proposed approach for the caption prediction subtask builds upon our participation in previous editions where we used encoder-to-decoder model <ref type="bibr" coords="2,359.90,495.79,11.41,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,374.04,495.79,7.60,10.91" target="#b4">5]</ref>. As in the previous edition, we employ the Convolutional vision Transformer (CvT) <ref type="bibr" coords="2,339.59,509.34,12.84,10.91" target="#b5">[6]</ref> as the encoder and DistilGPT2 <ref type="bibr" coords="2,493.16,509.34,12.83,10.91" target="#b6">[7]</ref> as the decoder, forming the CvT2DistilGPT2 encoder-to-decoder model <ref type="bibr" coords="2,401.51,522.89,11.28,10.91" target="#b7">[8]</ref>. The novelty for this edition lies in the use of reinforcement learning to optimise the model for the primary metric and the means of conditioning the decoder on the visual features. For reinforcement learning, we employed BERTScore as the reward for Self-Critical Sequence Training (SCST) <ref type="bibr" coords="2,456.92,563.53,11.43,10.91" target="#b8">[9]</ref>.</p><p>Motivated by the Pre-trained Language Model (PaLM) with continuous observations of modalities Embodied into the token embedding space (PaLM-E) <ref type="bibr" coords="2,368.05,590.63,16.09,10.91" target="#b9">[10]</ref>, we investigated a different method of conditioning the decoder on the visual features extracted via the encoder. The standard approach of conditioning is through the cross-attention of the decoder. However, as with PaLM-E, the visual features can be mapped to the token embedding space and used to prompt the decoder. This has the advantage of requiring no cross attention, as shown in Figure <ref type="figure" coords="2,89.04,658.38,3.81,10.91" target="#fig_1">1</ref>. However, this at the cost of increasing the input sequence length, and thus the size of the self-attention matrices of each head. We aim to determine if there is a performance difference between conditioning via the self-attention, rather than the cross-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The dataset for the task is an updated and extended version of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="3,159.95,172.69,16.11,10.91" target="#b10">[11]</ref>, which was formed from figures in open-access biomedical journal articles from PubMed Central. All images in the dataset were accompanied by a caption, which form the labels for the caption prediction task. Each caption was pre-processed by removing links from the captions. The splits for the dataset are as follows:</p><p>‚Ä¢ Training set: 60 918 images and their corresponding captions.</p><p>‚Ä¢ Validation set: 10 437 images and their corresponding captions.</p><p>‚Ä¢ Test set: 10 473 images and their corresponding captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Models</head><p>The two encoder-to-decoder models that we used for our submissions are shown in Figure <ref type="figure" coords="3,500.08,345.26,3.79,10.91" target="#fig_1">1</ref>. CvT-21 was the encoder (specifically, the microsoft/cvt-21-384-22k checkpoint) <ref type="bibr" coords="3,476.37,358.81,11.43,10.91" target="#b5">[6,</ref><ref type="bibr" coords="3,490.53,358.81,7.62,10.91" target="#b7">8]</ref>. <ref type="foot" coords="3,501.96,357.05,3.71,7.97" target="#foot_0">1</ref>Layer normalisation was applied to its last hidden state, followed by a projection to the decoder's hidden state size. Each image was resized using bilinear interpolation so that its smallest side had a length of 384 and its largest side maintained the aspect ratio. Next, the resized image was cropped to a size of R 3√ó384√ó384 . The crop location was random during training and centred during testing. During training, the image was rotated around its centre where the angle of rotation was sampled from ùí∞[-5 ‚àò , 5 ‚àò ]. Finally, the image was standardised using the mean and standard deviation provided with the CvT-21 checkpoint.</p><p>DistilGPT2, along with its tokenizer, was used for decoding <ref type="bibr" coords="3,374.21,467.20,11.58,10.91" target="#b6">[7]</ref>. Greedy search and beam search with four beams were employed during validation and testing, respectively. The maximum number of tokens for the labels and the generated captions was 256. During testing, a penalty was applied during caption generation to the probability of tokens to prevent trigrams from appearing more than once in a caption (the penalty was realised by setting a token's probability to zero). Before training, both the CvT-21 and DistilGPT2 checkpoints were used to warm-start the encoder and decoder, respectively.</p><p>For the model conditioned via the cross-attention (CA) in Figure <ref type="figure" coords="3,406.01,562.04,5.17,10.91" target="#fig_1">1</ref> A (CvT2DistilGPT2-CA), randomly initialised multi-head cross-attention modules were added to each Transformer block. Here, the visual features from the encoder were passed as the keys and values to the cross-attention heads. For the model conditioned via the self-attention (SA) in Figure <ref type="figure" coords="3,490.57,602.69,5.17,10.91" target="#fig_1">1</ref> B (CvT2DistilGPT2-SA), the visual features and token embeddings were concatenated before adding the position embeddings. The visual features occupied the first 576 positions with the token embeddings occupying the remaining positions (DistilGPT2 accommodates up to 1 024</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Scores for each of the runs of team CSIRO. The primary metric is highlighted in grey. positions). Here, the encoder learns to map the visual features to the token embedding space, √† la PaLM-E <ref type="bibr" coords="4,138.17,214.66,16.25,10.91" target="#b9">[10]</ref>. The mapped visual features were then used to prompt the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Training: Two stages of training were performed: Teacher Forcing (TF) <ref type="bibr" coords="4,429.76,228.21,16.41,10.91" target="#b11">[12]</ref>, followed by SCST. Gradient descent optimisation was performed with AdamW <ref type="bibr" coords="4,386.79,241.76,17.90,10.91" target="#b12">[13]</ref> with a mini-batch size of 32 at an initial learning rate of 5e-5 for TF and 5e-6 for SCST. The models were trained with NVIDIA Tesla P100 16 GB GPUs and automatic mixed precision. For TF, early stopping with a patience of eight was employed. For SCST, three epochs were completed and validation was performed every 1  10 of an epoch. The validation BERTScore was the monitored metric for early stopping and checkpoint selection. For SCST, the baseline was generated with greedy search, while the sample was produced with top-k sampling (ùëò = 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metrics</head><p>The primary metric for this edition of the caption prediction subtask was BERTScore with microsoft/deberta-xlarge-mnli as the model <ref type="bibr" coords="4,324.19,386.33,11.57,10.91" target="#b2">[3]</ref>. ROUGE-1 was the secondary metric <ref type="bibr" coords="4,89.29,399.88,16.41,10.91" target="#b13">[14]</ref>. The following metrics were also included in the evaluation: METEOR <ref type="bibr" coords="4,431.91,399.88,16.41,10.91" target="#b14">[15]</ref>, CIDEr <ref type="bibr" coords="4,486.67,399.88,16.41,10.91" target="#b15">[16]</ref>, BLEU-1 <ref type="bibr" coords="4,125.07,413.43,16.09,10.91" target="#b16">[17]</ref>, BLEURT (BLEURT-20) <ref type="bibr" coords="4,246.52,413.43,16.08,10.91" target="#b17">[18]</ref>, and CLIPScore <ref type="bibr" coords="4,334.88,413.43,16.08,10.91" target="#b18">[19]</ref>. For all metrics, both the generated and label captions were pre-processed by converting to lower-case, replacing numbers with 'number', and removing punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results &amp; Discussion</head><p>The results for each of our four submissions are shown in Table <ref type="table" coords="4,384.82,499.15,3.81,10.91">1</ref>. As expected, employing BERTScore as a reward for SCST improved the BERTScore. Choosing BERTScore as the reward positively impacted the scores for ROUGE and BLEURT, while negatively impacting the scores for BLEU and METEOR, and had no noticeable impact on CIDEr and CLIPScore. When comparing the method of conditioning on the visual features, conditioning via the crossattention outperformed conditioning via the self-attention on six out of the seven metrics with TF (CvT2DistilGPT2-CA vs. CvT2DistilGPT2-SA). However, for SCST, conditioning via the self-attention performed better than conditioning via the cross-attention for three of the metrics, while they performed equally for METEOR (CvT2DistilGPT2-CA + SCST (BERTScore) vs. CvT2DistilGPT2-SA + SCST (BERTScore)). This indicates that there is no substantial difference between their performance.</p><p>The leaderboard for the competition is shown in Table <ref type="table" coords="4,358.51,648.20,3.81,10.91">2</ref>. Run 4 (CvT2DistilGPT2-SA + SCST (BERTScore)) was compared to the runs of the other participants as it scored the highest BERTScore. Team CSIRO ranked first based on the primary metric (BERTScore), with a score of 0.643. Team CSIRO also attained the highest CLIPScore, the third highest ROUGE, BLEURT, and CIDEr scores, the fourth highest METEOR score, and the sixth highest BLEU score. The lower rank of Run 4 for METEOR and BLEU could be attributed to optimising with SCST with BERTScore as the reward.</p><p>Shown in Figures <ref type="figure" coords="5,179.14,154.71,4.98,10.91">2</ref> and<ref type="figure" coords="5,205.69,154.71,4.98,10.91">3</ref> are generated reports for given medical images from the validation set. Here, we inspect the impact of SCST on the generated reports (compared to only using TF), as this had the largest impact on performance. Here, we use CvT2DistilGPT2-SA. Shown in Figure <ref type="figure" coords="5,132.08,195.36,5.09,10.91">2</ref> are examples where SCST outperforms TF (in terms of the BERTScore). For image 000414, both TF and SCST identify that there is contrast. SCST identifies the correct plane and provides more details about the modality. However, neither identifies the empty sella. For image 002044, both identify the modality and body part correctly. TF does not identify the opacity. While SCST correctly identifies the opacity, the location was incorrect. In Figure <ref type="figure" coords="5,89.29,263.11,5.17,10.91">3</ref> are examples where TF outperforms SCST. For image 008243, both identify the modality. TF identifies that there is an aneurysm of the Internal Carotid Artery (ICA), but incorrectly identifies the left ICA instead of the right ICA. TF also identifies that there is damage to the right ICA (pseudoaneurysm) which is semantically similar to what was described in the label (aneurysmal rupture). SCST incorrectly identifies the artery and the abnormality. For image 000193, TF identifies the right coronal artery, which is connected to the mitral valve. While SCST identifies the body part, it introduces a false positive abnormality and identifies the wrong artery. It should be noted that SCST identified calcification in three out of the four examples shown in Figures <ref type="figure" coords="5,168.41,371.50,5.06,10.91">2</ref> and<ref type="figure" coords="5,195.32,371.50,3.73,10.91">3</ref>, which indicates that SCST could increase hallucinations. While this is a small sample of the differences between SCST and TF, it is clear that SCST did not improve performance across all examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Leaderboard for the caption prediction subtask of ImageCLEFmedical Caption 2023. The primary metric used to rank the participants is highlighted in grey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team Name</head><p>Run ) from the validation set, their corresponding labels, and the corresponding generated captions for both CvT2DistilGPT2-SA with TF and additionaly with SCST. The BERTScores for each generated report are also given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we detailed our participation in the caption prediction subtask of ImageCLEFmedical Caption 2023. By leveraging SCST with the primary metric, BERTScore, team CSIRO was able to rank first amongst participating teams. We also investigated conditioning the decoder on the visual features via the cross-attention or self-attention. The results indicate that there is no substantial difference in performance between the two configurations. This demonstrates that there is no penalty when removing the cross-attention and instead using the self-attention to condition the decoder on the visual features. While the selected metrics for this edition have improved the evaluation process considerably, they are still general-domain metrics. The evaluation process could be improved by including domain-specific metrics that better capture the semantic similarity between captions. Such a metric could be derived from domain-specific encoders, such as PubMedBERT <ref type="bibr" coords="6,234.27,580.23,17.91,10.91" target="#b19">[20]</ref> or CXR-BERT (general) <ref type="bibr" coords="6,361.81,580.23,16.25,10.91" target="#b20">[21]</ref>. from the validation set, their corresponding labels, and the corresponding generated captions for both CvT2DistilGPT2-SA with TF and additionally with SCST. The BERTScores for each generated report are also given.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,420.73,417.29,8.93;2,89.29,432.73,418.23,8.87;2,89.29,444.42,369.83,9.14;2,332.65,329.44,56.57,60.53"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decoder conditioned on the visual features of the image via A) the cross-attention, and B) the self-attention. The visual features are extracted with the encoder. CC BY [Muacevic et al. (2022)]. ùëÅ is the number of Transformer blocks. [BOS] is the beginning-of-sentence special token.</figDesc><graphic coords="2,332.65,329.44,56.57,60.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,437.64,417.29,8.93;7,89.29,449.65,416.70,8.87;7,89.29,461.60,416.69,8.87;7,89.29,473.56,42.88,8.87"><head>631 Figure 3 :</head><label>6313</label><figDesc>Figure 3: Images 008243 (CC BY [Muacevic et al. (2021)]) and 000193 (CC BY [Ruiz et al. (2021)) from the validation set, their corresponding labels, and the corresponding generated captions for both CvT2DistilGPT2-SA with TF and additionally with SCST. The BERTScores for each generated report are also given.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,95.62,476.04,404.02,164.88"><head>BERTScore ROUGE BLEURT BLEU METEOR CIDEr CLIPScore</head><label></label><figDesc></figDesc><table coords="5,95.62,488.30,393.55,152.63"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>002044</cell><cell></cell><cell></cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Contrast-enhanced T1-weighted</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell cols="4">sagittal image of the brain, 1 month after initial presentation. The arrow</cell><cell cols="4">Chest x-ray of patient 2 with right middle to lower lung opacity.</cell></row><row><cell></cell><cell cols="3">shows a mostly empty sella.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">MRI of the brain with contrast show-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Generated (TF)</cell><cell cols="4">ing a large pituitary stalk (blue ar-row) and a large cerebellar peduncu-</cell><cell cols="4">Chest X-ray showing bilateral inter-stitial infiltrates</cell></row><row><cell></cell><cell cols="3">lated mass (red arrow).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTScore (TF)</cell><cell>0.585</cell><cell></cell><cell></cell><cell></cell><cell>0.715</cell><cell></cell><cell></cell></row><row><cell>Generated (SCST)</cell><cell cols="4">Sagittal T1-weighted MRI scan of the brain showing a calcification in the left cerebellum (blue arrow).</cell><cell cols="4">Chest X-ray showing a left pul-monary opacity.</cell></row><row><cell cols="2">BERTScore (SCST) 0.714</cell><cell></cell><cell></cell><cell></cell><cell>0.797</cell><cell></cell><cell></cell></row><row><cell cols="9">Figure 2: Images 000414 (CC BY-NC [Murvelashvili et al. (2021)]) and 002044 (CC BY-NC [Ogamba et</cell></row><row><cell>al. (2021)]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSIRO</cell><cell>4</cell><cell>0.643</cell><cell>0.245</cell><cell cols="2">0.314 0.161</cell><cell>0.080</cell><cell>0.203</cell><cell>0.815</cell></row><row><cell>closeAI2023</cell><cell>7</cell><cell>0.628</cell><cell>0.240</cell><cell cols="2">0.321 0.185</cell><cell cols="2">0.087 0.238</cell><cell>0.807</cell></row><row><cell>AUEB-NLP-Group</cell><cell>2</cell><cell>0.617</cell><cell>0.213</cell><cell cols="2">0.295 0.169</cell><cell>0.072</cell><cell>0.147</cell><cell>0.804</cell></row><row><cell>PCLmed</cell><cell>5</cell><cell>0.615</cell><cell>0.253</cell><cell cols="2">0.317 0.217</cell><cell>0.092</cell><cell>0.232</cell><cell>0.802</cell></row><row><cell>VCMI</cell><cell>5</cell><cell>0.615</cell><cell>0.218</cell><cell cols="2">0.308 0.165</cell><cell>0.073</cell><cell>0.172</cell><cell>0.808</cell></row><row><cell>KDE-Lab Med</cell><cell>3</cell><cell>0.615</cell><cell>0.222</cell><cell cols="2">0.301 0.156</cell><cell>0.072</cell><cell>0.182</cell><cell>0.806</cell></row><row><cell>SSN MLRG</cell><cell>1</cell><cell>0.602</cell><cell>0.211</cell><cell cols="2">0.277 0.142</cell><cell>0.062</cell><cell>0.128</cell><cell>0.776</cell></row><row><cell>DLNU CCSE</cell><cell>1</cell><cell>0.601</cell><cell>0.203</cell><cell cols="2">0.263 0.106</cell><cell>0.056</cell><cell>0.133</cell><cell>0.773</cell></row><row><cell>CS Morgan</cell><cell>10</cell><cell>0.582</cell><cell>0.156</cell><cell cols="2">0.224 0.057</cell><cell>0.044</cell><cell>0.084</cell><cell>0.759</cell></row><row><cell cols="2">Clef-CSE-GAN-Team 2</cell><cell>0.582</cell><cell>0.218</cell><cell cols="2">0.269 0.145</cell><cell>0.070</cell><cell>0.174</cell><cell>0.789</cell></row><row><cell>Bluefield-2023</cell><cell>3</cell><cell>0.578</cell><cell>0.153</cell><cell cols="2">0.272 0.154</cell><cell>0.060</cell><cell>0.101</cell><cell>0.784</cell></row><row><cell>IUST NLPLAB</cell><cell>6</cell><cell>0.567</cell><cell>0.290</cell><cell cols="4">0.223 0.268 0.100 0.177</cell><cell>0.807</cell></row><row><cell>SSNSheerinKavitha</cell><cell>4</cell><cell>0.544</cell><cell>0.087</cell><cell cols="2">0.215 0.075</cell><cell>0.026</cell><cell>0.014</cell><cell>0.687</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.57,671.03,275.36,8.97"><p>https://huggingface.co/microsoft/cvt-21-384-22k (last accessed 03/07/2023).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,92.57,670.89,338.28,8.97"><p>https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-general (last accessed 02/07/2023).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="funder">CSIRO's Machine Learning and Artificial Intelligence Future Science Platform</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,607.60,394.53,10.91;7,112.66,621.15,395.17,10.91;7,112.66,634.70,394.53,10.91;7,112.66,648.25,395.17,10.91;7,112.39,661.80,394.80,10.91;8,112.48,86.97,394.70,10.91;8,112.66,100.52,395.17,10.91;8,112.66,114.06,393.32,10.91;8,112.66,127.61,394.53,10.91;8,112.33,141.16,120.27,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,224.93,86.97,282.25,10.91;8,112.66,100.52,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcƒ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.94,100.52,143.89,10.91;8,112.66,114.06,393.32,10.91;8,112.66,127.61,134.07,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,277.25,127.61,220.25,10.91">Springer Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,154.71,394.52,10.91;8,112.66,168.26,395.17,10.91;8,112.66,181.81,393.33,10.91;8,112.66,195.36,247.61,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,298.72,168.26,209.11,10.91;8,112.66,181.81,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.89,181.81,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="8,429.41,181.81,76.58,10.91;8,112.66,195.36,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,208.91,393.33,10.91;8,112.66,222.46,371.28,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
		<title level="m" coord="8,377.60,208.91,128.39,10.91;8,112.66,222.46,98.12,10.91">BERTScore: Evaluating Text Generation with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,236.01,394.53,10.91;8,112.66,249.56,394.52,10.91;8,112.66,263.11,67.18,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,287.09,236.01,194.09,10.91">AEHRC CSIRO at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,127.24,249.56,326.21,10.91">Proceedings of the 12th International Conference of the CLEF Association</title>
		<meeting>the 12th International Conference of the CLEF Association<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,276.66,395.17,10.91;8,112.66,290.20,393.32,10.91;8,112.66,303.75,173.10,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,431.73,276.66,76.10,10.91;8,112.66,290.20,98.13,10.91">CSIRO at Image-CLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Belous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,255.23,290.20,250.75,10.91;8,112.66,303.75,76.73,10.91">Proceedings of the 13th International Conference of the CLEF Association</title>
		<meeting>the 13th International Conference of the CLEF Association<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,317.30,393.33,10.91;8,112.66,330.85,393.32,10.91;8,112.33,344.40,394.36,10.91;8,112.66,357.95,277.21,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,371.26,317.30,134.73,10.91;8,112.66,330.85,99.29,10.91">CvT: Introducing Convolutions to Vision Transformers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00009</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9710031/.doi:10.1109/ICCV48922.2021.00009" />
	</analytic>
	<monogr>
		<title level="m" coord="8,257.73,330.85,248.25,10.91;8,112.33,344.40,29.16,10.91">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,371.50,394.52,10.91;8,112.66,385.05,394.51,10.91;8,112.66,398.60,196.96,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,294.58,371.50,212.60,10.91;8,112.66,385.05,118.22,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.01108</idno>
		<idno type="arXiv">arXiv:1910.01108[cs</idno>
		<ptr target="http://arxiv.org/abs/1910.01108.doi:10.48550/arXiv.1910.01108" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,412.15,393.61,10.91;8,112.66,425.70,394.51,10.91;8,112.66,439.25,196.96,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2201.09405</idno>
		<idno type="arXiv">arXiv:2201.09405</idno>
		<ptr target="http://arxiv.org/abs/2201.09405.doi:10.48550/arXiv.2201.09405" />
		<title level="m" coord="8,292.19,412.15,214.08,10.91;8,112.66,425.70,118.97,10.91">Improving Chest X-Ray Report Generation by Leveraging Warm-Starting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,452.79,393.33,10.91;8,112.66,466.34,395.16,10.91;8,112.66,479.89,394.04,10.91;8,112.66,493.44,235.67,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,363.58,452.79,142.41,10.91;8,112.66,466.34,92.89,10.91">Self-Critical Sequence Training for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.131</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8099614/.doi:10.1109/CVPR.2017.131" />
	</analytic>
	<monogr>
		<title level="m" coord="8,251.85,466.34,255.97,10.91;8,112.66,479.89,60.24,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,506.99,394.53,10.91;8,112.66,520.54,395.17,10.91;8,112.66,534.09,394.61,10.91;8,112.28,547.64,395.39,10.91;8,112.66,561.19,266.92,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.03378</idno>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<ptr target="http://arxiv.org/abs/2303.03378.doi:10.48550/arXiv.2303.03378" />
		<title level="m" coord="8,468.48,534.09,38.79,10.91;8,112.28,547.64,193.51,10.91">PaLM-E: An Embodied Multimodal Language Model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,574.74,393.33,10.91;8,112.33,588.29,395.50,10.91;8,112.66,601.84,394.53,10.91;8,112.66,615.39,393.53,10.91;8,112.66,628.93,393.33,10.91;8,112.66,642.48,393.32,10.91;8,112.66,656.03,354.54,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,370.24,574.74,135.74,10.91;8,112.33,588.29,171.65,10.91;8,448.67,615.39,57.52,10.91;8,112.66,628.93,393.33,10.91;8,112.66,642.48,120.44,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
	</analytic>
	<monogr>
		<title level="s" coord="8,240.40,642.48,157.56,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Balocco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Martel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S.-L</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Moriconi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="180" to="189" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</note>
</biblStruct>

<biblStruct coords="8,112.66,669.58,393.32,10.91;9,112.66,86.97,397.48,10.91;9,112.66,100.52,200.38,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,223.84,669.58,282.14,10.91;9,112.66,86.97,72.16,10.91">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.2.270</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,135.59,100.52,172.34,10.91">conference Name: Neural Computation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,394.62,10.91;9,112.31,127.61,183.42,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
		<title level="m" coord="9,229.77,114.06,185.39,10.91">Decoupled Weight Decay Regularization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,393.33,10.91;9,112.28,168.26,394.90,10.91;9,112.39,181.81,395.28,10.91;9,112.66,195.36,397.48,10.91;9,112.36,211.35,43.94,7.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,205.93,141.16,300.06,10.91;9,112.66,154.71,37.64,10.91">Automatic evaluation of summaries using N-gram co-occurrence statistics</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073445.1073465</idno>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1073445.1073465.doi:10.3115/1073445.1073465" />
	</analytic>
	<monogr>
		<title level="m" coord="9,172.74,154.71,333.25,10.91;9,112.28,168.26,391.11,10.91">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -NAACL &apos;03<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,222.46,393.33,10.91;9,112.66,236.01,395.17,10.91;9,112.66,249.56,394.53,10.91;9,112.28,263.11,395.00,10.91;9,112.66,276.66,157.69,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,210.22,222.46,295.77,10.91;9,112.66,236.01,162.15,10.91">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W05-0909" />
	</analytic>
	<monogr>
		<title level="m" coord="9,301.31,236.01,206.52,10.91;9,112.66,249.56,394.53,10.91;9,112.28,263.11,189.41,10.91">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,290.20,395.17,10.91;9,112.66,303.75,394.53,10.91;9,112.66,317.30,395.01,10.91;9,112.66,330.85,167.31,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,281.45,290.20,226.37,10.91;9,112.66,303.75,15.85,10.91">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299087</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7299087/.doi:10.1109/CVPR.2015.7299087" />
	</analytic>
	<monogr>
		<title level="m" coord="9,150.10,303.75,326.34,10.91">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,344.40,393.32,10.91;9,112.66,357.95,395.16,10.91;9,112.66,371.50,394.52,10.91;9,112.66,385.05,395.01,10.91;9,112.66,398.60,155.44,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,308.17,344.40,197.81,10.91;9,112.66,357.95,85.52,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1073083.1073135.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="9,220.69,357.95,287.13,10.91;9,112.66,371.50,138.97,10.91">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics -ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics -ACL &apos;02<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,394.62,10.91;9,112.66,425.70,394.53,10.91;9,112.28,439.25,394.42,10.91;9,112.66,452.79,354.17,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,242.58,412.15,244.34,10.91">BLEURT: Learning Robust Metrics for Text Generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.704.doi:10.18653/v1/2020.acl-main.704" />
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,425.70,394.53,10.91;9,112.28,439.25,192.52,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,466.34,393.33,10.91;9,112.66,479.89,393.33,10.91;9,112.66,493.44,395.16,10.91;9,112.66,506.99,394.62,10.91;9,112.31,520.54,388.50,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,373.55,466.34,132.44,10.91;9,112.66,479.89,182.68,10.91">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Le</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.595</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.595.doi:10.18653/v1/2021.emnlp-main.595" />
	</analytic>
	<monogr>
		<title level="m" coord="9,324.52,479.89,181.46,10.91;9,112.66,493.44,395.16,10.91;9,112.66,506.99,32.81,10.91">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7514" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,394.53,10.91;9,112.66,547.64,394.53,10.91;9,112.28,561.19,395.39,10.91;9,112.66,574.74,221.00,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,112.66,547.64,390.05,10.91">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3458754.doi:10.1145/3458754" />
	</analytic>
	<monogr>
		<title level="j" coord="9,112.28,561.19,221.89,10.91">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,588.29,394.53,10.91;9,112.66,601.84,393.33,10.91;9,112.66,615.39,394.53,10.91;9,112.66,628.93,394.52,10.91;9,112.66,642.48,395.00,10.91;9,112.66,656.03,181.18,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,431.99,601.84,74.00,10.91;9,112.66,615.39,321.57,10.91">Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wetscherek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alvarez-Valle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.30,628.93,138.26,10.91">Computer Vision -ECCV 2022</title>
		<title level="s" coord="9,112.66,642.48,156.46,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ciss√©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
