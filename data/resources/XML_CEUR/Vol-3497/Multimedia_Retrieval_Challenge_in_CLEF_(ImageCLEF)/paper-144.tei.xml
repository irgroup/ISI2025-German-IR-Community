<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,385.11,15.42;1,89.29,106.66,325.34,15.42">BIT Mesra at ImageCLEF 2023: Fusion of Blended Image and Text Features for Medical VQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,97.98,11.96"><forename type="first">Sushmita</forename><surname>Upadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Birla Institute of Technology Mesra</orgName>
								<address>
									<settlement>Ranchi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.44,134.97,126.88,11.96"><forename type="first">Sanjaya</forename><surname>Shankar Tripathy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Birla Institute of Technology Mesra</orgName>
								<address>
									<settlement>Ranchi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,385.11,15.42;1,89.29,106.66,325.34,15.42">BIT Mesra at ImageCLEF 2023: Fusion of Blended Image and Text Features for Medical VQA</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">4061EEE2E52768243C46FB4B15EE7EE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VQA-MED</term>
					<term>BioBERT</term>
					<term>VGG Network</term>
					<term>Blended image features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the blended image and text features fusion for medical Visual Question Answering (VQA), submitted to the MEDVQA-GI task organized by ImageCLEF 2023. The goal of medical VQA is to interpret medical images and generate accurate answers to questions about the image. The proposed network uses a combination of deep and handcrafted methods to extract more abstract high level information and domain specific image feature representation. The question feature embeddings have been captured by the BioBERT model which has been trained specifically on medical datasets. Finally, the multimodal features have been combined together using the Multimodal Factorized High-Order Pooling with co-attention mechanism to generate a fused embedding representation. The unified embedding is then used for classification. MedVQA models can have significant applications in medical diagnosis, treatment planning, decision-making and assisting healthcare professionals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering (VQA) is a challenging multimodal problem which requires answering a text-based question by utilizing information provided in the input image. Like a human, the Artificial Intelligence (AI) machine is expected to intelligently utilize and understand the information conveyed by the contents in the image and find a relation which will help in answering the question. Colonoscopy is a procedure used to detect lesions and diseases in the colon. When VQA is used on colonoscopy images, it enables the retrieval of diverse information by answering various questions about these images. However, the application of VQA in medical images poses greater difficulties compared to general domain images <ref type="bibr" coords="1,492.22,496.34,11.58,10.91" target="#b0">[1]</ref>. This is because medical images are generally low resolution images which adds complexity to information retrieval. The creation of medical VQA databases demands expert involvement, making the process time-consuming and costly. Moreover, answering medical questions may require the model to be familiar with specialized medical terminologies potentially requiring training with medical knowledge databases <ref type="bibr" coords="1,285.05,564.09,11.43,10.91" target="#b0">[1]</ref>.</p><p>Multiple approaches have been investigated to analyze the features of input images and textual questions in VQA. Deep models like VGG16 <ref type="bibr" coords="2,321.00,100.52,11.46,10.91" target="#b1">[2]</ref>, VGG19 <ref type="bibr" coords="2,373.68,100.52,11.46,10.91" target="#b1">[2]</ref>, and ResNet <ref type="bibr" coords="2,445.21,100.52,12.87,10.91" target="#b2">[3]</ref> have been utilized to generate feature embedding representing the input image effectively. Simultaneously, sequential models like Gated recurrent unit (GRU) <ref type="bibr" coords="2,318.17,127.61,11.53,10.91" target="#b3">[4]</ref>, Long Short Term Memory (LSTM) <ref type="bibr" coords="2,493.04,127.61,12.95,10.91" target="#b4">[5]</ref> or deep models like transformers <ref type="bibr" coords="2,238.55,141.16,12.82,10.91" target="#b5">[6]</ref> have been explored to capture the feature representation or embeddings of a question. The individual features need to be combined to generate an effective and discriminative embedding vector. Feature concatenation, element wise multiplication and bilinear pooling are some of the methods to fuse two vectors. Recently, attention based methods have shown improved performance in tasks like machine translation <ref type="bibr" coords="2,461.61,195.36,11.33,10.91" target="#b6">[7]</ref>, image captioning <ref type="bibr" coords="2,140.99,208.91,13.00,10.91" target="#b7">[8]</ref> etc. Multi-modal Factorized Bilinear Pooling <ref type="bibr" coords="2,365.16,208.91,12.99,10.91" target="#b8">[9]</ref> (MFB) gives a compact and efficient fused feature embedding and has been utilized for VQA. It has been generalized in Multimodal Factorized High-Order Pooling (MFH) <ref type="bibr" coords="2,319.40,236.01,16.39,10.91" target="#b9">[10]</ref>. Overfitting has been avoided by employing Global Average Pooling <ref type="bibr" coords="2,229.33,249.56,18.85,10.91" target="#b10">[11]</ref> on the convolution output thereby improving accuracy in <ref type="bibr" coords="2,89.29,263.11,16.39,10.91" target="#b11">[12]</ref>. A powerful language model, Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" coords="2,126.57,276.66,16.31,10.91" target="#b12">[13]</ref>, captures embeddings from the input text and has been pre-trained with general domain data. Each embedding captures the meaning and contextual information of the words in the text. Bio-BERT <ref type="bibr" coords="2,185.26,303.75,17.75,10.91" target="#b13">[14]</ref> is a variant of BERT which has been pre-trained on medical data and medical texts. This model captures the semantic and context of the medical texts which makes it well suited to extract the domain specific features. Bio-BERT model was used to generate question embeddings in <ref type="bibr" coords="2,195.68,344.40,17.75,10.91" target="#b14">[15]</ref> and <ref type="bibr" coords="2,234.68,344.40,17.76,10.91" target="#b15">[16]</ref> which were a part of VQA MED 2020 and VQA MED 2021 challenges respectively. To predict answers, both classification and generation methods can be employed. A classifier will predict the most likely answer from a predefined set of labels. Generative models generate answers to the input question using networks like LSTMs, GRUs or Transformers.</p><p>The ImageCLEF association has in 2023 <ref type="bibr" coords="2,278.65,412.15,17.90,10.91" target="#b16">[17]</ref> organised Medical Visual Question Answering for GI : MEDVQA-GI2023 task <ref type="bibr" coords="2,225.18,425.70,16.15,10.91" target="#b17">[18]</ref>. There are three subtasks in this task: VQA, Visual location question answering (VLQA) and Visual question generation (VQG). It has been designed around colonoscopy images which covers the entire Gastrointestinal (GI) tract from mouth to anus. For VQA, the images and textual questions are to be combined to generate answers.</p><p>In this work, the VQA subtask has been approached as a classification problem with the unique entries as the class label. The feature embeddings are obtained for both the two modalities: images and text. These features have been fused using Multimodal Factorized High-Order Pooling with co-attention (MFH+CoAtt) <ref type="bibr" coords="2,272.72,520.54,16.35,10.91" target="#b9">[10]</ref>. Additionally, image features were extracted by utilizing a blend of deep learning techniques and handcrafted feature extraction methods <ref type="bibr" coords="2,487.30,534.09,16.23,10.91" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>VQA has two inputs, images and text, with unique features. The features derived from the image and text modalities can be individually analyzed to leverage their specific characteristics. These features can then be fused together to obtain a combined and informative feature vector that captures the complementary information from both modalities. In figure <ref type="figure" coords="2,415.34,633.36,3.74,10.91" target="#fig_0">1</ref>, the block diagram of the proposed system has been presented. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image feature extraction</head><p>Colonoscopy images can provide valuable information that can be useful in the detection and diagnosis of any disorder in the GI tract. The presence of abnormalities in a region can be identified by examining the color, texture, and shape variations present in these images. The image features have been extracted by utilizing both deep learning methods and handcrafted methods. This approach utilizes the power of deep models to automatically learn and extract relevant features from the image. Simultaneously, it also incorporates handcrafted techniques that capture domain specific characteristics from the image. The deep features are obtained using the VGG19 convolutional neural network <ref type="bibr" coords="3,312.27,432.37,11.58,10.91" target="#b1">[2]</ref>. The top classification layers from the network have been removed, and the features from the last max pooling layers have been considered for further analysis. Global average pooling <ref type="bibr" coords="3,340.69,459.46,18.03,10.91" target="#b10">[11]</ref> has been applied to the last max pool output to get a feature vector of length 512. The initial layers of a CNN extract low level features from the input. The subsequent layers extract more abstract high level features that represent complex patterns, shapes, and structures within the input. The handcrafted methods have been employed to extract the features specific to the medical image. Uniform Local Binary Pattern Histogram (LBPH) <ref type="bibr" coords="3,212.27,527.21,18.06,10.91" target="#b19">[20]</ref> identifies patterns in an image generating rotation invariant patterns. Uniform patterns primarily correspond to significant micro-textures such as lines, spots, corners, and similar features, and non-uniform patterns represent complex micro-textures like random patterns or network like structures etc. <ref type="bibr" coords="3,315.07,567.86,16.08,10.91" target="#b18">[19]</ref>. This operator is performed locally, and the histogram is calculated which represents the distribution of patterns in that region. Finally, all histograms are concatenated to get the final feature. To extract the features, the image has been divided into windows of size 20x20 considering an overlap of 8 pixels. The Gray Level Co-occurrence Matrix (GLCM) <ref type="bibr" coords="3,229.10,622.05,17.94,10.91" target="#b20">[21]</ref> is defined by calculating how often a pair of pixels with a specific value occur in the image. This matrix is analyzed to calculate statistical features like contrast, dissimilarity, correlation, energy, homogeneity and Angular Second Moment (ASM). The LBPH and GLCM features are concatenated to get a feature vector of length 3634. Principal Component Analysis (PCA), which is a dimentionality reduction method, is applied on the resulting features to get the final embedding of length 643. In figure <ref type="figure" coords="4,386.72,100.52,4.97,10.91" target="#fig_0">1</ref> VGG19, GLCM and LBPH blocks extract the respective image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Question feature extraction</head><p>In addition to the input image, the other input is a textual question about the image. Like image, the features extracted from the text is also important. The feature representation from the pre-trained BERT <ref type="bibr" coords="4,167.66,190.89,17.76,10.91" target="#b12">[13]</ref> model is used for various downstream tasks like text classification, named entity recognition, question answering, by fine tuning. Bio-BERT <ref type="bibr" coords="4,381.20,204.44,20.92,10.91" target="#b13">[14]</ref> is a transformer-based architecture with a stack of 12 encoder layers. These layers extract the contextual relations from the input sequence <ref type="bibr" coords="4,178.21,231.54,16.41,10.91" target="#b12">[13]</ref>. There are 12 attention heads in every encoder layer to perform the multi-head self-attention mechanism. The encoder layer operates on each token to generate a representation of length 768. The multiple heads operate in parallel, and its purpose is to extract different patterns and dependencies from the input sequence. The attention head outputs are combined and passed through a feed forward network generating the final feature representation of the encoder. In this work, the output of the last layer has been utilized generating a feature embedding of length 768. The Bio-BERT block in figure <ref type="figure" coords="4,338.91,312.83,5.07,10.91" target="#fig_0">1</ref> extracts question features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature fusion</head><p>The image and text feature embeddings individually capture the characteristics and context of the two modalities. The multimodal feature fusion methods aim to integrate these embeddings into a more detailed representation which will help in generating relevant answers for the VQA task. The Bilinear pooling operation generates a high dimensional feature vector by combining the two input feature vectors by taking their outer product. Multimodal Factorized Bilinear (MFB) pooling method <ref type="bibr" coords="4,191.19,430.31,12.74,10.91" target="#b8">[9]</ref> expands the individual feature representation to higher dimensional space and then uses factorization to get the compact representation. MFH <ref type="bibr" coords="4,410.19,443.85,17.76,10.91" target="#b9">[10]</ref> captures complex interactions between the features generating a compact representation of the feature vector. In VQA, through visual attention mechanism, the model focuses on specific segments of the image which are crucial for generating the answer of the question. In MFH with co-attention <ref type="bibr" coords="4,486.67,484.50,16.41,10.91" target="#b9">[10]</ref>, along with visual attention, question attention is also performed. Question attention focuses on specific parts of the question.</p><p>In this work, the question and images features have been fused using MFH with co-attention. Additionally, the deep learning and handcrafted features are merged using MFB pooling method to get a fused image feature of length 1000. Both methods are shown in figure <ref type="figure" coords="4,456.05,552.25,5.17,10.91" target="#fig_0">1</ref> in blocks MFH+CoAtt and MFB. Finally, the features obtained from the fusion of image and question are passed through a full-connected (FC) layer to classify and generate the answer. By learning the effective fused feature representation, the model can generate an appropriate prediction and classify accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset description</head><p>The VQAMed 2023 dataset <ref type="bibr" coords="5,207.38,402.66,17.75,10.91" target="#b17">[18]</ref> contains 2000 training images and 1938 test images. The dataset consists of GI tract images spanning from mouth to anus. Specifically, it includes images from various procedures such as gastroscopy, colonoscopy, and capsule endoscopy. A set of 18 questions has been defined, and every image is processed with the question set to generate the corresponding answers for each image. The questions within the dataset cover a wide range of topics, including abnormalities, surgical instruments, and normal findings related to factors like location, color, and more. The training set consists of 36000 unique image-QA pairs. For the testing data, answers are to be predicted for the same predefined set of questions for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Training</head><p>For every image, the answers of 18 predefined questions have been provided for the training set, and the unique entries are treated as a single label. The multimodal image and the corresponding question embeddings are fused to get a descriptive feature vector for the input image question pair. The classification layer utilizes the information encoded in the fused features and aims to predict the most relevant answer. The model has been trained for 30 epochs. L2 regularizer and dropouts have been used to avoid over-fitting, and the batch size is set to 64. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>The performance of the VQA model in the MEDVQA-GI <ref type="bibr" coords="6,351.82,382.35,18.07,10.91" target="#b17">[18]</ref> task was evaluated using the accuracy score metric <ref type="bibr" coords="6,187.46,395.90,16.13,10.91" target="#b17">[18]</ref>. The overall accuracy of the proposed model is 75.19%. Additionally, question-wise and image-wise accuracy was also measured. Table <ref type="table" coords="6,387.42,409.45,5.12,10.91" target="#tab_0">1</ref> shows the question-wise accuracy score, while the image-wise accuracy score is presented graphically in figure <ref type="figure" coords="6,486.79,423.00,5.17,10.91" target="#fig_1">2</ref> by grouping the accuracy scores in the range 0-0.5, 0.5-0.7 and 0.7-1. The question-wise accuracy reveal that the proposed model achieved comparatively high accuracy for the majority of questions. However, there were two specific questions, "What color is the abnormality?" and "Where in the image is the abnormality?" where the model's accuracy decreased noticeably. These questions were about color and location of abnormality and the model needs to be improved further to capture these details about abnormality region. Also, figure <ref type="figure" coords="6,444.68,504.30,5.03,10.91" target="#fig_1">2</ref> suggest that for 73.9% of all the images the image-wise accuracy score was more than 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, our model submitted for VQA subtask in the MEDVQA-GI challenge of Image-CLEF 2023 was described. The proposed model achieved an overall accuracy of 75.19%. Our model combined deep and domain-specific handcrafted features, to obtained effective feature representation for the input image. In addition, the text features were obtained from Bio-BERT model which is pre-trained on medical images. The fusion of the image and text features using MFH+CoAtt method generated a meaningful embedding for classification. Moving forward, the model's accuracy will be improved by exploring alternative feature extraction methods and evaluating its performance on additional datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,279.35,169.13,8.93;3,89.29,84.19,416.70,182.60"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed model block diagram</figDesc><graphic coords="3,89.29,84.19,416.70,182.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,324.18,182.29,8.93;6,183.05,84.19,229.18,227.43"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Image-wise accuracy of the model</figDesc><graphic coords="6,183.05,84.19,229.18,227.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,350.74,261.15"><head>Table 1</head><label>1</label><figDesc>Question-wise accuracy score of the model</figDesc><table coords="5,155.55,122.10,284.18,229.54"><row><cell>Question</cell><cell>Accuracy Score</cell></row><row><cell>What type of procedure is the image taken from?</cell><cell>0.983488</cell></row><row><cell>Have all polyps been removed?</cell><cell>0.881837</cell></row><row><cell>Is this finding easy to detect?</cell><cell>0.755934</cell></row><row><cell>Is there a green/black box artefact?</cell><cell>0.941692</cell></row><row><cell>Is there text?</cell><cell>0.931373</cell></row><row><cell>What color is the abnormality?</cell><cell>0.107843</cell></row><row><cell>What color is the anatomical landmark?</cell><cell>0.994840</cell></row><row><cell>How many findings are present?</cell><cell>0.748710</cell></row><row><cell>How many polyps are in the image?</cell><cell>0.863261</cell></row><row><cell>How many instruments are in the image?</cell><cell>0.873581</cell></row><row><cell>Where in the image is the abnormality?</cell><cell>0.083591</cell></row><row><cell>Where in the image is the instrument?</cell><cell>0.738390</cell></row><row><cell>Are there any abnormalities in the image?</cell><cell>0.782250</cell></row><row><cell>Are there any anatomical landmarks in the image?</cell><cell>0.763674</cell></row><row><cell>Are there any instruments in the image?</cell><cell>0.792054</cell></row><row><cell>Where in the image is the anatomical landmark?</cell><cell>0.789990</cell></row><row><cell>What is the size of the polyp?</cell><cell>0.737874</cell></row><row><cell>What type of polyp is present?</cell><cell>0.764706</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,145.59,393.32,10.91;7,112.66,159.14,219.82,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,400.40,145.59,105.58,10.91;7,112.66,159.14,89.78,10.91">Medical visual question answering: A survey</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10056</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,172.69,393.33,10.91;7,112.66,186.24,173.19,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="7,246.14,172.69,259.84,10.91;7,112.66,186.24,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,199.79,393.33,10.91;7,112.66,213.34,395.01,10.91;7,112.66,226.89,137.64,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,257.53,199.79,202.41,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,484.09,199.79,21.90,10.91;7,112.66,213.34,301.41,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,240.44,393.33,10.91;7,112.66,253.99,393.33,10.91;7,112.66,267.54,393.33,10.91;7,112.66,281.08,395.01,10.91;7,112.66,294.63,132.20,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,346.21,240.44,159.77,10.91;7,112.66,253.99,178.74,10.91">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-4012</idno>
		<ptr target="https://aclanthology.org/W14-4012.doi:10.3115/v1/W14-4012" />
	</analytic>
	<monogr>
		<title level="m" coord="7,313.82,253.99,192.17,10.91;7,112.66,267.54,393.33,10.91;7,112.66,281.08,47.51,10.91">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, Association for Computational Linguistics</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,308.18,393.98,10.91;7,112.41,321.73,48.96,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,248.39,308.18,108.87,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,366.74,308.18,91.27,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,335.28,394.53,10.91;7,112.66,348.83,394.53,10.91;7,112.66,362.38,393.32,10.91;7,112.66,375.93,394.03,10.91;7,112.66,389.48,330.77,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,176.01,348.83,105.33,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="7,314.47,362.38,191.52,10.91;7,112.66,375.93,34.49,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,403.03,393.33,10.91;7,112.66,416.58,395.17,10.91;7,112.66,430.13,394.53,10.91;7,112.66,443.67,184.83,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,252.29,403.03,253.69,10.91;7,112.66,416.58,36.77,10.91">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m" coord="7,291.15,416.58,216.68,10.91;7,112.66,430.13,92.21,10.91">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="7,371.28,430.13,131.29,10.91">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,457.22,394.53,10.91;7,112.66,470.77,393.32,10.91;7,112.33,484.32,393.90,10.91;7,112.66,497.87,395.00,10.91;7,112.66,511.42,227.77,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,480.15,457.22,27.04,10.91;7,112.66,470.77,304.95,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/xuc15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="7,141.74,484.32,312.04,10.91;7,124.94,498.89,186.37,9.72">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>PMLR, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="7,112.66,524.97,395.17,10.91;7,112.66,538.52,393.32,10.91;7,112.66,552.07,152.05,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,228.76,524.97,279.07,10.91;7,112.66,538.52,147.34,10.91">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,283.53,538.52,222.45,10.91;7,112.66,552.07,121.87,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,565.62,393.33,10.91;7,112.66,579.17,393.32,10.91;7,112.66,592.72,395.01,10.91;7,112.66,606.27,213.88,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,275.64,565.62,230.35,10.91;7,112.66,579.17,213.02,10.91">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2018.2817340</idno>
		<idno>doi:</idno>
		<ptr target="10.1109/tnnls.2018.2817340" />
	</analytic>
	<monogr>
		<title level="j" coord="7,334.43,579.17,171.55,10.91;7,112.66,592.72,100.92,10.91">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,619.81,320.28,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m" coord="7,220.06,619.81,88.34,10.91">Network in network</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,633.36,393.33,10.91;7,112.66,646.91,298.09,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,268.54,633.36,237.44,10.91;7,112.66,646.91,145.79,10.91">Zhejiang university at imageclef 2019 visual question answering in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,282.13,646.91,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,86.97,393.33,10.91;8,112.66,100.52,311.37,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="8,326.58,86.97,179.40,10.91;8,112.66,100.52,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,393.33,10.91;8,112.66,127.61,393.98,10.91;8,112.41,141.16,48.96,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,361.64,114.06,144.35,10.91;8,112.66,127.61,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,394.29,127.61,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,154.71,393.33,10.91;8,112.66,168.26,336.11,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,236.88,154.71,269.10,10.91;8,112.66,168.26,184.62,10.91">bumjunjung at vqa-med 2020: Vqa model based on feature extraction and multi-modal feature fusion</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,320.15,168.26,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,355.21,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,266.44,181.81,239.55,10.91;8,112.66,195.36,203.15,10.91">Yunnan university at vqa-med 2021: Pretrained biobert for medical domain visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,339.25,195.36,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,208.91,394.53,10.91;8,112.66,222.46,395.17,10.91;8,112.66,236.01,394.53,10.91;8,112.66,249.56,395.17,10.91;8,112.39,263.11,394.80,10.91;8,112.48,276.66,394.70,10.91;8,112.66,290.20,395.17,10.91;8,112.66,303.75,393.32,10.91;8,112.66,317.30,394.53,10.91;8,112.33,330.85,120.27,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,224.93,276.66,282.25,10.91;8,112.66,290.20,229.52,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, social media and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,364.96,290.20,142.87,10.91;8,112.66,303.75,393.32,10.91;8,112.66,317.30,134.07,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,277.25,317.30,220.25,10.91">Springer Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,393.73,10.91;8,112.66,357.95,394.52,10.91;8,112.66,371.50,394.52,10.91;8,112.66,385.05,58.60,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,463.26,344.40,43.14,10.91;8,112.66,357.95,390.64,10.91">Overview of imageclefmedical 2023 -medical visual question answering for gastrointestinal tract</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,127.16,371.50,111.10,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="8,245.61,371.50,173.62,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,398.60,393.33,10.91;8,112.66,412.15,395.17,10.91;8,112.66,425.70,395.01,10.91;8,112.66,439.25,119.84,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,351.54,398.60,154.45,10.91;8,112.66,412.15,395.17,10.91;8,112.66,425.70,88.54,10.91">Combining deep and handcrafted image features for presentation attack detection in face recognition systems using visiblelight camera sensors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">R</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18030699</idno>
		<ptr target="https://www.mdpi.com/1424-8220/18/3/699.doi:10.3390/s18030699" />
	</analytic>
	<monogr>
		<title level="j" coord="8,209.51,425.70,33.84,10.91">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,452.79,393.33,10.91;8,112.66,466.34,393.32,10.91;8,112.66,479.89,372.18,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,283.21,452.79,222.78,10.91;8,112.66,466.34,210.73,10.91">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2002.1017623</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,331.69,466.34,174.29,10.91;8,112.66,479.89,112.28,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,493.44,393.33,10.91;8,112.33,506.99,394.84,10.91;8,112.36,522.98,103.29,7.90" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,302.16,493.44,175.27,10.91">Textural features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.1973.4309314</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,484.95,493.44,21.04,10.91;8,112.33,506.99,241.56,10.91">IEEE Transactions on Systems, Man, and Cybernetics SMC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
