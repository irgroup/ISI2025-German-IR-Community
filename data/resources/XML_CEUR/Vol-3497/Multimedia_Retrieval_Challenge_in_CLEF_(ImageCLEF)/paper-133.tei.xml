<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.05,75.20,450.99,16.92;1,72.05,96.08,451.25,16.92;1,72.05,116.60,134.72,16.92">SSN MLRG at ImageCLEFmedical Caption 2023: Automatic Concept Detection and Caption Prediction using ConceptNet and Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.05,159.28,66.88,11.88"><forename type="first">Sheerin</forename><surname>Sitara</surname></persName>
							<email>sheerinsitaran@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,141.94,159.28,76.95,11.88"><forename type="first">Noor</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.13,159.28,89.22,11.88"><forename type="first">Kavitha</forename><surname>Srinivasan</surname></persName>
							<email>kavithas@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.05,75.20,450.99,16.92;1,72.05,96.08,451.25,16.92;1,72.05,116.60,134.72,16.92">SSN MLRG at ImageCLEFmedical Caption 2023: Automatic Concept Detection and Caption Prediction using ConceptNet and Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63FB35FA90B3453484ED770B6A6FBD29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>caption prediction</term>
					<term>concept detection</term>
					<term>ViT</term>
					<term>multilabel classification</term>
					<term>image captioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An automatic image captioning attains tremendous advancement in the last few years. However most of the medical data are, publicly unavailable and exist in unstructured and unlabelled format are real challenges in developing the medical system. To address these issues, ImageCLEF forum is conducting many tasks on the medical domain from 2016 onwards. This year one of the tasks is medical concept detection and caption prediction. For this task, our team has proposed two techniques using ConceptNet and Vision Transformer (ViT). The concept detection models are developed using multi-label classification and resulted the F1score and secondary F1-score as 0.464 and 0.860 respectively. The caption prediction models are implemented using Vision Transformer (ViT), which resulted a BERT and CLIPScore of 0.544 and 0.687 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Images are extensively used for conveying enormous amounts of information over internet and social media and hence there is an increasing demand for image data analytics for designing efficient information processing systems. This leads to the development of systems with capability to automatically analyze the scenario contained in the image and to express it in meaningful natural language sentences. Image caption generation is an integral part of many useful systems and applications such as visual question answering, surveillance video analysis, video captioning, automatic image retrieval, assistance for visually impaired people, biomedical imaging, robotics and so on. A good captioning system is capable of highlighting the contextual information in the image similar to human cognitive system. In the recent years, several techniques for automatic caption generation in images have been proposed that can effectively solve many computer vision challenges related to medical image captioning. The different types of medical imaging modalities are Computed Tomography (CT), X-Ray (XR), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), angiogram, mammogram and ultrasound. The ImageCLEF forum <ref type="bibr" coords="1,390.84,622.31,13.07,10.04" target="#b0">[1]</ref> is conducting various tasks related to medical images such as caption prediction, concept detection, Tuberculosis (TB) type detection, Multi-Drug Resistant (MDR) detection, TB severity score calculation, CT report generation and Visual Question Answering (VQA) from 2016 onwards. In this, we have participated in concept detection and caption prediction tasks during the current year.</p><p>In concept detection and caption prediction tasks <ref type="bibr" coords="2,309.79,109.43,11.55,10.04" target="#b1">[2]</ref>, the dataset given by ImageCLEF consists of different modalities such as CT, XR, PET, angiogram and ultrasound images. The concepts and captions corresponds to these images are created by medical annotator from PubMED articles and Unified Medical Language System (UMLS) terms. Finally, these datasets are validated and verified by medical domain experts. These datasets are used to develop concept detection and caption prediction model using suitable techniques and evaluated performance metrics, are discussed in the following paragraphs.</p><p>The concept detection approaches are: information retrieval <ref type="bibr" coords="2,373.05,199.48,12.70,10.04" target="#b2">[3]</ref> and multi-label classification approaches. Among these approaches, multi-label classification using ConceptNet <ref type="bibr" coords="2,437.30,213.16,13.07,10.04" target="#b3">[4]</ref> are used for this task execution. The multi-label classification approach (model 1) is chosen because it has an ability to find conditional dependencies between the labels and the independence between the labels are computed based on the specific condition. The caption prediction techniques are grouped into, (i). Techniques which performs both image and test processing like Visual Transformers (ViT) <ref type="bibr" coords="2,487.93,267.54,11.95,10.04" target="#b4">[5]</ref>, (ii). Combination of techniques for image processing and text processing using deep learning techniques <ref type="bibr" coords="2,72.05,294.92,11.81,10.04" target="#b5">[6]</ref>. Among the techniques, ViT is chosen because it can be applied to low-level vision, segmentation, and multi-modality.</p><p>The performance metrics given by ImageCLEF for evaluating concept detection and caption prediction tasks are: F1 Score <ref type="bibr" coords="2,205.68,343.88,11.56,10.04" target="#b6">[7]</ref>, BiLingual Evaluation Understudy (BLEU) <ref type="bibr" coords="2,414.26,343.88,11.54,10.04" target="#b7">[8]</ref>, Bilingual Evaluation Understudy (BLEURT) <ref type="bibr" coords="2,180.12,357.59,12.70,10.04" target="#b8">[9]</ref> score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) <ref type="bibr" coords="2,502.87,357.59,16.92,10.04" target="#b9">[10]</ref>, Metric for Evaluation of Translation with Explicit Ordering (METEOR) <ref type="bibr" coords="2,393.72,371.27,16.94,10.04" target="#b10">[11]</ref>, Consensus-based Image Description Evaluation (CIDEr) <ref type="bibr" coords="2,215.06,384.95,16.90,10.04" target="#b11">[12]</ref>, Semantic Propositional Image Caption Evaluation (SPICE) <ref type="bibr" coords="2,502.87,384.95,16.92,10.04" target="#b12">[13]</ref>, CLIPScore <ref type="bibr" coords="2,125.35,398.63,18.47,10.04" target="#b13">[14]</ref> and Bidirectional Encoder Representations from Transformers (BERT) Score <ref type="bibr" coords="2,502.87,398.63,16.63,10.04">[15]</ref>. Among these, except F1-score, all other performance metrics are used to evaluate the results of caption prediction tasks.</p><p>The remaining part of the paper are discussed with following subsections. In Section 2, the concept detection and caption prediction tasks and datasets are discussed. The design of the proposed system is explained in Section 3. A brief summary about the implementation, result and the evaluation of all runs for both tasks are given in Section 4 and, conclusion and future work is summarized at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Dataset Description</head><p>In this section, two sub tasks of image captioning and given datasets are discussed. The two sub tasks includes, ImageCLEF concept detection and caption prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ImageCLEF Concept Detection and Caption Prediction Task</head><p>The automatic concept detection and caption prediction are identifying the presence and location of relevant concepts in a large corpus of medical images. Based on the visual image content, this sub task provides the building blocks for the scene understanding step by identifying the individual components from which captions are composed. The concepts can be further applied for context-based image and information retrieval purposes. On the basis of the concept vocabulary detected in the first sub task as well as the visual information of their interaction in the image, participating systems are tasked with composing coherent captions for the entirety of an image. In this step, rather than the mere coverage of visual concepts, detecting the interplay of visible elements is crucial for strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ImageCLEF Concept Detection and Caption Prediction dataset</head><p>The concept detection and caption prediction datasets comprises of training set, validation set and test set and it is represented in terms of number of images, concepts and captions in Table <ref type="table" coords="3,459.84,118.79,4.27,10.04" target="#tab_0">1</ref>. The number of images are equally distributed as 75%, 12.5% and 12.5% for training set, validation set and test set for both datasets. In concept detection dataset, each image corresponds to one or more concept IDs and each concept ID represents one concept names. In caption prediction dataset, each image corresponds to only one caption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Design</head><p>The system design of the proposed concept detection and caption prediction tasks are shown in Figure <ref type="figure" coords="3,104.78,331.64,5.58,10.04" target="#fig_0">1</ref> and 2. In Figure <ref type="figure" coords="3,191.45,331.64,4.27,10.04" target="#fig_0">1</ref>, multilabel classification based concept detection models are developed based on images, concept IDs and concept names in the training phase and, the generated model is validated by detecting the concept for the radiology images in the test set. In Figure <ref type="figure" coords="3,477.66,359.03,4.10,10.04" target="#fig_1">2</ref>, caption prediction models are developed using ViT, based on the radiology images and captions in the training phase. The generated caption prediction model is validated by predicting the caption for the medical images in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Detection</head><p>The concept detection model is developed by multi-label classification using ConceptNet <ref type="bibr" coords="3,488.47,458.80,11.54,10.04" target="#b3">[4]</ref>. The multi-label classification system predicts all the suitable concepts which has probability value greater than criterion value for each image in the test set. In the training phase, based on the frequency, the high-level semantic type on the Top-100 concepts are extracted. Then image features are extracted using ConceptNet and mapped with respective concepts in the training phase. For this, all layers except the last layer in ConceptNet is freeze so weights remain same throughout the training and only the weights from added layers updates gradually. Then global average pooling, dense layer with sigmoid activation function are added after the last layer which predicts the probability value for each image. The global average pooling used in this model creation acts as a great alternative for Convolutional Neural network (CNN) because it generates the one feature map for each corresponding concept category. The number of nodes in the dense layer is maintain to be equal to the number of concept names then only each node in this layer generates the probability value for each concept with respect to the image. Among the probability value, the concept which has probability value greater than criterion value is considered as the predicted concept. Moreover, the model is fine-tuned by minimizing the mean square error between the predicted and ground truth value. Then in the testing phase, the generated model is evaluated for radiology images in the test set which detects one or more concepts for each images.</p><p>The concepts extracted for test set under different criteria are combined by (i). union of union (i.e., merging list of concepts from two results), (ii). Intersection of intersection (i.e., merging the common concepts from two results). Regarding the training process, these models were trained during 10 epochs, using the Adam optimizer with a learning rate of 10-4. These models also used the "2 Phases" strategy, where the backbone layers are frozen for 5 epochs and then unfroze them for the remaining epochs. The best model is saved based on the lowest validation loss and its system design is shown in Figure <ref type="figure" coords="3,496.88,753.75,4.27,10.04" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caption Prediction</head><p>The caption prediction system is developed using Vision Transformer (ViT). Because ViT uses selfattention mechanism and it derives information from whole image. This pre-trained transformer based model generates the caption based on the context of the image. The system design is shown in Figure <ref type="figure" coords="4,517.63,290.58,5.58,10.04" target="#fig_1">2</ref> for better understanding. The encoder in ViT receives as input an image divided into patches of 16×16 pixels. The decoder receives the ground-truth caption as input (i.e., training is done using teacher forcing) and the encoder hidden states as inputs to the cross attention layers. The model is trained autoregressively for next token prediction using causal (or unidirectional) self-attention, which means that a given token can only attend to previous tokens. The model was implemented using the Vision Encoder Decoder class from the Hugging Face Transformers library and chose a tiny Data-efficient image Transformer (DeiT) pretrained on ImageNet for the encoder. For the decode, leverage pretrained weights from the Distilled-GPT2, a distilled version of the GPT-2 architecture are used. This model is initially trained for 20 epochs, and then for an additional 20 epochs starting from the checkpoint with the lowest validation loss. The AdamW optimiser with an initial learning rate of 5×10-5, linearly decayed are chosen. Due to limitations of the computational resources available unable to fine tune the model using self-critical sequence training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>The hardware and software required for the implementation of concept detection and caption prediction model includes, (i). Intel i5 processor with NVIDIA GeForce Ti 4800 at 4.3GHZ clock speed, 16GB RAM, Graphical Processing Unit and 2TB disk space, (ii). Linux -Ubuntu 20.04 operating system, Python 3.7 package with required libraries like tensorflow 2.13.0, torch 2.0, sklearn 0.20, nltk, pickle, pandas, etc., Among the results, run1 obtained better performance value in terms of F1 score and F1 score manual and it is italicized in Table <ref type="table" coords="5,193.85,109.43,4.27,10.04" target="#tab_1">2</ref>. From run2, it has been inferred that considering top-100 concepts gives better result. In concept detection task, we have submitted two successful submissions and achieved seventh rank in ImageCLEF 2023 Concept Detection task. Among the results, run1 obtained better performance value in terms of F1 score and F1 score manual and it is italicized in Table <ref type="table" coords="5,193.85,465.64,4.27,10.04" target="#tab_1">2</ref>. From run2, it has been inferred that considering top-100 concepts gives better result. In concept detection task, we have submitted two successful submissions and achieved seventh rank in ImageCLEF 2023 Concept Detection task. The overall ranking achieved by top teams are listed in Table <ref type="table" coords="5,154.09,507.04,4.14,10.04">3</ref>.</p><p>The brief description about each run are listed in Table <ref type="table" coords="5,340.37,528.66,4.27,10.04" target="#tab_2">4</ref>. The description about each run are as follows. In run1, the number of epochs is fixed to be 20, used adam optimizer and maintain learning rate to be 0.005, batch size is 32. The run2 is same as run1, but the learning rate is reduced to 0.001 and used Stochastic Gradient Descent (SGD) optimizer and number of epochs fixed to be 40. From the results, it has been inferred that run4 achieved better performance value and it is italicized in Table <ref type="table" coords="5,515.40,579.44,4.31,10.04" target="#tab_2">4</ref>. The overall results show that the lowest learning rate, epochs and SGD gives better result. The overall ranking achieved by top teams are given in Table <ref type="table" coords="5,291.75,604.64,4.30,10.04" target="#tab_3">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, medical concept detection and caption prediction models are developed for ImageCLEF image captioning task. The concept detection tasks are implemented by multi-label classification approach by ConceptNet by considering only the top 100 concepts. The caption prediction task is implemented by ViT. From the results of these models, it has been inferred that multi-label classification approach using ConceptNet by considering only the top 100 concepts gives better results. As compared with the best scores given by ImageCLEF, the proposed concept detection model lacks only by 0.058 and 0.065 in terms of F1-score and F1-score manual respectively. And the proposed caption prediction model lacks only by 0.098 in terms of BLEU score. In future work, the performance can be enhanced by Generative Pre-trained Transformer instead of BERT. The overall performance of the system can be improved by reducing the irrelevant samples, increasing the number of epochs and maintaining the minimum learning rate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.05,205.21,276.51,11.16;4,72.00,72.00,455.40,122.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System design of proposed concept detection model</figDesc><graphic coords="4,72.00,72.00,455.40,122.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.05,620.45,277.73,11.16;4,72.00,465.12,452.52,143.28"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System design of proposed caption prediction model</figDesc><graphic coords="4,72.00,465.12,452.52,143.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.05,195.13,445.46,74.90"><head>Table 1</head><label>1</label><figDesc>Dataset description for concept detection and caption prediction</figDesc><table coords="3,77.45,231.49,440.06,38.54"><row><cell>Task</cell><cell>Input/Output</cell><cell cols="4">Training Set Validation Set Test Set Total</cell></row><row><cell>Concept Detection/</cell><cell>#Images</cell><cell>60918</cell><cell>10437</cell><cell>10473</cell><cell>81828</cell></row><row><cell>Caption Prediction</cell><cell>#Concept/#Captions</cell><cell>60918</cell><cell>10437</cell><cell>-</cell><cell>71355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.05,158.38,399.74,269.40"><head>Table 2</head><label>2</label><figDesc>Brief description about each runs</figDesc><table coords="5,72.05,194.77,399.74,233.01"><row><cell>Run Number</cell><cell>F1-Score</cell><cell cols="2">F1-Score Manual</cell></row><row><cell>1</cell><cell>0.464</cell><cell></cell><cell>0.860</cell></row><row><cell>2</cell><cell>0.461</cell><cell></cell><cell>0.856</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Top ranking of ImageCLEF 2023 concept detection task</cell><cell></cell><cell></cell></row><row><cell>Team Name</cell><cell>F1-Score</cell><cell>F1-Score Manual</cell><cell>Rank</cell></row><row><cell>AUEB-NLP-Group</cell><cell>0.522</cell><cell>0.925</cell><cell>1</cell></row><row><cell>KDE-Lab_Med</cell><cell>0.507</cell><cell>0.932</cell><cell>2</cell></row><row><cell>VCMI</cell><cell>0.499</cell><cell>0.916</cell><cell>3</cell></row><row><cell>IUST_NLPLAB</cell><cell>0.495</cell><cell>0.880</cell><cell>4</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>0.495</cell><cell>0.910</cell><cell>5</cell></row><row><cell>CS_Morgan</cell><cell>0.483</cell><cell>0.890</cell><cell>6</cell></row><row><cell>SSNSheerinKavitha</cell><cell>0.464</cell><cell>0.860</cell><cell>7</cell></row><row><cell>closeAI2023</cell><cell>0.448</cell><cell>0.856</cell><cell>8</cell></row><row><cell>SSN_MLRG</cell><cell>0.017</cell><cell>0.112</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.05,629.48,440.35,83.69"><head>Table 4</head><label>4</label><figDesc>Brief description about each runs</figDesc><table coords="5,77.45,665.84,434.95,47.33"><row><cell>Run Number</cell><cell>BERTScore</cell><cell>ROUGE</cell><cell>BLEURT</cell><cell>BLEU</cell><cell>METEOR</cell><cell cols="2">CIDEr CLIPScore</cell></row><row><cell>3</cell><cell>0.543</cell><cell>0.086</cell><cell>0.215</cell><cell>0.074</cell><cell>0.025</cell><cell>0.014</cell><cell>0.685</cell></row><row><cell>4</cell><cell>0.544</cell><cell>0.086</cell><cell>0.215</cell><cell>0.074</cell><cell>0.025</cell><cell>0.014</cell><cell>0.687</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.05,74.12,439.76,256.92"><head>Table 5</head><label>5</label><figDesc>Top ranking of ImageCLEF 2023 caption prediction task</figDesc><table coords="6,77.45,110.48,434.36,220.56"><row><cell>Team Name</cell><cell>BERT Score</cell><cell>ROUGE</cell><cell>BLEURT</cell><cell>BLEU</cell><cell>METEOR</cell><cell>CIDEr</cell><cell>Score CLIP</cell><cell>Rank</cell></row><row><cell>CSIRO</cell><cell>0.642</cell><cell>0.244</cell><cell>0.313</cell><cell>0.161</cell><cell>0.079</cell><cell>0.202</cell><cell>0.814</cell><cell>1</cell></row><row><cell>closeAI2023</cell><cell>0.628</cell><cell>0.240</cell><cell>0.320</cell><cell>0.184</cell><cell>0.087</cell><cell>0.237</cell><cell>0.807</cell><cell>2</cell></row><row><cell>AUEB-NLP-Group</cell><cell>0.617</cell><cell>0.213</cell><cell>0.295</cell><cell>0.169</cell><cell>0.071</cell><cell>0.146</cell><cell>0.803</cell><cell>3</cell></row><row><cell>PCLmed</cell><cell>0.615</cell><cell>0.252</cell><cell>0.316</cell><cell>0.217</cell><cell>0.092</cell><cell>0.231</cell><cell>0.802</cell><cell>4</cell></row><row><cell>VCMI</cell><cell>0.614</cell><cell>0.217</cell><cell>0.308</cell><cell>0.165</cell><cell>0.073</cell><cell>0.172</cell><cell>0.808</cell><cell>5</cell></row><row><cell>KDE-Lab_Med</cell><cell>0.614</cell><cell>0.222</cell><cell>0.301</cell><cell>0.156</cell><cell>0.072</cell><cell>0.181</cell><cell>0.806</cell><cell>6</cell></row><row><cell>SSN_MLRG</cell><cell>0.601</cell><cell>0.211</cell><cell>0.277</cell><cell>0.141</cell><cell>0.061</cell><cell>0.128</cell><cell>0.775</cell><cell>7</cell></row><row><cell>DLNU_CCSE</cell><cell>0.600</cell><cell>0.202</cell><cell>0.262</cell><cell>0.105</cell><cell>0.055</cell><cell>0.133</cell><cell>0.772</cell><cell>8</cell></row><row><cell>CS_Morgan</cell><cell>0.581</cell><cell>0.156</cell><cell>0.224</cell><cell>0.056</cell><cell>0.043</cell><cell>0.083</cell><cell>0.759</cell><cell>9</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>0.581</cell><cell>0.218</cell><cell>0.269</cell><cell>0.145</cell><cell>0.070</cell><cell>0.173</cell><cell>0.789</cell><cell>10</cell></row><row><cell>Bluefield-2023</cell><cell>0.577</cell><cell>0.153</cell><cell>0.271</cell><cell>0.154</cell><cell>0.060</cell><cell>0.100</cell><cell>0.783</cell><cell>11</cell></row><row><cell>IUST_NLPLAB</cell><cell>0.566</cell><cell>0.289</cell><cell>0.222</cell><cell>0.268</cell><cell>0.100</cell><cell>0.177</cell><cell>0.806</cell><cell>12</cell></row><row><cell>SSNSheerinK avitha</cell><cell>0.544</cell><cell>0.086</cell><cell>0.215</cell><cell>0.074</cell><cell>0.025</cell><cell cols="2">0.014 0.687</cell><cell>13</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>Our profound gratitude to <rs type="institution">Sri Sivasubramaniya Nadar College</rs> <rs type="affiliation">of Engineering, Department of CSE</rs>, for allowing us to utilize the <rs type="institution">High Performance Computing Laboratory</rs> and GPU Server for the execution of this challenge successfully.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,108.07,660.00,415.82,10.15;6,108.07,673.79,415.26,10.04;6,108.07,687.49,415.43,10.04;6,108.07,701.17,415.15,10.04;6,108.07,714.85,415.53,10.04;6,108.07,728.53,414.97,10.04;6,108.07,741.76,415.50,10.16;6,108.07,755.55,415.42,10.04;7,108.07,74.15,415.26,10.04;7,108.07,87.83,415.20,10.04;7,108.07,101.51,415.07,10.04;7,108.07,115.19,34.44,10.04;7,228.31,115.19,30.85,10.04;7,344.96,115.19,58.90,10.04;7,489.68,115.19,33.74,10.04;7,110.95,128.89,105.77,10.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,239.90,755.55,283.59,10.04;7,108.07,74.15,289.08,10.04">Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,417.96,74.15,105.37,10.04;7,108.07,87.83,415.20,10.04;7,108.07,101.51,207.84,10.04">Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,354.31,101.51,168.83,10.04;7,108.07,115.19,34.44,10.04;7,228.31,115.19,24.68,10.04">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 14th International Conference of the CLEF Association (CLEF<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09-18">2023. September 18-21, 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,108.07,142.57,414.97,10.04;7,108.07,156.25,415.68,10.04;7,108.07,169.46,415.46,10.16;7,108.07,183.28,414.87,10.04;7,108.07,196.96,340.63,10.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,108.07,169.46,396.30,10.16">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,108.07,183.28,414.87,10.04;7,108.07,196.96,122.51,10.04">Experimental IR Meets Multilinguality, Multimodality, and Interaction. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,210.64,415.68,10.04;7,108.07,224.32,415.02,10.04;7,108.07,238.02,415.29,10.04;7,108.07,251.70,376.95,10.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,310.19,210.64,213.56,10.04;7,108.07,224.32,410.38,10.04">Analysis and Implementation of the Bray-Curtis Distance-Based Similarity Measure for Retrieving Information from the Medical Repository</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bala</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-13-2354-6_14</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-981-13-2354-6_14" />
	</analytic>
	<monogr>
		<title level="m" coord="7,123.52,238.02,322.38,10.04">International Conference on Innovative Computing and Communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,265.38,415.71,10.04;7,108.07,279.06,415.50,10.04;7,108.07,292.76,194.43,10.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,413.18,265.38,110.60,10.04;7,108.07,279.06,415.50,10.04;7,108.07,292.76,115.96,10.04">Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Allot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Islamaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,231.62,292.76,38.35,10.04">Database</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,306.44,415.59,10.04;7,108.07,320.12,415.93,10.04;7,108.07,333.44,78.08,10.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,321.29,306.44,198.25,10.04">Vision transformer with deformable attention</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,120.31,320.12,382.02,10.04">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4794" to="4803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,347.51,415.18,10.04;7,108.07,360.11,415.07,10.04;7,108.07,373.07,231.90,10.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,127.15,360.11,391.12,10.04">Sentiment Analysis of Image with Text Caption using Deep Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Chaubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Asha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Guptav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alhassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,108.07,373.07,199.67,10.04">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,385.31,415.57,10.04;7,108.07,398.99,415.34,10.04;7,108.07,412.69,414.97,10.04;7,108.07,426.26,99.65,10.16" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,296.47,385.31,227.17,10.04;7,108.07,398.99,415.34,10.04;7,108.07,412.69,88.07,10.04">SSN MLRG at ImageCLEFmedical caption 2022: Medical concept detection and caption prediction using transfer learning and transformer based learning approaches</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S N</forename><surname>Mohameda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasanb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,221.78,412.69,224.41,10.04">Proceedings of the working notes of CLEF 2022</title>
		<meeting>the working notes of CLEF 2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-08">September 5 -8 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,439.69,415.48,10.04;7,108.07,453.37,415.64,10.04;7,108.07,467.08,183.24,10.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,332.50,439.69,191.05,10.04;7,108.07,453.37,88.26,10.04">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,221.54,453.37,302.17,10.04;7,108.07,467.08,117.25,10.04">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,480.76,415.28,10.04;7,108.07,494.80,148.69,10.04" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,276.65,480.76,242.23,10.04">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04696</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,106.99,516.42,411.22,10.04;7,112.39,529.02,178.92,10.04" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,212.73,516.42,262.67,10.04">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,497.83,516.42,20.38,10.04;7,112.39,529.02,123.63,10.04">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004-07">2004, July</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.63,541.98,416.91,10.04;7,105.19,554.58,418.29,10.04;7,102.67,567.20,421.34,10.04" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,279.47,541.98,244.07,10.04;7,105.19,554.58,196.00,10.04">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,325.63,554.58,197.85,10.04;7,102.67,567.20,366.75,10.04">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005-06">2005, June</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.35,579.80,411.89,10.04;7,108.07,592.40,281.24,10.04" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,363.10,579.80,156.14,10.04;7,108.07,592.40,125.33,10.04">CIDEr-R: Robust consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">O D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Colombini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13701</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,106.99,605.00,407.98,10.04;7,108.07,617.60,406.54,10.04;7,108.07,630.23,415.58,10.04;7,108.07,643.19,148.83,10.04" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,383.14,605.00,131.83,10.04;7,108.07,617.60,108.78,10.04">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,238.82,617.60,270.93,10.04">Computer Vision-ECCV 2016: 14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-10-11">2016. October 11-14, 2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.35,655.79,413.64,10.04;7,105.55,668.39,344.65,10.04" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,417.47,655.79,103.51,10.04;7,105.55,668.39,188.60,10.04">Clipscore: A referencefree evaluation metric for image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
