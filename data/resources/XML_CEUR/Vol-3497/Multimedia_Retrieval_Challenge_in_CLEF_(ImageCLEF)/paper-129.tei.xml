<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.19,345.98,15.42;1,89.29,106.11,399.52,15.42;1,89.29,128.02,343.24,15.43">UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering</title>
				<funder>
					<orgName type="full">VNUHCM University of Information Technology&apos;s Scientific Research Support Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,156.34,64.62,11.96"><forename type="first">Triet</forename><forename type="middle">M</forename><surname>Thai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Unviversity of Information Technology</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.48,156.34,49.08,11.96"><forename type="first">Anh</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Unviversity of Information Technology</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.48,156.34,58.51,11.96"><forename type="first">Hao</forename><forename type="middle">K</forename><surname>Tieu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Unviversity of Information Technology</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.92,156.34,64.86,11.96"><forename type="first">Linh</forename><forename type="middle">N P</forename><surname>Bui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Unviversity of Information Technology</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,88.93,170.29,91.85,11.96"><forename type="first">Thien</forename><forename type="middle">T B</forename><surname>Nguyen</surname></persName>
							<email>thienntb@uit.edu.vn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Science and Engineering</orgName>
								<orgName type="institution">Unviversity of Information Technology</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<addrLine>Ho Chi</addrLine>
									<settlement>Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.19,345.98,15.42;1,89.29,106.11,399.52,15.42;1,89.29,128.02,343.24,15.43">UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B5B7E2ACC591FB3F931522A631C33189</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual question answering</term>
					<term>multimodal learning</term>
					<term>BERT</term>
					<term>pre-trained models</term>
					<term>gastrointestinal imaging</term>
					<term>colonoscopy analysis</term>
					<term>medical image processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, artificial intelligence has played an important role in medicine and disease diagnosis, with many applications to be mentioned, one of which is Medical Visual Question Answering (MedVQA). By combining computer vision and natural language processing, MedVQA systems can assist experts in extracting relevant information from medical image based on a given question and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023 challenge carried out a visual question answering (VQA) task in the gastrointestinal domain, which includes gastroscopy and colonoscopy images. Our team approached Task 1 -Visual Question Answering of the challenge by proposing a multimodal learning method with image enhancement to improve the VQA performance on gastrointestinal images. The multimodal architecture is set up with a BERT encoder and different pre-trained vision models based on convolutional neural network (CNN) and Transformer architecture for features extraction from question and endoscopy image. The result of this study highlights the dominance of Transformer-based vision models over the CNNs and demonstrates the effectiveness of the image enhancement process, with six out of the eight vision models achieving better F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the development test set, while also producing good result on the private test set with accuracy of 82.01%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The digestive system is one of the most complex and essential systems in the human body, consisting of various organs such as the mouth, stomach, intestines, and rectum. From the process of digestion in the stomach to the absorption of nutrients in the small and large intestines, and finally the elimination of waste through the rectum, the entire process involves the interaction and coordination of each organ to ensure the supply of nutrients and energy to the body. Any issues that occur in any part of the digestive system can directly impact the entire and enhanced image data. The final results, with accuracy up to 87.25% on the development test set and 82.01% on the private test set, demonstrate the potential of the proposed method in improving the performance of VQA systems in the field of gastrointestinal endoscopy imaging in general and colonoscopy in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Colonoscopy Image Analysis</head><p>With the advancement of modern advanced technology, AI has made significant contributions to the field of healthcare, specifically in the progress of the colonoscopy examination process. Currently, two potential approaches with AI being utilized for colonoscopy image analysis, including Computer-Aided Detection (CAD) and Deep Learning (DL) systems. In the CAD approach, the system utilizes image processing algorithms to improve the performance of endoscopic procedures, enabling physicians to easily detect lesions in hard-to-identify locations and reduce the chances of misdiagnosis <ref type="bibr" coords="3,264.35,288.50,11.28,10.91" target="#b3">[4]</ref>. On the other hand, the DL-based system employs a deep learning model trained on specific datasets, which enhances the accuracy of lesion detection compared to the CAD-based system <ref type="bibr" coords="3,247.77,315.60,11.28,10.91" target="#b4">[5]</ref>. However, developing algorithms for automatic analysis and anomaly detection in endoscopic images requires preliminary image preprocessing to address various factors, such as specular highlights, interlacing or artefacts that impact the system's performance <ref type="bibr" coords="3,189.67,356.25,11.43,10.91" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing Methods for Colonoscopy Images</head><p>In reality, the quality of endoscopy images depends on various factors such as the skill of the performing physician, limitations of the equipment, and certain environmental conditions. Some common difficulties in processing endoscopy images include black masks, ghost colors, interlacing, specular highlights, and uneven lighting <ref type="bibr" coords="3,326.69,446.62,11.49,10.91" target="#b6">[7]</ref>. Black masks are the occurrence of a black border around the edges of the image due to the use of lenses in the endoscopy system that have a black frame surrounding the edges. This frame can hinder the development of algorithms. To address this issue, techniques such as restoration, thresholding, cropping, or inpainting are necessary. Specular highlights, which are bright spots reflected from tumors or polyps captured by the camera, can disrupt the algorithms. Therefore, to remove them, we can employ detection or inpainting methods. Additionally, for issues like interlacing, ghost colors, and uneven lighting, segmentation methods can be applied to achieve optimal results <ref type="bibr" coords="3,493.30,541.47,12.68,10.91" target="#b5">[6]</ref> [8] <ref type="bibr" coords="3,104.84,555.02,11.40,10.91" target="#b8">[9]</ref>. Overall, preprocessing steps play a crucial role in mitigating the challenges commonly encountered with colonoscopy images. The mentioned techniques will help improve the overall quality of the images, thereby enhancing the performance of analysis and diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Medical Visual Question Answering</head><p>Medical visual question answering (MedVQA) is an important field in medical AI that combines VQA challenges with healthcare applications. By integrating medical images and clinically relevant questions, MedVQA systems aim to provide plausible and convincing answers. While VQA has been extensively studied in general domains, MedVQA presents unique opportunities for exploration. Currently, there are 8 publicly available MedVQA datasets, including VQA-MED-2018 <ref type="bibr" coords="4,111.12,114.06,16.09,10.91" target="#b9">[10]</ref>, VQA-RAD <ref type="bibr" coords="4,180.90,114.06,16.09,10.91" target="#b10">[11]</ref>, VQA-MED-2019 <ref type="bibr" coords="4,275.40,114.06,16.09,10.91" target="#b11">[12]</ref>, RadVisDial <ref type="bibr" coords="4,348.26,114.06,16.09,10.91" target="#b12">[13]</ref>, PathVQA <ref type="bibr" coords="4,413.43,114.06,16.09,10.91" target="#b13">[14]</ref>, VQA-MED-2020 <ref type="bibr" coords="4,89.29,127.61,16.41,10.91" target="#b14">[15]</ref>, SLAKE <ref type="bibr" coords="4,148.93,127.61,16.41,10.91" target="#b15">[16]</ref>, and VQA-MED-2021 <ref type="bibr" coords="4,269.62,127.61,16.41,10.91" target="#b14">[15]</ref>. These datasets serve as valuable resources for advancing MedVQA research.</p><p>The basic framework of MedVQA systems typically contains an image encoder, a question encoder, a fusion algorithm, and an answering component. Other frameworks may exclude the question encoder when the question is simple. Common choices for image encoder are ResNet <ref type="bibr" coords="4,124.76,195.36,18.07,10.91" target="#b16">[17]</ref> and VGGNet <ref type="bibr" coords="4,208.08,195.36,18.07,10.91" target="#b17">[18]</ref> that are pre-trained on ImageNet dataset <ref type="bibr" coords="4,420.72,195.36,16.41,10.91" target="#b18">[19]</ref>. For language encoders, Transformer-based architectures such as BERT <ref type="bibr" coords="4,345.75,208.91,17.91,10.91" target="#b19">[20]</ref> or BioBERT <ref type="bibr" coords="4,421.72,208.91,17.91,10.91" target="#b20">[21]</ref> are commonly applied because of their proven advantages, besides the Recurrent Neural Networks (LSTM <ref type="bibr" coords="4,89.29,236.01,16.41,10.91" target="#b21">[22]</ref>, Bi-LSTM <ref type="bibr" coords="4,156.68,236.01,16.41,10.91" target="#b22">[23]</ref>, GRU <ref type="bibr" coords="4,204.99,236.01,15.89,10.91" target="#b23">[24]</ref>). The fusion stage, the core component of VQA methods, has typical fusion algorithms, including the attention mechanism and the pooling module. Common attention mechanisms are the Stacked Attention Networks (SAN) <ref type="bibr" coords="4,384.48,263.11,16.31,10.91" target="#b24">[25]</ref>, the Bilinear Attention Networks (BAN) <ref type="bibr" coords="4,165.66,276.66,16.18,10.91" target="#b25">[26]</ref>, or the Hierarchical Question-Image Co-Attention (HieCoAtt) <ref type="bibr" coords="4,460.39,276.66,16.17,10.91" target="#b26">[27]</ref>. Most multimodal pooling practices are concatenation, sum, and element-wise product. The attention mechanism can aggregate with the pooling module. The answering component has two modes of output depending on the properties of the answer. The classification mode is used if the answer is brief and limited to one or two words. Otherwise, if the response is in free-form format, the generation modules such as LSTM or GRU are taken into account. There are additional techniques to the basic concept, for instance, Sub-task strategy, Global Average Pooling <ref type="bibr" coords="4,486.71,357.95,16.38,10.91" target="#b27">[28]</ref>, Embedding-based Topic Model, Question-Conditioned Reasoning, and Image Size Encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Dataset Descriptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Descriptions</head><p>Identifying lesions in endoscopy images is currently one of the most popular applications of artificial intelligence in the medical field. For the task at ImageCLEFmed-MEDVQA-GI-2023 <ref type="bibr" coords="4,89.29,478.11,11.58,10.91" target="#b1">[2]</ref>, the main focus will be on VQA and visual question generation (VQG). The main goal is to provide support to healthcare experts in diagnosis by combining image and text data for analysis. The task consists of three sub-tasks:</p><p>1. VQA (Visual Question Answering): For the visual question answering part, participants are required to generate a textual answer to a given textual question-image pair. This task involves combining endoscopy images from the dataset with textual answers to respond to questions. 2. VQG (Visual Question Generation): This is the reverse task of VQA, where participants need to generate textual questions based on given textual answers and image pairs. 3. VLQA (Visual Location Question Answering): Participants are provided with an image and a question, and they are required to provide an answer by providing a segmentation mask for the image.</p><p>In this study, our team only focuses on the VQA task (Task 1) for the provided endoscopy image dataset. In general, we receive a textual question along with the corresponding image,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Questions and sample answers from ImageCLEFmed-MEDVQA-GI-2023 dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Questions Sample Answers</head><p>0 What type of procedure is the image taken from? "Colonoscopy", "Gastroscopy" 1 Have all polyps been removed? "Yes", "No", "Not relevant" 2 Is this finding easy to detect? "Yes", "No", "Not relevant" 3 Is there a green/black box artifact? "Yes", "No" 4 Is there text? "Yes", "No" 5 What color is the abnormality? "Red", "Pink", "White, Yellow", ... 6 What color is the anatomical landmark?</p><p>"Red", "Red, White", "Pink, Red, grey", ... 7 How many findings are present? "0", "1", "2", "3", "4", "5", ... 8 How many polyps are in the image? "0", "1", "2", "3", "4", "5", ... 9 How many instruments are in the image? "0", "1", "2", "3" 10 Where in the image is the abnormality? "Center", "Lower-left", "Lower-right, Center-right", ... 11 Where in the image is the instrument? "Center", "Lower-left", "Lower-right, Center-right", ... 12 Are there any abnormalities in the image?</p><p>"No", "Polyp", "Ulcerative colitis", "Oesophagitis", ... 13 Are there any anatomical landmarks in the image? "No", "Z-line" "Cecum", "Ileum", "Pylorus", "Not relevant" 14 Are there any instruments in the image?</p><p>"No", "Tube", "Biopsy forceps", "Metal clip", 'Polyp snare, Tube', ... 15 Where in the image is the anatomical landmark?</p><p>"Center", "Lower-left", "Lower-right, Center-right", ... 16 What is the size of the polyp? "&lt; 5mm", "5-10mm", "11-20mm", "&gt;20mm", "Not relevant", ... 17 What type of polyp is present? "Paris ip", "Paris iia", "Paris is", "Paris is, Paris iia", ... and the main task is to generate accurate and appropriate answers based on information from both sources. For example, for an image containing a colon polyp with the following question, "Where in the image is the polyp located?", the proposed VQA system should return answer giving a textual description of where in the image the polyp is located, like upper-left or in the center of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Information</head><p>The new dataset released for the ImageCLEFmed-MEDVQA-GI-2023 challenge is based on the HyperKvasir dataset <ref type="bibr" coords="5,180.15,611.19,16.09,10.91" target="#b28">[29]</ref>, the largest gastrointestinal collections with more than 100,000 images, with the additional question-and-answer ground truth developed by medical collaborators. The development set and test set include a total of 3949 images from different procedures such as gastroscopy and colonoscopy, spanning the entire gastrointestinal tract, from mouth to anus. Each image has a total of 18 questions about abnormalities, surgical instruments, normal  findings and other artefacts, with multiple answers possible for each, as shown in Table <ref type="table" coords="6,466.74,277.78,3.66,10.91">1</ref>. Not all questions will be relevant to the provided image, and the VQA system should be able to handle cases where there is no correct answer. Figure <ref type="figure" coords="6,299.01,304.88,5.11,10.91" target="#fig_0">1</ref> depicts several examples of question-answer pairs on common abnormalities in gastrointestinal tract, such as Colon Polyps, Oesophagitis, and Ulcerative Colitis. As shown in Figure <ref type="figure" coords="6,279.19,331.98,8.59,10.91" target="#fig_0">1d</ref>, there are three possible answers to the question "What color is the abnormality?": "Pink, " "Red, " and "White", and a typical VQA system should be able to identify all three colors. In general, the image may contains a variety of noise and components that locates across abnormalities, such as highlight spots or instruments, which pose a significant challenge in developing efficient VQA systems for gastrointestinal domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Approach</head><p>The method used in this study is based on a standard framework that is commonly used to tackle general VQA problems. Figure <ref type="figure" coords="6,276.08,458.35,5.17,10.91" target="#fig_1">2</ref> depicts an overview of the proposed method for ImageCLEFmed-MEDVQA-GI-2023 dataset. In general, the VQA architecture employs powerful pre-trained models to extract visual and textual features from image-question pairs, which are then combined into a joint embedding using a fusion algorithm and passed to a classifier module to generate the appropriate answer. To improve the quality of the region of interest and achieve better VQA performance, the original image is passed through a series of enhancement procedures before being fed into the image encoder for features extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Enhancement</head><p>The purpose of the image pre-processing and enhancement steps is to remove noise and artifacts, which are frequently caused by the equipment used in diagnostic or environmental difficulties. Some of the major problems to be mentioned are black mask, specular highlights, interlacing or uneven lighting. The impact of these elements, such as black mask and specular highlights, is significant since they, like the polyp, create valley information and affect the performance of polyp localization, causing the VQA system to generate incorrect answers. This study employs pre-processing and enhancing methods to cope with specular highlights and black mask in colonoscopy image, which are prevalent artifacts in the dataset provided. The desired outcome is an enhanced image with no specular reflection or black frame while retaining the visual features of the region of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Specular Highlights Removal</head><p>The removal of specular highlights from colonoscopy image includes two sequential processes: detection of specular highlights and highlights inpainting. Figure <ref type="figure" coords="7,373.44,501.53,4.97,10.91" target="#fig_2">3</ref> depicts the overall procedure of the method, the outcome of which is generally based on the combination of Telea inpainting algorithm with initial image restoration after several modification steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specular highlights detection</head><p>First, it is necessary to convert the image from the original RGB channel to grey scale to process the subsequent procedure. Rather than adaptive thresholding, the proposed approach employs standard thresholding method with a fixed threshold value to identify specular highlights in all images. This is due to the gastrointestinal image's varied textures and components, and if not done properly, may result in information loss. Some samples of the dataset contain text, high exposure regions and brightly colored instrument, as described in Figure <ref type="figure" coords="7,192.62,638.68,3.81,10.91" target="#fig_3">4</ref>. Aside from text in white color, high exposure regions are parts of specular highlights that received excessively high intensity compared to regular highlight spots, while the instruments are sometimes in white or blue color. After thresholding, these factors may emerge in the mask, as shown in Figure <ref type="figure" coords="8,301.14,247.31,8.74,10.91" target="#fig_3">4b</ref>, and affect the inpainting outcome. Thus, the following step is to remove these undesired elements from the mask in order to assure consistency. To cope with these problems, two directions are considered, either to perform segmentation for text, polyp and instrument, separately, or remove the parts that meet certain size threshold. For simplicity, the second approach is used in this study.</p><p>The preprocessing step consists of several morphology transformations interspersed by contour detection and removal. More specifically, a dilation operation with kernel size 3 √ó 3 is performed initially to connect the pixels related to undesirable parts. Among the obtained contours, those whose scaled area following the Modified Z-scores formula <ref type="bibr" coords="8,429.21,355.70,16.33,10.91" target="#b29">[30]</ref>, as shown in Formula 1, exceeds 17.0 are removed from the mask. The mask is then passed into another erosion module with the same settings to restore the initial highlights intensity. Finally, Gaussian filter of size 19 √ó 19 is applied to reduce the intensity of highlights area and improve the inpainting performance.</p><formula xml:id="formula_0" coords="8,267.63,433.35,239.01,24.43">ùëÜ ùëñ = |ùë† ùëñ -ùë† Àú| ùëÄ ùê¥ùê∑<label>(1)</label></formula><p>where:</p><p>‚Ä¢ ùëÜ ùëñ : is the scaled area of contour ùëñ based on modified Z-score.</p><p>‚Ä¢ ùë† ùëñ : is the area of contour ùëñ ‚Ä¢ ùë† Àú: is the median area of all contours ‚Ä¢ ùëÄ ùê¥ùê∑ = ùëöùëíùëëùëñùëéùëõ(|ùë† ùëñ -ùë† Àú|), ‚àÄùëñ = 1..ùëõ: is the Median Absolute Deviation of contour areas</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights inpainting</head><p>Once the mask of specular highlights has been achieved, the image regions indicated by the mask are then reconstructed through an efficient inpainting operation. First, a filter of size 3√ó3 slides across every pixels of the original image and calculate the average value. The process is repeated ùëÅ times to ensure a desirable outcome. We then perform an initial restoration on the image by directly replacing its pixels under the specular highlights mask with pixels from the blur image. Despite the drastically reduced intensity, specular highlight spots still remains in the reconstructed image, as shown in Figure <ref type="figure" coords="8,393.82,628.93,8.40,10.91" target="#fig_2">3e</ref>. To obtained the final result, Telea algorithm <ref type="bibr" coords="8,193.73,642.48,16.38,10.91" target="#b30">[31]</ref>, a powerful image inpainting strategy, is applied to eliminate the remaining noisy and dim highlights. The inpainted image is noticeably higher in quality, with specular highlights removed without negatively impacting other areas of the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Black Mask Removal</head><p>Previous research has shown that black masks do generate valley information, which can reduce polyp localization performance. Based on this, we propose a black mask removal strategy for the VQA task that still retains black box information in order to answer the question "Is there a green/black box artefact?". In general, an artificial mask of black frame is initially created based on its border width, and then the inpainting operation is performed to remove the black frame from the image. The overall procedure is described in Figure <ref type="figure" coords="9,397.68,491.88,3.81,10.91" target="#fig_5">5</ref>. Our method does not use cropping or thresholding directly to detect and remove the black mask because it may contain the black box artifact, shadow regions, or black instrument, the removal of which causes information loss and decreases VQA performance.</p><p>To detect the border width, we first perform a grey scale conversion and inverse thresholding with erosion operation to remove noise, and then measure the distance from each edge of the image to the nearest pixel that does not belong to the mask. After determining the width of the border, the crucial step of the method is to create an artificial mask with internal octagon shape. This can be done by creating two sub-masks, one rectangle and one circle, followed by a bitwise OR operation to combine them into the final mask, as show in Figure <ref type="figure" coords="9,407.60,613.82,8.26,10.91" target="#fig_5">5c</ref>. The circle mask is created with a center point based on the information of border width and a radius calculated by multiplying the ordinate of the center point by a value ùúé (ùúé &gt; 1). In some cases, the final mask is not always octagonal, as shown in the last example, but it still covers the main region of interest. Finally, the inpainting of black mask is completed using the same procedure as described in the previous section for specular highlights, giving the final enhanced image with black mask removed. If a black box artefact exists in the bottom-left corner, as shown in the second example, it will not be significantly affected as long as its size is greater than the area of the mask at the respective position. For images containing an expanded black mask labeled as black box artefact, we process by creating a simulated green box that contains the text and placing it in bottom-left corner. By doing so, the text and box artefact information still remain after the inpainting procedure. Though the obtained results are quite satisfactory, there are still some cases where the mask is not completely removed and need further processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multimodal Fusion Architecture</head><p>Since this study focus mainly on the VQA task, the architecture should be capable of extracting meaningful features from the question and corresponding image, and incorporating them to give the correct answer. Our multimodal fusion architecture is set up with important components such as an image encoder for feature extraction from images, a text encoder for features extraction from questions, a fusion algorithm for unifying modalities and a classifier for producing the appropriate answer. The proposed approach uses pre-trained Bidirectional Encoder Representations based on Transformers (BERT) <ref type="bibr" coords="10,301.94,312.75,17.87,10.91" target="#b19">[20]</ref> to extract textual features from questions. As a bidirectional model, it can learn the meaning of words in a sentence by considering both the words that come before and after them. With massive pre-training data, BERT can be fine-tuned and achieved state-of-the-art results on a number of natural language processing (NLP) benchmarks. For features extraction from the images, this study set up and experiment with eight different pre-trained models that are belong to two main concepts:</p><p>‚Ä¢ CNN-based architectures including Resnet152 <ref type="bibr" coords="10,329.42,402.45,16.27,10.91" target="#b31">[32]</ref>, Inception-v4 <ref type="bibr" coords="10,418.12,402.45,16.27,10.91" target="#b32">[33]</ref>, MobileNetV2 <ref type="bibr" coords="10,116.56,416.00,18.07,10.91" target="#b33">[34]</ref> and EfficientNet <ref type="bibr" coords="10,220.03,416.00,16.41,10.91" target="#b34">[35]</ref>. The group of models take advantage of traditional CNN's components such the convolutional layer, pooling layer, residual block and fully connected layer to achieve significant result in computer vision field. The training of CNN-based model is more efficient with less computational resources compared to new approaches based on Transformers. ‚Ä¢ Transformer-based architectures including ViT <ref type="bibr" coords="10,337.18,484.94,16.41,10.91" target="#b35">[36]</ref>, DeiT <ref type="bibr" coords="10,389.30,484.94,16.41,10.91" target="#b36">[37]</ref>, Swin Transformer <ref type="bibr" coords="10,116.56,498.49,18.05,10.91" target="#b37">[38]</ref> and BEiT <ref type="bibr" coords="10,184.36,498.49,16.40,10.91" target="#b38">[39]</ref>. The family of models leverages a massive amount of training data and Transformer's multi-head self-attention for a game-changing breakthrough in the computer vision field. ViT (Vision Transformer) and other models inspired from it initially encodes the image as patch embeddings and pass them into a regular Transformer Encoder for feature extraction, which is similar to text data. Currently they are considered as the prominent architectures to achieve state-of-the-art performance on a variety of tasks in computer vision such as image classification, object detection, and semantic image segmentation.</p><p>After obtaining the embeddings of text and image, a multimodal fusion method based on concatenation is used to combine these features along the embedding dimension. The unified embedding matrix is then passed through an intermediate fully connected layer with drop out 0.5 and ReLU activation followed by a classification layer to produce the final output. Because there can be more than one appropriate answer for each question, we approach the VQA task as a multi-label classification problem. To successfully train the proposed arhitecture, multi-label binarization is used to encode a list of all possible answers into a binary vector. Furthermore, the final layer is configured with sigmoid activation function to return an output vector of the same size containing the corresponding probability for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Preparation</head><p>The development set released for the VQA challenge contains 2000 images of gastroscopy and colonoscopy procedures. In order to experiment and evaluate our method, we randomly divided the provided development set into three parts: train, validation, and test, with 1600 images for training and 200 images for each validation and test set. The data preparation process is designed to ensure that each abnormality has the same proportion in the training, validation, and testing sets, and that each image contains all 18 questions. This produces 28,800 question-answer pairs on the training set, 3600 pairs for validation and 3600 pairs for test. All images from development set and private test set are first passed into an image enhancement block, where numerous image preprocessing methods are applied to remove specular highlights and black mask from the images. The enhanced results are then used as input in the training and testing of the proposed VQA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment Configurations</head><p>Many experiments are carried out in order to evaluate the performance of the proposed methods toward the ImageCLEFmed-MEDVQA-GI-2023 challenge. Specifically, each pre-trained vision model is initialized and experimented as an image encoder and unify with BERT encoder through concatenation fusion for multimodal learning. Table <ref type="table" coords="11,365.45,603.71,5.12,10.91" target="#tab_1">2</ref> gives the general information of pre-trained models used in this study including vision model name, version and number of parameters for each fusion model. Through experiments, we can discover the potential and limitation of each model for the VQA task and thus, choose the best method for giving the final prediction on the private test set of the competition.</p><p>To achieve a comparative result, we set up the same hyperparameters for all experiments. The models are trained in 15 epochs with batch size of 64. We utilize the Adam optimizer <ref type="bibr" coords="12,461.66,100.52,17.76,10.91" target="#b39">[40]</ref> using weighted decay with an initial learning rate of 5e-5 and a linear scheduler to decrease learning rate 6.67% after each epoch. Since we approach the VQA task as multi-label classification, the output layer is configured to return a tensor containing probabilities of answers, where the final predicted answers for each question can be achieved using threshold value of 0.5. Due to this, the BCEWithLogitsLoss function, which combines a Sigmoid layer and the BCELoss, is applied in the training process. After each epoch, the training loss and validation loss are calculated, and the performance are then evaluated on classification metrics such as accuracy, precision, recall and F1-Score. To ensure a meaningful result for multi-label classification, the metrics are calculated using ground truth and prediction sets of binary vectors, in which recall, precision and F1-scores should be calculated on each sample and find their average. The model's state that obtains best F1-Score is used for prediction in the testing phase.</p><p>The proposed architecture are implemented in PyTorch and trained on the Kaggle platform with hardware specifications: Intel(R) Xeon(R) CPU @ 2.00GHz; GPU Tesla P100 16 GB with CUDA 11.4. The comparative result of different pre-trained image model on the testing set is shown in Table <ref type="table" coords="13,114.98,394.47,3.66,10.91" target="#tab_2">3</ref>. It is clear that, with no image enhancement, Swin-B achieves the best result with 86.64% accuracy and 90.90% F1-Score while BEiT-B gives a slightly lower performance with accuracy of 86.47% and 90.74% F1-Score. CNN-based vision models have acceptable results, but cannot be compared with the result of Transformer architecture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>With image enhancement, six out of eight vision models from both CNN and Transformer architectures achieve a better performance on F1-Score metric. BEiT-B has an outstanding result with accuracy and F1-Score of 87.2% and 91.85%, respectively. Overall, the enhancement process helps to improve the F1-Score at least 0.4% and up to 1.11% on VQA performance. The result of the convolutional models is still under when compared with Transformers architecture models.</p><p>We found that the BERT and BEiT fusion (BERT+BEiT) with image enhancement is the best method of our approach and use it for prediction in final private test phase. Our method obtains a good result on the private test set with an accuracy of 82.01%. Table <ref type="table" coords="13,452.80,543.51,5.17,10.91" target="#tab_3">4</ref> illustrates the performance evaluation of BERT+BEiT fusion on each question from the development test set compared with the private test set. In general, there are 14/18 questions with predicted answers achieve greater than 80% accuracy on the development test set, while 11/18 questions on the private test set achieve the same result. Our method still struggles to produce full and precise answers for questions with multiple answers, such as "What color is the abnormality?" or questions that refer to the location of the abnormality, anatomical landmark, and instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Works</head><p>Along with performing image enhancement, we also set up and experimented using various powerful pre-trained image models together with the BERT encoder for our proposed multimodal architecture in the VQA task at ImageCLEFmed-MEDVQA-GI-2023 <ref type="bibr" coords="14,381.86,138.38,11.28,10.91" target="#b1">[2]</ref>. The visual enhancement steps, which include specular hightlights and black mask removals, help improve multimodal learning performance on the dataset by up to 1.11% F1-Score. Our best method, BERT+BEiT fusion with image enhancement, achieved 87.25% on development test set and 82.01% on the private test set by the accuracy. Through performance analysis, there are question cases that require multiple positions or colors in the answer, which are our limitations in this study. In summary, there are factors that have significant impact on our solution for the VQA task such as answer imbalance, noise, and artifacts.</p><p>Our future research for this task is to improve the accuracy of the model in giving the correct answer by enriching the features from images and questions through instrument segmentation and polyp localization with methods such as U-net <ref type="bibr" coords="14,346.94,273.87,16.41,10.91" target="#b40">[41]</ref>, ResUnet++ <ref type="bibr" coords="14,423.90,273.87,18.07,10.91" target="#b41">[42]</ref> developed on object-specific datasets such as Kvasir-Instrument <ref type="bibr" coords="14,318.86,287.42,18.07,10.91" target="#b42">[43]</ref> and Kvasir-seg <ref type="bibr" coords="14,409.63,287.42,16.41,10.91" target="#b43">[44]</ref>. Other advanced colonoscopy image preprocessing techniques such as interlacing removal or uneven lighting removal can be examined to improve the image quality. From the proposed system, an intelligent chatbot application can be implemented for question-answering from medical images and help improve colonoscopy analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,444.16,416.69,8.93;5,89.29,456.17,226.61,8.87;5,291.15,320.98,101.73,97.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of question-answer pairs along with common abnormalities in gastrointestinal image from ImageCLEFmed-MEDVQA-GI-2023 dataset</figDesc><graphic coords="5,291.15,320.98,101.73,97.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,240.18,405.42,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of the multimodal architecture with image enhancement for VQA challenge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,360.40,317.60,8.93;7,94.81,222.07,130.63,112.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An overview of stages of the specular highlights inpainting method</figDesc><graphic coords="7,94.81,222.07,130.63,112.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,200.68,417.79,8.93;8,89.29,212.68,190.84,8.87;8,91.90,84.19,98.96,79.17"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of specular highlights detection from a colonoscopy image that contains text, high exposure regions and a white instrument.</figDesc><graphic coords="8,91.90,84.19,98.96,79.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,88.99,325.79,197.09,9.96;9,313.26,325.79,72.63,9.96;9,413.40,325.79,80.70,9.96"><head></head><label></label><figDesc>(a) Image with black mask (b) Border detection (c) Artificial mask (d) Enhanced Image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,89.29,344.27,416.69,8.93;9,89.29,356.28,416.69,8.87;9,88.93,368.24,189.63,8.87;9,196.07,263.62,98.95,55.29"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Stages of black mask removal process. The first row illustrates an image with a standard black mask, the second row depicts an image containing a black square and the last row contains image with black mask marked as black box artefact.</figDesc><graphic coords="9,196.07,263.62,98.95,55.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,120.28,88.44,379.84,130.70"><head>Image What color is the abnormality? Enhanced Image Question</head><label></label><figDesc></figDesc><table coords="6,182.13,108.95,317.99,110.19"><row><cell>Image Enhancement Specular Highlights Removal Black Mask Removal</cell><cell>Image Encoder BERT</cell><cell>Concatenate Image Embedding Word Embedding</cell><cell>Fully Connected Layer</cell><cell>P Center Multi-label Classification Center White Pink ... Red P Red P Pink P White Fully Connected Layer Polyp P Polyp ...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,88.99,90.49,411.10,140.45"><head>Table 2</head><label>2</label><figDesc>Statistics of multimodal fusion with pre-trained vision and language models for the VQA challenge</figDesc><table coords="11,95.21,122.00,404.88,108.94"><row><cell>Models</cell><cell cols="2">Version Spaces vision model name</cell><cell># Parameters</cell></row><row><cell>BERT+ViT</cell><cell>base</cell><cell>"google/vit-base-patch16-224-in21k"</cell><cell>196M</cell></row><row><cell>BERT+DEiT</cell><cell>base</cell><cell>"facebook/deit-base-distilled-patch16-224"</cell><cell>196M</cell></row><row><cell>BERT+Swin</cell><cell>base</cell><cell>"microsoft/swin-base-patch4-window7-224-in22k"</cell><cell>197M</cell></row><row><cell>BERT+ BEiT</cell><cell>base</cell><cell>"microsoft/beit-base-patch16-224-pt22k-ft22k"</cell><cell>196M</cell></row><row><cell>BERT+ResNet152</cell><cell>v1.5</cell><cell>"microsoft/resnet-152"</cell><cell>169M</cell></row><row><cell>BERT+Inception</cell><cell>v4</cell><cell>"inception_v4"</cell><cell>153M</cell></row><row><cell>BERT+MobileNet</cell><cell>V2</cell><cell>"google/mobilenet_v2_1.0_224"</cell><cell>112M</cell></row><row><cell>BERT+EfficientNet</cell><cell>b3</cell><cell>"google/efficientnet-b3"</cell><cell>121M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,88.99,365.08,416.99,299.51"><head>Table 3</head><label>3</label><figDesc>Comparative performance of the multimodal fusion method with vision models on the development test set.</figDesc><table coords="12,141.78,406.80,311.72,257.79"><row><cell>Vision Models</cell><cell cols="4">Accuracy Precision Recall F1-Score</cell></row><row><cell>No image enhancement</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet152</cell><cell>0.8419</cell><cell>0.8917</cell><cell>0.8867</cell><cell>0.8857</cell></row><row><cell>Inception-v4</cell><cell>0.8619</cell><cell>0.9133</cell><cell>0.9067</cell><cell>0.9067</cell></row><row><cell>MobileNetV2</cell><cell>0.8444</cell><cell>0.8932</cell><cell>0.8951</cell><cell>0.8906</cell></row><row><cell>EfficientNet-B3</cell><cell>0.8581</cell><cell>0.9065</cell><cell>0.9049</cell><cell>0.9023</cell></row><row><cell>ViT-B/16</cell><cell>0.8636</cell><cell>0.9134</cell><cell>0.9089</cell><cell>0.9078</cell></row><row><cell>DeiT-B</cell><cell>0.8611</cell><cell>0.9100</cell><cell>0.9026</cell><cell>0.9033</cell></row><row><cell>Swin-B</cell><cell>0.8664</cell><cell>0.9152</cell><cell>0.9094</cell><cell>0.9090</cell></row><row><cell>BEiT-B</cell><cell>0.8647</cell><cell>0.9158</cell><cell>0.9068</cell><cell>0.9074</cell></row><row><cell>With image enhancement</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet152</cell><cell>0.8453 ‚Üë</cell><cell>0.8942</cell><cell>0.8894</cell><cell>0.8885 ‚Üë</cell></row><row><cell>Inception-v4</cell><cell>0.8625 ‚Üë</cell><cell>0.9121</cell><cell>0.9073</cell><cell>0.9071 ‚Üë</cell></row><row><cell>MobileNetV2</cell><cell>0.8422 ‚Üì</cell><cell>0.8935</cell><cell>0.8882</cell><cell>0.8867 ‚Üì</cell></row><row><cell>EfficientNet-B3</cell><cell>0.8572 ‚Üì</cell><cell>0.9081</cell><cell>0.9079</cell><cell>0.9046 ‚Üë</cell></row><row><cell>ViT-B/16</cell><cell>0.8631 ‚Üì</cell><cell>0.9126</cell><cell>0.9086</cell><cell>0.9073 ‚Üì</cell></row><row><cell>DeiT-B</cell><cell>0.8625 ‚Üë</cell><cell>0.9122</cell><cell>0.9052</cell><cell>0.9055 ‚Üë</cell></row><row><cell>Swin-B</cell><cell>0.8717 ‚Üë</cell><cell>0.9245</cell><cell>0.9159</cell><cell>0.9168 ‚Üë</cell></row><row><cell>BEiT-B</cell><cell>0.8725 ‚Üë</cell><cell>0.9253</cell><cell cols="2">0.9184 0.9185 ‚Üë</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,88.99,90.49,418.65,264.31"><head>Table 4</head><label>4</label><figDesc>Performance evaluation of BERT+BEiT fusion with image enhancement for each question on the development test set and private test set</figDesc><table coords="13,91.85,133.18,411.58,221.61"><row><cell>Question</cell><cell cols="3">Development Test Set Accuracy Precision Recall F1-Score</cell><cell>Private Test Set Accuracy</cell></row><row><cell>Are there any abnormalities in the image?</cell><cell>0.9700</cell><cell>0.9750</cell><cell>0.9725 0.9733</cell><cell>0.8091</cell></row><row><cell>Are there any anatomical landmarks in the image?</cell><cell>0.9300</cell><cell>0.9300</cell><cell>0.9300 0.9300</cell><cell>0.6940</cell></row><row><cell>Are there any instruments in the image?</cell><cell>0.9050</cell><cell>0.9275</cell><cell>0.9200 0.9217</cell><cell>0.7688</cell></row><row><cell>Have all polyps been removed?</cell><cell>0.9550</cell><cell>0.9575</cell><cell>0.9600 0.9583</cell><cell>0.9721</cell></row><row><cell>How many findings are present?</cell><cell>0.8400</cell><cell>0.8400</cell><cell>0.8400 0.8400</cell><cell>0.7807</cell></row><row><cell>How many instrumnets are in the image?</cell><cell>0.9650</cell><cell>0.9650</cell><cell>0.9650 0.9650</cell><cell>0.8901</cell></row><row><cell>How many polyps are in the image?</cell><cell>0.9650</cell><cell>0.9650</cell><cell>0.9650 0.9650</cell><cell>0.9577</cell></row><row><cell>Is there a green/black box artefact?</cell><cell>0.9500</cell><cell>0.9500</cell><cell>0.9500 0.9500</cell><cell>0.9732</cell></row><row><cell>Is there text?</cell><cell>0.9250</cell><cell>0.9250</cell><cell>0.9250 0.9250</cell><cell>0.8787</cell></row><row><cell>Is this finding easy to detect?</cell><cell>0.8900</cell><cell>0.8900</cell><cell>0.8900 0.8900</cell><cell>0.8044</cell></row><row><cell>What color is the abnormality?</cell><cell>0.5800</cell><cell>0.9025</cell><cell>0.8563 0.8597</cell><cell>0.4969</cell></row><row><cell>What color is the anatomical landmark?</cell><cell>0.9400</cell><cell>0.9400</cell><cell>0.9400 0.9400</cell><cell>1.0000</cell></row><row><cell>What is the size of the polyp?</cell><cell>0.8600</cell><cell>0.8650</cell><cell>0.8700 0.8667</cell><cell>0.8535</cell></row><row><cell>What type of polyp is present?</cell><cell>0.8650</cell><cell>0.8800</cell><cell>0.8725 0.8750</cell><cell>0.8132</cell></row><row><cell>What type of procedure is the image taken from?</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000 1.0000</cell><cell>0.9938</cell></row><row><cell>Where in the image is the abnormality?</cell><cell>0.6600</cell><cell>0.9251</cell><cell>0.8805 0.8842</cell><cell>0.5872</cell></row><row><cell>Where in the image is the anatomical landmark?</cell><cell>0.7150</cell><cell>0.8847</cell><cell>0.8848 0.8766</cell><cell>0.7203</cell></row><row><cell>Where in the image is the instrument?</cell><cell>0.7900</cell><cell>0.9332</cell><cell>0.9096 0.9125</cell><cell>0.7688</cell></row><row><cell>All</cell><cell>0.8725</cell><cell cols="2">0.9253 0.9184 0.9185</cell><cell>0.8201</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by The <rs type="funder">VNUHCM University of Information Technology's Scientific Research Support Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,472.42,394.53,10.91;14,112.48,485.97,394.70,10.91;14,112.66,499.52,394.62,10.91;14,112.66,513.06,347.41,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,112.66,499.52,155.77,10.91">Quality indicators for colonoscopy</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Rex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Schoenfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">M</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Fennerty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lieb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Rizk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weinberg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.gie.2014.07.058</idno>
		<ptr target="https://doi.org/10.1016/j.gie.2014.07.058.doi:10.1016/j.gie.2014.07.058" />
	</analytic>
	<monogr>
		<title level="j" coord="14,279.33,499.52,125.56,10.91">Gastrointestinal Endoscopy</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="31" to="53" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,526.61,393.73,10.91;14,112.66,540.16,394.53,10.91;14,112.66,553.71,394.52,10.91;14,112.66,567.26,58.60,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,463.26,526.61,43.14,10.91;14,112.66,540.16,390.68,10.91">Overview of imageclef medical 2023 -medical visual question answering for gastrointestinal tract</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,127.16,553.71,111.10,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,245.61,553.71,173.62,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,580.81,394.53,10.91;14,112.48,594.36,395.34,10.91;14,112.66,607.91,394.53,10.91;14,112.66,621.46,395.17,10.91;14,112.39,635.01,394.80,10.91;14,112.48,648.56,394.70,10.91;14,112.66,662.11,395.17,10.91;15,112.66,86.97,393.32,10.91;15,112.66,100.52,394.52,10.91;15,112.33,114.06,120.27,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,228.06,648.56,279.12,10.91;14,112.66,662.11,228.44,10.91">Overview of imageclef 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Meliha Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,363.94,662.11,143.89,10.91;15,112.66,86.97,393.32,10.91;15,112.66,100.52,136.27,10.91">Experimental IR Meets Multilin-guality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="15,280.09,100.52,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,127.61,394.53,10.91;15,112.66,141.16,393.61,10.91;15,112.66,154.71,393.33,10.91;15,112.66,168.26,121.22,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,282.53,141.16,223.74,10.91;15,112.66,154.71,315.24,10.91">Performance of artificial intelligence in colonoscopy for adenoma and polyp detection: a systematic review and meta-analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Spadaccini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iannone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Maselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jovani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Areia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dinis-Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,436.16,154.71,69.83,10.91;15,112.66,168.26,47.43,10.91">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,181.81,394.53,10.91;15,112.66,195.36,393.33,10.91;15,112.39,208.91,335.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,112.66,195.36,393.33,10.91;15,112.39,208.91,184.10,10.91">Real-time detection of colon polyps during colonoscopy using deep learning: systematic validation with four independent datasets</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-S</forename><surname>Byeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,304.66,208.91,75.46,10.91">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">8379</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.33,10.91;15,112.66,236.01,394.53,10.91;15,112.66,249.56,313.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,266.30,222.46,239.69,10.91;15,112.66,236.01,157.47,10.91">Colonoscopy image pre-processing for the development of computer-aided diagnostic tools</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>S√°nchez-Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Z</forename><surname>Soto</surname></persName>
		</author>
		<idno type="DOI">10.5772/67842</idno>
		<ptr target="https://doi.org/10.5772/67842.doi:10.5772/67842" />
	</analytic>
	<monogr>
		<title level="m" coord="15,366.55,236.01,76.88,10.91">Surgical Robotics</title>
		<editor>
			<persName><forename type="first">S</forename><surname>K√º√ß√ºk</surname></persName>
		</editor>
		<meeting><address><addrLine>IntechOpen, Rijeka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,393.72,10.91;15,112.66,276.66,378.12,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,349.12,263.11,157.26,10.91;15,112.66,276.66,170.21,10.91">High-quality colonoscopy: A review of quality indicators and best practices</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Turshudzhyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,290.99,276.66,115.85,10.91">Gastroenterology Insights</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="162" to="172" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,395.17,10.91;15,112.66,303.75,393.33,10.91;15,112.66,317.30,395.01,10.91;15,112.66,330.85,167.31,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,265.47,290.20,242.36,10.91;15,112.66,303.75,149.49,10.91">Impact of image preprocessing methods on polyp localization in colonoscopy frames</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Vilari√±o</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2013.6611256</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,292.18,303.75,213.81,10.91;15,112.66,317.30,290.91,10.91">2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7350" to="7354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,344.40,393.33,10.91;15,112.66,357.95,393.33,10.91;15,112.66,371.50,126.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,322.25,344.40,183.73,10.91;15,112.66,357.95,207.61,10.91">Automatic segmentation and inpainting of specular highlights for endoscopic imaging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ameling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,331.20,357.95,174.79,10.91;15,112.66,371.50,47.83,10.91">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,385.05,393.33,10.91;15,112.66,398.60,362.94,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,384.40,385.05,121.59,10.91;15,112.66,398.60,208.08,10.91">Overview of imageclef 2018 medical domain visual question answering task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,346.99,398.60,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,412.15,393.32,10.91;15,112.39,425.70,369.31,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,363.98,412.15,142.01,10.91;15,112.39,425.70,234.39,10.91">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,355.50,425.70,62.55,10.91">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,394.62,10.91;15,112.66,452.79,393.33,10.91;15,112.66,466.34,66.96,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,464.35,439.25,42.93,10.91;15,112.66,452.79,297.24,10.91">Vqa-med: Overview of the medical visual question answering task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,440.16,452.79,65.82,10.91;15,112.66,466.34,27.24,10.91">CLEF (working notes)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,479.89,394.53,10.91;15,112.34,493.44,393.64,10.91;15,112.41,506.99,359.72,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,239.94,493.44,158.38,10.91">Towards visual dialog for radiology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kanjaria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ballah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Beymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,421.75,493.44,84.23,10.91;15,112.41,506.99,281.99,10.91">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<meeting>the 19th SIGBioMed Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,520.54,393.33,10.91;15,112.66,534.09,267.54,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10286</idno>
		<title level="m" coord="15,300.48,520.54,205.51,10.91;15,112.66,534.09,84.94,10.91">Pathvqa: 30000+ questions for medical visual question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,547.64,393.33,10.91;15,112.66,561.19,393.33,10.91;15,112.66,574.74,393.32,10.91;15,112.66,588.29,227.05,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,449.69,547.64,56.29,10.91;15,112.66,561.19,393.33,10.91;15,112.66,574.74,66.96,10.91">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,203.01,574.74,302.98,10.91;15,112.66,588.29,95.19,10.91">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2021-09">September 2021, 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,601.84,393.33,10.91;15,112.66,615.39,393.32,10.91;15,112.66,628.93,368.75,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,368.84,601.84,137.14,10.91;15,112.66,615.39,302.34,10.91">Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,461.85,615.39,44.13,10.91;15,112.66,628.93,244.84,10.91">IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,642.48,395.17,10.91;15,112.66,656.03,395.01,10.91;15,112.41,669.58,38.81,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,259.74,642.48,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,488.38,642.48,19.45,10.91;15,112.66,656.03,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.66,100.52,226.28,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m" coord="16,247.99,86.97,258.00,10.91;16,112.66,100.52,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,114.06,394.53,10.91;16,112.28,127.61,395.55,10.91;16,112.66,141.16,236.19,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,249.07,127.61,214.65,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,472.07,127.61,35.76,10.91;16,112.66,141.16,147.19,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,154.71,393.33,10.91;16,112.66,168.26,356.92,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,304.23,154.71,201.76,10.91;16,112.66,168.26,120.95,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><forename type="middle">C</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,256.95,168.26,115.21,10.91">Proceedings of naacL-HLT</title>
		<meeting>naacL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,181.81,393.33,10.91;16,112.66,195.36,393.98,10.91;16,112.41,208.91,48.96,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,361.64,181.81,144.35,10.91;16,112.66,195.36,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,394.29,195.36,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,222.46,393.98,10.91;16,112.41,236.01,48.96,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,253.72,222.46,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,376.65,222.46,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,249.56,393.32,10.91;16,112.66,263.11,172.11,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,232.02,249.56,173.19,10.91">Bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,414.04,249.56,91.95,10.91;16,112.66,263.11,78.03,10.91">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,276.66,395.16,10.91;16,112.66,290.20,393.32,10.91;16,112.66,303.75,223.38,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Merri√´nboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m" coord="16,133.03,290.20,372.95,10.91;16,112.66,303.75,46.51,10.91">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,317.30,393.33,10.91;16,112.66,330.85,393.33,10.91;16,112.66,344.40,126.79,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,298.65,317.30,207.33,10.91;16,112.66,330.85,44.87,10.91">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,188.20,330.85,317.78,10.91;16,112.66,344.40,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,357.95,395.17,10.91;16,112.66,371.50,393.33,10.91;16,112.66,385.05,42.06,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="16,334.62,357.95,173.20,10.91;16,112.66,371.50,159.92,10.91">Dendritic cortical microcircuits approximate the backpropagation algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,280.80,371.50,225.19,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,398.60,393.33,10.91;16,112.66,412.15,368.78,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,271.58,398.60,234.41,10.91;16,112.66,412.15,84.94,10.91">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,206.41,412.15,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,425.70,374.56,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m" coord="16,221.26,425.70,88.34,10.91">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,439.25,394.53,10.91;16,112.66,452.79,395.17,10.91;16,112.66,466.34,394.52,10.91;16,112.66,479.89,394.53,10.91;16,112.66,493.44,397.48,10.91;16,112.36,509.43,140.39,7.90" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="16,449.38,466.34,57.80,10.91;16,112.66,479.89,389.45,10.91">HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borgli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Eskeland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Randel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Stensland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Garcia-Ceja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-020-00622-y</idno>
		<ptr target="https://doi.org/10.1038/s41597-020-00622-y.doi:10.1038/s41597-020-00622-y" />
	</analytic>
	<monogr>
		<title level="j" coord="16,112.66,493.44,67.26,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">283</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,520.54,393.32,10.91;16,112.66,534.09,395.01,10.91;16,112.41,547.64,42.06,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="16,221.02,520.54,284.96,10.91;16,112.66,534.09,235.14,10.91">Volume 16: How to detect and handle outliers, The ASQC Basic References in Quality Control: Statistical Techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Iglewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoaglin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,448.57,534.09,22.27,10.91">Ph.D</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Mykytka</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,561.19,393.33,10.91;16,112.66,574.74,306.12,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="16,155.36,561.19,296.99,10.91">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1080/10867651.2004.10487596</idno>
	</analytic>
	<monogr>
		<title level="j" coord="16,461.49,561.19,44.50,10.91;16,112.66,574.74,66.69,10.91">Journal of Graphics Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,588.29,393.33,10.91;16,112.66,601.84,395.00,10.91;16,112.66,615.39,137.64,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="16,258.15,588.29,202.68,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,601.84,307.90,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,628.93,393.33,10.91;16,112.66,642.48,393.33,10.91;16,112.66,656.03,353.60,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="16,331.70,628.93,174.29,10.91;16,112.66,642.48,194.75,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,334.23,642.48,171.76,10.91;16,112.66,656.03,203.53,10.91">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,669.58,393.32,10.91;17,112.66,86.97,393.32,10.91;17,112.66,100.52,395.01,10.91;17,112.66,114.06,397.48,10.91;17,112.66,130.06,61.74,7.90" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="16,365.60,669.58,140.38,10.91;17,112.66,86.97,97.74,10.91">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00474.doi:10.1109/CVPR.2018.00474" />
	</analytic>
	<monogr>
		<title level="m" coord="17,256.88,86.97,249.10,10.91;17,112.66,100.52,84.93,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,141.16,394.53,10.91;17,112.66,154.71,122.77,10.91" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="17,186.32,141.16,316.07,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,168.26,395.16,10.91;17,112.66,181.81,393.33,10.91;17,112.41,195.36,317.06,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m" coord="17,421.13,181.81,84.86,10.91;17,112.41,195.36,254.77,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,208.91,393.33,10.91;17,112.66,222.46,393.33,10.91;17,112.66,236.01,243.13,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="17,408.51,208.91,97.48,10.91;17,112.66,222.46,235.84,10.91">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,376.08,222.46,129.91,10.91;17,112.66,236.01,78.76,10.91">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,249.56,393.33,10.91;17,112.39,263.11,314.07,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m" coord="17,370.82,249.56,135.17,10.91;17,112.39,263.11,183.23,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,276.66,395.01,10.91;17,112.66,292.65,97.35,7.90" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="17,267.66,276.66,208.11,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,303.75,373.41,10.91" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="17,144.99,303.75,163.89,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,317.30,395.17,10.91;17,112.66,330.85,395.01,10.91;17,112.66,346.84,97.35,7.90" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="17,278.21,317.30,229.62,10.91;17,112.66,330.85,78.43,10.91">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1505.04597" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,357.95,395.17,10.91;17,112.66,371.50,393.33,10.91;17,112.66,385.05,394.51,10.91;17,112.66,401.04,115.16,7.90" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="17,150.84,371.50,311.45,10.91">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISM46123.2019.00049</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.66,385.05,233.50,10.91">IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,412.15,394.53,10.91;17,112.33,425.70,394.94,10.91;17,112.66,439.25,394.61,10.91;17,112.66,452.79,368.52,10.91" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="17,423.87,425.70,83.40,10.91;17,112.66,439.25,374.06,10.91">Kvasir-instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Emanuelsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Garcia-Ceja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.66,452.79,94.28,10.91">MultiMedia Modeling</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,466.34,394.53,10.91;17,112.66,479.89,394.53,10.91;17,112.48,493.44,393.51,10.91;17,112.66,506.99,164.71,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="17,112.66,479.89,165.80,10.91">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,299.99,493.44,96.43,10.91">MultiMedia Modeling</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W.-T</forename><surname>Chu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-W</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-C</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">De</forename><surname>Neve</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
