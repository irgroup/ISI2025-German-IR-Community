<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.05,351.59,15.39">KDE Lab at ImageCLEFmedical Caption 2023</title>
				<funder ref="#_YPeFAEt #_jFrtEnx #_2PVPa6x">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,114.37,74.27,10.68"><forename type="first">Hiroki</forename><surname>Shinoda</surname></persName>
							<email>shinoda.hiroki.vo@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.92,114.37,64.67,10.68"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>masaki.aono.ss@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.24,114.37,85.40,10.68"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa.tetsuya.um@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.30,114.37,76.24,10.68"><forename type="first">Kazuki</forename><surname>Shimizu</surname></persName>
							<email>shimizu@heart-center.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho</addrLine>
									<postCode>441-8530</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.93,128.32,89.81,10.68"><forename type="first">Takuyuki</forename><surname>Komoda</surname></persName>
							<email>komoda@heartcenter.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho</addrLine>
									<postCode>441-8530</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.75,128.32,77.34,10.68"><forename type="first">Takuya</forename><surname>Togawa</surname></persName>
							<email>t.togawa0316@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho</addrLine>
									<postCode>441-8530</postCode>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.05,351.59,15.39">KDE Lab at ImageCLEFmedical Caption 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">24ADA3FBEA86ED1B18EF95F6984B976A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Image Captioning</term>
					<term>CNN-RNN</term>
					<term>Caption Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes KDE Lab approach in ImageCLEFmedical Prediction Task 2023. ImageCLEFmedical Caption Task 2023 consists of two sub tasks, Caption Prediction and Concept Detection. Concept Detection aims to identify concepts from medical images. Caption Prediction generates description from medical images. For experiment, we applied retrieval approach and deep learning approach for tasks. In Concept Detection, we employed two methods that are fine-tuned Convolutional Neural Network (CNN) approach and retrieval approach based K-Nearest Neighbor (KNN) and cosine similarity using CNN features. In Caption Prediction, we attempted four methods that are retrieval approach based cosine similarity using Term Frequency-Inverse Document Frequency (TF-IDF) features, Show and Tell, Show, Attend and Tell, Caption Transformer. Finally, our submission with fine-tuned ResNet-152 achieved 2nd place in the Concept Detection. Additionally, our submission with Show, Attend and Tell achieved 6th place in Caption Prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image CLEF has been held as part of CLEF since 2003. Image CLEF 2023 <ref type="bibr" coords="1,412.75,427.47,12.85,9.74" target="#b0">[1]</ref> focus on multiple applications between different tasks, and ImageCLEFmedical Caption Task <ref type="bibr" coords="1,425.94,441.02,12.90,9.74" target="#b1">[2]</ref> is one of them. ImageCLEFmedical Caption Task 2023 <ref type="bibr" coords="1,266.64,454.57,13.00,9.74" target="#b1">[2]</ref> consists of two sub tasks, Caption Prediction and Concept Detection. Caption Prediction identify concepts based on the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,158.64,481.67,12.69,9.74" target="#b2">[3]</ref> from medical images. Caption Prediction generates description from medical images.</p><p>In this paper, we describe the KDE Lab approach. For Concept Detection, we employed two approaches that are fine-tuned Convolutional Neural Network (CNN) approach and retrieval approach. First approach is retrieval approach that is based on AUEB model <ref type="bibr" coords="1,480.39,535.86,13.00,9.74" target="#b3">[4]</ref> in ImageCLEFmedical Caption Task 2022. We extracted features from medical images using CNN pre-trained on ImageNet <ref type="bibr" coords="1,206.19,562.96,11.59,9.74" target="#b4">[5]</ref>, DenseNet-121 <ref type="bibr" coords="1,292.32,562.96,12.99,9.74" target="#b5">[6]</ref> and ResNet-152 <ref type="bibr" coords="1,383.71,562.96,11.59,9.74" target="#b6">[7]</ref>, then we selected top ùëò concepts with cosine similarity. Second approach is fine-tuned CNN approach that is employed DenseNet-121 <ref type="bibr" coords="2,154.26,101.64,11.41,9.74" target="#b5">[6]</ref>, EfficientNet-B0 <ref type="bibr" coords="2,242.83,101.64,11.41,9.74" target="#b7">[8]</ref>, EfficientNetV2-M <ref type="bibr" coords="2,341.23,101.64,12.83,9.74" target="#b8">[9]</ref> and ResNet-152 <ref type="bibr" coords="2,428.87,101.64,11.41,9.74" target="#b6">[7]</ref>. We add Feed-Forward Neural Network(FFNN) as multi-label classifier to CNN pre-trained on ImageNet <ref type="bibr" coords="2,491.92,115.19,11.45,9.74" target="#b4">[5]</ref>, then we fine-tune our model. Finally, we classify concepts with trained model. For Caption Prediction, we attempted four approaches that are retrieval approach, Show and Tell <ref type="bibr" coords="2,458.56,142.29,16.09,9.74" target="#b9">[10]</ref>, Show, Attend and Tell <ref type="bibr" coords="2,161.70,155.84,16.38,9.74" target="#b10">[11]</ref>, Caption Transformer <ref type="bibr" coords="2,283.22,155.84,16.38,9.74" target="#b11">[12]</ref>. Retrieval approach is based on AUEB model <ref type="bibr" coords="2,89.29,169.38,12.79,9.74" target="#b3">[4]</ref> in ImageCLEFmedical Caption Task 2022. We extracted features from medical images with CNN that is employed DenseNet-121 <ref type="bibr" coords="2,252.67,182.93,12.69,9.74" target="#b5">[6]</ref> and ResNet-152 <ref type="bibr" coords="2,338.82,182.93,12.69,9.74" target="#b6">[7]</ref> pre-trained on ImageNet <ref type="bibr" coords="2,464.63,182.93,11.28,9.74" target="#b4">[5]</ref>. Then we extracted Term Frequency-Inverse Document Frequency (TF-IDF) features among top ùëò sentences in the term of feature similarity. Finally, we selected the highest similarity among TF-IDF features. Show and Tell <ref type="bibr" coords="2,230.98,223.58,17.93,9.74" target="#b9">[10]</ref> and Show, Attend and Tell <ref type="bibr" coords="2,371.98,223.58,17.94,9.74" target="#b10">[11]</ref> are Convolutional Neural Network and Recurrent Neural Network (CNN-RNN) approaches. In the architecture, we employed DenseNet-121 <ref type="bibr" coords="2,185.57,250.68,11.31,9.74" target="#b5">[6]</ref>, EfficientNet-B0 <ref type="bibr" coords="2,273.06,250.68,11.30,9.74" target="#b7">[8]</ref>, EfficientNetV2-M <ref type="bibr" coords="2,370.23,250.68,12.71,9.74" target="#b8">[9]</ref> and ResNet-152 <ref type="bibr" coords="2,456.80,250.68,12.72,9.74" target="#b6">[7]</ref> as CNN and Long Short-Term Memory (LSTM) <ref type="bibr" coords="2,262.66,264.23,17.85,9.74" target="#b12">[13]</ref> as Recurrent Neural Network (RNN). Furthermore, we experimented with the Caption Transformer <ref type="bibr" coords="2,312.52,277.78,16.41,9.74" target="#b11">[12]</ref>, which consists of a Transformer <ref type="bibr" coords="2,487.91,277.78,18.08,9.74" target="#b13">[14]</ref> encoder and decoder.</p><p>We submitted ten submissions for both tasks. As a result, our 10th submission for Concept Detection achieved 2nd place, and our 3rd submission for Caption Prediction achieved 6th place. In the following, we will describe dataset, methods, experiment and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>In this section, we describe dataset for both tasks in ImageCLEFmedical Caption Task 2023 <ref type="bibr" coords="2,489.91,390.60,13.32,9.74" target="#b1">[2]</ref>. Dataset including images, captions and concepts is extended of the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,159.03,417.70,16.09,9.74" target="#b14">[15]</ref>. The number of images is broken down into the training set, which consists of 60,918 images; the validation set, consisting of 10,437 images; and the test set, which comprises 10,473 images. The type of images in the dataset include various modalities such as CT, MRI, and X-ray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>In Concept Detection, dataset consists of pair of image and concepts generated by UMLS <ref type="bibr" coords="2,492.26,508.07,11.55,9.74" target="#b2">[3]</ref>. Table <ref type="table" coords="2,116.14,521.62,5.15,9.74" target="#tab_0">1</ref> shows the top 10 ranking concepts in terms of frequency in training set. The highest concept is C0040405 that appears 20,955 times. In dataset, the number of unique concepts is 2,125, which is less than last year.</p><p>For experiment, we used dataset for training and tuning models. Additionally, the proportion of each set is the same as provided dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>In Caption Prediction, dataset consists of pair of images and captions. Table <ref type="table" coords="2,424.30,625.55,4.99,9.74">2</ref> shows the top 10 ranking words in terms of frequency in training set. In training set with stop-words, the is the most frequency word, which appears 86,173 times. After to remove stop-words, showing that For experiment, we used dataset for training and tuning models. Additionally, the proportion of each set is the same as provided dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we describe the methods used for the submission to Concept Detection and Caption Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>We employed two approaches that are fine-tuned CNN approach and retrieval approach for Concept Detection. Additionally, we attempted two preprocessing types to images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Preprocessing</head><p>Images in the dataset have two types that are color and grayscale. Therefore, we attempted grayscale transform and colorization, as below.</p><p>‚Ä¢ Grayscale Transform : We converted to grayscale(Y) from color(RGB), as below.</p><formula xml:id="formula_0" coords="4,229.86,173.48,276.13,9.74">Y = 0.299 √ó R + 0.587 √ó G + 0.114 √ó B<label>(1)</label></formula><p>‚Ä¢ Colorization : Grayscale images consist of one channel, hence we stacked channel to increase to three from one.</p><p>For training images, we applied random cropping, resizing and horizontal flip as data augmentation. First, we applied random cropping. We calculated cropping height ùëê‚Ñé train and width ùëêùë§ train from original image height ‚Ñé and width ùë§ :</p><formula xml:id="formula_1" coords="4,249.11,283.35,256.88,25.15">ùëê‚Ñé train = ‚àö ùë§ √ó ‚Ñé √ó ùë† ùëü<label>(2)</label></formula><formula xml:id="formula_2" coords="4,248.02,313.63,257.97,11.51">ùëêùë§ train = ‚àöùë§ √ó ‚Ñé √ó ùë† √ó ùëü<label>(3)</label></formula><p>where ùë† denotes scaling, and ùëü is aspect ration. The parameter of scaling ùë† is assigned random value from 0.08 to 1.0, and the parameter of aspect ration is assigned random value from 3/4 to 4/3. Then, we applied resizing. The size of resizing is assigned height ùëü‚Ñé train =224 and width ùëüùë§ train =224. Finally, we applied horizontal flip. The probability of horizontal flip is assigned ùëù=0.5. For test images, we applied resizing and center cropping. The parameters for operations, the resizing size is assigned height ùëü‚Ñé test =256 and width ùëüùë§ test =256 and center cropping size is assigned height ùëê‚Ñé test =224 and width ùëêùë§ test =224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Retrieval Approach</head><p>Retrieval approach achieved the best result in ImageCLEFmedical Caption Task 2021 <ref type="bibr" coords="4,487.15,467.47,16.41,9.74" target="#b15">[16]</ref>. Furthermore, many teams <ref type="bibr" coords="4,203.25,481.02,11.24,9.74" target="#b3">[4,</ref><ref type="bibr" coords="4,216.51,481.02,12.50,9.74" target="#b16">17,</ref><ref type="bibr" coords="4,231.02,481.02,13.95,9.74" target="#b17">18]</ref> attempted retrieval approach in ImageCLEFmedical Caption Task 2022. Thus, we experimented retrieval approach employed CNN and K-Nearest Neighbor (KNN).</p><p>Our approach employed CNN and KNN based on AUEB Lab's approach <ref type="bibr" coords="4,409.48,521.66,12.69,9.74" target="#b3">[4]</ref> in ImageCLEFmedical Caption Task 2022. First, we fine-tuned CNN pre-trained on ImageNet <ref type="bibr" coords="4,431.13,535.21,13.00,9.74" target="#b4">[5]</ref> with medical images. We employed DenseNet-121 <ref type="bibr" coords="4,249.74,548.76,12.69,9.74" target="#b5">[6]</ref> and ResNet-152 <ref type="bibr" coords="4,334.24,548.76,12.69,9.74" target="#b6">[7]</ref> as CNN. Then, we extracted features from medical images with CNN excluding final feed-forward layer. Additionally, we calculated cosine similarity between feature extracted from test image and features extracted from train images. Finally, we selected concepts by weighted majority decision among top ùëò data with high similarity. Weights are assigned as the reciprocal of the ranking. In ImageCLEFmedical Caption Task 2022, some teams applied KNN assigned ùëò = 1. Additionally, if the parameter ùëò is assigned the larger value, the more time it takes. Therefore, we explored the parameter ùëò in KNN from 1 to 50 in increments of 10. Consequently, we assigned ùëò = 10, 20, because F1 Score is the highest in validation set. Figure <ref type="figure" coords="4,263.28,657.16,5.17,9.74">1</ref> shows the architecture of our retrieval approach for Concept Detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Fine-tuned CNN Approach</head><p>In ImageCLEFmedical Caption Task 2022, many teams applied various deep learning approach <ref type="bibr" coords="5,89.29,434.50,11.36,9.74" target="#b3">[4,</ref><ref type="bibr" coords="5,103.38,434.50,12.55,9.74" target="#b16">17,</ref><ref type="bibr" coords="5,118.65,434.50,12.54,9.74" target="#b17">18,</ref><ref type="bibr" coords="5,133.92,434.50,8.21,9.74">19</ref>]. We attempted fine-tuned CNN approach with four different CNN that employed ResNet-152 <ref type="bibr" coords="5,89.29,461.60,11.58,9.74" target="#b6">[7]</ref>, EfficientNet-B0 <ref type="bibr" coords="5,180.68,461.60,11.59,9.74" target="#b7">[8]</ref>, EfficientNetV2-M <ref type="bibr" coords="5,282.12,461.60,13.00,9.74" target="#b8">[9]</ref> and DenseNet-121 <ref type="bibr" coords="5,385.09,461.60,11.59,9.74" target="#b5">[6]</ref>. Our model consists of fine-tuned CNN and FFNN. First, we fine-tuned CNN with medical images in training set. Then, we extracted features from medical images in test set. Finally, we identified concepts using our models. For Experimental details, Adam optimizer is used with learning rate 10 -4 . Additionally, Binary Cross Entropy is used as loss function. Using validation data, we evaluated combination between model and preprocessing, and submitted six models with high F1 Score. Submitted models are two combinations. First one is colorization and EfficientNet-B0 <ref type="bibr" coords="5,412.26,542.90,11.28,9.74" target="#b7">[8]</ref>, EfficientNetV2-M <ref type="bibr" coords="5,89.29,556.45,13.00,9.74" target="#b8">[9]</ref> or DenseNet-121 <ref type="bibr" coords="5,185.27,556.45,13.00,9.74" target="#b5">[6]</ref> as encoder. Second one is grayscale transform and ResNet-152 <ref type="bibr" coords="5,491.73,556.45,11.59,9.74" target="#b6">[7]</ref>, EfficientNet-B0 <ref type="bibr" coords="5,162.00,569.99,13.00,9.74" target="#b7">[8]</ref> or DenseNet-121 <ref type="bibr" coords="5,258.15,569.99,13.00,9.74" target="#b5">[6]</ref> as encoder. Figure <ref type="figure" coords="5,362.41,569.99,5.17,9.74">2</ref> shows the architecture of our fine-tuned CNN approach for Concept Detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>In Caption Prediction, we attempted four approaches that are retrieval approach, Show and Tell <ref type="bibr" coords="5,89.29,646.82,16.25,9.74" target="#b9">[10]</ref>, Show, Attend and Tell <ref type="bibr" coords="5,213.39,646.82,17.91,9.74" target="#b10">[11]</ref> and Caption Transformer <ref type="bibr" coords="5,350.14,646.82,16.25,9.74" target="#b11">[12]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Preprocessing</head><p>Preprocessing Image is same as Concept Detection(Sec 3.1.1). As preprocessing caption, we attempted only lowercase conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Retrieval Approach</head><p>For Caption Prediction, some teams <ref type="bibr" coords="6,248.89,339.40,11.27,9.74" target="#b3">[4,</ref><ref type="bibr" coords="6,262.91,339.40,13.98,9.74" target="#b19">20]</ref> attempted retrieval approach in ImageCLEFmedical Caption Task 2022. Therefore, we experimented retrieval approach based on AUEB's approach <ref type="bibr" coords="6,89.29,366.50,12.84,9.74" target="#b3">[4]</ref> in ImageCLEFmedical Caption Task 2022.</p><p>First, we extracted features from medical images with CNN pre-trained in ImageNet <ref type="bibr" coords="6,492.21,380.05,11.59,9.74" target="#b4">[5]</ref>. We employed ResNet-152 <ref type="bibr" coords="6,207.01,393.60,12.98,9.74" target="#b6">[7]</ref> as CNN. Then, we calculated cosine similarity between feature extracted from test image and features extracted from train images. Additionally, we selected top ùëò data with high similarity. Then, we converted to TF-IDF features from top ùëò captions. Finally, we calculated cosine similarity among those, and the highest one is predicted. We explored parameter ùëò with validation data. Thus, we assigned ùëò = 50, since Bidirectional Encoder Representations from Transformers (BERT) score <ref type="bibr" coords="6,350.76,461.35,17.93,9.74" target="#b20">[21]</ref> is the highest in validation set. Figure <ref type="figure" coords="6,120.36,474.89,5.07,9.74" target="#fig_2">3</ref> shows the architecture of our retrieval approach for Caption Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Show and Tell</head><p>This method is CNN-RNN approach based on Show and Tell <ref type="bibr" coords="6,360.23,524.22,16.26,9.74" target="#b9">[10]</ref>. In our model, we employed ResNet-152 <ref type="bibr" coords="6,143.41,537.77,11.59,9.74" target="#b6">[7]</ref>, DenseNet-121 <ref type="bibr" coords="6,228.01,537.77,11.58,9.74" target="#b5">[6]</ref>, EfficientNet-B0 <ref type="bibr" coords="6,318.40,537.77,13.00,9.74" target="#b7">[8]</ref> and EfficientNetV2-M <ref type="bibr" coords="6,435.86,537.77,13.00,9.74" target="#b8">[9]</ref> as CNN and LSTM <ref type="bibr" coords="6,118.50,551.32,17.86,9.74" target="#b12">[13]</ref> as RNN. For generating caption, we applied greedy decoding how selecting highest probability word. For experimental details, Adam optimizer is used with learning rate 10 -4 . Additionally, Cross Entropy loss is used for loss function. We trained models for 20 epochs with training set. While training, we saved models with minimum loss using validation set. Using validation data, we evaluated combination between model and preprocessing, and submitted four models with high BERT Score <ref type="bibr" coords="6,247.45,619.07,16.31,9.74" target="#b20">[21]</ref>. Submitted models that employed EfficientNet-B0 <ref type="bibr" coords="6,493.09,619.07,12.89,9.74" target="#b7">[8]</ref> and EfficientNetV2-M <ref type="bibr" coords="6,192.52,632.62,13.00,9.74" target="#b8">[9]</ref> as encoder applied colorization or grayscale transform. Figure <ref type="figure" coords="6,501.06,632.62,5.17,9.74" target="#fig_3">4</ref> shows the architecture of our Show and Tell model for Caption Prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Show, Attend and Tell</head><p>This method is CNN-RNN approach based on Show, Attend and Tell <ref type="bibr" coords="7,415.55,371.90,16.42,9.74" target="#b10">[11]</ref>. In our model, we employed ResNet-152 <ref type="bibr" coords="7,209.25,385.45,11.59,9.74" target="#b6">[7]</ref>, DenseNet-121 <ref type="bibr" coords="7,296.27,385.45,11.59,9.74" target="#b5">[6]</ref>, EfficientNet-B0 <ref type="bibr" coords="7,389.09,385.45,13.00,9.74" target="#b7">[8]</ref> and EfficientNetV2-M <ref type="bibr" coords="7,89.29,399.00,13.00,9.74" target="#b8">[9]</ref> as CNN and LSTM <ref type="bibr" coords="7,195.50,399.00,18.08,9.74" target="#b12">[13]</ref> as RNN. Experimental settings such as optimizer, loss function and parameters are same as Show and Tell (Sec 3.2.3). Using validation data, we evaluated combination between model and preprocessing, and submitted four models with high BERT Score <ref type="bibr" coords="7,116.36,439.64,16.27,9.74" target="#b20">[21]</ref>. Submitted models that employed ResNet-152 <ref type="bibr" coords="7,343.05,439.64,12.86,9.74" target="#b6">[7]</ref> and DenseNet-121 <ref type="bibr" coords="7,443.07,439.64,12.86,9.74" target="#b5">[6]</ref> as encoder applied colorization or grayscale transform. Figure <ref type="figure" coords="7,311.60,453.19,4.97,9.74" target="#fig_4">5</ref> shows the architecture of our Show, Attend and Tell model for Caption Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Caption Transformer</head><p>In ImageCLEFmedical Caption Task 2022, some teams <ref type="bibr" coords="7,334.66,516.07,16.50,9.74">[19,</ref><ref type="bibr" coords="7,353.90,516.07,14.08,9.74" target="#b16">17]</ref> used approach based on transformer <ref type="bibr" coords="7,124.23,529.62,18.08,9.74" target="#b13">[14]</ref> and BERT <ref type="bibr" coords="7,196.51,529.62,16.42,9.74" target="#b21">[22]</ref>. Therefore, we attempted Caption Transformer <ref type="bibr" coords="7,442.98,529.62,18.07,9.74" target="#b11">[12]</ref> based on transformer <ref type="bibr" coords="7,144.46,543.17,17.82,9.74" target="#b13">[14]</ref> encoder and decoder. In our model architecture, the encoder extracts features from patch images, while the decoder outputs word probabilities based on previous words and features extracted from images using the attention mechanism. Experimental settings such as optimizer, loss function and parameters are same as Show and Tell (Sec 3.2.3) and Show, Attend and Tell (Sec 3.2.4). We submitted the lowest loss model with validation data. Additionally, we attempted only preprocessing grayscale transform. Figure <ref type="figure" coords="7,366.85,610.92,5.10,9.74" target="#fig_6">6</ref> shows the architecture of our Caption Transformer model for Caption Prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results &amp; Discussion</head><p>In this section, we describe submissions, results and discussion for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>We submitted ten predictions that were identified by models for Concept Detection. Submitted models are fine-tuned CNN approaches and retrieval approaches. Fine-tuned CNN approaches, a total of six models were submitted. We attempted colorization (with color images) and grayscale transform (with grayscale images) for fine-tuned CNN. In detail, we submitted ResNet-152, DenseNet-121 and EfficientNetV2-M with grayscale images, and submitted DenseNet-121, EfficientNet-B0 and EfficientNetV2-M with color images. Retrieval approaches, we submitted four models in different condition. In detail, we submitted ResNet-152 and DenseNet-121 as encoder with grayscale images, and the parameter ùëò in KNN is ùëò = 10, 20. For selecting models, we compared models under various conditions with validation data, and submitted ten models with the highest score. Evaluate metrics is used F1 Score that is calculated between ùë¶ ùëùùëüùëíùëë (predicted binary arrays) and ùë¶ ùë°ùëüùë¢ùëí (correct binary arrays). Additionally, this task used F1 Score Manual that is calculated using manually concepts. Table <ref type="table" coords="8,128.60,611.19,5.17,9.74" target="#tab_1">3</ref> shows result of our submissions for Concept Detection. For test data, our best approach is EfficientNetV2-M + FFNN with color images that had 0.5074 as F1 Score and 0.9320 as F1 Score Manual. Additionally, the model achieved 2nd place on the ranking of task.</p><p>We show comparing such as models and conditions. First, comparing retrieval approaches and fine-tuned CNN approaches, EfficientNetV2-M + FFNN with color images that is best  performed in fine-tuned CNN approaches is higher F1 Score than retrieval approaches such as ResNet-152 + KNN (ùëò = 10). Therefore, we indicated that fine-tuned CNN approach is better performance than retrieval approach among our approaches in concept detection task. Incidentally, we attempted Vision Transformer-B16 <ref type="bibr" coords="9,314.88,496.01,19.75,9.74" target="#b22">[23]</ref> as encoder. However, the model using Vision Transformer-B16 <ref type="bibr" coords="9,196.70,509.56,20.18,9.74" target="#b22">[23]</ref> is lower F1 score than other CNN-based models. Therefore, we didn't submit that model. In the future work, we would like to compare CNN-based models and transformer-based models in medical images. Then, comparing image types, preprocessing colorization at EfficientNetV2-M + FFNN is higher performance than grayscale transform at same architecture. Furthermore, EfficientNet-B0 + FFNN with color images improved performance than using grayscale images. Hence, we indicated that using colorization is better performance than using grayscale transform among our approaches in concept detection task. Incidentally, we couldn't attempt retrieval approaches with colorization. In the future work, we would like to attempt retrieval approaches with colorization and compare grayscale and colorization. Additionally, we would like to attempt other colorization such as pseudo-colorization because colorization was better performance than grayscale transform. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>We submitted ten predicted captions that were generated by models for Caption Prediction. Submitted models are one retrieval approach, one Caption Transformer, four Show and Tell and four Show, Attend and Tell. retrieval approach employed ResNet-152 as encoder, and the parameter ùëò in the model is set 50. Caption Transformer was used only grayscale images. Show and Tell, we submitted four predictions, EfficientNet-B0 as encoder with grayscale images, EfficientNetV2-M as encoder with grayscale images, EfficientNet-B0 as encoder with color images and EfficientNetV2-M as encoder with color images. We submitted four predictions with Show, Attend and Tell. In detail, we employed ResNet-152 as encoder with grayscale images, DenseNet-121 as encoder with grayscale images, ResNet-152 as encoder with color images and DenseNet-121 as encoder with color images. For selecting models, we compared models under various conditions with validation data, and submitted ten models with the highest F1 score. In Evaluate metrics, the primary metric is used BERT Score <ref type="bibr" coords="10,342.58,453.51,17.87,9.74" target="#b20">[21]</ref> that aims to measure the quality comparing between predicted captions and correct captions. Furthermore, the secondary metric is used ROUGE <ref type="bibr" coords="10,157.60,480.60,17.76,9.74" target="#b23">[24]</ref> that measures the number of matching unigram between predicted captions and correct captions. Additionally, BLEURT <ref type="bibr" coords="10,291.93,494.15,16.41,9.74" target="#b24">[25]</ref>, BLUE <ref type="bibr" coords="10,344.24,494.15,16.42,9.74" target="#b25">[26]</ref>, METEOR <ref type="bibr" coords="10,413.30,494.15,16.42,9.74" target="#b26">[27]</ref>, CIDEr <ref type="bibr" coords="10,468.27,494.15,18.08,9.74" target="#b27">[28]</ref> and CLIPScore <ref type="bibr" coords="10,138.27,507.70,17.91,9.74" target="#b28">[29]</ref> are used as metric for test set. Table <ref type="table" coords="10,128.55,521.25,5.17,9.74" target="#tab_2">4</ref> shows result of our submissions for Caption Prediction. For test data, our best approach based on BERT Score <ref type="bibr" coords="10,237.15,534.80,18.07,9.74" target="#b20">[21]</ref> is Show, Attend and Tell : ResNet-152 with grayscale images that had 0.6145 as BERT Score <ref type="bibr" coords="10,259.29,548.35,17.85,9.74" target="#b20">[21]</ref> and 0.2223 as ROUGE <ref type="bibr" coords="10,378.50,548.35,16.18,9.74" target="#b23">[24]</ref>. Additionally, the model achieved 6th place on the ranking of task.</p><p>We show comparing such as models and conditions. First, comparing approaches, Show, Attend and Tell : ResNet-152 with grayscale images is best performance, and Show, Attend and Tell : ResNet-152 with color images is 2nd ranking in our submission. Therefore, we thought that Show, Attend and Tell is better approach than other models. Incidentally, we attempted Vision Transformer-B16 <ref type="bibr" coords="10,196.77,629.65,20.18,9.74" target="#b22">[23]</ref> as encoder in Show and Tell. However, the model using Vision Transformer-B16 <ref type="bibr" coords="10,164.62,643.19,20.18,9.74" target="#b22">[23]</ref> is lower BERT score for validation set than other CNN-based models. Therefore, we didn't submit that model. In addition, Caption Transformer is lowest BERT score than other model. The validity of the Transformer-based model in medical imaging needs to be validated. Then, comparing preprocessing, we used colorization and grayscale transform. As a result, Show and Tell : EfficientNetV2-M and Show, Attend and Tell : DenseNet-121 that using grayscale transform are better performance than using colorization. On the other hands, Show and Tell : EfficientNet-B0 and Show, Attend and Tell : DenseNet-121 that using colorization are better performance than using grayscale transform. Thus, we thought that preprocessing effectiveness depends on encoder. In the future work, the effectiveness of more detailed preprocessing needs to be verified. Incidentally, we would like to attempt other colorization such as pseudo-colorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we described our approach in Concept Detection and Caption Prediction in ImageCLEFmedical Caption Task 2023. For Concept Detection, we employed retrieval approach and fine-tuned CNN approach. Furthermore, we conducted parameter tuning and attempted two preprocessing, colorization and grayscale transform. As a result of the submission, we achieved 2nd place with fine-tuned CNN approach using EfficientNetV2-M. For Caption Prediction, we attempted retrieval approach, CNN-RNN approaches and Caption Transformer. As a result of the submission, we achieved 6th place with Show, Attend and Tell using ResNet-152 as CNN. In feature work, we will compare transformer-based model and CNN-based model. Furthermore, we will attempt pseudo-colorization to outperform our current approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,231.81,416.69,8.91;5,88.93,243.77,292.65,8.91"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Flow of our retrieval approach(CNN + KNN). CNN for test image and train images shared weights. CC BY [Ng et al. (2015)], CC BY-NC [Al Mulhim et al. (2022)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,205.71,416.70,8.91;6,89.29,217.66,103.40,8.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Flow of our retrieval approach for caption prediction. CC BY [Ng et al. (2015)], CC BY-NC [Al Mulhim et al. (2022)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,312.70,364.47,8.91"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of Show and Tell model. CC BY-NC [Al Mulhim et al. (2022)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,89.29,335.92,396.77,8.91"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The architecture of Show, Attend and Tell model. CC BY-NC [Al Mulhim et al. (2022)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,89.29,416.73,393.27,8.91"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The architecture of Caption Transformer model. CC BY-NC [Al Mulhim et al. (2022)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.67,416.99,416.35"><head>Table 1</head><label>1</label><figDesc>Top 10 concepts by frequency in training set for Concept Detection.</figDesc><table coords="3,88.99,122.26,416.99,384.76"><row><cell cols="2">Rank Concept</cell><cell>Name</cell><cell cols="2">Frequency</cell></row><row><cell cols="4">1 C0040405 X-Ray Computed Tomography</cell><cell>20,955</cell></row><row><cell cols="3">2 C1306645 Plain x-ray</cell><cell></cell><cell>17,108</cell></row><row><cell cols="4">3 C0024485 Magnetic Resonance Imaging</cell><cell>10,062</cell></row><row><cell cols="3">4 C0041618 Ultrasonography</cell><cell></cell><cell>8,390</cell></row><row><cell cols="3">5 C0817096 Chest</cell><cell></cell><cell>6,805</cell></row><row><cell cols="4">6 C1999039 Anterior-Posterior</cell><cell>5,907</cell></row><row><cell cols="3">7 C0449900 Contrast used</cell><cell></cell><cell>4,945</cell></row><row><cell cols="3">8 C0002978 angiogram</cell><cell></cell><cell>4,194</cell></row><row><cell cols="4">9 C0037303 Bone structure of cranium</cell><cell>3,058</cell></row><row><cell cols="3">10 C1996865 Postero-Anterior</cell><cell></cell><cell>2,911</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Top 10 words by frequency in Caption Prediction.</cell><cell></cell></row><row><cell cols="3">All words (include stop-words)</cell><cell cols="2">Exclude stop-words</cell></row><row><cell>Rank Word</cell><cell cols="2">Frequency</cell><cell>Rank Word</cell><cell>Frequency</cell></row><row><cell>1 the</cell><cell></cell><cell>86,173</cell><cell>1 showing</cell><cell>16,849</cell></row><row><cell>2 .</cell><cell></cell><cell>74,743</cell><cell>2 right</cell><cell>13,475</cell></row><row><cell>3 of</cell><cell></cell><cell>59,286</cell><cell>3 arrow</cell><cell>13,383</cell></row><row><cell>4 )</cell><cell></cell><cell>35,865</cell><cell>4 left</cell><cell>13,250</cell></row><row><cell>5 (</cell><cell></cell><cell>35,770</cell><cell>5 CT</cell><cell>12,836</cell></row><row><cell>6 ,</cell><cell></cell><cell>31,015</cell><cell>6 image</cell><cell>8,397</cell></row><row><cell>7 and</cell><cell></cell><cell>28,532</cell><cell>7 The</cell><cell>8,123</cell></row><row><cell>8 in</cell><cell></cell><cell>23,855</cell><cell>8 scan</cell><cell>7,960</cell></row><row><cell>9 with</cell><cell></cell><cell>21,789</cell><cell cols="2">9 tomography</cell><cell>7,006</cell></row><row><cell>10 a</cell><cell></cell><cell>21,522</cell><cell>10 shows</cell><cell>6,801</cell></row><row><cell cols="5">appears 16,849 times is the most frequency word. According our analysis, the longest length of</cell></row><row><cell cols="4">captions in dataset is 469 words and the shortest is 1 word.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,90.67,375.02,165.52"><head>Table 3</head><label>3</label><figDesc>Submission results for Concept Detection.</figDesc><table coords="10,131.26,122.24,332.75,133.94"><row><cell>Run ID Model name</cell><cell cols="3">Image type F1 Score F1 Score Manual</cell></row><row><cell>1 ResNet-152 + FFNN</cell><cell>grayscale</cell><cell>0.4979</cell><cell>0.9259</cell></row><row><cell>2 DenseNet-121 + FFNN</cell><cell>grayscale</cell><cell>0.4992</cell><cell>0.9235</cell></row><row><cell cols="2">3 EfficientNetV2-M + FFNN grayscale</cell><cell>0.5000</cell><cell>0.9221</cell></row><row><cell cols="2">4 ResNet-152 + KNN (k=10) grayscale</cell><cell>0.3991</cell><cell>0.7417</cell></row><row><cell cols="2">5 ResNet-152 + KNN (k=20) grayscale</cell><cell>0.3886</cell><cell>0.7252</cell></row><row><cell>6 DenseNet-121 + KNN</cell><cell>grayscale</cell><cell>0.1061</cell><cell>0.2263</cell></row><row><cell>7 DenseNet-121 + KNN</cell><cell>grayscale</cell><cell>0.0993</cell><cell>0.2135</cell></row><row><cell>8 DenseNet-121 + FFNN</cell><cell>color</cell><cell>0.5016</cell><cell>0.9221</cell></row><row><cell>9 EfficientNet-B0 + FFNN</cell><cell>color</cell><cell>0.4979</cell><cell>0.9223</cell></row><row><cell cols="2">10 EfficientNetV2-M + FFNN color</cell><cell>0.5074</cell><cell>0.9320</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,90.67,414.96,104.01"><head>Table 4</head><label>4</label><figDesc>Submission results for Caption Prediction.</figDesc><table coords="11,94.03,119.67,409.91,75.01"><row><cell>Run ID Model name</cell><cell cols="8">Image type BERT Score [21] ROUGE [24] BLEURT [25] BLUE [26] METEOR [27] CIDEr [28] CLIPScore [29]</cell></row><row><cell>1 Show and Tell : EfficientNet-B0</cell><cell>grayscale</cell><cell>0.6088</cell><cell>0.2160</cell><cell>0.2979</cell><cell>0.1640</cell><cell>0.0699</cell><cell>0.1519</cell><cell>0.8043</cell></row><row><cell>2 Show and Tell : EfficientNetV2-M</cell><cell>grayscale</cell><cell>0.6082</cell><cell>0.2143</cell><cell>0.2911</cell><cell>0.1585</cell><cell>0.0686</cell><cell>0.1569</cell><cell>0.8027</cell></row><row><cell>3 Show, Attend and Tell : ResNet-152</cell><cell>grayscale</cell><cell>0.6145</cell><cell>0.2223</cell><cell>0.3013</cell><cell>0.1564</cell><cell>0.0724</cell><cell>0.1818</cell><cell>0.8062</cell></row><row><cell>4 Show, Attend and Tell : DenseNet-121</cell><cell>grayscale</cell><cell>0.6094</cell><cell>0.2004</cell><cell>0.2766</cell><cell>0.1249</cell><cell>0.0596</cell><cell>0.1320</cell><cell>0.7828</cell></row><row><cell cols="2">5 Retrieval approach : ResNet-152 (ùëò = 50) grayscale</cell><cell>0.5789</cell><cell>0.1838</cell><cell>0.2904</cell><cell>0.1484</cell><cell>0.0698</cell><cell>0.0837</cell><cell>0.7826</cell></row><row><cell>6 Caption Transformer</cell><cell>grayscale</cell><cell>0.4425</cell><cell>0.1079</cell><cell>0.2968</cell><cell>0.0709</cell><cell>0.0528</cell><cell>0.0057</cell><cell>0.7304</cell></row><row><cell>7 Show and Tell : EfficientNet-B0</cell><cell>color</cell><cell>0.6097</cell><cell>0.2204</cell><cell>0.3004</cell><cell>0.1694</cell><cell>0.0724</cell><cell>0.1608</cell><cell>0.8080</cell></row><row><cell>8 Show and Tell : EfficientNetV2-M</cell><cell>color</cell><cell>0.6044</cell><cell>0.2166</cell><cell>0.3011</cell><cell>0.1743</cell><cell>0.0730</cell><cell>0.1605</cell><cell>0.8066</cell></row><row><cell>9 Show, Attend and Tell : ResNet-152</cell><cell>color</cell><cell>0.6143</cell><cell>0.2319</cell><cell>0.3063</cell><cell>0.1749</cell><cell>0.0772</cell><cell>0.1989</cell><cell>0.8083</cell></row><row><cell>10 Show, Attend and Tell : DenseNet-121</cell><cell>color</cell><cell>0.6107</cell><cell>0.2152</cell><cell>0.2935</cell><cell>0.1577</cell><cell>0.0693</cell><cell>0.1585</cell><cell>0.8041</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Part of this research was carried out with the support of the <rs type="grantName">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</rs> and the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (C) (issue numbers <rs type="grantNumber">22K12149</rs> and <rs type="grantNumber">22K12040</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YPeFAEt">
					<orgName type="grant-name">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</orgName>
				</org>
				<org type="funding" xml:id="_jFrtEnx">
					<idno type="grant-number">22K12149</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_2PVPa6x">
					<idno type="grant-number">22K12040</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,626.89,394.53,9.74;11,112.66,640.44,395.16,9.74;11,112.66,653.99,394.52,9.74;12,112.66,88.09,395.17,9.74;12,112.39,101.64,394.80,9.74;12,112.48,115.19,394.69,9.74;12,112.66,128.74,395.17,9.74;12,112.66,142.29,393.33,9.74;12,112.66,155.84,394.52,9.74;12,112.33,169.38,120.27,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,223.96,115.19,283.22,9.74;12,112.66,128.74,231.13,9.74">Overview of ImageCLEF 2023: Multimedia Retrieval in Medical, SocialMedia and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,366.82,128.74,141.00,9.74;12,112.66,142.29,393.33,9.74;12,112.66,155.84,136.27,9.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="12,280.08,155.84,221.53,9.74">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,182.93,394.53,9.74;12,112.66,196.48,395.16,9.74;12,112.66,210.03,393.33,9.74;12,112.66,223.58,247.61,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,298.71,196.48,209.11,9.74;12,112.66,210.03,171.63,9.74">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,308.11,210.03,113.90,9.74">CLEF2023 Working Notes</title>
		<title level="s" coord="12,429.51,210.03,76.48,9.74;12,112.66,223.58,97.38,9.74">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,237.13,393.32,9.74;12,112.66,250.68,258.51,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,186.62,237.13,319.36,9.74;12,112.66,250.68,51.86,9.74">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,173.16,250.68,98.78,9.74">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,264.23,395.16,9.74;12,112.66,277.78,394.60,9.74;12,112.66,291.33,180.49,9.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,130.94,277.78,198.05,9.74">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zachariadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Trakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/#paper-101" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1355" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,304.88,393.33,9.74;12,112.66,318.43,393.32,9.74;12,112.33,331.98,299.40,9.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,345.41,304.88,160.58,9.74;12,112.66,318.43,66.08,9.74">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.48,318.43,280.50,9.74;12,112.33,331.98,53.95,9.74">IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,345.52,393.33,9.74;12,112.66,359.07,393.32,9.74;12,112.66,372.62,185.09,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,355.69,345.52,150.31,9.74;12,112.66,359.07,41.53,9.74">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.87,359.07,322.10,9.74;12,112.66,372.62,86.61,9.74">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,386.17,394.61,9.74;12,112.66,399.72,394.53,9.74;12,112.66,413.27,80.57,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,267.97,386.17,214.90,9.74">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,399.72,389.59,9.74">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,426.82,394.53,9.74;12,112.66,440.37,393.33,9.74;12,112.66,453.92,394.53,9.74;12,112.66,467.47,357.68,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,176.64,426.82,325.55,9.74">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,294.11,440.37,211.88,9.74;12,112.66,453.92,131.47,9.74;12,315.67,453.92,186.98,9.74">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning (ICML)<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="12,112.66,481.02,393.32,9.74;12,112.33,494.57,394.85,9.74;12,112.39,508.11,395.27,9.74;12,112.66,521.66,236.96,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,177.02,481.02,223.41,9.74">EfficientNetV2: Smaller Models and Faster Training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/tan21a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,143.20,494.57,359.01,9.74;12,179.20,508.11,184.91,9.74">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning (ICML)<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="12,112.66,535.21,393.32,9.74;12,112.66,548.76,393.32,9.74;12,112.66,562.31,185.09,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,318.68,535.21,187.29,9.74;12,112.66,548.76,43.09,9.74">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.57,548.76,321.40,9.74;12,112.66,562.31,86.61,9.74">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,575.86,394.53,9.74;12,112.66,589.41,393.32,9.74;12,112.33,602.96,394.85,9.74;12,112.39,616.51,395.29,9.74;12,112.66,630.06,280.05,9.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,480.16,575.86,27.03,9.74;12,112.66,589.41,304.64,9.74">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/xuc15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,142.93,602.96,359.28,9.74;12,175.81,616.51,187.29,9.74">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML)<address><addrLine>PMLR, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="12,112.66,643.61,395.16,9.74;12,112.66,657.16,395.01,9.74;12,112.66,671.95,89.53,8.14" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,313.14,643.61,194.68,9.74;12,112.66,657.16,68.52,9.74">CPTR: Full Transformer Network for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2101.10804" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,88.09,393.98,9.74;13,112.41,101.64,212.23,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,251.22,88.09,114.59,9.74">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,375.53,88.09,93.26,9.74">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,115.19,394.53,9.74;13,112.66,128.74,394.53,9.74;13,112.66,142.29,393.32,9.74;13,112.66,155.84,395.01,9.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,175.25,128.74,108.07,9.74">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="m" coord="13,314.47,142.29,191.52,9.74;13,112.66,155.84,88.69,9.74">Advances in Neural Information Processing Systems (NeuralIPS)</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,182.93,393.33,9.74;13,112.33,196.48,395.50,9.74;13,112.66,210.03,394.53,9.74;13,112.66,223.58,393.53,9.74;13,112.66,237.13,393.32,9.74;13,112.66,250.68,392.28,9.74" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,370.24,182.93,135.74,9.74;13,112.33,196.48,171.47,9.74;13,448.67,223.58,57.52,9.74;13,112.66,237.13,393.32,9.74;13,112.66,250.68,118.52,9.74">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<editor>D. Stoyanov, Z. Taylor, S. Balocco, R. Sznitman, A. Martel, L. Maier-Hein, L. Duong, G. Zahnd, S. Demirci, S. Albarqouni, S.-L. Lee, S. Moriconi, V. Cheplygina, D. Mateus, E. Trucco, E. Granger, P. Jannin</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="180" to="189" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</note>
</biblStruct>

<biblStruct coords="13,112.66,264.23,393.32,9.74;13,112.66,277.78,394.53,9.74;13,112.66,291.33,393.32,9.74;13,112.33,304.88,395.33,9.74;13,112.66,318.43,200.09,9.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,464.91,264.23,41.07,9.74;13,112.66,277.78,187.37,9.74">Aueb nlp group at imageclefmed caption tasks 2021</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-96" />
	</analytic>
	<monogr>
		<title level="m" coord="13,174.45,291.33,331.53,9.74;13,112.33,304.88,51.32,9.74;13,242.38,304.88,129.06,9.74">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum (CLEF 2021)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting><address><addrLine>Aachen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2936. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="13,112.66,331.98,393.32,9.74;13,112.66,345.52,395.00,9.74;13,112.66,359.07,205.17,9.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,329.46,331.98,176.52,9.74;13,112.66,345.52,36.22,9.74">CMRE-UoG team at ImageCLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">D</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/#paper-103" />
	</analytic>
	<monogr>
		<title level="m" coord="13,179.19,345.52,186.77,9.74">Concept Detection and Image Captioning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1381" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,372.62,393.32,9.74;13,112.66,386.17,394.52,9.74;13,112.66,399.72,273.20,9.74" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="13,375.29,372.62,130.69,9.74;13,138.76,386.17,299.55,9.74">Medical Concept Detection with Image Retrieval and Code Ensemble</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsuneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/#paper-123" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
	<note>Kdelab at ImageCLEFmedical</note>
</biblStruct>

<biblStruct coords="13,112.66,413.27,395.16,9.74;13,112.66,426.82,394.03,9.74;13,112.30,440.37,95.55,9.74" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Belous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/#paper-109" />
		<title level="m" coord="13,445.71,413.27,62.11,9.74;13,112.66,426.82,136.38,9.74">CSIRO at Im-ageCLEFmedical Caption 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1455" to="1473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,453.92,393.32,9.74;13,112.66,467.47,394.03,9.74;13,112.30,481.02,95.55,9.74" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsuneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-3180/#paper-122" />
		<title level="m" coord="13,375.29,453.92,130.69,9.74;13,112.66,467.47,132.65,9.74">Kdelab at ImageCLEFmedical 2022 Caption Prediction Task</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1596" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,494.57,393.33,9.74;13,112.66,508.11,395.00,9.74;13,112.66,522.91,89.53,8.14" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="13,375.45,494.57,130.53,9.74;13,112.66,508.11,96.14,9.74">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1904.09675" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,535.21,393.32,9.74;13,112.33,548.76,395.33,9.74;13,112.66,562.31,179.39,9.74" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="13,316.83,535.21,189.14,9.74;13,112.33,548.76,184.24,9.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,575.86,395.16,9.74;13,112.66,589.41,393.33,9.74;13,112.41,602.96,395.25,9.74;13,112.66,616.51,261.54,9.74" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="13,419.65,589.41,86.34,9.74;13,112.41,602.96,260.71,9.74">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2010.11929" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,630.06,395.16,9.74;13,112.66,643.61,394.52,9.74;13,112.66,657.16,230.72,9.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,157.53,630.06,266.21,9.74">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m" coord="13,448.09,630.06,59.73,9.74;13,112.66,643.61,286.62,9.74">Text Summarization Branches Out, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,670.70,394.52,9.74;14,112.66,88.09,390.46,9.74" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="13,255.63,670.70,246.68,9.74">BLEURT: Learning Robust Metrics for Text Generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2004.04696" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,101.64,393.32,9.74;14,112.66,115.19,393.32,9.74;14,112.66,128.74,394.52,9.74;14,112.66,142.29,396.59,9.74;14,112.66,157.08,111.32,8.14" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,315.65,101.64,190.32,9.74;14,112.66,115.19,103.13,9.74">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="14,239.49,115.19,266.49,9.74;14,112.66,128.74,133.28,9.74">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,169.38,393.32,9.74;14,112.66,182.93,393.32,9.74;14,112.33,196.48,394.84,9.74;14,112.66,210.03,366.48,9.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,230.21,169.38,275.77,9.74;14,112.66,182.93,109.08,9.74">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3348</idno>
		<ptr target="https://aclanthology.org/W14-3348.doi:10.3115/v1/W14-3348" />
	</analytic>
	<monogr>
		<title level="m" coord="14,245.76,182.93,260.22,9.74;14,112.33,196.48,242.85,9.74">Proceedings of the Ninth Workshop on Statistical Machine Translation, Association for Computational Linguistics</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation, Association for Computational Linguistics<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,223.58,393.32,9.74;14,112.66,237.13,393.32,9.74;14,112.66,250.68,185.09,9.74" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,313.38,223.58,192.60,9.74;14,112.66,237.13,46.69,9.74">CIDEr: Consensus-Based Image Description Evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,186.69,237.13,319.29,9.74;14,112.66,250.68,86.61,9.74">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,264.23,393.32,9.74;14,112.66,277.78,395.01,9.74;14,112.66,291.33,179.39,9.74" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="14,372.74,264.23,133.24,9.74;14,112.66,277.78,175.46,9.74">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2104.08718" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,304.88,393.32,9.74;14,112.66,318.43,393.32,9.74;14,112.14,331.98,327.69,9.74" xml:id="b29">
	<analytic>
		<ptr target="http://ceur-ws.org/Vol-3180/" />
	</analytic>
	<monogr>
		<title level="m" coord="14,344.33,304.88,161.64,9.74;14,112.66,318.43,288.32,9.74">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum (CLEF)</title>
		<title level="s" coord="14,479.59,318.43,26.39,9.74;14,112.14,331.98,100.46,9.74">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum (CLEF)<address><addrLine>Aachen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
