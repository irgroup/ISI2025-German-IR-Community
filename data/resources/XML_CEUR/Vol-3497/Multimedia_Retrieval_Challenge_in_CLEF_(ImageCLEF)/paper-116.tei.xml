<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,105.69,408.93,15.42;1,89.29,127.60,375.14,15.42;1,89.29,149.52,410.98,15.43;1,89.29,171.44,343.41,15.43;1,89.29,193.78,300.74,11.96">Correlating Biomedical Image Fingerprints between GAN-generated and Real Images using a ResNet Backbone with ML-based Downstream Comparators and Clustering: ImageCLEFmed GANs, 2023 Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,219.68,99.56,11.96"><forename type="first">Haricharan</forename><surname>Bharathi</surname></persName>
							<email>haricharan2010267@ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.45,219.68,83.09,11.96"><forename type="first">Anirudh</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.13,219.68,106.69,11.96"><forename type="first">Vishal</forename><surname>Venkataramani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.41,219.68,79.55,11.96"><forename type="first">Karthik</forename><surname>Desingu</surname></persName>
							<email>karthik19047@cse.ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,233.63,97.94,11.96"><forename type="first">Lekshmi</forename><surname>Kalinathan</surname></persName>
							<email>lekshmik@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,105.69,408.93,15.42;1,89.29,127.60,375.14,15.42;1,89.29,149.52,410.98,15.43;1,89.29,171.44,343.41,15.43;1,89.29,193.78,300.74,11.96">Correlating Biomedical Image Fingerprints between GAN-generated and Real Images using a ResNet Backbone with ML-based Downstream Comparators and Clustering: ImageCLEFmed GANs, 2023 Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9A77747601FF329C8C603899FF4F4A9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Network</term>
					<term>ImageCLEF</term>
					<term>Support Vector Machines</term>
					<term>Heirarchical Clustering</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
					<term>ResNet</term>
					<term>Convolutional Neural Networks</term>
					<term>Few shot learning</term>
					<term>Relational model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To address the challenge of ImageClef GAN 2023, this paper proposes a comprehensive approach that incorporates models with a similarity constraint in common.A relation network based on few-shot learning to capture the underlying similarities between real and artificial images is used. This model learns to differentiate between the two classes by leveraging the information from the limited labeled data. Secondly, Agglomerative clustering is used to group similar images together. By identifying clusters predominantly composed of real images, the authors enhance the ability to distinguish between real and artificial images effectively. Lastly, SVM is implemented for classification. The SVM model is trained using the combined feature representations obtained from the real and artificial images. The performance of the SVM on a test set containing 200 real images is evaluated, predicting which of these images were generated using real images. The experimental results demonstrate the effectiveness of the relational model in accurately identifying real images within the test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ImageCLEF GANs task 2023 <ref type="bibr" coords="1,233.57,522.07,12.69,10.91" target="#b0">[1]</ref> is an evaluation campaign organised by the CLEF initiative <ref type="bibr" coords="1,89.29,535.62,11.58,10.91" target="#b1">[2]</ref>. Being the first edition of the task, the primary objective is to detect similarities between synthetic biomedical images generated by GANs and the real images used for training, in order to assess the probability of specific real images being used in the training process. The main</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, Generative Adversarial Networks (GANs) have gained significant attention in the medical field for various image generation and translation tasks. Several studies have explored the use of GANs for medical image synthesis, image-to-image translation, and specifically, the identification or detection of synthetic images. Numerous works have investigated the generation of synthetic medical images using GANs. For instance, <ref type="bibr" coords="2,381.02,470.77,74.24,10.91" target="#b2">Choi et al. (2018)</ref> proposed a method called "StarGAN" <ref type="bibr" coords="2,202.63,484.32,12.68,10.91" target="#b2">[3]</ref> for multi-domain image synthesis, which was successfully applied to generating diverse and realistic brain MRI images. Meanwhile, the paper by <ref type="bibr" coords="2,453.70,497.87,53.97,10.91">Kench et al.</ref> presents SliceGAN <ref type="bibr" coords="2,174.19,511.42,12.73,10.91" target="#b3">[4]</ref> , an architecture that utilizes generative adversarial networks (GANs) to generate high-quality 3D datasets from a single representative 2D image.</p><p>Synthetic images play a crucial role in the medical field as they offer several important advantages and address significant challenges. First, the generation of synthetic images allows for the augmentation of limited or insufficient datasets. In many medical imaging applications, acquiring large and diverse annotated datasets can be challenging and time-consuming. By generating synthetic images, researchers can expand the training data, thereby improving the robustness and generalization of machine learning models. Second, synthetic images enable the simulation of rare or difficult-to-obtain medical scenarios. Certain conditions or diseases may have low prevalence or be challenging to capture through traditional imaging methods. Synthetic images provide a means to create representative cases, allowing researchers and clinicians to study and understand these conditions better, develop diagnostic tools, and explore treatment strategies. Moreover, synthetic images can address privacy concerns related to patient data. Medical images often contain sensitive information, making it difficult to share or publicly release datasets. By generating synthetic images that preserve the statistical and anatomical properties of real data while removing specific patient information, privacy can be maintained, enabling more open collaboration and facilitating research advancements. In summary, synthetic images are indispensable in the medical field, serving as a valuable resource for data augmentation, rare scenario simulation and privacy preservation. Their utilization empowers researchers, clinicians, and technologists to address critical challenges, enhance diagnostic accuracy, improve patient care, and advance medical imaging technologies.</p><p>The paper by Nataraj et al. <ref type="bibr" coords="3,228.82,208.91,12.99,10.91" target="#b4">[5]</ref> proposed a novel approach for detecting GAN-generated fake images by combining co-occurrence matrices and deep learning techniques. The authors extracted the co-occurrence matrices on three color channels of the pixel domain and trained a deep convolutional neural network (CNN) model. The experimental results on two diverse GAN datasets, based on image-to-image translations and facial attributes/expressions, demonstrated the promising performance of the proposed approach, achieving over 99% classification accuracy. The approach also exhibited good generalization capabilities when trained on one dataset and tested on the other. GANs have also been utilized for image-to-image translation tasks in the medical domain. For example, The paper by Zhu et al. <ref type="bibr" coords="3,332.84,317.30,12.85,10.91" target="#b5">[6]</ref> introduces CycleGAN, an approach for translating images from one domain to another without requiring paired training data.</p><p>In summary, prior works have demonstrated the potential of GANs in generating synthetic medical images and performing image-to-image translation tasks. However, the problem of distinguishing synthetic and real medical images remains an active research area, requiring robust methodologies to ensure the reliability and integrity of generated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>The following section explains in detail the systems that were utilized in our submissions for the task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relational Model</head><p>A Relational model was employed which takes a pair of input images comprising a real image and a generated image, with the objective of determining the relationship or similarity between them.</p><p>The two distance metrics considered were L2-Euclidean distance and Cosine distance between samples and support set prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Deep Learning Architectures Considered</head><p>The family of ResNet <ref type="bibr" coords="3,180.02,610.48,14.56,10.91" target="#b6">[7]</ref> mdodels were considered as backbones, ResNet-10, ResNet-12, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152 ResNet101, a widely adopted convolutional neural network model introduced in 2015, addresses the degradation problem encountered in deep networks. This problem refers to the phenomenon where increasing network depth leads to saturated accuracy followed by a rapid decline. To </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Few-shot Learning</head><p>Few-shot learning <ref type="bibr" coords="4,169.06,343.85,13.57,10.91" target="#b7">[8]</ref> is a machine learning approach that deals with the challenge of learning new concepts or classes with limited labeled training data. In traditional machine learning, a substantial amount of labeled data is typically required to train models effectively. Fewshot learning aims to recognise novel visual categories from very few labelled examples. The availability of only one or very few examples challenges the standard 'fine-tuning' practice in deep learning. However, few-shot learning aims to enable learning from a small number of labeled examples, often referred to as the support set or the few-shot training set. The key idea behind few-shot learning is to leverage the knowledge gained from a larger set of base classes or categories to learn new classes with limited labeled data. This is achieved by exploiting the similarities and transferable knowledge between the base classes and the novel classes. Fewshot learning can be achieved through various techniques, including metric-based approaches, model-based methods, or generative models. These methods often utilize techniques such as siamese networks, meta-learning, or data augmentation strategies to enable effective learning with limited labeled data. <ref type="bibr" coords="4,203.45,519.99,13.00,10.91" target="#b7">[8]</ref> deals with the implementation of relation network for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Clustering</head><p>In order to understand the patterns underlying in the data provided, other methods were implemented. One such method which gave success was using an Unsupervised mode of learning, in particular, forming clusters of the real and generated images. Over the several algorithms of clustering available, the one used is Hierarchical Agglomerative Clustering. Hierarchical Agglomerative clustering is a clustering algorithm that aims to group similar data points into clusters based on their pairwise distances or similarities. It starts with each data point as an individual cluster and iteratively merges the most similar clusters until a termination criterion is met. This merging process continues until all data points are part of a single cluster. Hierarchical clustering has the ability to capture the nested structure of data. It is a bottom up approach which builds clusters from the bottom by merging smaller clusters into larger ones.</p><p>The choice of similarity or distance metric is crucial in hierarchical clustering. Commonly used metrics include Euclidean distance, Manhattan distance, cosine similarity, or correlation coefficients, depending on the nature of the data being clustered. The linkage criterion determines how the distances or similarities between clusters are calculated. Some popular linkage criteria are Single Linkage, Complete Linkage, Average Linkage, Ward's Linkage. The type of linkage used in the model is Ward's linkage where the distance between two clusters is determined by the increase in the total within-cluster sum of squares that would result from merging the clusters. Although it is computationally expensive, it can perform on the given dataset with ease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Support Vector Machine</head><p>Another method used encompasses the Supervised mode of Learning with the usage of Support Vector Machines. SVM works by finding an optimal hyperplane that maximally separates the data points of the two classes in the feature space. The hyperplane is defined by a subset of training examples called support vectors. SVMs aim to achieve a balance between maximizing the margin, i.e., the distance between the hyperplane and the closest data points of each class, and minimizing classification errors. It performs well even in high-dimensional feature spaces, where the number of features is much larger than the number of training examples. SVM can handle non-linear classification problems by employing kernel functions. Kernel functions transform the data into a higher-dimensional space where a linear hyperplane can be used for separation. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. The RBF kernel used in the model allows SVM to effectively handle complex, non-linear decision boundaries. It can capture intricate relationships between features and adapt to non-linear patterns in the data. The RBF kernel implicitly maps the input data into a higherdimensional space, avoiding the need for explicit feature engineering or manual transformation. This makes it suitable for cases where the underlying data structure is not well-defined or the relationship between features is non-linear. SVM is utilized in medical research for disease diagnosis, prognosis, and prediction. It can classify medical data such as patient records, genetic data, or medical images to aid in decision-making and support clinical diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Relational Model</head><p>An abstract base class for a few-shot learning model was introduced where the backbone model ResNet-101 used for feature extraction <ref type="bibr" coords="5,292.91,609.21,13.56,10.91" target="#b8">[9]</ref> were instantiated taking a support set as an input. Feature maps are extracted from both images using separate backbone networks. These feature maps are then concatenated to form a comparison candidate tensor. Subsequently, the comparison candidate tensor is passed through the relation module, which is a convolutional neural network. This module produces relation scores indicating the relationship or similarity between the two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hierarchical Clustering</head><p>The ResNet50 model pre-trained on the ImageNet dataset is loaded. The model is configured to exclude the top dense layer and use global average pooling for feature extraction. Feature extraction is performed by normalization of the pixel values of the images from 0 to 1 and feeding them to the ResNet50 model. Feature agglomeration is used to reduce the dimensions of the feature vectors. A label of 0 is assigned to the 80 not used images, a label of 1 is assigned to the 80 used images. Finally, Hierarchical Agglomerative clustering <ref type="bibr" coords="6,389.97,217.99,17.48,10.91" target="#b9">[10]</ref> with the ward linkage method is implemented on the feature vectors of the training images. The labels for the 200 real test images are predicted by applying the trained clustering model to their feature vectors. A Dendrogram <ref type="bibr" coords="6,153.68,258.64,21.64,10.91" target="#b10">[11]</ref> is plotted which computes the Cophenetic correlation coefficient <ref type="bibr" coords="6,475.86,258.64,18.07,10.91" target="#b11">[12]</ref> to evaluate the hierarchical clustering model's quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Support Vector Machines</head><p>A ResNet101 model is employed without its top dense layer to extract feature vectors from each image. The train images are rescaled to lie between 0 and 1 and their features are extracted using the employed model. Additional feature vectors are created by concatenating the feature vectors from the 80 used and 80 not used training images with the feature vectors from the 500 generated images. These concatenated feature vectors are used for training a support vector machine <ref type="bibr" coords="6,125.38,389.66,20.62,10.91" target="#b12">[13]</ref> (SVM) classifier. The SVM classifier is trained using the concatenated feature vectors and their corresponding labels (1 for used images, 0 for not used images) with the help of rbf kernel <ref type="bibr" coords="6,146.53,416.76,18.91,10.91" target="#b13">[14]</ref>.The 10,000 generated images along with 200 real images given for testing are preprocessed and fed to the ResNet101 model for feature extraction as well. The trained classifier is then used to predict the labels (used or not used) for the test feature vectors. The prediction is driven by a majority count of 1's or 0's. EigenValue Analysis is then performed to store the maximum eigenvalue for each feature vector of the used training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Conclusion</head><p>Table <ref type="table" coords="6,115.79,529.58,5.07,10.91" target="#tab_0">1</ref> depicts the official results with the F1-Score being the competition's official metric. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,204.32,416.70,8.93;4,89.29,216.33,158.02,8.87;4,89.29,84.19,416.64,113.55"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow described by the Relational model where image feature vectors are concatenated and suitable distance metric employed</figDesc><graphic coords="4,89.29,84.19,416.64,113.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,567.55,379.53,81.83"><head>Table 1</head><label>1</label><figDesc>Submission runs and their respective scoresSubmission# Accuracy Precision Specificity Recall F1-Score TP TN FP FN</figDesc><table coords="6,147.73,616.60,319.33,32.78"><row><cell>1</cell><cell>50.5</cell><cell>50.3</cell><cell>22</cell><cell>79</cell><cell>61.4</cell><cell>79 22 78 21</cell></row><row><cell>2</cell><cell>49.5</cell><cell>49.5</cell><cell>44</cell><cell>55</cell><cell>52.1</cell><cell>55 44 56 45</cell></row><row><cell>3</cell><cell>52.5</cell><cell>53.7</cell><cell>69</cell><cell>36</cell><cell>43.1</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to express their gratitude to the <rs type="institution">Department of Computer Science and Engineering, Sri Sivasubramaniya Nadar College of Engineering, Chennai, India</rs> (https://www.ssn.edu.in/) for providing the GPU resources for model training and testing.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All results have been submitted under the team name Clef-CSE-GAN and the results are as follows. The relational model has performed the best among the three similarity based approaches achieving a F1-Score of 61.4 denoted by Submission 1. The relational model used ResNet-101 as the backbone model for feature extraction. We notice a higher number of false positives 78 when compared to the other approaches. This might stem from the fact that the decision boundary was very low or the model was not very complex. Future improvements that can be undertaken are to use a more complex model, ones preferably pre-trained on medical image data and tuning the hyperparameters.</p><p>Submission 2 talks about the scores achieved by the Hierarchical clustering approach. It gives us an F1-Score of 52.1 which is the second best amongst the three. The threshold for the number of clusters was decided after plotting a dendogram and taking an approximate threshold.</p><p>Submission 3 portrays the Support Vector Machine's performance with an F1-Score of 43.1. The SVM has a higher number of false negatives-64 when compared to other approaches. This might be because the SVM has a very high sensitivity and might also be because the feature vectors that represent the data need to be more comprehensive. A more complex model can be deployed and the hyperparameters can be tunes for further improvement in scores</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,434.55,393.33,10.91;7,112.66,448.10,393.33,10.91;7,112.66,461.65,393.33,10.91;7,112.66,475.20,356.56,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,448.60,434.55,57.38,10.91;7,112.66,448.10,334.73,10.91">Overview of ImageCLEFmedical GANs 2023 task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,464.74,448.10,41.25,10.91;7,112.66,461.65,287.68,10.91;7,421.96,461.65,84.03,10.91;7,112.66,475.20,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="7,143.49,475.20,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="7,112.66,488.75,394.53,10.91;7,112.66,502.30,395.17,10.91;7,112.66,515.85,394.53,10.91;7,112.66,529.40,395.17,10.91;7,112.39,542.95,394.80,10.91;7,112.48,556.50,394.70,10.91;7,112.66,570.05,395.17,10.91;7,112.66,583.60,393.32,10.91;7,112.66,597.15,394.52,10.91;7,112.33,610.69,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,224.93,556.50,282.25,10.91;7,112.66,570.05,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcıa Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,363.94,570.05,143.89,10.91;7,112.66,583.60,393.32,10.91;7,112.66,597.15,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,280.09,597.15,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,624.24,393.33,10.91;7,112.66,637.79,393.33,10.91;7,112.66,651.34,341.92,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,336.28,624.24,169.71,10.91;7,112.66,637.79,251.94,10.91">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,393.97,637.79,112.02,10.91;7,112.66,651.34,244.01,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,86.97,395.16,10.91;8,112.66,100.52,195.84,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kench</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Cooper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07708</idno>
		<title level="m" coord="8,210.98,86.97,296.85,10.91;8,112.66,100.52,65.57,10.91">Generating 3d structures from a 2d slice with gan-based dimensionality expansion</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,114.06,395.17,10.91;8,112.66,127.61,393.32,10.91;8,112.66,141.16,165.76,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nataraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Bappy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06836</idno>
		<title level="m" coord="8,247.31,127.61,258.67,10.91;8,112.66,141.16,35.79,10.91">Detecting gan generated fake images using co-occurrence matrices</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,154.71,395.17,10.91;8,112.66,168.26,393.32,10.91;8,112.66,181.81,168.20,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,285.32,154.71,222.51,10.91;8,112.66,168.26,138.11,10.91">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,274.15,168.26,231.83,10.91;8,112.66,181.81,70.55,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,195.36,393.60,10.91;8,112.66,208.91,146.44,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Targ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lyman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08029</idno>
		<title level="m" coord="8,249.00,195.36,225.38,10.91">Resnet in resnet: Generalizing residual architectures</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,222.46,394.61,10.91;8,112.66,236.01,393.33,10.91;8,112.66,249.56,276.41,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,410.57,222.46,96.70,10.91;8,112.66,236.01,180.71,10.91">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,324.10,236.01,181.89,10.91;8,112.66,249.56,178.50,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,263.11,393.33,10.91;8,112.66,276.66,394.52,10.91;8,112.66,290.20,55.16,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,341.60,263.11,164.38,10.91;8,112.66,276.66,39.23,10.91">Detnet: Design backbone for object detection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,174.76,276.66,301.91,10.91">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,303.75,393.33,10.91;8,112.66,317.30,104.31,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,214.82,303.75,291.17,10.91">Hierarchical clustering, Introduction to HPC with MPI for Data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,317.30,33.25,10.91">Science</title>
		<imprint>
			<biblScope unit="page" from="195" to="211" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,330.85,340.77,10.91" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caliński</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dendrogram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,231.10,330.85,190.41,10.91">Wiley StatsRef: Statistics Reference Online</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,393.98,10.91;8,112.66,357.95,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,170.67,344.40,187.85,10.91">On the cophenetic correlation coefficient</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Farris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,370.65,344.40,90.77,10.91">Systematic Zoology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="279" to="285" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,371.50,375.65,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,208.31,371.50,109.06,10.91">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,326.19,371.50,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,385.05,393.33,10.91;8,112.28,398.60,212.39,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,231.65,385.05,224.69,10.91">Parameter selection in svm with rbf kernel function</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Qubo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,479.03,385.05,26.95,10.91;8,112.28,398.60,118.24,10.91">World Automation Congress 2012</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
