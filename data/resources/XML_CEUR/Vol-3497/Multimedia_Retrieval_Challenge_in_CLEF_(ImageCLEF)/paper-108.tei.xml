<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,374.35,15.42;1,89.29,106.66,265.51,15.42">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
				<funder>
					<orgName type="full">University of Applied Sciences and Arts Dortmund (FH Dortmund), Germany</orgName>
				</funder>
				<funder ref="#_5qFgh2Y">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Essex GCRF QR Engagement Fund</orgName>
				</funder>
				<funder ref="#_z2ersQs">
					<orgName type="full">Research England</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,84.98,11.96"><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
							<email>johannes.rueckert@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="laboratory">SSNSheerinKavitha</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.66,134.97,88.31,11.96"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>abenabacha@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">Clef-CSE-GAN-Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.57,134.97,116.30,11.96"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff29">
								<orgName type="department">VCMI*</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,422.46,134.97,62.10,11.96"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<email>louise.bloch@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute for Artificial Intelligence in Medicine (IKIM)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff20">
								<orgName type="laboratory">IUST_NLPLAB</orgName>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="laboratory">SSNSheerinKavitha</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,80.30,11.96"><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
							<email>raphael.bruengel@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute for Artificial Intelligence in Medicine (IKIM)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff20">
								<orgName type="laboratory">IUST_NLPLAB</orgName>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="laboratory">SSNSheerinKavitha</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.29,148.92,102.99,11.96"><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
							<email>ahmad.idrissi-yaghir@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="laboratory">SSNSheerinKavitha</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.41,148.92,81.46,11.96"><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
							<email>henning.schaefer@uk-essen.de</email>
							<affiliation key="aff5">
								<orgName type="department">Institute for Transfusion Medicine</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.00,148.92,77.66,11.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff6">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff15">
								<orgName type="department">CS_Morgan</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,162.87,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff20">
								<orgName type="laboratory">IUST_NLPLAB</orgName>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="laboratory">SSNSheerinKavitha</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<address>
									<postCode>2023</postCode>
									<settlement>Bluefield</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">SSN College Of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="department">Baidu Intelligent Health Unit</orgName>
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Beijing, Shenzhen</settlement>
									<country>China and, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff16">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff17">
								<orgName type="institution">CSIRO</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff18">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff19">
								<orgName type="department">Imaging and Computer Vision Group</orgName>
								<orgName type="laboratory">CSIRO Data61</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Pullenvale, Brisbane</settlement>
									<region>Queensland, Queensland</region>
									<country>Australia, Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff21">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff22">
								<orgName type="laboratory">KDE-Lab_Med</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff23">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">KDE Laboratory</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff24">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff25">
								<orgName type="department" key="dep1">ADSPLAB</orgName>
								<orgName type="department" key="dep2">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff26">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff28">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff30">
								<orgName type="institution">University of Porto</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff31">
								<orgName type="institution">INESC TEC</orgName>
								<address>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,374.35,15.42;1,89.29,106.66,265.51,15.42">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CE05124DF4642B73605383CD1C27C5DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Computer Vision</term>
					<term>Multi-Label Classification</term>
					<term>Image Captioning</term>
					<term>Image Understanding</term>
					<term>Radiology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEFmedical 2023 Caption task on caption prediction and concept detection follows similar challenges held from 2017-2022. The goal is to extract Unified Medical Language System (UMLS) concept annotations and/or define captions from image data. Predictions are compared to original image captions. Images for both tasks are part of the Radiology Objects in COntext version 2 (ROCOv2) dataset. For concept detection, multi-label predictions are compared against UMLS terms extracted from the original captions with additional manually curated concepts via the F1-score. For caption prediction, the semantic similarity of the predictions to the original captions is evaluated using the BERTScore. The task attracted strong participation with 27 registered teams, 13 teams submitted 116 graded runs for the two subtasks. Participants mainly used multi-label classification systems for the concept detection subtask, the winning team AUEB-NLP-Group used an ensemble of three CNNs. For the caption prediction subtask, most teams used encoder-decoder architectures, with the winning team CSIRO using an encoder-decoder framework with an additional reinforcement learning optimization step.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="foot" coords="2,141.67,109.52,3.71,7.97" target="#foot_0">1</ref> is the image retrieval and classification lab of the CLEF (Conference and Labs of the Evaluation Forum) conference. ImageCLEF 2023 consists of the ImageCLEFmedical, ImageCLEFfusion and ImageCLEFaware labs, with the ImageCLEFmedical lab being divided into the subtasks MEDIQA-Sum (natural language semantic retrieval), Caption, GANs (generation of medical images), and MEDVQA-GI (gastrointestinal visual question answering).</p><p>The Caption task was first proposed as part of the ImageCLEFmedical <ref type="bibr" coords="2,419.65,179.03,13.00,10.91" target="#b0">[1]</ref> in 2016. In 2017 and 2018 <ref type="bibr" coords="2,131.00,192.57,11.27,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,145.01,192.57,8.91,10.91" target="#b2">3]</ref> the ImageCLEFmedical caption task comprised two subtasks: concept detection and caption prediction. In 2019 <ref type="bibr" coords="2,227.48,206.12,12.69,10.91">[4]</ref> and 2020 <ref type="bibr" coords="2,284.12,206.12,11.28,10.91" target="#b4">[5]</ref>, the task concentrated on the concept detection subtask extracting Unified Medical Language System ® (UMLS) Concept Unique Identifiers (CUIs) <ref type="bibr" coords="2,119.84,233.22,12.84,10.91" target="#b5">[6]</ref> from radiology images.</p><p>In 2021 <ref type="bibr" coords="2,134.56,246.77,11.29,10.91" target="#b6">[7]</ref>, both subtasks, concept detection and caption prediction, were running again due to participants demands. The focus in 2021 was on making the task more realistic by using fewer images which were all manually annotated by medical doctors. As additional data of similar quality is hard to acquire, the 2022 ImageCLEFmedical caption task <ref type="bibr" coords="2,422.97,287.42,12.80,10.91" target="#b7">[8]</ref> continued with both subtasks albeit with an extended version of the Radiology Objects in COntext (ROCO) <ref type="bibr" coords="2,493.24,300.97,12.75,10.91" target="#b8">[9]</ref> dataset used for both subtasks, which was already used in 2020 and 2019. The 2023 edition of ImageCLEFmedical caption continues in the same vein, once again using a ROCO-based dataset for both subtasks but switching from BLEU to BERTScore as the primary evaluation metric for caption prediction.</p><p>This paper sets forth the approaches for the caption task: automated cross-referencing of medical images and captions into predicted coherent captions and UMLS concept detection in radiology images as a separate subtask. This task is a part of the ImageCLEF benchmarking campaign, which has proposed medical image understanding tasks since 2003; a new suite of tasks is generated each subsequent year. Further information on the other proposed tasks at ImageCLEF 2023 can be found in Ionescu et al. <ref type="bibr" coords="2,299.21,436.46,16.25,10.91" target="#b9">[10]</ref>. This is the 7th edition of the ImageCLEFmedical caption task. Just like in 2016 <ref type="bibr" coords="2,450.93,450.01,11.44,10.91" target="#b0">[1]</ref>, 2017 <ref type="bibr" coords="2,491.93,450.01,11.44,10.91" target="#b1">[2]</ref>, 2018 <ref type="bibr" coords="2,112.48,463.56,11.50,10.91" target="#b2">[3]</ref>, 2021 <ref type="bibr" coords="2,153.73,463.56,11.50,10.91" target="#b6">[7]</ref>, and 2022 <ref type="bibr" coords="2,214.26,463.56,12.91,10.91" target="#b7">[8]</ref> both subtasks of concept detection and caption prediction are included in ImageCLEFmedical Caption 2023. Like in 2022, an extended subset of the ROCO <ref type="bibr" coords="2,493.30,477.11,12.68,10.91" target="#b8">[9]</ref> dataset is used, with images that are not licensed CC BY or CC BY-NC removed.</p><p>Manual generation of the knowledge of medical images is a time-consuming process prone to human error. As this process requires assistance for the better and easier diagnoses of diseases that are susceptible to radiology screening, it is important that we better understand and refine automatic systems that aid in the broad task of radiology-image metadata generation. The purpose of the ImageCLEFmedical 2023 caption prediction and concept detection tasks is the continued evaluation of such systems. Concept detection and caption prediction information is applicable to unlabelled and unstructured datasets and medical datasets that do not have textual metadata. The ImageCLEFmedical caption task focuses on the medical image understanding in the biomedical literature and specifically on concept extraction and caption prediction based on the visual perception of the medical images and medical text data such as medical caption or UMLS CUIs paired with each image (see Figure <ref type="figure" coords="2,301.58,639.70,3.57,10.91" target="#fig_0">1</ref>).</p><p>In 2023, for the development data, an extended subset of the ROCO <ref type="bibr" coords="3,409.51,86.97,12.99,10.91" target="#b8">[9]</ref> dataset from 2022 was used, with new images from the same source added for the validation and test sets, while images from articles with licenses other than CC BY and CC BY-NC were removed.</p><p>This paper presents an overview of the ImageCLEFmedical caption task 2023 including the task and participation in Section 2, the data creation in Section 3, and the evaluation methodology in Section 4. The results are described in Section 5, followed by conclusion in Sections 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Participation</head><p>In 2023, the ImageCLEFmedical caption task consisted of two subtasks: concept detection and caption prediction.</p><p>The concept detection subtask follows the same format proposed since the start of the task in 2017 <ref type="bibr" coords="3,124.53,253.99,11.58,10.91" target="#b1">[2]</ref>. Participants are asked to predict a set of concepts defined by the UMLS CUIs <ref type="bibr" coords="3,492.99,253.99,13.00,10.91" target="#b5">[6]</ref> based on the visual information provided by the radiology images.</p><p>The caption prediction subtask follows the original format of the subtask used between 2017 and 2018 <ref type="bibr" coords="3,156.27,294.63,11.49,10.91" target="#b1">[2,</ref><ref type="bibr" coords="3,170.70,294.63,7.65,10.91" target="#b2">3]</ref>. This subtask was paused and it is running again since 2021 because of participant demand. This subtask aims to automatically generate captions for the radiology images provided.</p><p>In 2023, 27 teams registered and signed the End-User-Agreement that is needed to download the development data. 13 teams submitted 116 graded runs for evaluation (12 teams submitted working notes) attracting similar attention than in 2022 <ref type="bibr" coords="3,339.02,362.38,11.41,10.91" target="#b7">[8]</ref>. Each of the groups was allowed a maximum of 10 graded runs per subtask.</p><p>Table <ref type="table" coords="3,127.27,389.48,5.12,10.91">1</ref> shows all the teams who participated in the task and their submitted runs. 9 teams participated in the concept detection subtask this year, 6 of those teams also participated in 2022 <ref type="bibr" coords="3,89.29,416.58,11.28,10.91" target="#b7">[8]</ref>. 13 teams submitted runs to the caption prediction subtask, 7 of those teams also participated in 2022. Three of the teams participated also in 2021. Overall, 9 teams participated in both subtasks, and four teams participated only in the caption prediction subtask. Unlike in 2022, no teams participated only in the concept detection subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Creation</head><p>Figure <ref type="figure" coords="3,120.36,515.85,5.07,10.91" target="#fig_0">1</ref> shows an example from the dataset provided by the task.</p><p>Like last year, a dataset that originates from biomedical articles of the PMC Open Access Subset<ref type="foot" coords="3,117.93,541.19,3.71,7.97" target="#foot_2">2</ref>  <ref type="bibr" coords="3,124.85,542.95,17.80,10.91" target="#b21">[22]</ref> was used and was extended with new images added since the last time the dataset was updated in October 2021. The overall lower number of images is due to the removal of non-CC BY images (including CC BY-SA and CC BY-ND).</p><p>Unlike last year, no extensive caption pre-processing beyond the removal of links was performed to keep the captions as realistic as possible. Captions in languages other than English were also removed.</p><p>From the resulting captions, concepts were extracted using the Medical Concept Annotation Toolkit (MedCAT) <ref type="bibr" coords="3,175.57,637.79,16.41,10.91" target="#b22">[23]</ref>. MedCAT, which is capable of extracting biomedical concepts from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Participating groups in the ImageCLEFmedical 2023 caption task and their graded runs submitted to both subtasks: T1-Concept Detection and T2-Caption Prediction. Teams with previous participation in 2022 are marked with an asterisk (*).</p><p>unstructured text, was trained on the MIMIC-III dataset <ref type="bibr" coords="4,346.92,562.87,18.07,10.91" target="#b23">[24]</ref> and links to SNOMED CT IDs, which were later mapped to CUIs and TUIs of the UMLS2022AB release <ref type="foot" coords="4,422.44,574.67,3.71,7.97" target="#foot_3">3</ref> . During concept extraction, concepts were retained only if they exceeded a frequency threshold of 10 occurences, and semantic filters were applied to focus on visually observable and interpretable concepts. For example, concepts of semantic type T029 (Body Location or Region) or T060 (Diagnostic Procedure) are relevant, while concepts of semantic type T054 (Social Behavior) cannot be derived from the image if it would appear in the caption. In addition, manual filtering was performed to exclude UMLS concepts that were either incorrectly detected by the pipeline or were still not related to the image content in any way after semantic filtering. Blacklisted concepts often include qualifiers that would divert actual interest to, for example, anatomical localization or a pathological process, and would also introduce bias, since qualifiers are used in a highly individual and variable manner. Entity linking systems tend to link concepts with ambiguous synonyms incorrectly, e.g. C0994894 (Patch Dosage Form) may be linked if the caption refers to a region that is patchy. In case of high frequency occurrence of such concepts, they were merged to the correct concept via mapping. Due to the different filtering approach, this year's dataset contains 2,125 concepts compared to 8,374 last year. Additional concepts were assigned to all images addressing their image modality. Six medical image modalities of concepts were covered: X-ray, Computer Tomography (CT), Magnetic Resonance Imaging (MRI), ultrasound, and Positron Emission Tomography (PET) as well as modality combinations (e.g., PET/CT) as standalone concept. For images of the X-ray modality further concepts on the represented anatomy were assigned, covering specific anatomical body regions of the Image Retrieval in Medical Application (IRMA) <ref type="bibr" coords="5,358.40,545.05,17.76,10.91" target="#b24">[25]</ref> classification: cranium, spine, upper extremity/arm, chest, breast/mamma, abdomen, pelvis, and lower extremity/leg. New for this year's dataset is the addition of manually validated directionality concepts for x-ray images. Directionality refers to the x-ray imaging orientation according to IRMA: coronal posteroanterior (PA), coronal anteroposterior (AP), sagittal, or transversal. Each of the described concept extensions were created performing a two-stage process. In the first stage, predictions via classification models were created and assigned as annotations. For modality prediction for all images a model trained on the ROCO dataset <ref type="bibr" coords="5,322.09,639.90,11.44,10.91" target="#b8">[9]</ref>, and for anatomy prediction for X-ray modality images a model trained on an existing IRMA-annotated image dataset <ref type="bibr" coords="5,444.06,653.45,17.90,10.91" target="#b25">[26]</ref> was used. For directionality, roughly 20,000 images were manually annotated to train an initial classifier.</p><p>In the second stage, these annotations underwent manual quality control measures, involving correction of faulty predictions and filtering of images that did not represent one of the minded modality or anatomy concepts.</p><p>The following subsets were distributed to the participants where each image has one caption and one or more concepts (UMLS-CUI):</p><p>• Training set including 60,918 radiology images and associated captions and concepts, with a total of 263,091 concept occurrences and 2,125 unique concepts. • Validation set including 10,437 radiology images and associated captions and concepts, with a total of 46,584 concept occurrences and 1,945 unique concepts. • Test set including 10,473 radiology images, with a total of 46,955 concept occurrences and 1,936 unique concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Methodology</head><p>In this year's edition, the performance evaluation for the concept detection subtask is carried out in the same way as last year, while the primary evaluation metric for the caption prediction subtask is changed from BLEU to BERTScore. Both tasks are evaluated separately. AIcrowd was not used as a challenge platform this year, instead participants were asked to upload their submissions to a cloud share file drop, with information about whether each submission was successfully evaluated and announced on a website that was regularly updated. An important difference to the last years was the fact that participants were unaware of their own scores on the test set until after the submission deadline. This was done to avoid teams optimizing their approaches based on test set results, which would amount to information leakage.</p><p>For the concept detection subtask, the balanced precision and recall trade-off were measured in terms of F1-scores. Like last year, a secondary F1-score is computed using a subset of concepts that was manually curated. On the one hand, this involves the different image modalities (X-ray, Angiography, Ultrasound, CT, MRI, PET, and Combined such as PET/CT). On the other hand, if applicable, for X-ray also the most prominently depicted body region (cranium, chest, upper extremity, spine, abdomen, pelvis, and lower extremity), and the capture directionality (coronal anteroposterior, coronal posteroanterior, sagittal, and transversal) were involved.</p><p>As a pre-processing step for evaluating the second task, all captions were lowercased, punctuation was removed, and numbers were replaced by the token "number". This step ensures uniformity and focuses the evaluation on the linguistic content. The performance of caption prediction is evaluated based on BERTScore <ref type="bibr" coords="6,267.84,553.18,16.09,10.91" target="#b26">[27]</ref>, which is a metric that computes a similarity score for each token in the generated text with each token in the reference text. It uses the pre-trained contextual embeddings from BERT-based models and matches words by cosine similarity. In this work, the pre-trained model microsoft/deberta-xlarge-mnli<ref type="foot" coords="6,340.18,592.08,3.71,7.97" target="#foot_4">4</ref> was used because it is the model that correlates best with human scoring according to the authors <ref type="foot" coords="6,356.98,605.63,3.71,7.97" target="#foot_5">5</ref> . Since evaluating generated text and image captioning is very challenging and should not be based on a single metric, additional evaluation metrics were explored in this year's edition in order to find the metrics that correlate well with human judgments for this task. First, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) <ref type="bibr" coords="7,184.38,100.52,17.94,10.91" target="#b27">[28]</ref> score was adopted as a secondary metric that counts the number of overlapping units such as n-grams, word sequences, and word pairs between the generated text and the reference. Specifically, the ROUGE-1 (F-measure) score was calculated, which measures the number of matching unigrams between the model-generated text and a reference. All individual scores for each caption are then summed and averaged over the number of captions, resulting in the final score. In addition to ROUGE, the Metric for Evaluation of Translation with Explicit ORdering (METEOR) <ref type="bibr" coords="7,220.67,181.81,17.78,10.91" target="#b28">[29]</ref> was explored, which is a metric that evaluates the generated text by aligning it to reference and calculating a sentence-level similarity score. Furthermore, the Consensus-based Image Description Evaluation (CIDEr) <ref type="bibr" coords="7,349.55,208.91,17.75,10.91" target="#b29">[30]</ref> metric was also adopted. CIDEr is an automatic evaluation metric that calculates the weights of n-grams in the generated text, and the reference text based on term frequency and inverse document frequency (TF-IDF) and then compares them based on cosine similarity. Another metric used is the BiLingual Evaluation Understudy (BLEU) score <ref type="bibr" coords="7,202.28,263.11,16.09,10.91" target="#b30">[31]</ref>, which is a geometric mean of n-gram scores from 1 to 4. For this task, the focus was on the BLEU-1 score, which takes into account unigram precision. Compared to last year, BLEURT and CLIPScore were newly introduced. BLEURT (Bilingual Evaluation Understudy with Representations from Transformers.) <ref type="bibr" coords="7,333.05,303.75,17.85,10.91" target="#b31">[32]</ref> is specifically designed to evaluate natural language generation in English. It uses a pre-trained model that has been fine-tuned to emulate human judgments about the quality of the generated text. The strength of BLEURT lies in its end-to-end training, which enables it to model human judgments effectively and makes it robust to domain and quality variations. For this evaluation, the BLEURT-20 model was used. CLIPScore <ref type="bibr" coords="7,163.46,371.50,17.81,10.91" target="#b32">[33]</ref> is an innovative metric that diverges from the traditional reference-based evaluations of image captions. Instead, it aligns with the human approach of evaluating caption quality without references by evaluating the alignment between text and image content. The metric employs CLIP (Contrastive Language-Image Pretraining) <ref type="bibr" coords="7,372.31,412.15,16.13,10.91" target="#b33">[34]</ref>, a cross-modal model that has been pre-trained on a massive dataset of 400 million image-caption pairs sourced from the web. The model is used to compute similarity scores between images and text. The introduction of BLEURT and CLIPScore in this edition aims to further align the evaluation process with human judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For the concept detection and caption prediction subtasks, Tables <ref type="table" coords="7,379.12,524.97,5.01,10.91" target="#tab_0">2</ref> and<ref type="table" coords="7,405.78,524.97,5.01,10.91" target="#tab_1">3</ref> show the best results from each of the participating teams. The results will be discussed in this section. The full list of results are shown in appendix A in tables 5, 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results for the Concept Detection subtask</head><p>In 2023, 9 teams participated in the concept prediction subtask, submitting 47 graded runs. Table <ref type="table" coords="7,115.79,615.34,5.07,10.91" target="#tab_0">2</ref> presents the results achieved in the submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUEB-NLP-Group</head><p>Like in previous years, the AUEB-NLP-Group submitted the best performing result with a primary F1-score of 0.5223 <ref type="bibr" coords="7,310.29,654.40,17.76,10.91" target="#b10">[11]</ref> and a secondary F1-score of 0.9258. The winning approach was an ensemble of three CNNs (EfficientNetB0, DenseNet121, and EfficientNetB0v2) followed by a feed-forward neural network (FFNN) classification head, which is a very similar approach as last year <ref type="bibr" coords="8,312.21,320.17,16.08,10.91" target="#b34">[35]</ref>, where an ensemble of two such models won the concept detection subtask. They also experimented with training separate models for the different modalities, which did not lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KDE-Lab_Med</head><p>The KDE-Lab_Med team submitted the second best performing approach, with a primary F1-score of 0.5074 <ref type="bibr" coords="8,272.45,383.33,18.05,10.91" target="#b17">[18]</ref> and a secondary F1-score of 0.9321, which was the highest overall secondary F1-score. Their best approach is a single CNN+FFNN model with an EfficientNetV2-M backbone. They experimented with image pre-processing by either converting color to grayscale or colorization of grayscale images by stacking color channels. The latter approach performed better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VCMI</head><p>The VCMI team achieved the third place in the concept detection subtask with a primary F1-score of 0.4998 <ref type="bibr" coords="8,200.88,473.59,17.85,10.91" target="#b20">[21]</ref> and a secondary F1-score of 0.9162. Their best approach utilizes an autoregressive multi-label classification system with a VGG16 network pre-trained on ImageNet, which instead of using a single classification layer at the end, uses 17 classification layers each predicting 125 concepts. For any images that are not assigned any concepts using this model, an image retrieval system assigns concepts appearing in at least two of the four most similar images in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IUST_NLPLAB</head><p>The IUST_NLPLAB team reached a primary F1-score of 0.4959 <ref type="bibr" coords="8,459.43,563.85,18.07,10.91" target="#b16">[17]</ref> and a secondary F1-score of 0.8804. They used a multi-label classification system based on the vision-language model PubMedCLIP for their best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clef-CSE-GAN-Team</head><p>The Clef-CSE-GAN-Team achieved a primary F1-score of 0.4957 <ref type="bibr" coords="8,488.03,613.47,17.96,10.91" target="#b12">[13]</ref> and a secondary F1-score of 0.9106. They employed a multi-label classification system with a DenseNet121 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS_Morgan</head><p>The CS_Morgan team reached a primary F1-score of 0.4834 <ref type="bibr" coords="8,415.23,663.08,17.76,10.91" target="#b14">[15]</ref> and a secondary F1-score of 0.8902. Their best approach used a multi-label classification system with a DenseNet121 backbone using CheXNet pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSN_MLRG and SSNSheerinKavitha</head><p>The team SSN_MLRG achieved a primary F1-score of 0.4649 <ref type="bibr" coords="9,148.78,135.91,17.76,10.91" target="#b19">[20]</ref> and a secondary F1-score of 0.8603. They employed a multi-label classification system using ConceptNet.</p><p>To summarize, in the concept detection subtasks, the groups used primarily multi-label classification systems, with image retrieval systems consistently performing worse for teams who experimented with them. One team successfully used an image retrieval system as a fallback when the multi-label classification system did not predict any concepts <ref type="bibr" coords="9,436.60,213.59,16.08,10.91" target="#b20">[21]</ref>. As in 2022, the AUEB-NLP-Group once again achieved the top scores by increasing their ensemble from two to three models <ref type="bibr" coords="9,180.99,240.69,16.25,10.91" target="#b10">[11]</ref>.</p><p>The overall F1 scores increased compared to last year which is not surprising considering a reduced number of concepts for this year's edition of the challenge.</p><p>While one team experimented with a novel autoregressive multi-label classification system which tries to model relationships between concepts and another team tried training separate models for the different modalities, these experiments did not yield better results compared to the winning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results for the Caption Prediction subtask</head><p>In this seventh edition, the caption prediction subtask attracted 13 teams which submitted 69 graded runs. Tables <ref type="table" coords="9,179.19,384.94,5.07,10.91" target="#tab_1">3</ref> and<ref type="table" coords="9,206.14,384.94,5.07,10.91" target="#tab_2">4</ref> present the results of the submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSIRO</head><p>The CSIRO team achieved first place in the caption prediction subtask with a BERTScore of 0.6413 <ref type="bibr" coords="10,161.04,367.99,17.76,10.91" target="#b15">[16]</ref> and a ROUGE score of 0.2463. The winning approach, which also reached the highest CLIPScore, consists of an encoder-decoder framework based on the Convolutional Vision Transformer (CVT) as the encoder and DistilGPT2 as the decoder. This approach was already used by them in last year's addition and reached the overall highest BERTScore then. For this year, they added a reinforcement learning step "to optimize the model for the primary metric and the means of conditioning the decoder on the visual features" <ref type="bibr" coords="10,159.95,449.28,16.41,10.91" target="#b15">[16]</ref>, which further improved the performance and set them apart from the competition.</p><p>closeAI The closeAI team reached the second place spot with a BERTScore of 0.6281 <ref type="bibr" coords="10,469.28,485.35,17.80,10.91" target="#b13">[14]</ref> and a ROUGE score of 0.2401. Their approach, which reached top scores in the BLEURT and CIDEr metrics, consisted of a BLIP-2 framework with a ViT-g image encoder from EVA-CLIP, a Q-Former and OPT2.7 as the LLM with post-processing to remove duplicate content from the generated captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUEB-NLP-Group</head><p>The AUEB-NLP-Group reached a BERTScore of 0.6170 <ref type="bibr" coords="10,425.72,562.06,17.76,10.91" target="#b10">[11]</ref> and a ROUGE score of 0.2130, placing third. Their best approach is a novel captioning pipeline using a denoising model to rewrite captions produced by a CNN-RNN encoder decoder model using sequence to sequence models BART and T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCLmed</head><p>The PCLmed team achieved a BERTScore of 0.6152 <ref type="bibr" coords="10,379.55,625.22,18.07,10.91" target="#b18">[19]</ref> and a ROUGE score of 0.2528. Much like closeAI, they used a BLIP-2 framework with an EVA-ViT-g encoder, a Query Transformer, and ChatGLM-6B as the LLM with a final beam search with a repetition penalty to generate the captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VCMI</head><p>The VCMI team achieved a BERTScore of 0.6147 <ref type="bibr" coords="11,348.59,86.97,18.07,10.91" target="#b20">[21]</ref> and a ROUGE score of 0.2175.</p><p>They used an encoder-decoder framework with a Data-efficient image Transformer (DeiT) as the encoder and DistilGPT-2 as the decoder for their best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KDE-Lab_Med</head><p>The KDE-Lab_Med team reached a BERTScore of 0.6145 <ref type="bibr" coords="11,423.19,135.88,17.97,10.91" target="#b17">[18]</ref> and a ROUGE score of 0.2223. Their best approach is a CNN-RNN system based on Show, Attend, and Tell with a ResNet152 backbone and LSTM as RNN. They also experimented with a Caption Transformer system, which did not perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSN_MLRG and SSNSheerinKavitha</head><p>The team SSN_MLRG achieved a BERTScore of 0.6019 <ref type="bibr" coords="11,149.59,211.90,18.03,10.91" target="#b19">[20]</ref> and a ROUGE score of 0.2112. They used an encoder-decoder system with DeiT as the encoder and Distilled-GPT2 as the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DLNU_CCSE</head><p>The DLNU_CCSE team reached a BERTScore of 0.6005 and a ROUGE score of 0.2029. They used an encoder-decoder framework with a ResNet-101 encoder and an LSTM decoder for their best approach. They did not submit working notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS_Morgan</head><p>The CS_Morgan team reached a BERTScore of 0.5819 <ref type="bibr" coords="11,396.79,296.18,17.99,10.91" target="#b14">[15]</ref> and a ROUGE score of 0.1564. They used an encoder-decoder system with a Vision Transformer (ViT) as the encoder, where the decoder generates keywords which are then transformed into captions by a T5 generative model fine-tuned for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clef-CSE-GAN-Team</head><p>The Clef-CSE-GAN-Team achieved a BERTScore of 0.5816 <ref type="bibr" coords="11,467.93,358.65,18.07,10.91" target="#b12">[13]</ref> and a ROUGE score of 0.2181. They used an encoder-decoder approach with a ResNet101 encoder and an LSTM decoder for their best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bluefield</head><p>The Bluefield team reached a BERTScore of 0.5780 <ref type="bibr" coords="11,379.63,407.56,18.07,10.91" target="#b11">[12]</ref> and a ROUGE score of 0.1534. They first classified the images into six groups roughly corresponding to the imaging modalities, and then used six different CLIP models with a ResNet50 backbone to generate captions for the images.</p><p>IUST_NLPLAB Last years winners reached a BERTScore of 0.5669 <ref type="bibr" coords="11,397.23,470.03,17.98,10.91" target="#b16">[17]</ref> and the overall best ROUGE score of 0.2898. Like last year, they used a multi-label classification approach where the top 20 words words are returned as the caption in the order of their probability. This system once again achieved top scores in the ROUGE, BLEU, and METEOR scores, but did not perform as well on the remaining metrics, including the primary metric BERTScore.</p><p>To summarize, in the caption prediction subtask most teams experimented with encoderdecoder frameworks with different backbones and LSTM decoders. Unsurprisingly, teams increasingly used LLMs in the decoding step and to help generate or refine captions. BLIP-2 was used for the first time and achieved good results (second and fourth place). One novelty was the use of reinforcement learning to refine and improve upon last year's best solution in terms of BERTScore, which ended up winning this year's competition after the change of primary scores from BLEU to BERTScore.</p><p>The aforementioned change of evaluation metrics had a big effect on the outcome of the challenge, with last year's winner placing second to last according to the BERTScore evaluation while still winning in terms of the ROUGE, BLEU and METEOR scores with a similar approach as last year. We will continue to evaluate and explore different possible metrics or combination of metrics, but the evaluation of generated captions remains difficult.</p><p>BERTScore and ROUGE scores were used to predict captions. Unlike the previous edition, BERTScore replaced BLEU as the primary score for a more refined evaluation of the caption task. The adoption of BERTScore reflects the intent to prioritize semantic alignment and information preservation in the generated captions rather than focusing on the frequency of n-gram matches, which is the basis of BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This year's caption task of ImageCLEFmedical once again ran with both subtasks, concept detection and caption prediction. It once again used a ROCO-based dataset with additional manual annotations for X-ray directionality. It attracted 13 teams who submitted 116 graded runs using a cloud file drop instead of the AICrowd platform, which was not available to be used this year. For the concept detection task, the F1-score and a secondary F1-score, considering only the manually curated concepts, were used. After adding a number of additional metrics to the caption prediction task last year, the primary metric was changed from BLEU to BERTScore for this year, hoping to reward semantic similarity instead of just n-gram overlap. The caption prediction subtask was more popular than the concept detection subtask this year, with all 9 teams participating in both subtasks, and four teams participating only in the caption prediction subtask. As before, the teams generally approached the tasks completely separately, not really making use of generated concepts for the predicted captions. Like last year, teams generally used multi-label classification systems for the concept detection subtask, last year's winning team simply scaling up their approach to use three instead of two ensembles to once again reach top scores. Retrieval-based systems were still used by some teams, but were consistently outperformed by multi-label classification systems. For the caption prediction subtask, encoderdecoder frameworks were used by most teams, with LLMs being used to generate or refine the captions by some teams. BLIP-2 was used for the first time and achieved good results. Reinforcement learning helped last year's top scoring team in terms of BERTScore further increase last year's score and take the top spot.</p><p>The scores for both tasks have improved compared to last year. For the concept detection subtask, this is partly due to the decreased number of concepts. For caption prediction, BERTScore and ROUGE scores have improved, illustrating the beneficial shift to BERTScore as the primary metric, which emphasizes semantic alignment and information preservation over n-gram frequency.</p><p>For next year's ImageCLEFmedical Caption challenge, some possible improvements include an improved caption prediction evaluation metric which is specific to medical texts, and improving manually validated concept quality with the help of a medical professional. It will also be important to make sure that no models are used that were pre-trained on PubMedCentral data, since these models will already have seen the original captions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,304.92,295.09,9.96;5,384.38,303.55,4.41,6.97;5,391.77,304.92,114.21,9.96;5,89.29,316.88,302.32,9.96;5,316.62,97.78,186.80,144.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a radiology image with the corresponding UMLS ® CUIs and caption extracted from the 2023's ImageCLEFmedical caption task. CC-BY [Ali et al. (2020)]</figDesc><graphic coords="5,316.62,97.78,186.80,144.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,90.49,417.24,189.58"><head>Table 2</head><label>2</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2023 Concept Detection subtask. Only the best run based on the achieved F1-score is listed for each team, together with the corresponding secondary F1-score based on manual annotations as well as the team rankings based on the primary and secondary F1-score. The full results are shown in table 5 in appendix A.</figDesc><table coords="8,134.79,157.03,325.69,123.04"><row><cell>Group Name</cell><cell>Best Run</cell><cell cols="3">F1 Secondary F1 Rank (secondary)</cell></row><row><cell>AUEB-NLP-Group</cell><cell cols="2">4 0.5223</cell><cell>0.9258</cell><cell>1 (2)</cell></row><row><cell>KDE-Lab_Med</cell><cell cols="2">10 0.5074</cell><cell>0.9321</cell><cell>2 (1)</cell></row><row><cell>VCMI</cell><cell cols="2">8 0.4998</cell><cell>0.9162</cell><cell>3 (3)</cell></row><row><cell>IUST_NLPLAB</cell><cell cols="2">7 0.4959</cell><cell>0.8804</cell><cell>4 (6)</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell cols="2">1 0.4957</cell><cell>0.9106</cell><cell>5 (4)</cell></row><row><cell>CS_Morgan</cell><cell cols="2">2 0.4834</cell><cell>0.8902</cell><cell>6 (5)</cell></row><row><cell>SSNSheerinKavitha</cell><cell cols="2">1 0.4649</cell><cell>0.8603</cell><cell>7 (7)</cell></row><row><cell>closeAI2023</cell><cell cols="2">2 0.0900</cell><cell>0.2152</cell><cell>8 (8)</cell></row><row><cell>SSN_MLRG</cell><cell cols="2">3 0.0173</cell><cell>0.1122</cell><cell>9 (9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,413.28,417.24,249.36"><head>Table 3</head><label>3</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2023 Caption Prediction subtask. Only the best run based on the achieved BERTScore is listed for each team, together with the corresponding secondary ROUGE score as well as the team rankings based on the primary BERTScore and secondary ROUGE score. Additional scores are shown in Table 4. The full results are shown in tables 6 and 7 in appendix A.</figDesc><table coords="9,137.48,491.78,320.32,170.86"><row><cell>Group Name</cell><cell cols="4">Best Run BERTScore ROUGE Rank (secondary)</cell></row><row><cell>CSIRO</cell><cell>2</cell><cell>0.6413</cell><cell>0.2463</cell><cell>1 (3)</cell></row><row><cell>closeAI2023</cell><cell>7</cell><cell>0.6281</cell><cell>0.2401</cell><cell>2 (4)</cell></row><row><cell>AUEB-NLP-Group</cell><cell>2</cell><cell>0.6170</cell><cell>0.2130</cell><cell>3 (8)</cell></row><row><cell>PCLmed</cell><cell>5</cell><cell>0.6152</cell><cell>0.2528</cell><cell>4 (2)</cell></row><row><cell>VCMI</cell><cell>5</cell><cell>0.6147</cell><cell>0.2175</cell><cell>5 (7)</cell></row><row><cell>KDE-Lab_Med</cell><cell>3</cell><cell>0.6145</cell><cell>0.2223</cell><cell>6 (5)</cell></row><row><cell>SSN_MLRG</cell><cell>1</cell><cell>0.6019</cell><cell>0.2112</cell><cell>7 (9)</cell></row><row><cell>DLNU_CCSE</cell><cell>1</cell><cell>0.6005</cell><cell>0.2029</cell><cell>8 (10)</cell></row><row><cell>CS_Morgan</cell><cell>10</cell><cell>0.5819</cell><cell>0.1564</cell><cell>9 (11)</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>2</cell><cell>0.5816</cell><cell>0.2181</cell><cell>10 (6)</cell></row><row><cell>Bluefield-2023</cell><cell>3</cell><cell>0.5780</cell><cell>0.1534</cell><cell>11 (12)</cell></row><row><cell>IUST_NLPLAB</cell><cell>6</cell><cell>0.5669</cell><cell>0.2898</cell><cell>12 (1)</cell></row><row><cell>SSNSheerinKavitha</cell><cell>4</cell><cell>0.5441</cell><cell>0.0866</cell><cell>13 (13)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,417.24,237.40"><head>Table 4</head><label>4</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2023 Caption Prediction subtask for additional metrics BLEURT, BLEU, METEOR, CIDEr and CLIPScore. These correspond to the best BERTScore-based runs of each team, listed in Table3. The full results are shown in tables 6 and 7 in appendix A.</figDesc><table coords="10,114.15,157.03,366.98,170.86"><row><cell>Group Name</cell><cell cols="2">Best Run BLEURT</cell><cell cols="3">BLEU METEOR CIDEr CLIPScore</cell></row><row><cell>CSIRO</cell><cell>4</cell><cell cols="2">0.3137 0.1615</cell><cell>0.0798 0.2025</cell><cell>0.8147</cell></row><row><cell>closeAI2023</cell><cell>7</cell><cell cols="2">0.3209 0.1846</cell><cell>0.0873 0.2377</cell><cell>0.8075</cell></row><row><cell>AUEB-NLP-Group</cell><cell>2</cell><cell cols="2">0.2950 0.1692</cell><cell>0.0720 0.1466</cell><cell>0.8039</cell></row><row><cell>PCLmed</cell><cell>5</cell><cell cols="2">0.3166 0.2172</cell><cell>0.0921 0.2315</cell><cell>0.8021</cell></row><row><cell>VCMI</cell><cell>5</cell><cell cols="2">0.3084 0.1653</cell><cell>0.0734 0.1720</cell><cell>0.8082</cell></row><row><cell>KDE-Lab_Med</cell><cell>3</cell><cell cols="2">0.3014 0.1565</cell><cell>0.0724 0.1819</cell><cell>0.8062</cell></row><row><cell>SSN_MLRG</cell><cell>1</cell><cell cols="2">0.2774 0.1418</cell><cell>0.0615 0.1284</cell><cell>0.7759</cell></row><row><cell>DLNU_CCSE</cell><cell>1</cell><cell cols="2">0.2630 0.1059</cell><cell>0.0557 0.1332</cell><cell>0.7725</cell></row><row><cell>CS_Morgan</cell><cell>10</cell><cell cols="2">0.2242 0.0566</cell><cell>0.0436 0.0840</cell><cell>0.7593</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>2</cell><cell cols="2">0.2690 0.1450</cell><cell>0.0702 0.1737</cell><cell>0.7893</cell></row><row><cell>Bluefield-2023</cell><cell>3</cell><cell cols="2">0.2716 0.1543</cell><cell>0.0601 0.1009</cell><cell>0.7837</cell></row><row><cell>IUST_NLPLAB</cell><cell>6</cell><cell cols="2">0.2230 0.2685</cell><cell>0.1004 0.1773</cell><cell>0.8068</cell></row><row><cell>SSNSheerinKavitha</cell><cell>4</cell><cell cols="2">0.2152 0.0749</cell><cell>0.0258 0.0143</cell><cell>0.6873</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="18,88.99,90.49,454.58,608.01"><head>Table 6</head><label>6</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2023 Caption Prediction.</figDesc><table coords="18,95.27,121.17,448.30,577.33"><row><cell>Group Name</cell><cell cols="4">Run BERTScore ROUGE BLEURT</cell><cell cols="3">BLEU METEOR CIDEr CLIPScore</cell></row><row><cell>CSIRO</cell><cell>2</cell><cell>0.6413</cell><cell>0.2463</cell><cell cols="2">0.3151 0.1589</cell><cell>0.0795 0.2071</cell><cell>0.8143</cell></row><row><cell>closeAI2023</cell><cell>7</cell><cell>0.6281</cell><cell>0.2401</cell><cell cols="2">0.3209 0.1846</cell><cell>0.0873 0.2377</cell><cell>0.8075</cell></row><row><cell>closeAI2023</cell><cell>8</cell><cell>0.6243</cell><cell>0.2517</cell><cell cols="2">0.3164 0.1743</cell><cell>0.0882 0.2586</cell><cell>0.8071</cell></row><row><cell>CSIRO</cell><cell>1</cell><cell>0.6225</cell><cell>0.2430</cell><cell cols="2">0.3053 0.2055</cell><cell>0.0898 0.2130</cell><cell>0.8154</cell></row><row><cell>CSIRO</cell><cell>3</cell><cell>0.6189</cell><cell>0.2347</cell><cell cols="2">0.3063 0.1922</cell><cell>0.0844 0.1975</cell><cell>0.8130</cell></row><row><cell>AUEB-NLP-Group</cell><cell>2</cell><cell>0.6170</cell><cell>0.2130</cell><cell cols="2">0.2950 0.1692</cell><cell>0.0720 0.1466</cell><cell>0.8039</cell></row><row><cell>PCLmed</cell><cell>5</cell><cell>0.6152</cell><cell>0.2528</cell><cell cols="2">0.3166 0.2172</cell><cell>0.0921 0.2315</cell><cell>0.8021</cell></row><row><cell>PCLmed</cell><cell>4</cell><cell>0.6148</cell><cell>0.2533</cell><cell cols="2">0.3160 0.2176</cell><cell>0.0922 0.2323</cell><cell>0.8020</cell></row><row><cell>AUEB-NLP-Group</cell><cell>3</cell><cell>0.6147</cell><cell>0.2144</cell><cell cols="2">0.2878 0.1523</cell><cell>0.0696 0.1583</cell><cell>0.8059</cell></row><row><cell>VCMI</cell><cell>5</cell><cell>0.6147</cell><cell>0.2175</cell><cell cols="2">0.3084 0.1653</cell><cell>0.0734 0.1720</cell><cell>0.8082</cell></row><row><cell>KDE-Lab_Med</cell><cell>3</cell><cell>0.6145</cell><cell>0.2223</cell><cell cols="2">0.3014 0.1565</cell><cell>0.0724 0.1819</cell><cell>0.8062</cell></row><row><cell>KDE-Lab_Med</cell><cell>9</cell><cell>0.6143</cell><cell>0.2319</cell><cell cols="2">0.3064 0.1750</cell><cell>0.0773 0.1990</cell><cell>0.8083</cell></row><row><cell>PCLmed</cell><cell>3</cell><cell>0.6142</cell><cell>0.2521</cell><cell cols="2">0.3154 0.2228</cell><cell>0.0930 0.2280</cell><cell>0.8027</cell></row><row><cell>PCLmed</cell><cell>2</cell><cell>0.6142</cell><cell>0.2521</cell><cell cols="2">0.3154 0.2228</cell><cell>0.0930 0.2280</cell><cell>0.8027</cell></row><row><cell>VCMI</cell><cell>3</cell><cell>0.6138</cell><cell>0.2181</cell><cell cols="2">0.3058 0.1618</cell><cell>0.0723 0.1709</cell><cell>0.8089</cell></row><row><cell>KDE-Lab_Med</cell><cell>10</cell><cell>0.6108</cell><cell>0.2152</cell><cell cols="2">0.2935 0.1577</cell><cell>0.0694 0.1586</cell><cell>0.8042</cell></row><row><cell>VCMI</cell><cell>6</cell><cell>0.6103</cell><cell>0.1948</cell><cell cols="2">0.2893 0.1233</cell><cell>0.0602 0.1368</cell><cell>0.7996</cell></row><row><cell>AUEB-NLP-Group</cell><cell>4</cell><cell>0.6099</cell><cell>0.2189</cell><cell cols="2">0.2991 0.1920</cell><cell>0.0742 0.1447</cell><cell>0.7978</cell></row><row><cell>KDE-Lab_Med</cell><cell>7</cell><cell>0.6097</cell><cell>0.2204</cell><cell cols="2">0.3004 0.1695</cell><cell>0.0725 0.1609</cell><cell>0.8081</cell></row><row><cell>VCMI</cell><cell>4</cell><cell>0.6096</cell><cell>0.1938</cell><cell cols="2">0.2888 0.1252</cell><cell>0.0592 0.1244</cell><cell>0.7920</cell></row><row><cell>KDE-Lab_Med</cell><cell>4</cell><cell>0.6094</cell><cell>0.2005</cell><cell cols="2">0.2767 0.1249</cell><cell>0.0596 0.1321</cell><cell>0.7829</cell></row><row><cell>KDE-Lab_Med</cell><cell>1</cell><cell>0.6089</cell><cell>0.2160</cell><cell cols="2">0.2979 0.1640</cell><cell>0.0699 0.1519</cell><cell>0.8043</cell></row><row><cell>KDE-Lab_Med</cell><cell>2</cell><cell>0.6082</cell><cell>0.2144</cell><cell cols="2">0.2912 0.1585</cell><cell>0.0687 0.1569</cell><cell>0.8028</cell></row><row><cell>closeAI2023</cell><cell>6</cell><cell>0.6080</cell><cell>0.2439</cell><cell cols="2">0.3281 0.2267</cell><cell>0.0938 0.2374</cell><cell>0.8069</cell></row><row><cell>PCLmed</cell><cell>1</cell><cell>0.6079</cell><cell>0.2422</cell><cell cols="2">0.3108 0.2247</cell><cell>0.0894 0.1839</cell><cell>0.8050</cell></row><row><cell>AUEB-NLP-Group</cell><cell>1</cell><cell>0.6065</cell><cell>0.2273</cell><cell cols="2">0.3049 0.2061</cell><cell>0.0790 0.1662</cell><cell>0.8026</cell></row><row><cell>closeAI2023</cell><cell>5</cell><cell>0.6063</cell><cell>0.2449</cell><cell cols="2">0.3306 0.2217</cell><cell>0.0948 0.2438</cell><cell>0.8070</cell></row><row><cell>AUEB-NLP-Group</cell><cell>8</cell><cell>0.6059</cell><cell>0.1885</cell><cell cols="2">0.2730 0.1222</cell><cell>0.0606 0.1276</cell><cell>0.8010</cell></row><row><cell>KDE-Lab_Med</cell><cell>8</cell><cell>0.6044</cell><cell>0.2167</cell><cell cols="2">0.3011 0.1744</cell><cell>0.0730 0.1605</cell><cell>0.8066</cell></row><row><cell>closeAI2023</cell><cell>1</cell><cell>0.6039</cell><cell>0.2333</cell><cell cols="2">0.2984 0.1580</cell><cell>0.0751 0.1897</cell><cell>0.7943</cell></row><row><cell>closeAI2023</cell><cell>2</cell><cell>0.6039</cell><cell>0.2333</cell><cell cols="2">0.2984 0.1580</cell><cell>0.0751 0.1897</cell><cell>0.7943</cell></row><row><cell>SSN_MLRG</cell><cell>1</cell><cell>0.6019</cell><cell>0.2112</cell><cell cols="2">0.2774 0.1418</cell><cell>0.0615 0.1284</cell><cell>0.7759</cell></row><row><cell>DLNU_CCSE</cell><cell>1</cell><cell>0.6005</cell><cell>0.2029</cell><cell cols="2">0.2630 0.1059</cell><cell>0.0557 0.1332</cell><cell>0.7725</cell></row><row><cell>AUEB-NLP-Group</cell><cell>9</cell><cell>0.5960</cell><cell>0.2155</cell><cell cols="2">0.3050 0.2040</cell><cell>0.0807 0.1360</cell><cell>0.8043</cell></row><row><cell>closeAI2023</cell><cell>3</cell><cell>0.5939</cell><cell>0.2301</cell><cell cols="2">0.3284 0.1947</cell><cell>0.0882 0.2215</cell><cell>0.8050</cell></row><row><cell>closeAI2023</cell><cell>4</cell><cell>0.5927</cell><cell>0.2364</cell><cell cols="2">0.3305 0.1942</cell><cell>0.0899 0.2232</cell><cell>0.8075</cell></row><row><cell>AUEB-NLP-Group</cell><cell>6</cell><cell>0.5880</cell><cell>0.1708</cell><cell cols="2">0.2590 0.1341</cell><cell>0.0539 0.0816</cell><cell>0.7569</cell></row><row><cell>DLNU_CCSE</cell><cell>2</cell><cell>0.5874</cell><cell>0.1886</cell><cell cols="2">0.2704 0.1152</cell><cell>0.0559 0.1115</cell><cell>0.7942</cell></row><row><cell>CS_Morgan</cell><cell>10</cell><cell>0.5819</cell><cell>0.1564</cell><cell cols="2">0.2242 0.0566</cell><cell>0.0436 0.0840</cell><cell>0.7593</cell></row><row><cell>Clef-CSE-GAN-Team</cell><cell>2</cell><cell>0.5816</cell><cell>0.2181</cell><cell cols="2">0.2690 0.1450</cell><cell>0.0702 0.1737</cell><cell>0.7893</cell></row><row><cell>CS_Morgan</cell><cell>4</cell><cell>0.5791</cell><cell>0.1541</cell><cell cols="2">0.2649 0.1331</cell><cell>0.0568 0.1731</cell><cell>0.7772</cell></row><row><cell>KDE-Lab_Med</cell><cell>5</cell><cell>0.5789</cell><cell>0.1838</cell><cell cols="2">0.2905 0.1484</cell><cell>0.0698 0.0838</cell><cell>0.7826</cell></row><row><cell>Bluefield-2023</cell><cell>3</cell><cell>0.5780</cell><cell>0.1534</cell><cell cols="2">0.2716 0.1543</cell><cell>0.0601 0.1009</cell><cell>0.7837</cell></row><row><cell>Bluefield-2023</cell><cell>2</cell><cell>0.5777</cell><cell>0.1539</cell><cell cols="2">0.2714 0.1540</cell><cell>0.0597 0.1048</cell><cell>0.7832</cell></row><row><cell>VCMI</cell><cell>8</cell><cell>0.5750</cell><cell>0.1464</cell><cell cols="2">0.2682 0.1447</cell><cell>0.0555 0.0732</cell><cell>0.7852</cell></row><row><cell>VCMI</cell><cell>1</cell><cell>0.5734</cell><cell>0.1427</cell><cell cols="2">0.2648 0.1382</cell><cell>0.0533 0.0676</cell><cell>0.7819</cell></row><row><cell>IUST_NLPLAB</cell><cell>6</cell><cell>0.5669</cell><cell>0.2898</cell><cell cols="2">0.2230 0.2685</cell><cell>0.1004 0.1773</cell><cell>0.8068</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="19,88.99,233.61,443.39,297.18"><head>Table 7</head><label>7</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2023 Caption Prediction (continued).</figDesc><table coords="19,95.27,264.29,437.11,266.50"><row><cell>Group Name</cell><cell cols="6">Run BERTScore ROUGE BLEURT BLEU METEOR CIDEr CLIPScore</cell></row><row><cell>VCMI</cell><cell>2</cell><cell>0.5647</cell><cell>0.1284</cell><cell>0.2554 0.1243</cell><cell>0.0457 0.0491</cell><cell>0.7664</cell></row><row><cell>IUST_NLPLAB</cell><cell>2</cell><cell>0.5647</cell><cell>0.2708</cell><cell>0.2088 0.2412</cell><cell>0.0895 0.1594</cell><cell>0.8049</cell></row><row><cell>AUEB-NLP-Group</cell><cell>7</cell><cell>0.5630</cell><cell>0.1682</cell><cell>0.2793 0.1514</cell><cell>0.0656 0.0486</cell><cell>0.7602</cell></row><row><cell>IUST_NLPLAB</cell><cell>4</cell><cell>0.5612</cell><cell>0.2797</cell><cell>0.2103 0.2592</cell><cell>0.0954 0.1617</cell><cell>0.8061</cell></row><row><cell>IUST_NLPLAB</cell><cell>10</cell><cell>0.5560</cell><cell>0.2750</cell><cell>0.2121 0.2643</cell><cell>0.0959 0.1422</cell><cell>0.8007</cell></row><row><cell>CS_Morgan</cell><cell>12</cell><cell>0.5558</cell><cell>0.1272</cell><cell>0.2569 0.1199</cell><cell>0.0344 0.0164</cell><cell>0.7338</cell></row><row><cell>IUST_NLPLAB</cell><cell>8</cell><cell>0.5534</cell><cell>0.2687</cell><cell>0.2034 0.2639</cell><cell>0.0946 0.1341</cell><cell>0.8030</cell></row><row><cell>CS_Morgan</cell><cell>5</cell><cell>0.5508</cell><cell>0.1070</cell><cell>0.2373 0.1040</cell><cell>0.0351 0.0481</cell><cell>0.7165</cell></row><row><cell>IUST_NLPLAB</cell><cell>5</cell><cell>0.5494</cell><cell>0.2898</cell><cell>0.2008 0.2685</cell><cell>0.0996 0.1739</cell><cell>0.8042</cell></row><row><cell>CS_Morgan</cell><cell>13</cell><cell>0.5482</cell><cell>0.1144</cell><cell>0.2435 0.1180</cell><cell>0.0323 0.0142</cell><cell>0.6909</cell></row><row><cell>IUST_NLPLAB</cell><cell>1</cell><cell>0.5463</cell><cell>0.2708</cell><cell>0.1894 0.2412</cell><cell>0.0887 0.1559</cell><cell>0.8028</cell></row><row><cell>IUST_NLPLAB</cell><cell>3</cell><cell>0.5445</cell><cell>0.2797</cell><cell>0.1862 0.2592</cell><cell>0.0945 0.1584</cell><cell>0.8026</cell></row><row><cell>SSNSheerinKavitha</cell><cell>4</cell><cell>0.5441</cell><cell>0.0866</cell><cell>0.2152 0.0749</cell><cell>0.0258 0.0143</cell><cell>0.6873</cell></row><row><cell>CS_Morgan</cell><cell>6</cell><cell>0.5438</cell><cell>0.1107</cell><cell>0.1817 0.0026</cell><cell>0.0329 0.0925</cell><cell>0.7582</cell></row><row><cell>SSNSheerinKavitha</cell><cell>3</cell><cell>0.5436</cell><cell>0.0860</cell><cell>0.2151 0.0746</cell><cell>0.0259 0.0143</cell><cell>0.6858</cell></row><row><cell>CS_Morgan</cell><cell>9</cell><cell>0.5419</cell><cell>0.0924</cell><cell>0.1735 0.0406</cell><cell>0.0210 0.0187</cell><cell>0.6821</cell></row><row><cell>AUEB-NLP-Group</cell><cell>5</cell><cell>0.5417</cell><cell>0.1682</cell><cell>0.2780 0.1323</cell><cell>0.0639 0.0388</cell><cell>0.7600</cell></row><row><cell>IUST_NLPLAB</cell><cell>9</cell><cell>0.5394</cell><cell>0.2750</cell><cell>0.1770 0.2643</cell><cell>0.0961 0.1416</cell><cell>0.7969</cell></row><row><cell>IUST_NLPLAB</cell><cell>7</cell><cell>0.5367</cell><cell>0.2687</cell><cell>0.1678 0.2639</cell><cell>0.0947 0.1335</cell><cell>0.7967</cell></row><row><cell>CS_Morgan</cell><cell>8</cell><cell>0.5087</cell><cell>0.0264</cell><cell>0.1205 0.0034</cell><cell>0.0107 0.0125</cell><cell>0.6819</cell></row><row><cell>KDE-Lab_Med</cell><cell>6</cell><cell>0.4425</cell><cell>0.1079</cell><cell>0.2968 0.0709</cell><cell>0.0528 0.0057</cell><cell>0.7305</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.02,154.80,8.97"><p>https://www.imageclef.org/ [last accessed:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,249.98,671.02,42.61,8.97"><p>2023-06-28]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="3,92.57,671.01,288.74,8.97"><p>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ [last accessed: 2023-06-17]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="4,92.57,671.03,413.41,8.97"><p>https://www.nlm.nih.gov/pubs/techbull/nd22/nd22_umls_2022ab_release_available.html [last accessed: 2023-06-17]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="6,92.57,660.07,295.51,8.97"><p>https://huggingface.co/microsoft/deberta-xlarge-mnli [last accessed: 2023-06-17]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="6,92.57,671.03,235.27,8.97"><p>https://github.com/Tiiiger/bert_score [last accessed: 2023-06-17]   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">University of Essex GCRF QR Engagement Fund</rs> provided by <rs type="funder">Research England</rs> (grant number <rs type="grantNumber">G026</rs>). The work of <rs type="person">Louise Bloch</rs> and <rs type="person">Raphael Brüngel</rs> was partially funded by a PhD grant from the <rs type="funder">University of Applied Sciences and Arts Dortmund (FH Dortmund), Germany</rs>. The work of <rs type="person">Ahmad Idrissi-Yaghir</rs> and <rs type="person">Henning Schäfer</rs> was funded by a PhD grant from the <rs type="funder">DFG</rs> <rs type="programName">Research Training Group 2535</rs> Knowledgeand data-based personalisation of medicine at the point of care (WisPerMed).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_z2ersQs">
					<idno type="grant-number">G026</idno>
				</org>
				<org type="funding" xml:id="_5qFgh2Y">
					<orgName type="program" subtype="full">Research Training Group 2535</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Full results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,237.65,393.59,10.91;13,112.66,251.20,394.53,10.91;13,112.66,264.75,80.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,380.21,237.65,126.04,10.91;13,112.66,251.20,77.03,10.91">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,212.47,251.20,284.72,10.91">Working Notes of CLEF 2016 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="219" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,278.30,393.58,10.91;13,110.82,291.85,395.17,10.91;13,112.66,305.40,394.53,10.91;13,112.66,318.95,377.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,344.91,278.30,161.33,10.91;13,110.82,291.85,331.44,10.91">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/invited_paper_7.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,466.22,291.85,39.77,10.91;13,112.66,305.40,313.33,10.91">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017., 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,332.50,393.59,10.91;13,112.66,346.05,393.33,10.91;13,112.66,359.59,395.01,10.91;13,112.66,373.14,153.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,379.38,332.50,126.87,10.91;13,112.66,346.05,130.78,10.91">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/invited_paper_4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,266.33,346.05,239.66,10.91;13,112.66,359.59,92.04,10.91">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018., 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,386.69,393.33,10.91;13,112.66,400.24,394.52,10.91;13,112.14,413.79,395.05,10.91;13,112.66,427.34,395.16,10.91;13,111.60,440.89,282.09,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,361.85,386.69,144.14,10.91;13,112.66,400.24,129.60,10.91">Overview of the ImageCLEFmed 2019 Concept Detection Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_245.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,112.14,413.79,350.09,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,344.82,427.34,156.80,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,454.44,395.16,10.91;13,112.66,467.99,393.33,10.91;13,112.14,481.54,395.05,10.91;13,112.66,495.09,58.60,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,400.32,454.44,107.51,10.91;13,112.66,467.99,320.03,10.91">Overview of the Image-CLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,460.63,467.99,45.35,10.91;13,112.14,481.54,64.41,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="13,253.54,481.54,165.70,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1166</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,508.64,393.32,10.91;13,112.66,522.18,377.26,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,186.64,508.64,319.34,10.91;13,112.66,522.18,52.45,10.91">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,173.80,522.18,103.68,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,535.73,394.53,10.91;13,112.66,549.28,394.53,10.91;13,112.66,562.83,394.53,10.91;13,112.66,576.38,135.21,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,167.43,549.28,335.54,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.66,562.83,323.53,10.91">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1101" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,589.93,395.17,10.91;13,111.81,603.48,395.37,10.91;13,112.66,617.03,393.33,10.91;13,112.66,630.58,216.46,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,326.22,603.48,180.96,10.91;13,112.66,617.03,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,316.25,617.03,189.74,10.91;13,112.66,630.58,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,644.13,393.33,10.91;13,112.33,657.68,393.65,10.91;14,112.66,86.97,395.17,10.91;14,112.41,100.52,394.78,10.91;14,112.66,114.06,394.53,10.91;14,112.66,127.61,361.01,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,370.24,644.13,135.74,10.91;13,112.33,657.68,393.65,10.91;14,112.66,86.97,395.17,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset, in: Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.41,100.52,389.37,10.91;14,179.72,114.06,180.34,10.91">7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="14,112.66,141.16,394.53,10.91;14,112.66,154.71,395.17,10.91;14,112.66,168.26,394.53,10.91;14,112.66,181.81,395.17,10.91;14,112.39,195.36,394.80,10.91;14,112.48,208.91,394.70,10.91;14,112.66,222.46,395.17,10.91;14,112.66,236.01,393.32,10.91;14,112.66,249.56,394.52,10.91;14,112.33,263.11,120.27,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,224.93,208.91,282.25,10.91;14,112.66,222.46,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,363.94,222.46,143.89,10.91;14,112.66,236.01,393.32,10.91;14,112.66,249.56,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="14,280.09,249.56,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,393.32,10.91;14,112.66,290.20,393.33,10.91;14,112.14,303.75,296.27,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,478.24,276.66,27.75,10.91;14,112.66,290.20,190.90,10.91">AUEB NLP group at ImageCLEFmedical caption</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kaliosis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moschovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalambakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,354.01,290.20,151.98,10.91;14,112.14,303.75,167.82,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,317.30,393.33,10.91;14,112.66,330.85,393.33,10.91;14,112.14,344.40,296.27,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,419.49,317.30,86.50,10.91;14,112.66,330.85,213.52,10.91">Multi-stage medical image captioning using classification and CLIP</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Togawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,354.25,330.85,115.99,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,478.71,330.85,27.27,10.91;14,112.14,344.40,146.03,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,393.33,10.91;14,112.66,371.50,395.17,10.91;14,111.60,385.05,158.48,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,271.50,357.95,234.48,10.91;14,112.66,371.50,73.02,10.91">Concept detection and image caption generation in medical imaging</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Yeshwanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.99,371.50,115.34,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,335.39,371.50,172.44,10.91;14,111.60,385.05,10.14,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,398.60,393.32,10.91;14,112.66,412.15,394.52,10.91;14,112.66,425.70,325.73,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,396.28,398.60,109.70,10.91;14,112.66,412.15,255.25,10.91">Transferring pre-trained large language-image model for medical image captioning</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,390.89,412.15,111.64,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,112.66,425.70,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,439.25,395.16,10.91;14,112.66,452.79,393.33,10.91;14,112.66,466.34,395.17,10.91;14,111.60,479.89,158.48,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,278.61,439.25,229.21,10.91;14,112.66,452.79,393.33,10.91;14,112.66,466.34,91.00,10.91">Concept detection and caption prediction in Image-CLEFmedical caption 2023 with convolutional neural networks, vision and text-to-text transfer transformers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Layode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,226.30,466.34,281.52,10.91;14,111.60,479.89,30.41,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,493.44,394.62,10.91;14,112.66,506.99,394.53,10.91;14,112.66,520.54,58.60,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,283.06,493.44,204.29,10.91">A concise model for medical image captioning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,506.99,325.62,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,534.09,395.17,10.91;14,112.66,547.64,395.16,10.91;14,112.66,561.19,212.07,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,392.22,534.09,115.61,10.91;14,112.66,547.64,118.12,10.91">IUST_NLPLAB at Image-CLEFmedical caption tasks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lotfollahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nobakhtian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajihosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eetemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,274.21,547.64,110.66,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,392.19,547.64,115.63,10.91;14,112.66,561.19,61.84,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,574.74,395.17,10.91;14,112.66,588.29,394.53,10.91;14,112.66,601.84,188.37,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,426.27,574.74,81.57,10.91;14,112.66,588.29,94.99,10.91">KDE lab at Image-CLEFmedical caption</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Togawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,251.81,588.29,255.37,10.91;14,112.66,601.84,59.92,10.91">CLEF2023 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,615.39,393.53,10.91;14,112.66,628.93,394.53,10.91;14,112.66,642.48,188.37,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,271.03,615.39,235.17,10.91;14,112.66,628.93,114.49,10.91">Customizing general-purpose foundation models for medical report generation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,250.43,628.93,113.37,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="14,371.26,628.93,135.93,10.91;14,112.66,642.48,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,656.03,393.33,10.91;14,112.66,669.58,393.33,10.91;15,112.66,86.97,356.56,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,257.78,656.03,248.21,10.91;14,112.66,669.58,284.37,10.91">SSN MLRG at caption 2023: Automatic concept detection and caption prediction using ConceptNet and vision transformer</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,420.15,669.58,85.84,10.91;15,112.66,86.97,23.42,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="15,143.49,86.97,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,100.52,393.32,10.91;15,112.66,114.06,395.17,10.91;15,112.66,127.61,394.53,10.91;15,112.66,141.16,188.37,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="15,419.94,100.52,86.05,10.91;15,112.66,114.06,395.17,10.91;15,112.66,127.61,94.99,10.91">Detecting concepts and generating captions from medical images: Contributions of the VCMI team to Image-CLEFmedical caption</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rio-Torto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Patrício</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,251.81,127.61,112.75,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="15,372.01,127.61,135.18,10.91;15,112.66,141.16,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,154.71,393.33,10.91;15,112.66,168.26,395.01,10.91;15,112.66,181.81,143.58,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,176.64,154.71,264.55,10.91">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,450.78,154.71,55.20,10.91;15,112.66,168.26,309.87,10.91">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,393.33,10.91;15,112.66,208.91,396.29,10.91;15,112.07,224.90,227.91,7.90" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,112.66,195.36,393.33,10.91;15,112.66,208.91,81.93,10.91">Multi-domain clinical natural language processing with medcat: The medical concept annotation toolkit</title>
		<idno type="DOI">10.1016/j.artmed.2021.102083</idno>
		<ptr target="https://doi.org/10.1016/j.artmed.2021.102083" />
	</analytic>
	<monogr>
		<title level="j" coord="15,207.62,208.91,156.39,10.91">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">102083</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.01,394.52,10.91;15,112.66,249.56,394.53,10.91;15,112.66,263.11,397.48,10.91;15,112.66,279.10,43.94,7.90" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="15,273.64,249.56,228.93,10.91">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wei H. Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.35.doi:10.1038/sdata.2016.35" />
	</analytic>
	<monogr>
		<title level="j" coord="15,112.66,263.11,64.45,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,393.33,10.91;15,112.66,303.75,394.61,10.91;15,112.66,317.30,395.01,10.91;15,112.66,330.85,119.84,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,391.06,290.20,114.93,10.91;15,112.66,303.75,134.85,10.91">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.480677</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,409.45,303.75,97.82,10.91;15,112.66,317.30,337.98,10.91">Medical Imaging 2003: PACS and Integrated Medical Information Systems: Design and Evaluation</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Ratib</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,344.40,393.33,10.91;15,112.66,357.95,394.51,10.91;15,112.66,373.94,92.42,7.90" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="15,251.62,344.40,254.37,10.91">IRMA Bilder in 193 Kategorien für ImageCLEFmed</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ott</surname></persName>
		</author>
		<idno type="DOI">10.18154/RWTH-2016-06143</idno>
		<ptr target="https://publications.rwth-aachen.de/record/667225.doi:10.18154/RWTH-2016-06143" />
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">363</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,385.05,393.33,10.91;15,112.66,398.60,394.53,10.91;15,112.66,412.15,394.03,10.91;15,112.66,425.70,109.64,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,384.35,385.05,121.63,10.91;15,112.66,398.60,98.70,10.91">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
	</analytic>
	<monogr>
		<title level="m" coord="15,238.03,398.60,269.16,10.91;15,112.66,412.15,43.84,10.91">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,395.17,10.91;15,112.66,452.79,394.61,10.91;15,112.66,466.34,157.69,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,157.59,439.25,266.49,10.91">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m" coord="15,448.01,439.25,59.81,10.91;15,112.66,452.79,291.76,10.91">Text Summarization Branches Out, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,479.89,393.33,10.91;15,112.66,493.44,393.32,10.91;15,112.33,506.99,394.94,10.91;15,112.31,520.54,289.63,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,230.46,479.89,275.52,10.91;15,112.66,493.44,109.29,10.91">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3348</idno>
		<ptr target="http://aclweb.org/anthology/W14-3348.doi:10.3115/v1/W14-3348" />
	</analytic>
	<monogr>
		<title level="m" coord="15,245.52,493.44,260.46,10.91;15,112.33,506.99,50.10,10.91">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,534.09,393.33,10.91;15,112.66,547.64,393.33,10.91;15,112.33,561.19,395.33,10.91;15,112.66,574.74,167.31,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,302.99,534.09,203.00,10.91;15,112.66,547.64,45.51,10.91">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299087</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7299087/.doi:10.1109/CVPR.2015.7299087" />
	</analytic>
	<monogr>
		<title level="m" coord="15,188.48,547.64,317.51,10.91;15,112.33,561.19,30.92,10.91">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,588.29,393.32,10.91;15,112.66,601.84,393.53,10.91;15,112.66,615.39,203.57,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="15,308.17,588.29,197.81,10.91;15,112.66,601.84,88.86,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,224.89,601.84,281.30,10.91;15,112.66,615.39,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,628.93,394.62,10.91;15,112.66,642.48,394.53,10.91;15,112.28,656.03,394.42,10.91;15,112.66,669.58,354.17,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="15,245.83,628.93,240.15,10.91">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.704.doi:10.18653/v1/2020.acl-main.704" />
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,642.48,394.53,10.91;15,112.28,656.03,192.52,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.66,100.52,393.33,10.91;16,112.66,114.06,395.16,10.91;16,112.66,127.61,394.62,10.91;16,112.31,141.16,388.50,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="16,375.61,86.97,130.38,10.91;16,112.66,100.52,179.57,10.91">CLIPScore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Le</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.595</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.595.doi:10.18653/v1/2021.emnlp-main.595" />
	</analytic>
	<monogr>
		<title level="m" coord="16,322.99,100.52,183.00,10.91;16,112.66,114.06,395.16,10.91;16,112.66,127.61,32.81,10.91">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7514" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,154.71,394.53,10.91;16,112.66,168.26,393.33,10.91;16,112.66,181.81,393.33,10.91;16,112.66,195.36,394.53,10.91;16,112.39,208.91,394.89,10.91;16,112.66,222.46,226.03,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="16,318.79,168.26,187.19,10.91;16,112.66,181.81,131.82,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="16,396.58,181.81,109.40,10.91;16,112.66,195.36,235.16,10.91;16,176.68,209.92,178.09,9.72">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML<address><addrLine>Virtual Event; PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07">2021. July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="16,112.66,236.01,395.17,10.91;16,112.66,249.56,394.53,10.91;16,112.66,263.11,294.58,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="16,440.73,236.01,67.10,10.91;16,112.66,249.56,226.09,10.91">AUEB NLP group at ImageCLEFmed caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zachariadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Trakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,386.92,249.56,120.26,10.91;16,112.66,263.11,175.50,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Androutsopoulos</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
