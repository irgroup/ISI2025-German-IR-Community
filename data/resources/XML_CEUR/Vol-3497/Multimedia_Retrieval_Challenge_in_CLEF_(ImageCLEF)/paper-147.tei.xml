<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,412.07,15.42;1,89.29,106.66,131.75,15.42;1,89.29,129.00,310.14,11.96">Concept Detection and Image Caption Generation in Medical Imaging Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,154.90,90.08,11.96"><forename type="first">Varsha</forename><surname>Yeshwanth</surname></persName>
							<email>varshayeshwanth@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar (SSN) College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.36,154.90,97.94,11.96"><forename type="first">Lekshmi</forename><surname>Kalinathan</surname></persName>
							<email>lekshmik@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar (SSN) College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,412.07,15.42;1,89.29,106.66,131.75,15.42;1,89.29,129.00,310.14,11.96">Concept Detection and Image Caption Generation in Medical Imaging Notebook for the ImageCLEFmedical Caption Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">B9D5B870D8F0BC2FD73818B7F562A65D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-label classification</term>
					<term>Image caption prediction</term>
					<term>concept detection</term>
					<term>LSTM</term>
					<term>DenseNet121</term>
					<term>CNN</term>
					<term>ResNet-101</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical images help in the detection, diagnosis and prognosis of diseases. Extracting accurate insights from them is an indispensable factor in providing reliable healthcare. Currently, this task is performed manually, making it time consuming and labour intensive. The use of deep learning can automate this process of extracting concepts from medical images and turning them into reports. In this paper we propose two methods for the same as a part of the ImageCLEFmedical task. For concept detection the DenseNet121 architecture was used, and it achieved an F1 score of 0.49. We present a CNN encoder with an LSTM based decoder for caption prediction, with the model achieving a BERTScore 0.58.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Medical image analysis and interpretation is an important task perturbing healthcare professionals. Extracting insights from radiology scans and developing accurate interpretations of them is a crucial phase in the diagnosis process. This process is heavily dependent on human technicians who still physically carry out the analysis, which is both time consuming and labour intensive. This is a hindrance and calls for the development of automated techniques that can identify the major concepts in the scans and also translate them into condensed textual descriptions without compromising on the efficacy of the results. After all, the diagnosis can only be as good as the data.</p><p>This paper describes clef-CSE-Gan-Team's approach to the ImageCLEFmedical Caption task, which can broadly be classified into two, concept detection and caption prediction. The concept detection subtask is aimed at identifying the UMLS (Unified Medical Language System) <ref type="bibr" coords="1,491.27,532.99,14.72,10.91" target="#b0">[1]</ref> concepts embodied inside the image. UMLS concepts are unique identifiers assigned to various medical and health-related terms. This standardised vocabulary serves as a tool for effective communication and information exchange between healthcare professionals across various disciplines. Hence, it is important to accurately identify all the concepts present in an image, especially because this information can also be used in subsequent automation tasks like caption prediction or computerised prognosis.</p><p>The next subtask deals with the generation of condensed textual captions for the original images. Human-readable captions are essential to help medical professionals understand the insights contained in an image and to provide them with all the information they need to create a successful treatment plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Methodology</head><p>The following sections describe the methods used for the subtasks and the intuition behind their usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>The ImageCLEFmedical 2023 <ref type="bibr" coords="2,221.40,276.61,12.88,10.91" target="#b1">[2]</ref> Caption dataset <ref type="bibr" coords="2,309.93,276.61,12.88,10.91" target="#b2">[3]</ref> consisted of 60,918 training samples and had 2125 unique labels. Owing to the diversity and magnitude of data, we decided to deploy a deep neural network, specifically DenseNet, for the task. Figure <ref type="figure" coords="2,386.41,303.71,5.13,10.91" target="#fig_0">1</ref> shows an overview of its architecture. DenseNets <ref type="bibr" coords="2,193.24,317.26,15.07,10.91" target="#b3">[4]</ref> or Densely Connected CNNs solve the vanishing gradient problem of traditional CNNs by connecting every layer in the network with each other, increasing feature reusability and flow of information. The model has L(L+1)/2 direct connections for L layers. DenseNets also have fewer parameters since the feature maps from previous layers are concatenated with those from subsequent layers instead of being summed. This makes them ideal candidates for tasks like multi-label classification, where the dimensionality of the label space is high. Furthermore, feature concatenation enables the model to access features at both local and global resolutions. This is advantageous for multi-label classification since multiple labels can be identified at the same time. The structure of DenseNet model is split into dense blocks. The feature map's dimensionality remains constant inside each block, while the number of filters varies. Transition layers are used between the blocks to reduce the number of channels. The last layer is a bottleneck layer to improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>The dataset contained 60,918 training samples, each with an associated medical caption. The model proposed generates the caption word by word. This is thanks to the attention mechanism <ref type="bibr" coords="2,89.29,543.13,11.29,10.91" target="#b4">[5]</ref>, which allows it to focus only on those parts of the image that are relevant to the next word it is going to produce. The model itself is an encoder-decoder architecture, as shown in Figure <ref type="figure" coords="2,89.29,570.23,3.81,10.91" target="#fig_1">2</ref>, combined with a beam search algorithm in the final phase. An encoder is used to convert all input images into a fixed-coded sequence. For the encoder, a ResNet-101 <ref type="bibr" coords="2,434.56,583.78,13.00,10.91" target="#b5">[6]</ref> [7] has been used because of its deep architecture, skip connections that help to learn the residual mappings, and the ability of the model to generalise well, thanks to the pretraining done on the ImageNet dataset. The encoder is needed to understand the semantics, extract the features, and also help in capturing the spatial and contextual information. A decoder is then used to construct a natural language representation by transforming the encoded information. Its job is to generate captions word by word by looking at the encoded images and paying attention to the important aspects.</p><p>Beam search used in the decoding process involves considering the top k candidates in the first step and generating k second words for each. Combinations of the first and second words are then chosen based on additive scores. This process is repeated for subsequent words, selecting the top k combinations. At each step, the best overall score is used to determine the terminating sequence among the k sequences generated. This helps in choosing the sequence of words with the highest overall score from a group of candidate sequences instead of letting the decoder rigidly choose the best word alone at each stage.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head><p>For both the subtasks, the PyTorch machine learning framework was used for the implementation. The code used, can be accessed at https://github.com/clefVP/medical-image-captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>The DenseNet121 model was implemented for the concept detection subtask. The PyTorch implementation of the model was used, with certain changes to adapt the default definition for the current task. The model consisted of four dense blocks and had a growth rate of 32. Every layer increased the number of features added by the growth rate. The Softmax function in the final classification layer was replaced with the Sigmoid function to refine the model for multi-label classification. The Sigmoid function is not constrained to producing probabilities that sum up to one, thus making it more suitable for multi-output tasks. For training, all the images were resized to 256 X 256 pixels and normalised. Transfer learning was leveraged by using the pretrained imagenet1k_v1 <ref type="bibr" coords="4,250.59,282.95,12.86,10.91" target="#b7">[8]</ref> weights. These weights were obtained by training the DenseNet121 model on the ImageNet dataset, which contains 14,197,122 manually annotated images. The following parameters were utilised -a batch size of 32, a learning rate of 1 × 10 -4 and the Adam optimizer <ref type="bibr" coords="4,195.91,323.60,13.52,10.91" target="#b8">[9]</ref>, and the Binary Cross Entropy (BCE) loss function. The BCE loss function was chosen since it treats each label independently and outputs a probability for each label. A threshold value of 0.5 for the predictions was fixed after considering the F1 scores of different values in a fixed search space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>An encoder-decoder architecture has been used for the caption prediction task. The input images were encoded into a fixed form using a prevalent CNN architecture like ResNet-101 as the encoder. The last two layers of the architecture were removed owing to the fact they would be unnecessary in an encoding pipeline. The model creates smaller representations of the input images, with each ensuing representation having more number of learned channels than the previous. The encoder produces an encoded sequence for each image having 2,048 learned channels. A batch size of 16 was used along with a encoder learning rate of 1e-4.</p><p>The encoded images are subsequently fed into an LSTM <ref type="bibr" coords="4,348.61,508.82,17.79,10.91" target="#b9">[10]</ref> based decoder which computes a weighted average across all pixels of the encoded image (important pixels being given more weight-Attention) and this weighted representation can be fed to the decoder as the initial hidden state. At each subsequent decoding step, for each pixel in the Attention network, weights are created using the encoded image and the preceding hidden state. For the training, overly long captions were truncated to fit a caption length that was greater than or equal to the lengths of 99 percent of the data. All the remaining captions were padded to uniformly have this caption length. The LSTM Decoder receives the previous word produced and the weighted average of the encoding to produce the subsequent word. The decoder used a learning rate of 4 × 10 -4 . In the implementation of the network, the Adam optimiser was used as the optimiser and cross-entropy as the primary loss function. Gradient clipping was also carried out to a magnitude of 5. Finally, the decoder's output is passed through a linear layer that transforms the output into a score for each word in the vocabulary. Using beam search, the sequence of words with the highest overall score is chosen from a group of candidate sequences. Figure <ref type="figure" coords="5,415.32,301.28,4.97,10.91" target="#fig_4">5</ref> and the subsequent description depicts the generated caption for an image from the validation set and shows how it compares with the actual caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The DenseNet121 model implemented for the concept detection subtask achieved an F1 score of 0.4957 and a manual F1 score of 0.9105. The F1-score <ref type="bibr" coords="5,320.39,400.56,18.18,10.91" target="#b10">[11]</ref> is an evaluation metric that combines the precision and recall metrics. In classification tasks where the dataset is imbalanced, accuracy alone is an ineffective metric since it is biased towards the majority class. Using a metric like the F1 score that considers both precision and recall can thus help mitigate the impact of class imbalance. Thus for the task of concept detection, the F1 score was used as the primary metric. An F1-score Manual, calculated using a subset of manually validated concepts (anatomy, directionality, and image modality) was also provided by the organizers <ref type="bibr" coords="5,415.45,481.85,11.56,10.91" target="#b2">[3]</ref>. A few examples of the model's predictions for the task are shown in Figure <ref type="figure" coords="5,353.83,495.40,5.09,10.91" target="#fig_2">3</ref> and Figure <ref type="figure" coords="5,412.06,495.40,3.75,10.91" target="#fig_3">4</ref>. Table <ref type="table" coords="5,449.67,495.40,5.09,10.91" target="#tab_0">1</ref> depicts the results obtained: We employed DenseNet121 for concept detection as it proved effective in handling the task of identifying multiple labels associated with each image. The dense architecture of DenseNet121 facilitated comprehensive feature extraction, allowing us to accurately detect various concepts within the images. However, when it came to caption prediction, our requirements differed. We didn't require intricate feature identification, but rather the association of images with given For the caption prediction task, the model accomplished a BERTScore of 0.58 and a ROUGE score of 0.22. A summarised report of the results obtained on the test set is shown below in Table <ref type="table" coords="6,116.50,355.30,3.81,10.91" target="#tab_1">2</ref>. BERTScore is used as the primary metric instead of BLEU, and ROUGE is used as a secondary metric.</p><p>BERTScore <ref type="bibr" coords="6,153.98,382.40,18.07,10.91" target="#b11">[12]</ref> is an evaluation metric based on the BERT (Bidirectional Encoder Representations from Transformers) language model. It utilizes word embeddings to measure the similarity between the reference text and generated text. Cosine similarity is used to make the comparison and outputs a score between 0 and 1. ROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation) <ref type="bibr" coords="6,173.88,436.60,18.17,10.91" target="#b12">[13]</ref> is an evaluation metric that measures the overlap of unigrams between the reference and generated text. It outputs a score between 0 and 1 indicating the extent of overlap.</p><p>The performance of the model could be attributed to the encoder-decoder architecture as it is able to understand the context better and focus its attention on the parts of the image that matter the most. Furthermore, truncating overly long captions in the training set has helped remove the outliers (taken to be captions that had lengths representing the top 1 percent) and train the model faster.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The image captioning task in ImageCLEFmedical 2023 had two subtasks -concept detection and caption prediction. The dataset had 60,918 training samples, 10437 validation samples and 2,125 unique labels. To efficiently handle the diverse data, a deep neural network was chosen for the concept detection subtask. The DenseNet121 model was implemented for the concept detection task and it yielded an F1 score of 0.49. Further improvements can be made by accounting for the imbalance in the data, since the maximum representation of a label was 20,000 samples while the minimum representation was just 3. Additionally the classes could be clustered according to type as well.</p><p>To deal with the caption prediction task, an encoder-decoder architecture was chosen. This was important to take advantage of transfer learning in the encoding phase and attention mechanisms in the decoding stage. A beam search was performed to optimally predict the captions, keeping in mind the correctness of the whole sequence. It yielded a BERTScore of 0.58 and a ROUGE score of 0.22. Additional improvements can be made by including the text embedded inside the images for caption generation, and furthermore, the definitions of the concepts detected in the concept detection sub-task may also be employed to frame more relevant captions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,431.76,168.65,8.93;3,129.72,218.07,333.35,201.13"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The DenseNet121 architecture.</figDesc><graphic coords="3,129.72,218.07,333.35,201.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,643.41,274.32,8.93;3,129.72,470.57,333.36,160.28"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The encoder-decoder architecture for caption generation</figDesc><graphic coords="3,129.72,470.57,333.36,160.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,236.58,152.58,8.93;5,129.72,84.19,333.37,139.83"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of a prediction</figDesc><graphic coords="5,129.72,84.19,333.37,139.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,236.40,152.58,8.93;6,129.72,84.19,333.37,139.65"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of a prediction</figDesc><graphic coords="6,129.72,84.19,333.37,139.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,300.84,180.59,8.93;7,193.47,84.19,208.35,156.26"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of caption prediction.</figDesc><graphic coords="7,193.47,84.19,208.35,156.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,535.51,298.82,57.92"><head>Table 1</head><label>1</label><figDesc>Concept Detection on the test set.</figDesc><table coords="5,207.47,567.12,180.35,26.30"><row><cell>Model</cell><cell>F1 score Manual F1 score</cell></row><row><cell cols="2">DenseNet121 0.495730 0.910585</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,558.00,411.41,67.00"><head>Table 2</head><label>2</label><figDesc>Caption Prediction on the test set Perioperative transoesophageal echocardiography mid-oesophageal aortic valve long-axis view showing a stent protruding from the right coronary artery almost 1.cm into the Sinus of Valsalva (arrow).</figDesc><table coords="6,94.88,589.24,405.52,35.76"><row><cell>Model</cell><cell cols="2">BERTScore ROUGE BLEURT</cell><cell>BLEU</cell><cell>METEOR</cell><cell>CIDEr</cell><cell>CLIPScore</cell></row><row><cell>ResNet-101 Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ LSTM Decoder</cell><cell>0.581625</cell><cell cols="4">0.218103 0.269043 0.145035 0.070155 0.173664</cell><cell>0.789327</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,89.29,281.69,349.80,8.93"><head>Predicted Caption: Coronary angiography showing the right coronary artery.</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>The authors would like to thank the <rs type="institution">Department of Computer Science and Engineering, Sri Sivasubramaniya Nadar College of Engineering, Chennai, India</rs> (https://www.ssn.edu.in/) for providing the computational resources for model training and testing.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,393.33,10.91;8,112.66,124.83,258.51,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,188.99,111.28,316.99,10.91;8,112.66,124.83,51.86,10.91">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,173.16,124.83,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,138.38,394.53,10.91;8,112.66,151.93,395.17,10.91;8,112.66,165.48,394.53,10.91;8,112.66,179.03,395.17,10.91;8,112.39,192.57,394.80,10.91;8,112.48,206.12,394.70,10.91;8,112.66,219.67,395.17,10.91;8,112.66,233.22,393.32,10.91;8,112.66,246.77,394.52,10.91;8,112.33,260.32,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,224.93,206.12,282.25,10.91;8,112.66,219.67,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,363.94,219.67,143.89,10.91;8,112.66,233.22,393.32,10.91;8,112.66,246.77,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,280.09,246.77,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,273.87,394.52,10.91;8,112.66,287.42,395.17,10.91;8,112.66,300.97,393.33,10.91;8,112.66,314.52,247.61,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,298.72,287.42,209.11,10.91;8,112.66,300.97,171.81,10.91">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.89,300.97,114.04,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="8,429.41,300.97,76.58,10.91;8,112.66,314.52,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,328.07,393.32,10.91;8,112.66,341.62,393.33,10.91;8,112.33,355.17,175.50,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,358.51,328.07,147.47,10.91;8,112.66,341.62,38.41,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,159.39,341.62,346.60,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,368.71,394.53,10.91;8,112.66,382.26,393.33,10.91;8,112.66,395.81,395.17,10.91;8,112.30,409.36,227.73,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,481.24,368.71,25.95,10.91;8,112.66,382.26,305.83,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.14,382.26,64.85,10.91;8,112.66,395.81,395.17,10.91;8,164.20,409.36,35.59,10.91">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct coords="8,112.66,422.91,393.33,10.91;8,112.66,436.46,394.51,10.91;8,112.66,452.45,73.62,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,250.70,422.91,194.04,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.94,422.91,53.05,10.91;8,112.66,436.46,296.26,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,463.56,394.53,10.91;8,112.66,477.11,393.33,10.91;8,112.33,490.66,246.71,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,197.71,463.56,305.04,10.91">Comparison of VGG and ResNet used as encoders for image captioning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Atliha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Šešok</surname></persName>
		</author>
		<idno type="DOI">10.1109/eStream50540.2020.9108880</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,477.11,379.82,10.91">IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,504.21,393.53,10.91;8,112.66,517.76,393.33,10.91;8,112.33,531.30,206.66,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,469.13,504.21,37.07,10.91;8,112.66,517.76,270.28,10.91">Transfer learning for medical image classification: a literature review</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cosa-Linan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jannesari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Maros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ganslandt</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12880-022-00793-7</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,391.81,517.76,101.02,10.91">BMC medical imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,544.85,393.33,10.91;8,112.33,558.40,29.19,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,238.21,544.85,167.55,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,571.95,393.33,10.91;8,112.66,585.50,378.95,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,250.08,571.95,255.91,10.91;8,112.66,585.50,96.12,10.91">A review of recurrent neural networks: LSTM cells and network architectures</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_01199</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,217.02,585.50,89.43,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,599.05,393.59,10.91;8,112.66,612.60,98.12,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grandini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bagli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Visani</surname></persName>
		</author>
		<idno>ArXiv abs/2008.05756</idno>
		<title level="m" coord="8,255.98,599.05,214.85,10.91">Metrics for multi-class classification: an overview</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,626.15,393.33,10.91;8,112.66,639.70,394.53,10.91;8,112.66,653.25,394.61,10.91;8,112.31,666.80,188.32,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,384.35,626.15,121.63,10.91;8,112.66,639.70,98.70,10.91">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
	</analytic>
	<monogr>
		<title level="m" coord="8,238.03,639.70,269.16,10.91;8,112.66,653.25,44.01,10.91">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,133.03,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,154.48,86.97,243.31,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,419.57,86.97,86.42,10.91;9,112.66,100.52,55.58,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
