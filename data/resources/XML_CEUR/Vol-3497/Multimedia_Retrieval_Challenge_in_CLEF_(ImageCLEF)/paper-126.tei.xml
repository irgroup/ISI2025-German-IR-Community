<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,417.30,15.42">AUEB NLP Group at ImageCLEFmedical Caption 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,116.63,91.67,5.42"><forename type="first">Panagiotis</forename><surname>Kaliosis</surname></persName>
							<email>pkaliosis@aueb.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>GR-104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.61,116.63,99.04,5.42"><forename type="first">Georgios</forename><surname>Moschovis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>GR-104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.29,116.63,110.66,5.42"><forename type="first">Foivos</forename><surname>Charalampakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>GR-104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.10,130.58,85.34,5.42"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>GR-104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.43,130.58,103.58,5.42"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>GR-104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,417.30,15.42">AUEB NLP Group at ImageCLEFmedical Caption 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A155788BCA33B63CB0A5EA28C77CAE2F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Computer Vision</term>
					<term>Biomedical Images</term>
					<term>Convolutional Neural Networks</term>
					<term>Multi-Label Classification</term>
					<term>Caption Generation</term>
					<term>Generative Models</term>
					<term>Transformers</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the methods that the AUEB NLP Group experimented with during its participation in the 7th edition of the ImageCLEFmedical Caption sub-tasks, namely Concept Detection and Caption Prediction. The former intends to automatically classify biomedical images into a set of one or more tags based solely on the visual input, while the latter aims to generate a syntactically and semantically accurate diagnostic caption that addresses the medical conditions depicted on a given image. For the Concept Detection sub-task, extending our previous work, we utilized a wide range of Convolutional Neural Network encoders followed by a Feed-Forward Neural Network, both in a single-task and a multi-task fashion, as well as combined with a contrastive learning approach. Our methods concerning the Caption Prediction sub-task are influenced by both our previous work and recent progress in Natural Language Processing (NLP) methods. Our two base systems use CNN-RNN and Transformer-to-Transformer encoder-decoder architectures, respectively. Additionally, we experimented with a Transformer-based denoising component, which was trained to reformulate the generated captions in a more syntactically coherent and medically accurate way. Our group ranked 1 st in Concept Detection and 3 rd in Caption Prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,143.26,451.59,12.83,4.94" target="#b0">[1]</ref> is a multi-modal machine learning campaign running every year since 2003 as part of the Cross Language Evaluation Forum (CLEF) 1 . It fosters research breakthroughs, as well as the development of advanced multimedia processing systems in the areas of computer vision, image analysis, classification and retrieval in multi-modal, cross-language contexts <ref type="bibr" coords="1,492.43,492.23,11.43,4.94" target="#b0">[1]</ref>. ImageCLEFmedical is one of the four main tasks of this year's ImageCLEF campaign. It consists of a series of challenges that range from image captioning to synthetic image generation and question-answering. We participated in the ImageCLEFmedical Caption task, which took place for the 7 th time <ref type="bibr" coords="1,161.55,546.43,11.58,4.94" target="#b1">[2]</ref>. Following the previous years' campaigns, the task consisted of two subtasks, namely Concept Detection and Caption Prediction. The goal in Concept Detection is to link a biomedical image with one or more medical concepts (categories), whereas in Caption Prediction the goal is to automatically generate a draft diagnostic report that accurately outlines the medical situation, as well as the topology of the body structures and organs shown in the image.</p><p>Diagnostic Captioning still constitutes a challenging research problem that aims to assist the diagnostic process for a patient by providing a draft report, rather than replacing the doctors and any human factor involved in the procedure <ref type="bibr" coords="2,304.87,171.52,11.32,4.94" target="#b2">[3]</ref>. It may thus be viewed as an assistive tool, capable of providing an initial draft diagnostic report regarding the patient's condition. Such a draft would ideally allow the doctors' attention to focus on important regions of the image <ref type="bibr" coords="2,493.20,198.62,12.78,4.94" target="#b3">[4]</ref> and aid them to produce more accurate medical diagnoses with improved accuracy and speed <ref type="bibr" coords="2,492.63,212.17,11.28,4.94" target="#b4">[5]</ref>. Experienced clinicians could improve their throughput, by analyzing faster and more efficiently the large volume of medical examinations that they daily handle. Less experienced clinicians could ideally consider the automatically generated captions in order to reduce the probability of clinical errors <ref type="bibr" coords="2,167.18,266.37,11.58,4.94" target="#b5">[6]</ref>. Concept Detection may assist Diagnostic Captioning by detecting key concepts that need to be mentioned in the draft report. It can also be used to index medical images by relevant concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">AUEB NLP Group contributions</head><p>In this work we present the experiments conducted, as well as the systems submitted as part of AUEB NLP Group's participation in this year's Concept Detection and Caption Prediction tasks. We experimented with several extensions of our previous work <ref type="bibr" coords="2,366.38,370.29,11.23,4.94" target="#b6">[7,</ref><ref type="bibr" coords="2,380.13,370.29,7.42,4.94" target="#b7">8,</ref><ref type="bibr" coords="2,390.07,370.29,7.42,4.94" target="#b8">9,</ref><ref type="bibr" coords="2,400.02,370.29,12.50,4.94" target="#b9">10,</ref><ref type="bibr" coords="2,415.04,370.29,13.95,4.94" target="#b10">11]</ref> in the Diagnostic Captioning task, in addition to a number of new approaches influenced by the expeditious progress of Transformer-based <ref type="bibr" coords="2,225.76,397.39,17.76,4.94" target="#b11">[12]</ref> Deep Learning methods in Sequence-to-Sequence (Seq2Seq) architectures <ref type="bibr" coords="2,149.88,410.94,17.91,4.94" target="#b12">[13]</ref> and Large Language Models (LLMs) <ref type="bibr" coords="2,332.62,410.94,16.25,4.94" target="#b13">[14]</ref>.</p><p>Our submissions to the Concept Detection sub-task revolve around two main directions. In the first one, we employed a Convolutional Neural Network (CNN) encoder in order to obtain the images' visual representations, followed by a Feed-Forward Neural Network (FFNN) that classifies the images into one or more medical concepts. In the second direction, we employed contrastive learning <ref type="bibr" coords="2,178.78,478.69,16.30,4.94" target="#b14">[15,</ref><ref type="bibr" coords="2,197.46,478.69,12.23,4.94" target="#b15">16]</ref>, aiming at bringing the high-dimensional representations of images and their assigned concepts closer in the vector space. Finally, we experimented with various ensembles of our proposed systems, either by performing majority voting based on each system's predictions or by calculating the intersection and the union of their predicted concepts.</p><p>For the Caption Prediction sub-task, our work can be divided into three major directions. The first one, following our last year's submissions <ref type="bibr" coords="2,295.85,546.43,16.12,4.94" target="#b10">[11]</ref>, is a Show and Tell model <ref type="bibr" coords="2,430.76,546.43,16.11,4.94" target="#b16">[17]</ref>, which more specifically adopts an architecture that includes a CNN and a Recurrent Neural Network (RNN). The CNN-RNN architecture still remains competitive, while it also lays the foundations for further experiments, such as investigating new variants and modified forms <ref type="bibr" coords="2,425.84,587.08,16.15,4.94" target="#b17">[18]</ref>. Furthermore, we implemented an encoder-decoder model, where we employed Transformers for both the encoder and decoder components. More specifically, we employed a Vision Transformer (ViT) <ref type="bibr" coords="2,89.29,627.73,18.07,4.94" target="#b18">[19]</ref> instance as the image encoder and a GPT-2 <ref type="bibr" coords="2,317.25,627.73,18.07,4.94" target="#b19">[20]</ref> decoder in charge of generating the predicted captions. As our third major direction, we implemented a novel pipeline, where we used a denoising sequence-to-sequence model on top of the two aforementioned architectures. We trained our denoising model to rewrite or rephrase the initial draft radiology reports by providing it with the ground truth captions. Thus, it was able to learn and subsequently correct the common mistakes of our two base models, resulting in a more fluent and consistent generated caption.</p><p>Extending our history of successful entries <ref type="bibr" coords="3,290.66,130.88,11.26,4.94" target="#b6">[7,</ref><ref type="bibr" coords="3,304.64,130.88,7.44,4.94" target="#b7">8,</ref><ref type="bibr" coords="3,314.80,130.88,12.51,4.94" target="#b9">10,</ref><ref type="bibr" coords="3,330.02,130.88,13.97,4.94" target="#b10">11]</ref> in the ImageCLEFmedical campaign, our submissions ranked 1 st among 10 participating groups in the Concept Detection sub-task and 3 rd among 13 participating groups in the Caption Prediction sub-task. In Section 2 below, we provide insight into this year's dataset, followed by a discussion of our methods in Section 3. In Section 4, we present our experimental results for each sub-task. Finally, in Section 5 we summarize our findings and suggest directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>In this year's edition of the ImageCLEFmedical Caption task, a dataset consisting of 71,355 biomedical images along with their respective medical concepts, in the form of UMLS <ref type="bibr" coords="3,487.91,270.80,18.07,4.94" target="#b20">[21]</ref> terms,<ref type="foot" coords="3,116.39,281.71,3.71,3.61" target="#foot_0">2</ref> and diagnostic captions was provided. The set was originally split by the organizers into training and validation subsets. Following the previous years' campaigns, the dataset constitutes an updated and extended version of the Radiology Objects in Context (ROCO) dataset <ref type="bibr" coords="3,486.67,311.44,16.41,4.94" target="#b21">[22]</ref>, which originates from a range of biomedical articles available in the PubMed Central Open Access (PMC OA) subset <ref type="foot" coords="3,198.67,335.91,3.71,3.61" target="#foot_1">3</ref> .</p><p>The dataset, common for both sub-tasks, comprised images of different modalities (i.e., X-Ray, Computed Tomography), although no further insight was provided regarding the different types of images included. Concept Detection is a multi-label classification problem covering a broad range of 2,125 distinct biomedical concepts, originating from the Unified Medical Language System (UMLS) <ref type="bibr" coords="3,160.95,406.29,16.26,4.94" target="#b20">[21]</ref>, whereas caption prediction aims at open-ended generation of diagnostic texts for the medical images. After merging the provided training and validation data, we split them into three subsets, holding out a development (private test) subset for evaluation purposes. We followed a 75%-10%-15% split, keeping relatively equal data distributions in all three subsets. We confirmed it by comparing the concepts distribution between the subsets. Thus, we considered 53,516 images as our training data, 7,135 images as our validation set, while the remaining 10,704 images constituted our held-out development set. Moreover, an official test set, consisting of 10,473 images was shared. All of our submissions were evaluated based on their performance on the official test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>Regarding the Concept Detection sub-task, a set of one or more medical concepts were originally assigned to each radiology image. The concepts are offered in the form of Concept Unique Identifiers (CUIs) in accordance with the Unified Medical Language System (UMLS) <ref type="bibr" coords="3,453.98,591.51,16.08,4.94" target="#b20">[21]</ref>. For example, the biomedical concept "Pericardial Effusion" is associated with the CUI term "C0031039". Each concept is retrieved from the image's corresponding diagnostic caption in order to be employed as the training target. Some examples of images and their corresponding ground truth concepts can be found in Figure <ref type="figure" coords="4,259.23,103.78,3.74,4.94" target="#fig_0">1</ref>. The dataset contains 2,125 distinct biomedical concepts. It is highly imbalanced in terms of concepts, as there are some that appear more than 20,000 times, while others are assigned to only 4 or 5 images. Figure <ref type="figure" coords="4,209.71,423.76,5.14,4.94" target="#fig_1">2</ref> below illustrates the dataset's long-tail distribution (left plot) by plotting the number of each concept's appearances in descending order against its index (class index). Furthermore, after performing a thorough exploratory analysis of this year's dataset, we observed that some concepts were more common, while also representing a greater category of medical examinations, such as X-Ray or Ultrasonography. Besides, we observed that the vast majority of the images is associated with one of these concepts, in addition to the rest, more specific concepts. Based on this observation, we decided to explore the potential of a multi-task classification model based on a shared backbone encoder, which will be described in Section 3.</p><p>Table <ref type="table" coords="4,128.62,544.11,5.17,4.94">1</ref> shows the ten most common concepts that occur in the dataset. We were able to partition the data into four main modalities; Computed Tomography Scan (CT), X-Ray, Magnetic Resonance Imaging (MRI) and Ultrasonography, based on the corresponding concept's appearances. Later on, these modalities became the central focus of our work, mainly in the Concept Detection sub-task. The maximum and minimum number of concepts assigned to a single image are 32 and 1, occurring in 1 and 7,109 images respectively. The average number of assigned tags per image is 3.74. The aforementioned observations are outlined in the histogram in Figure <ref type="figure" coords="4,131.96,638.95,8.42,4.94" target="#fig_1">2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The ten most common concepts (CUIs) and corresponding UMLS term found in the ImageCLEFmedi-cal2023 dataset, along with the number of images they are associated with.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most common concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>In the Caption Prediction sub-task, the images are accompanied by a diagnostic caption that expresses the medical conditions present in the image. There are 71,355 captions across the whole dataset, one for each provided image. Similarly to last year's campaign, the vast majority of the captions, specifically 99.46% (70,974 out of 71,355 captions) are unique. This is an important differentiation from previous versions of this task, where this percentage was significantly lower <ref type="bibr" coords="5,117.73,668.16,16.38,4.94" target="#b10">[11]</ref>. Consequently, typical retrieval methods based on nearest neighbours search <ref type="bibr" coords="5,487.95,668.16,18.03,4.94" target="#b22">[23]</ref> are not so efficient this year, including extended variations with weighting mechanisms relying on the cosine similarities of the retrieved images <ref type="bibr" coords="6,312.80,103.78,16.41,4.94" target="#b23">[24]</ref>. Therefore, more elaborate captioning methods are needed. We found out that the maximum number of words in a single caption is 315 (occured once), while the minimum is 1 (encountered 134 times). The average caption length is 16.04 words. These statistics refer to the dataset as a whole, but we have carefully checked that they remain consistent in all three subsets. The five most common captions, as well as the ten most popular words, after excluding the stopwords, can be found in Tables <ref type="table" coords="6,362.95,185.07,5.09,4.94">2</ref> and<ref type="table" coords="6,389.99,185.07,5.09,4.94">3</ref> respectively. In Figure <ref type="figure" coords="6,499.68,185.07,3.75,4.94" target="#fig_2">3</ref>, we provide a histogram, as well as a box-plot, both showing that most of the captions do not exceed 100 tokens. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The five most common captions found in the ImageCLEFmedical2023 dataset alongside the number of images they are associated with. According to the organizers <ref type="bibr" coords="7,224.71,90.23,11.28,4.94" target="#b1">[2]</ref>, each caption is pre-processed before evaluated in the following manner:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most common captions</head><p>â€¢ The caption is converted to lower-case.</p><p>â€¢ Numbers are replaced by words, e.g., number 10 becomes "ten".</p><p>â€¢ Punctuation is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The ten most common words and their frequencies in the ImageCLEFmedical2023 dataset, after removing stop-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most common words (excluding stop-words) Word</head><p>showing Unlike last year's campaign <ref type="bibr" coords="7,228.01,286.59,16.39,4.94" target="#b10">[11]</ref>, we decided to perform experiments while both adopting and ignoring the pre-processing procedure during training, taking into consideration that this year stop-words were not removed during pre-processing by the organizers. Removal of the stop-words could potentially lead to distortion of important words in either the predicted or the ground truth captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we present the methods we used in our submissions for both the Concept Detection and the Caption Prediction sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>Our submissions in this year's Concept Detection sub-task are based on three groundwork systems. First, following our previous work <ref type="bibr" coords="7,286.39,488.05,11.38,4.94" target="#b6">[7,</ref><ref type="bibr" coords="7,300.49,488.05,7.48,4.94" target="#b7">8,</ref><ref type="bibr" coords="7,310.69,488.05,12.55,4.94" target="#b9">10,</ref><ref type="bibr" coords="7,325.97,488.05,12.34,4.94" target="#b10">11]</ref>, we thoroughly experimented with a CNN+FFNN system, as well as a multitask classifier with a more complex, yet similar architecture. Moreover, we implemented a contrastive learning retrieval-based classifier, using it as a standalone system as well as combining it with the best performing CNN+FFNN system. Additionally, we made several submissions using ensembles (majority, union and intersection-based) of the three aforementioned systems, as they achieved higher performance on the primary evaluation metric of the task in our held-out development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">CNN+FFNN</head><p>Our first system utilizes a CNN encoder backbone, followed by an FFNN classification head that employs one or more hidden layers. The image features that represent the visual input are extracted from the last convolutional layer of the CNN. Then, a global pooling layer is used in order to acquire the final feature vector. We experimented with three global pooling strategies; max, average and Generalized-Mean (GeM) global pooling <ref type="bibr" coords="7,348.28,672.84,16.15,4.94" target="#b24">[25]</ref>, which all resulted in enhanced performance compared to no pooling scheme. Max pooling retrieves the maximum value of each feature map, while average pooling computes the respective mean value <ref type="bibr" coords="8,433.18,103.78,16.17,4.94" target="#b25">[26]</ref>. In addition, GeM pooling is a generalized version of both the max and average pooling strategies <ref type="bibr" coords="8,469.11,117.33,16.25,4.94" target="#b24">[25]</ref>.</p><p>Specifically, given an input image, the CNN encoder outputs a three-dimensional tensor ğ‘‹ of shape ğ» Ã— ğ‘Š Ã— ğ¾. ğ¾ denotes the number of channels (feature maps), while ğ» and ğ‘Š represent the image's height and width. Let ğ‘‹ ğ‘˜ be a feature map, hence equal to ğ» ğ‘˜ Ã— ğ‘Š ğ‘˜ for ğ‘˜ âˆˆ [1, 2, . . . , ğ¾], and ğ‘š, ğ‘, ğ‘” be the max, average and GeM pooling functions respectively. The pooling layer's output for input ğ‘‹ ğ‘˜ is a single value ğ‘£ ğ‘˜ that can be computed based on Equations 1, 2, and 3, hereunder, depending on the pooling strategy employed:</p><formula xml:id="formula_0" coords="8,241.85,219.45,264.79,21.02">ğ‘£ (ğ‘š) ğ‘˜ = ğ‘š(ğ‘‹ ğ‘˜ ) = max ğ‘¥âˆˆğ‘‹ ğ‘˜ ğ‘¥<label>(1)</label></formula><formula xml:id="formula_1" coords="8,220.47,251.57,286.17,38.19">ğ‘£ (ğ‘) ğ‘˜ = ğ‘(ğ‘‹ ğ‘˜ ) = â› â 1 |ğ‘‹ ğ‘˜ | â€¢ âˆ‘ï¸ ğ‘¥âˆˆğ‘‹ ğ‘˜ ğ‘¥ â â <label>(2)</label></formula><formula xml:id="formula_2" coords="8,210.32,298.99,296.31,42.38">ğ‘£ (ğ‘”) ğ‘˜ = ğ‘”(ğ‘‹ ğ‘˜ ) = â› â 1 |ğ‘‹ ğ‘˜ | â€¢ âˆ‘ï¸ ğ‘¥âˆˆğ‘‹ ğ‘˜ ğ‘¥ ğ‘ ğ‘˜ â â  1 ğ‘ ğ‘˜ (3)</formula><p>GeM pooling is equivalent to max pooling when ğ‘ ğ‘˜ â†’ âˆ, and equivalent to average pooling when ğ‘ ğ‘˜ = 1 <ref type="bibr" coords="8,157.64,366.11,16.41,4.94" target="#b24">[25]</ref>. The hyperparameter ğ‘ ğ‘˜ can either be trained by integrating it in the network's training process, or be manually initialized beforehand.</p><p>The FFNN component, consisting of multiple hidden and dropout <ref type="bibr" coords="8,396.66,393.21,17.99,4.94" target="#b26">[27]</ref> layers, classifies the image into one or more concepts. The network's output layer consists of |ğ¶| neurons, where ğ¶ is the set of the unique concepts in the dataset and is featured with sigmoid activation gates in order to squash the neurons' values between 0 and 1, hence transforming them into probabilities. We therefore end up with one probability per label and if it exceeds a specific threshold value ğ‘¡, then the corresponding concept is assigned to the image. The threshold (same value for all concepts) was selected by performing a grid search in the range (0.1, 0.7) on our validation set aiming to optimize the competition's primary metric, the ğ¹ 1 score. Our model was trained by minimizing the binary cross-entropy loss, treating each concept as a binary classification problem. Moreover, we used the Adam <ref type="bibr" coords="8,266.71,515.15,18.01,4.94" target="#b27">[28]</ref> optimizer, as well as a linear decreasing learning rate strategy and early stopping based on our validation set loss with patience equal to 3. We do not exploit the validation set for our final model since there is no guarantee that the same number of epochs is the best when using all training data and it has been previously observed that "the gain of re-training the model after merging all the splits is almost negligible" <ref type="bibr" coords="8,468.52,569.35,16.11,4.94" target="#b28">[29]</ref>. We experimented with various initial learning rates (e.g., 1ğ‘’ -3, 1ğ‘’ -4) and decreasing factors (e.g., 0.1, 0.05) using random search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">CNN+FFNN-based Multi-task Classifier</head><p>Our second system adopts the CNN+FFNN architecture described in the previous section and utilizes it in a multi-task fashion. We observed that some of the medical concepts were more common and represented generic medical terms (see also Section 2). This observation led us into experimenting with a multi-task classification model composed of a shared encoding backbone and two task-specific classification heads. The first head corresponds to a single-label classification task (Modality prediction), while the second one to a multi-label classification problem (Modality-specific concepts prediction). An overview of the system's architecture can be found in Figure <ref type="figure" coords="9,160.79,144.43,3.74,4.94" target="#fig_3">4</ref>.</p><p>The first head is in charge of classifying the image into one out of five candidate classes; the four main modalities (namely X-Ray, Computed Tomography, Magnetic Resonance Imaging and Ultrasonography) or none of them. Concurrently, the second head performs multi-label classification on the image features excluding the main modality tags, attempting to identify the rest of the concepts present in the image. The intuition behind this method is that, through the aggregated loss, the shared backbone will be driven to learn optimized image representations suitable for both tasks. Moreover, we stay consistent with our first system and use multiple hidden and dropout <ref type="bibr" coords="9,89.29,501.99,17.97,4.94" target="#b26">[27]</ref> layers. The Modality Prediction head consists of five neurons on the output layer, one for each modality (including the "None" option), featured with a Softmax activation function. It was trained by attempting to minimize the categorical cross-entropy loss. On the other end, the Modality-specific classification head's output layer consists of |ğ¶| -4 neurons (where |ğ¶| denotes the overall number of possible concepts) alongside a sigmoid activation gate, and is trained by minimizing the binary cross-entropy loss. Both components' learning rates were initialized at a relatively low value, swiftly increased to a pre-defined maximum and then slowly decreased until the end of the optimization process. This strategy is shown to preserve training stability and minimize the degree of divergence in the network's parameters, especially in the deeper layers <ref type="bibr" coords="9,151.35,623.94,16.25,4.94" target="#b29">[30]</ref>.</p><p>The entire network, composed of the shared backbone encoder and the two task-specific classifiers, is trained based on the aggregated loss that derives from the two FFNN components. Specifically, let â„’ be the network's loss, ğ¶ğ¶ğ¸ loss be the single-label classifier's loss, and finally ğµğ¶ğ¸ loss be the multi-label classification component's loss. The total loss is equal to:</p><formula xml:id="formula_3" coords="10,111.25,124.41,395.39,9.57">â„’ = ğ›¾ â€¢ ğ¶ğ¶ğ¸ loss (ğ‘¦ single , ğ‘¦ ^single ) + (1 -ğ›¾) â€¢ ğµğ¶ğ¸ loss (ğ‘¦ multirest , ğ‘¦ ^multirest ), 0 &lt; ğ›¾ â‰¤ 1 (4)</formula><p>where ğ›¾ is initialized to 0.5 and can either stay fixed or be automatically adapted during training. In the case where ğ›¾ is adaptive, we used the following approach. At the end of each epoch, if the total loss is increased compared to the previous epoch, we proceed to examine the partial task-specific losses. If only the ğ¶ğ¶ğ¸ loss increased, then we increase ğ›¾ by a pre-defined factor (e.g., 10%), aiming to put more emphasis on reducing the ğ¶ğ¶ğ¸ loss throughout the next epoch. The same procedure is followed vice versa regarding the ğµğ¶ğ¸ loss . In case both losses increased, then we slightly adjust ğ›¾ either upwards or downwards, depending on which loss increased more. Moreover, even if the total loss decreased, we still attempt to optimize the losses' weights.</p><p>To do so, we modify ğ›¾'s value, in accordance with which loss decreased more between the two. We either increase or decrease it aiming to place greater emphasis on the component with the less decreased loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Contrastive Learning-based Tagger</head><p>This system is based on the idea proposed by CLIP <ref type="bibr" coords="10,316.41,334.70,16.22,4.94" target="#b15">[16]</ref>, which is a framework based on multimodal learning. CLIP employs a contrastive learning objective and jointly trains an image encoder and a text encoder to predict the correct pairings of (image, text) examples. The (bidirectional) constrastive objective aims at bringing the representations of true pairings closer in the vector space, while pushing the representations of mismatching pairs far away.</p><p>Based on this approach, we formulate a similar training procedure where we utilize the available (image, concepts) pairs instead. We again use an image encoder and a text encoder based on BERT <ref type="bibr" coords="10,158.59,429.55,16.15,4.94" target="#b30">[31]</ref>. The encoders are trained to map the image representations and their gold concepts representations to nearby points in a joint representation space (see Figure <ref type="figure" coords="10,476.48,443.10,3.65,4.94" target="#fig_4">5</ref>). We compute the embeddings of the 2,125 concepts using the text encoder before training and treat them as trainable variables, which we update instead of updating the text encoder's parameters. We use a bidirectional temperature-scaled version of the binary cross-entropy function as the training objective with the images-concepts similarity matrix ğ‘† âˆˆ R ğ‘›Ã—|ğ¶| (computed via the dot product of the respective embeddings in a batch of ğ‘› images) as the logits (see Eq. 5). The goal is to maximize the similarity of the image embeddings with the embeddings of the gold truth concepts assigned to each image.</p><formula xml:id="formula_4" coords="10,164.22,557.41,342.42,26.73">â„’ CLIP (ğ‘¦ multi , ğ‘†) = ğµğ¶ğ¸(ğ‘¦ multi , ğ‘†/ğœ ) + ğµğ¶ğ¸(ğ‘¦ ğ‘‡ multi , ğ‘† ğ‘‡ /ğœ ) 2 (5)</formula><p>where ğœ is the temperature hyper-parameter.</p><p>During inference, given an image, we compute the similarities between its embedding and the |ğ¶| concepts and assign to it the top-ğ‘˜ most similar concepts. We select to learn the ğ‘˜ parameter using the following scheme: we create a dataset ğ’Ÿ â€² = {s ğ‘– , ğ‘ ğ‘– } ğ‘ ğ‘–=1 where s ğ‘– = [ğ‘  ğ‘–1 , . . . , ğ‘  ğ‘–|ğ¶| ] is the vector that contains the similarities of the embedding of the ğ‘–-th image with each embedding of the |ğ¶| concepts and ğ‘ ğ‘– is the number of concepts assigned to this image. Using ğ’Ÿ â€² , we train a Multi-Layer Perceptron (MLP) regressor in order to predict the number of assigned concepts (ğ‘ ğ‘– ) of the ğ‘–-th image based on its similarity vector with the |ğ¶| concepts. Thus, we feed this network image-concepts similarities and it outputs a number ğ‘˜ as the expected assigned concepts for this image.</p><formula xml:id="formula_5" coords="11,366.09,222.16,108.05,60.64">ğ¶ğ¶ 1 ğ¶ğ¶ 2 ğ¶ğ¶ 3 â€¦ ğ¶ğ¶ ğ‘šğ‘š ğ¼ğ¼ 1 ğ¶ğ¶ 1 ğ¼ğ¼ 1 ğ¶ğ¶ 2 ğ¼ğ¼ 1 ğ¶ğ¶ 3 â€¦ ğ¼ğ¼ 1 ğ¶ğ¶ ğ‘šğ‘š ğ¼ğ¼ 2 ğ¶ğ¶ 1 ğ¼ğ¼ 2 ğ¶ğ¶ 2 ğ¼ğ¼ 2 ğ¶ğ¶ 3 â€¦ ğ¼ğ¼ 2 ğ¶ğ¶ ğ‘šğ‘š</formula><p>We also used this system with an ensemble-like design together with a FFNN. In the system described above, we added a trainable FFNN classifier which was fed the image embeddings from the CNN encoder. These same embeddings were also used in the calculation of the similarity matrix. The final output logits were formed by interpolating the classifier's logits ğ¿ ğ‘ âˆˆ R ğ‘›Ã—|ğ¶| and the similarity matrix ğ‘†: ğ¿ ğ‘†ğ‘ = ğœ† â€¢ ğœ(ğ¿ ğ‘ ) + (1 -ğœ†) â€¢ ğœ(ğ‘†), where ğœ† is a trainable parameter and ğœ is the sigmoid function. Additionally, the system was trained using both the â„’ CLIP and the standard binary cross entropy loss ğµğ¶ğ¸ loss :</p><formula xml:id="formula_6" coords="11,151.58,585.85,355.06,24.43">â„’ ensemble (ğ‘¦ multi , ğ¿ ğ‘†ğ‘ ) = â„’ CLIP (ğ‘¦ multi , ğ¿ ğ‘†ğ‘ ) + ğµğ¶ğ¸ loss (ğ‘¦ multi , ğ¿ ğ‘†ğ‘ ) 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>Our submissions in the Caption Prediction sub-task again revolve around three main systems, two of which follow an encoder-decoder approach. The first one utilizes a CNN as the encoder and an RNN as the decoder, while the second one is Transformer-based, employing a ViT <ref type="bibr" coords="12,476.78,90.23,17.76,4.94" target="#b18">[19]</ref> as the encoding unit and OpenAI's GPT-2 <ref type="bibr" coords="12,260.42,103.78,17.76,4.94" target="#b19">[20]</ref> as the decoding unit. Furthermore, we implemented a sequence-to-sequence <ref type="bibr" coords="12,199.16,117.33,18.02,4.94" target="#b12">[13]</ref> denoising component, which when employed on top of the two aforementioned systems forms a novel captioning pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">CNN-RNN</head><p>Our first system is based on the CNN-RNN encoder-decoder <ref type="bibr" coords="12,367.57,180.20,18.07,4.94" target="#b16">[17]</ref> method, which employs a CNN encoder and an RNN decoder that generates the caption.</p><p>In Figure <ref type="figure" coords="12,142.49,207.30,3.67,4.94" target="#fig_5">6</ref>, we present a high-level overview of the system's architecture. The CNN encoder is responsible for extracting image representations, which are then passed to the decoder. The RNN decoder has been implemented with Gated Recurrent Units (GRU cells) <ref type="bibr" coords="12,467.69,234.40,18.07,4.94" target="#b31">[32]</ref> and concatenates the encoded visual features with the hidden states of its encoding cells. At each recurrent step, the previous GRU cell's state, which contains knowledge about the extracted visual features and the part of the caption that has been generated so far, is passed alongside the previously generated word as an input to the current GRU cell. Afterwards, the GRU output is passed to an MLP component that yields a probability distribution over the model's vocabulary words and the one with the highest probability as the sentence's next token. This recurrent process terminates once a special token, denoting the end of the generated sequence, is predicted. The model is trained by attempting to maximize the likelihood of the provided ground truth caption given a visual instance <ref type="bibr" coords="12,289.10,356.34,16.25,4.94" target="#b16">[17]</ref>. Following the pre-processing steps of Show&amp;Tell <ref type="bibr" coords="13,322.47,90.23,16.30,4.94" target="#b16">[17]</ref>, we first added two special tokens in each training caption, a &lt;start of sequence&gt; and an &lt;end of sequence&gt; token. Next, we created the model's vocabulary by keeping all words that appeared at least 4 times throughout the training set, replacing the out-of-vocabulary (OOV) words with the &lt;UNK&gt; special token. We experimented with multiple maximum length values ranging from 40 to 120 tokens, unlike our last year's submission <ref type="bibr" coords="13,188.74,157.97,16.18,4.94" target="#b10">[11]</ref>, where we had used a fixed maximum length of 40 tokens based on preliminary experiments.</p><p>As far as the decoding method is concerned, we ran experiments using both greedy and beam search decoding <ref type="bibr" coords="13,164.27,198.62,16.29,4.94" target="#b32">[33]</ref>. In the former option, we selected the word with the highest probability yielded by the MLP component at each step, while in the latter case we would search for the most probable sequences of tokens, by maintaining and updating a set of the ğ‘› best candidates at each decoding step. The selection of these candidates is based on the likelihood of each path being the correct choice. It is calculated as the sum of the log probabilities of the so far generated sequence's tokens. Greedy decoding can be considered as a special case of beam search decoding, when the beam size is equal to one (ğ‘› = 1). We experimented with numerous values for the beam size ğ‘›, specifically ğ‘› âˆˆ {2, 3, 5}. Overall, beam search decoding resulted in better performance than following the greedy choice at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">ViT-GPT2</head><p>Our second system for the Caption Prediction sub-task is also based on the encoder-decoder framework, only that in this case we employed Transformer-based encoders and decoders. Influenced by the expeditious progress in the domain of Large Language Models (LLMs), as well as the impressive performance that these systems are able to achieve in NLP and Speech Recognition tasks, we decided to create a pipeline where Transformers are also utilized for computer vision <ref type="bibr" coords="13,164.37,424.09,16.25,4.94" target="#b33">[34]</ref>.</p><p>The encoding component of our model, which is responsible for extracting the feature representation of a given image, consists of a Vision Transformer (ViT) <ref type="bibr" coords="13,414.68,451.19,18.07,4.94" target="#b18">[19]</ref> instance loaded from a pre-trained checkpoint. Regarding the decoding component, we employed GPT-2 <ref type="bibr" coords="13,486.83,464.74,16.28,4.94" target="#b19">[20]</ref>, an open source, autoregressive LLM that achieves notable results in numerous text generation tasks. We also experimented with its distilled version (distilGPT2) as it is considered to be more time efficient with little to no decrement in performance <ref type="bibr" coords="13,337.72,505.39,16.09,4.94" target="#b34">[35]</ref>. However, we preferred to use the GPT-2 base version, as it performed better in preliminary experiments.</p><p>GPT-2 <ref type="bibr" coords="13,130.71,532.48,17.85,4.94" target="#b19">[20]</ref> is an autoregressive decoder-only model that is composed of a stack of 12 Transformer decoder blocks. Each one of these blocks sequentially processes the visual representation of the image, obtained by the image encoder, and the so far generated tokens. Following the last decoder block, a dense linear layer followed by a Softmax activation function is in charge of yielding a probability distribution over the model's vocabulary, and thus predict the next generated token. The process described so far forms a single decoding step. A vector containing the word embedding of each step's output, concatenated with its positional embedding is autoregressively fed to the bottom decoder block. This gradual, step-wise generation procedure is repeated until a special token, which denotes the end of the generated sequence, is predicted. We experimented with multiple decoding strategies; namely greedy decoding, beam search decoding (as described in Section 3.2.1), as well as top-ğ‘˜ and nuclear sampling <ref type="bibr" coords="13,445.28,667.98,16.56,4.94" target="#b32">[33,</ref><ref type="bibr" coords="13,464.57,667.98,12.42,4.94" target="#b35">36]</ref>. Both beam search decoding and the two sampling methods achieved equally competitive performance. In addition, we followed the same pre-processing steps that we have previously described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">2xE-D: Captioning Model + Seq2Seq denoiser</head><p>Our third system is a denoising model, which we employ on top of the two aforementioned caption prediction systems (see Sections 3.2.1 and 3.2.2) resulting in a novel captioning pipeline. The model is trained on the captions output by our two basic systems and the corresponding ground truth captions, in order to improve readability. Both the original generative pipeline and the denoising component feature an encoder and a decoder. Hence, we call this system 2xE-D, where E-D denotes the encoder-decoder architecture. For the denoising part, we experimented with two prominent sequence to sequence architectures, BART <ref type="bibr" coords="14,372.75,233.90,17.91,4.94" target="#b36">[37]</ref> and T5 <ref type="bibr" coords="14,426.85,233.90,16.25,4.94" target="#b37">[38]</ref>.</p><p>BART is a denoising autoencoder which is trained by reconstructing text that has been distorted by an arbitrary noise function <ref type="bibr" coords="14,273.57,261.00,16.41,4.94" target="#b36">[37]</ref>. It constitutes of a bidirectional encoder and a left-to-right autoregressive decoder. The denoising autoencoder is pre-trained on a series of tasks, which have been altered by one or more of the following corruption processes applied stochastically to the input sequences:</p><p>â€¢ Random token masking.</p><p>â€¢ Random token deletion.</p><p>â€¢ ğ‘˜-random tokens masking (employing a single masking token). â€¢ Sentence permutation. â€¢ Document rotation.</p><p>In detail, we intended to employ an instance of BART on a task similar to the one that it has been originally pre-trained on. We started off from a pre-trained BART checkpoint and fine-tuned it by providing the intermediate captions as input and the respective ground truth captions as the target text. We utilized the large version of the model, which contains 12 bidirectional encoder blocks and an equal number of decoder blocks. Table <ref type="table" coords="14,441.60,456.06,5.17,4.94">4</ref> shows three captions; the provided ground truth, the CNN-RNN generated one, and its revised version generated by our Seq2Seq denoising model. The denoiser was able to correct part of the initial generated caption, as it successfully revised the existing medical condition from "a mass" to "a lesion" and also accurately re-addressed the point of contention from "a liver lobe" to "a hepatic lobe". Moreover, it chose to state "Computed Tomography" as its abbreviation ("CT"), which is a common tactic in diagnostic reports <ref type="bibr" coords="14,260.00,537.35,16.25,4.94" target="#b38">[39]</ref>.</p><p>Extending this idea, we decided to fine-tune BART in a larger collection of noisy and denoised caption pairs. Therefore, we implemented a noise-insertion function, in accordance with the aforementioned noise transformations that BART is pre-trained on <ref type="bibr" coords="14,391.80,578.00,16.32,4.94" target="#b36">[37]</ref>, and applied it to our training ground truth captions. In this way, we created an alternative text-to-text training set, consisting of (noisy -ground truth) caption pairs. We once again fine-tuned a pre-trained BART instance on the newly created dataset in order to build a ClinicalBART model, hoping it would acquire extended knowledge of the biomedical domain, and therefore generate more medically fluent text sequences.</p><p>Furthermore, we also experimented with T5, another encoder-decoder model pre-trained in a series of both supervised and unsupervised tasks <ref type="bibr" coords="14,323.30,672.84,16.41,4.94" target="#b37">[38]</ref>, including denoising tasks. Last but not least, we were granted access to ClinicalT5 through PhysioNet <ref type="foot" coords="15,382.57,87.59,3.71,3.61" target="#foot_2">4</ref> . ClinicalT5 is a biomedical version of T5, pre-trained on the MIMIC-III dataset <ref type="bibr" coords="15,329.08,103.78,16.41,4.94" target="#b39">[40]</ref>. We further fine-tuned ClinicalT5 similarly to BART, in order to rephrase the intermediate captions produced by the CNN-RNN model (see Section 3.2.1) to approximate the gold ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Comparison between the caption generated from our CNN-RNN model, its BART-generated rewritten version (denoted as BART@CNN-RNN) and the ground truth diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated captions comparison Ground Truth</head><p>Full-body CT scan showing hepatic lesions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-RNN</head><p>computed tomography scan of the abdomen and pelvis showing a mass in the right lobe of the liver BART@CNN-RNN CT scan of the abdomen and pelvis showing a large cystic lesion in the right hepatic lobe</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments, Submissions and Results</head><p>In this section, we provide details and insight into our experiments regarding this year's campaign <ref type="bibr" coords="15,134.44,344.79,11.29,4.94" target="#b1">[2]</ref>. Moreover, we share details about our submissions and the scores achieved in our held-out development set, as well as the official test set of the competition for both sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>In the Concept Detection sub-task, we submitted our nine best performing models, after evaluating them on our held-out development set. We submitted a single instance of our CNN+FFNN model (see Section 3.1.1) and two instances of our Contrastive learning-based tagger (henceforward ContrastiveTagger, see Section 3.1.3). The rest of our submissions were ensemble systems. We investigated the combination of the predictions of two or more instances by calculating the union or the intersection of their predicted concept sets. We also experimented with a majority voting rule. That is, given an ensemble system consisting of ğ‘› models, a concept is assigned to the image if at least ğ‘› 2 + 1 models predicted it. All of our submitted ensemble systems were combinations of our CNN+FFNN and CNN+FFNN-based multi-task classifiers (henceforward MultiTask-CNN+FFNN ). This year's primary evaluation metric for the Concept Detection sub-task was the ğ¹ 1 -score between the predicted and the ground truth captions. It is calculated as the sum of the ğ¹ 1 -score for each test image, divided by the total number of test images. Each partial score is calculated between the binary multi-hot candidate vector and the corresponding ground truth vector. Precisely, let ğ¹ 1 be the overall ğ¹ 1 -score, and ğ‘“ 1 ^be the individual ğ¹ 1 -score for every test image.</p><p>Moreover, let ğ‘ ğ‘¡ and ğ‘” ğ‘¡ be the predicted and ground truth concepts for an image ğ‘¡. Finally, let ğ‘‡ denote the test set. Then, ğ¹ 1 is computed as:</p><formula xml:id="formula_7" coords="16,245.11,95.72,261.53,29.64">ğ¹ 1 = 1 |ğ‘‡ | âˆ‘ï¸ ğ‘¡âˆˆğ‘‡ ğ‘“ 1 ^(ğ‘ ğ‘¡ , ğ‘” ğ‘¡ )<label>(7)</label></formula><p>Moreover, a secondary evaluation metric was calculated that only included manually validated concepts, such as anatomy, topography and modality <ref type="bibr" coords="16,328.27,149.17,11.43,4.94" target="#b1">[2]</ref>.</p><p>In the case of our first two systems (CNN+FFNN, MultiTask-CNN+FFNN ), and specifically regarding their backbone component, we experimented with a wide range of CNN encoders. Namely, we trained the two networks using state-of-the-art CNN architectures, like EfficientNet <ref type="bibr" coords="16,89.29,203.36,16.41,4.94" target="#b40">[41]</ref>, DenseNet <ref type="bibr" coords="16,159.95,203.36,18.07,4.94" target="#b41">[42]</ref> and ResNet <ref type="bibr" coords="16,235.46,203.36,16.41,4.94" target="#b42">[43]</ref>. In addition, we extended the CNN experimental range compared to our previous participations, by utilizing MobileNet <ref type="bibr" coords="16,382.70,216.91,16.42,4.94" target="#b43">[44]</ref>, InceptionNet <ref type="bibr" coords="16,468.30,216.91,18.06,4.94" target="#b44">[45]</ref> and CheXNet <ref type="bibr" coords="16,131.84,230.46,16.09,4.94" target="#b45">[46]</ref>. We also experimented with Vision Transformers (ViT) <ref type="bibr" coords="16,393.03,230.46,16.08,4.94" target="#b18">[19]</ref>, as well as older CNN encoders like VGG <ref type="bibr" coords="16,173.90,244.01,17.78,4.94" target="#b46">[47]</ref> and AlexNet <ref type="bibr" coords="16,251.96,244.01,16.11,4.94" target="#b47">[48]</ref>. However, they were not included in our submissions as they did not provide competitive results. These were either pre-trained on ImageNet <ref type="bibr" coords="16,476.18,257.56,17.83,4.94" target="#b48">[49]</ref> or were trained with uniformly initialized weights. As expected, the model instances pre-trained on ImageNet <ref type="bibr" coords="16,367.66,550.34,18.06,4.94" target="#b48">[49]</ref> performed better than the randomly initialized ones in terms of the corresponding ğ¹ 1 score. The training loss converged faster, despite the fact that biomedical images like the ones we deal with, come from a different domain compared to ImageNet's training set. Moreover, CNN backbones outperformed ViT, which is in line with previous observations that they typically outperform other architectures such as ViT and Hybrid-ViT in classification and semantic segmentation for generic images <ref type="bibr" coords="16,487.08,618.08,16.08,4.94" target="#b49">[50]</ref>, as well as classification of biomedical images <ref type="bibr" coords="16,286.67,631.63,16.31,4.94" target="#b28">[29,</ref><ref type="bibr" coords="16,305.52,631.63,7.49,4.94" target="#b4">5]</ref>. EfficientNetB0 <ref type="bibr" coords="16,385.56,631.63,17.75,4.94" target="#b40">[41]</ref> and DenseNet-121 <ref type="bibr" coords="16,488.22,631.63,17.76,4.94" target="#b41">[42]</ref> were the two best performing ones for both systems in terms of the primary evaluation metric (Equation <ref type="formula" coords="16,135.10,658.73,3.57,4.94" target="#formula_7">7</ref>).</p><p>We also experimented with freezing some of the encoder's layers, in order to speed up the training process and also prevent their weights from being modified, in an effort to preserve the model's already acquired knowledge <ref type="bibr" coords="17,255.08,103.78,17.93,4.94" target="#b50">[51]</ref> and potentially prevent catastrophic forgetting <ref type="bibr" coords="17,487.33,103.78,16.27,4.94" target="#b51">[52]</ref>. However, our experiments showed that training the whole network resulted in higher ğ¹ 1 score, while the speed up in terms of training time was not large enough in order to trade off the higher performance levels obtained by a fully-trainable network. Moreover, we experimented with data augmentation techniques <ref type="bibr" coords="17,253.07,157.97,18.07,4.94" target="#b52">[53]</ref> (i.e, random rotation, random cropping) during the loading of each image, but they did not provide any significant improvement in the system's performance. Furthermore, we observed that despite the relatively high performance in terms of the primary evaluation metric, our models were not able to achieve satisfactory results in the prediction of the under-represented concepts. In other words, the high ğ¹ 1 -score levels were due to the system's good performance in the common concepts (see Table <ref type="table" coords="17,380.40,506.14,3.65,4.94">1</ref>), rather than to an overall classification ability. In an attempt to tackle this behaviour, we experimented with training a different instance of our CNN+FFNN classifier for each one of the four main modalities, in hopes that each classifier would be able to excel at some modality-specific characteristics. The results were mixed; two of the modality classifiers (X-Ray and MRI) were able to achieve almost 30% increase in their performance, while the other two performed even worse compared to the original version of the model. Overall, this approach did not manage to achieve more competitive results.</p><p>In Table <ref type="table" coords="17,140.29,614.54,3.81,4.94" target="#tab_3">5</ref>, we list all the methods we experimented with during our participation in this year's Concept Detection sub-task, along with the best score achieved in our development set for each one of the methods, as we experimented with numerous configurations (i.e. learning rate scheduler, number of hidden layers). To facilitate easier referencing in the rest of this section, we assign a unique ID to each method in the first column of Table <ref type="table" coords="17,413.97,668.73,3.66,4.94" target="#tab_3">5</ref>. Moreover, in Table <ref type="table" coords="18,89.29,90.23,3.70,4.94" target="#tab_4">6</ref>, we present an overview of our nine valid submissions regarding the Concept Detection task. We include each method's performance on the primary ğ¹ 1 -score in both the development and test subset, as well as the official results regarding the secondary evaluation metric. The last column contains the rank of our systems across all the task's submitted runs.</p><p>Our team officially ranked 1 st among 10 participating research groups in terms of the primary evaluation metric. Our best performing model was a union ensemble consisting of three instances of our CNN+FFNN system, where three different encoding backbones were used; EfficientNetB0 <ref type="bibr" coords="18,154.79,185.07,16.09,4.94" target="#b40">[41]</ref>, EfficientNetB0v2 and DenseNet121 <ref type="bibr" coords="18,331.54,185.07,16.09,4.94" target="#b41">[42]</ref>. Furthermore, we ranked 2nd in the secondary evaluation metric by employing a single CNN+FFNN instance, using EfficientNetB0 <ref type="bibr" coords="18,89.29,212.17,17.91,4.94" target="#b40">[41]</ref> as the image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>In the Caption Prediction sub-task, we also submitted nine systems, which were selected after evaluating them on our development set. We submitted two instances of our CNN-RNN encoderdecoder (see Section 3.2.1) and two instances of our Transformer-based ViT-GPT2 model (see Section 3.2.2). The difference between each model's submissions lays in the number of beams used during the beam search decoding. We submitted instances where the beam size was equal to three and five, and therefore denote them as CNN-RNN-BS3, CNN-RNN-BS5, ViT-GPT2-BS3 and ViT-GPT2-BS5. In addition, we submitted five instances of our Seq2Seq denoising system employed on top of the four aforementioned submissions. The denoising models utilized were T5 <ref type="bibr" coords="18,103.95,370.29,16.41,4.94" target="#b37">[38]</ref>, ClinicalT5, BART <ref type="bibr" coords="18,210.88,370.29,16.41,4.94" target="#b36">[37]</ref>, as well as ClinicalBART (BART-large further pre-trained in ImageCLEF captions, see Section 3.2.3).</p><p>In this year's campaign, BERTscore <ref type="bibr" coords="18,253.74,397.39,17.76,4.94" target="#b53">[54]</ref> was used as the primary evaluation metric, in contrast to last year that used BLEU <ref type="bibr" coords="18,215.45,410.94,16.40,4.94" target="#b54">[55]</ref>. ROUGE-1 <ref type="bibr" coords="18,286.31,410.94,18.06,4.94" target="#b55">[56]</ref> constitutes the secondary evaluation metric. Unlike BLEU and ROUGE-1, BERTscore <ref type="bibr" coords="18,269.39,424.49,17.96,4.94" target="#b53">[54]</ref> offers a more contextual evaluation system, as it leverages BERT's <ref type="bibr" coords="18,166.84,438.04,17.76,4.94" target="#b30">[31]</ref> word embeddings and attempts to compute the semantic affinity between the words of the predicted and ground truth captions based on their cosine similarity. Regarding our CNN-RNN model, we relied on our last year's experiments and only adopted the encoding architectures that performed best; EfficientNetB0 <ref type="bibr" coords="19,366.01,103.78,17.78,4.94" target="#b40">[41]</ref> and DenseNet121 <ref type="bibr" coords="19,465.73,103.78,16.10,4.94" target="#b41">[42]</ref>. The encoder extracted the image representations, which we stored, before feeding them to the RNN decoding unit. We experimented with retrieving the image features from either a pre-trained CNN instance or the encoding unit of our best performing CNN+FFNN classification model, in hopes that it has learned to generate quality biomedical image representations through the training procedure. An interesting research point would be to try to train the CNN and the RNN encoder concurrently. Overall, the CNN-RNN encoder decoder achieved decent performance in the BERTscore <ref type="bibr" coords="19,156.13,198.62,17.80,4.94" target="#b53">[54]</ref> metric and, as in our last year's participation <ref type="bibr" coords="19,376.04,198.62,17.80,4.94" target="#b10">[11]</ref> noteworthy scores in the ROUGE-1 <ref type="bibr" coords="19,135.60,212.17,17.91,4.94" target="#b55">[56]</ref> evaluation metric.</p><p>Our ViT-GPT2 model did not yield the expected results. We experimented with numerous configurations, like higher or lower learning rate along with scheduling techniques, increased generation penalty, as well as data augmentation. Specifically, we transformed each image on the fly, during the loading process. We first rotated it by an angle of 30 degrees towards a random direction and then resized it to 224 Ã— 224 Ã— 3 pixels, which is the size that we selected to employ for every image. In this way, a slightly different view of the same image was passed to the encoding component in each epoch aiming to increase the data variety, improve the model's robustness, as well as prevent it from quickly overfitting <ref type="bibr" coords="19,380.31,320.56,16.25,4.94" target="#b52">[53]</ref>.</p><p>Our best submission, which managed to rank 7 th out of 70 submitted systems, was the 2xE-D model, which is comprised of one of the aforementioned captioning models and a subsequent denoising component. Specifically, the instance that used BART outperformed the three other denoising models; T5 <ref type="bibr" coords="19,182.60,374.76,16.09,4.94" target="#b37">[38]</ref>, ClinicalT5 and ClinicalBART (see section 3.2.3). We also experimented with multiple configurations, as well as decoding schemes. In this case, beam search decoding outperformed both nucleus and top-ğ‘˜ sampling <ref type="bibr" coords="19,303.03,401.86,16.43,4.94" target="#b32">[33,</ref><ref type="bibr" coords="19,322.19,401.86,14.03,4.94" target="#b35">36]</ref> in multiple preliminary experiments. In Table <ref type="table" coords="20,137.86,90.23,3.66,4.94" target="#tab_5">7</ref>, we present a summary of our nine submissions, including the method's identifiers, their performance on the primary and secondary metric for both the development and test set, as well as its official rank across 70 submitted systems. Our group ranked 3 rd among 13 teams in the Caption Prediction sub-task based on the primary evaluation metric. Our best model was BART@CNN-RNN-BS3, followed in close distance, by ClinicalBART@CNN-RNN-BS3, the biomedically-wise fine-tuned instance of the same system. In Table <ref type="table" coords="20,431.37,157.97,5.09,4.94" target="#tab_7">8</ref> we present our submissions' performance on all the official metrics, as reported by the organizers, in order to provide a more thorough evaluation of their capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Regarding Concept Detection, our best-performing system was a CNN+FFNN pipeline (Section 3.1.1), while our remaining submissions included a CNN+FFNN-based multi-task classifier (Section 3.1.2), a contrastive learning-based system with a CLIP-like objective (Section 3.1.3) and ensembles employing the aforementioned approaches based on majority voting, union, intersection, as well as scaling by a factor ğœ† in the case of our contrastive system. Our ensembles based on the CNN+FFNN pipeline, including its multi-task version, were ranked at positions 1, 2, 3, 4, 5, 6 and 7 among approximately 60 systems in the respective sub-task, which is consistent with their successful performance in previous years <ref type="bibr" coords="20,300.84,338.54,16.48,4.94" target="#b9">[10,</ref><ref type="bibr" coords="20,320.05,338.54,12.36,4.94" target="#b10">11]</ref>, while our best-performing individual CNN+FFNN system was ranked at position 8 <ref type="bibr" coords="20,291.75,352.09,11.43,4.94" target="#b1">[2]</ref>.</p><p>In the Caption Prediction sub-task, we ranked 3 rd among the participating groups, by both extending our previous work <ref type="bibr" coords="20,226.71,379.19,18.06,4.94" target="#b10">[11]</ref> and exploiting the state-of-the-art methods in NLP. Our systems included a typical Show and Tell model <ref type="bibr" coords="20,314.20,392.74,18.06,4.94" target="#b16">[17]</ref> with a CNN backbone encoder and a recurrent decoder with GRU cells <ref type="bibr" coords="20,248.40,406.29,16.41,4.94" target="#b31">[32]</ref>, a Transformer-based pipeline using a ViT encoder <ref type="bibr" coords="20,89.29,419.84,18.07,4.94" target="#b18">[19]</ref> and GPT-2 decoder <ref type="bibr" coords="20,200.37,419.84,16.42,4.94" target="#b19">[20]</ref>, as well as a sequence-to-sequence <ref type="bibr" coords="20,381.62,419.84,18.07,4.94" target="#b12">[13]</ref> denoising autoencoder employed on top of the two other systems, in order to rephrase and correct the initial draft radiology reports.</p><p>In future work, we plan to expand our research in biomedical LLMs and their reasoning abilities, towards the goal of exploiting the generative capabilities of models like BioGPT <ref type="bibr" coords="20,488.04,474.04,17.94,4.94" target="#b56">[57]</ref> or BioMedLM <ref type="bibr" coords="20,153.24,487.58,17.86,4.94" target="#b57">[58]</ref> to produce high-quality captions; possibly via instruction tuning and, more generally, alignment with user needs <ref type="bibr" coords="20,249.72,501.13,16.09,4.94" target="#b58">[59]</ref>. Furthermore, apart from making use of the knowledge encoded in the weights of the LLMs, we aim to shed light in the use of dense retrieval <ref type="bibr" coords="20,476.32,514.68,17.99,4.94" target="#b59">[60]</ref> in biomedical image captioning <ref type="bibr" coords="20,223.56,528.23,11.48,4.94" target="#b4">[5,</ref><ref type="bibr" coords="20,238.07,528.23,12.42,4.94" target="#b28">29]</ref>, based on architectures similar to Retrieval Augmented Generation <ref type="bibr" coords="20,142.36,541.78,16.31,4.94" target="#b60">[61]</ref>. Such pipelines will allow us to increase the LLMs' capacity by an additional, non-parametric memory, in the form of a FAISS index <ref type="bibr" coords="20,327.27,555.33,16.09,4.94" target="#b61">[62]</ref>, towards the goal of improving their reasoning abilities. We would also be interested to discover potential associations between the two sub-tasks. Last but not least, the qualitative differences in the captions generated by the different methods are to be considered, since they highlight their practical usefulness in real-life scenarios <ref type="bibr" coords="20,133.48,609.53,11.36,4.94" target="#b4">[5,</ref><ref type="bibr" coords="20,147.57,609.53,12.32,4.94" target="#b28">29]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,351.80,418.36,8.93;4,89.29,365.57,154.16,4.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three example images from the ImageCLEFmedical2023 [2] dataset, along with their corresponding CUIs and UMLS terms [21].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,514.85,416.70,9.15;5,89.29,526.81,416.70,8.74;5,88.99,540.81,252.90,4.79"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Visualization of the data's long-tail distribution. The ğ‘¦-axis shows the number of appearances for each tag, and the ğ‘¥-axis the tag's class index. (b) Histogram with 20 fixed-size bins (horizontal axis) depicting the number of gold tags per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,458.72,416.70,9.15;6,89.29,470.67,416.70,8.74;6,89.29,484.67,416.69,4.79;6,88.93,496.62,26.97,4.79"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Histogram that visualizes the captions' length distribution. The ğ‘¦-axis contains the number of images that fall into each bin, while the ğ‘¥-axis contains the number of words included in the caption. (b) Box-plot over the same distribution, which highlights the outliers in the range of 100 to 200 words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,453.09,335.59,8.93;9,313.99,352.47,88.08,83.65"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of our CNN+FFNN-based Multi-task classifier architecture.</figDesc><graphic coords="9,313.99,352.47,88.08,83.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,89.29,353.21,416.70,9.15;11,89.29,365.17,416.95,8.74;11,89.06,379.17,416.92,4.79;11,89.29,391.12,76.56,4.79"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of our CLIP-based approach. For each batch of ğ‘› images, we compute the embeddings for ğ‘š = |ğ¶| concepts (and the ğ‘› images) and aim to maximize the red-colored similarity values which correspond to the similarities between each image and its gold truth concepts. Figure adapted from [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,89.29,580.58,417.78,9.65;12,88.93,592.54,402.06,9.65;12,111.30,434.85,55.11,105.17"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An overview of our CNN-RNN system's architecture. ğ‘Š ğ‘’ denotes a word embedding matrix, while ğ‘‚ğ‘ˆ ğ‘‡ ğ‘– represents a 1-hot vector of the previously generated word. Figure adapted from [17].</figDesc><graphic coords="12,111.30,434.85,55.11,105.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="16,88.99,296.86,418.70,233.83"><head>Table 5 Summary of our individual experiments (no ensembles included) in the ImageCLEFmedi- cal2023 Concept Detection sub-task.</head><label>5</label><figDesc>The table contains the best scores that our systems achieved in our held-out development set for each method.</figDesc><table coords="16,153.20,348.81,279.91,181.88"><row><cell></cell><cell cols="2">Individual Concept Detection Experiments</cell></row><row><cell>ID</cell><cell>Method</cell><cell>Development</cell></row><row><cell>cd1</cell><cell>EfficientNetB0@CNN+FFNN</cell><cell>0.5173</cell></row><row><cell>cd2</cell><cell>DenseNet121@CNN+FFNN</cell><cell>0.5152</cell></row><row><cell>cd3</cell><cell>EfficientNetB0v2@CNN+FFNN</cell><cell>0.5151</cell></row><row><cell>cd4</cell><cell>MobileNet@CNN+FFNN</cell><cell>0.5128</cell></row><row><cell>cd5</cell><cell>InceptionNetv3@CNN+FFNN</cell><cell>0.5127</cell></row><row><cell>cd6</cell><cell>ResNet101@CNN+FFNN</cell><cell>0.5116</cell></row><row><cell>cd7</cell><cell>DenseNet169@CNN+FFNN</cell><cell>0.5093</cell></row><row><cell>cd8</cell><cell>CheXNet@CNN+FFNN</cell><cell>0.4910</cell></row><row><cell>cd9</cell><cell>ViT@CNN+FFNN</cell><cell>0.4776</cell></row><row><cell>cd10</cell><cell>EfficientNetB0@MulTitask-CNN+FFNN</cell><cell>0.4802</cell></row><row><cell>cd11</cell><cell>DenseNet121@MultiTask-CNN+FFNN</cell><cell>0.4792</cell></row><row><cell>cd12</cell><cell>ContrastiveTagger</cell><cell>N/A</cell></row><row><cell>cd13</cell><cell>MultitaskContrastiveTagger</cell><cell>0.5080</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="17,88.84,210.82,417.14,232.71"><head>Table 6 Summary of our submissions in the ImageCLEFmedical2023 Concept Detection sub-task.</head><label>6</label><figDesc>The table contains the scores that our systems achieved in our held-out development set and the official test set, along with rankings among all the systems submitted from 10 participating teams. The annotation @U denotes a union ensemble, @UoI indicates a union of intersection ensemble, while the * symbol means that the corresponding submission was late, and therefore is not ranked.</figDesc><table coords="17,96.17,286.68,404.68,156.85"><row><cell></cell><cell></cell><cell cols="3">AUEB NLP Group -Submission Table</cell><cell></cell><cell></cell></row><row><cell>ID</cell><cell>Run ID</cell><cell>Approach</cell><cell cols="2">Primary F1 Dev Test</cell><cell cols="2">Secondary F1 Rank</cell></row><row><cell>cd15</cell><cell>4</cell><cell>3xCNN+FFNN@U -(cd1, cd2, cd3)</cell><cell cols="2">0.5224 0.5222</cell><cell>0.9258</cell><cell>1</cell></row><row><cell>cd16</cell><cell>8</cell><cell>2xCNN+FFNN@UoI -(cd15, cd17)</cell><cell cols="2">0.5223 0.5220</cell><cell>0.9276</cell><cell>2</cell></row><row><cell>cd17</cell><cell>2</cell><cell>3xCNN+FFNN@U -(cd1, cd2, cd4)</cell><cell cols="2">0.5223 0.5218</cell><cell>0.9220</cell><cell>3</cell></row><row><cell>cd18</cell><cell>7</cell><cell>2xCNN+FFNN@U -(cd1, cd2)</cell><cell cols="2">0.5217 0.5212</cell><cell>0.9277</cell><cell>4</cell></row><row><cell>cd19</cell><cell>6</cell><cell>Union Ensemble of cd1, cd2, cd10</cell><cell cols="2">0.5214 0.5208</cell><cell>0.9154</cell><cell>5</cell></row><row><cell>cd20</cell><cell>3</cell><cell>3xCNN+FFNN@U -(cd2, cd3, cd4)</cell><cell cols="2">0.5221 0.5207</cell><cell>0.9234</cell><cell>6</cell></row><row><cell>cd21</cell><cell>5</cell><cell>3xCNN+FFNN@U -(cd1, cd2, cd5)</cell><cell cols="2">0.5216 0.5188</cell><cell>0.9194</cell><cell>7</cell></row><row><cell>cd22</cell><cell>1</cell><cell cols="3">EfficientNetB0@CNN+FFNN -(cd1) 0.5216 0.5174</cell><cell>0.9306</cell><cell>8</cell></row><row><cell>cd23</cell><cell>10</cell><cell>ContrastiveTagger -(cd12)</cell><cell>N/A</cell><cell>0.4423</cell><cell>0.8112</cell><cell>30</cell></row><row><cell>cd24</cell><cell>11</cell><cell cols="3">MultiTaskContrastiveTagger -(cd13)* 0.5080 0.5092</cell><cell>0.9112</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="18,88.99,477.33,416.99,42.48"><head>Table 7 Summary of our submissions in the ImageCLEFmedical2023 Caption Prediction sub-task.</head><label>7</label><figDesc>The table contains the scores that our systems achieved in our held-out development set and the official test set, along with rankings among all the captioning systems submitted from 13 participating teams.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="18,97.11,529.28,404.25,144.50"><head>AUEB NLP Group -Submission Table ID Run ID</head><label></label><figDesc></figDesc><table coords="18,97.11,543.99,404.25,129.79"><row><cell></cell><cell></cell><cell>Approach</cell><cell cols="2">BERTscore Dev Test</cell><cell cols="2">ROUGE-1 Dev Test</cell><cell>Rank</cell></row><row><cell>cp1</cell><cell>2</cell><cell>BART@CNN-RNN-BS3</cell><cell cols="3">0.6141 0.6170 0.2111</cell><cell>0.2130</cell><cell>7</cell></row><row><cell>cp2</cell><cell>3</cell><cell>ClinicalBART@CNN-RNN-BS3</cell><cell>0.6118</cell><cell>0.6147</cell><cell>0.2123</cell><cell>0.2143</cell><cell>10</cell></row><row><cell>cp3</cell><cell>4</cell><cell>ClinicalT5@CNN-RNN-BS3</cell><cell>0.5923</cell><cell>0.6098</cell><cell>0.2146</cell><cell>0.2188</cell><cell>19</cell></row><row><cell>cp4</cell><cell>1</cell><cell>CNN-RNN-BS3</cell><cell>0.6046</cell><cell cols="3">0.6064 0.2249 0.2273</cell><cell>27</cell></row><row><cell>cp5</cell><cell>8</cell><cell>BART@CNN-RNN-BS5</cell><cell>0.6043</cell><cell>0.6058</cell><cell>0.1881</cell><cell>0.1884</cell><cell>29</cell></row><row><cell>cp6</cell><cell>9</cell><cell>CNN-RNN-BS5</cell><cell>0.5933</cell><cell>0.5960</cell><cell>0.2146</cell><cell>0.2155</cell><cell>35</cell></row><row><cell>cp7</cell><cell>6</cell><cell>BART@ViT-GPT2-BS5</cell><cell>0.5861</cell><cell>0.5879</cell><cell>0.1711</cell><cell>0.1708</cell><cell>38</cell></row><row><cell>cp8</cell><cell>7</cell><cell>ViT-GPT2-BS5</cell><cell>0.5703</cell><cell>0.5629</cell><cell>0.1787</cell><cell>0.1682</cell><cell>51</cell></row><row><cell>cp9</cell><cell>5</cell><cell>ViT-GPT2-BS3</cell><cell>0.5421</cell><cell>0.5416</cell><cell>0.1697</cell><cell>0.1682</cell><cell>65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="19,88.99,427.61,416.99,48.92"><head>Table 8 Summary of our submissions regarding the Caption Prediction sub-task.</head><label>8</label><figDesc>The table contains each system's performance on all officially reported measures. AUEB</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="19,95.70,467.60,405.11,130.15"><head>NLP Group Submissions -Evaluation on All Metrics ID BERTscore ROUGE-1 BLEURT BLEU METEOR CIDEr CLIPscore Rank</head><label></label><figDesc></figDesc><table coords="19,95.70,492.31,398.17,105.45"><row><cell>cp1</cell><cell>0.6170</cell><cell>0.2130</cell><cell>0.2950</cell><cell>0.1692</cell><cell>0.0719</cell><cell>0.1466</cell><cell>0.8038</cell><cell>7</cell></row><row><cell>cp2</cell><cell>0.6147</cell><cell>0.2143</cell><cell>0.2877</cell><cell>0.1522</cell><cell>0.0695</cell><cell>0.1582</cell><cell>0.8059</cell><cell>10</cell></row><row><cell>cp3</cell><cell>0.6098</cell><cell>0.2188</cell><cell>0.2991</cell><cell>0.1919</cell><cell>0.0742</cell><cell>0.1447</cell><cell>0.7978</cell><cell>19</cell></row><row><cell>cp4</cell><cell>0.6064</cell><cell>0.2273</cell><cell>0.3048</cell><cell>0.2061</cell><cell>0.0789</cell><cell>0.1661</cell><cell>0.8025</cell><cell>27</cell></row><row><cell>cp5</cell><cell>0.6058</cell><cell>0.1884</cell><cell>0.2730</cell><cell>0.1222</cell><cell>0.0606</cell><cell>0.1275</cell><cell>0.8010</cell><cell>29</cell></row><row><cell>cp6</cell><cell>0.5960</cell><cell>0.2155</cell><cell>0.3050</cell><cell>0.2039</cell><cell>0.0807</cell><cell>0.1360</cell><cell>0.8043</cell><cell>35</cell></row><row><cell>cp7</cell><cell>0.5879</cell><cell>0.1708</cell><cell>0.2590</cell><cell>0.1340</cell><cell>0.0539</cell><cell>0.0815</cell><cell>0.7569</cell><cell>38</cell></row><row><cell>cp8</cell><cell>0.5629</cell><cell>0.1682</cell><cell>0.2793</cell><cell>0.1514</cell><cell>0.0655</cell><cell>0.0486</cell><cell>0.7602</cell><cell>51</cell></row><row><cell>cp9</cell><cell>0.5416</cell><cell>0.1682</cell><cell>0.2780</cell><cell>0.1322</cell><cell>0.0638</cell><cell>0.0388</cell><cell>0.7600</cell><cell>65</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,92.57,662.71,312.60,4.06"><p>UMLS: https://www.nlm.nih.gov/research/umls/index.html, Last accessed: 2023-07-07</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,92.57,673.67,358.65,4.06"><p>PMC Open Access: https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/, Last accessed: 2023-07-07</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="15,92.57,673.66,288.60,4.06"><p>https://www.physionet.org/content/clinical-t5/1.0.0/, Last accessed: 2023-07-07</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="21,112.66,114.54,394.53,4.94;21,112.66,128.09,395.17,4.94;21,112.66,141.64,394.53,4.94;21,112.66,155.19,395.17,4.94;21,112.39,168.74,394.80,4.94;21,112.48,182.29,394.70,4.94;21,112.66,195.84,395.17,4.94;21,112.66,209.39,393.32,4.94;21,112.66,222.94,394.52,4.94;21,112.33,236.48,120.27,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,224.93,182.29,282.25,4.94;21,112.66,195.84,228.44,4.94">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrÄƒgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>StorÃ¥s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>SchÃ¶ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,363.94,195.84,143.89,4.94;21,112.66,209.39,393.32,4.94;21,112.66,222.94,136.27,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="21,280.09,222.94,221.52,4.94">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,250.03,394.52,4.94;21,112.66,263.58,395.17,4.94;21,112.66,277.13,393.33,4.94;21,112.66,290.68,247.61,4.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="21,298.72,263.58,209.11,4.94;21,112.66,277.13,171.81,4.94">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,307.89,277.13,114.04,4.94">CLEF2023 Working Notes</title>
		<title level="s" coord="21,429.41,277.13,76.58,4.94;21,112.66,290.68,97.38,4.94">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,304.23,393.32,4.94;21,112.66,317.78,269.01,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="21,395.93,304.23,110.06,4.94;21,112.66,317.78,27.24,4.94">Diagnostic captioning: a survey</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papamichail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,148.36,317.78,164.59,4.94">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,331.33,393.33,4.94;21,112.66,344.88,393.32,4.94;21,112.66,358.43,395.01,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="21,432.08,331.33,73.91,4.94;21,112.66,344.88,364.12,4.94">Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,112.66,358.43,314.04,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,371.98,393.33,4.94;21,112.66,385.53,393.80,4.94;21,112.66,399.07,263.16,4.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="21,175.25,371.98,234.52,4.94">Medical image captioning based on Deep Architectures</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moschovis</surname></persName>
		</author>
		<ptr target="http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-323528" />
		<imprint>
			<date type="published" when="2022">2022. 2023-07-07</date>
			<pubPlace>Stockholm, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>KTH Royal Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="21,112.66,412.62,395.17,4.94;21,112.66,426.17,394.53,4.94;21,112.28,439.72,376.16,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="21,324.75,412.62,183.09,4.94;21,112.66,426.17,12.71,4.94">A Survey on Biomedical Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,148.30,426.17,358.89,4.94;21,112.28,439.72,186.39,4.94">Proceedings of the Second Workshop on Shortcomings in Vision and Language, Association for Computational Linguistics</title>
		<meeting>the Second Workshop on Shortcomings in Vision and Language, Association for Computational Linguistics<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,453.27,393.33,4.94;21,112.66,466.82,394.53,4.94;21,112.66,478.12,392.61,9.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="21,312.26,453.27,193.73,4.94">AUEB NLP group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,155.04,466.82,346.76,4.94">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="21,353.21,478.12,122.23,9.72">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-09">2019. September 9-12. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,493.92,395.17,4.94;21,112.66,507.47,393.33,4.94;21,112.66,518.77,393.33,9.72;21,112.66,532.32,78.22,9.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="21,381.82,493.92,126.02,4.94;21,112.66,507.47,83.13,4.94">AUEB NLP group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,240.86,507.47,265.13,4.94;21,112.66,521.02,75.33,4.94">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="21,436.51,518.77,69.48,9.72;21,112.66,532.32,48.40,9.72">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09-22">2020. September 22-25. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,548.12,393.32,4.94;21,112.66,561.67,394.61,4.94;21,112.41,575.21,394.78,4.94;21,112.66,588.76,240.89,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,319.82,548.12,186.16,4.94;21,112.66,561.67,57.11,4.94">Medical Image Tagging by Deep Learning and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,192.59,561.67,314.68,4.94;21,112.41,575.21,293.11,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2020">September 22-25, 2020. 2020</date>
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,602.31,393.33,4.94;21,112.66,615.86,393.33,4.94;21,112.66,629.41,393.53,4.94;21,112.66,640.71,319.54,9.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,457.06,602.31,48.93,4.94;21,112.66,615.86,178.41,4.94">AUEB NLP group at ImageCLEFmed Caption tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,337.39,615.86,168.60,4.94;21,112.66,629.41,248.52,4.94">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="21,212.12,640.71,122.23,9.72">CEUR Workshop Proceedings</title>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09-21">2021. September 21-24. 2936. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,656.51,395.17,4.94;21,112.66,670.06,394.52,4.94;22,112.66,90.23,362.61,4.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,440.73,656.51,67.10,4.94;21,112.66,670.06,235.09,4.94">AUEB NLP Group at ImageCLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zachariadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Trakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,391.42,670.06,115.77,4.94;22,112.66,90.23,175.50,4.94">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1355" to="1373" />
		</imprint>
	</monogr>
	<note>Androutsopoulos</note>
</biblStruct>

<biblStruct coords="22,112.66,103.78,395.17,4.94;22,112.66,117.33,394.53,4.94;22,112.39,130.88,73.69,4.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="22,484.04,103.78,23.79,4.94;22,112.66,117.33,143.10,4.94">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,278.07,117.33,224.37,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="22,112.66,144.43,394.53,4.94;22,112.66,157.97,276.72,4.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,261.17,144.43,240.91,4.94">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,112.66,157.97,32.84,4.94">NIPS&apos;14</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,171.52,394.53,4.94;22,112.34,185.07,394.85,4.94;22,112.28,198.62,288.83,4.94" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno>ArXiv abs/2303.18223</idno>
		<title level="m" coord="22,112.28,198.62,151.96,4.94">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,212.17,393.33,4.94;22,112.66,225.72,393.32,4.94;22,112.66,239.27,166.42,4.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="22,333.80,212.17,172.18,4.94;22,112.66,225.72,152.76,4.94">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,288.43,225.72,217.55,4.94;22,112.66,239.27,135.92,4.94">Proceedings of the 37th International Conference on Machine Learning, ICML&apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML&apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,252.82,394.53,4.94;22,112.66,266.37,393.32,4.94;22,112.66,279.92,393.33,4.94;22,112.66,293.47,265.33,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="22,315.73,266.37,190.25,4.94;22,112.66,279.92,134.40,4.94">Learning Transferable Visual Models from Natural Language Supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,270.35,279.92,235.64,4.94;22,112.66,293.47,78.76,4.94">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,307.02,393.33,4.94;22,112.66,320.56,393.98,4.94;22,112.33,334.11,78.48,4.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="22,321.33,307.02,184.66,4.94;22,112.66,320.56,41.25,4.94">Show and Tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,185.83,320.56,320.81,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,347.66,393.32,4.94;22,112.33,361.21,393.65,4.94;22,112.28,374.76,185.37,4.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="22,444.45,347.66,61.53,4.94;22,112.33,361.21,254.67,4.94">From Show to Tell: A survey on Deep Learning-Based Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cascianelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fiameni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,375.66,361.21,130.32,4.94;22,112.28,374.76,153.45,4.94">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,388.31,395.16,4.94;22,112.66,401.86,393.33,4.94;22,112.41,415.41,393.57,4.94;22,112.66,428.96,155.93,4.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="22,419.65,401.86,86.34,4.94;22,112.41,415.41,259.29,4.94">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,394.36,415.41,111.63,4.94;22,112.66,428.96,125.91,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,442.51,393.33,4.94;22,112.66,456.06,246.01,4.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="22,412.10,442.51,93.89,4.94;22,112.66,456.06,141.16,4.94">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,262.00,456.06,56.95,4.94">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,469.61,393.32,4.94;22,112.66,483.16,209.62,4.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="22,186.64,469.61,319.34,4.94;22,112.66,483.16,52.45,4.94">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,173.80,483.16,103.68,4.94">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,496.70,394.61,4.94;22,112.28,510.25,393.70,4.94;22,112.33,523.80,394.85,4.94;22,112.66,537.35,303.41,4.94" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<title level="m" coord="22,339.37,496.70,167.90,4.94;22,112.28,510.25,393.70,4.94;22,112.33,523.80,134.47,4.94;22,322.39,523.80,180.16,4.94">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="22,112.66,550.90,394.53,4.94;22,112.66,564.45,393.33,4.94;22,112.66,578.00,394.53,4.94;22,112.66,589.30,391.58,9.72" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="22,112.66,564.45,236.06,4.94">Clinically Accurate Chest X-Ray Report Generation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,377.34,564.45,128.64,4.94;22,112.66,578.00,154.72,4.94;22,203.15,589.30,180.99,9.72">Proceedings of the Machine Learning for Healthcare Conference</title>
		<meeting>the Machine Learning for Healthcare Conference<address><addrLine>MLHC; Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-08">2019. 9-10 August 2019. 2019</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="269" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="22,112.66,605.10,393.33,4.94;22,112.66,618.65,335.96,4.94" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="22,195.20,605.10,266.38,4.94">Exploring Deep Learning Methods for Medical Image Tagging</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Athens, Greece</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Athens University of Economics and Business</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="22,112.66,632.20,393.33,4.94;22,112.66,645.75,393.98,4.94;22,112.41,659.29,48.96,4.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="22,277.26,632.20,228.72,4.94;22,112.66,645.75,47.25,4.94">Fine-tuning CNN image retrieval with no human annotation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>RadenoviÄ‡</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,169.31,645.75,293.82,4.94">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,672.84,393.71,4.94;23,112.66,90.23,393.33,4.94;23,112.33,103.78,52.21,4.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="22,498.63,672.84,7.73,4.94;23,112.66,90.23,297.63,4.94">A Comparison of Pooling Methods for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aamir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nawi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arshad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alruban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alaybani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,419.18,90.23,74.13,4.94">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">8643</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,117.33,393.33,4.94;23,112.26,130.88,393.73,4.94;23,112.41,144.43,91.35,4.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="23,424.65,117.33,81.34,4.94;23,112.26,130.88,216.56,4.94">Dropout: A simple way to prevent Neural Networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,336.80,130.88,169.19,4.94">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,157.97,393.32,4.94;23,112.66,171.52,160.04,4.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="23,191.13,157.97,195.18,4.94">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,394.77,157.97,111.21,4.94;23,112.66,171.52,128.12,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,185.07,393.33,4.94;23,112.14,198.62,367.10,4.94" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="23,232.76,185.07,183.25,4.94">Neuraldynamicslab at imageclef medical</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moschovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>FransÃ©n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,460.68,185.07,45.31,4.94;23,112.14,198.62,248.02,4.94">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,212.17,394.61,4.94;23,112.66,225.72,393.33,4.94;23,112.66,239.27,299.13,4.94" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="23,316.09,212.17,191.18,4.94;23,112.66,225.72,97.66,4.94">A Closer Look at Deep Learning Heuristics: Learning rate restarts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,217.97,225.72,288.02,4.94;23,112.66,239.27,162.44,4.94">Warmup and Distillation, in: 7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,252.82,393.32,4.94;23,112.33,266.37,393.65,4.94;23,112.66,279.92,393.32,4.94;23,112.66,293.47,393.33,4.94;23,112.66,307.02,281.85,4.94" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="23,320.80,252.82,185.18,4.94;23,112.33,266.37,192.76,4.94">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,330.46,266.37,175.52,4.94;23,112.66,279.92,393.32,4.94;23,112.66,293.47,99.97,4.94">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="23,112.66,320.56,393.32,4.94;23,112.66,334.11,210.78,4.94" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="23,295.20,320.56,210.78,4.94;23,112.66,334.11,144.62,4.94">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ã‡</forename><surname>GÃ¼lÃ§ehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,347.66,393.71,4.94;23,112.66,361.21,135.04,4.94" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="23,255.91,347.66,250.45,4.94;23,112.66,361.21,28.13,4.94">Decoding Methods in Neural Language Generation: A Survey</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>ZarrieÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>SchÃ¼z</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,149.40,361.21,53.51,4.94">Information</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,374.76,394.53,4.94;23,112.28,388.31,159.36,4.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="23,351.90,374.76,150.51,4.94">Transformers in Vision: A Survey</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,112.28,388.31,114.57,4.94">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,401.86,394.52,4.94;23,112.34,415.41,394.85,4.94;23,112.28,428.96,128.06,4.94" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="23,229.94,415.41,272.39,4.94">A short study on compressing decoder-based Language Models</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">E</forename><surname>Mesbahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Anchuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hajimolahoseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<idno>ArXiv abs/2110.08460</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,442.51,394.53,4.94;23,112.33,456.06,365.72,4.94" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="23,270.01,442.51,232.37,4.94">On Decoding Strategies for Neural Text Generators</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,112.33,456.06,276.71,4.94">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="997" to="1012" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,469.61,395.17,4.94;23,112.66,483.16,395.17,4.94;23,112.66,496.70,393.32,4.94;23,112.66,510.25,394.52,4.94;23,112.66,523.80,125.93,4.94" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="23,146.14,483.16,361.68,4.94;23,112.66,496.70,174.42,4.94">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,311.16,496.70,194.82,4.94;23,112.66,510.25,390.36,4.94">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,537.35,394.53,4.94;23,112.66,550.90,395.01,4.94;23,112.66,564.45,182.76,4.94" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="23,112.66,550.90,377.78,4.94">Exploring the limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,501.64,550.90,6.03,4.94;23,112.66,564.45,78.46,4.94">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,578.00,393.33,4.94;23,112.66,591.55,206.44,4.94" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="23,193.51,578.00,181.97,4.94">A study of abbreviations in clinical notes</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stetson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,383.85,578.00,122.14,4.94;23,112.66,591.55,145.53,4.94">AMIA, Annual Symposium proceedings / AMIA Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="821" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,605.10,394.53,4.94;23,112.66,618.65,393.33,4.94;23,112.33,632.20,62.36,4.94" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="23,191.72,618.65,229.94,4.94">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">W</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,431.50,618.65,66.25,4.94">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,645.75,394.53,4.94;23,112.66,659.29,394.52,4.94;23,112.66,670.60,393.32,9.72;24,112.66,87.98,133.52,9.72" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="23,183.60,645.75,318.59,4.94">Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,127.50,659.29,375.06,4.94">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,103.78,393.33,4.94;24,112.66,117.33,394.53,4.94;24,112.66,130.88,90.72,4.94" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="24,356.26,103.78,149.73,4.94;24,112.66,117.33,39.90,4.94">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,197.71,117.33,304.54,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,144.43,393.32,4.94;24,112.66,157.97,395.00,4.94" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="24,254.59,144.43,207.56,4.94">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,112.66,157.97,307.90,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,171.52,394.53,4.94;24,112.66,185.07,395.17,4.94;24,112.66,198.62,95.32,4.94" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="24,158.52,185.07,349.31,4.94;24,112.66,198.62,29.86,4.94">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,212.17,394.53,4.94;24,112.28,225.72,393.92,4.94;24,112.30,239.27,240.58,4.94" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="24,181.88,225.72,142.56,4.94">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,347.79,225.72,158.41,4.94;24,112.30,239.27,172.53,4.94">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,252.82,394.53,4.94;24,112.66,266.37,393.33,4.94;24,112.66,279.92,344.83,4.94" xml:id="b45">
	<monogr>
		<title level="m" type="main" coord="24,322.01,266.37,183.98,4.94;24,112.66,279.92,210.00,4.94">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>CoRR abs/1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,293.47,393.33,4.94;24,112.66,307.02,394.53,4.94;24,112.66,320.56,159.47,4.94" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="24,242.31,293.47,263.68,4.94;24,112.66,307.02,52.37,4.94">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,188.57,307.02,313.99,4.94">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">May 7-9, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,334.11,393.33,4.94;24,112.33,347.66,29.19,4.94" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="24,183.33,334.11,287.16,4.94">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,480.13,334.11,25.86,4.94">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,361.21,393.33,4.94;24,112.66,374.76,394.53,4.94;24,112.66,388.31,80.57,4.94" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="24,345.26,361.21,160.73,4.94;24,112.66,374.76,65.95,4.94">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,224.82,374.76,277.69,4.94">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,401.86,393.33,4.94;24,112.33,415.41,386.73,4.94" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="24,297.83,401.86,208.16,4.94;24,112.33,415.41,118.87,4.94">Weakly-Supervised Semantic Segmentation via Transformer Explainability</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Athanasiadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moschovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tuoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,254.05,415.41,156.83,4.94">ML Reproducibility Challenge 2021</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Fall Edition</note>
</biblStruct>

<biblStruct coords="24,112.66,428.96,393.33,4.94;24,112.66,442.51,296.60,4.94" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="24,209.30,428.96,296.68,4.94;24,112.66,442.51,167.92,4.94">How Deeply to Fine-Tune a Convolutional Neural Network: A Case Study Using a Histopathology Dataset</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Castelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,289.02,442.51,75.45,4.94">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,456.06,393.33,4.94;24,112.66,469.61,116.37,4.94" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="24,178.61,456.06,225.99,4.94">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,414.94,456.06,91.04,4.94;24,112.66,469.61,37.51,4.94">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,483.16,393.33,4.94;24,112.66,496.70,394.52,4.94;24,112.66,510.25,80.57,4.94" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="24,256.34,483.16,249.64,4.94;24,112.66,496.70,94.48,4.94">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>MikoÅ‚ajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grochowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,230.54,496.70,271.58,4.94">2018 International Interdisciplinary PhD Workshop (IIPhDW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,523.80,393.33,4.94;24,112.66,537.35,394.53,4.94;24,112.66,550.90,232.70,4.94" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="24,375.26,523.80,130.72,4.94;24,112.66,537.35,100.43,4.94">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,238.84,537.35,268.35,4.94;24,112.66,550.90,43.33,4.94">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,564.45,393.33,4.94;24,112.66,578.00,393.33,4.94;24,112.66,591.55,394.53,4.94;24,112.66,605.10,170.53,4.94" xml:id="b54">
	<analytic>
		<title level="a" type="main" coord="24,318.60,564.45,187.38,4.94;24,112.66,578.00,100.78,4.94">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,237.08,578.00,268.90,4.94;24,112.66,591.55,133.28,4.94">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,618.65,395.17,4.94;24,112.66,632.20,395.01,4.94;24,112.41,645.75,28.67,4.94" xml:id="b55">
	<analytic>
		<title level="a" type="main" coord="24,156.04,618.65,254.16,4.94">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,433.54,618.65,74.29,4.94;24,112.66,632.20,270.67,4.94">Text Summarization Branches Out, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,659.29,393.33,4.94;24,112.66,672.84,394.85,4.94" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="24,368.26,659.29,137.72,4.94;24,112.66,672.84,237.12,4.94">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,358.50,672.84,117.68,4.94">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,90.23,351.01,4.94" xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Biomedlm</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,103.78,394.53,4.94;25,112.66,117.33,395.17,4.94;25,112.66,130.88,393.32,4.94;25,112.66,144.43,355.61,4.94" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="25,271.78,130.88,234.21,4.94;25,112.66,144.43,71.08,4.94">Training language models to follow instructions with human feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,206.95,144.43,231.06,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,157.97,393.33,4.94;25,112.66,171.52,393.33,4.94;25,112.66,185.07,393.33,4.94;25,112.66,198.62,264.60,4.94" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="25,477.87,157.97,28.12,4.94;25,112.66,171.52,258.10,4.94">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,397.16,171.52,108.82,4.94;25,112.66,185.07,393.33,4.94;25,112.66,198.62,131.70,4.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,212.17,394.53,4.94;25,112.14,225.72,395.69,4.94;25,112.66,239.27,394.52,4.94;25,112.66,252.82,198.50,4.94" xml:id="b60">
	<analytic>
		<title level="a" type="main" coord="25,293.66,225.72,214.17,4.94;25,112.66,239.27,87.53,4.94">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>KÃ¼ttler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="25,223.18,239.27,228.69,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,266.37,395.17,4.94;25,112.66,279.92,156.41,4.94" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="25,255.31,266.37,179.93,4.94">Billion-scale similarity search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,444.73,266.37,63.11,4.94;25,112.66,279.92,77.55,4.94">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
