<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,395.85,15.42;1,89.29,106.66,220.65,15.42;1,89.29,129.00,415.59,11.96;1,89.29,142.94,63.95,11.96">Language-based Colonoscopy Image Analysis with Pretrained Neural Networks Notebook for the Medical Visual Question Answering for GI Task -MEDVQA-GI Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,168.85,88.83,11.96"><forename type="first">Patrycja</forename><surname>Cieplicka</surname></persName>
							<email>patrycja.cieplicka@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Independent Researcher</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.27,168.85,46.51,11.96"><forename type="first">Julia</forename><surname>Kłos</surname></persName>
							<email>julia.klos159@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Independent Researcher</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.93,168.85,84.36,11.96"><forename type="first">Maciej</forename><surname>Morawski</surname></persName>
							<email>mpfmorawski@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Independent Researcher</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.80,168.85,73.57,11.96"><forename type="first">Jarosław</forename><surname>Opała</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Independent Researcher</orgName>
								<address>
									<settlement>Wrocław</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,395.85,15.42;1,89.29,106.66,220.65,15.42;1,89.29,129.00,415.59,11.96;1,89.29,142.94,63.95,11.96">Language-based Colonoscopy Image Analysis with Pretrained Neural Networks Notebook for the Medical Visual Question Answering for GI Task -MEDVQA-GI Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">892D2CC251CFCA15A7C2CE2E92062B36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>colonoscopy images</term>
					<term>transfer learning</term>
					<term>Visual Question Answering</term>
					<term>Visual Question Generation</term>
					<term>Visual Location Question Answering</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, our solutions for tasks from the ImageCLEF 2023 Challenge Medical Visual Question Answering for GI Task -MEDVQA-GI [1] are presented. The aim of the Visual Question Answering (VQA) task was to generate answers based on the given colonoscopy image and corresponding questions. For this problem, a multilabel classification approach was proposed. The solution included neural network training with the utilization of pretrained encoders used to generate embeddings of image and text. The Visual Question Generation (VQG) task was to generate text questions from a given colonoscopy image and answer. To solve this task, we also applied a multilabel classifier consisting of two pretrained encoders used to create embeddings of images and text. The Visual Location Question Answering (VLQA) task was focused on generating segmentation masks covering the area of abnormality based on a given colonoscopy image and specific question. For this purpose, two separate pretrained semantic segmentation models were fine-tuned. For the VQA task, the proposed model achieved an accuracy of 0.82 on the test dataset. For the VLQA task an accuracy of 0.95, a Jaccard index of 0.67, and a Dice coefficient of 0.68 were achieved on the test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the most well-liked uses of artificial intelligence in medicine is the identification of abnormalities. The research has thus far mostly concentrated on single-image or video analysis. Through the addition of several modalities to the work, like text, we want to add a fresh perspective to the area of identification of lesions in colonoscopy images. The ImageCLEF 2023 Challenge <ref type="bibr" coords="1,136.94,546.18,12.86,10.91" target="#b1">[2]</ref> Medical Visual Question Answering for GI Task, MEDVQA-GI <ref type="bibr" coords="1,433.56,546.18,11.45,10.91" target="#b0">[1]</ref>, was focused on generating answers to questions and questions to answers about colonoscopy images. The first and second part of the task was to generate text output, while the third part of the task was to locate the abnormalities providing segmentation masks. The intention was to make medical image analysis simpler for medical professionals by combining text and visual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Visual Question Answering</head><p>In the Visual Question Answering (VQA) task, the goal was to generate accurate answers based on the given image and corresponding questions. Our proposed approach involved the creation of a model designed to handle both image and text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Review of existing solutions</head><p>Existing solutions to the VQA task have seen significant progress, as evidenced by papers such as <ref type="bibr" coords="2,89.29,263.07,11.33,10.91" target="#b2">[3]</ref>, <ref type="bibr" coords="2,107.11,263.07,11.33,10.91" target="#b3">[4]</ref>, and <ref type="bibr" coords="2,143.85,263.07,11.33,10.91" target="#b4">[5]</ref>. These approaches leverage deep learning architectures, attention mechanisms, and reasoning techniques to effectively extract visual features and address challenges in VQA. According to Marino et al. <ref type="bibr" coords="2,205.36,290.16,11.28,10.91" target="#b5">[6]</ref>, VQA can be classified into two categories. First, we can make use of symbolic knowledge, which can be represented using graphs. In this way implicit knowledge is encoded in the weights of a model trained using different datasets. The second case is supported by transformer-based language models like BERT <ref type="bibr" coords="2,360.60,330.81,11.45,10.91" target="#b6">[7]</ref>. However, further research is needed in order to progress and improve generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Dataset</head><p>For both the VQA and Visual Question Generation (VQG) tasks, the training dataset contained 2,000 images. 500 images had 19 paired questions, and 1,500 images had 18 matching questions. For each question, at least one answer was provided, sometimes multiple answers were possible for a single question. The answers could be one of the possible types: number, text, yes/no and segmentation. Segmentation-type answers and questions were skipped in VQA and VQG sub-tasks as there are part of the Visual Location Question Answering (VLQA) task.</p><p>Both VQA and VQG were interpreted as multilabel classification tasks, where multiple outputs are possible as correct answers/questions. The dataset was split into training and validation sets using an 80:20 ratio. This division allowed for effective model training on the majority of the data while reserving a smaller portion for evaluating the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Presented solution</head><p>The schema of our solution for the VQA task is shown in Figure <ref type="figure" coords="2,379.08,565.36,3.77,10.91" target="#fig_0">1</ref>. Details of the solution are presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Pretrained Transformers</head><p>Pretrained Transformers-based models were utilized in order to generate embeddings that were able to extract meaningful representations from both the image and text inputs. To achieve this, two separate pretrained transformer networks were employed: the microsoft/beit-base-patch16-224-pt22k-ft22k <ref type="bibr" coords="2,164.28,668.88,12.99,10.91" target="#b7">[8]</ref> model was used as an image encoder. This Vision Transformer model had been specifically trained to process images and generate rich embeddings that capture intricate visual features. The albert-base-v2 <ref type="bibr" coords="3,278.95,228.64,12.69,10.91" target="#b8">[9]</ref> was chosen as the text encoder. This transformerbased architecture excels at encoding textual information and producing contextualized word embeddings. The choice of the models was based on experiments considering classification quality. Models were trained for 12 epochs, applying a mini-batch size of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Concatenation</head><p>After obtaining the image and text embeddings, they were concatenated into a single representation. This step allowed us to fuse the visual and textual information, enabling the subsequent layers to effectively process and analyze the combined features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Classification Layers</head><p>The concatenated embeddings were passed to the following layers for classification:</p><p>1. Dense layer 1: The combined embeddings were passed through a dense layer consisting of 4096 neurons. Rectified Linear Unit (ReLU) activation was applied to introduce nonlinearity and enhance the network's expressive power. To prevent overfitting, a dropout rate of 0.5 was incorporated, which randomly omits connections during training. 2. Dense layer 2: Next, the outputs from the previous layer were fed into a second dense layer with 2048 neurons. Again, ReLU activation and dropout with a rate of 0.5 were applied to promote non-linearity and prevent overfitting. 3. Final dense layer: Finally, the processed embeddings were fed into a dense layer with a number of neurons equal to the total number of labels in the dataset, which in the described case was 63. This layer employed an appropriate activation function suitable for multilabel classification. 4. Loss function: In order to train the network, the binary cross entropy loss function was utilized. This loss function effectively measures the dissimilarity between predicted answers and ground-truth labels, allowing the network to optimize its parameters for accurate prediction during multi-label classification training. Where in the image is the abnormality? 0.6615</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Results</head><p>Models performance is compared with the use of accuracy metric defined as: accuracy = number of correct predictions total number of predictions . Table <ref type="table" coords="4,122.26,386.46,5.17,10.91" target="#tab_0">1</ref> presents the performance of our proposed approach. On the validation dataset, our model achieved an impressive accuracy score of 0.8386. Furthermore, on the more challenging competition final test dataset, our model demonstrated robust performance, achieving an accuracy score of 0.8193. In table <ref type="table" coords="4,136.46,440.66,3.74,10.91" target="#tab_1">2</ref>, there are presented chosen questions with corresponding accuracy scores on the final test dataset. It is interesting that the model achieves a very high accuracy on one of the questions about color while struggling with the other. These results, while showing areas for enhancement, indicate the generalization capability of our approach and its potential for real-world applications. The achieved accuracy scores highlight the advancements made in VQA research and provide a strong foundation for further improvements in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visual Question Generation</head><p>In the VQG subtask, the aim was to generate text questions from a given colonoscopy image and text answer. In VQG, most images could have many questions generated, even if a model is guided by supporting second input such as an answer. Providing answer narrows the space of expected questions. Nevertheless, the evaluation of VQG is a non-trivial task, that requires checking grammatical coherence and relevance to the given image of generated question. Sometimes, especially in the area of medicine, deep domain knowledge is required. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of existing solutions</head><p>Recently, many multimodal tasks have been in the area of interest of the research community, such as VQA, multimodal translation, and image captioning. VQG domain remains underresearched despite the great focus on both text and image processing.</p><p>The pioneering paper <ref type="bibr" coords="5,199.64,276.30,17.95,10.91" target="#b9">[10]</ref> in the field of VQG used model-generated captions and an RNNbased encoder-decoder system to create questions. Only a few studies have looked into VQG since then. There was shown how well a Generative Adversarial Network (GAN) can be used in VQG systems, enabling non-deterministic and diverse outputs <ref type="bibr" coords="5,372.72,316.95,16.41,10.91" target="#b10">[11]</ref>. The model suggested by Jain et al. <ref type="bibr" coords="5,131.63,330.50,17.75,10.91" target="#b11">[12]</ref> used a Variational Autoencoder (VAE) rather than a GAN, however, their superior results necessitate the use of a target response during inference. In order to get around this unrealistic need, in the proposed solution by Krishna <ref type="bibr" coords="5,333.75,357.60,16.41,10.91" target="#b12">[13]</ref>, answer categories were added to the VQA <ref type="bibr" coords="5,130.28,371.15,17.84,10.91" target="#b13">[14]</ref> dataset and suggested a model that does not require a response during inference. Considering that their design takes advantage of the target's information as input. More recently, Scialom et al. <ref type="bibr" coords="5,150.11,398.25,17.79,10.91" target="#b14">[15]</ref> improved a BERT ( <ref type="bibr" coords="5,254.30,398.25,11.47,10.91" target="#b6">[7]</ref>) model using model-based object attributes and actual image captions to evaluate the cross-modal performance of pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Presented solution</head><p>For the VQG task, the best results were obtained for the same encoders as for the VQA task: microsoft/beit-base-patch16-224-pt22k-ft22k as image encoder and albert-base-v2 used as text encoder. Features created by two embeddings were concatenated into a single representation. Then, they were passed to the following classification layers: two dense layers consisting of each of 8192 neurons. ReLU was applied as an activation function. The final dense layer consists of 16 neurons, which is equal to the total number of possible questions in the dataset. This layer employed an appropriate activation function suitable for multi-label classification.</p><p>Having a very limited dataset that includes only 16 unique questions, we decided to apply a multimodal classifier to solve this task. Given image i and answer a, we expect to receive one of 16 possible questions about colonoscopy. With two different inputs, our architecture makes use of two separate pretrained embeddings for text and image. Encoded data are joined to create an enormous vector of features including information about both the image and the answer. Such a vector of features is an input for fully connected layers. The schema of the proposed structure is shown in Figure <ref type="figure" coords="5,172.27,637.66,3.66,10.91" target="#fig_1">2</ref>, and the idea is identical to the one proposed for VQA, described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3 Results of VQG model chosen for submission</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Validation dataset Accuracy 0.4610</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>In Table <ref type="table" coords="6,128.83,194.76,3.81,10.91">3</ref>, the performance of our VQG approach is presented. Using the validation dataset, our model reached an accuracy of 0.4610, which indicates that this approach might not be the best option for this task and that there is a lot of room for improvement. The results for the final test dataset have not been published so far, so they will be filled in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visual Location Question Answering</head><p>The VLQA task focus on generating visual outputs in response to textual queries, specifically through the segmentation of relevant regions in the image. In this particular VLQA task, the goal was to generate segmentation masks covering the area of abnormality based on a given colonoscopy image and question defined as "Where exactly in the image is the [ABNORMALITY] located?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Review of existing solutions</head><p>VLQA task can be described as a language-based semantic segmentation that aims to generate pixel-level segmentation masks based on both image content and textual instructions (in our case, colonoscopy images and text questions). One of the first papers that introduced this problem <ref type="bibr" coords="6,335.27,438.61,17.99,10.91" target="#b15">[16]</ref> proposed an end-to-end trainable recurrent long short-term memory (LSTM) and convolutional neural network (CNN) model that jointly learns to process visual and linguistic information. They indicated that this novel task of language-based segmentation differs from traditional semantic segmentation because it is not limited to a fixed set of categories and/or rectangular regions.</p><p>As attention mechanisms <ref type="bibr" coords="6,214.23,506.35,17.82,10.91" target="#b16">[17]</ref> have been shown to be a powerful technique in deep learning models, particularly in natural language processing tasks, Gong et al. <ref type="bibr" coords="6,401.46,519.90,17.96,10.91" target="#b17">[18]</ref> proposed in 2019 a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features to segment the object referred by the language expression in the image.</p><p>In 2022, the LSeg model <ref type="bibr" coords="6,204.63,574.10,17.75,10.91" target="#b18">[19]</ref> was proposed, which uses a text encoder to compute embeddings of descriptive input labels together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. This approach is described as a zero-shot semantic segmentation method, that aims to segment unseen objects without any additional samples of novel classes.</p><p>The text-guided image manipulation task is similar to the language-based semantic segmentation task in terms of input (text + image) and output (image). Since there are approaches that are using generative models <ref type="bibr" coords="7,217.99,86.97,16.33,10.91" target="#b19">[20]</ref>, <ref type="bibr" coords="7,241.14,86.97,17.98,10.91" target="#b20">[21]</ref> to solve this task, they can be also one of the solutions for language-based semantic segmentation task.</p><p>However, if the number of labels and associated questions are known, another approach can be traditional semantic segmentation, which is one of the fundamental topics in computer vision and it aims to assign semantic labels to every pixel in an image. Most of the solutions use fully convolutional networks, that have spatial pyramid pooling module <ref type="bibr" coords="7,396.62,154.71,18.07,10.91" target="#b21">[22]</ref> or encoder-decoder structure <ref type="bibr" coords="7,131.18,168.26,16.09,10.91" target="#b22">[23]</ref>, <ref type="bibr" coords="7,153.73,168.26,16.09,10.91" target="#b23">[24]</ref>. The most known state-of-the-art solutions are Detectron2 <ref type="bibr" coords="7,430.28,168.26,16.08,10.91" target="#b24">[25]</ref>, DeepLabv3+ <ref type="bibr" coords="7,89.29,181.81,16.41,10.91" target="#b25">[26]</ref>, transformed-based model BEiT-3 <ref type="bibr" coords="7,268.27,181.81,18.07,10.91" target="#b26">[27]</ref> and the newest Meta AI model called Segment Anything (SAM) <ref type="bibr" coords="7,165.64,195.36,16.25,10.91" target="#b27">[28]</ref>.</p><p>As the number of labels and corresponding questions was known, we decided to implement a traditional semantic segmentation model for the VLQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset</head><p>The training dataset provided includes 683 questions for which the answer is segmentation mask, i.e., 500 questions about the exact location of polyps and 183 questions about the exact location of instruments.</p><p>The provided masks are binary images, which size was the same as the colonoscopy image for which the question about the exact location of abnormality was asked. The images in the training dataset varied in size, i.e., with widths from 396 to 1920 pixels, and heights from 352 to 1072 pixels.</p><p>A total of 983 colonoscopy images were used for training and validation of the segmentation models, 683 images to which the appropriate mask was assigned (500 images containing polyps, 183 images containing instruments), and 300 images that contained neither polyps nor instruments. The images were then split into a training set (90% of the images for each category) and a validation set (10% of the images for each category) to make sure that the metrics on which the best models were selected were determined on images that were not included in the training process. Additionally, during training image augmentation techniques commonly used in computer vision were applied -random cropping, random horizontal and vertical flipping, random rotations, and random adjustment of brightness.</p><p>The test dataset contained 1,949 different colonoscopy images including images where polyps and instruments can be found. The test dataset did not specify which photos contained abnormalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Presented solution</head><p>Since the goal of the task was the segmentation of two specific abnormalities, i.e., instruments and polyps, we decided to use the current state-of-the-art solution in this field -Detectron2 <ref type="bibr" coords="7,89.29,592.90,18.06,10.91" target="#b24">[25]</ref> -a platform for object detection, segmentation and other visual recognition tasks developed by Meta AI Research.</p><p>Due to the fact that the images in the dataset did not always contain masks for both abnormalities, we trained two separate segmentation models, i.e., one for localizing polyps and one for localizing instruments (Figure <ref type="figure" coords="7,243.80,647.09,3.64,10.91" target="#fig_2">3</ref>). Each model was separately trained on images assigned for its category and images without any abnormality. In addition, hyperparameters (number of epochs, learning rate, and the minimum confidence score required for an object detection called threshold) were optimized separately for both models. The best model was selected by the highest IoU value on the validation dataset. For instrument segmentation, a model trained for 1,000 epochs was chosen, with an initial learning rate of 0.00025 and a threshold of 0.8. For polyp segmentation, a model trained for 1,500 epochs was chosen, with an initial learning rate of 0.00025 and a threshold of 0.85.</p><p>To train the models, we have used transfer learning techniques instead of starting from scratch and training a model on a new dataset. By utilizing this technique, we could significantly reduce the amount of data and time required for training. This was especially important because of the small size of the provided dataset. During training, we have used a Mask R-CNN object detection model <ref type="bibr" coords="8,163.49,415.64,17.91,10.91" target="#b28">[29]</ref> pretrained on the COCO Dataset for Semantic Image Segmentation <ref type="bibr" coords="8,485.83,415.64,16.25,10.91" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>On the validation dataset, metrics were calculated separately for both models. Accuracy, mean IoU and Dice coefficient were calculated only for questions for which the ground-truth masks contained an abnormality. In addition, for each model, it was determined for what portion of the images, that did not contain abnormality, the model incorrectly detected its presence. The results of the models chosen for submission are shown in Table <ref type="table" coords="8,373.87,519.56,3.74,10.91" target="#tab_2">4</ref>.</p><p>The results on the test dataset are shown in Table <ref type="table" coords="8,317.24,533.11,3.66,10.91">5</ref>. It is worth mentioning that the following assumptions were added in the calculation of these metrics compared to the calculation of metrics on the validation dataset: if there was no abnormality in the generated mask and the ground truth mask, then all metrics were equal to 1, and if the generated mask contained an object and the ground truth mask did not, then all metrics (except accuracy) were equal to 0.</p><p>Figure <ref type="figure" coords="8,131.70,600.86,5.15,10.91" target="#fig_3">4</ref> shows example images from the test dataset and masks that were generated from the question "Where exactly in the image is the polyp located?". Example A shows the correct segmentation of a polyp. The generated mask is close to the ground-truth mask. Example B shows that the polyp was detected, but the mask is larger than it should be. Besides the polyp, the mask also includes a yellow substance. Example C, on the other hand, shows the detection of a polyp that was not marked in the ground-truth mask from the test dataset. However, it is  important to notice that the model's discovery is similar to polyp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In conclusion, this paper presented our solutions for each of the subtasks in the ImageCLEF 2023 MEDVQA-GI challenge. Our solution to the VQA task relied heavily on two crucial factors: the selection of pretrained encoders, which facilitated efficient feature extraction, and the careful tuning of the dense layer sizes and dropout rates, ensuring optimal information processing. These factors greatly influenced the performance and accuracy of the approach. The experiments showed that an inaccurate choice of encoders may notably lower the quality of the solution.</p><p>The achieved accuracy on the challenge test dataset did not vary significantly from the accuracy on the development test dataset, which shows the generalization capability of the model. The VQG system presented generates questions based on images and answers using a combination of text transformers, vision transformers, and a multi-label classifier. However, there is still room for improvement. Future work could focus on further improving the text and image encoding models and exploring different classification models. There are various approaches to VQG problems, including the use of generative adversarial networks. Although, due to the fact that in medical problems the precision of solution is crucial, we considered a predefined set of questions, thus our approach was multi-label classification model training.</p><p>In our solution to the VLQA task, we used the state-of-the-art platform in the field of object detection and segmentation, Detectron2 <ref type="bibr" coords="10,276.07,236.01,16.42,10.91" target="#b24">[25]</ref>, to tackle the task of segmenting two specific abnormalities: instruments and polyps. Due to the provided dataset, where not all images contained masks for both abnormalities, we decided to train two separate segmentation models. Future work could focus on expanding and completing the dataset. A larger dataset used for training could probably significantly improve the obtained results. Moreover, if masks of both instruments and polyps were available for all colonoscopy images, it would be possible to train and test a multiclass segmentation model. In addition, it would be worth extending the study to check the results for other models, particularly language-based semantic segmentation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,177.49,200.71,8.93;3,89.29,84.19,416.69,80.73"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schema of the proposed VQA solution.</figDesc><graphic coords="3,89.29,84.19,416.69,80.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,177.49,201.88,8.93;5,89.29,84.19,416.69,80.73"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schema of the proposed VQG solution.</figDesc><graphic coords="5,89.29,84.19,416.69,80.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,242.54,206.18,8.93;8,89.29,84.19,416.70,145.79"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schema of the proposed VLQA solution.</figDesc><graphic coords="8,89.29,84.19,416.70,145.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,495.81,416.69,8.93;9,88.99,507.82,124.71,8.87;9,89.29,254.66,416.72,234.40"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of masks generated by the model trained for polyp segmentation. All input images (A, B, C) from the test dataset.</figDesc><graphic coords="9,89.29,254.66,416.72,234.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,312.03,57.00"><head>Table 1</head><label>1</label><figDesc>Results of the VQA model chosen for submission</figDesc><table coords="4,194.26,121.19,206.76,26.30"><row><cell>Metric</cell><cell cols="2">Validation dataset Final test dataset</cell></row><row><cell>Accuracy</cell><cell>0.8386</cell><cell>0.8193</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,169.74,374.73,92.78"><head>Table 2</head><label>2</label><figDesc>Results of the VQA model chosen for submission on final test dataset -by chosen questions</figDesc><table coords="4,190.49,201.36,214.31,61.16"><row><cell>Question</cell><cell>Accuracy</cell></row><row><cell>What color is the anatomical landmark?</cell><cell>0.9990</cell></row><row><cell>What color is the abnormality?</cell><cell>0.5784</cell></row><row><cell>Where in the image is the instrument?</cell><cell>0.7585</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.97,90.49,388.43,148.21"><head>Table 4</head><label>4</label><figDesc>Results of the VLQA models chosen for submission on validation datasets</figDesc><table coords="9,88.97,121.19,388.43,117.51"><row><cell>Model</cell><cell>Accuracy</cell><cell>IoU</cell><cell>Dice</cell><cell cols="3">Unexpected abnormality detection</cell></row><row><cell>INSTRUMENT</cell><cell>0.9911</cell><cell cols="2">0.8766 0.9320</cell><cell cols="3">0% of images without abnormalities</cell></row><row><cell>POLYP</cell><cell>0.9427</cell><cell cols="5">0.7252 0.8022 6.667% of images without abnormalities</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">VLQA submission results on test dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Metrics Accuracy Jaccard F1-score Recall Precision</cell><cell>Dice</cell></row><row><cell>Results</cell><cell>0.9521</cell><cell>0.6660</cell><cell>0.6769</cell><cell>0.6810</cell><cell>0.6796</cell><cell>0.6769</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to express our sincere gratitude to the organizers of Medical Visual Question Answering for GI Task for their support and assistance throughout the research and publication process. In particular, to <rs type="person">Steven A. Hicks</rs> and <rs type="person">Michael A. Riegler</rs> for their guidance and prompt responses to our inquiries.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The source code is available via GitHub:</p><p>• https://github.com/paatrycjaa/ImageCLEF2023-MEDVQA-GI-VisionQAries,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,488.75,393.33,10.91;10,112.66,502.30,394.52,10.91;10,112.66,515.85,394.52,10.91;10,112.66,529.40,58.60,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,452.40,488.75,53.59,10.91;10,112.66,502.30,390.27,10.91">Overview of ImageCLEFmedical 2023 -Medical Visual Question Answering for Gastrointestinal Tract</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.16,515.85,111.10,10.91">CLEF2023 Working Notes</title>
		<title level="s" coord="10,245.61,515.85,173.62,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,542.95,394.53,10.91;10,112.66,556.50,395.17,10.91;10,112.66,570.05,394.53,10.91;10,112.66,583.60,395.17,10.91;10,112.39,597.15,394.80,10.91;10,112.48,610.69,394.70,10.91;10,112.66,624.24,395.17,10.91;10,112.66,637.79,393.32,10.91;10,112.66,651.34,394.52,10.91;10,112.33,664.89,120.27,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,224.93,610.69,282.25,10.91;10,112.66,624.24,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.94,624.24,143.89,10.91;10,112.66,637.79,393.32,10.91;10,112.66,651.34,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,280.09,651.34,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,180.86,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,259.60,86.97,246.39,10.91;11,112.66,100.52,43.99,10.91">attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Show</surname></persName>
		</author>
		<idno>ArXiv abs/1704.03162</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,394.53,10.91;11,112.66,127.61,122.77,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,219.55,114.06,282.76,10.91">Dual Attention Networks for Multimodal Reasoning and Matching</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.33,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,393.33,10.91;11,112.66,181.81,284.28,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,414.89,141.16,91.09,10.91;11,112.66,154.71,303.31,10.91">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,439.62,154.71,66.36,10.91;11,112.66,168.26,334.13,10.91">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,393.33,10.91;11,112.66,208.91,393.59,10.91;11,112.66,222.46,394.92,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,381.14,195.36,124.85,10.91;11,112.66,208.91,300.83,10.91">KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,438.21,208.91,68.04,10.91;11,112.66,222.46,286.80,10.91">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14106" to="14116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,236.01,393.33,10.91;11,112.33,249.56,318.51,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,320.46,236.01,185.52,10.91;11,112.33,249.56,188.13,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,263.11,395.01,10.91;11,112.66,279.10,97.35,7.90" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,258.58,263.11,218.22,10.91">BEiT: BERT Pre-Training of Image Transformers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.53,10.91;11,112.66,303.75,370.27,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,392.55,290.20,113.64,10.91;11,112.66,303.75,240.16,10.91">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,393.33,10.91;11,112.66,330.85,172.55,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,310.78,317.30,195.21,10.91;11,112.66,330.85,42.35,10.91">Automatic Generation of Grounded Visual Questions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06530</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.33,10.91;11,112.66,357.95,393.32,10.91;11,112.66,371.50,394.53,10.91;11,112.66,385.05,219.21,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,289.89,344.40,216.10,10.91;11,112.66,357.95,201.56,10.91">A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,337.51,357.95,168.47,10.91;11,112.66,371.50,390.21,10.91">Proceedings of the 27th International Conference on Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics, Association for Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1763" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,398.60,393.33,10.91;11,112.28,412.15,189.66,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,246.93,398.60,259.06,10.91;11,112.28,412.15,59.23,10.91">Creativity: Generating Diverse Questions using Variational Autoencoders</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03493</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,394.52,10.91;11,112.66,439.25,122.77,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,271.07,425.70,231.39,10.91">Information Maximizing Visual Question Generation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11207</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,452.79,393.33,10.91;11,112.66,466.34,220.09,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m" coord="11,449.72,452.79,56.27,10.91;11,112.66,466.34,89.45,10.91">VQA: Visual Question Answering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,479.89,393.32,10.91;11,112.33,493.44,307.94,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="11,366.34,479.89,139.65,10.91;11,112.33,493.44,177.68,10.91">What BERT Sees: Cross-Modal Transfer for Visual Question Generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-A</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10832</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,506.99,394.62,10.91;11,112.66,520.54,394.52,10.91;11,112.41,534.09,17.62,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,258.98,506.99,226.86,10.91">Segmentation from Natural Language Expressions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,520.54,136.81,10.91">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,394.53,10.91;11,112.66,561.19,393.33,10.91;11,112.66,574.74,222.96,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,177.31,561.19,111.34,10.91">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,312.27,561.19,193.72,10.91;11,112.66,574.74,33.92,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,395.17,10.91;11,111.80,601.84,394.18,10.91;11,112.66,615.39,394.53,10.91;11,112.66,628.93,168.00,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,285.82,588.29,222.01,10.91;11,111.80,601.84,209.33,10.91">Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,343.83,601.84,162.15,10.91;11,112.66,615.39,210.97,10.91">Proceedings of the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21</title>
		<meeting>the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="456" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,642.48,393.33,10.91;11,112.66,656.03,352.76,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,383.65,642.48,122.33,10.91;11,112.66,656.03,58.78,10.91">Language-driven Semantic Segmentation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,194.89,656.03,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,393.33,10.91;12,112.66,86.97,384.51,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,327.17,669.58,178.82,10.91;12,112.66,86.97,222.19,10.91">Text-guided style transfer-based image manipulation using multimodal generative models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Togo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Haseyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,343.65,86.97,54.37,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="64860" to="64870" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,100.52,393.33,10.91;12,112.66,114.06,395.17,10.91;12,112.66,127.61,394.52,10.91;12,112.66,141.16,85.06,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,366.29,100.52,139.69,10.91;12,112.66,114.06,145.02,10.91">Text as Neural Operator:Image Manipulation by Text Instruction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,280.39,114.06,227.43,10.91;12,112.66,127.61,124.70,10.91">Proceedings of the 29th ACM International Conference on Multimedia, MM &apos;21</title>
		<meeting>the 29th ACM International Conference on Multimedia, MM &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,154.71,393.53,10.91;12,112.30,168.26,394.89,10.91;12,112.66,181.81,112.22,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,244.22,154.71,261.97,10.91;12,112.30,168.26,80.98,10.91">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.25,168.26,134.22,10.91">Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,393.33,10.91;12,112.66,208.91,394.53,10.91;12,112.66,222.46,332.00,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,286.16,195.36,219.83,10.91;12,112.66,208.91,86.69,10.91">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,221.89,208.91,285.30,10.91;12,112.66,222.46,58.19,10.91">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,236.01,393.53,10.91;12,112.28,249.56,393.70,10.91;12,112.66,263.11,146.06,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,294.78,236.01,211.41,10.91;12,112.28,249.56,159.25,10.91">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,279.76,249.56,226.22,10.91;12,112.66,263.11,51.98,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,276.66,394.04,10.91;12,112.66,290.20,156.24,10.91" xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
	</analytic>
	<monogr>
		<title level="j" coord="12,361.15,276.66,48.12,10.91">Detectron</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,303.75,393.33,10.91;12,112.66,317.30,393.68,10.91;12,112.66,330.85,291.96,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,369.17,303.75,136.82,10.91;12,112.66,317.30,254.02,10.91">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,390.32,317.30,116.02,10.91;12,112.66,330.85,18.15,10.91">Computer Vision -ECCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,344.40,394.53,10.91;12,112.66,357.95,393.32,10.91;12,112.66,371.50,150.35,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="12,229.04,357.95,276.94,10.91;12,112.66,371.50,120.35,10.91">Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,385.05,394.53,10.91;12,112.28,398.60,389.31,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02643</idno>
		<title level="m" coord="12,304.14,398.60,81.34,10.91">Segment Anything</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,412.15,395.01,10.91;12,112.66,425.70,264.57,10.91" xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1703.06870" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,439.25,394.53,10.91;12,112.66,452.79,393.33,10.91;12,112.33,466.34,285.48,10.91" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="12,215.48,452.79,196.77,10.91">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll'a R</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
