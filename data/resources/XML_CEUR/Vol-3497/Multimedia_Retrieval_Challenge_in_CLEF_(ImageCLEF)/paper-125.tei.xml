<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,75.47,448.29,16.98;1,72.00,96.23,437.12,16.98;1,72.00,117.01,247.77,16.98;1,72.00,147.28,236.24,10.80">Concept Detection and Caption Prediction in ImageCLEFmedical Caption 2023 with Convolutional Neural Networks, Vision and Text-to-Text Transfer Transformers Notebook for the CS_Morgan Lab at CLEF 2023</title>
				<funder ref="#_Wg85jC7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_PtBASaw #_MpeUqdx">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,173.74,93.67,10.80"><forename type="first">Md</forename><forename type="middle">Rakibul</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Medical</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.70,173.74,76.31,10.80"><forename type="first">Oyebisi</forename><surname>Layode</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Medical</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,284.36,173.74,115.98,10.80"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Medical</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,75.47,448.29,16.98;1,72.00,96.23,437.12,16.98;1,72.00,117.01,247.77,16.98;1,72.00,147.28,236.24,10.80">Concept Detection and Caption Prediction in ImageCLEFmedical Caption 2023 with Convolutional Neural Networks, Vision and Text-to-Text Transfer Transformers Notebook for the CS_Morgan Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7FCF3978DA80C4A583581461E5B2A70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Imaging</term>
					<term>Image Annotation</term>
					<term>Caption Prediction</term>
					<term>Concept Detection</term>
					<term>Multi-label Classification</term>
					<term>Deep Learning</term>
					<term>Natural Language Processing</term>
					<term>GPT2</term>
					<term>T5</term>
					<term>MRCNN</term>
					<term>ViT</term>
					<term>Ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work discusses the participation of CS_Morgan in the Concept Detection and Caption Prediction tasks of the ImageCLEFmedical 2023 Caption benchmark evaluation campaign. The goal of this task is to automatically identify relevant concepts and their locations in images, as well as generate coherent captions for the images. The dataset used for this task is a subset of the extended Radiology Objects in Context (ROCO) dataset. The implementation approach employed by us involved the use of pre-trained Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Text-to-Text Transfer Transformer (T5) architectures. These models were leveraged to handle the different aspects of the tasks, such as concept detection and caption generation. In the Concept Detection task, the objective was to classify multiple concepts associated with each image. We utilized several deep learning architectures with 'sigmoid' activation to enable multilabel classification using the Keras framework. We submitted a total of five ( <ref type="formula" coords="1,235.22,390.37,4.08,9.45">5</ref>) runs for this task, and the best run achieved an F1 score of 0.4834, indicating its effectiveness in detecting relevant concepts in the images. For the Caption Prediction task, we successfully submitted eight (8) runs. Our approach involved combining the ViT and T5 models to generate captions for the images. For the caption prediction task, the ranking is based on the BERTScore, and our best run achieved a score of 0.5819 based on generating captions using the fine-tuned T5 model from keywords generated using the pretrained ViT as the encoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.02" lry="841.98"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generating accurate captions and concepts for biomedical images through automated processes poses a significant challenge in the field of artificial intelligence (AI). Successfully addressing this challenge necessitates the utilization of techniques from Computer Vision to comprehend the underlying concepts within the image, as well as the ability to grasp the relationships that exist among these concepts <ref type="bibr" coords="1,139.22,633.76,8.25,9.88" target="#b3">[1]</ref><ref type="bibr" coords="1,151.60,633.76,8.25,9.88" target="#b4">[2]</ref>. Additionally, techniques from natural language processing (NLP) are crucial for generating descriptive text that effectively represents the content of the image.</p><p>Over the past decade, remarkable advancements have been achieved in leveraging Deep Learning methodologies to automatically generate clinical reports and captions based on medical images. The primary objective of these advancements is to support healthcare professionals in making precise decisions and enhance the efficiency of the diagnostic process <ref type="bibr" coords="2,347.80,112.68,11.69,9.88" target="#b5">[3]</ref>.</p><p>By employing Computer Vision techniques, AI systems can interpret the complex visual elements and structures within biomedical images, enabling them to identify and understand the key concepts and features depicted. This involves tasks such as object recognition, segmentation, and image classification <ref type="bibr" coords="2,132.80,163.34,8.25,9.88" target="#b3">[1]</ref><ref type="bibr" coords="2,141.06,163.34,4.13,9.88" target="#b4">[2]</ref><ref type="bibr" coords="2,145.18,163.34,8.25,9.88" target="#b5">[3]</ref>.</p><p>Moreover, the AI system must possess the capability to establish connections and associations among the identified concepts within the image. This contextual understanding is vital for generating coherent and meaningful descriptions <ref type="bibr" coords="2,244.52,201.26,8.25,9.88" target="#b6">[4]</ref><ref type="bibr" coords="2,256.90,201.26,8.25,9.88" target="#b7">[5]</ref>. Natural language processing techniques come into play here, as they facilitate the conversion of the interpreted concepts into human-readable text <ref type="bibr" coords="2,475.66,213.92,8.25,9.88" target="#b6">[4]</ref><ref type="bibr" coords="2,488.04,213.92,8.25,9.88" target="#b7">[5]</ref>. This process involves tasks such as language modeling, semantic analysis, and text generation <ref type="bibr" coords="2,466.72,226.58,8.24,9.88" target="#b6">[4]</ref><ref type="bibr" coords="2,479.07,226.58,8.24,9.88" target="#b7">[5]</ref>.</p><p>The integration of these techniques from both Computer Vision and NLP has proven instrumental in automating the generation of captions and concepts for biomedical images <ref type="bibr" coords="2,414.88,251.84,8.25,9.88" target="#b6">[4]</ref><ref type="bibr" coords="2,427.26,251.84,8.25,9.88" target="#b7">[5]</ref>. This advancement in AI technology provides valuable support to clinicians by assisting them in making accurate and informed decisions in a timely manner <ref type="bibr" coords="2,245.06,277.16,11.69,9.88" target="#b7">[5]</ref>. By reducing the time required for diagnosis and enhancing the comprehensiveness of medical reports, these AI systems contribute to improving patient care and overall healthcare outcomes <ref type="bibr" coords="2,198.14,302.42,11.69,9.88" target="#b7">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset and Task Overview</head><p>The ImageCLEFmedical 2023 caption dataset <ref type="bibr" coords="2,295.34,360.20,8.84,9.88" target="#b8">[6]</ref><ref type="bibr" coords="2,308.60,360.20,8.84,9.88" target="#b9">[7]</ref> was divided into the training, validation, and test set. The training set contains 60,918 radiology images, the validation set contains 10,437 radiology images while the test set contains 10,473 radiology images. For the training and validation set, every image had a corresponding textual description (caption) that was provided.</p><p>The Concept Detection Task is an important step in achieving automatic image captioning and scene comprehension in the field of medical images <ref type="bibr" coords="2,273.47,423.46,11.68,9.88" target="#b8">[6]</ref>. Its purpose is to identify and locate relevant concepts within a large collection of medical images. By analyzing the visual content of the images, this subtask lays the groundwork for understanding the various elements that make up captions. Furthermore, these identified concepts can be utilized for context-based image retrieval and information retrieval. The evaluation of this task is performed using set coverage metrics, including precision, recall, and their combinations. These metrics measure the extent to which the set of concepts detected by the system aligns with the ground truth. In the Concept Detection Task, a subset of the UMLS 2022AB release was employed to generate the concepts. UMLS offers a range of tools and resources that empower users to access, comprehend, and navigate diverse health-related terminologies, including medical codes, classifications, ontologies, and databases. Within the biomedical domain, the UMLS employs a Concept Unique Identifier (CUI) as a specific identifier to represent distinct concepts. Each concept present in UMLS is assigned a CUI, which ensures a standardized and consistent means to reference and establish connections among related information across various terminologies and resources within the UMLS framework. ImageCLEF medical 2023 training dataset <ref type="bibr" coords="2,328.95,587.86,8.86,9.88" target="#b8">[6]</ref><ref type="bibr" coords="2,342.24,587.86,8.86,9.88" target="#b9">[7]</ref> has 227,156 labels/concepts and the validation dataset combinedly has 40,042 labels/concepts. Among these, 2,125 unique concept identifiers are used once or multiple times, because each image is labeled by one/multiple CUIs. Figure <ref type="figure" coords="2,72.00,625.84,5.49,9.88" target="#fig_0">1</ref> illustrates the frequency of the most CUIs (both from train and valid dataset) and their corresponding medical terms. On the other hand, when making a medical inference or captions on medical images, it is not uncouth to identify key parts of the image that can be combined to make a meaningful deduction on its possible contents <ref type="bibr" coords="3,112.82,383.18,8.24,9.88" target="#b8">[6]</ref><ref type="bibr" coords="3,125.17,383.18,8.24,9.88" target="#b9">[7]</ref>. This approach to deducing meaning from images draws some similarities with how visual transformers process image data. In recent times, transformers, which are based on the attention mechanism <ref type="bibr" coords="3,124.72,408.52,11.76,9.88" target="#b10">[8]</ref>, were primarily utilized in sequence modeling and machine translation tasks. However, Transformers have gradually emerged as the predominant deep learning models for many NLP tasks <ref type="bibr" coords="3,72.00,433.78,11.69,9.88" target="#b11">[9]</ref>. Visual transformers in a similar manner to the application of Transformers in NLP tasks place importance on certain parts of the image features when making inference on an image. This is different from legacy convolutional neural networks (CNN) models that make inference on an image based on the entire image feature space. The mechanism of action of visual transformers suit image captioning tasks better than CNN models since the words in the caption typically only describe a part of the image. This paper involves the annotation of medical images to generate textual descriptions (captions) using the dataset under the ImageCLEFmed 2023 caption prediction task <ref type="bibr" coords="3,371.02,509.68,8.33,9.88" target="#b8">[6]</ref><ref type="bibr" coords="3,383.52,509.68,8.33,9.88" target="#b9">[7]</ref>, which is a subset of a larger Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="3,282.12,522.34,16.90,9.88" target="#b12">[10]</ref>. Our last participation involved the use of a visual transformer that consisted of an encoder-decoder architecture where meaningful image representations were generated from the encoder region, attention was then subsequently placed on these image representations to iteratively generate a sequence of words in the decoder. This year's participation builds on the previous work by replacing the trained decoder with a fine-tuned pretrained language model. The encoder was also replaced with pretrained models so the millions of mappings learned from the pretrained models can be leveraged for better results. This year, the evaluation criteria for the results obtained is given as the BERTScore between the generated captions and the ground truth captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We approached the Image Captioning and Concept detection tasks as two separate problems. The methods used in our participation are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Medical Concept Detection Task</head><p>The task of concept detection is often viewed as a multi-label classification problem. This means that it involves predicting one or more concept labels for each instance of an image. In other words, an image can be associated with multiple classes or concepts simultaneously, and these concepts are not mutually exclusive. The goal is to identify and assign the relevant concept labels to the image accurately, allowing for the possibility that an image may be associated with zero or more concepts. The approach implemented here for the concept detection task can be illustrated by addressing the following steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data Preparation</head><p>The dataset used for caption prediction tasks consists of radiology images along with their corresponding labels or annotations. You can refer to Figure <ref type="figure" coords="4,343.34,195.74,5.49,9.88" target="#fig_2">3</ref> and<ref type="figure" coords="4,370.92,195.74,5.49,9.88" target="#fig_3">4</ref> for visual representations of this dataset. During the data preparation phase, both the images and labels undergo preprocessing. For all the submission runs, the image preprocessing techniques involve reshaping, rotation (at 45 or 90 degrees), and flipping (horizontally and vertically) augmentation techniques. Reshaping is performed to obtain two different image shapes, namely (224 x 224 x 3) and (331 x 331 x 3), as required by the pretrained Convolutional Neural Network (CNN) models used for the multi-label prediction task. This allows the images to be compatible with the CNN models and ensures consistent input dimensions for the network. As for the labels or annotations, they are transformed into a vectorized format using the Multi-label Binarizer method from the scikit-learn library. This method represents the multiple labels or Clinical Unique Identifiers (CUIs) as binary vectors. In this dataset, there are a total of 2,125 unique CUIs used in both the training and validation sets.</p><p>The steps of reshaping the images to match the requirements of the CNN models and converting the multi-label annotations into a binary representation using the Multi-label Binarizer method from scikitlearn ensure that the dataset is appropriately formatted and ready for use in the caption prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Model Preparation</head><p>To tackle the task of concept detection, we approached it by submitting five different attempts. For the first two runs, we utilized two pre-trained convolutional neural networks (CNNs) such as DenseNet169 <ref type="bibr" coords="4,136.29,443.26,18.41,9.88" target="#b13">[11]</ref> and DenseNet121 <ref type="bibr" coords="4,243.55,443.26,16.89,9.88" target="#b13">[11]</ref>, which were trained on datasets like ImageNet <ref type="bibr" coords="4,484.26,443.26,18.45,9.88" target="#b14">[12]</ref> and CheXNet <ref type="bibr" coords="4,117.34,455.92,16.92,9.88" target="#b15">[13]</ref>, respectively. On the fourth run we employed the Vision Transformer (ViT) <ref type="bibr" coords="4,487.71,455.92,18.67,9.88" target="#b16">[14]</ref> for image feature extraction followed by the k-nearest neighbors (kNN) algorithm to find the most similar training/validation images for a test image.</p><p>Furthermore, we employed an ensemble method based on the weighted average technique, using the ConvNeXtLarge <ref type="bibr" coords="4,150.79,506.50,18.42,9.88" target="#b17">[15]</ref> and NasNetLarge <ref type="bibr" coords="4,260.24,506.50,18.42,9.88" target="#b18">[16]</ref> CNN architectures on the third run. This involved combining the predictions from these models in a weighted manner. Additionally, we applied a majority voting technique-based ensemble method on several ImageNet weight based pre-trained CNNs including ResNet50 <ref type="bibr" coords="4,162.37,544.42,16.94,9.88" target="#b19">[17]</ref>, Xception <ref type="bibr" coords="4,229.31,544.42,16.91,9.88">[18]</ref>, VGG16 <ref type="bibr" coords="4,290.69,544.42,16.90,9.88" target="#b21">[19]</ref>, and InceptionResNetV2 <ref type="bibr" coords="4,423.19,544.42,18.44,9.88" target="#b22">[20]</ref> on our fifth run.</p><p>For a more comprehensive understanding of these submission runs, you can refer to Section 4.1, which provides further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Training and Prediction</head><p>We adopted the Keras framework to carry out the training and prediction processes. All the models mentioned above were constructed by initializing the Adam optimizer and compiled using binary crossentropy. This choice was made instead of using categorical cross-entropy to treat each output label as an independent Bernoulli distribution, allowing for the possibility that the labels are not mutually exclusive.</p><p>Once the training is completed, both the models and label binarizers are saved to a disk, specifically a cloud storage system, for future use. During the prediction phase on the test set, these saved models and binarizers are loaded to facilitate the prediction process.</p><p>To ensure efficient computation and handle the high memory requirements of the training process, we acquired a high-performance computing infrastructure equipped with four NVIDIA® T4 GPU drivers (4-units) from a cloud service provider. These resources were utilized in parallel using the TensorFlow 2.11 mirrored strategy, which allows for distributed training across multiple GPUs.</p><p>Furthermore, the training procedures were conducted on the VertexAI workbench, a component of the Google Cloud platform specifically designed for machine learning tasks. This choice of platform provided a convenient and robust environment for performing the necessary computations and managing the overall training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Medical Image Captioning Task</head><p>The methods employed in the image captioning task majorly involved the use of pretrained models. The first approach involved training a Stochastic Gradient Descent (SGD) <ref type="bibr" coords="5,395.75,208.40,18.53,9.88" target="#b23">[21]</ref> classifier that will return the most similar captions for a query image. The SGD classifier was trained on bottleneck features obtained from two types of pretrained models, Mask Region with Convolutional Neural Networks (MRCNN) <ref type="bibr" coords="5,123.04,246.32,18.40,9.88" target="#b24">[22]</ref> pretrained on the MSCOCO dataset <ref type="bibr" coords="5,307.89,246.32,18.54,9.88" target="#b25">[23]</ref> and Google Vision Transformer (ViT) <ref type="bibr" coords="5,504.51,246.32,18.51,9.88" target="#b16">[14]</ref> trained on ImageNet. The second approach was divided into two stages with the first stage involving training a transformer model to sequentially generate keywords based on an image input. The MRCNN <ref type="bibr" coords="5,72.00,284.30,18.36,9.88" target="#b24">[22]</ref> and ViT <ref type="bibr" coords="5,133.77,284.30,18.41,9.88" target="#b16">[14]</ref> pretrained model acted as the encoder part of the transformer while a decoder was trained to sequentially generate keywords based on the image embeddings and the previously seen keywords. The second stage involved fine-tuning different types of pretrained language models to generate meaningful captions from the keywords generated from the first stage. Generative Pretrained Transformer 2 (GPT2) <ref type="bibr" coords="5,174.30,334.88,18.38,9.88" target="#b26">[24]</ref> and Google T5 <ref type="bibr" coords="5,264.13,334.88,18.42,9.88" target="#b27">[25]</ref> were fine-tuned for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Text Preprocessing (Generating Keywords from Captions)</head><p>The ImageCLEFmedical 2023 caption training dataset <ref type="bibr" coords="5,338.29,392.62,8.88,9.88" target="#b8">[6]</ref><ref type="bibr" coords="5,351.61,392.62,8.88,9.88" target="#b9">[7]</ref> has a corpus of 23,237 words. An analysis of the distribution of these words shows that only about 295 words occur in more than 500 captions with there being 19,877 words that occur in less than 20 captions out of the entire 60,918 image captions. The keywords were selected as the 500 topmost occurring words excluding English stop words like "and", "when", "if", "a", "the" etc. (See Table <ref type="table" coords="5,297.90,443.26,5.49,9.88" target="#tab_0">1</ref> for some instances). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Image Feature Encoding</head><p>We chose MRCNN <ref type="bibr" coords="5,182.80,654.72,18.46,9.88" target="#b24">[22]</ref> and ViT <ref type="bibr" coords="5,250.84,654.72,18.40,9.88" target="#b16">[14]</ref> as our feature encoders to obtain region/object-based encodings of the images.</p><p>The MRCNN <ref type="bibr" coords="5,148.72,679.98,18.40,9.88" target="#b24">[22]</ref> architecture was developed as an object instance segmentation model that adopts a two-stage approach which includes a first stage that proposes candidate bounding boxes for detected objects (Region Proposal Networks, RPN) and a second stage that predicts a class, bounding box offset, and a binary mask for each region of interest (ROI). The region proposal features just before the classification layer was adopted as the image encodings. These features were obtained from a version of the MRCNN <ref type="bibr" coords="5,145.80,743.28,18.44,9.88" target="#b24">[22]</ref> architecture pretrained on the MSCOCO dataset <ref type="bibr" coords="5,387.50,743.28,17.04,9.88" target="#b25">[23]</ref>. The encoded 16 x 1024dimension image embeddings are subsequently passed into other parts of the image captioning model.</p><p>The ViT <ref type="bibr" coords="6,128.06,74.76,18.41,9.88" target="#b16">[14]</ref> architecture converts 16x16 patches of an image input into flattened linear projects that are combined with the positional embeddings and passed as a sequence of patch embeddings to an encoder. A classification multi-layer perceptron is placed on top of the encoder to map the encoded embeddings to predicted classes. 1,024-dimension features were obtained from the pooling layer just before the classification layer of the pre-trained ViT <ref type="bibr" coords="6,311.01,125.36,18.43,9.88" target="#b16">[14]</ref> model. These features represent the image embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">SGD classifier + pre-trained features</head><p>The SGD classifier was trained using the scikit's library <ref type="bibr" coords="6,337.23,195.63,17.08,9.98" target="#b28">[26]</ref>. This involved training a Multioutput Classifier using an SGD classifier wrapped in a OneVsRest Classifier as the estimator. The classifier was also trained with scikit's adaptive learning rate option alongside the default options for the classifier. The same training parameters were applied to train the classifier on the MRCNN <ref type="bibr" coords="6,485.14,233.72,18.65,9.88" target="#b24">[22]</ref> and ViT <ref type="bibr" coords="6,91.98,246.32,18.37,9.88" target="#b16">[14]</ref> features. The results were generated from the classifier by obtaining the encoded features from the pretrained MRCNN <ref type="bibr" coords="6,179.33,258.98,18.45,9.88" target="#b24">[22]</ref> and ViT <ref type="bibr" coords="6,240.47,258.98,18.33,9.88" target="#b16">[14]</ref> models and querying the features on the classifier to return a caption that is most similar based on the query feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Keyword Transformer Model (KTM)</head><p>The transformer model used in this task consists of an encoder and decoder model that receives a 224 x 224 image alongside a tokenized text input padded up to a maximum length of 20 tokens including a start and end token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.1.">Encoder-Decoder</head><p>The encoder model consists of a pretrained model that receives a 224 x 224 x 3 image input and gives bottleneck feature output of 16 x 1,024 for the MRCNN <ref type="bibr" coords="6,351.71,425.08,18.46,9.88" target="#b24">[22]</ref> model and 1,024 for the ViT <ref type="bibr" coords="6,504.57,425.08,18.45,9.88" target="#b16">[14]</ref> model.</p><p>The decoder model learns to map the image embedding output of the encoder model to text embeddings. The text embeddings are an addition of the text tokens (which are the indexes of the caption word in the corpus) passed to a Keras text embedding layer and the positional encodings of the words in the generated captions. The decoder auto-regressively tracks the positions of the predicted words as the encoder output is passed as the keys and value to the decoder while the prior decoder predictions are passed as the decoder query for which a next word is predicted.</p><p>The decoder architecture consists of an input layer (receives the encoder and text tokens) =&gt; the text tokens are passed to a text embedding layer (text embeddings + positional encodings) =&gt; the result of the text embedding layer is passed to a dropout layer =&gt; the result of the dropout layer and the image encoding output is passed to a decoder layer which consists of a causal self-attention followed by a cross attention layer, the text encodings are passed to the self-attention layer learn the relations between the words. The result of the self-attention is then passed to the cross-attention layer alongside the encoder output =&gt; the result of the cross-attention layer is then passed to a feed forward layer =&gt; the output of the decoder layer is passed to a dense layer with the number of neurons equivalent to the vocabulary size of the corpus. Figures <ref type="figure" coords="6,242.24,627.46,5.49,9.88" target="#fig_1">2</ref> and<ref type="figure" coords="6,269.11,627.46,5.49,9.88" target="#fig_2">3</ref> illustrate the corresponding decoder architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.2.">Training</head><p>All the images from the ImageCLEFmedical 2023 caption dataset <ref type="bibr" coords="7,383.66,335.90,8.91,9.88" target="#b8">[6]</ref><ref type="bibr" coords="7,397.03,335.90,8.91,9.88" target="#b9">[7]</ref> were resized to 224 x 224 x 3 and normalized by dividing the pixel values by 255. The KTM was trained with an Adam optimizer <ref type="bibr" coords="7,72.00,361.16,18.36,9.88" target="#b29">[27]</ref> with a masked version of the sparse categorical entropy loss, a learning rate set at 0.001 and a batch size of 256. The decoder model was trained using a 256-feature projection dimension and a feed forward dimension of 256. The decoder was also trained on 8 attention heads and a stack of 16 decoder layers. The training was set for 200 epochs with early stopping applied and the last best model restored after a patience of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Keyword to Caption Model (KTC)</head><p>The KTC model involved fine-tuning pretrained language models to generate meaningful caption sentences from keywords. GPT2 <ref type="bibr" coords="7,222.33,482.20,18.41,9.88" target="#b26">[24]</ref> and T5 <ref type="bibr" coords="7,280.06,482.20,18.34,9.88" target="#b27">[25]</ref> were fine-tuned on the ImageCLEFmedical 2023 caption dataset <ref type="bibr" coords="7,141.25,494.86,8.28,9.88" target="#b8">[6]</ref><ref type="bibr" coords="7,153.67,494.86,8.28,9.88" target="#b9">[7]</ref>. GPT-2 <ref type="bibr" coords="7,202.45,494.86,18.37,9.88" target="#b26">[24]</ref> is a transformers model that was pre-trained on a very large corpus of English data in a self-supervised manner <ref type="bibr" coords="7,273.07,507.46,16.94,9.88" target="#b30">[28]</ref>. The model largely follows after the OpenAI GPT model architecture <ref type="bibr" coords="7,157.81,520.12,18.43,9.88" target="#b31">[29]</ref> with few modifications including the layer normalization being moved to the input of each sub-block, an additional layer normalization being added after the final self-attention block, scaling the weights of residual layers at initialization by a factor of 1/ √ N where N is the number of residual layers, an expansion of the vocabulary to 50,257, an increase in the context size from 512 to 1,024 tokens <ref type="bibr" coords="7,133.82,570.76,16.93,9.88" target="#b26">[24]</ref>. The T5 <ref type="bibr" coords="7,196.84,570.76,18.40,9.88" target="#b27">[25]</ref> architecture follows the general transformer approach as originally proposed <ref type="bibr" coords="7,115.39,583.36,11.69,9.88" target="#b10">[8]</ref>. This involves mapping an input sequence of tokens to a sequence of embeddings that is subsequently passed into the encoder, with the encoder consisting of a stack of "blocks", each of which comprises a self-attention layer and a small feed-forward network. Layer normalization <ref type="bibr" coords="7,459.37,608.68,18.50,9.88" target="#b32">[30]</ref> is applied to the input of each subcomponent while a residual skip connection is used to add each subcomponent's input to its output. Dropout <ref type="bibr" coords="7,193.89,634.00,18.47,9.88" target="#b33">[31]</ref> is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The T5 <ref type="bibr" coords="7,408.38,646.60,18.61,9.88" target="#b27">[25]</ref> decoder has a similar structure to the encoder except that on top of the self-attention layer that attends to the output of the encoder <ref type="bibr" coords="7,109.54,671.94,16.92,9.88" target="#b27">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.1.">Fine-Tuning</head><p>The keywords were passed to the models as input while the captions were passed as the expected output. Both the input and output data were tokenized and padded, the input data was padded up to a maximum length of 20, while the output data was padded up to a maximum length of 50. GPT2 <ref type="bibr" coords="7,504.49,754.98,18.65,9.88" target="#b26">[24]</ref> was fine-tuned for 25 epochs with an Adam weight decay optimizer <ref type="bibr" coords="8,372.95,74.76,16.92,9.88" target="#b29">[27]</ref>, a learning rate of 0.0005 and a weight decay rate of 0.01. T5 <ref type="bibr" coords="8,208.47,87.42,18.41,9.88" target="#b27">[25]</ref> was fine-tuned for 41 epochs with a Adafactor optimizer, a learning rate of 0.0001, decay rate of 0.8 and a clip threshold of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Description of Runs and Results</head><p>We submitted 13 runs in total for this year's participation, 8 runs were submitted for the caption prediction task while 5 runs were submitted for the concept detection task as shown in Table <ref type="table" coords="8,486.83,170.48,5.49,9.88" target="#tab_1">2</ref> and<ref type="table" coords="8,514.34,170.48,4.12,9.88" target="#tab_4">3</ref>. For Concept Detection, the primary evaluation metric was the F1 score, we obtained a best score of 0.4834 for Run_2 and ranked 6th out of 9 participants. For the Caption Prediction task, the primary evaluation metric is the BERTScore, additional evaluation metrics used include ROUGE, METEOR, CIDEr, the BLEU score. The best score recorded was for Run_10 with a BERTScore of 0.2549. The best scores are in boldface. Overall, we ranked 6th out of 9 groups for the Concept Detection task and ranked 9th out of 13 groups for the Caption Prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Medical Concept Detection Task</head><p>Table <ref type="table" coords="8,118.36,304.10,5.49,9.88" target="#tab_1">2</ref> shows the summary of the run submissions for the concept detection task and respective F1 scores. The submitted runs for the Concept Detect tasks are described as follows:</p><p>1. Run_1_Dense169: Run 1 corresponds to a model that uses the DenseNet169 <ref type="bibr" coords="8,447.46,600.64,18.31,9.88" target="#b13">[11]</ref> architecture. DenseNet169 <ref type="bibr" coords="8,171.92,613.30,18.31,9.88" target="#b13">[11]</ref> is a convolutional neural network architecture that has shown promising performance in various computer vision tasks, including multi-label classification of medical images <ref type="bibr" coords="8,142.38,638.62,11.92,9.88" target="#b8">[6,</ref><ref type="bibr" coords="8,157.56,638.62,7.96,9.88" target="#b9">7]</ref>. It is characterized by its densely connected layers, where each layer is directly connected to every other layer in a feed-forward fashion. When using DenseNet169 <ref type="bibr" coords="8,488.38,651.24,18.29,9.88" target="#b13">[11]</ref> for multi-label prediction in Keras, it is common to leverage pretraining on the Imagenet <ref type="bibr" coords="8,504.58,663.90,18.39,9.88" target="#b14">[12]</ref> dataset. Imagenet <ref type="bibr" coords="8,188.24,676.56,18.31,9.88" target="#b14">[12]</ref> is a large-scale dataset that consists of millions of labeled images from thousands of categories <ref type="bibr" coords="8,212.84,689.22,16.98,9.88" target="#b14">[12]</ref>. By utilizing transfer learning and fine-tuning techniques, we have adapted the pretrained DenseNet169 <ref type="bibr" coords="8,277.58,701.88,18.28,9.88" target="#b13">[11]</ref> model to our concept detection task using Keras libraries. A Dense layer with 2,125 unique number classes is used on the top followed by a flatten layer and global average pooling layer.</p><p>2. Run_2_Dense121: Run 2 corresponds to a model that uses the pre-trained DenseNet121 <ref type="bibr" coords="9,504.70,74.76,18.31,9.88" target="#b13">[11]</ref> architecture with CheXNet <ref type="bibr" coords="9,236.12,87.42,18.31,9.88" target="#b15">[13]</ref> weights. The original paper of CheXNet <ref type="bibr" coords="9,452.92,87.42,18.53,9.88" target="#b15">[13]</ref> trained on ChestX-ray14 dataset <ref type="bibr" coords="9,207.89,100.02,18.45,9.88" target="#b15">[13]</ref> is an architecture of 121 CNN layers to detect labels for various pathologies based on more than 100,000 chest X-rays images. This fine-tuned model shows promising results among the five submissions we made for the task. Fig. <ref type="figure" coords="9,451.06,125.36,5.49,9.88" target="#fig_3">4</ref> illustrates our implemented model for this run. 3. Run_3_Ensemble_Convnext_Nasnet: Run 3 corresponds to a model that uses an ensemble of architectures, namely NasNetLarge <ref type="bibr" coords="9,277.34,315.98,18.39,9.88" target="#b18">[16]</ref> and ConvNeXtLarge <ref type="bibr" coords="9,392.62,315.98,16.88,9.88" target="#b17">[15]</ref>, with a weighted average approach (Fig. <ref type="figure" coords="9,174.89,328.64,3.98,9.88">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5: Architecture of Run_3_Ensemble_Convnext_Nasnet</head><p>NasNetLarge is a neural architecture search (NAS) network that automatically searches for an optimal CNN architecture <ref type="bibr" coords="9,224.62,525.52,16.96,9.88" target="#b18">[16]</ref>. It utilizes a reinforcement learning-based approach to discover an architecture that is well-suited for a given task. On the other hand, ConvNeXtLarge is another CNN architecture that focuses on capturing complex patterns and relationships within images <ref type="bibr" coords="9,141.44,563.44,16.88,9.88" target="#b17">[15]</ref>. It employs grouped convolutions and split-transform-merge operations to enhance its representation power and performance <ref type="bibr" coords="9,296.28,576.10,16.96,9.88" target="#b17">[15]</ref>. We implemented weighted average ensemble technique to combine predictions from these two architectures. Each model's predictions were weighted based on their validation accuracy. The purpose of the weighted average ensemble was to leverage the strengths of these models.  <ref type="bibr" coords="9,220.47,690.72,16.88,9.88" target="#b16">[14]</ref>. On the other hand, k-Nearest Neighbors (kNN) assigns labels to test samples based on the labels of the k most similar training and validation samples. In this case, kNN is applied to the features extracted by ViT. By finding the k nearest training images based on their feature similarity, the algorithm utilizes their known labels as the predicted labels for the test image. The value of k=14 has been determined through a process of experimentation and evaluation. The F1 scores for 100 different random values of k, ranging from 5 to 500, have been calculated and compared to identify the value that yields the best performance. This process ensures that the chosen value of k provides the optimal balance between capturing the diversity of similar images and minimizing potential noise or outliers. Figure <ref type="figure" coords="10,464.68,100.02,5.49,9.88" target="#fig_4">6</ref> depicts the architecture used on this fourth run. 5. Run_5_Ensemble_ResNet_VGG_Xcep_Incept: Run 5 corresponds to a model that uses an ensemble (majority voting technique based) of multiple architectures (Fig. <ref type="figure" coords="10,463.93,285.44,3.98,9.88" target="#fig_5">7</ref>), including ImageNet <ref type="bibr" coords="10,156.11,298.04,18.31,9.88" target="#b14">[12]</ref> weight based pre-trained ResNet50 <ref type="bibr" coords="10,344.93,298.04,16.92,9.88" target="#b19">[17]</ref>, Xception <ref type="bibr" coords="10,415.91,298.04,16.87,9.88">[18]</ref>, VGG16 <ref type="bibr" coords="10,481.32,298.04,16.87,9.88" target="#b21">[19]</ref>, and InceptionResNetV2 <ref type="bibr" coords="10,200.26,310.70,16.91,9.88" target="#b22">[20]</ref>. The ensemble approach utilizes a majority voting technique for decision-making. The ResNet50 architecture consists of residual blocks with skip connections, allowing the network to learn more complex representations. Xception is an extension of the Inception architecture that replaces the traditional convolutional layers with depth wise separable convolutions. VGG16 is a deep convolutional neural network architecture with 16 layers. Ensemble methods combine the predictions of multiple models to improve overall performance and robustness. The majority voting technique is a simple ensemble approach where the final prediction is determined by selecting the label that receives the most votes from individual models. See Figure <ref type="figure" coords="10,242.69,411.94,5.49,9.88" target="#fig_5">7</ref> for further illustration of the architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Medical Image Captioning Task</head><p>The submitted successful runs for the Caption Prediction tasks are described as follows:</p><p>Table <ref type="table" coords="11,137.10,289.82,5.49,9.88" target="#tab_4">3</ref> shows the summary of the run submissions for the caption prediction task and the respective BERTScore, ROUGE, BLEURT, BLEU, METEOR, CIDEr, and CLIP Score values. Our best scores and explanations for the corresponding performance metrices are as follows.</p><p>• BERTScore = 0.5819: BERTScore or Bidirectional Encoder Representations from Transformers measures the similarity between machine-generated text and reference text based on contextual embeddings. The BERTScore ranges from 0 to 1, with higher scores indicating greater similarity or better quality. In this case, a BERTScore of 0.5819 suggests that the machine-generated text has a moderate level of similarity to the reference text. Ordering score of 0.0568 is an evaluation metric used to measure the quality of machine translations. METEOR scores typically range from 0 to 1, with higher scores indicating better translation quality. METEOR considers both lexical and syntactic aspects of translation quality. • CIDEr = 0.1731: CIDEr or Consensus-based Image Description Evaluation is used in image captioning tasks to assess the quality of generated captions. CIDEr scores typically range from 0 to 1, with higher scores indicating better caption quality. CIDEr considers the consensus among reference captions and encourages diversity in the generated captions.</p><p>Here, the score represents a relatively low level of caption quality.</p><p>• CLIP Score = 0.7771: CLIP (Contrastive Language-Image Pretraining) is a method developed by OpenAI to measure the proximity of encoded text and image vectors. The CLIP score-based Cosine similarity ranges from +1 to -1. Here the calculated score of 0.7771 represents moderate identical vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This working note paper describes the approaches and outcomes of the CS_Morgan group's involvement in the ImageCLEFmedical 2023 Caption task. We participated in both the Concept Detection and Caption Prediction tasks within this evaluation. Our most successful results were obtained by employing a Convolutional Neural Network (CNN) architecture for concept detection and transformer-based methods, specifically ViT and T5, for the caption prediction task.</p><p>Looking ahead, we intend to further investigate Transformers that are tailored to the medical domain and utilize improved fusion mechanisms. This area of research has gained significant interest in the medical field as it enables the capture of global context, surpassing the capabilities of CNNs, which primarily focus on local receptive fields. We believe that exploring these domain-specific Transformers will enhance our future performance in the ImageCLEFmedical task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,128.72,331.33,337.34,10.98;3,91.60,72.00,425.90,257.05"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The most 10 frequent CUIs and their corresponding medical terms</figDesc><graphic coords="3,91.60,72.00,425.90,257.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,150.50,738.41,293.87,10.98;6,142.25,649.98,328.19,86.10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Image of the decoder layer used in the described model.</figDesc><graphic coords="6,142.25,649.98,328.19,86.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,147.38,276.85,300.22,10.98;7,249.98,72.00,113.05,202.55"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image of the decoder model used in the described model.</figDesc><graphic coords="7,249.98,72.00,113.05,202.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,178.22,275.95,238.54,10.98;9,198.50,160.54,197.40,113.15"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of Run_2_Dense121 (CheXNet)</figDesc><graphic coords="9,198.50,160.54,197.40,113.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,205.22,245.41,184.56,10.98;10,190.25,122.60,214.00,120.55"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of Run_4_ViT_kNN</figDesc><graphic coords="10,190.25,122.60,214.00,120.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,140.96,688.79,312.93,10.98;10,122.00,434.44,350.68,252.05"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Architecture of Run_5_Ensemble_ResNet_VGG_Xcep_Incept</figDesc><graphic coords="10,122.00,434.44,350.68,252.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,455.43,441.96,148.98"><head>Table 1 Example</head><label>1</label><figDesc></figDesc><table coords="5,87.42,468.81,426.54,135.60"><row><cell></cell><cell>of Captions and corresponding Keywords</cell><cell></cell></row><row><cell>#</cell><cell>Caption</cell><cell>Keywords</cell></row><row><cell>1</cell><cell>epicardial vessel from the distal right coronary</cell><cell>vessel distal right coronary artery</cell></row><row><cell></cell><cell>artery white arrow collateralize the diagonal</cell><cell>white arrow branch black arrow</cell></row><row><cell></cell><cell>branch black arrow</cell><cell></cell></row><row><cell>2</cell><cell>plain xray of the patient before surgery show an</cell><cell>plain xray patient surgery extensive</cell></row><row><cell></cell><cell>extensive soft tissue swell</cell><cell>soft tissue</cell></row><row><cell>3</cell><cell>computed tomography show a renal transplant lie</cell><cell>tomography renal right iliac fossa</cell></row><row><cell></cell><cell>in the right iliac fossa and a polycystic leave native</cell><cell>leave kidney within pelvis</cell></row><row><cell></cell><cell>kidney arise from within the patient's pelvis</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,328.87,444.23,242.81"><head>Table 2</head><label>2</label><figDesc>List of the Run Submissions with the corresponding F1-scores**</figDesc><table coords="8,91.20,361.99,425.03,183.56"><row><cell>Submissions</cell><cell>Architecture</cell><cell>Pretrained</cell><cell>F1</cell><cell>F1-Score</cell></row><row><cell></cell><cell></cell><cell>Weights</cell><cell>Score*</cell><cell>Manual*</cell></row><row><cell>Run_1</cell><cell>DenseNet 169</cell><cell>Imagenet</cell><cell>0.4368</cell><cell>0.8542</cell></row><row><cell>Run_2</cell><cell>DenseNet 121</cell><cell>CheXNet</cell><cell>0.4834</cell><cell>0.8901</cell></row><row><cell>Run_3</cell><cell>NasNetLarge and ConvNeXtLarge with</cell><cell>Imagenet</cell><cell>0.0060</cell><cell>0.1444</cell></row><row><cell></cell><cell>Weighted Avg. Ensemble</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run_4</cell><cell>ViT (Feature Extraction) and kNN</cell><cell>Imagenet</cell><cell>0.1008</cell><cell>0.3728</cell></row><row><cell></cell><cell>(k=14)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run_5</cell><cell>ResNet50, Xception, VGG16, and</cell><cell>Imagenet</cell><cell>0.4791</cell><cell>0.8582</cell></row><row><cell></cell><cell>InceptionResNetV2 with Ensemble/Late</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fusion (Majority Voting Technique)</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="8,72.00,553.92,272.46,7.98;8,72.00,563.70,166.91,7.98"><p>*The F1 Scores are found from https://fh-dortmund.sciebo.de/s/ZdPRXcPfMZ5cbUy ** Source: https://www.imageclef.org/2023/fusion</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,108.02,392.51,415.04,252.63"><head>BLEU = 0.1331: BLEU</head><label></label><figDesc>BLEURT or Bilingual Evaluation Understudy with Representations from Transformers score of 0.2649 indicates the moderate level of quality or similarity of a machine-generated translation compared to human reference translations, as measured by the BLEURT evaluation metric. BLEURT scores typically range from negative values to positive values, with higher positive scores indicating better quality or similarity. This metric utilizes a pre-trained transformer-based model to calculate the similarity score.• or Bilingual Evaluation Understudy is an evaluation metric used to measure the quality of machine translations. BLEU scores range from 0 to 1, with higher scores indicating better translation quality. The score of 0.1331 suggests a relatively low level of translation quality. Moreover, it measures the precision of n-grams (commonly unigrams, bigrams, trigrams, etc.) in the machine-generated translation compared to the reference translations. • METEOR = 0.0568: METEOR or Metric for Evaluation of Translation with Explicit</figDesc><table coords="11,108.02,443.20,414.93,48.58"><row><cell>ROUGE scores typically range from 0 to 1, with higher scores indicating greater similarity</cell></row><row><cell>or better quality. The score of 0.1564 suggests a relatively low level of overlap between the</cell></row><row><cell>machine-generated output and the reference text.</cell></row><row><cell>• BLEURT = 0.2649:</cell></row></table><note coords="11,108.02,392.51,414.99,9.99;11,126.02,405.22,396.97,9.88;11,126.02,417.88,397.04,9.88;11,126.02,430.54,397.02,9.88"><p>• ROUGE = 0.1564: ROUGE or Recall-Oriented Understudy for Gisting Evaluation score indicates the level of overlap between machine-generated captions and reference captions, as measured by the ROUGE evaluation metrics (e.g., ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence), and ROUGE-S (skip-bigram co-occurrence)).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,86.22,139.09,409.35,259.31"><head>Table 3</head><label>3</label><figDesc>List of the Run Submissions ranked by corresponding BERTScores**</figDesc><table coords="12,99.38,172.45,396.19,199.92"><row><cell cols="6">Runs BERTScore* ROUGE* BLEURT* BLEU* METEOR* CIDEr* CLIP Score*</cell></row><row><cell>Run_4 0.5791</cell><cell>0.1541</cell><cell>0.2649 0.1331</cell><cell>0.0568</cell><cell>0.1731</cell><cell>0.7771</cell></row><row><cell>Run_5 0.5508</cell><cell>0.1070</cell><cell>0.2373 0.1040</cell><cell>0.0351</cell><cell>0.0481</cell><cell>0.7165</cell></row><row><cell>Run_6 0.5438</cell><cell>0.1107</cell><cell>0.1817 0.0026</cell><cell>0.0329</cell><cell>0.0925</cell><cell>0.7582</cell></row><row><cell>Run_8 0.5087</cell><cell>0.0264</cell><cell>0.1205 0.0034</cell><cell>0.0107</cell><cell>0.0125</cell><cell>0.6819</cell></row><row><cell>Run_9 0.5419</cell><cell>0.0924</cell><cell>0.1735 0.0406</cell><cell>0.0210</cell><cell>0.0187</cell><cell>0.6821</cell></row><row><cell>Run_10 0.5819</cell><cell>0.1564</cell><cell>0.2242 0.0566</cell><cell>0.0436</cell><cell>0.0840</cell><cell>0.7593</cell></row><row><cell>Run_12 0.5558</cell><cell>0.1272</cell><cell>0.2569 0.1199</cell><cell>0.0344</cell><cell>0.0164</cell><cell>0.7338</cell></row><row><cell>Run_13 0.5481</cell><cell>0.1144</cell><cell>0.2435 0.1180</cell><cell>0.0323</cell><cell>0.0142</cell><cell>0.6910</cell></row></table><note coords="12,108.02,380.68,264.16,7.98;12,108.02,390.42,166.89,7.98"><p>*The scores are found from https://fh-dortmund.sciebo.de/s/ZdPRXcPfMZ5cbUy ** Source: https://www.imageclef.org/2023/fusion</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">National Science Foundation (NSF)</rs> grant (<rs type="grantNumber">ID. 2131307</rs>) "<rs type="projectName">CISE-MSI</rs>: <rs type="projectName">DP</rs>: <rs type="projectName">IIS: III: Deep Learning-Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Wg85jC7">
					<idno type="grant-number">ID. 2131307</idno>
					<orgName type="project" subtype="full">CISE-MSI</orgName>
				</org>
				<org type="funded-project" xml:id="_PtBASaw">
					<orgName type="project" subtype="full">DP</orgName>
				</org>
				<org type="funded-project" xml:id="_MpeUqdx">
					<orgName type="project" subtype="full">IIS: III: Deep Learning-Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,94.13,74.76,428.50,9.88;11,108.02,87.42,314.90,9.88" xml:id="b0">
	<monogr>
		<title level="m" coord="11,108.02,74.76,414.61,9.88;11,108.02,87.42,266.31,9.88">Run_4: Run 4 involved using the trained KNN classifier used to retrieve the most similar caption based on extracted features from the pre-trained ViT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.13,100.02,428.56,9.88;11,108.02,112.68,343.08,9.88" xml:id="b1">
	<monogr>
		<title level="m" coord="11,268.40,100.02,254.30,9.88;11,108.02,112.68,178.56,9.88">trained KNN classifier used to retrieve the most similar caption based on extracted features from</title>
		<imprint/>
	</monogr>
	<note>Run_5: Run_5 involved using the. the pre-trained MRCNN</note>
</biblStruct>

<biblStruct coords="11,94.13,175.94,354.03,9.88;11,467.98,175.94,54.94,9.88;11,108.02,188.60,134.33,9.88" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,149.00,175.94,299.16,9.88;11,467.98,175.94,54.94,9.88;11,108.02,188.60,128.98,9.88">Run_9 involved generating captions using the fine-tuned T5 [model from keywords generated in Run_8</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.02,706.74,414.74,9.88;12,108.02,719.40,414.81,9.88;12,108.02,732.06,254.89,9.88" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,438.75,719.40,84.09,9.88;12,108.02,732.06,223.02,9.88">Medicat: A dataset of medical images, captions, and textual references</title>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravanthi</forename><surname>Parasa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,74.65,415.00,9.99;13,108.02,87.42,415.02,9.88;13,108.02,100.02,24.75,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,412.47,74.76,110.55,9.88;13,108.02,87.42,221.75,9.88">Automatic captioning for medical imaging (mic): a rapid review of literature</title>
		<author>
			<persName coords=""><forename type="first">Djamila-Romaissa</forename><surname>Beddiar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mourad</forename><surname>Oussalah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tapio</forename><surname>Sepp Änen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,336.34,87.42,126.58,9.88">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,112.68,414.84,9.88;13,108.02,125.36,414.99,9.88;13,108.02,138.02,205.59,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,473.24,112.68,49.62,9.88;13,108.02,125.36,309.10,9.88">Design and development of a multimodal biomedical information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,425.20,125.36,97.81,9.88;13,108.02,138.02,107.96,9.88">Journal of Computing Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,150.68,414.61,9.88;13,108.02,163.34,415.04,9.88;13,108.02,175.94,85.56,9.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,386.41,150.68,136.23,9.88;13,108.02,163.34,44.73,9.88">A survey on biomedical image captioning</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasiliki</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,173.24,163.34,344.94,9.88">Proceedings of the second workshop on shortcomings in vision and language</title>
		<meeting>the second workshop on shortcomings in vision and language</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,188.60,414.86,9.88;13,108.02,201.26,395.34,9.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,382.15,188.60,140.73,9.88;13,108.02,201.26,204.50,9.88">Yale image finder (yif): a new search engine for retrieving biomedical images</title>
		<author>
			<persName coords=""><forename type="first">Songhua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Mccusker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Krauthammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,320.26,201.26,63.60,9.88">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1968" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,213.81,415.00,9.99;13,108.02,226.58,414.92,9.88;13,108.02,239.18,415.00,9.88;13,108.02,251.84,414.74,9.88;13,108.02,264.50,414.99,9.88;13,108.02,277.16,414.70,9.88;13,108.02,289.71,414.82,9.99;13,108.02,302.42,414.82,9.88;13,108.02,315.08,414.77,9.88;13,108.02,327.74,414.64,9.88;13,108.02,340.40,414.84,9.88;13,108.02,353.06,274.68,9.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,239.20,302.42,283.64,9.88;13,108.02,315.08,288.88,9.88">Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,417.86,315.08,104.93,9.88;13,108.02,327.74,414.64,9.88;13,108.02,340.40,207.31,9.88">Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,353.57,340.40,169.29,9.88;13,108.02,353.06,61.62,9.88">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 14th International Conference of the CLEF Association (CLEF<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-09-18">2023. September 18-21, 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,108.02,365.72,414.78,9.88;13,108.02,378.32,415.04,9.88;13,108.02,390.89,414.84,9.99;13,108.02,403.66,414.71,9.88;13,108.02,416.32,340.28,9.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,108.02,390.89,395.85,9.99">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,108.02,403.66,414.71,9.88;13,108.02,416.32,122.45,9.88">Experimental IR Meets Multilinguality, Multimodality, and Interaction. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,428.98,414.66,9.88;13,108.02,441.58,414.93,9.88;13,108.02,454.24,130.71,9.88" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,270.46,441.58,105.76,9.88">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,383.74,441.58,139.21,9.88;13,108.02,454.24,82.14,9.88">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,466.90,414.98,9.88;13,108.02,479.56,414.90,9.88;13,108.02,492.22,205.00,9.88" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,274.52,479.56,140.68,9.88">A survey of visual transformers</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongchao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,423.52,479.56,99.40,9.88;13,108.02,492.22,172.77,9.88">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,504.77,414.96,9.99;13,108.02,517.48,414.86,9.88;13,108.02,530.14,414.92,9.88;13,108.02,542.80,414.90,9.88;13,108.02,555.46,414.74,9.88;13,108.02,568.12,301.87,9.88" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,108.02,517.48,280.81,9.88">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Johannes R Ückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,407.55,517.48,115.32,9.88;13,108.02,530.14,414.92,9.88;13,108.02,542.80,414.90,9.88;13,108.02,555.46,42.03,9.88;13,238.61,555.46,195.22,9.88">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="13,108.02,580.72,414.68,9.88;13,108.02,593.38,414.82,9.88;13,108.02,606.04,217.55,9.88" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,486.69,580.72,36.01,9.88;13,108.02,593.38,148.68,9.88">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,276.14,593.38,246.70,9.88;13,108.02,606.04,102.86,9.88">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,618.70,415.00,9.88;13,108.02,631.36,414.80,9.88;13,108.02,644.02,182.06,9.88" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,418.65,618.70,104.37,9.88;13,108.02,631.36,126.71,9.88">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,260.30,631.36,262.52,9.88;13,108.02,644.02,49.05,9.88">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.02,656.64,414.88,9.88;13,108.02,669.30,415.02,9.88;13,108.02,681.96,414.98,9.88;13,108.02,694.62,24.75,9.88" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,401.67,669.30,121.38,9.88;13,108.02,681.96,254.36,9.88">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaylie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hershel</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daisy</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarti</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,108.02,707.28,414.69,9.88;13,108.02,719.88,414.81,9.88;13,108.02,732.54,414.99,9.88;13,108.02,745.20,110.58,9.88" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="13,120.92,732.54,332.91,9.88">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,108.02,74.65,414.83,9.99;14,108.02,87.42,414.91,9.88;14,108.02,100.02,246.48,9.88" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,434.01,74.76,88.84,9.88;14,108.02,87.42,178.69,9.88">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Doll Ár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,305.90,87.42,217.03,9.88;14,108.02,100.02,131.84,9.88">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,112.68,414.71,9.88;14,108.02,125.36,414.83,9.88;14,108.02,138.02,290.88,9.88" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,425.75,112.68,96.99,9.88;14,108.02,125.36,202.06,9.88">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,335.02,125.36,187.83,9.88;14,108.02,138.02,176.12,9.88">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,150.68,414.81,9.88;14,108.02,163.34,414.75,9.88;14,108.02,175.94,152.76,9.88" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,375.97,150.68,146.86,9.88;14,108.02,163.34,48.43,9.88">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,184.04,163.34,338.73,9.88;14,108.02,175.94,49.05,9.88">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,188.49,414.91,9.99;14,108.02,201.26,415.01,9.88;14,108.02,213.92,52.24,9.88" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,197.56,188.60,304.79,9.88">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName coords=""><forename type="first">Fran</forename><surname>Çois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chollet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,108.02,201.26,352.48,9.88">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,226.58,414.99,9.88;14,108.02,239.18,256.39,9.88" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="14,299.12,226.58,223.90,9.88;14,108.02,239.18,78.05,9.88">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,108.02,251.84,415.07,9.88;14,108.02,264.50,414.96,9.88;14,108.02,277.16,239.82,9.88" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,463.83,251.84,59.26,9.88;14,108.02,264.50,289.60,9.88">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,415.60,264.50,107.38,9.88;14,108.02,277.16,155.85,9.88">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,289.82,414.91,9.88;14,108.02,302.42,203.19,9.88" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,283.58,289.82,162.88,9.88">A stochastic approximation method</title>
		<author>
			<persName coords=""><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,457.30,289.82,65.63,9.88;14,108.02,302.42,100.09,9.88">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,314.97,414.95,9.99;14,108.02,327.74,362.60,9.88" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,399.26,315.08,49.17,9.88">Mask r-cnn</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,468.04,315.08,54.93,9.88;14,108.02,327.74,248.20,9.88">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,340.40,414.71,9.88;14,108.02,352.95,414.72,9.99;14,108.02,365.72,415.00,9.88;14,108.02,378.32,290.79,9.88" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,297.40,353.06,206.26,9.88">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,108.02,365.72,256.03,9.88">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,391.00,414.69,9.88;14,108.02,403.66,360.79,9.88" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,108.02,403.66,234.52,9.88">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,349.66,403.66,54.79,9.88">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,416.32,414.69,9.88;14,108.02,428.98,414.65,9.88;14,108.02,441.58,413.30,9.88" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,279.17,428.98,243.50,9.88;14,108.02,441.58,101.07,9.88">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,216.38,441.58,190.68,9.88">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,454.13,415.04,9.99;14,108.02,466.90,415.00,9.88;14,108.02,479.56,415.01,9.88;14,108.02,492.22,24.77,9.88" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,493.68,466.90,29.34,9.88;14,108.02,479.56,148.29,9.88">Scikitlearn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Ga Ël Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertrand</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathieu</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ron</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,264.01,479.56,185.14,9.88">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,504.88,414.91,9.88;14,108.02,517.48,105.08,9.88" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="14,261.81,504.88,191.87,9.88">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,108.02,530.14,414.74,9.88;14,108.02,542.69,414.79,9.99;14,108.02,555.46,414.98,9.88;14,108.02,568.12,24.75,9.88" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="14,458.12,542.69,64.69,9.98;14,108.02,555.46,255.79,9.88">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Émi Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,108.02,580.72,414.76,9.88;14,108.02,593.38,209.54,9.88" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="14,433.37,580.72,89.41,9.88;14,108.02,593.38,177.82,9.88">Improving language understanding by generative pre-training</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,108.02,606.04,414.97,9.88;14,108.02,618.70,110.63,9.88" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="14,365.16,606.04,87.43,9.88">Layer normalization</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,108.02,631.36,414.75,9.88;14,108.02,644.02,415.03,9.88;14,108.02,656.64,241.98,9.88" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,176.56,644.02,288.18,9.88">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,471.14,644.02,51.91,9.88;14,108.02,656.64,128.00,9.88">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
