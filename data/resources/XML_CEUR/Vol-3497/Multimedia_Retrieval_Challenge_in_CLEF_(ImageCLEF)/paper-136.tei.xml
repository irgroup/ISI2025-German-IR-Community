<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.05,375.28,15.39;1,89.29,106.97,398.85,15.39;1,89.29,128.89,149.46,15.39">Concept Detection and Caption Prediction from Medical Images using Gradient Boosted Ensembles and Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,158.21,115.20,10.68"><forename type="first">Mirunalini</forename><surname>Palaniappan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.15,158.21,100.55,10.68"><forename type="first">Haricharan</forename><surname>Bharathi</surname></persName>
							<email>haricharan2010267@ssn.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.37,158.21,137.29,10.68"><forename type="first">Eeswara</forename><forename type="middle">Anvesh</forename><surname>Chodisetty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.87,172.16,83.91,10.68"><forename type="first">Anirudh</forename><surname>Bhaskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.80,172.16,80.34,10.68"><forename type="first">Karthik</forename><surname>Desingu</surname></persName>
							<email>karthik19047@cse.ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Engineering</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.05,375.28,15.39;1,89.29,106.97,398.85,15.39;1,89.29,128.89,149.46,15.39">Concept Detection and Caption Prediction from Medical Images using Gradient Boosted Ensembles and Deep Learning</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">37C17271B740615682BF069811EAE9B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>concept detection</term>
					<term>caption prediction</term>
					<term>natural language processing</term>
					<term>computer vision ensemble</term>
					<term>feature extraction</term>
					<term>deep learning</term>
					<term>automated image captioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper outlines the contributions of our team in the annual ImageCLEFmedical Caption Task, which encompasses the Concept Detection and Caption Prediction sub-tasks. The concept detection sub-task focuses on automatically assigning appropriate medical concepts, based on Clinical Concept Unique Identifiers (CUIs), as tags of medical images. CUIs are unique identifiers assigned to medical concepts in the Unified Medical Language System (UMLS). They are based on a hierarchical structure and represent a standardized representation of various medical concepts, including diseases, anatomical structures, procedures, and more, while the caption prediction sub-task generates preliminary diagnostic captions for medical images, aiding medical professionals in preparing diagnostic reports. In the concept detection subtask, our approach involved using deep learning models to perform feature extraction, employing three distinct DenseNet models for feature extraction from the images. Subsequently, we utilized an XGBoost gradient boosting model to predict the Concept Unique Identifiers (CUIs) associated with a given image. In the caption prediction subtask, we used a model that utilizes a pre-trained InceptionV3 on the extended ROCO dataset to extract image features, which are then fed into a retrained LSTM model for caption generation. The method preprocesses the input image, extracts features using InceptionV3, and generates captions using the LSTM model through beam search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ImageCLEFmedical Caption 2023 task <ref type="bibr" coords="1,279.47,504.47,12.83,9.74" target="#b0">[1]</ref> is the 7 th edition of the caption task as a part of ImageCLEF 2023 <ref type="bibr" coords="1,167.94,518.02,11.59,9.74" target="#b1">[2]</ref>. Similar to the previous edition <ref type="bibr" coords="1,328.13,518.02,11.58,9.74" target="#b2">[3]</ref>, the task consists of two subtasks: a Concept Detection task and a Caption Prediction task. The Concept Detection subtask involves recognizing and locating pertinent concepts within a vast collection of medical images. This is done by identifying the various Concept Unique Identifiers (CUIs) from the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,208.87,572.22,11.58,9.74" target="#b3">[4]</ref>. The Caption Prediction subtask involved generating cohesive captions that encompass the entire image. This is done by generating suitable captions based on the various CUIs generated from the first subtask.</p><p>Image captioning plays a crucial role in comprehending visual content. The groundbreaking research paper by Vinyals et al. <ref type="bibr" coords="2,225.64,128.74,12.69,9.74" target="#b4">[5]</ref> demonstrate the transformative potential of image captioning, showcasing how deep neural networks can generate accurate and coherent descriptions for visual content. With the surge in digital images and the need for automated image analysis, accurate and descriptive captions are essential. Image captioning enables improved searchability, context-aware information retrieval, and enhanced user experiences. The paper by Karpathy and Fei-Fe <ref type="bibr" coords="2,139.85,196.48,13.00,9.74" target="#b5">[6]</ref> demonstrates the effectiveness of the their proposed model, highlighting the power of deep visual-semantic alignments in generating high-quality image descriptions.</p><p>In the medical field, image captioning has emerged as a valuable tool facilitating comprehensive understanding and analysis of visual medical content. The Unified Medical Language System (UMLS) <ref type="bibr" coords="2,163.30,250.68,13.00,9.74" target="#b3">[4]</ref> plays a crucial role in solving the issue of concept detection in medical image analysis. By providing a comprehensive and standardized vocabulary of Clinical Concept Unique Identifiers (CUIs), UMLS enables accurate and consistent labeling of medical concepts within images. This allows for automatic assignment of appropriate medical concepts as tags to medical images, facilitating efficient retrieval, analysis, and interpretation of medical data. Also, the UMLS resolves inter-observer variability arising from differing concept identifications among doctors, providing a standardized vocabulary and unique identifiers. This promotes consistency, harmonization, and effective collaboration in medical image analysis, enhancing accuracy and reliability. Furthermore, generating error-free reports from medical images is of utmost importance in healthcare. Caption prediction models, when applied to medical images, can automatically generate textual descriptions or reports summarizing the content and findings within the images. The accuracy of these reports is vital for accurate diagnosis, treatment planning, and communication among healthcare professionals. By ensuring the generation of error-free reports, caption prediction models enhance patient care by reducing the potential for misinterpretation or miscommunication of critical information. This, in turn, improves the efficiency and effectiveness of medical decision-making, leading to better patient outcomes. Therefore, it is crucial for caption prediction models to generate error-free reports from medical images, contributing to enhanced healthcare delivery, and patient safety.</p><p>The accurate and descriptive captions generated by image captioning systems assist healthcare professionals in interpreting complex medical images and aid in diagnosis, treatment planning, and medical education. The paper by Selivanov et al. <ref type="bibr" coords="2,342.34,521.66,13.00,9.74" target="#b6">[7]</ref> contributed to the advancement of medical image captioning by demonstrating the efficacy of utilizing generative pretrained transformers, improving the generation of accurate and contextually relevant captions for medical images.</p><p>From the 2022 edition of the ImageCLEFmedical Caption task, we found that with respect to the first subtask, the AUEB-NLP-Group <ref type="bibr" coords="2,281.15,589.41,12.93,9.74" target="#b7">[8]</ref> achieved the highest primary F1-score with an ensemble of EfficientNetV2-B0 backbones, CMRE-UoG <ref type="bibr" coords="2,333.45,602.96,12.78,9.74" target="#b8">[9]</ref> proposed an image retrieval system with an ensemble of DenseNet-201, and the CSIRO group used an ensemble of DenseNet-161 with top 1% threshold optimization for multi-label classification. With respect to the Caption Prediction subtask, we found that the IUST_NLPLAB <ref type="bibr" coords="2,368.32,643.61,17.98,9.74" target="#b9">[10]</ref> team achieved the highest scores in caption prediction subtask, surpassing competitors significantly, using a multi-label classification system, while the AUEB-NLP-Group <ref type="bibr" coords="2,320.92,670.70,13.00,9.74" target="#b7">[8]</ref> and CSIRO <ref type="bibr" coords="2,389.89,670.70,18.08,9.74" target="#b10">[11]</ref> teams also presented competitive results with their respective models based on Show and Tell and CvT-21 with DistilGPT2. A Python implementation of the experiments described in this paper will be made available at: https://github.com/karthik-d/ImageCLEFmedical-Captioning-2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the field of caption prediction, several notable works have explored innovative approaches to generate descriptive and contextually relevant captions for images. The paper by Vinyals <ref type="bibr" coords="3,493.06,187.36,12.93,9.74" target="#b4">[5]</ref> introduces an encoder-decoder framework using CNN and RNN for image captioning. It achieves state-of-the-art performance by learning the correlation between image features and descriptive captions. The paper's contribution lies in automating accurate and contextually relevant caption generation for images. The research paper by Karpathy and Fei-Fe <ref type="bibr" coords="3,385.82,241.56,12.79,9.74" target="#b5">[6]</ref> introduces a model that utilizes deep neural networks to generate accurate and coherent descriptions for images. It demonstrates the effectiveness of visual-semantic alignments in producing high-quality image captions. The approach improves the searchability, context-aware information retrieval, and user experiences related to image content. This paper showcases the transformative potential of deep learning techniques in advancing caption prediction for enhanced image understanding and analysis. The paper by Anderson et al. <ref type="bibr" coords="3,285.28,322.86,18.01,9.74" target="#b11">[12]</ref> introduces a two-stage attention mechanism for image captioning. The bottom-up attention identifies salient image regions, while the top-down attention generates contextually relevant words based on the visual and linguistic context. This approach improves the quality of generated captions by focusing on relevant image regions and incorporating both visual and language information. The model's effectiveness has been demonstrated through state-of-the-art performance on various captioning benchmarks, showcasing its application in generating more accurate and descriptive image captions. You et al. <ref type="bibr" coords="3,103.42,417.70,18.07,9.74" target="#b12">[13]</ref> proposed a novel approach to image captioning by incorporating semantic attention mechanisms. The model dynamically attends to relevant regions in the image while generating captions, resulting in more accurate and contextually rich descriptions. This paper enhances the caption prediction process by improving the alignment between visual and semantic information. In the research done by Xu <ref type="bibr" coords="3,210.48,471.90,16.13,9.74" target="#b13">[14]</ref>, an attention-based model is introduced, leveraging techniques from machine translation and object detection, to automatically generate descriptive captions for images, achieving state-of-the-art performance on benchmark datasets and demonstrating the model's ability to focus on salient objects during caption generation.</p><p>Specifically in the medical field, the aforementioned paper by Selivanov et al. <ref type="bibr" coords="3,447.74,526.09,12.87,9.74" target="#b6">[7]</ref> addresses the limitations of existing models in medical image captioning and proposes a new architecture that combines two language models with image and text attention mechanisms. The proposed approach outperforms current state-of-the-art models and introduces a new preprocessing pipeline for radiology reports, leading to higher natural language generation metrics. The results demonstrate the effectiveness of the proposed methods in generating descriptive and informative captions for medical images, particularly in chest X-Ray image captioning. The combination of language models and the use of the GPT-3 model show significant improvements in text generation scores. Furthermore, it suggests that large language models (LLMs) can play a crucial role in enhancing the performance of report generation from image features detected through convolutional image models.</p><p>In conclusion, the field of caption prediction has witnessed significant advancements through innovative approaches proposed in several influential papers. These works have demonstrated the effectiveness of various techniques, such as encoder-decoder frameworks, deep neural networks, attention mechanisms, and semantic attention mechanisms, in generating contextually appropriate captions for images. The application of these methodologies extends to diverse domains, including image understanding, content retrieval, and multimedia captioning systems. These advancements open doors for improved image comprehension, content retrieval, and aid in medical decision-making by providing precise and informative image captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Previous Iterations of ImageCLEF for Medical Image Captioning</head><p>Since the 2021 edition of ImageCLEFmedical caption task <ref type="bibr" coords="4,355.37,232.66,16.42,9.74" target="#b14">[15]</ref>, the team had noticed in the concept detection subtask that different teams had employed deep learning models, such as DenseNet, InceptionV3, and MobileNet-v2, either as multi-label classifiers or in information retrieval-oriented solutions using image embeddings. In the 2021 edition, more modern architectures like EfficientNets and Visual Transformers (ViT) were introduced, resulting in improved F1-scores compared to previous years. In the caption prediction subtask, teams had utilized variations of the Show, Attend and Tell model, incorporated Transformer-based architectures, and explored the use of general language models like GPT-2, while finding that simple architectures outperformed pretraining with medically oriented datasets.</p><p>With respect to the ImageCLEFmedical Caption task 2022 <ref type="bibr" coords="4,367.74,354.60,11.58,9.74" target="#b2">[3]</ref>, the following results were inferred from the concept detection subtask :</p><p>‚Ä¢ The AUEB-NLP-Group <ref type="bibr" coords="4,221.90,390.67,12.89,9.74" target="#b7">[8]</ref> achieved the best performance in the concept detection task with an ensemble of two EfficientNetV2-B0 backbones and a single classification layer, using the union of predicted concepts for the ensemble. ‚Ä¢ The CMRE-UoG team <ref type="bibr" coords="4,217.25,432.67,12.94,9.74" target="#b8">[9]</ref> proposed an image retrieval system with an ensemble of five DenseNet-201 models, retrieving 100 different images each and assigning the union of predicted CUIs to each image. ‚Ä¢ The CSIRO group <ref type="bibr" coords="4,199.04,474.67,18.06,9.74" target="#b10">[11]</ref> experimented with multiple backbones and their best approach involved an ensemble of 43 DenseNet-161 models with top-1% threshold optimization for multi-label classification. ‚Ä¢ The SSN MLRG team <ref type="bibr" coords="4,212.32,516.68,17.83,9.74" target="#b15">[16]</ref> employed DenseNet for multi-label classification and an information retrieval system for their caption prediction model.</p><p>In the second subtask, namely Caption Prediction, the following were the major takeaways from the 2022 edition of the contest :</p><p>‚Ä¢ The IUST_NLPLAB team <ref type="bibr" coords="4,235.50,588.81,18.07,9.74" target="#b9">[10]</ref> employed a multi-label classification system based on ResNet-50, treating each word as a label and assigning 26 words in order of their probability to each image, resulting in their superior performance with a BLEU score of 0.4828. ‚Ä¢ The AUEB-NLP-Group <ref type="bibr" coords="4,222.11,630.81,12.91,9.74" target="#b7">[8]</ref> utilized the Show and Tell model, consisting of a CNN-RNN encoder-decoder with an EfficientNet-B0 backbone, achieving a BLEU score of 0.3222, demonstrating competitive performance in various evaluation metrics.</p><p>‚Ä¢ The CSIRO group <ref type="bibr" coords="5,203.41,88.09,18.07,9.74" target="#b10">[11]</ref> experimented with different encoder-to-decoder models and achieved their best results using CvT-21 as the encoder and DistilGPT2 as the decoder. They obtained the overall best BERTScore of 0.6234, showcasing the effectiveness of their chosen model combination. ‚Ä¢ The SSN MLRG team <ref type="bibr" coords="5,216.13,143.64,18.08,9.74" target="#b15">[16]</ref> employed a Sparse Auto Encoder (SAE) with a Multi-Layer Perceptron (MLP) and a Gated Recurrent Unit (GRU) for their caption prediction model.</p><p>Based on the above results, our team decided to use DenseNet models as feature extractors as it gave promising results in the previous year and feed the feature vectors to a gradient boosting model to detect the CUIs. For the caption prediction subtask, the team decided to go with an InceptionV3 model pretrained and then finetuned to extract the feature vectors. An LSTM-based caption generation model is employed to generate captions for the given images, incorporating the beam search algorithm to explore multiple possible captions and select the most probable one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>The following section explains in detail the systems that were utilized in our submissions for the Concept Detection and Caption Prediction sub-tasks. A Python implementation of the experiments described in this paper will be made available at: https://github.com/karthik-d/ ImageCLEFmedical-Captioning-2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The current edition of the task consisted of a total of 60,918 images allocated to the training set, 10,437 images in the validation set, and 10,473 images in the testing set. These images were selected from the extended and revised version of the Radiology Objects in Context (ROCO) dataset, which is derived from biomedical articles available in the PMC OpenAccess subset.</p><p>The concepts used in the Concept Detection task were extracted from the UMLS 2022 AB release, and applied to the images. These images were subsequently filtered based on their semantic type. Building on a suggestion from the previous year, concepts with low frequency were eliminated in the current implementation. The captions used in the study were obtained from annotated medical literature, and any hyperlinks present in the original text were excluded or removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Concept Detection</head><p>For Concept Detection, the team decided to use three different models of DenseNet <ref type="bibr" coords="5,475.25,581.67,18.07,9.74" target="#b16">[17]</ref> as feature extractors, namely DenseNet-121, DenseNet-169 and DenseNet-201. The feature vectors where then given as input to an XGBoost ensemble classifier to predict the class labels for a given observation.</p><p>Feature extraction is a technique used in image processing to extract pertinent and distinct visual elements from photographs. In order to discover and record characteristic patterns, textures, shapes, or colours that are indicative of certain objects, structures, or concepts, image data must first be analysed. When performing concept recognition or classification tasks, these extracted features act as compact representations of the images and are put into a booster method like XGBoost. In order to understand complicated patterns and correlations, the booster uses the extracted features. This enables precise prediction and decision-making based on the visual properties present in the images.</p><p>Multiple deep learning architectures were considered for the concept detection task. Deep convolutional neural networks were first presented by Huang et al. in 2017 <ref type="bibr" coords="6,449.20,169.38,18.08,9.74" target="#b16">[17]</ref> and are distinguished by their densely linked layers. By creating dense connections across layers, the DenseNet design tries to solve the vanishing gradient issue and encourage feature reuse. ResNet <ref type="bibr" coords="6,123.50,210.03,17.92,9.74" target="#b17">[18]</ref> , short for Residual Network, is a deep convolutional neural network architecture introduced by He et al. in 2015. ResNet designs are renowned for their creative use of residual connections, which, by overcoming the degradation issue that arises with adding more layers, allow for the training of very deep networks. EfficientNet <ref type="bibr" coords="6,343.31,250.68,17.77,9.74" target="#b18">[19]</ref> is a family of deep convolutional neural network architectures introduced by Tan et al. in 2019. The key idea behind EfficientNet is to achieve state-of-the-art performance with high efficiency in terms of both computational resources and model size. EfficientNet models have been designed using a compound scaling method that balances the network depth, width, and resolution to achieve optimal performance.</p><p>Ensemble methods <ref type="bibr" coords="6,188.94,318.43,18.08,9.74" target="#b19">[20]</ref> are techniques employed to combine multiple models to produce improved results. They boast higher accuracy scores than the individual models themselves. Boosting is a prominent ensembling technique used wherein new models are added to the existing features of the model to correct errors. Our solution adopted a gradient-boosting ensemble approach for concept detection to identify the various labels for a given image. XGBoost <ref type="bibr" coords="6,131.48,386.17,17.96,9.74" target="#b20">[21]</ref> is an implementation of gradient boosted decision trees designed for speed and performance. The authors decided to implement the XGBoost library package above all the other boosters because of its higher execution performance. The choice of DenseNet models as the base learners for the XGBoost ensemble was motivated by several factors. Using the scores obtained from models trained on the training dataset found in Table <ref type="table" coords="6,127.75,593.95,3.74,9.74" target="#tab_0">1</ref>, it observed that EfficientNet and the ResNet models had significantly higher losses and lower accuracy as compared to the DenseNet models. The superior performance exhibited by the DenseNet models made them a compelling option for inclusion in the XGBoost ensemble. The aim of constructing an ensemble model is to leverage the strengths of individual base learners and mitigate their weaknesses through collective decision-making. The DenseNet models were favored due to its architecture's dense connectivity and its ability to encourage complementary learning make it well-suited for ensemble learning. Since each of the models have its own unique perspective on the data, the ensemble can exploit their diverse strengths and compensate for individual weaknesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Caption Prediction</head><p>The UMLS utilizes standardized vocabulary (CUIs) to accurately label medical concepts in image analysis, enhancing retrieval and interpretation. Caption prediction models generating errorfree reports improve patient care, facilitating precise diagnosis and treatment planning while minimizing miscommunication risks. By bridging concept detection gaps and ensuring reliable reports, UMLS and caption prediction models contribute to enhanced healthcare delivery and patient safety.</p><p>For Caption Prediction, our team decided to use a fine-tuned InceptionV3 model to extract the features, which were given as input to an LSTM model that generated captions based on those features. Beam search was used to explore multiple possible sequences of words and select the most likely caption based on the model's predictions and the specified beam index.</p><p>InceptionV3 <ref type="bibr" coords="7,159.22,300.41,18.07,9.74" target="#b21">[22]</ref> is a convolutional neural network architecture that was introduced by Google in 2015. Its primary purpose is to perform image classification tasks and it has gained significant popularity in various computer vision applications. The fundamental concept behind InceptionV3 revolves around utilizing inception modules, which enable the network to effectively capture features at different spatial scales. These modules comprise parallel convolutional layers with varying sizes, enabling the network to learn both local and global features. Additionally, InceptionV3 incorporates techniques like batch normalization and regularization to enhance training and generalization. The model is pretrained on a large dataset, such as ImageNet <ref type="bibr" coords="7,208.48,408.80,16.36,9.74" target="#b22">[23]</ref>, and can be fine-tuned for specific tasks by replacing the final classification layer. InceptionLSTM is a neural network architecture used for image captioning tasks. It combines the InceptionV3 convolutional neural network with a Long Short-Term Memory (LSTM) recurrent neural network. The InceptionV3 model extracts visual features from input images, while the LSTM generates a sequence of words as captions based on those features. This architecture enables the model to capture both visual and semantic information, resulting in meaningful and contextually relevant image captions.</p><p>Beam search is a decoding algorithm commonly used in sequence generation tasks, such as machine translation and image captioning. It explores multiple possible sequences of words by maintaining a beam of the most likely candidates at each decoding step. The beam width or beam size determines the number of candidates retained at each step. Beam search is applied during the caption generation process to select the most likely captions based on the model's predictions. It helps to generate more diverse and accurate captions by considering multiple hypotheses simultaneously and choosing the one with the highest probability. The team used beam search to generate captions based on the fine-tuned InceptionV3 and LSTM model's predictions, allowing it to produce more accurate and contextually relevant captions for the given images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>The detection process was divided into two stages: Feature Extraction and Boosting. In Feature Extraction, transfer learning techniques were used with pre-trained models such as ResNet-101, EfficientNet-B2, DenseNet-121, DenseNet-169, and DenseNet-201. The Adam Optimizer was used for all models, and the validation loss was monitored. Since each image had multiple labels, a Label Encoder was used to assign a unique label to each CUI ID. This was then converted into a multi-hot encoded array using a Multi-Label Binarizer. Both the input image and encoded labels were given as input to all the models. However, ResNet-101 and EfficientNet-B2 had high validation loss, so they were excluded as feature extractors. We continued with the DenseNet models, where we extracted the core architecture and added an additional dense and flatten layer with 4096 nodes to represent the feature vectors. The initial DenseNet models were trained with input images, and the weights of the core architecture were frozen. Then, the models were retrained after adding the new dense layer for all three DenseNet architectures. Categorical Cross Entropy loss was used to monitor model performance.</p><p>In the second stage, XGBoost was used. The feature vectors extracted from DenseNet models (DenseNet-121, DenseNet-169, and DenseNet-201) were fed into XGBoost. The predicted outputs were plotted on a Receiver Operator Characteristics (ROC) curve, and a probability threshold was determined. This threshold was used to truncate the predicted labels, resulting in the final output labels.</p><p>The categorical cross entropy is a metric that quantifies the disparity between two discrete probability distributions. The Softmax activation function is employed at the output layer to generate a probability distribution across all classes. Softmax is a mathematical function that transforms a vector of numbers into a vector of probabilities. Each probability corresponds to the relative magnitude of its corresponding value in the vector. In essence, it normalizes the outputs, converting them from weighted sums to probabilities that add up to one. Table <ref type="table" coords="8,500.89,445.00,5.10,9.74" target="#tab_1">2</ref> contains different training parameters used to train each model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>For the Caption Prediction subtask, the proposed idea involved using a pretrained InceptionV3 model which was fine-tuned as feature extractor to give the feature vectors as input to an InceptionLSTM model to generate the captions. Furthermore, the beam search algorithm was employed to explore and evaluate multiple potential sequences of words for generating captions. It involved considering different word combinations based on the model's predictions and a specified beam index. The aim was to identify the most probable caption by considering a limited number of top-scoring candidates at each step of the caption generation process.</p><p>The training workflow began by resizing them to a fixed size of 299x299 pixels to ensure consistency. The captions were tokenized into individual words and encoded into numerical sequences using a tokenizer. Two models were utilized in the training procedure: a pre-trained InceptionV3 convolutional neural network (CNN) and a Long Short-Term Memory (LSTM) recurrent neural networks. The pre-trained InceptionV3 model was employed to extract visual features from the input images. Each image was passed through the InceptionV3 model, and the output of the second-to-last layer was extracted as the visual feature representation. The training process involved generating captions for the input images. Initially, a start token was provided as the input to the LSTM model, which then predicted the next word in the caption sequence. This process was repeated iteratively, with the predicted word being fed back into the LSTM model as input for the next iteration. The objective was to maximize the probability of generating the ground truth captions for the given images. To train the LSTM model, a loss function was defined to measure the discrepancy between the predicted captions and the ground truth captions. The loss function utilized was typically the cross-entropy loss. The weights of the LSTM model were updated through backpropagation using an optimizer, such as Adam, to minimize the loss. The training procedure involved iterating over the entire dataset multiple times, known as epochs. In each epoch, the dataset was randomly shuffled to introduce diversity during training. The model was trained in mini-batches, where a subset of images and their corresponding captions were fed into the model simultaneously. The gradients computed during backpropagation were accumulated over the mini-batches, and the model's weights were updated after each batch. Various hyperparameters were tuned to optimize the performance of the model. These included the learning rate, batch size, number of LSTM units, and the maximum length of the generated captions. Different combinations of hyperparameters were experimented with to find the optimal configuration. To prevent overfitting and determine the optimal training stopping point, a validation set was utilized. After each epoch, the model's performance on the validation set was evaluated using evaluation metrics. If the model's performance on the validation set did not improve for a certain number of epochs, training was stopped early to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The concept detection sub-task gave a F1-score of 0.0173 and F1-score manual of 0.1172. The gradient boosting approach has a very low F1-score as the model predicted multiple CUIs for a single image hinting at the fact that all feature vectors extracted from the images were given equal importance. This can be overcome by choosing the most important feature vectors that represent the data and generating the appropriate CUIs by feeding them to the XGBoost model. The approach for caption prediction gave a BERTScore of 0.6019 which ranked seventh on the leaderboards. The authors were able to achieve a very high CLIPScore of 0.7759. CLIPScore tries to mimic human judgement and gives a score based on the compatibility of image and caption pair. A very low METEORScore of 0.0615 indicated that the generated captions had many grammatical errors and were not fluent. These shortcomings can be overcome by using a more well trained Language model which has been trained on more data and by fine tuning the architecture. Post editing and human feedback can also be given to train better Language models. To improve the approach, more complex deep learning architectures can be used for feature extraction and transformers can be deployed in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,441.36,310.83,105.76"><head>Table 1</head><label>1</label><figDesc>Caption prediction models training scores.</figDesc><table coords="6,195.45,472.95,204.38,74.16"><row><cell>Model</cell><cell cols="2">Epochs Accuracy</cell><cell>Loss</cell></row><row><cell>DenseNet121</cell><cell>85</cell><cell>0.212</cell><cell>65.361</cell></row><row><cell>DenseNet169</cell><cell>50</cell><cell>0.198</cell><cell>6752.75</cell></row><row><cell>DenseNet201</cell><cell>31</cell><cell>0.186</cell><cell>36.022</cell></row><row><cell>EffecientNetB2</cell><cell>28</cell><cell>0.191</cell><cell>77810.516</cell></row><row><cell>ResNet101</cell><cell>10</cell><cell>0.062</cell><cell>11320.178</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,485.64,416.99,103.51"><head>Table 2</head><label>2</label><figDesc>Model training parameters used to train each of the convolutional neural networks used for this classification task.</figDesc><table coords="8,158.87,526.94,277.53,62.21"><row><cell>Parameter</cell><cell cols="4">Optimizer Learning rate Batch size Epochs</cell></row><row><cell>DenseNet121</cell><cell>Adam</cell><cell>1ùëí-8</cell><cell>8</cell><cell>85</cell></row><row><cell>DenseNet169</cell><cell>Adam</cell><cell>1ùëí-8</cell><cell>8</cell><cell>50</cell></row><row><cell>DenseNet201</cell><cell>Adam</cell><cell>1ùëí-8</cell><cell>32</cell><cell>31</cell></row><row><cell>EffecientNetB2</cell><cell>Adam</cell><cell>1ùëí-5</cell><cell>8</cell><cell>28</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to express their gratitude to the <rs type="institution">Machine Learning Research Group (MLRG)</rs>, <rs type="institution">Department of Computer Science and Engineering, Sri Sivasubramaniya Nadar College of Engineering, Chennai, India</rs> (https://www.ssn.edu.in/) for providing the GPU resources for model training and testing.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,313.74,394.53,9.74;10,112.66,327.28,395.16,9.74;10,112.66,340.83,393.33,9.74;10,112.66,354.38,247.61,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,298.71,327.28,209.11,9.74;10,112.66,340.83,171.63,9.74">Overview of ImageCLEFmedical 2023 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,308.11,340.83,113.90,9.74">CLEF2023 Working Notes</title>
		<title level="s" coord="10,429.51,340.83,76.48,9.74;10,112.66,354.38,97.38,9.74">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,367.93,394.53,9.74;10,112.66,381.48,395.16,9.74;10,112.66,395.03,394.52,9.74;10,112.66,408.58,395.17,9.74;10,112.39,422.13,394.80,9.74;10,112.48,435.68,394.69,9.74;10,112.66,449.23,395.16,9.74;10,112.66,462.78,393.33,9.74;10,112.66,476.33,394.52,9.74;10,112.33,489.87,120.27,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,224.78,435.68,282.39,9.74;10,112.66,449.23,228.11,9.74">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcƒ±a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,364.12,449.23,143.70,9.74;10,112.66,462.78,393.33,9.74;10,112.66,476.33,136.27,9.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,280.08,476.33,221.53,9.74">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,503.42,395.16,9.74;10,112.34,516.97,393.64,9.74;10,112.66,530.52,393.32,9.74;10,112.14,544.07,198.62,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,306.47,516.97,199.51,9.74;10,112.66,530.52,144.36,9.74">Overview of ImageCLEFmedical 2022-caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,280.48,530.52,130.06,9.74">CEUR Workshop Proceedings</title>
		<title level="s" coord="10,479.21,530.52,26.76,9.74;10,112.14,544.07,100.46,9.74">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1294" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,557.62,393.31,9.74;10,112.66,571.17,258.51,9.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,188.91,557.62,317.06,9.74;10,112.66,571.17,51.86,9.74">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,173.16,571.17,98.78,9.74">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,584.72,394.53,9.74;10,112.66,598.27,370.61,9.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,296.48,584.72,206.31,9.74">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1411.4555" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,611.82,394.52,9.74;10,112.66,625.37,394.52,9.74;10,112.66,638.92,65.30,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,213.82,611.82,289.10,9.74">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.61,625.37,349.98,9.74">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,652.46,393.32,9.74;10,112.66,666.01,395.00,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,470.41,652.46,35.57,9.74;10,112.66,666.01,244.29,9.74">Medical image captioning via generative pretrained transformers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Selivanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">Y</forename><surname>Rogov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chesakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fedulova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,365.17,666.01,76.26,9.74">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4171</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,88.09,395.16,9.74;11,112.66,101.64,393.58,9.74;11,112.66,115.19,323.24,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,131.71,101.64,219.72,9.74">AUEB NLP Group at ImageCLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zachariadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Trakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,396.08,101.64,110.16,9.74;11,112.66,115.19,293.42,9.74">Proceedings of the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,128.74,393.32,9.74;11,112.66,142.29,393.32,9.74;11,112.66,155.84,300.22,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,334.00,128.74,171.97,9.74;11,112.66,142.29,34.80,9.74;11,175.85,142.29,178.09,9.74">CMRE-UoG team at ImageCLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Dalla</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,376.83,142.29,129.15,9.74;11,112.66,155.84,270.40,9.74">Proceedings of the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Concept Detection and Image Captioning</note>
</biblStruct>

<biblStruct coords="11,112.66,169.38,394.52,9.74;11,112.66,182.93,393.32,9.74;11,112.66,196.48,300.22,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,112.66,182.93,234.65,9.74">IUST_NLPLAB at ImageCLEFmedical Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajihosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lotfollahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nobakhtian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Javid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Omidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eetemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,371.16,182.93,134.82,9.74;11,112.66,196.48,270.40,9.74">Proceedings of the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,210.03,393.32,9.74;11,112.66,223.58,393.32,9.74;11,112.66,237.13,204.22,9.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,463.41,210.03,42.56,9.74;11,112.66,223.58,122.84,9.74">CSIRO at ImageCLEFmedical Caption</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Santa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Belous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,279.63,223.58,226.34,9.74;11,112.66,237.13,174.40,9.74">Proceedings of the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2022 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,250.68,393.33,9.74;11,112.66,264.23,393.33,9.74;11,112.66,277.78,394.65,9.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,439.41,250.68,66.57,9.74;11,112.66,264.23,315.77,9.74">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,452.23,264.23,53.75,9.74;11,112.66,277.78,296.73,9.74">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,291.33,394.61,9.74;11,112.66,304.88,395.01,9.74;11,112.41,318.43,185.00,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,297.69,291.33,188.68,9.74">Image captioning with semantic attention</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.503</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,136.59,304.88,322.29,9.74">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,331.98,394.53,9.74;11,112.66,345.52,393.32,9.74;11,112.66,359.07,271.35,9.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,480.16,331.98,27.03,9.74;11,112.66,345.52,311.56,9.74">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,447.58,345.52,58.40,9.74;11,112.66,359.07,141.18,9.74">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,372.62,394.53,9.74;11,112.66,386.17,393.32,9.74;11,112.66,399.72,393.57,9.74;11,112.66,413.27,98.03,9.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,112.66,386.17,315.03,9.74">Overview of the imageclefmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,451.11,386.17,54.88,9.74;11,112.66,399.72,361.22,9.74">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2021-09">September 2021, 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,426.82,393.32,9.74;11,112.66,440.37,393.32,9.74;11,112.66,453.92,393.32,9.74;11,112.66,467.47,394.52,9.74;11,112.66,481.02,394.52,9.74;11,112.66,494.57,382.85,9.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,262.51,426.82,243.47,9.74;11,112.66,440.37,393.32,9.74;11,112.66,453.92,86.05,9.74">SSN MLRG at imageclefmedical caption 2022: Medical concept detection and caption prediction using transfer learning and transformer based learning approaches</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-113.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,452.94,453.92,53.04,9.74;11,112.66,467.47,389.14,9.74">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="11,382.38,481.02,124.81,9.74;11,112.66,494.57,21.79,9.74">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1505" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,508.11,393.33,9.74;11,112.66,521.66,394.53,9.74;11,112.66,535.21,226.76,9.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,359.67,508.11,146.32,9.74;11,112.66,521.66,38.41,9.74">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.82,521.66,305.43,9.74">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,548.76,393.32,9.74;11,112.66,562.31,395.00,9.74;11,112.66,575.86,127.87,9.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,257.90,548.76,202.58,9.74">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,562.31,307.90,9.74">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,589.41,394.53,9.74;11,112.66,602.96,347.27,9.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,178.31,589.41,323.97,9.74">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.74,602.96,202.02,9.74">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,616.51,394.52,9.74;11,112.28,630.06,183.08,9.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,240.10,616.51,263.80,9.74">Ensembling neural networks: many could be better than all</title>
		<author>
			<persName coords=""><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,112.28,630.06,94.07,9.74">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,643.61,393.32,9.74;11,112.66,657.16,394.03,9.74;11,112.41,670.70,272.90,9.74" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xgboost</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145%2F2939672.2939785.doi:10.1145/2939672.2939785" />
		<title level="m" coord="11,270.86,643.61,235.12,9.74;11,112.66,657.16,240.76,9.74">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,88.09,393.33,9.74;12,112.66,101.64,393.32,9.74;12,112.66,115.19,182.19,9.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,344.34,88.09,161.65,9.74;12,112.66,101.64,89.15,9.74">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,229.22,101.64,276.76,9.74;12,112.66,115.19,84.28,9.74">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,128.74,393.33,9.74;12,112.66,142.29,283.88,9.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,295.97,128.74,210.02,9.74;12,112.66,142.29,70.43,9.74">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,191.91,142.29,130.83,9.74">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
