<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.70,82.86,437.18,16.56;1,89.30,104.70,433.92,16.56;1,89.30,128.20,342.17,12.00">A Dual of Stacked Attention Networks (SAN&apos;s) and VGG-16 Model-Based Visual Question Answering Evaluation Working notes for the ImageCLEFmed MEDVQA-GI Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.30,154.00,78.08,12.00"><forename type="first">Rohit</forename><forename type="middle">Raj</forename><surname>Gunti</surname></persName>
							<email>rgunti@vols.utk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tennessee</orgName>
								<address>
									<addrLine>1345 Circle Park Drive, 412 Communications Building</addrLine>
									<settlement>Knoxville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tennessee</orgName>
								<address>
									<addrLine>1345 Circle Park Drive, 451 Communications Building</addrLine>
									<settlement>Knoxville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.45,154.00,73.51,12.00"><forename type="first">Abebe</forename><surname>Rorissa</surname></persName>
							<email>arorissa@utk.edu</email>
						</author>
						<title level="a" type="main" coord="1,88.70,82.86,437.18,16.56;1,89.30,104.70,433.92,16.56;1,89.30,128.20,342.17,12.00">A Dual of Stacked Attention Networks (SAN&apos;s) and VGG-16 Model-Based Visual Question Answering Evaluation Working notes for the ImageCLEFmed MEDVQA-GI Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">62B26A4160E0DF868A3BAE691AF61F2E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MEDVQA</term>
					<term>Stacked Attention Networks</term>
					<term>VGG-16</term>
					<term>colonoscopy images</term>
					<term>CNN classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research aims to assess the number of open-source systems based on deep learning is growing, and various data preprocessing techniques are proposed. Considering the timeliness of Artificial Intelligence (AI) systems, particularly in terms of their immediate responsiveness and predictive capabilities, the evaluation of deep learning projects holds significant appeal for a diverse array of researchers, developers, and enthusiasts. The AI-based applications, like ChatGPT, draws interest due to their ability to rapidly generate informed responses and predictions, showcasing the potential for groundbreaking advancements in the field. The researchers can benefit from a wide selection of models and different data preprocessing methods without the need to start from scratch. Consequently, competitions like Medical Visual Question Answering for Gastrointestinal (MEDVQA-GI) Task, identifying Lesions in Colonoscopy images, organized by ImageCLEF Medical 2023, provide an opportunity for community-driven researchers to utilize multiple open-source algorithms such as Visual VGG-16 (Visual Geometry Group-16) Convolutional Neural Network model, and Long Short Term Memory (LSTM) models, enabling them to address complex challenges like identifying lesions in colonoscopy images effectively. Tasks like MEDVQA-GI allow participants to refine the literature and make the researchers work on new aspects of the field by adding multiple modalities to the picture. This study focuses on evaluating an open source system, namely Stacked Attention Networks for Image Question Answering, which utilizes a Task 1 approach, i.e., combines images and textual questions to generate textual answers, commonly applied in various research domains. The evaluation results, including assigned scores by the ImageClef MEDVQA-GI committee and other study observations, demonstrate that the selected system is highly suitable for Task 1. The system incorporates several preprocessing techniques, such as tokenization, word embedding using Word2Vec, preprocessing of questions and answers, question filtering, and feature extraction from images using the VGG16 model. Additionally, noteworthy observations were made regarding Task 2 (visual question generation) throughout the evaluation process. Overall, this research provides insights into the effectiveness of the Stacked Attention Networks for Image Question Answering open-source system for Task 1, highlighting the significance of the employed data preprocessing techniques and model selection. The findings contribute to the understanding of the capabilities of deep learning models and their applicability in addressing complex problems like identifying lesions in colonoscopy images. The results also offer valuable guidance to researchers, developers, and enthusiasts in choosing suitable open-source systems for their specific needs, saving them time and effort in model development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.56" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning-trained open-source systems have gained significant popularity due to their effectiveness in various domains and applications <ref type="bibr" coords="2,317.78,112.87,11.33,11.04" target="#b0">[1]</ref>. These systems leverage different data preprocessing techniques to improve training and accuracy. Evaluating these systems is crucial to attracting researchers, developers, and enthusiasts, providing them with a diverse range of models and data preprocessing methods without starting from scratch. Competitions like the Medical Visual Question Answering for GI task (MEDVQA-GI) <ref type="bibr" coords="2,426.93,166.99,11.41,11.04" target="#b1">[2]</ref>, organized by ImageCLEF medical <ref type="bibr" coords="2,180.52,180.55,13.15,11.04" target="#b2">[3]</ref> in 2023, offer a platform for community-driven researchers to utilize open-source algorithms, such as VGG-16, convolutional neural networks (CNNs), and LSTM models, to tackle complex challenges like identifying lesions in colonoscopy images.</p><p>This study focuses on the ImageCLEF MEDVQA-GI Task 1, which asks the participants to generate textual answers from image-question pairs <ref type="bibr" coords="2,330.22,248.74,11.33,11.04" target="#b1">[2]</ref>. The challenge primarily focuses on the application of image question-answering in the medical field, specifically in the domain of colonoscopy image analysis. The objective is to enhance the accuracy and usability of deep learning open-systems for identifying lesions in colonoscopy images. By incorporating multiple modalities such as visual question answering and visual question generation, the output of the analysis can be made more accessible to medical experts <ref type="bibr" coords="2,402.93,316.42,11.31,11.04" target="#b1">[2]</ref>.</p><p>In addition to Task 1, the team participating in this study also proposes solutions for Task 2, where textual questions should be created from image-answer pairs. This expanded scope allows for a comprehensive exploration of the challenges associated with colonoscopy image analysis and facilitates advancements in the field <ref type="bibr" coords="2,316.66,384.58,11.50,11.04" target="#b3">[4,</ref><ref type="bibr" coords="2,331.53,384.58,7.91,11.04" target="#b4">5,</ref><ref type="bibr" coords="2,342.92,384.58,7.67,11.04" target="#b5">6]</ref>. Furthermore, similar approaches combining images and textual answers have proven successful in various research domains, including coral ecology for coral identification <ref type="bibr" coords="2,305.04,411.60,13.02,11.04" target="#b6">[7]</ref> and synthetic aperture radar imagery to identify natural disasters <ref type="bibr" coords="2,210.11,425.16,11.33,11.04" target="#b7">[8]</ref>, among others. The paper <ref type="bibr" coords="2,350.68,425.16,12.48,11.04" target="#b3">[4]</ref> introduces SAN that extend the attention mechanism, successfully employed in image captioning and machine translation, to enable multi-step reasoning. The overall architecture of SAN is illustrated, consisting of three main components: the image model (utilizing a CNN to extract high-level image representations), the question model (employing a CNN or LSTM to extract a semantic vector of the question), and the stacked attention model (performing multi-step reasoning to locate image regions relevant to the question for answer prediction). The SAN operates by using the question vector to query the image vectors in the first visual attention layer. It combines the question vector and the retrieved image vectors to refine the query vector for querying the image vectors again in the second attention layer. The higher-level attention layer produces a more focused attention distribution, emphasizing regions more relevant to the answer. Finally, the image features from the highest attention layer are combined with the last query vector to predict the answer. This work makes three contributions. First, it proposes the stacked attention network as a solution for image QA tasks. Second, extensive evaluations on four image QA benchmarks demonstrate that the multiple-layer SAN outperforms previous stateof-the-art approaches by a significant margin. Third, the paper conducts a detailed analysis, visually showcasing the outputs of different attention layers of the SAN and illustrating the step-by-step process through which the SAN progressively focuses attention on relevant visual clues leading to the answer.</p><p>Yang et. al's work <ref type="bibr" coords="3,182.57,85.75,13.14,11.04" target="#b3">[4]</ref> (Stacked Attention Networks for Image Question Answering), shown in Figure <ref type="figure" coords="3,137.49,99.31,3.95,11.04" target="#fig_0">1</ref>, serves as the foundation for the study's investigation, training on different datasets, involving both text and images. In addition, the prior work <ref type="bibr" coords="3,424.24,112.87,13.20,11.04" target="#b5">[6]</ref> titled "A Dual Convolutional Neural Networks and regression model based Coral Reef Annotation and Localization" addresses the task of coral reef annotation and localization. Although the specific task differs from image question answering, it utilizes CNNs for image analysis and demonstrates the efficacy of combining CNNs with regression models. Additionally, Gunti and Rorissa's (2021) work <ref type="bibr" coords="3,309.81,473.52,13.36,11.04" target="#b4">[5]</ref> titled "A Convolutional Neural Networks based Coral Reef Annotation and Localization" explores the use of CNNs for coral reef annotation and localization tasks. While not directly related to image question answering, it aligns with the domain of image analysis and demonstrates the effectiveness of CNNs in similar contexts. Considering these works, our contribution lies in adapting the model architecture for multientry classification using categorical cross-entropy loss for Task 1 and Task 2. Furthermore, the preprocessing steps to train the model for virtual question generation tasks are updated to be compatible with the proposed multi-entry classification training.</p><p>By exploring these aspects and contributing to the existing literature <ref type="bibr" coords="3,444.47,595.47,12.57,11.04" target="#b3">[4,</ref><ref type="bibr" coords="3,461.38,595.47,8.38,11.04" target="#b4">5,</ref><ref type="bibr" coords="3,474.22,595.47,8.16,11.04" target="#b5">6]</ref>, this study aims to gain insights into the performance of sparse categorical cross-entropy and categorical cross-entropy loss functions, as well as the potential of the proposed model for virtual question generation tasks. Previous works in related domains, such as coral reef annotation and localiza-tion utilizing CNNs and VGG, have been referenced to inform the training configuration adaptations and accuracy improvements made in this investigation -While the open-source system originally utilizes an Attention Network, the literature opts for a Dense network with a tanh activation function based on alternative implementations of the SAN.</p><p>Moreover, the research aims to investigate the feasibility of using the proposed model for training virtual question generators. Several contributions have been made to existing works in this regard. Firstly, the model architecture has been modified to enable training using "categorical_cross_entropy" for multi-entry classification in Task 1 and Task 2. Secondly, adjustments and attempts have been made to the data preprocessing steps to train the model for virtual question generation. Additionally, various combinations of learning rates, dropout rates, L1 and L2 normalization strengths, and model architectures have been employed to train and save the model, which is available on GitHub 2 .</p><p>In this working notes paper, we assess the selected system's performance on solving Task 1, as well as we make observations related to Task 2. The testing has been performed by the ImageClef MEDVQA-GI committee <ref type="bibr" coords="4,251.49,133.99,11.38,11.04" target="#b1">[2]</ref>, assigning accuracy scores to each prediction and system based on its performance. Additionally, supporting observations are made throughout the evaluation process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and methods</head><p>In this study, two types of data were utilized: annotated data for developing the solution used for training and testing and test data without annotations for the final evaluation of the proposed solution.</p><p>To investigate the effectiveness of the system, the Hyper Kvasir dataset <ref type="bibr" coords="4,487.78,386.86,19.94,11.04" target="#b11">[12]</ref> (datasets.simula.no/hyper-Kvasir) was employed. This dataset was augmented with question-and-answer ground truth developed in collaboration with medical partners. It comprises a wide range of images covering the entire gastrointestinal tract, spanning from the mouth to the anus. The dataset encompasses various conditions, including abnormalities, surgical instruments, and normal findings, obtained from different procedures like gastroscopy, colonoscopy, and capsule endoscopy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>Radiology images <ref type="bibr" coords="4,189.30,509.88,12.57,11.04" target="#b8">[9]</ref> play a crucial role in clinical decision-making and population screening, particularly for conditions like cancer. To assist clinicians in managing large volumes of images, automated systems that can answer questions about image contents have gained prominence. Visual Question Answering (VQA) in the medical domain is an emerging field of artificial intelligence that explores approaches to this form of clinical decision support. Before the competition challenge, the VQA-RAD dataset <ref type="bibr" coords="4,391.70,577.71,12.53,11.04" target="#b7">[8]</ref> was experimented with as the first manually constructed dataset, where clinicians asked naturally occurring questions about radiology images and provided reference answers. The images and questions were manually categorized, offering insights into clinically relevant tasks and the appropriate natural language to phrase them. Through evaluation with well-known algorithms, the superior quality of this dataset over automatically constructed ones is demonstrated. VQA tools that focus on improving patient care. By utilizing this dataset, the study can develop and refine algorithms that effectively address clinical/colon challenges and enhance medical decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>× × ×</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approach</head><p>Trained the models with two different preprocessing approaches: • train.json is the manipulated gt.json file annotated separately with 36863 entries separated by every question_id (qid) ranging from 1 -36863 as represented in Figure <ref type="figure" coords="5,173.81,508.92,4.19,11.04" target="#fig_2">2</ref>.</p><p>The Stacked Attention Networks for Image Question Answering system incorporates several preprocessing techniques, such as tokenization, word embedding using Word2Vec, preprocessing of questions and answers <ref type="bibr" coords="5,203.81,555.12,16.69,11.04" target="#b9">[10]</ref>, question filtering, and feature extraction from images using the VGG16 model <ref type="bibr" coords="5,152.36,568.68,11.37,11.04" target="#b5">[6]</ref>. The workflow of the chosen evaluated open-source system involves several steps, including loading GoogleNews vectors, creating an h5 file containing question vectors and labels, tokenizing questions and converting them into feature vectors, converting answers into labels, and storing the data in the h5 format. Furthermore, images are preprocessed using VGG16 preprocessing layers to obtain dimensions of 14 14 512, which are subsequently reshaped to 196 512 <ref type="bibr" coords="5,186.89,636.75,12.05,11.04" target="#b3">[4]</ref>.</p><p>The model architecture, shown in Figure <ref type="figure" coords="5,284.59,649.95,3.95,11.04" target="#fig_0">1</ref>, consists of passing the question layer through a Long Short-Term Memory (LSTM) and the preprocessed image through a dense network with a tanh/ReLU activation function <ref type="bibr" coords="6,223.37,107.59,16.02,11.04" target="#b10">[11]</ref>. The resulting vectors are concatenated and passed through additional dense layers, followed by a final layer with a softmax activation function. For better performance <ref type="bibr" coords="6,146.66,134.71,17.98,11.04" target="#b10">[11]</ref> in multi-class classification (training the data as group entries -gt.json) using categorical cross-entropy loss function, the softmax activation function is replaced with ReLU, where the categorical cross-entropy loss function is applied, the softmax activation function is replaced with relu <ref type="bibr" coords="6,172.75,175.39,16.77,11.04" target="#b10">[11]</ref>.</p><p>The sparse categorical cross-entropy loss function for single-entry classification (train.json) and categorical cross-entropy loss function for multi-entry classification (gt.json) tasks with 901 unique answer labels was considered for training in Task 1. Additionally, a comparison of training history is made between models trained with "sparse_categorical_cross_entropy" and "categorical_cross_entropy" loss functions <ref type="foot" coords="6,285.43,244.36,3.86,6.96" target="#foot_0">1</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and discussion</head><p>The evaluation results demonstrate that the selected system is highly suitable for Task 1 singleentry training. From the below ImageCLEF MEDVQA-GI evaluation, the model evaluated has decent scores overall score as shown in Table <ref type="table" coords="7,304.63,413.88,4.19,11.04" target="#tab_0">1</ref>, which satisfies RQ1.</p><p>Overall, this study sheds light on the capabilities of deep learning models and their applicability in addressing complex challenges <ref type="bibr" coords="7,255.29,440.88,11.75,11.04" target="#b1">[2,</ref><ref type="bibr" coords="7,269.45,440.88,7.67,11.04" target="#b6">7]</ref>. It highlights the significance of data preprocessing techniques and model selection in achieving high performance. The results not only contribute to the understanding of image analysis but also offer practical guidance for those interested in utilizing open-source systems for similar tasks, ultimately facilitating advancements in the field. The findings of this research provide guidance to researchers, developers, and enthusiasts in selecting appropriate open-source systems for their specific needs. By leveraging pre-existing models available on Drive, they can save time and effort in model development. The effectiveness of the Stacked Attention Networks for the Image Question Answering system in Task 1 underscores the importance of utilizing proper data preprocessing techniques and training histories. The evaluation metrics presented in Table <ref type="table" coords="8,332.71,180.55,5.79,11.04" target="#tab_1">2</ref> demonstrate promising performance across various measures, including F1-score, accuracy, recall, Matthews correlation coefficient (MCC), and mean Intersection over Union (mIOU). These metrics collectively indicate a positive outcome for the model under evaluation. However, upon closer examination, it is apparent that the reported Dice coefficient, which is typically utilized to assess segmentation algorithms, deviates significantly from the expected range of 0 to 1. The Dice coefficient is calculated using a formula that compares the predicted segmentation to the ground truth segmentation, and a value of 1 signifies a perfect match.</p><p>The Dice coefficient value of 328.0456197082703 reported in the results is clearly outside the acceptable range and raises concerns about the accuracy of its computation. Consequently, this discrepancy necessitates further investigation to ascertain the reason behind this anomalous calculation, such as potential errors or inaccuracies in the implementation. Although the other metrics indicate favorable performance, the unreliable Dice coefficient warrants a comprehensive examination and potential refinement of the calculation method. It is essential to address this discrepancy in upcoming competitions of MEDVQA to ensure the reliability and validity of the reported results.</p><p>The data preprocessing techniques (train.json) and model selection play a significant role in achieving this performance. Moreover, valuable insights are gained regarding Task 2 and Task 3, reflecting on RQ3, and contributing to a better understanding of the capabilities of deep learning models in addressing complex problems like identifying lesions in colonoscopy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑑𝑒 = 𝑀 𝑑𝑒([𝑒, 𝑎𝑔𝑒], [ = 𝑎])</head><p>(1)</p><formula xml:id="formula_0" coords="8,186.53,497.66,320.66,44.93">𝑑𝑒 = 𝑀 𝑑𝑒([𝑎, 𝑎𝑔𝑒], [ = 𝑒]) (2) 𝑑𝑒 = 𝑀 𝑑𝑒([𝑎, 𝑎𝑔𝑒], [ = 𝑒])<label>(3)</label></formula><p>The model equation for training, as represented by model = Model([ques, images], [out]) as represented in equation ( <ref type="formula" coords="8,221.14,564.48,4.20,11.04">1</ref>), signifies that the model is being trained by optimizing the parameters of the neural network based on the provided inputs (questions and images) to predict the output (answer probabilities).</p><p>Assumptions and attempts are made for training Task 2 using equation (2)</p><p>• It assumes that the answers (ans) are provided as input, along with the images (images), to train the model. • The model aims to predict the questions (ques) corresponding to the given answers and images.</p><p>• This implies that there is a relationship between the provided answers and the target questions, which the model is expected to learn during training.</p><p>Assumptions and attempts are made for training Task 3 using equation (3)</p><p>• It assumes that masks (masks) are provided as input along with the images (images) for training the model. • The model is designed to predict the questions (ques) based on the given masks and images. • This implies that there is an underlying connection between the provided masks and the target questions, which the model is intended to capture during the training process.</p><p>In both cases, the model combined with the provided inputs and outputs, aims to learn the associations between the given data and the target questions. The training process involves adjusting the model's parameters to minimize the discrepancy between the predicted questions and the ground truth questions for the given inputs. To facilitate further implementation, the source code attempts for Task 2 and Task 3 is readily accessible on GitHub<ref type="foot" coords="9,472.18,275.44,3.86,6.96" target="#foot_2">3</ref> .</p><p>Based on our findings, we conclude that from Task 1, we propose to apply the following two first equations as future work for improving the results on Task 1 and 2. Our team did not participate in Task 3 of the challenge, which asked to generate image segmentations from pairs of images and textual questions. For future work, we would still test equation 3 for solving Task 3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.30,443.41,289.43,9.68;3,107.25,182.84,434.95,248.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stacked Attention Network for ImageClef 2023 MedVQA-GI.</figDesc><graphic coords="3,107.25,182.84,434.95,248.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.54,188.11,132.93,11.04;4,89.30,215.23,241.82,11.04;4,89.30,237.79,287.15,11.04;4,89.30,260.26,413.16,11.04;4,116.66,273.94,246.87,11.04"><head></head><label></label><figDesc>Research questions addressed. RQ1 How effective is the proposed model for Task 1? RQ2 Is data manipulation effective during training and testing? RQ3 Is the investigation leading to the feasibility of the proposed model for the training virtual question generator and image segmentation, Task 2?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,101.30,199.88,404.80,9.68;5,101.30,212.23,28.29,9.21;5,101.20,83.25,451.80,111.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: left side shows the gt.json format (multi-entry) right side shows the train.json format (single entry).</figDesc><graphic coords="5,101.20,83.25,451.80,111.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,103.70,350.14,402.45,11.04;5,116.66,363.58,342.81,11.04;5,131.30,379.90,307.76,11.04;5,149.66,393.70,337.07,11.04;5,149.66,408.00,358.77,11.04;5,161.06,421.56,87.49,11.04;5,103.58,438.84,403.10,11.04;5,116.42,452.40,389.92,11.04;5,116.42,465.96,322.19,11.04"><head>1 . 2 . 2 .</head><label>122</label><figDesc>Extracted the image, question, and answer vectors from the input JSON, as shown in the left side of Figure 2, format provided by the ImageCLEF MEDVQA -gt.json • gt.json is the JSON file with 2000SAN entries each with two values: -ImageID -example training Image name -"clb0lbwzadoyc086u0brshvx5." -Labels -consist of 18 questions with AnswerType and Answer values as represented in Figure Extracted the image, question, and answer vectors as the individual entries summing up to 36683 entries, as shown in the right side of Figure 2, from the manually manipulated JSON format as demonstrated by the open-source system -train.json:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,89.06,88.52,354.38,82.04"><head>Table 1</head><label>1</label><figDesc>MedVQA-GI committee evaluated scores.</figDesc><table coords="7,153.74,120.07,289.71,50.49"><row><cell>Task</cell><cell>Metric</cell><cell>Score</cell></row><row><cell>Global metrics</cell><cell>Accuracy</cell><cell>0.441</cell></row><row><cell cols="3">Question-based metric Average Accuracy of all 18 questions 0.463</cell></row><row><cell>Image-based metric</cell><cell>Average Accuracy of images</cell><cell>0.441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,89.06,191.72,377.66,118.91"><head>Table 2</head><label>2</label><figDesc>The evaluation of a single-entry classification by employing sparse categorical cross-entropy.</figDesc><table coords="7,232.73,224.23,132.09,86.40"><row><cell>Metric</cell><cell>Score</cell></row><row><cell cols="2">F1-Score 0.9097830620656927</cell></row><row><cell cols="2">Accuracy 0.9152173913043479</cell></row><row><cell>Recall</cell><cell>0.9152173913043479</cell></row><row><cell>MCC</cell><cell>0.9040461706444576</cell></row><row><cell>MIOU</cell><cell>0.8516313376491703</cell></row><row><cell>Dice</cell><cell>328.0456197082703</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,92.54,275.65,425.29,9.00;6,92.66,286.57,36.23,9.00"><p>https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-overthe-other</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,92.66,480.63,168.78,9.00"><p>https://github.com/rohitgunti/MEDVQA-GI</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,92.42,726.42,158.27,9.00"><p>https://github.com/rohitgunti/MEDVQA-GI</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,92.66,737.45,147.68,9.00"><p>https://github.com/uakarsh/med-vqa</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to express their deepest gratitude to Akarsh 4 , whose invaluable contributions and expertise have been instrumental in the understanding and implementation of the model.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.34,491.88,395.28,11.04;9,112.34,505.44,395.45,11.04;9,112.34,520.79,92.73,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,170.81,491.88,336.81,11.04;9,112.34,505.44,168.19,11.04">Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42979-021-00815-1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,288.79,505.44,97.54,11.04">SN Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">420</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.70,532.56,395.14,11.04;9,112.70,546.12,394.73,11.04;9,112.70,559.56,395.02,11.04;9,112.70,573.12,63.83,11.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,464.86,532.56,42.98,11.04;9,112.70,546.12,390.86,11.04">Overview of imageclef medical 2023 -medical visual question answering for gastrointestinal tract</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.73,559.56,113.42,11.04">CLEF2023 Working Notes</title>
		<title level="s" coord="9,247.34,559.56,172.87,11.04">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.70,586.71,394.52,11.04;9,112.70,600.27,395.68,11.04;9,112.70,613.83,394.25,11.04;9,112.70,627.39,395.73,11.04;9,112.46,640.95,394.70,11.04;9,112.46,654.51,395.48,11.04;9,112.34,668.07,396.09,11.04;9,112.34,681.63,395.66,11.04;9,112.34,695.19,395.58,11.04;9,112.34,708.75,128.29,11.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,222.73,654.51,285.22,11.04;9,112.34,668.07,231.52,11.04">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.76,668.07,143.67,11.04;9,112.34,681.63,395.66,11.04;9,112.34,695.19,134.96,11.04">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,281.50,695.19,221.24,11.04">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,85.75,393.49,11.04;10,112.70,99.31,393.79,11.04;10,112.70,112.87,137.41,11.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,293.70,85.75,212.49,11.04;10,112.70,99.31,46.51,11.04">Stacked attention networks for image question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,182.79,99.31,323.70,11.04;10,112.70,112.87,51.67,11.04">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,126.43,393.96,11.04;10,112.70,139.99,289.35,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,205.49,126.43,301.17,11.04;10,112.70,139.99,50.37,11.04">A convolutional neural networks based coral reef annotation and localization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gunti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rorissa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,189.65,139.99,102.93,11.04">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1229" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,153.55,395.70,11.04;10,112.70,167.11,395.68,11.04;10,112.70,180.67,395.19,11.04;10,112.70,194.11,395.38,11.04;10,112.70,207.67,96.12,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,228.77,153.55,279.63,11.04;10,112.70,167.11,203.63,11.04">A dual convolutional neural networks and regression model based coral reef annotation and localization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Guntia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rorissaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,337.82,167.11,170.56,11.04;10,112.70,180.67,395.19,11.04;10,112.70,194.11,153.88,11.04">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="10,274.85,194.11,184.79,11.04">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,221.23,394.87,11.04;10,112.70,234.79,395.01,11.04;10,112.70,248.38,132.01,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,413.47,221.23,94.09,11.04;10,112.70,234.79,199.53,11.04">Imageclefcoral task: coral reef image annotation and localisation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.33,234.79,130.86,11.04">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1318" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,122.78,261.94,383.62,11.04;10,112.70,275.50,389.23,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,308.83,261.94,197.57,11.04;10,112.70,275.50,238.70,11.04">Fully convolutional neural network for rapid flood segmentation in synthetic aperture radar imagery</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nemni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Belabbes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bromley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.75,275.50,68.60,11.04">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2532</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.46,289.06,395.54,11.04;10,112.46,302.62,395.58,11.04;10,112.46,316.06,126.93,11.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,362.94,289.06,145.06,11.04;10,112.46,302.62,239.66,11.04">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.251</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,360.43,302.62,64.55,11.04">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180251</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,329.62,394.71,11.04;10,112.70,343.30,395.01,11.04;10,112.70,358.53,95.25,8.76" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,271.97,329.62,235.44,11.04;10,112.70,343.30,136.95,11.04">The impact of preprocessing on word embedding quality: A comparative study</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Homayounpour</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-022-09620-5</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,258.29,343.30,91.15,11.04">Lang. Resour. Eval</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="257" to="291" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.70,370.30,393.91,11.04;10,112.70,383.86,122.54,11.04" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m" coord="10,184.37,370.30,241.06,11.04">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.70,397.42,393.65,11.04;10,112.70,411.00,393.61,11.04;10,112.70,424.56,393.56,11.04;10,112.70,438.00,258.45,11.04" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,434.57,411.00,71.74,11.04;10,112.70,424.56,388.30,11.04">HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borgli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Eskeland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Randel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T D</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-020-00622-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.70,438.00,63.65,11.04">Scientific data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
