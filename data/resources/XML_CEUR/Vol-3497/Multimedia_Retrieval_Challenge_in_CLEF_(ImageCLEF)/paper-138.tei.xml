<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.19,375.28,15.42;1,89.29,106.11,397.21,15.42;1,89.29,128.02,292.56,15.43">PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,156.34,74.72,11.96"><forename type="first">Viktor</forename><surname>Schlegel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.55,156.34,32.76,11.96"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,226.95,156.34,55.08,11.96"><forename type="first">Yuping</forename><surname>Wu</surname></persName>
							<email>yuping.wu@manchester.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.67,156.34,100.29,11.96"><forename type="first">Anand</forename><surname>Subramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.93,170.29,102.43,11.96"><forename type="first">Thanh-Tung</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.01,170.29,127.92,11.96"><forename type="first">Abhinav</forename><forename type="middle">Ramesh</forename><surname>Kashyap</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.57,170.29,58.75,11.96"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.97,170.29,66.71,11.96"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,184.24,143.01,11.96"><forename type="first">Riza</forename><forename type="middle">Theresa</forename><surname>Batista-Navarro</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.94,184.24,73.72,11.96"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.94,184.24,73.64,11.96"><forename type="first">Goran</forename><surname>Nenadic</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.19,375.28,15.42;1,89.29,106.11,397.21,15.42;1,89.29,128.02,292.56,15.43">PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">2B6415E58D74D9CE9AB881EE89826973</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Abstractive Summarisation</term>
					<term>AI for Healthcare</term>
					<term>Dialogue Summarisation</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes PULSAR, our system submission at the ImageClef 2023 MediQA-Sum task on summarising patient-doctor dialogues into clinical records. The proposed framework relies on domainspecific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM. We find limited evidence towards the efficacy of domain-specific pre-training and data augmentation, while scaling up the language model yields the best performance gains. Our approach was ranked second and third among 13 submissions on task B of the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the recent successes of generative large language models (LLMs) on a variety of tasks <ref type="bibr" coords="1,493.22,456.74,12.77,10.91" target="#b0">[1]</ref> and domains <ref type="bibr" coords="1,151.58,470.29,11.58,10.91" target="#b1">[2]</ref>, even in the face of data scarcity <ref type="bibr" coords="1,322.19,470.29,11.58,10.91" target="#b2">[3]</ref>, there is vivid interest in identifying potential application scenarios that could benefit from the power of LLMs. One of the promising domains is healthcare <ref type="bibr" coords="1,190.06,497.39,12.95,10.91" target="#b3">[4]</ref> as many administrative tasks involve the transformation of textual data. LLM-based approaches that assist hospital staff in repetitive administrative tasks have the potential to improve operational efficiency and documentation quality, optimise revenue streams, reduce cognitive load on healthcare experts, and ultimately result in better and more effective patient care <ref type="bibr" coords="1,184.72,551.58,11.43,10.91" target="#b4">[5]</ref>.</p><p>A range of different scenarios have been investigated for the suitability of LLM-based assistance, such as summarising patient progress notes as discharge summaries <ref type="bibr" coords="1,428.23,578.68,12.94,10.91" target="#b5">[6]</ref> or identifying problems that need treatment during a patient's hospital course <ref type="bibr" coords="1,373.66,592.23,11.38,10.91" target="#b6">[7]</ref>. One of the potential tasks is summarising doctor-patient dialogue as medical records <ref type="bibr" coords="1,358.46,605.78,11.58,10.91" target="#b7">[8]</ref>. Dialogue summarisation, an established task in the Natural Language Processing (NLP) community, aims to identify salient topics in a multi-turn dialogue <ref type="bibr" coords="2,225.76,100.52,11.31,10.91" target="#b8">[9]</ref>. State-of-the-art approaches typically formulate the problem as abstractive summarisation, making the task a prime candidate for further investigation of the potential of LLMs in clinical settings. In this scenario, conversations between patients and doctors need to be transformed into (excerpts of) clinical documentation. For example, if a 27 year old female patient mentions that they are experiencing "Sore throat, runny nose, dry cough and fever 37.5 ‚àò C", the corresponding entry can be the "Subjective" section of a medical record excerpt, e.g., "Patient is a 27 year old female who presents with sore throat, runny nose dry cough and a fever of 37.5 ‚àò C. " This documentation is typically performed by the consulting doctor or an attending nurse. Despite bearing potential impact for automation, with clinical staff spending at least 35 minutes of their time every other day on writing such clinical notes <ref type="bibr" coords="2,89.29,236.01,16.32,10.91" target="#b9">[10]</ref>, this task was underexplored by the NLP community, compared to other hospital-related tasks, such as clinical coding <ref type="bibr" coords="2,219.86,249.56,16.44,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,239.02,249.56,12.33,10.91" target="#b11">12]</ref>, or generating radiology reports <ref type="bibr" coords="2,401.11,249.56,16.26,10.91" target="#b12">[13]</ref>. More recently, the task has received more attention <ref type="bibr" coords="2,234.76,263.11,16.12,10.91" target="#b13">[14]</ref>, however studies thus far have either focussed on narrow department selections <ref type="bibr" coords="2,192.69,276.66,16.55,10.91" target="#b14">[15,</ref><ref type="bibr" coords="2,212.43,276.66,12.42,10.91" target="#b15">16]</ref>, did not focus on medical documentation generation <ref type="bibr" coords="2,472.62,276.66,16.42,10.91" target="#b16">[17]</ref>, or have not released their data publicly <ref type="bibr" coords="2,253.88,290.20,16.25,10.91" target="#b17">[18]</ref>.</p><p>To that end, the ImageClef 2023 MediSum shared task released a collection of dialogues and corresponding clinical notes in an effort to spark interest and advance the state of the art in dialogue as clinical note summarisation <ref type="bibr" coords="2,301.04,330.85,11.58,10.91" target="#b7">[8]</ref>. The task revolves around three core subtasks: (A) identifying the topic of a conversation from a selection of possible medical note sections (i.e., "Subjective" in the previous example), (B) summarising conversation snippets to appropriate sections in medical records, and, finally, (C) summarising full conversations to full medical records. While conversations are synthetic, the corresponding clinical notes are real, doctor-written documentation.</p><p>Our guiding objective to participate in this task was to investigate, how well a recently proposed LLM training framework can generalise to new tasks with as little adaptation as possible <ref type="bibr" coords="2,127.40,439.25,16.15,10.91" target="#b18">[19]</ref>. At its core, the framework (i) fine-tunes a LLM with a pre-training objective that learns to reconstruct a pseudo-summary consisting of automatically extracted medical terms and (ii) employs data augmentation (DA) by instructing Black-Box LLMs to obtain task-specific training data. As such, the DA framework supports any LLM, such as Bloom <ref type="bibr" coords="2,434.02,479.89,16.30,10.91" target="#b19">[20]</ref>, GPT-3 <ref type="bibr" coords="2,488.02,479.89,17.96,10.91" target="#b20">[21]</ref> or GPT-3.5 <ref type="bibr" coords="2,139.78,493.44,16.25,10.91" target="#b21">[22]</ref>.</p><p>Our submission for task B was ranked second best overall among all participants. Although we have not actively sought to compete in Task C, we observed that our data augmentation technique could improve the performance, particularly when the training data is scarce. These findings underline the potential of LLMs in various settings as well as the generalisability of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Definition</head><p>In this section, we describe and formalise the three tasks of the ImageClef 2023 MediSum challenge.</p><p>Task A -Dialogue2Topic Classification In this task, participants need to identify the topic of a conversation. The list of possible topics corresponds to the 20 different fine-grained sections that can be part of a medical record, such as "Subjective", i.e., the subjective description of symptoms by the patient.</p><p>Task B -Dialogue2Note Summarization Here, participating systems need to convert a conversation on a specific topic into a corresponding section in the medical record. This task can be regarded as conditional generation, sequence-to-sequence translation or abstractive summarisation. Approaches are evaluated on multiple natural language generation metrics, both based on n-gram overlap, i.e., ROUGE <ref type="bibr" coords="3,287.07,210.57,16.41,10.91" target="#b22">[23]</ref>, as well as semantic similarity <ref type="bibr" coords="3,446.30,210.57,16.56,10.91" target="#b23">[24,</ref><ref type="bibr" coords="3,465.64,210.57,12.42,10.91" target="#b24">25]</ref> Task C -Full-Encounter Dialogue2Note Summarization This task is formulated similarly to Task B, however here, the inputs are full notes and the evaluated systems need to generate medical record outputs for the four general sections "Subjective", "Objective Exam", "Objective Results" and "Assessment and Plan". This task features only 67 training and 20 validation examples, with 40 examples reserved for testing. The systems are evaluated based on their output for each of the sections using the ROUGE metrics from Task B; the results are averaged across all sections. An alternative mode of evaluation combines all outputs into one single record and measures the n-gram overlap by means of the ROUGE score.</p><p>The tasks appear to be arranged as a progression, where, given a dialogue, a segmentation and classification model could segment the topics of the conversation (Task A) to be used as input for a Dialogue Snippet Summarisation Model (Task B), the output of which can be arranged as a full medical record (Task C). However, as our goal was to evaluate how well the proposed framework generalises to the tasks with as little adaptation as possible, we decide not to make any task-specific adaptations even if they could provide beneficial given the particular arrangement of the tasks. Thus, we do not rely on any additional information, treat tasks B and C in isolation, and disregard task A for not being a generative task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Language model Pre-training</head><p>Motivated by the success of predicting masked words <ref type="bibr" coords="3,334.08,550.92,18.05,10.91" target="#b25">[26]</ref> and contiguous spans <ref type="bibr" coords="3,455.47,550.92,18.05,10.91" target="#b26">[27]</ref> as selfsupervised training objectives, we customised the pre-training objectives for the medical domain generation task to concatenate "gap text spans (sentences)" into a pseudo-summary. Each masked span is a medical term from the input text identified by the QuickUMLS <ref type="bibr" coords="3,465.97,591.57,18.07,10.91" target="#b27">[28]</ref> or a NER model fine-tuned on a N2C2 dataset (i2b2-2010 challenge <ref type="bibr" coords="3,370.09,605.12,15.76,10.91" target="#b28">[29]</ref>). Specifically, as shown in Figure <ref type="figure" coords="3,121.13,618.67,3.81,10.91">1</ref>, pre-training consisted of three different policies: first, when both QuickUMLS and N2C2 NER models identified entities, the QuickUMLS results were used in 70% of cases and the results of the N2C2 NER model were used in 30%. Second, when only one of them predicted any output, that output was used for masking. Third, when neither had any output, then 15% of the sentences were masked at random. These text spans were replaced with "sentinel" mask tokens &lt; ùëíùë•ùë°ùëüùëé ùëñùëë ùëñ &gt; to inform the model that input was masked. In order to provide the model with sufficient medical knowledge, we used the MIMIC-III <ref type="bibr" coords="4,334.20,369.41,16.41,10.91" target="#b10">[11]</ref>, a pre-trained corpus of 2 million data, which consists of a large number of clinical records, such as admission notes, discharge summaries or lab results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augmentation (DA)</head><p>Both tasks suffer from scarcity of training data, specifically Task C, which requires generating comprehensive clinical notes based on lengthy patient-doctor conversations based on only 67 training examples. These may be insufficient to train a model capable of performing well on the task. To address this issue, we adopt data augmentation to generate additional examples for training, as this has been shown to improve performance in data-scarce scenarios <ref type="bibr" coords="4,454.66,500.43,16.43,10.91" target="#b29">[30,</ref><ref type="bibr" coords="4,473.82,500.43,12.32,10.91" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompting Strategy</head><p>We observed that Large Language Models (LLMs) such as ChatGPT are proficient in understanding clinical context and manipulating clinical data. Therefore, we utilise a pre-existing LLM to generate data for the model's training. Ideally, the data generation approach would involve providing conversations and requesting the LLM to produce the corresponding medical note. However, we are limited by the fact that we only have 67 full-length conversations in our dataset. Nonetheless, we have access to a significantly larger number of medical notes. Hence, we invert the task by prompting the LLM with a medical note (or its snippet) and ask it to generate a hypothetical conversation between the doctor and the patient. We then use the generated conversations as input to train our model to produce the corresponding clinical note.</p><p>We employ the OpenAI ChatGPT API (gpt-35-turbo) for data augmentation, utilising a two-stage prompting strategy to generate data effectively. In the first stage, we use in-context learning with one-shot prompting to prompt the LLM to generate a fictitious conversation between the doctor and patient based on the medical note, while adhering to important guidelines. We provide only one example picked from the training set as we are limited by token context windows for the API. In the second stage (only performed for task C), we prompt the model to include conversational fillers such as "ums", "uh", and "hmm" to the generated conversation from the first stage, as we noticed that the model did not include these fillers despite our instructions in the first stage.</p><p>Dataset Utilised For task B, we extract matching subsection headings from the MIMIC-III database <ref type="bibr" coords="5,131.80,223.51,16.41,10.91" target="#b10">[11]</ref>, adapting the pre-processing method from Yang et al. <ref type="bibr" coords="5,401.35,223.51,17.91,10.91" target="#b31">[32]</ref> to identify section headers. We rank the generations based on their average Rouge similarity to all training instances and pick the top-scoring ùëõ conversations.</p><p>For task C, we utilise a corpus of freely available medical notes scraped from MTSamples, which is available on Kaggle<ref type="foot" coords="5,215.91,275.95,3.71,7.97" target="#foot_0">1</ref> . Since the dataset contains medical transcriptions of notes from various medical specialities, we devise a method to pick samples from the dataset that are the closest to the medical notes in our training set. To do this, we identify and curate a list of the section headers in the training set through a heuristic approach by exploiting the fact that section headers are usually written in all capital letters. We split the document by newline and extract the lines which are fully upper-cased and add these contents to our list of section headers. We then score the medical notes in MTSamples based on the number of headers that each document has based on the curated list from the previous step and pick the top ùëõ documents from MTSamples with the highest scores to use as input for DA. We end up with a corpus of 746 data samples due to the fact that some inputs were flagged as offensive by OpenAI's content moderating policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments set-up</head><p>We aim to empirically evaluate, how well our framework can solve the problem of converting patient dialogues to medical records. We pursue the following questions:</p><p>(i) How well can our proposed approach convert doctor-patient dialogues to Medical Records? (ii) Does the domain-specific pre-training objective improve performance? (iii) What is the impact of model scale on the performance? (iv) Does synthetic data augmentation improve performance on the tasks?</p><p>To answer question (i) we empirically evaluate our proposed framework on the task B and C test sets of the ImageClef Challenge. For evidence towards question (ii), we compare the performance of PULSAR to that of equally-sized Flan-t5 models. Regarding question (iii), we compare the performance of variously sized models of the same architecture and for question (iv), we compare the performance of models trained on available data only to those fine-tuned on synthetically generated conversation data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Pre-training PULSAR-* is initialised with weights from the corresponding Flan-t5-* models <ref type="bibr" coords="6,103.82,401.91,17.78,10.91" target="#b32">[33]</ref> and pre-trained with four NVIDIA Tesla A100 80GB GPUs for 1 epoch on all MIMIC-III notes. Huggingface Accelerate is used to optimise GPU memory usage with the Fully Sharded Data Parallel (FSDP) paradigm. We set the training batch size per GPU device as 4 and the gradient accumulation step as 8 to accelerate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head><p>We fine-tune all models for 3 epochs. We experiment with encoder-decoder Flan-T5, PULSAR and Clinical-T5 <ref type="bibr" coords="6,267.33,484.86,19.23,10.91" target="#b33">[34]</ref>models, with the configurations *-Large (0.9B Parameters), *-3B and *-11B. Unless stated otherwise, the models are trained on two A100 80GB GPUs with cumulative batch size of 8 and the learning rate of 3 -5 . For the largest of them, i.e., (Flan-t5-11B and PULSAR-11B), we use FSDP with CPU offloading. We also experiment with a decoder-only model, LLAMA-13B, freezing and quantising the base model in eight bit <ref type="bibr" coords="6,488.23,539.06,17.75,10.91" target="#b34">[35]</ref> and using the parameter-efficient LoRA <ref type="bibr" coords="6,267.74,552.61,17.88,10.91" target="#b35">[36]</ref> method. More details on hyper-parameter choice are reported in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and analysis</head><p>At a glance, Table <ref type="table" coords="6,176.80,615.88,5.17,10.91" target="#tab_0">1</ref> shows the results of our empirical study and Table <ref type="table" coords="6,426.27,615.88,5.17,10.91" target="#tab_1">2</ref> shows the final ranking of all participating systems according to the official evaluation by task organisersIn the following, we discuss our findings in context of the questions outlined in the motivation of this empirical study. Our approach generalises well to the dialogue summarisation task. Overall, our approach generalises well to Task B, with our best model (Table <ref type="table" coords="7,355.49,332.72,3.66,10.91" target="#tab_0">1</ref>, 11B1) surpassing the 50 Rouge-1 scores mark, which means that on average, half of the prediction tokens are found in reference and vice versa. The high Rouge-L score of 44 suggests that most of these overlapping tokens indeed form a sequence. However, these scores may be "boosted" by the presence of many short target sequences in the evaluation data, such as "Noncontributory." or "No known allergies.", when a dialogue revolves around a topic that does not contribute to the patients' hospital visit. We find that the utilising the outputs of task A-the section headers-does not contribute to improving the overall performance, compare Table <ref type="table" coords="7,319.51,427.56,3.76,10.91" target="#tab_0">1</ref>, L2 and L4. We observed the same trend across all model sizes (not reported here for brevity).</p><p>In the absence of established baselines, we interpret the official rankings of the shared task in Table <ref type="table" coords="7,115.79,468.21,5.07,10.91" target="#tab_1">2</ref> as additional evidence towards the success of our approach.</p><p>There is no conclusive evidence that domain-specific pre-training is beneficial. Comparing 11B1 and 11B2, and 3B1 and 3B2 in Table <ref type="table" coords="7,299.44,510.52,3.66,10.91" target="#tab_0">1</ref>, respectively, we observe that domain-specific pre-training by learning to predict missing medical terms in MIMIC-III notes appears not beneficial, with the gap being smaller for bigger models. One possible reason for this is the domain mismatch between pre-training and application data. MIMIC-III is dominated by inpatient progress notes which track the patients' status along the hospital stay and contain abbreviations, repetitions, incomplete sentences and medical jargon. Conversely, the medical records in the challenge are well-written and stem most likely from admission notes or outpatient encounters, where most of the initial documentation about a new patients' particulars, such as their chief complaint, medical history and drug allergies happens. Additionally, input dialogues have a colloquial tone, further adding to the domain mismatch between pre-training and fine-tuning.</p><p>Model scale yields the biggest performance improvements. Comparing L*, 3B* and 11B* results in Table <ref type="table" coords="8,158.94,100.52,3.68,10.91" target="#tab_0">1</ref>, we can see a clear trend where larger models of the same family consistently perform better. The biggest hike in performance is observed between the 3B and 11B models. This observation is in line with most literature on model scale as driver of performance and the reason for emergent abilities in LLMs <ref type="bibr" coords="8,258.70,141.16,16.25,10.91" target="#b36">[37]</ref>.</p><p>We also find that the model trained with adapters can learn to perform on the task successfully, despite the relatively small (around 1.1% of the full 7B model) number of trainable parameters. However, our results suggest that updating all models' parameters is more effective, as even smaller models outperform the 7B adapter model (Table <ref type="table" coords="8,340.39,195.36,3.74,10.91" target="#tab_0">1</ref>, L2, 3B* compared to 7B1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation can be helpful if training data is extremely scarce.</head><p>Larger models obtain enough signal from the training data of Task B, as there is no clear improvement in scores for the 3B models (Table <ref type="table" coords="8,228.78,251.22,3.68,10.91" target="#tab_0">1</ref>, 3B1 vs. 3B3 and 3B2 vs. 3B4). Meanwhile, data augmentation can lead to consistent, albeit minor, improvements for smaller models (Table <ref type="table" coords="8,447.06,264.77,3.81,10.91" target="#tab_0">1</ref>, L2 vs. L3). When training data is scarce (i.e., Task C) data augmentation helps with the performance. Subjectively, models exhibit typical generation errors such as hallucination and input copying, (see Figure <ref type="figure" coords="8,139.80,305.41,5.05,10.91" target="#fig_2">2</ref> in Appendix) and data augmentation seems to alleviate this issue. Quantitatively, data augmentation improves performance across all metrics (27.64 vs 29.41 R1, 9.79 vs 11.60 R2, 16.24 vs 19.18 RL and 23.63 vs 26.08 RLSum without and with DA, respectively). We find the results promising, as the optimised model seems to perform well without any task-specific adaptation. Ultimately, however, this simple approach does not compete with other, potentially task specific information exploiting submissions, with the best of them scoring almost 20 Rouge-1 points higher (20.32 R2, 24.30 RL and 45.06 RLSum).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present an LLM framework and adapt it to the task of dialogue note summarisation. While we find that the approach generalises well to this new task, there is mixed evidence of the efficacy of both domain-specific pre-training and data augmentation. Our experiments seem to align with the "bitter lesson of AI"<ref type="foot" coords="8,273.98,484.23,3.71,7.97" target="#foot_1">2</ref> , in that model scale seems to trump domain-specific adaptations. This, in turn, supports the narrative of the transformative potential of LLMs in healthcare <ref type="bibr" coords="8,138.45,513.08,16.25,10.91" target="#b37">[38]</ref>, as larger LLMs become more readily available.</p><p>Our findings suggest further avenues for future work: We argued that the pre-training objective may suffer from domain mismatch. As such, experimenting with other domainspecific objectives might improve the performance of the downstream tasks. Furthermore, it is unclear how the choice of hyper-parameters for both training and inference stages (i.e., decoding arguments) impacts the overall performance. Finally, we have left it for future work to investigate, whether data augmentation could provide beneficial with a more advanced filtering strategy, for example by only augmenting examples with certain length or specific section headers. As such, we will expand the work reported in this paper by experimenting with different pre-training objectives, performing a more rigorous hyper-parameter optimisation and investigating the impact of data augmentation more closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The results described in this paper should be interpreted within the following context:</p><p>‚Ä¢ The language of the conversations is English. Due to the dominance of English data during pre-training, it is expected that all LLMs that we inspected perform better on English. It is unclear how well the approach will transfer to other languages. ‚Ä¢ The conversations are synthetic in that they have been written based on existing medical notes, rather than transcribed from real patient-doctor dialogues. While the quality has been evaluated by medical professionals, it is unclear how well the performance would translate to real-world scenarios. ‚Ä¢ The obtained results should be regarded as preliminary, as robust empirical results such as hyper-parameter optimisation for fine-tuning, pre-training policy selection, exhaustive search for best-performing prompts for data augmentation and strategies for data selection are often impossible given the time constraints of academic challenges and shared tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyper-parameters for training and inference</head><p>We initialise LoRA with ùëü = 16, ùõº = 16 on the query, key, value and output projection weights of all layers of the base model (ùëÑ, ùêæ, ùëâ and ùëÇ, respectively). The model is trained on a single A100 80GB GPU with a learning rate of 3 -4 for the adapter weights. For both encoder-decoder and decoder only settings, during training, we optimise the parameters of the language models to minimise the cross-entropy loss between each token of the prediction and the corresponding token of the ground truth answer sequence using teacher forcing. For encoder-decoder models, we limit the length of input dialogues to at most 496 and the length of output notes to at most 214 tokens, respectively (95th percentile). For the decoder model, we limit the length of input and output combined to at most 696 tokens. During inference, we set no limits to input and output sequence lengths and decode the prediction using beam search with 6 (4 for LLaMa), temperature of 1.0, top k of 50 (40 for LLaMa) and top p of 1.0 (0.7 for LLaMa).</p><p>For task C, we use the same arguments as for task C, with the exception of limiting the input length to 2048 and output length to 990 during training, in order to fit the GPU during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative example</head><p>Figure <ref type="figure" coords="12,120.61,332.50,5.12,10.91">1</ref> shows qualitative examples generated by our models trained on task C training data with and without data augmentation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flan-T5-3B</head><p>Flan-T5-3B-746DG Alexander is a 62-year-old male, with a past medical history significant for reflux, who presents for follow-up of his chronic problems. He is so thankful you put him on that medicine for his reflux. The protonix that he had, wmade an amazing change in his life. He's really comfortable now. He eats whatever he wants, and he feels so much better. The doctor is glad to hear that. Okay. How are you doing, kind of, managing your diet? I know, you have to do some lifestyle modifications, like cutting back on caffeine and spicy foods and alcohol. How are you doing with that? [patient] I'm doing really well. I moved over from caffeine, over to green tea. [doctor] Okay. <ref type="bibr" coords="12,238.45,550.27,35.58,8.96">[patient]</ref> and it, it is so, m-it doesn't cause as much problem as it did with, when he was drinking so many energy drinks a day... HISTORY OF PRESENT ILLNESS: Alexander is a 62-year-old male with a past medical history significant for reflux. He presents for follow-up of his chronic problems. He has a good support system at home. He has a big family. All his kids call and check on him every day. He does not have any symptoms of chest pain, shortness of breath, belly pain, nausea or vomiting. He does not have any symptoms of nausea or vomiting. On physical examination, I do not hear any carotid bruits in his neck. I do hear a slight 2/6 systolic ejection murmur, which I've heard in the past, so that's stable. His lungs are nice and clear, and he does have 1+ pitting edema bilaterally in his lower extremities. I think he is doing a good job watching his diet. He could just be retaining a little bit of fluid, maybe just from standing all day. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,478.06,210.57,28.18,10.91;3,89.29,224.12,365.54,10.91"><head></head><label></label><figDesc>. 1201 training and 100 validation examples are provided. 200 examples form the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,268.84,416.69,8.93;4,89.29,282.62,416.95,4.79;4,89.29,294.57,416.69,4.79;4,89.29,306.53,247.95,4.79"><head>Infant remains on prong [MASK 1 , 2 ] of 5 .Figure 1 :</head><label>1251</label><figDesc>Figure 1:Example of the pre-train objective of PULSAR. Both Masked Language Model (MLM) and Gap Sentences Generation (GSG) have been employed in this scenario. Red and orange arrows exemplify the UMLS and N2C2 MLM masking strategy, respectively. Meanwhile, the black arrow shows the GSG masking strategy, where a whole sentence has been masked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,89.29,604.13,418.35,8.93;12,89.29,617.91,245.87,4.79"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example excerpts of outputs for Task C produced by models with and without data augmentation. An instance of input copying is highlighted in italics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.97,90.49,418.67,238.69"><head>Table 1</head><label>1</label><figDesc>Validation set performance as measured by {1,2}-gram overlap Rouge-{1,2} and longest sequence overlap Rouge-L Rouge-LSum. Models with the *-ùëõDG were augmented with ùëõ synthetic examples. The *header suffix denotes that the section header was used as input.</figDesc><table coords="6,163.63,142.04,268.02,187.14"><row><cell>ID</cell><cell>Setting</cell><cell></cell><cell cols="2">Rouge</cell><cell></cell></row><row><cell></cell><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>RLSum</cell></row><row><cell></cell><cell>11B and 7B models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">11B2 PULSAR-11B</cell><cell cols="3">49.20 22.25 41.36</cell><cell>45.57</cell></row><row><cell cols="2">11B1 Flan-T5-11B</cell><cell cols="3">50.75 27.92 44.93</cell><cell>47.58</cell></row><row><cell>7B1</cell><cell>LLaMA-7B-LoRA</cell><cell>39.3</cell><cell>15.7</cell><cell>33.1</cell><cell>33.1</cell></row><row><cell></cell><cell>3B models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3B4</cell><cell>PULSAR-3B-735DG</cell><cell cols="3">37.89 17.83 30.40</cell><cell>34.76</cell></row><row><cell>3B3</cell><cell>Flan-T5-3B-735DG</cell><cell cols="3">41.44 19.05 33.93</cell><cell>38.41</cell></row><row><cell>3B2</cell><cell>PULSAR-3B</cell><cell cols="3">37.92 16.64 30.64</cell><cell>34.58</cell></row><row><cell>3B1</cell><cell>Flan-T5-3B</cell><cell cols="3">41.91 19.41 33.76</cell><cell>38.04</cell></row><row><cell></cell><cell cols="2">Large models (&lt; 1B parameters)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L4</cell><cell cols="4">Flan-T5-large-header 38.70 16.82 31.85</cell><cell>36.22</cell></row><row><cell>L3</cell><cell cols="4">Flan-T5-large-1000DG 39.41 17.04 31.85</cell><cell>36.78</cell></row><row><cell>L2</cell><cell>Flan-T5-large</cell><cell cols="3">39.27 17.42 31.95</cell><cell>36.49</cell></row><row><cell>L1</cell><cell>Clinical-T5-large</cell><cell cols="3">19.11 8.34 16.31</cell><cell>17.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,418.52,202.03"><head>Table 2</head><label>2</label><figDesc>Test set performance as measured by {1,2}-gram overlap Rouge-{1,2}, longest sequence overlap Rouge-L and Rouge-LSum and semantic similarity metrics BertScore-F1 and Bleurt; ranked by their aggregation.</figDesc><table coords="7,127.08,130.08,341.11,162.43"><row><cell>submission</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell cols="4">RLSum BS-F1 Bleurt Agg</cell></row><row><cell>SuryaKiran_run3</cell><cell cols="3">43.98 18.44 35.01</cell><cell>35.01</cell><cell>72.31</cell><cell cols="2">55.67 57.32</cell></row><row><cell>Flan-T5-11B</cell><cell cols="3">42.99 20.04 35.69</cell><cell>35.69</cell><cell>72.18</cell><cell>55.49</cell><cell>56.89</cell></row><row><cell>PULSAR-11B</cell><cell cols="3">41.78 19.25 34.16</cell><cell>34.16</cell><cell>72.11</cell><cell>55.52</cell><cell>56.47</cell></row><row><cell>Tredence_run1</cell><cell cols="2">42.44 17.24</cell><cell>35.3</cell><cell>35.3</cell><cell>72.07</cell><cell>53.3</cell><cell>55.94</cell></row><row><cell>SuryaKiran_run2</cell><cell cols="2">42.09 18.83</cell><cell>34.2</cell><cell>34.2</cell><cell>71.37</cell><cell>54.23</cell><cell>55.90</cell></row><row><cell>SuryaKiran_run1</cell><cell cols="3">40.56 17.59 32.72</cell><cell>32.72</cell><cell>71.09</cell><cell>53.24</cell><cell>54.96</cell></row><row><cell>LLaMA-7B-LoRA</cell><cell>38.15</cell><cell>17.3</cell><cell>31.42</cell><cell>31.42</cell><cell>71.77</cell><cell>51.52</cell><cell>53.82</cell></row><row><cell>HuskyScribe_run1</cell><cell cols="3">37.67 15.04 31.26</cell><cell>31.26</cell><cell>70.54</cell><cell>50.37</cell><cell>52.86</cell></row><row><cell>Tredence_run2</cell><cell cols="3">36.21 13.84 29.66</cell><cell>29.66</cell><cell>68.82</cell><cell>47.29</cell><cell>50.77</cell></row><row><cell>uetcorn_run1</cell><cell cols="3">29.11 10.73 22.94</cell><cell>22.94</cell><cell>65.85</cell><cell>49.42</cell><cell>48.13</cell></row><row><cell>uetcorn_run2</cell><cell cols="3">28.75 10.69 23.09</cell><cell>23.09</cell><cell>65.96</cell><cell>49.22</cell><cell>47.98</cell></row><row><cell>uetcorn_run3</cell><cell>28.72</cell><cell>10.7</cell><cell>23.04</cell><cell>23.04</cell><cell>65.92</cell><cell>49.13</cell><cell>47.92</cell></row><row><cell cols="4">SKKU-DSAIL_run1 26.03 11.31 18.22</cell><cell>18.22</cell><cell>59.29</cell><cell>53.05</cell><cell>46.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,92.57,671.04,390.58,8.97"><p>https://mtsamples.com/ and https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions, respectively</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,92.57,671.04,218.57,8.97"><p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,330.85,394.53,10.91;9,112.28,344.40,394.91,10.91;9,112.66,357.95,308.17,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,213.11,344.40,289.74,10.91">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.29,357.95,242.71,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,371.50,395.17,10.91;9,112.66,385.05,393.33,10.91;9,112.66,398.60,279.14,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,365.76,371.50,142.07,10.91;9,112.66,385.05,153.59,10.91">Large language models are fewshot clinical information extractors</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,288.90,385.05,217.08,10.91;9,112.66,398.60,181.13,10.91">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1998" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,394.53,10.91;9,112.66,425.70,393.32,10.91;9,112.66,439.25,266.90,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,274.48,425.70,169.72,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,452.29,425.70,53.69,10.91;9,112.66,439.25,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,394.53,10.91;9,112.66,466.34,393.60,10.91;9,112.66,479.89,146.44,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<title level="m" coord="9,249.16,466.34,223.02,10.91">Large language models encode clinical knowledge</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,493.44,393.33,10.91;9,112.66,506.99,71.06,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,315.41,493.44,109.96,10.91">Ai in health and medicine</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,433.69,493.44,72.30,10.91">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,395.17,10.91;9,112.66,534.09,393.33,10.91;9,112.66,547.64,327.13,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,295.96,520.54,211.87,10.91;9,112.66,534.09,393.33,10.91;9,112.66,547.64,85.12,10.91">Discharge summary hospital course summarisation of in patient electronic health record text with clinical concept guided deep pre-trained transformer models</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,206.59,547.64,150.17,10.91">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page">104358</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,561.19,393.33,10.91;9,112.66,574.74,393.33,10.91;9,112.33,588.29,29.19,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,286.42,561.19,219.57,10.91;9,112.66,574.74,63.21,10.91">Bionlp workshop 2023 shared task 1a: Problem list summarization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dligach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.45,574.74,311.92,10.91">Proceedings of the 22nd Workshop on Biomedical Language Processing</title>
		<meeting>the 22nd Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.84,393.32,10.91;9,112.66,615.39,394.53,10.91;9,112.66,628.93,394.53,10.91;9,112.66,642.48,58.60,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,375.45,601.84,130.53,10.91;9,112.66,615.39,389.97,10.91">Overview of the mediqa-sum task at imageclef 2023: Summarization and classification of doctor-patient conversations</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.07,628.93,113.05,10.91">CLEF 2023 Working Notes</title>
		<title level="s" coord="9,247.43,628.93,172.42,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,656.03,393.73,10.91;9,112.66,669.58,341.81,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,224.12,656.03,282.27,10.91;9,112.66,669.58,36.68,10.91">A survey on dialogue summarization: Recent advances and new frontiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.97,669.58,252.37,10.91">CLEF 2023 Working Notes, CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,395.17,10.91;10,112.66,100.52,393.33,10.91;10,112.28,114.06,135.89,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,354.33,86.97,153.50,10.91;10,112.66,100.52,180.74,10.91">Use of electronic clinical documentation: time spent and team interactions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hripcsak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Vawdrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Bostwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,302.31,100.52,203.67,10.91;10,112.28,114.06,51.96,10.91">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="112" to="117" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.53,10.91;10,112.66,141.16,394.53,10.91;10,112.66,154.71,121.12,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,290.79,141.16,211.96,10.91">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,154.71,62.55,10.91">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,394.53,10.91;10,112.66,181.81,393.33,10.91;10,112.66,195.36,107.17,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13998</idno>
		<title level="m" coord="10,112.66,181.81,316.81,10.91">Mimic-iv-icd: A new benchmark for extreme multilabel classification</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.71,10.91;10,112.66,222.46,268.41,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,282.47,208.91,223.89,10.91;10,112.66,222.46,27.24,10.91">Deep learning in generating radiology reports: A survey</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M A</forename><surname>Monshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,148.36,222.46,149.67,10.91">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">101878</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,395.01,10.91;10,112.66,263.11,48.96,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,277.15,236.01,228.84,10.91;10,112.66,249.56,111.91,10.91">An empirical study of clinical note generation from doctor-patient encounters</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,247.58,249.56,214.67,10.91">EACL, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2283" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.33,10.91;10,112.66,290.20,352.41,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,198.44,276.66,307.55,10.91;10,112.66,290.20,218.65,10.91">Automatically generating psychiatric case notes from digital transcripts of doctor-patient conversations using text mining</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kahanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,340.12,290.20,52.25,10.91">PeerJ Prepr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">27497</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,394.52,10.91;10,112.34,317.30,393.64,10.91;10,112.66,330.85,393.33,10.91;10,112.66,344.40,243.65,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,208.16,317.30,297.82,10.91;10,112.66,330.85,127.08,10.91">Generating medical reports from patient-doctor conversations using sequence-to-sequence models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Enarvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amoia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-A</forename><surname>Teba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Delaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,262.18,330.85,243.81,10.91;10,112.66,344.40,165.75,10.91">Proceedings of the First Workshop on Natural Language Processing for Medical Conversations</title>
		<meeting>the First Workshop on Natural Language Processing for Medical Conversations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,395.17,10.91;10,112.66,371.50,393.33,10.91;10,112.14,385.05,388.90,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,257.27,357.95,250.56,10.91;10,112.66,371.50,255.22,10.91">Towards automating medical scribing: Clinic visit dialogue2note sentence alignment and snippet summarization</title>
		<author>
			<persName coords=""><forename type="first">W.-W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen-Yildiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,391.00,371.50,114.99,10.91;10,112.14,385.05,311.00,10.91">Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations</title>
		<meeting>the Second Workshop on Natural Language Processing for Medical Conversations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.33,10.91;10,112.66,412.15,387.76,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08666</idno>
		<title level="m" coord="10,341.15,398.60,164.84,10.91;10,112.66,412.15,205.77,10.91">summarize: Global summarization of medical dialogue by exploiting local structures</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,425.70,394.53,10.91;10,112.66,439.25,393.53,10.91;10,112.66,452.79,393.33,10.91;10,112.66,466.34,133.86,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,258.76,439.25,247.43,10.91;10,112.66,452.79,393.33,10.91;10,112.66,466.34,29.25,10.91">Pulsar: Pre-training with extracted healthcare terms for summarising patients&apos; problems and data augmentation with black-box large language models</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Batista-Navarro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nenadic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,479.89,394.53,10.91;10,112.66,493.44,393.33,10.91;10,112.66,506.99,207.59,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iliƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Castagn√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gall√©</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m" coord="10,226.71,493.44,279.27,10.91;10,112.66,506.99,24.89,10.91">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,394.53,10.91;10,112.66,534.09,394.53,10.91;10,112.66,547.64,394.53,10.91;10,112.66,561.19,394.53,10.91;10,112.66,574.74,260.24,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,112.66,574.74,172.82,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,394.53,10.91;10,112.66,601.84,393.33,10.91;10,112.66,615.39,395.01,10.91;10,112.66,628.93,59.11,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,225.45,601.84,280.54,10.91;10,112.66,615.39,37.81,10.91">Training language models to follow instructions with human feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,173.91,615.39,234.70,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,642.48,393.33,10.91;10,112.66,656.03,133.03,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,154.48,642.48,243.31,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.57,642.48,86.42,10.91;10,112.66,656.03,55.58,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,669.58,394.62,10.91;11,112.66,86.97,394.53,10.91;11,112.66,100.52,90.72,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,250.65,669.58,233.34,10.91">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,86.97,390.38,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,393.33,10.91;11,112.66,127.61,383.20,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,384.35,114.06,121.63,10.91;11,112.66,127.61,90.07,10.91">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,225.34,127.61,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,395.17,10.91;11,112.66,154.71,393.97,10.91;11,112.66,168.26,28.67,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="11,392.45,141.16,115.38,10.91;11,112.66,154.71,198.89,10.91">Spanbert: Improving pretraining by representing and predicting spans</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,320.00,154.71,149.72,10.91">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,394.53,10.91;11,112.66,195.36,395.01,10.91;11,112.66,208.91,152.96,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="11,112.66,195.36,350.58,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,471.76,195.36,35.91,10.91;11,112.66,208.91,48.65,10.91">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,393.33,10.91;11,112.66,236.01,232.03,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,226.45,222.46,279.53,10.91;11,112.66,236.01,43.20,10.91">Quickumls: a fast, unsupervised approach for medical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,178.74,236.01,73.58,10.91">MedIR workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,394.53,10.91;11,112.66,263.11,364.35,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="11,326.84,249.56,180.35,10.91;11,112.66,263.11,122.44,10.91">i2b2/va challenge on concepts, assertions, and relations in clinical text</title>
		<author>
			<persName coords=""><forename type="first">√ñ</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">R</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Duvall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,242.91,263.11,150.16,10.91">J. Am. Medical Informatics Assoc</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="552" to="556" />
			<date type="published" when="2010">2010. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,395.17,10.91;11,112.66,290.20,394.52,10.91;11,112.66,303.75,65.30,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="11,211.13,276.66,234.38,10.91">Generating datasets with pretrained language models</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,468.83,276.66,39.00,10.91;11,112.66,290.20,364.75,10.91">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6943" to="6951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,393.60,10.91;11,112.66,330.85,393.33,10.91;11,112.66,344.40,107.17,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Batista-Navarro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Nenadic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16000</idno>
		<title level="m" coord="11,348.07,317.30,158.20,10.91;11,112.66,330.85,314.38,10.91">Do you hear the people sing? key point analysis via iterative clustering and abstractive summarisation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,357.95,393.33,10.91;11,112.66,371.50,387.57,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="11,349.06,357.95,156.93,10.91;11,112.66,371.50,205.21,10.91">Knowledge injected prompt based fine-tuning for multi-label few-shot icd coding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03304</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,385.05,395.17,10.91;11,112.66,398.60,393.32,10.91;11,112.66,412.15,107.17,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m" coord="11,227.28,398.60,205.78,10.91">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,425.70,394.53,10.91;11,112.28,439.25,393.71,10.91;11,112.66,452.79,107.17,10.91" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08091</idno>
		<title level="m" coord="11,233.79,439.25,194.31,10.91">Do we still need clinical language models?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,466.34,393.33,10.91;11,112.66,479.89,288.63,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
		<title level="m" coord="11,335.19,466.34,170.80,10.91;11,112.66,479.89,106.92,10.91">Llm. int8 (): 8-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,493.44,395.17,10.91;11,112.66,506.99,393.33,10.91;11,112.66,520.54,99.15,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="11,456.57,493.44,51.26,10.91;11,112.66,506.99,190.01,10.91">Lora: Lowrank adaptation of large language models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.53,506.99,174.45,10.91;11,112.66,520.54,69.13,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,534.09,394.53,10.91;11,112.66,547.64,393.33,10.91;11,112.66,561.19,163.42,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="11,232.76,547.64,192.86,10.91">Emergent abilities of large language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,434.41,547.64,71.57,10.91;11,112.66,561.19,123.70,10.91">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,574.74,394.53,10.91;11,112.66,588.29,393.59,10.91;11,112.66,601.84,146.44,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-W</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11568</idno>
		<title level="m" coord="11,112.66,588.29,358.39,10.91">Large ai models in health informatics: Applications, challenges, and the future</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
