<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,370.13,15.42;1,88.78,106.66,338.13,15.42;1,89.29,128.58,212.21,15.43;1,89.29,150.91,300.74,11.96">Analyzing the Similarity between Artificial and Training Images in Generative Models: The PicusLabMed Contribution Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
				<funder ref="#_dXFxaNr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,81.00,11.96"><forename type="first">Michela</forename><surname>Gravina</surname></persName>
							<email>michela.gravina@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio 21</addrLine>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.44,176.82,81.73,11.96"><forename type="first">Stefano</forename><surname>Marrone</surname></persName>
							<email>stefano.marrone@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio 21</addrLine>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.68,176.82,70.06,11.96"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
							<email>carlo.sansone@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio 21</addrLine>
									<postCode>80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,370.13,15.42;1,88.78,106.66,338.13,15.42;1,89.29,128.58,212.21,15.43;1,89.29,150.91,300.74,11.96">Analyzing the Similarity between Artificial and Training Images in Generative Models: The PicusLabMed Contribution Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">11AD8D62F34E4BC13E8A75D715CD6038</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Networks</term>
					<term>Image Similarity</term>
					<term>Generative Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative models represent one of the most innovative and interesting applications of artificial intelligence (AI), able to generate realistic synthetic data by learning the characteristics of the training samples. In medical imaging, they are widely used to generate high-resolution medical images belonging to different modalities, improving diagnosis and patient care. However, the surprising performance of generative models has raised concerns about the relationship between artificial and real instances in terms of similarity, which may introduce privacy and ethical issues. To this aim, the ImageCLEFmed GAN challenge has been organized, asking participants to evaluate the hypothesis that generative models produce images containing the fingerprints of the samples used during the training. In this paper, we describe the methodology implemented to take part in the competition, exploiting the ability of deep neural networks to provide a high-level representation of the input data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the emergence of generative models in the field of Artificial Intelligence (AI) has sparked significant interest and innovation. These models, powered by advanced Machine Learning (ML) algorithms, have the remarkable ability to generate new synthetic data samples, through a synthesis process that learns the characteristics of the distribution of the training dataset. Generative models, such as variational autoencoders (VAEs) <ref type="bibr" coords="1,391.40,508.50,11.30,10.91" target="#b0">[1]</ref>, generative adversarial networks (GANs) <ref type="bibr" coords="1,169.76,522.05,11.43,10.91" target="#b1">[2]</ref>, and diffusion models <ref type="bibr" coords="1,283.28,522.05,11.43,10.91" target="#b2">[3]</ref>, have shown immense potential across various domains.</p><p>In the realm of medical imaging, AI generative models represent a real revolution by enabling the generation of synthetic images with exceptional realism and accuracy. These models can generate high-resolution images that mimic various medical imaging modalities, such as computed tomography (CT), magnetic resonance imaging (MRI), and X-ray scans. The application of AI generative models in medical imaging holds significant implications for enhancing diagnostic accuracy, data augmentation for training Deep Learning (DL) algorithms, and even aiding in the development of novel imaging techniques. However, as with any advanced technology, it becomes crucial to address the considerations related to data privacy, ethical implications, challenges associated with their use, and the interpretability of the generated images.</p><p>In this context, the ImageCLEF2023 <ref type="bibr" coords="2,269.77,195.36,12.99,10.91" target="#b3">[4]</ref> conference organized the ImageCLEFmed GAN challenge <ref type="bibr" coords="2,135.59,208.91,11.58,10.91" target="#b4">[5]</ref>, focused on examining the hypothesis of an existing relationship between the synthetic images and the samples used in the training of the generative model. The competition resonates in the scientific community since, if this hypothesis is correct, synthetic biomedical images may be subject to the same sharing and usage limitations as real data, while, on the other hand, generative networks confirm their potential to create rich datasets free of ethical and privacy regulations. The aim of the task is to identify in the artificially created biomedical images distinctive patterns or characteristics known as "fingerprints", that help in determining the set of images employed during the training phase of the generative model.</p><p>In this paper, we present the methodology implemented to take part in the ImageCLEFmed GAN challenge <ref type="bibr" coords="2,161.52,330.85,11.59,10.91" target="#b4">[5]</ref>. In particular, we exploit the ability of the DL models to provide a representation of the input data relying on Convolutional Neural Networks (CNNs) to extract the features from the real and generated images. These features represent the fingerprints that we then analyze, adopting a ML model for the identification of the samples used during the development of the generative model among all the real instances. We propose two variants for the features extraction step, introducing Vector-Net, a convolutional network that learns how to map the input image in an efficient representation, and leveraging a Deforming Autoencoder (DAE) <ref type="bibr" coords="2,119.18,425.70,11.43,10.91" target="#b5">[6]</ref>, that provides a latent vector in an unsupervised manner.</p><p>The rest of the paper is organized as follows: Section 2 introduces the implemented methodology; Section 3 descrives the experimental set-up; Section 4 reports the obtained results; finally Section 5 provides some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In image processing, the generative model, or generator, is typically a deep network that learns how to map a fixed latent distribution to the distribution of real data ğ‘ ğ‘Ÿ . Denoting with ğ‘ ğ‘” the distribution of the artificial instances, the goal is to learn ğ‘ ğ‘” which approximates ğ‘ ğ‘Ÿ as closely as possible <ref type="bibr" coords="2,127.19,565.62,12.70,10.91" target="#b1">[2]</ref> with the aim of creating new samples preserving the intrinsic characteristics of the real ones. In the ImageCLEFmed GAN <ref type="bibr" coords="2,262.66,579.17,12.86,10.91" target="#b4">[5]</ref> challenge we are asked to evaluate the relationship between ğ‘ ğ‘” and ğ‘ ğ‘Ÿ , investigating the possibility to distinguish among all the images belonging to ğ‘ ğ‘Ÿ , the subset used during the training of the generative network. In other words, we asses the partition of the set of real data ğ‘… into two subsets, ğ‘ˆ and ğ‘ ğ‘ˆ , corresponding to the images used (ğ‘ˆ ) and not used (ğ‘ ğ‘ˆ ) during the development of the model, respectively.</p><p>In our methodology, we rely on CNNs to extract the features from the real and generated images, thanks to their ability to autonomously learn the data representations well-suited for the specific task to be solved. In particular, we explore two different approaches, where the former leverages the features extracted by Vector-Net, a convolutional network aware bout the hypothesis to be tested, while the latter considers the latent representation provided by a Deforming Autoencoder (DAE) <ref type="bibr" coords="3,230.36,127.61,12.81,10.91" target="#b5">[6]</ref> trained in an unsupervised manner. The extracted features vector represents the "fingerprint" that we propose to use to analyze the relationship between ğ‘ ğ‘” and ğ‘ ğ‘Ÿ by exploiting a ML model specifically trained for the distinction between elements belonging to ğ‘ˆ and ğ‘ ğ‘ˆ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vector-Net for fingerprint extraction</head><p>Vector-Net is the proposed CNN that aims to provide a mapping function ğ‘“ able to project the input images in a latent space where the extracted fingerprints contribute to the identification of the samples used for the training of the generative model among all the real instances. In particular, denoting with ğ‘¥ a generic input belonging to one of the sets ğº, ğ‘ˆ , and ğ‘ ğ‘ˆ , ğ‘“ is applied to generate the representation ğ‘¥ Ëœ, where ğ‘¥ Ëœ= ğ‘“ (ğ‘¥), whose characteristics are considered for the distinction of the elements in ğ‘…. Figure <ref type="figure" coords="3,297.32,285.73,5.00,10.91" target="#fig_0">1</ref> shows an illustrative example of the effects of the mapping function on the samples of ğº, ğ‘ˆ , and ğ‘ ğ‘ˆ , referred as ğ‘” (ğ‘” âˆˆ ğº), ğ‘¢ (ğ‘¢ âˆˆ ğ‘ˆ ), and ğ‘›ğ‘¢ (ğ‘›ğ‘¢ âˆˆ ğ‘ ğ‘ˆ ). Indeed, it highlights that although it is extremely hard to discriminate ğ‘”, ğ‘¢, and ğ‘›ğ‘¢, the resulting fingerprints ğ‘” Ëœ, ğ‘¢ Ëœand ğ‘›ğ‘¢ Ëœcan be used to analyze the hidden relationship between the three sets of images. The architecture of the implemented Vector-Net consists of six convolutional blocks, a Global Average Pooling <ref type="bibr" coords="3,166.76,608.83,13.00,10.91" target="#b6">[7]</ref> operation and a fully connected layer. In particular, each convolutional block includes a convolutional layer, with a 4 Ã— 4 kernel and values of padding and stride set to 1 and 2 respectively, in order to provide a reduction of the dimensionality of the features maps, followed by Batch Normalization and ReLU as activation function. The first convolutional layer presents a one-channel input and 32 output channels, that are doubled by each step in the chain of the six convolutional blocks. The Global Average Pooling is used to generate a features vector which feds the last fully connected layer with 1024 input and 64 output neurons.</p><p>The 64-element vector represents the fingerprint ğ‘¥ Ëœextracted from a generic input ğ‘¥, that embeds an image in a 64-dimensional Euclidean space. As suggested in the work proposed in <ref type="bibr" coords="4,89.29,141.16,11.48,10.91" target="#b7">[8]</ref>, we constrain this vector to live in a hypersphere ensuring that â€–ğ‘¥ Ëœâ€–2 = 1 in introducing a normalization criterion among the generated fingerprints.</p><p>In our methodology, we evaluate the ability of the function ğ‘“ in producing fingerprints that enhance the supposed similarity between the sets ğº and ğ‘ˆ and make clear the distinction with ğ‘ ğ‘ˆ . To this aim, we consider the distance of the vector representations as a measure of their similarity, adopting the Euclidean metric. Then, we use the triplet loss <ref type="bibr" coords="4,438.57,208.91,13.00,10.91" target="#b8">[9]</ref> to train the implemented Vector-Net to minimize the distance between ğ‘” Ëœ, and ğ‘¢ Ëœ, while maximizing the dissimilarity with ğ‘›ğ‘¢ Ëœ. Indeed, the triplet loss <ref type="bibr" coords="4,291.92,236.01,12.88,10.91" target="#b8">[9]</ref> ensures that a reference element, denoted as anchor (a), is located close to positive (p) samples and beyond a certain margin (ğ‘š) from the negative (n) ones, guaranteeing that ğ‘‘(ğ‘, ğ‘) + ğ‘š &lt; ğ‘‘(ğ‘, ğ‘›), where ğ‘‘ denotes the Euclidean distance, as represented in Figure <ref type="figure" coords="4,240.61,276.66,3.74,10.91" target="#fig_1">2</ref>. </p><formula xml:id="formula_0" coords="4,244.78,477.87,261.85,10.91">ğ‘‘(ğ‘” Ëœ, ğ‘¢ Ëœ) + ğ‘š &lt; ğ‘‘(ğ‘” Ëœ, ğ‘›ğ‘¢ Ëœ)<label>(1)</label></formula><p>âˆ€ ğ‘” Ëœ, ğ‘¢ Ëœand ğ‘›ğ‘¢ Ëœgenerated by ğ‘“ when applied to the samples of ğº, ğ‘ˆ and ğ‘ ğ‘ˆ . The Equation 1 is required to make the network able to search for the supposed similarity between the images ğ‘¢ and ğ‘”, extracting for them fingerprints that are close (or similar) in the Euclidean space, when compared to those belonging to the ğ‘›ğ‘¢ elements. This allows the introduction of another constraint treating the representations ğ‘¢ Ëœ, ğ‘” Ëœand ğ‘›ğ‘¢ Ëœas the anchor, the positive and the negative samples respectively, and formalized as follows:</p><formula xml:id="formula_1" coords="4,244.46,595.98,262.18,10.91">ğ‘‘(ğ‘¢ Ëœ, ğ‘” Ëœ) + ğ‘š &lt; ğ‘‘(ğ‘¢ Ëœ, ğ‘›ğ‘¢ Ëœ)<label>(2)</label></formula><p>We need a CNN able to extract fingerprints from ğ‘ˆ that allow the recognition of the used images, exploiting their similarity with the representations ğ‘” Ëœand, at the same time, their diversity from the ğ‘›ğ‘¢ Ëœ. The Equation 2 is introduced to explicitly separate the features vectors obtained from ğ‘¢ and ğ‘›ğ‘¢, while further ensuring the similarity between ğ‘¢ Ëœand ğ‘” Ëœ. We argue that the diversity between ğ‘¢ Ëœand ğ‘›ğ‘¢ Ëœshould be preserved since it is the result of the absence of a relationship between the sets ğ‘ˆ and ğ‘ ğ‘ˆ , which consist of independent samples representing the distribution of the real data (ğ‘ˆ âŠ† ğ‘… and ğ‘ ğ‘ˆ âŠ† ğ‘… ). Indeed, while ğ‘ˆ and ğº have been involved in the development of the same generative model, thus explaining the need of testing the hypothesis of their similarity, ğ‘ˆ and ğ‘ ğ‘ˆ are two separate sets extracted from the same distribution.</p><p>In general, the presence in the set ğ‘…, including both ğ‘ˆ and ğ‘ ğ‘ˆ , of images belonging to different and unrelated patients suggests the absence of a relationship among the real instances, which is exploited in the definition of a third constraint focusing on the ğ‘ˆ set and supporting the training of the CNN for the extraction of the fingerprints. In particular, we propose to bind the function ğ‘“ to extract the vector representations from images in such a way as to minimize the distance between ğ‘” Ëœand ğ‘¢ Ëœ, and preserve the independence between the different elements of ğ‘ˆ . Denoting with ğ‘¢ 1 and ğ‘¢ 2 two instances belonging to the set of used samples, with ğ‘¢ 1 âˆˆ ğ‘ˆ , ğ‘¢ 2 âˆˆ ğ‘ˆ | ğ‘¢ 1 Ì¸ = ğ‘¢ 2 , we formalize the third constraint as follows:</p><formula xml:id="formula_2" coords="5,240.64,260.52,266.00,11.36">ğ‘‘(ğ‘¢ 1 Ëœ, ğ‘” Ëœ) + ğ‘š &lt; ğ‘‘(ğ‘¢ 1 Ëœ, ğ‘¢ 2 Ëœ)<label>(3)</label></formula><p>where ğ‘¢ 1 Ëœand ğ‘¢ 2 Ëœare the fingerprint extracted with the application of ğ‘“ . The Equation <ref type="formula" coords="5,468.79,285.02,3.68,10.91" target="#formula_2">3</ref>, which considers the representations extracted from ğ‘ˆ both anchor and negative vectors, is introduced to make the CNN focus only on the search of the similarity between ğ‘” Ëœand ğ‘¢ Ëœ, minimizing their distance, while maintaining the distinctiveness among the different and unrelated real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DAE for unsupervised fingerprint extraction</head><p>The Deforming Autoencoder (DAE) <ref type="bibr" coords="5,253.84,375.40,12.99,10.91" target="#b5">[6]</ref> is an unsupervised encoder-decoder architecture designed to disentangle an image in its main components of shape and texture. It exploits the basic notion that creating an image involves the combination of two processes: a synthesis of appearance on a coordinate system with no distortion (referred to as a "template"), followed by a second deformation that includes shape diversity. The network architecture is reported in Figure <ref type="figure" coords="5,119.99,443.14,3.69,10.91" target="#fig_2">3</ref>, consisting of an Encoder (ğ¸) and a set of two decoders (ğ· ğ‘  and ğ· ğ‘¡ ), for the synthesis of the shape (S) and the texture (T), respectively. Denoting with ğ‘¥ a generic input image, ğ¸ implements a function providing the latent representation ğ‘¥ Ëœ(ğ‘¥ Ëœ= ğ¸(ğ‘¥)) which is then used by ğ· ğ‘  and ğ· ğ‘¡ . In particular, ğ‘¥ Ëœis then split into two different parts ğ‘¥ Ëœğ‘ , and ğ‘¥ Ëœğ‘¡ (ğ‘¥ Ëœ= [ğ‘¥ Ëœğ‘ , ğ‘¥ Ëœğ‘¡]), representing the latent shape and texture, thus introducing a clear image decomposition. Each of these parts is fed to a specific decoder generating the disentangled components of the image. The DAE is trained according to its ability to reconstruct the input starting from its main components using the loss function (ğ¿ ğ·ğ´ğ¸ ) proposed in <ref type="bibr" coords="6,491.79,127.61,11.55,10.91" target="#b5">[6]</ref>, which consists of the sum of two elements: the reconstruction loss (ğ¿ ğ‘Ÿğ‘’ğ‘ ), implemented as the standard ğ‘™ 2 norm, and the warping loss (ğ¿ ğ‘¤ğ‘ğ‘Ÿğ‘ ) used to create visually realistic samples <ref type="bibr" coords="6,480.36,154.71,11.43,10.91" target="#b5">[6]</ref>.</p><p>The encoder architecture consists of an initial convolutional layer with a 4 Ã— 4 kernel, values of padding and stride set to 1 and 2 respectively, followed by a LeakyReLU activation function, a chain of ğ‘› ğ‘’ encoding blocks, and a last convolutional layer with a 4 Ã— 4 kernel and a Sigmoid function. Each encoding block includes a convolutional operation, with a 4 Ã— 4 kernel and values of padding and stride set to 1 and 2 respectively, a Batch Normalization, and a LeakyReLU function. The first layer presents an input channel set to 1, with 32 output channels, that are doubled in each step of the chain of the encoding blocks. The last convolutional operation provides a 128-element vector corresponding to the latent representation of the input.</p><p>The ğ· ğ‘  and ğ· ğ‘¡ present the same basic architecture, but differ for the activation functions. In particular, both decoders consist of a chain of ğ‘› ğ‘‘ decoding blocks, followed by a transposed convolution operation with a 3 Ã— 3 kernel, and activation function that is the HardTanh in ğ· ğ‘¡ and the Sigmoid in ğ· ğ‘  . Each decoding block includes a transposed convolution layer with a 4 Ã— 4 kernel, a Batch Normalization, and a ReLU or hyperbolic (tanh) function for ğ· ğ‘¡ and ğ· ğ‘¤ respectively.</p><p>In our methodology, we train the DAE considering the samples belonging to the sets of ğº, ğ‘ˆ , and ğ‘ ğ‘ˆ . The encoder ğ¸ is then used to generate a 128-element vector for each input image that is treated as the "fingerprint" of each instance, extracted in an unsupervised manner. Indeed, the information about the source of the images is not exploited, with the aim of testing if the decomposition in the shape and texture components generates elements highlighting the similarity between ğº, and ğ‘ˆ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Set-Up</head><p>The datasets involved in the competition consist of axial 3D computed tomography (CT) images of about 8000 lung tuberculosis patients, stored in the form of 8 bit/pixel PNG images with dimensions of 256x256 pixels. The synthetic images are 256x256 pixels in size and are generated using a Diffusion Model <ref type="bibr" coords="6,198.91,524.97,11.43,10.91" target="#b2">[3]</ref>. The training test provided to test the hypothesis of the similarity between the artificial and the real samples includes 500 synthetic images (ğº), 80 real instances not used for the development of generative neural networks (ğ‘ ğ‘ˆ ) as well as 80 real images taken from the image set used for training corresponding generative model (ğ‘ˆ ). In the test set, a total of 10000 generated images and 200 real samples are provided.</p><p>After their training, the Vector-Net and the encoder of the DAE are applied to the all the elements ğ‘” âˆˆ ğº, ğ‘¢ âˆˆ ğ‘ˆ , and ğ‘›ğ‘¢ âˆˆ ğ‘ ğ‘ˆ , generating the representation ğ‘” Ëœ, ğ‘¢ Ëœ, and ğ‘›ğ‘¢ Ëœ. The extracted fingerprints are analyzed by assessing their effectiveness in the distinction between elements belonging to ğ‘ˆ and ğ‘ ğ‘ˆ . We leverage two ML models, namely the Support Vector Machines (SVM) <ref type="bibr" coords="6,168.80,646.91,18.07,10.91" target="#b9">[10]</ref> and the K-Nearest Neighbours (KNN) <ref type="bibr" coords="6,367.65,646.91,16.42,10.91" target="#b10">[11]</ref>, to detect among the real instances, those used during the development of the generative model. In other words, we perform a binary classification task, exploiting the fingerprint extracted from the networks, associating the label "1" to those belonging to used images ğ‘¢ Ëœand "0" to the representations ğ‘›ğ‘¢ Ëœ. The choice of the SVM and KNN relies on the fact that they represent two different models, where the former aims to determine a hyperplane to separate the classes, while the latter applies the distance metric that can be interpreted as a similarity score.</p><p>The experiments implemented for the ImageCLEFmed GAN challenge <ref type="bibr" coords="7,412.78,154.71,12.78,10.91" target="#b4">[5]</ref> vary according to different aspects related to the characteristics of the networks used for the extraction of the fingerprints, the involved ML model, and the set of features representations used to train the SVM and KNN.</p><p>When the Vector-Net is used for fingerprint extraction, we evaluate the contribution of the proposed constraints by adding them one at a time. As a consequence, we denote as Vector-Net(1), Vector-Net(1,2) and Vector-Net(1,2,3) the approaches including the Equation <ref type="formula" coords="7,452.77,236.01,3.66,10.91" target="#formula_0">1</ref>, Equations 1 and 2, and Equations 1, 2, 3, respectively.</p><p>As aforementioned, the DAE is used to provide an unsupervised features vector definition. We consider three different variants obtained by changing the set of images used during the training of the network, with the aim of evaluating if their presence or absence affects the fingerprints extraction. In particular, we denote as DAE(G,U), DAE(G,NU), and DAE(G,U,NU), the experiments involving the DAE architecture and the sets of samples specified in brackets. It is worth noting that after the training step, all the approaches are applied to all the elements ğ‘” âˆˆ ğº, ğ‘¢ âˆˆ ğ‘ˆ , and ğ‘›ğ‘¢ âˆˆ ğ‘ ğ‘ˆ .</p><p>Two different ML models are used for the identification of the subset of the real images used for the development of the generative network. After a hyper-parameter optimization step, we set the number of neighbors in KNN to 5, and explore both linear and the polynomial (degree = 2) kernel in the SVM to investigate also more complex boundaries among the vector representations, denoting with SVM-Linear and SVM-2 the two variants, respectively.</p><p>In our experiments, we also evaluate the impact of the fingerprints extracted from ğº in the classification task proposing two training strategies, referred as "REAL" and "FULL". The former uses only the fingerprints generated from ğ‘ˆ and ğ‘ ğ‘ˆ , namely ğ‘¢ Ëœ, and ğ‘›ğ‘¢ Ëœ, to train the two ML models, while the latter includes also the representations of the elements of ğº by associating to the vectors ğ‘” Ëœthe same label of ğ‘¢ Ëœ. In both cases, we aim to evaluate the effectiveness of the CNNs in the generation of features representations that enhance the supposed similarity between ğº and ğ‘ˆ , and the dissimilarity with ğ‘ ğ‘ˆ . However, in the "REAL" strategy, we explicitly evaluate how the extracted fingerprints are able to determine a separation among the elements of ğ‘ˆ and ğ‘ ğ‘ˆ , without relying on the presence of ğ‘” Ëœ. When the "FULL" option is explored, we apply the adaptive synthetic sampling approach (Adasyn <ref type="bibr" coords="7,295.41,547.64,16.76,10.91" target="#b11">[12]</ref>) to handle the imbalance between the labels "1" and "0".</p><p>During the experiments, we train the Vector-Net and the DAE using the loss functions defined in Section 2. The DAE architecture is implemented following the work proposed in <ref type="bibr" coords="7,89.29,601.84,11.57,10.91" target="#b5">[6]</ref>, but modifying the networks to handle a 256 Ã— 256 input. In particular, we add encoding and decoding blocks both in the Encoder and Decoder, considering ğ‘› ğ‘’ and ğ‘› ğ‘‘ set to 5 and 7 respectively. The maximum number of epochs is set to 1000 and 500 for the DAE and the Vector-net, respectively. The batch size is 32, the learning rate for the triplet loss is 10 -5 , and 2 â€¢ 10 -4 for the ğ¿ ğ·ğ´ğ¸ . Adam optimizer is used with a decay set to 10 -4 .</p><p>Performance is evaluated in terms of accuracy (ACC), F1-score (F1) and Recall (R), as suggested in the competition. It is worth noting that the evaluation step is performed only considering the images belonging to ğ‘ˆ and ğ‘ ğ‘ˆ as test set, thus evaluating the hypothesis of the competition.</p><p>All the experiments were run in a 10-folds cross-validation (CV) setting, to better assess the generalization ability of each approach, using Pytorch for the training of the Vector-Net and the DAE and MATLAB 2020b for the classification task involving the SVM and the KNN. A Linux workstation equipped with Intel(R) Core(TM) i7-10700KF CPU, 64 GB of DDR4 RAM and a Nvidia RTX 3090 GPU is used. We took part in the ImageCLEFmed GAN challenge <ref type="bibr" coords="8,334.05,181.81,12.87,10.91" target="#b4">[5]</ref> with 10 different submissions. As a consequence, we have a partial evaluation of the experiments using also the real samples of test set provided by the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>This section reports the results of the implemented methodology, including different variants. In particular, Table <ref type="table" coords="8,162.83,281.08,4.98,10.91">1</ref> shows the performance of the experiments in CV setting, thus exploiting the dataset provided by the ImageCLEFmed GAN challenge <ref type="bibr" coords="8,333.18,294.63,11.28,10.91" target="#b4">[5]</ref>. It consists of six sections, that differ according to the network used for the fingerprints extraction step as detailed in the column Net., while the training strategy and the ML exploited for the classification are reported in columns Stretegy, and ML Model respectively. For each vector extractor, the best values are reported in bold.</p><p>Table <ref type="table" coords="8,126.56,362.38,4.98,10.91">1</ref> highlights that the fingerprints extracted with Vector-Net are able to provide a better separation between ğ‘ˆ ad ğ‘ ğ‘ˆ in comparison with those obtained with the DAE. We argue that this characteristic depends on the introduction of task-specific constraints during network training. Indeed, the results achieved involving the DAE reveal that relying solely on shape and texture information is not enough to determine supposed similarity among artificial images and the samples used during the training of the generative model. Moreover, the presence of the ğ‘” Ëœvectors in the "FULL" strategy negatively impacts performance, as a consequence of the evident difference among the distributions of real ğ‘ ğ‘Ÿ and generated ğ‘ ğ‘” data. Some preliminary experiments (not reported in this paper) focusing on the distinction of the elements of ğº from ğ‘… showed that it is possible to separate the synthetic images from the real ones with high accuracy (0.9970). Therefore, merging the fingerprints ğ‘” Ëœwith ğ‘¢ Ëœduring the training makes the ML model exploit the dissimilarity of ğ‘ ğ‘Ÿ and ğ‘ ğ‘” , determining a boundary that includes in the same region the representations of ğ‘ˆ ad ğ‘ ğ‘ˆ . In addition, it is worth noting that in the "FULL" strategy the presence of the features of ğº generates a very unbalanced dataset. Othe other hand, in the "REAL" strategy, the model is forced to determine a separation among elements belonging to the same distribution ğ‘ ğ‘Ÿ . This aspect also explains the reduced gap in the performance when the Vector-Net(1,2) is considered in comparison with the other experiments. Indeed, Equations 1 and 2 ensure that ğ‘‘(ğ‘” Ëœ, ğ‘¢ Ëœ) + ğ‘š &lt; ğ‘‘(ğ‘” Ëœ, ğ‘›ğ‘¢ Ëœ) and ğ‘‘(ğ‘¢ Ëœ, ğ‘” Ëœ) + ğ‘š &lt; ğ‘‘(ğ‘¢ Ëœ, ğ‘›ğ‘¢ Ëœ), thus enhancing the separation among ğ‘ˆ ad ğ‘ ğ‘ˆ .</p><p>A subset of the conducted experiments has been submitted to the ImageCLEFmed GAN challenge <ref type="bibr" coords="8,133.37,633.36,11.28,10.91" target="#b4">[5]</ref>. It is worth noting that only a part of the proposed variants has been implemented before the deadline of the competition. Indeed, we started to address the problem of the relationship between ğ‘ˆ and ğ‘ ğ‘ˆ and we continued after the end of the challenge. As a consequence, we</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,553.78,416.69,9.15;3,88.99,565.74,49.01,9.14;3,122.65,362.64,349.99,184.61"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrative example of the application of ğ‘“ to the element ğ‘” (ğ‘” âˆˆ ğº), ğ‘¢ (ğ‘¢ âˆˆ ğ‘ˆ ), and ğ‘›ğ‘¢ (ğ‘›ğ‘¢ âˆˆ ğ‘ ğ‘ˆ ).</figDesc><graphic coords="3,122.65,362.64,349.99,184.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,384.72,281.91,8.93;4,201.24,298.95,192.80,73.20"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relation between the samples when the triplet loss is used</figDesc><graphic coords="4,201.24,298.95,192.80,73.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,621.65,367.49,9.65;5,192.64,479.40,210.00,129.90"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DAE architecture consisting of an Encoder (ğ¸) and a two decoders (ğ· ğ‘  and ğ· ğ‘¡ )</figDesc><graphic coords="5,192.64,479.40,210.00,129.90" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is part of the <rs type="programName">POR FESR CAMPANIA 2014-2020 Synergy for COVID project</rs> (<rs type="grantNumber">CUP H69I22000710002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dXFxaNr">
					<idno type="grant-number">CUP H69I22000710002</idno>
					<orgName type="program" subtype="full">POR FESR CAMPANIA 2014-2020 Synergy for COVID project</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>only have a partial evaluation of the results considering the real images included in the test set.</p><p>Table <ref type="table" coords="9,115.09,568.10,4.97,10.91">2</ref> shows the performance obtained by submitting the variants with the SVM classifier, and the "FULL" training strategy. The column Submission details the name of the submission file and in the last row the experiment "PicusLabMed_submission10.csv" is generated by implementing a voting strategy among the other results. As we expected from the results reported in Table <ref type="table" coords="9,499.79,608.75,3.70,10.91">1</ref>, the presence of the ğ‘” Ëœrepresentations during the training of the ML models does not lead to good results. Moreover, the experiments with the Vector-Net (1,2) present overfitting showing good performance in Table <ref type="table" coords="9,212.27,649.40,3.75,10.91">1</ref>, but low values in Table <ref type="table" coords="9,328.60,649.40,3.75,10.91">2</ref>. We argue that this characteristic may reflect the weakness of the models when applied to a test set with different characteristics, thus highlighting the need for a more robust approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The surprising performance achieved by generative models in the creation of realistic synthetic images has raised the need to address considerations related to data privacy and ethical implications, especially in sensitive domains, such as the medical one. To this aim the ImageCLEFmed GAN challenge <ref type="bibr" coords="10,158.56,386.96,12.70,10.91" target="#b4">[5]</ref> has been organized, asking participants to test the hypothesis of the similarity between the set of real images used during the development of the generative model and the synthetic samples. In this paper, we described the methodology we implemented to take part in the competition, designing experiments that analyze the impact of two diverse CNNs in the fingerprints extraction step and the ability of the ML models in the classification with different training strategies. Indeed, Vector-Net and the DAE represent two approaches where the former explicitly exploits the constraints related to the hypothesis to be tested, while the latter aims to investigate if information about the texture and the shape of the image contribute to the evaluation of the supposed similarity. Despite the poor performance obtained from some experiments, we argue that there is the need to explore other approaches before ruling out all the concerns regarding the application of synthetic images in medical field. To this aim, we will consider the obtained results as a baseline, improving in future works the set of constraints required to extract fingerprints able to reveal the images used during the training of the generative models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,111.28,393.57,10.91;11,112.33,124.83,29.19,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m" coord="11,226.23,111.28,139.19,10.91">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,138.38,394.53,10.91;11,112.34,151.93,394.29,10.91;11,112.41,165.48,38.81,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,166.01,151.93,146.96,10.91">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,325.54,151.93,136.10,10.91">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,179.03,393.33,10.91;11,112.66,192.57,237.23,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,226.22,179.03,180.87,10.91">Denoising diffusion probabilistic models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,416.47,179.03,89.52,10.91;11,112.66,192.57,143.15,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,206.12,394.53,10.91;11,112.66,219.67,395.17,10.91;11,112.66,233.22,394.53,10.91;11,112.66,246.77,395.17,10.91;11,112.39,260.32,394.80,10.91;11,112.48,273.87,394.70,10.91;11,112.66,287.42,395.17,10.91;11,112.66,300.97,393.32,10.91;11,112.66,314.52,394.52,10.91;11,112.33,328.07,120.27,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,224.93,273.87,282.25,10.91;11,112.66,287.42,228.44,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>DrÄƒgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>StorÃ¥s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>SchÃ¶ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,363.94,287.42,143.89,10.91;11,112.66,300.97,393.32,10.91;11,112.66,314.52,136.27,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,280.09,314.52,221.52,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,341.62,393.33,10.91;11,112.66,355.17,393.33,10.91;11,112.66,368.71,393.33,10.91;11,112.66,382.26,356.56,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,448.60,341.62,57.38,10.91;11,112.66,355.17,334.73,10.91">Overview of ImageCLEFmedical GANs 2023 task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,464.74,355.17,41.25,10.91;11,112.66,368.71,287.68,10.91;11,421.96,368.71,84.03,10.91;11,112.66,382.26,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="11,143.49,382.26,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="11,112.66,395.81,393.32,10.91;11,112.66,409.36,377.32,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,457.24,395.81,48.74,10.91;11,112.66,409.36,299.81,10.91">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.84,409.36,22.98,10.91">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,422.91,374.56,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m" coord="11,221.26,422.91,88.34,10.91">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,436.46,393.33,10.91;11,112.66,450.01,393.33,10.91;11,112.66,463.56,136.93,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,283.76,436.46,222.22,10.91;11,112.66,450.01,62.79,10.91">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,198.64,450.01,307.35,10.91;11,112.66,463.56,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,477.11,393.61,10.91;11,112.66,490.66,297.69,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,303.81,477.11,202.45,10.91;11,112.66,490.66,69.48,10.91">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,194.88,490.66,170.67,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,504.21,375.65,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,208.31,504.21,109.06,10.91">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,326.19,504.21,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,517.76,393.33,10.91;11,112.66,531.30,103.40,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,188.72,517.76,165.93,10.91">Nearest neighbor pattern classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,361.66,517.76,144.33,10.91;11,112.66,531.30,29.61,10.91">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,544.85,393.54,10.91;11,112.66,558.40,393.33,10.91;11,112.33,571.95,356.42,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,270.34,544.85,235.85,10.91;11,112.66,558.40,90.71,10.91">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,252.52,558.40,253.47,10.91;11,112.33,571.95,232.28,10.91">IEEE international joint conference on neural networks (IEEE world congress on computational intelligence)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
