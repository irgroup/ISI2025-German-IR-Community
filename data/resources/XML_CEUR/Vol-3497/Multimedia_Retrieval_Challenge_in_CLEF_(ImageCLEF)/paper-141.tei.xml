<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,146.33,399.10,15.43;1,89.29,168.25,407.83,15.43;1,89.29,190.17,394.87,15.43;1,88.78,212.09,88.78,15.43;1,177.56,209.05,5.85,10.48;1,89.29,234.43,300.74,11.96">DMK-SSN at ImageCLEF 2023 Medical: Controlling the Quality of Synthetic Medical Images Created via GANs using Machine Learning and Image Hashing Techniques ⋆ Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,260.33,88.42,11.96"><forename type="first">Dhivya</forename><surname>Subburam</surname></persName>
							<email>dhivyas@ssn.edu.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.86,260.33,142.15,11.96"><forename type="first">Shriram</forename><forename type="middle">M</forename><surname>Sathyanarayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.17,260.33,78.04,11.96"><forename type="first">Bhavana</forename><surname>Anand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,274.28,91.96,11.96"><forename type="first">Kavitha</forename><surname>Srinivasan</surname></persName>
							<email>kavithas@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.76,274.28,131.33,11.96"><forename type="first">Mohanavalli</forename><surname>Subramaniam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<settlement>Kalavakkam</settlement>
									<region>Tamil Nadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,146.33,399.10,15.43;1,89.29,168.25,407.83,15.43;1,89.29,190.17,394.87,15.43;1,88.78,212.09,88.78,15.43;1,177.56,209.05,5.85,10.48;1,89.29,234.43,300.74,11.96">DMK-SSN at ImageCLEF 2023 Medical: Controlling the Quality of Synthetic Medical Images Created via GANs using Machine Learning and Image Hashing Techniques ⋆ Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BABB4ABC4CD91083E4E82D45F3BA9AE4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF2023</term>
					<term>Lung CT axial slices</term>
					<term>Image Hashing</term>
					<term>Convolutional Neural Network</term>
					<term>SIFT+KNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF, a platform dedicated to the evaluation and advancement of visual media analysis, has conducted a novel challenge in its medical track titled Controlling the Quality of Synthetic Medical Images created via GANs. This task investigates the hypothesis whether Generative Adversarial Networks (GANs) generated synthetic images retain distinct characteristics indicative of the real images used for training. This research aims to detect these characteristics and analyze the relationship between real and artificial biomedical image datasets. The task focuses on analyzing test image datasets to identify the presence of real images in the training process. The development dataset includes both artificial and real images of lung tuberculosis patients. Our team submitted two methods for evaluation: a CNN model achieved an F1 score of 0.4804, while the SIFT-KNN approach obtained an F1 score of 0.449. In addition to the models stated above, the pHash approach was applied which enabled to map real images to their corresponding synthetic counterparts. These findings provide valuable insights into detecting real image characteristics in synthetic biomedical datasets, highlighting the importance of addressing security concerns when employing GANs for medical image generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF is an evaluation forum organized annually that encompasses research tasks oriented towards image analysis and cross-language annotation. ImageCLEF 2023 <ref type="bibr" coords="2,403.85,124.83,11.28,10.91" target="#b0">[1]</ref>, focussed on various challenges which aims towards the amelioration of research contributions in visual analysis, annotation, classification, and retrieval tasks. The medical based tasks has been included from the second edition of the ImageCLEF under the tag ImageCLEFMedical <ref type="bibr" coords="2,420.31,165.48,15.38,10.91" target="#b1">[2]</ref> which annually hosted several medical domain based tasks for major achievements since 2004. Amongst the four tasks proposed for the year 2023, evaluating the synthetic images generated by the Generative adversarial Networks (GANs) <ref type="bibr" coords="2,218.23,206.12,12.69,10.91" target="#b2">[3]</ref> is indeed a completely new challenge in the track. The objective mainly focussed on evaluating the prevalent theory that the GANs produce synthetic images with the fingerprints of the real images employed during the GANs training. Thus, the goal of this task is to identify whether a GAN generated image do possess the fingerprint of the real image, if so then the synthetic biomedical images are also subject to the restrictions for usage and sharing similar to the actual real data. However, if the hypothesis is incorrect, then GANs could be used for data augmentation in order to produce extensive biomedical databases that are exempted from any privacy or ethical restrictions. This document illustrates DMK-SSN's participation in the ImageCLEF medical 2023 for the task Controlling the Quality of Synthetic Medical Images created via GANs <ref type="bibr" coords="2,233.62,328.07,15.82,10.91" target="#b1">[2]</ref>.</p><p>Since the discovery of x-rays, medical imaging has achieved significant advancements in medicine and fundamentally revolutionised the way diseases are identified and treated <ref type="bibr" coords="2,477.71,355.17,12.84,10.91" target="#b3">[4]</ref> <ref type="bibr" coords="2,490.55,355.17,12.84,10.91" target="#b4">[5]</ref>. The most common imaging modalities used to study the internal workings of the human body are computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound scanning. Shallow and deep learning techniques <ref type="bibr" coords="2,261.99,395.81,12.88,10.91" target="#b5">[6]</ref> have advanced dramatically in discriminative tasks across the key facets of healthcare because of the development of Computer Aided Diagnosis (CADs). One of the biggest difficulties encountered while applying these algorithms for disease diagnosis is the lack of patient datasets for model training. This presents a major risk, particularly in disease diagnosis. Additionally, models struggle to learn the underlying pattern in the datasets during the training phase if there is an uneven distribution of the classes. To overcome these challenges, several techniques are employed to scale up the databases. Traditional approaches involved geometric transformations such as rotation, scaling, translation, flipping, shearing and other approaches. However, deep model-based image synthesis can be achieved using GANs and its variations. In the following, we first describe related works involved in image augmentation and their impact on classification using deep models, to compare fingerprints between two images and image hashing approaches in Section 2, followed by the description of the dataset provided for ImageCLEF medical 2023 for the task Controlling the Quality of Synthetic Medical Images created via GANs in Section 3. In Section 4, we describe the details of methods employed, and Section 5 describes the experiments and results. Section 6 elucidates the conclusion for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Deep learning <ref type="bibr" coords="3,157.91,111.28,12.99,10.91" target="#b6">[7]</ref> have led the GAN has a lot of potential for medical imaging challenges because of its capacity to produce high quality, realistic, factual images. Several pioneering papers such as <ref type="bibr" coords="3,155.79,138.38,11.29,10.91" target="#b7">[8]</ref>, <ref type="bibr" coords="3,173.56,138.38,12.70,10.91" target="#b8">[9]</ref> [10]have demonstrated the effectiveness of GANs in tasks such as image generation, data augmentation, and quality control. In paper <ref type="bibr" coords="3,369.25,151.93,18.07,10.91" target="#b10">[11]</ref> utilized deep convolution GANs for data augmentation in CT images of 182 liver lesions (53 cysts, 64 metastases and 65 haemangiomas). In paper <ref type="bibr" coords="3,220.59,179.03,18.07,10.91" target="#b11">[12]</ref> have employed data augmentation for an improved lymph node segmentation in the CT images. Several works have shown an improved results in the segmentation and classification of images or lesions. In general, GANs include two networks, a discriminator which discriminates a real image from the generated image and a generator network which is capable to produce synthetic images from random noise by incorporating the feedback images. Thus, if the images are produced by learning the underlying patterns, then the generated images completely mimic the real images. To identify the relationship between a real image and the synthetic image can be identified by analysing the fingerprints of the two images. This could be achieved using certain hash functions on both the image. By employing these techniques, the ideal outcome is to create a distinct fingerprint or the hash value pertaining to that image. This assists to enable a quick search for the related generated images from the dataset. Thus, the binary values which are part of the hash values indicates which real image corresponds to the synthetic images. The GAN image hash and the Real image hash are compared using the hamming distance. The Hamming distance counts how many bits in two hash values' corresponding places are different from one another <ref type="bibr" coords="3,402.71,368.71,18.05,10.91" target="#b12">[13]</ref>. Greater similarity between the images is suggested by a smaller Hamming distance, whilst greater dissimilarity is suggested by a higher Hamming distance. Instead of stating the criterion directly, we determine the pair with the smallest hamming distance and mark it as similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>For the ImageCLEF 2023, for the task Controlling the Quality of Synthetic Medical Images created via GANs , the organizers shared us the benchmarking image set collected from 8000 lung tuberculosis patients. The imaging modality is CT, and the dataset comprises of axial slices of 3D CT scans obtained from those patients. Amongst them, there are normal and as well with lung lesions, or even more serious stages. The images are of 256×256 pixels and are in PNG file format. Diffuse neural networks are employed in the generation of synthetic images which are of same resolution similar to the real images. The initial development dataset comprised 500 synthetic images, around 80 images which were not used for the training of generative models and 80 real images used for training the generative models. However, the test dataset also included images using the generative models. In the test dataset, the synthetic images are 10,000 and the real images composed of 200 images, where the images are not segregated as used for generative and not used for generative training. Figure <ref type="figure" coords="3,380.67,617.03,5.17,10.91" target="#fig_0">1</ref> illustrates a sample image from all three classes of generated, used and not used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>With the development dataset shared as a part of the ImageCLEF Medical task we analysed the fingerprints between the two images, i.e generated image and the real images. The unique fingerprints were identified using the Perceptual Hashing algorithm illustrated in Figure <ref type="figure" coords="4,500.04,335.39,3.81,10.91" target="#fig_1">2</ref>. The pHash algorithm <ref type="bibr" coords="4,190.71,348.94,18.07,10.91" target="#b12">[13]</ref> was applied on the generated images, which produces a hash, a fingerprint of the image. Similarly, hashes for the Used and Unused images were generated. Note, the Used and Unused images are a part of the training dataset. Upon finding the hashing, hamming distance was computed between the used images and the GAN images. Once the hamming distance was computed, we check the closest pair and declare it to form a pair of source image and generated image. This observation was used to check the pattern of hamming distance between source and generated images. It was evident that the hamming distance values circled around 50. With this observation, we perform the same algorithm on the test datasets that consists of the real and generated images. Here upon calculating the minimum distance, we pass a threshold of 50 as observed. The source image will be found for anything below the threshold and the real image will be declared Unused if the minimum distance is more than the threshold. This approach is used as a proof of concept to check whether the images are appropriately classified as used or unused. The primary models in this section include a Convolutional Neural Network and a Scale-Invariant Feature Transform (SIFT) algorithm with the K-Nearest Neighbors (KNN) classifier. The hashing technique is a means to verify the results and acknowledge the outputs from these classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model 1 : Convolutional Neural Network</head><p>The first model devised for this task is based on a Convolutional Neural Network (CNN) architecture. CNNs are widely used for image classification tasks due to their ability to effectively capture spatial features from images. The model begins by loading and preprocessing the images. Resizing the images to a uniform size ensures that all images have the same dimensions, which is essential for further processing. The images are resized to a specific width and height, in this case, 64×64 pixels. By resizing the images, we eliminate any variations in size that could hinder the training process. After resizing, the training images are divided into two categories: used images and unused images. These categories are represented by the labels 1 and 0, respectively and are finally concatenated as train labels. This prepares the data for training the CNN model. The CNN model architecture consists of three convolutional layers, each followed by a max pooling layer. Convolutional layers apply filters to the input images, capturing local patterns and features. The max pooling layers downsample the output of the convolutional layers, reducing the spatial dimensions and retaining the most important features. This helps in reducing the computational complexity of the model. After the convolutional and max pooling layers, the output is flattened and passed through two dense layers. The flattened output is fed into the dense layers, which are fully connected layers. The activation function used in the convolutional layers and the first dense layer is the Rectified Linear Unit (ReLU) function, which introduces non-linearity and helps in capturing complex relationships in the data. The final dense layer uses a sigmoid activation function, which squashes the output to a value between 0 and 1, representing the probability of the image belonging to the used category. During training, the model is fine-tuned by attempting to minimize the binary cross entropy loss. This loss function measures the dissimilarity between the predicted probabilities and the true labels. The optimizer used for minimizing the loss is ADAM, which is an efficient optimization algorithm commonly used for training deep neural networks. In the testing phase, a threshold of 0.5 is used to convert the predicted probabilities into binary predictions. If the predicted probability is greater than or equal to 0.5, the image is classified as used (1), otherwise, it is classified as unused (0). These predictions are then printed out. The use of a CNN model along with appropriate preprocessing techniques and activation functions allows for the effective classification of images into used and unused categories. By training the model on a labeled dataset and fine-tuning it with optimization techniques, the model can learn to distinguish between the features of used and unused images. The resulting predictions provide insights into whether an unknown image was used to generate GAN images or not. The proposed models are depicted in Figure <ref type="figure" coords="5,474.96,639.10,3.74,10.91" target="#fig_2">3</ref>. In the training phase, the SIFT algorithm was first applied to the GAN images, used images and the unused images. The features were clustered accordingly to create a vocabulary of visual words. Similarly SIFT features were extracted for the test images. This was then passed through our classifier (KNN) to perform the required classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model 3 : Hashing techniques for verification</head><p>An improvement in the given tasks can be made by finding the source image from the set of real images for the GAN images provided. This can be done using hashing techniques, one such being perceptual hashing. This model starts by loading in the given test images, both real and GAN. Perceptual hashing is a technique where Discrete Cosine Transform is used to convert the image into a frequency domain. The low frequency components are then used to construct hash values. This approach makes pHash robust to changes in scale, rotations and minor modifications in the image. This ideally aims at generating a unique fingerprint (hash value) for the images which allows efficient searching and matching of similar images from our dataset. The hash values generated by pHash are binary strings that represent the perceptual content of the images. Once the hash values are computed and stored, hamming distance between the GAN image hash and Real image hash is performed. The Hamming distance measures the number of bit positions at which the corresponding bits in two hash values differ.</p><p>A lower Hamming distance indicates a higher similarity between the images, while a higher Hamming distance suggests greater dissimilarity. As mentioned earlier in the paper, a threshold of 50 was used to check if it is a used or unused image. Upon identification the source images are found for the Used images. This approach allows for studying the connections between the generated and real images, shedding light on the effectiveness of the SIFT+KNN and CNN models in determining the source images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>In this section, the proposed models are implemented, and the corresponding performance metrics are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">System specification</head><p>The proposed model is executed on a workstation with a six core Intel processor of 3.9GHZ, 32GB DDR4 RAM and NVIDIA GEFORCE RTX 11GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results of the proposed methods</head><p>The obtained results are illustrated in Table <ref type="table" coords="7,288.80,434.73,3.81,10.91" target="#tab_0">1</ref>. The submission 1 is based on the CNN model, which involves a 3 layered CNN to classify the used and unused images. The model is trained for 10 epochs using the Adam optimizer, and the loss function used is binary cross entropy.. With this model, we achieved an accuracy of 0.535 and a precision of 0.544. The specificity and recall are obtained as 0.64 and 0.43. For submission 2, the SIFT + KNN model was used and this model achieved an accuracy of 0.61, precision of 0.512, specificity of 0.6 and a recall of 0.4 as their performance metrics. Apart from the above models, as mentioned earlier in the paper, pHash technique was used. This technique helped us with the mapping of source images to their generated counterparts as in Figure <ref type="figure" coords="7,279.56,543.13,3.81,10.91" target="#fig_3">4</ref>. Below are some of the outputs from the pHash model. The first one was the output when the used set of images from the training dataset was used along with the GAN images. The second one shows the same content for the real images. The images with "None" value show that they were unused in the generation of images illustrated in Figure <ref type="figure" coords="7,179.86,597.33,3.74,10.91" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed two approaches using CNN and SIFT+KNN to classify the used and not used images for the generation of synthetic images. The F1 score of the first approach is   0.4804 while that of the second approach is 0.449. In terms of accuracy we find that the SIFT and KNN approach has an value of 0.61 while that of the CNN model is 0.535. Along with that, an image hashing technique, which identifies the fingerprint of the images through hash values is discussed. The hamming distance is calculated and based on the value, the generated images can be mapped to their appropriate real images. This assists us to segregate the used and unused images for the training of the diffusion network. In future work, these images can be experimented with using one-shot or few-shot learning, which assists in achieving faster and more efficient classification of generated images using different imaging modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,246.66,174.31,8.93;4,89.32,84.19,416.63,149.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample images from the dataset</figDesc><graphic coords="4,89.32,84.19,416.63,149.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,262.77,244.03,8.93;5,89.30,84.19,416.69,166.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Image hashing approach on development dataset</figDesc><graphic coords="5,89.30,84.19,416.69,166.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,274.19,259.89,8.93;6,89.31,84.18,416.67,177.44"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image hashing approach on the development dataset</figDesc><graphic coords="6,89.31,84.18,416.67,177.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,253.41,271.59,8.93;8,89.30,84.19,416.67,156.65"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mapping of source image for the given generated image</figDesc><graphic coords="8,89.30,84.19,416.67,156.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,89.29,456.39,271.42,8.93;8,89.29,280.26,416.69,163.57"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mapping the generated images to the given test dataset</figDesc><graphic coords="8,89.29,280.26,416.69,163.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,489.54,410.52,69.87"><head>Table 1</head><label>1</label><figDesc>Results of various loss functions for proposed modelTeam Submission # accuracy precision specificity recall f1_score tp tn fp tn</figDesc><table coords="8,95.77,538.59,403.74,20.82"><row><cell>DMK submission 1 0.535</cell><cell>0.544</cell><cell>0.64</cell><cell>0.43</cell><cell>0.4804</cell><cell>43 64 36 57</cell></row><row><cell>submission 2 0.61</cell><cell>0.512</cell><cell>0.6</cell><cell>0.4</cell><cell>0.449</cell><cell>40 62 38 60</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,145.59,394.53,10.91;9,112.66,159.14,395.17,10.91;9,112.66,172.69,394.53,10.91;9,112.66,186.24,394.52,10.91;9,112.66,199.79,393.72,10.91;9,112.66,213.34,395.17,10.91;9,112.66,226.89,395.17,10.91;9,112.66,240.44,393.59,10.91;9,112.66,253.99,380.72,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,463.73,199.79,42.66,10.91;9,112.66,213.34,395.17,10.91;9,112.66,226.89,77.37,10.91">Overview of ImageCLEF 2023: Multimedia retrieval in medical, socialmedia and recommender systems applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drăgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcıa Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J A A A R I C V K A S G I</forename><surname>Nikolaos Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,213.51,226.89,294.33,10.91;9,112.66,240.44,393.59,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,141.33,253.99,223.42,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,267.54,393.33,10.91;9,112.66,281.08,393.33,10.91;9,112.66,294.63,393.33,10.91;9,112.66,308.18,356.56,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,448.60,267.54,57.38,10.91;9,112.66,281.08,334.73,10.91">Overview of ImageCLEFmedical GANs 2023 task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,464.74,281.08,41.25,10.91;9,112.66,294.63,287.68,10.91;9,421.96,294.63,84.03,10.91;9,112.66,308.18,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="9,143.49,308.18,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="9,112.66,321.73,393.33,10.91;9,112.66,335.28,107.17,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m" coord="9,186.03,321.73,239.51,10.91">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,348.83,393.33,10.91;9,112.66,362.38,144.35,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,204.22,348.83,252.80,10.91">Three-dimensional imaging techniques: A literature review</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">H</forename><surname>Karatas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Toy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,463.99,348.83,41.99,10.91;9,112.66,362.38,86.68,10.91">European journal of dentistry</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,375.93,394.53,10.91;9,112.28,389.48,395.00,10.91;9,112.28,403.03,187.35,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,112.28,389.48,395.00,10.91;9,112.28,403.03,37.61,10.91">Applications of computational methods in biomedical breast cancer imaging diagnostics: A review</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Aruleba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Obaido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ogbuokiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">O</forename><surname>Fadaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Adekiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Aruleba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,158.37,403.03,83.60,10.91">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,416.58,393.33,10.91;9,112.66,430.13,394.52,10.91;9,112.66,443.67,246.45,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,430.84,416.58,75.14,10.91;9,112.66,430.13,252.75,10.91">Investigations of shallow and deep learning algorithms for tumor detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhivya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jenifer Anjali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohanavalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sripriya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="DOI">10.1109/HYDCON48903.2020.9242888</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,410.88,430.13,65.24,10.91">IEEE-HYDCON</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,457.22,330.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,255.76,457.22,60.92,10.91">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.99,457.22,28.99,10.91">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,470.77,395.17,10.91;9,112.66,484.32,393.33,10.91;9,112.66,497.87,358.11,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,414.14,470.77,93.69,10.91;9,112.66,484.32,199.84,10.91">Gan based data augmentation for enhanced tumor classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhivya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohanavalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karthika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shivani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mageswari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,336.18,484.32,169.80,10.91;9,112.66,497.87,263.56,10.91">2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,511.42,395.17,10.91;9,112.66,524.97,392.88,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,441.86,511.42,65.96,10.91;9,112.66,524.97,266.06,10.91">Dermatologistlevel classification of skin cancer with deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,387.55,524.97,28.99,10.91">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,538.52,393.32,10.91;9,112.66,552.07,392.36,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,367.83,538.52,138.15,10.91;9,112.66,552.07,140.24,10.91">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,261.38,552.07,149.55,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3106" to="3121" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,565.62,393.33,10.91;9,112.66,579.17,393.33,10.91;9,112.66,592.72,228.76,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,457.89,565.62,48.09,10.91;9,112.66,579.17,393.33,10.91;9,112.66,592.72,55.98,10.91">Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,176.56,592.72,75.85,10.91">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,606.27,393.33,10.91;9,112.26,619.81,393.73,10.91;9,112.66,633.36,274.49,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,351.41,606.27,154.58,10.91;9,112.26,619.81,198.32,10.91">Abnormal chest x-ray identification with generative adversarial one-class classifier</title>
		<author>
			<persName coords=""><forename type="first">Y.-X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,352.86,619.81,153.12,10.91;9,112.66,633.36,150.13,10.91">IEEE 16th international symposium on biomedical imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1358" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,646.91,395.17,10.91;9,112.66,660.46,296.95,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,320.11,646.91,187.72,10.91;9,112.66,660.46,76.21,10.91">A review of hashing based image authentication techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Karsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Laskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,197.40,660.46,156.36,10.91">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
