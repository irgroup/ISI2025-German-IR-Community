<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,395.97,15.42;1,89.29,106.66,377.12,15.42;1,89.29,128.58,241.50,15.43;1,89.29,150.91,300.74,11.96">Evaluating Privacy on Synthetic Images Generated using GANs: Contributions of the VCMI Team to ImageCLEFmedical GANs 2023 Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">FCT -Funda√ß√£o para a Ci√™ncia e a Tecnologia</orgName>
				</funder>
				<funder>
					<orgName type="full">National Funds</orgName>
				</funder>
				<funder ref="#_dU2WvK6">
					<orgName type="full">Portuguese Foundation for Science and Technology)</orgName>
				</funder>
				<funder ref="#_Bkfrng7 #_DD6Jv5s #_scxXFqn #_39Y9x9j #_6kTrZGf">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,97.34,11.96"><forename type="first">Helena</forename><surname>Montenegro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.55,176.82,54.81,11.96"><forename type="first">Pedro</forename><surname>Neto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.29,176.82,85.83,11.96"><forename type="first">Cristiano</forename><surname>Patr√≠cio</surname></persName>
							<email>cristiano.p.patricio@inesctec.pt</email>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Departamento de Inform√°tica</orgName>
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<addrLine>Rua Marqu√™s de √Åvila e Bolama</addrLine>
									<postCode>6201-001</postCode>
									<settlement>Covilh√£</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.04,176.82,77.58,11.96"><forename type="first">Isabel</forename><surname>Rio-Torto</surname></persName>
							<email>isabel.riotorto@inesctec.pt</email>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Departamento de Ci√™ncia de Computadores</orgName>
								<orgName type="department" key="dep2">Faculdade de Ci√™ncias</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua do Campo Alegre s/n</addrLine>
									<postCode>4169-007</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.93,190.76,81.12,11.96"><forename type="first">Tiago</forename><surname>Gon√ßalves</surname></persName>
							<email>tiago.f.goncalves@inesctec.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.33,190.76,74.00,11.96"><forename type="first">Lu√≠s</forename><forename type="middle">F</forename><surname>Teixeira</surname></persName>
							<email>luisft@fe.up.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,395.97,15.42;1,89.29,106.66,377.12,15.42;1,89.29,128.58,241.50,15.43;1,89.29,150.91,300.74,11.96">Evaluating Privacy on Synthetic Images Generated using GANs: Contributions of the VCMI Team to ImageCLEFmedical GANs 2023 Notebook for the ImageCLEFmedical GANs Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">836315E483A11D2949765520A70EB008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Privacy</term>
					<term>Deep Generative Models</term>
					<term>Generative Adversarial Networks</term>
					<term>Medical Image Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the main contributions of the VCMI Team to the ImageCLEFmedical GANs 2023 task. This task aims to evaluate whether synthetic medical images generated using Generative Adversarial Networks (GANs) contain identifiable characteristics of the training data. We propose various approaches to classify a set of real images as having been used or not used in the training of the model that generated a set of synthetic images. We use similarity-based approaches to classify the real images based on their similarity to the generated ones. We develop autoencoders to classify the images through outlier detection techniques. Finally, we develop patch-based methods that operate on patches extracted from real and generated images to measure their similarity. On the development dataset, we attained an F1-score of 0.846 and an accuracy of 0.850 using an autoencoder-based method. On the test dataset, a similarity-based approach achieved the best results, with an F1-score of 0.801 and an accuracy of 0.810. The empirical results support the hypothesis that medical data generated using deep generative models trained without privacy constraints threatens the privacy of patients in the training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning models have great potential to provide valuable insights that support medical diagnosis and treatment, having achieved promising results in various medical image analysis tasks. However, the training of these models requires large amounts of data, which is often difficult to obtain. Deep generative models can generate highly-realistic medical images <ref type="bibr" coords="2,482.94,151.93,11.37,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,497.02,151.93,8.96,10.91" target="#b1">2]</ref> and have been used to obtain large synthetic datasets to facilitate the training of models <ref type="bibr" coords="2,482.27,165.48,11.33,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,496.33,165.48,7.55,10.91" target="#b3">4]</ref>. Nonetheless, since generative models model the probability distribution of the data, there are concerns that the synthetic images obtained using these models may threaten the privacy of the patients whose images were used in their training. These concerns are aggravated by recent claims suggesting that it is possible to re-identify patients based on medical images such as chest radiographs <ref type="bibr" coords="2,145.65,233.22,12.88,10.91" target="#b4">[5]</ref> and magnetic resonance images <ref type="bibr" coords="2,306.42,233.22,11.47,10.91" target="#b5">[6]</ref>. In order to identify the potential privacy threats of using and sharing synthetic medical data in various real-world scenarios, a new challenge (ImageCLEFmedical GANs <ref type="bibr" coords="2,256.46,260.32,12.09,10.91" target="#b6">[7]</ref>) arose as part of the medical track of the ImageCLEF Challenge 2023 <ref type="bibr" coords="2,159.82,273.87,11.43,10.91" target="#b7">[8]</ref>.</p><p>ImageCLEF is a multi-modal challenge organized as part of the CLEF Initiative Labs<ref type="foot" coords="2,475.42,285.66,3.71,7.97" target="#foot_0">1</ref> (Conference and Labs of the Evaluation Forum) that proposes various tasks across different domains, aiming to promote the evaluation of technologies for annotation, indexing, classification and retrieval of multi-modal data. ImageCLEFmedical GANs <ref type="bibr" coords="2,347.64,328.07,12.99,10.91" target="#b6">[7]</ref> is a task of the medical track of the ImageCLEF Challenge 2023 aiming to verify whether the images generated by generative adversarial networks (GANs) <ref type="bibr" coords="2,221.37,355.17,12.84,10.91" target="#b8">[9]</ref> are sufficiently similar to the training data as to compromise its privacy. More specifically, given a set of synthetic images and a set of real images, the goal of the task is to identify which real images were used in the training of the model that generated the synthetic data. It is therefore a binary classification task, where the real images can be classified as "used" or "not used" in the training of the generative models.</p><p>Our team (VCMI team), composed of members of the Visual Computing and Machine Intelligence (VCMI) Research Group of the Institute for Systems and Computer Engineering, Technology and Science (INESC TEC) from Porto, Portugal, approached this challenge using various methods:</p><p>1. Similarity-based methods: identify the real images based on their similarity to the generated images. 2. Autoencoder-based methods: rely on autoencoders to identify images whose probability distribution differs from the generated data through outlier detection techniques, and to compare the real and generated images based on their latent representations. 3. Patch-based methods: extract patches from images and apply them to identify which real images are the most similar to the generated images.</p><p>The best results were obtained with a similarity-based approach that uses Structural Similarity Index Measure (SSIM) <ref type="bibr" coords="2,195.32,601.76,18.06,10.91" target="#b9">[10]</ref> to compute the similarity between real and generated images, achieving an accuracy of 0.810 and an F1-Score of 0.802 on the classification task. The remainder of this paper is organized as follows: section 2 provides an overview of the task and the data provided by the organisation to address the task; section 3 describes the different approaches developed to solve the task; section 4 presents the results and their discussion; and section 5 concludes this paper and recommends future work directions. The code related to this paper is publicly available in a GitHub repository<ref type="foot" coords="3,309.04,112.31,3.71,7.97" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>Given a set of images generated using diffuse neural networks <ref type="bibr" coords="3,362.59,172.69,16.08,10.91" target="#b10">[11]</ref>, and a set of real images, the goal of the task is to predict which of the provided real images were used in the training of the generative model.</p><p>To achieve this task, we had access to two datasets:</p><p>1. Development Dataset: contains 500 generated images and 160 real images annotated according to their use in the training of the generative network. Out of the real images, 80 were used and the remaining 80 were not used during training. 2. Test Dataset: contains 10,000 synthetic images and 200 real images, whose classes we aim to predict. Out of the real images, 100 were used and the remaining 100 were not used during training. The proportions of used and not used images in the real data were not disclosed until the communication of the results of the challenge.</p><p>The subsets of real images are composed of axial slices of 3D computed tomography images taken from a dataset of about 8,000 lung tuberculosis patients. The size of the real data in the datasets is considerably small (160 and 200 images for the development and test dataset, respectively), making it difficult to develop deep learning models trained solely on real data.</p><p>This section presents an overview of the relationship between the probability distributions of the different subsets of each dataset, and provides an exploratory data analysis based on the similarity between the images of the different sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of the Data Subsets</head><p>Figure <ref type="figure" coords="3,119.26,467.66,4.97,10.91" target="#fig_0">1</ref> provides an overview of the existing images in each subset and the relationship between their probability distributions. The goal of the task is to predict ùëÉ (ùë¢ | ùëù), where ùë¢ represents used images and ùëù represents real images to which we have access.</p><p>Deep generative models model the probability distribution of the data. As such, the probability distribution of the generated data should be similar to that of the images used to train the model. However, only a subset of the used images was provided to us, along with a subset of images that were not used in the model's training. As such, the probability distribution of the provided dataset should differ from that of the generated data, assuming that the generative model used to obtain the synthetic images has a limited generalization capacity, characteristic of deep learning models trained on restricted sets of data.</p><p>One of the difficulties of the challenge is that we do not have access to the whole set of images that were used to train the model. The probability distribution of the subset of used images is not necessarily the same as the probability distribution of the whole set of used images. As such, without having access to the whole set of used images, it is difficult to predict the set of real images whose probability distribution is the most similar to that of the generated data. Furthermore, some of the provided synthetic images may be similar to used images that were not provided to us, threatening their privacy. In case these used images are somewhat similar to some of the not used images that were provided to us, there is a risk that some not used images may be misclassified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Exploratory Data Analysis</head><p>The datasets present low variability, as all the images are very similar and centered, enabling the application of similarity metrics like SSIM to compare them. As such, we computed the SSIM between the generated and real images in both the provided datasets, presenting the results in Table <ref type="table" coords="4,115.79,397.01,3.74,10.91" target="#tab_0">1</ref>. The values of structural similarity calculated between generated images and real images are generally higher than the values calculated between real images. Furthermore, the generated images seem to be more similar to images that were used to train the generative network, than to not used images. These results suggest that some of the generated images may contain some identifiable factors of the images used during training.</p><p>Figure <ref type="figure" coords="4,131.53,647.64,5.12,10.91" target="#fig_1">2</ref> contains the pairs of images with highest structural similarity of the development dataset. Visually, the images of the generated-used pair are very similar. The images of the generated-not used pair and of the real-real pair present more pronounced differences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section describes the methods developed to classify the real images as "used" or "not used".</p><p>In specific, we describe the proposed methods organized in three groups: similarity-based methods, autoencoder-based methods and patch-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Similarity-based Methods</head><p>During the exploratory data analysis, we verified that some of the generated images are notably similar to real images, as the average SSIM and maximum SSIM between real and generated images surpass the values calculated between real images. As such, we devised various methods to classify the real images based on their similarity to the generated images. To compute the similarity between two images, we use SSIM and the Euclidean distance between the latent representations of images obtained with autoencoders (described in the following section) and with a ResNet-50 <ref type="bibr" coords="5,188.07,446.42,17.97,10.91" target="#b11">[12]</ref> model trained on ImageNet <ref type="bibr" coords="5,334.46,446.42,16.32,10.91" target="#b12">[13]</ref>. Using these metrics, we compute matrices of similarity between real and generated images, and between real images, and apply the methods described in the following subsections: threshold, retrieval, ranking, clustering and ensemble. Figure <ref type="figure" coords="5,187.42,487.06,5.12,10.91" target="#fig_3">3</ref> presents a representative example of how the threshold, retrieval and clustering approaches work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Threshold</head><p>The threshold approach finds real images whose similarity to their most similar generated image is higher than a threshold, classifying them as "used". The threshold is calculated based on the similarity between real images. We consider two thresholds: the maximum similarity between two images from the real data (MAX), and the sum between the average and standard deviation of the similarity between all real images (AVG). Figure <ref type="figure" coords="5,131.14,617.69,10.02,10.91" target="#fig_3">3a</ref> depicts the process of searching for the synthetic images that are the most similar to each real image and verifying whether their similarity is higher (gray lines) or lower (red lines) than the threshold.  The lighter colors represent samples that are classified as used by the methods, while darker colors represent outliers that are classified as not used. The threshold approach checks whether the distance between each real image and its closest generated image is higher than a threshold. The retrieval approach verifies whether a real image is the closest image to any of the generated images. The clustering approach forms a cluster based on the distance between all data samples from real and generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Retrieval</head><p>The retrieval approach finds a set of real images that are the most similar to at least one generated image, classifying them as "used". For each generated image, it retrieves the most similar real image, as depicted in Figure <ref type="figure" coords="6,216.49,378.45,8.59,10.91" target="#fig_3">3b</ref>. All retrieved images that are, therefore, the most similar to at least one of the generated images, are classified as "used". Real images that are not retrieved are classified as "not used".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Ranking</head><p>The ranking approach classifies real images based on a ranking that defines how similar they are to the generated images. The method starts by calculating a threshold that represents the average rank of similarity of a real image when compared with other real images. As such, for each real image, the method ranks the remaining real images based on their similarity. Then, it calculates the average rank of each real image, which is used as a threshold.</p><p>Once the threshold is set, the method ranks the real images according to their similarity to each generated image and calculates their average rank. Finally, if this average rank is higher than the threshold, then the image is classified as "used", as it shows high similarity with respect to the generated images. Otherwise, the image is classified as "not used".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Clustering</head><p>The clustering approach finds outliers in the data, classifying them as "not used". First, the method maps both generated and real images into a common space. Then, it uses the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) <ref type="bibr" coords="6,369.25,639.70,17.80,10.91" target="#b13">[14]</ref> algorithm to form clusters, as depicted in Figure <ref type="figure" coords="6,187.32,653.25,8.26,10.91" target="#fig_3">3c</ref>, and to find outliers. In this approach, we define outliers as images whose similarity to any other data point is lower than the sum of the average similarity with three times its standard deviation. Outliers identified in the subset of real images are classified as "not used", while the remaining images are classified as "used".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Ensemble</head><p>Since some of the proposed methods may be particularly good at identifying a specific subset of either used or not used images, we also implemented an ensemble model that merges the results of the different methods. For example, the retrieval method may be good at identifying a subset of the not used images, while the threshold method using the maximum similarity between two real images may be good at identifying a subset of the used images.</p><p>The ensemble model uses the results of ranking as a base. Then, it changes the class of the images that were classified as "used" by the threshold method to "used", independently from the class assigned by the ranking approach. Finally, it alters the class of the images that were classified as "not used" by the retrieval approach to "not used". Images that are simultaneously classified as "used" by the threshold method and "not used" by the retrieval method, are assigned the class "not used".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Autoencoder-based Methods</head><p>In the autoencoder-based methods, we devised different strategies to train autoencoders. We used two main methods to classify the images using autoencoders:</p><p>‚Ä¢ Computing the similarity between the images based on their latent representations, enabling the direct application of the techniques defined in the previous section. ‚Ä¢ Applying outlier detection techniques to identify data points from the real data that do not follow the probability distribution of the generated data.</p><p>We experimented with two types of architectures of autoencoders:</p><p>‚Ä¢ Basic Autoencoder: The encoder contains 5 stridden convolution layers with batch normalization and Leaky ReLU as the activation function. The decoder contains 5 convolution layers with upsampling, batch normalization and Leaky ReLU as the activation function.</p><p>‚Ä¢ ResNet Autoencoder: The encoder and decoder are composed of 5 blocks of convolution layers with batch normalization, Leaky ReLU as the activation function, and residual layers, similar to those of a ResNet <ref type="bibr" coords="7,273.59,540.81,16.25,10.91" target="#b11">[12]</ref>.</p><p>The following subsections explain in detail the approaches that we developed based on autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Outlier Detection with Autoencoder trained on Generated Data</head><p>The first autoencoder-based approach consists of using an autoencoder trained on the generated data to detect outliers among the real images. We start by splitting the generated data into training (95% of the data) and validation (5% of the data). Then, we train the autoencoder on the training data for 200 epochs, using a reconstruction loss to minimize the mean squared error between its input and output, as depicted in Figure <ref type="figure" coords="8,315.35,86.97,3.68,10.91" target="#fig_5">4</ref>. Afterwards, we apply the autoencoder to the validation data, measuring the corresponding reconstruction error, which is used to compute a threshold. In particular, we considered two thresholds: the maximum of the reconstruction error on the validation data (MAX), and the sum of the average of the reconstruction error with two times its standard deviation (AVG).  Finally, we apply the autoencoder to the real data and measure the reconstruction error. Images whose reconstruction error is higher than the threshold are images whose probability distribution deviates from the probability distribution of the generated data, and are, therefore, classified as "not used". All other real images are classified as "used".</p><p>Since this network was only trained on the generated data, we do not use its latent representations to compute the similarity between real and generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Autoencoder trained on Generated and Real Data</head><p>In a second approach, we train the autoencoder depicted in Figure <ref type="figure" coords="8,373.85,415.08,4.97,10.91" target="#fig_5">4</ref> simultaneously with real and generated data, minimizing the reconstruction error between its input and output. On inference, we compute the latent representations of the images and calculate a similarity metric based on the mean squared error between the latent vectors, which is used to apply the previously defined similarity-based techniques.</p><p>On the test dataset, we devised two experiments. In the first experiment, we use all the 10,000 generated images in addition to the 200 real images to train the autoencoder. In a second experiment, we use only 600 images of the generated data and all the real images, to emulate the dimensions of the development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Autoencoder with one Encoder and two Decoders</head><p>As the final autoencoder-based approach, we train an autoencoder with one encoder and two decoders, as depicted in Figure <ref type="figure" coords="8,231.10,586.35,3.80,10.91" target="#fig_7">5</ref>. The encoder receives both generated and real data. One of the decoders receives latent features of the real data, while the other receives latent features of the generated data. In each epoch, we provide a real image and a generated image to the network, and apply each decoder to the corresponding image, minimizing the reconstruction error between the inputs of the encoder and the output of each of the decoders. Thus, the decoders are trained simultaneously.  On inference, we apply the decoder of the generated images to the real images to detect outliers, in a similar manner to the method described in section 3.2.1. Moreover, we use the encoder to obtain latent features, which are used to calculate the similarity between images and apply the previously defined similarity-based techniques to classify the real samples. We define similarity as the opposite of the mean squared error between two feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Patch-based Methods</head><p>The patch-based methods extract patches from images and perform the operations described in the following subsections to classify the real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Matching Patches using Triplet Loss</head><p>The first patch-based approach consists of a model that compares image patches and predicts whether they belong to the same image. It aims to verify whether a generated image is sufficiently similar to a real image so that the model predicts that their patches belong to the same image.</p><p>The model is a Convolutional Neural Network (CNN) that extracts features from image patches and calculates the distance between them. We train the network using a triplet loss that maximizes the Euclidean distance between patches from different images and minimizes the distance between patches of the same image, as depicted in Figure <ref type="figure" coords="9,412.32,527.41,3.81,10.91" target="#fig_9">6</ref>. The model is only trained on patches from real images. Since generated images are slightly blurrier than the real images, we add Gaussian noise to the latent representations of the real images during training, to emulate the lack of quality of the generated images. The network is trained for 800 epochs and then applied to compare patches from real and generated images, to verify whether patches from generated images can be identified as belonging to a real image.</p><p>On inference, we calculate the Euclidean distance between patches of generated and real images and verify whether this distance is lower than the maximum distance between patches of the same image on the real data. For each generated image, if there is at least one real image whose distance is lower than the threshold, then that real image is classified as "used". Real images for which there is no generated image that is similar to it are classified as "not used".  In our implementation, the feature extractor contains 5 stridden convolution layers with batch normalization and Leaky ReLU as the activation function, followed by global average pooling and one fully-connected layer with linear activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Replacing Patches</head><p>In a final approach, we extract patches from all the generated images and put these patches in the same position on the real images, modifying them. Then, we pass the modified images through the autoencoder trained on all the data (from section 3.2.2) and verify its reconstruction error, as depicted in Figure <ref type="figure" coords="10,186.04,464.17,3.81,10.91" target="#fig_10">7</ref>. Real images that contain modified images with a low reconstruction error are similar to the generated images used to build the modified images and are, therefore, classified as "used". The remaining images are classified as "not used".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Decoder</head><p>Generated Image Real Image </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Table <ref type="table" coords="11,117.23,111.28,5.17,10.91">2</ref> exposes the results in terms of accuracy, precision, specificity, recall and F1-score, obtained with each approach on the development dataset. The methods that were applied to the test data are highlighted in bold. The official metric of the competition is the F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results on the development dataset. Metric refers to the similarity metric that was used to compute the similarity between images in the similarity-based approaches. AE stands for autoencoder. Comparing the similarity-based approaches, the ensemble method seems to lead to the best results across the methods using different similarity metrics to compare images. The threshold (MAX) and clustering approaches seem to be particularly good at identifying a subset of the used images, presenting high precision, suggesting that most of the images predicted as used were, indeed, used. Nonetheless, these approaches tend to present low recall, failing to detect a considerable amount of used images. The retrieval approach seems to be particularly good at detecting a subset of the not used images, as proven by its high recall value, indicating that most of the used images are classified as such. Nevertheless, its low specificity suggests that there is a substantial amount of misclassified not used images. Ranking seems to be a more balanced approach, presenting similar values of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The best results on the development dataset were obtained by comparing the latent representations of the simple autoencoder trained on both generated and real images, using the ranking approach. This method achieved an accuracy of 0.850 and an F1-score of 0.846. The method using the autoencoder with two decoders to compare the latent representations of the images achieved the worst results among the different metrics of the similarity-based approaches.</p><p>The methods using autoencoders for outlier detection were not capable of achieving higher results than the similarity-based methods. In these methods, using the maximum reconstruction error on the validation data as a threshold (MAX) leads to worse accuracy but better F1-score than using the average reconstruction error as a threshold (AVG).</p><p>Regarding the patch-based methods, the method that matches patches to verify whether two patches belong to the same image was incapable of distinguishing between used and not used images, achieving only 0.525 of accuracy on a balanced dataset. Its high recall and low specificity indicate that the method classifies most images as used. Nevertheless, the high recall led this method to achieve a slightly higher F1-score than the method that replaces patches of real images with ones from generated images. Replacing patches leads, however, to a considerably higher accuracy, showing a higher capacity of distinguishing between used and not used images.</p><p>Figure <ref type="figure" coords="12,130.92,439.25,5.01,10.91" target="#fig_11">8</ref> compares the visual results of the simple autoencoder with the ResNet autoencoder. The ResNet autoencoder seems to lead to higher-quality images. Nonetheless, the simple autoencoder seems to achieve better results in terms of F1-score and accuracy than the ResNet autoencoder, across the different approaches. The results of accuracy, precision, specificity, recall and F1-score obtained by some of the proposed approaches on the test data are presented in Table <ref type="table" coords="12,360.17,660.34,3.76,10.91" target="#tab_2">3</ref>. Submissions 5 and 6 represent the same approach but with models trained with different amounts of data. In submission 5, the autoencoder was trained on all 10,000 generated images and 200 real images. In submission 6, the autoencoder was only trained with 600 generated images and 200 real images. Unlike in the development dataset, the ranking and ensemble methods obtained the same results when applied to the test data. The method that achieved the best results on the test data was threshold (MAX) using SSIM as a similarity metric, achieving the highest accuracy and F1-score out of all the methods. Furthermore, ranking and retrieval using SSIM also present high F1-score, when compared to all other approaches. Retrieval seems to present high F1-Score, despite its relatively low accuracy, due to classifying most images as used, as can be seen in its high recall and low specificity.</p><p>Using a simple autoencoder to perform outlier detection led to the second-highest accuracy, but a relatively low F1-score. Ranking using the embeddings of the simple autoencoder trained with different amounts of data provided very similar results. Nonetheless, this method led to considerably worse results in the test dataset than in the development dataset, where it achieved the best results of F1-score.</p><p>The method that replaces patches obtained comparable results to these similarity-based methods using autoencoders. The worst results on the test dataset were obtained through the matching patches method and ranking using the latent representations of images obtained with a pre-trained ResNet model. Both these methods failed to distinguish between used and not used images, achieving low accuracy and F1-score.</p><p>The results on the development dataset differ substantially from the results on the test dataset, perhaps due to the difference in the dimensions of the datasets. Despite the differences in results between the test and development datasets, we were capable of developing approaches that achieved high F1-score and accuracy at identifying the used images for both sets. These results support the hypothesis that the synthetic images generated using deep generative models expose the identity of patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This paper described the work developed by the VCMI team for the ImageCLEFmedical GANs task. The goal of the task was to verify whether a set of images generated using a deep generative model contained identifiable properties of the data used to train the network. As such, we proposed various approaches to solve the binary classification task of classifying a set of real images according to whether they were used in the training of the networks that generated a set of synthetic images. The experiments confirmed the hypothesis that synthetic data threatens the privacy of the training data, as some of the proposed methods achieved high accuracy and F1-score on the datasets of the challenge.</p><p>One of the limitations of the proposed methods is that they do not consider that the classification task was only applied to a subset of the real images. As such, there may be synthetic images that threaten the privacy of real images not provided to us during the challenge and that may be similar to provided not used images, leading to their misclassification. To prevent this issue, future work considers the separation of the classification process into two steps: identifying which of the generated images threaten patient privacy of the provided subset of images, by obtaining the subset of generated images whose probability distribution is the most similar to the provided real images, and matching each of those generated images to the most similar real images. Future work will also consider the further development of the proposed methods.</p><p>To conclude, this paper, as well as the ImageCLEFmedical GANs challenge, serve to raise awareness about the potential privacy risks of using and sharing synthetic medical data in real-world applications. We highlight the importance of implementing privacy-preserving techniques when developing deep generative models on sensitive medical data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,214.83,401.11,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the relationship between the probability distribution of the task's datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,213.57,361.63,8.93;5,372.64,119.64,133.35,71.56"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pairs of the most similar images between the sets of the development dataset.</figDesc><graphic coords="5,372.64,119.64,133.35,71.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,221.45,416.70,8.93;6,88.93,233.45,417.05,8.87;6,89.29,245.41,416.70,8.87;6,89.29,257.36,416.88,8.87;6,89.29,269.32,416.70,8.87;6,89.29,281.28,416.70,8.87;6,89.29,293.23,167.72,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:Representative example of the similarity-based approaches. Diamonds represent real images while circles represent generated images. The lighter colors represent samples that are classified as used by the methods, while darker colors represent outliers that are classified as not used. The threshold approach checks whether the distance between each real image and its closest generated image is higher than a threshold. The retrieval approach verifies whether a real image is the closest image to any of the generated images. The clustering approach forms a cluster based on the distance between all data samples from real and generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,267.99,232.11,8.93;8,114.11,175.03,50.56,50.43"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of autoencoder for outlier detection.</figDesc><graphic coords="8,114.11,175.03,50.56,50.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,89.29,241.71,225.81,8.93;9,119.52,172.08,50.56,50.43"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of autoencoder with two decoders.</figDesc><graphic coords="9,119.52,172.08,50.56,50.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,89.29,309.49,289.11,8.93;10,119.37,192.42,101.90,101.90"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of method that matches patches using triplet loss.</figDesc><graphic coords="10,119.37,192.42,101.90,101.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="10,89.29,658.39,217.69,8.93;10,97.44,528.57,74.84,74.84"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overview of method that replaces patches.</figDesc><graphic coords="10,97.44,528.57,74.84,74.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,89.29,605.19,416.69,8.93;12,89.29,617.20,145.06,8.87;12,138.05,502.60,316.69,96.00"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Reconstruction of a synthetic image (left image) using the simple (middle image) and the ResNet autoencoders (right image).</figDesc><graphic coords="12,138.05,502.60,316.69,96.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,423.57,416.99,135.12"><head>Table 1</head><label>1</label><figDesc>Exploratory data analysis presenting the average, minimum and maximum SSIM between images of the subsets of the development and test datasets.</figDesc><table coords="4,109.65,467.14,375.97,91.55"><row><cell>Dataset</cell><cell>Subsets</cell><cell cols="3">Average SSIM Minimum SSIM Maximum SSIM</cell></row><row><cell></cell><cell>Real-Real</cell><cell>41.00%</cell><cell>16.59%</cell><cell>59.49%</cell></row><row><cell>Development</cell><cell>Generated-Real Generated-Used</cell><cell>42.46% 42.58%</cell><cell>21.08% 27.11%</cell><cell>73.94% 73.94%</cell></row><row><cell></cell><cell>Generated-Not Used</cell><cell>42.34%</cell><cell>21.08%</cell><cell>64.39%</cell></row><row><cell>Test</cell><cell>Real-Real Generated-Real</cell><cell>40.89% 42.34%</cell><cell>15.82% 17.07%</cell><cell>60.27% 83.67%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,88.99,170.17,406.56,179.72"><head>Table 3</head><label>3</label><figDesc>Results on the test dataset. "S. " refers to the submission number. AE stands for autoencoder.</figDesc><table coords="13,99.73,199.57,395.82,150.33"><row><cell>S. Method</cell><cell>Metric</cell><cell cols="5">Accuracy Precision Specificity Recall F1-Score</cell></row><row><cell>1 Ranking / Ensemble</cell><cell></cell><cell>0.685</cell><cell>0.637</cell><cell>0.51</cell><cell>0.86</cell><cell>0.731</cell></row><row><cell>2 Threshold (MAX)</cell><cell>SSIM</cell><cell>0.810</cell><cell>0.836</cell><cell>0.850</cell><cell>0.770</cell><cell>0.802</cell></row><row><cell>3 Retrieval</cell><cell></cell><cell>0.590</cell><cell>0.550</cell><cell>0.190</cell><cell>0.990</cell><cell>0.707</cell></row><row><cell cols="2">5 Ranking / Ensemble Simple AE</cell><cell>0.635</cell><cell>0.645</cell><cell>0.670</cell><cell>0.600</cell><cell>0.621</cell></row><row><cell cols="2">6 Ranking / Ensemble Simple AE</cell><cell>0.635</cell><cell>0.658</cell><cell>0.710</cell><cell>0.560</cell><cell>0.605</cell></row><row><cell cols="2">7 Ranking / Ensemble ResNet AE</cell><cell>0.615</cell><cell>0.616</cell><cell>0.620</cell><cell>0.610</cell><cell>0.613</cell></row><row><cell>8 Ranking / Ensemble</cell><cell>ResNet</cell><cell>0.460</cell><cell>0.458</cell><cell>0.480</cell><cell>0.440</cell><cell>0.448</cell></row><row><cell>4 Simple AE (AVG)</cell><cell>-</cell><cell>0.720</cell><cell>0.854</cell><cell>0.910</cell><cell>0.530</cell><cell>0.654</cell></row><row><cell>9 Matching Patches</cell><cell>-</cell><cell>0.500</cell><cell>0.500</cell><cell>0.470</cell><cell>0.530</cell><cell>0.514</cell></row><row><cell>10 Replacing Patches</cell><cell>-</cell><cell>0.615</cell><cell>0.693</cell><cell>0.770</cell><cell>0.520</cell><cell>0.594</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,671.02,199.39,8.97"><p>http://www.clef-initiative.eu (accessed on: 03-06-2023)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,92.57,671.03,238.28,8.97"><p>https://github.com/helenaMontenegro/imageclef23-medical-gans</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is financed by <rs type="funder">National Funds</rs> through the <rs type="funder">FCT -Funda√ß√£o para a Ci√™ncia e a Tecnologia</rs>, I.P. (<rs type="funder">Portuguese Foundation for Science and Technology)</rs> within the project <rs type="projectName">CAGING</rs>, with reference 2022.<rs type="grantNumber">10486</rs>.PTDC, and within PhD grants <rs type="grantNumber">2020.06434.BD</rs>, <rs type="grantNumber">2020.07034.BD</rs>, <rs type="grantNumber">2021.06872.BD</rs>, <rs type="grantNumber">2022.11566.BD</rs>, <rs type="grantNumber">2022.14516</rs>.BD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_dU2WvK6">
					<idno type="grant-number">10486</idno>
					<orgName type="project" subtype="full">CAGING</orgName>
				</org>
				<org type="funding" xml:id="_Bkfrng7">
					<idno type="grant-number">2020.06434.BD</idno>
				</org>
				<org type="funding" xml:id="_DD6Jv5s">
					<idno type="grant-number">2020.07034.BD</idno>
				</org>
				<org type="funding" xml:id="_scxXFqn">
					<idno type="grant-number">2021.06872.BD</idno>
				</org>
				<org type="funding" xml:id="_39Y9x9j">
					<idno type="grant-number">2022.11566.BD</idno>
				</org>
				<org type="funding" xml:id="_6kTrZGf">
					<idno type="grant-number">2022.14516</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,540.16,394.53,10.91;14,112.66,553.71,393.33,10.91;14,112.66,567.26,221.81,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ostmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03144</idno>
		<title level="m" coord="14,112.66,553.71,393.33,10.91;14,112.66,567.26,39.20,10.91">High-resolution medical image synthesis using progressively grown generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,580.81,395.00,10.91;14,112.66,594.36,393.60,10.91;14,112.66,607.91,146.44,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P.-D</forename><surname>Tudosiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05692</idno>
		<title level="m" coord="14,155.14,594.36,318.37,10.91">Neuromorphologicaly-preserving volumetric data encoding using vq-vae</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,621.46,395.17,10.91;14,112.66,635.01,394.53,10.91;14,112.66,648.56,394.62,10.91;15,112.66,86.97,393.98,10.91;15,112.41,100.52,32.84,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,112.66,648.56,394.62,10.91;15,112.66,86.97,234.49,10.91">Synthetic medical images for robust, privacy-preserving training of artificial intelligence: application to retinopathy of prematurity diagnosis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Coyner</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Singh</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ostmo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Chiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Imaging</surname></persName>
			<affiliation>
				<orgName type="collaboration">Retinopathy of Prematurity Consortium</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,358.47,86.97,108.72,10.91">Ophthalmology Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100126</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,393.33,10.91;15,112.66,127.61,393.33,10.91;15,112.28,141.16,135.89,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,315.04,114.06,190.95,10.91;15,112.66,127.61,185.44,10.91">Synthesizing electronic health records using improved generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Baowaly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,306.91,127.61,199.08,10.91;15,112.28,141.16,51.96,10.91">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="228" to="241" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,154.71,395.17,10.91;15,112.66,168.26,393.61,10.91;15,112.66,181.81,176.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,439.87,154.71,67.97,10.91;15,112.66,168.26,393.61,10.91;15,112.66,181.81,17.07,10.91">Deep learningbased patient re-identification is able to exploit the biometric nature of medical chest x-ray data</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Packh√§user</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>G√ºndel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>M√ºnster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Syben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Christlein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,137.92,181.81,77.89,10.91">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14851</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,393.61,10.91;15,112.66,208.91,393.33,10.91;15,112.66,222.46,395.01,10.91;15,112.41,236.01,38.81,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,222.08,195.36,284.19,10.91;15,112.66,208.91,107.31,10.91">Low-effort re-identification techniques based on medical imagery threaten patient privacy</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C M</forename><surname>Esmeral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,244.55,208.91,261.44,10.91;15,112.66,222.46,47.50,10.91">Medical Image Understanding and Analysis: 26th Annual Conference</title>
		<meeting><address><addrLine>MIUA; Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-07-27">2022. July 27-29, 2022. 2022</date>
			<biblScope unit="page" from="719" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,249.56,393.33,10.91;15,112.66,263.11,393.33,10.91;15,112.66,276.66,393.33,10.91;15,112.66,290.20,356.67,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,448.60,249.56,57.38,10.91;15,112.66,263.11,335.22,10.91">Overview of ImageCLEFmedical GANs 2023 Task -Identifying Training Data &quot;Fingerprints</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,464.74,263.11,41.25,10.91;15,112.66,276.66,287.68,10.91;15,421.96,276.66,84.03,10.91;15,112.66,290.20,23.42,10.91">Synthetic Biomedical Images Generated by GANs for Medical Image Security</title>
		<title level="s" coord="15,143.49,290.20,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>CLEF2023 Working Notes</note>
</biblStruct>

<biblStruct coords="15,112.66,303.75,394.52,10.91;15,112.66,317.30,395.17,10.91;15,111.81,330.85,395.37,10.91;15,112.66,344.40,394.53,10.91;15,112.28,357.95,395.55,10.91;15,112.66,371.50,393.33,10.91;15,112.66,385.05,395.16,10.91;15,112.66,398.60,393.32,10.91;15,112.66,412.15,394.53,10.91;15,112.33,425.70,120.27,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,200.85,371.50,305.14,10.91;15,112.66,385.05,209.25,10.91">Overview of ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Recommender Systems Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-M</forename><surname>DrƒÉgulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stor√•s</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sch√∂ler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,345.04,385.05,162.79,10.91;15,112.66,398.60,393.32,10.91;15,112.66,412.15,128.49,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="15,273.28,412.15,228.17,10.91">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,394.53,10.91;15,112.34,452.79,393.64,10.91;15,112.66,466.34,183.21,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,161.80,452.79,122.30,10.91">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,307.59,452.79,198.39,10.91;15,112.66,466.34,33.92,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,479.89,393.54,10.91;15,112.39,493.44,395.28,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,342.90,479.89,163.30,10.91;15,112.39,493.44,136.73,10.91">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,256.79,493.44,168.86,10.91">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,506.99,394.53,10.91;15,112.28,520.54,275.03,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,197.98,506.99,304.99,10.91">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,112.28,520.54,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,534.09,395.17,10.91;15,112.66,547.64,395.01,10.91;15,112.41,561.19,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,259.74,534.09,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,488.38,534.09,19.45,10.91;15,112.66,547.64,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,574.74,393.33,10.91;15,112.66,588.29,394.53,10.91;15,112.66,601.84,103.61,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,346.64,574.74,159.35,10.91;15,112.66,588.29,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,228.08,588.29,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,615.39,393.33,10.91;15,112.66,628.93,374.07,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,318.01,615.39,187.97,10.91;15,112.66,628.93,194.14,10.91">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
