<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.05,75.20,451.05,16.92;1,72.05,96.08,397.89,16.92">SSN MLRG at MEDVQA-GI 2023: Visual Question Generation and Answering using Transformer based Pre-trained Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.05,138.76,66.88,11.88"><forename type="first">Sheerin</forename><surname>Sitara</surname></persName>
							<email>sheerinsitaran@ssn.edu.in</email>
						</author>
						<author>
							<persName coords="1,141.81,138.76,76.95,11.88"><forename type="first">Noor</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.74,138.76,89.24,11.88"><forename type="first">Kavitha</forename><surname>Srinivasan</surname></persName>
							<email>kavithas@ssn.edu</email>
						</author>
						<author>
							<persName coords="1,336.82,138.76,120.44,11.88"><forename type="first">Raghuraman</forename><surname>Gopalsamy</surname></persName>
							<email>raghuramang@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.05,75.20,451.05,16.92;1,72.05,96.08,397.89,16.92">SSN MLRG at MEDVQA-GI 2023: Visual Question Generation and Answering using Transformer based Pre-trained Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F00613266A346FF10AFC9D2E9B1967C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical VQA</term>
					<term>Medical VQG</term>
					<term>ImageCLEF</term>
					<term>Vision Transformer</term>
					<term>SegFormer</term>
					<term>VisualBERT</term>
					<term>Category based Medical Visual Question Generation</term>
					<term>QA-pairs 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The technological development in current era demands the need of Artificial Intelligence (AI) in all fields. The AI in medical field is not an exception for various real time applications as per user demands. The applications are medical report summarization, image captioning, Visual Question Answering (VQA) and Visual Question Generation (VQG). ImageCLEF is one of the forum which constantly conducing the challenges in these applications. In this paper, for the given MEDVQA-GI dataset, three medical VQA and one medical VQG models are proposed. The medical VQA models are developed using VisionTransformer (ViT), SegFormer and VisualBERT techniques through a combination of eighteen QA-pairs based on categories and resulted an accuracy of 95.6%, 95.7% and 62.4% respectively. Also, the proposed medical VQG model is developed using Category based Medical Visual Question Generation (CMVQG) technique only.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.44" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Medical Visual Question Answering and Generation is an challenging field in Natural Language Processing (NLP) and Computer Vision because of the complex nature of both image and text. The ImageCLEF <ref type="bibr" coords="1,173.47,510.28,13.07,10.04" target="#b0">[1]</ref> , an online evaluation forum analysis the current trends and conducing the research related tasks since 2018. In 2018 <ref type="bibr" coords="1,256.68,523.98,13.25,10.04" target="#b1">[2]</ref> and 2019 <ref type="bibr" coords="1,315.15,523.98,11.53,10.04" target="#b2">[3]</ref>, they concentrate on visual questions related to different organs, planes, modalities and abnormalities. Then in 2020 <ref type="bibr" coords="1,390.48,537.66,12.70,10.04" target="#b3">[4]</ref> and 2021 <ref type="bibr" coords="1,450.16,537.66,11.81,10.04" target="#b4">[5]</ref>, ImageCLEF concentrated on abnormality type questions alone based on the inferences made from previous two years. This year <ref type="bibr" coords="1,153.05,565.02,11.53,10.04" target="#b5">[6]</ref>, they are conducing a task for colonoscopy based Visual Question Answering (VQA) and Visual Question Generation (VQG).</p><p>In VQA and VQG datasets given by ImageCLEF is based on the HyperKvasir dataset and Kvasir Instrument dataset. These datasets are used to develop proposed models using suitable techniques and evaluated performance metrics, are discussed in the following paragraphs. The VQA approaches are: joint embedding <ref type="bibr" coords="2,145.54,87.11,11.81,10.04" target="#b6">[7]</ref>, hybrid, compositional and transformer based techniques. Among these techniques, transformer based techniques is chosen because it hold the potential to understand the relationship between sequential elements and performs parallel processing more quickly. The different transformer based techniques used in this paper are, VisionTransformer (ViT) <ref type="bibr" coords="2,359.51,125.27,11.53,10.04" target="#b7">[8]</ref>, SegFormer <ref type="bibr" coords="2,428.24,125.27,11.55,10.04" target="#b8">[9]</ref>, VisualBERT <ref type="bibr" coords="2,505.39,125.27,18.47,10.04" target="#b9">[10]</ref> and Category based Medical Visual Question Generation (CMVQG for VQG). The reason behind choosing these techniques are, (i). ViT incorporates more global information then other pre-trained model at lower layers, leading to quantitatively different features (ii). VisualBERT is a simple and flexible framework for modelling a broad range of vision and language tasks (iii). SegFormer has its own advantage over the speed, accuracy, number of parameters (iv). The CMVQG generates the questions based on the category instead of answer in the VQA dataset.</p><p>The rest of the paper is organized as follows. Section 2 explains about the MEDVQA-GI 2023 task and dataset description. Section 3 briefs the system design of the proposed VQA and VQG models. Section 4 analyses the results using suitable quantitative metric, and Section 5 concludes with the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Dataset Description</head><p>In this section, two sub tasks of MEDVQA-GI 2023 and given dataset is explained. The two sub tasks includes, Visual Question Answering (VQA) and Visual Question Generation (VQG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ImageCLEF MEDVQA-GI 2023 task</head><p>ImageCLEF, a part of Conference and Labs of the Evaluation Forum is conducting tasks related to the medical domain since 2018. Its goal is that through the combination of text and image data the output of the analysis gets easier to use by medical experts.</p><p>In sub task A (VQA), the answer to the colonoscopy image needs to be generated with respect to the given the question-answer pairs. For example, given the image containing the colon polyp with the question, "What type of polyp is present?". Then the answer should be textual description of the type of the polyp located in the image.</p><p>In sub task B (VQG), the question to the colonoscopy image need to be generated based on the significant informantion present in the image and answer. The significant information includes, location, count, color, size, shape of the polyps, modality, abnormality type, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">ImageCLEF MEDVQA-GI 2023 dataset</head><p>The MEDVQA-GI 2023 task consists of two sub tasks namely, VQA and VQG. The image, mask and QA-pairs for both tasks are given in Table <ref type="table" coords="2,281.57,596.72,4.27,10.04" target="#tab_0">1</ref>. Each sub task consists of training set and test set. In these tasks, 18 QA-pairs are associated with each image so the count of QA-pairs is eighteen times the number of images. These questions are tabulated along with the frequent answers for each question and categories in Table <ref type="table" coords="2,158.05,637.79,4.27,10.04" target="#tab_1">2</ref>. Based on these categories, VQA and VQG model is generated and it is discussed in Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Design</head><p>The system design of the proposed medical VQA and VQG models are shown in Figures <ref type="figure" coords="3,494.49,558.18,8.54,10.04" target="#fig_0">1,</ref><ref type="figure" coords="3,506.33,558.18,4.10,10.04" target="#fig_1">2</ref>, 3 and 4. For medical VQA, three models are developed using VisionTransformer (ViT), SegFormer and VisualBERT based on its categories and one medical VQG model created using Category based Medical Visual Question Generation (CMVQG) techniques as given in Table <ref type="table" coords="3,415.64,599.24,4.31,10.04">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3 Dataset category description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Techniques</head><p>Categories Justification ViT Classification type QA-pairs (6) Possible to apply transformer architecture with selfattention for sequence of images without using convolutional layers. Numeric oriented QA-pairs <ref type="bibr" coords="3,264.41,692.86,12.37,11.16" target="#b2">(3)</ref> VisualBERT Location oriented QA-pairs (3) Capable of representing significant object in an image by a bounding region. SegFormer Color oriented QA-pairs <ref type="bibr" coords="3,249.98,740.41,12.37,11.16" target="#b5">(6)</ref> Feasible to do image segmentation by identifying different segment using "binary mask classification"</p><p>The three VQA models are developed using the colonoscopy image and QA-pairs in the training set and, is validated by predicting the label for the test set. The Vision Transformer (ViT) VQA model is developed for QA pairs under classification or numeric type and is shown in Figure <ref type="figure" coords="4,471.59,101.51,4.27,10.04" target="#fig_0">1</ref>. The ViT divides the input image into patches of 16×16 pixels and linearly projects the flattened patches. The QA-pairs are converted into tokens using patch and position embedding. Based on the patches and tokens, the model is trained autoregressively for predicting the next token under causal (or unidirectional) self-attention using Multi Layer Perceptron (MLP). The model was implemented using the Vision Encoder Decoder class from the Hugging Face Transformers library and tiny Data-efficient image Transformer (DeiT) pre-trained on ImageNet dataset.  In Figure <ref type="figure" coords="4,132.14,671.63,4.27,10.04">3</ref>, the images were divided into small patches which favor the dense prediction task. These patches are given as input to the attention layer to obtain multi-level features of the original image resolution. It is then passed to Multi-Layer Perceptron to implicitly discover useful alignments between both sets of inputs in terms of color and build up a new joint representation in the training phase. Finally, the generated model is validated by predicting the answers for the color oriented questions for the given colonoscopy image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.05,368.00,186.31,11.16;4,90.05,391.07,433.39,10.04;4,72.05,403.69,451.20,10.04;4,72.05,416.29,451.60,10.04;4,72.05,428.89,451.34,10.04;4,72.05,441.49,451.46,10.04;4,72.05,454.45,451.56,10.04;4,72.05,467.08,382.52,10.04;4,90.00,202.68,451.08,162.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: VQA system using ViT techniqueThe VisuaBERT is an extension of Bidirectional Encoder Representations from Transformers (BERT), model an image with respect to the bounding region in the image. In VisualBERT, the tokens and vocabulary lists are generated using position and segment embedding and it is concatenated with image features to generate model during the training phase. Faster Region based Convolutional Neural Networks (RCNN) is used in order to extract the features from an image and to represent the segmented region with the bounding box. From the segmented region, the appearance features are extracted and is then embedded with the text features to generate the model and it is shown in Figure2.</figDesc><graphic coords="4,90.00,202.68,451.08,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.05,648.92,221.21,11.16;4,72.00,489.60,457.20,155.52"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: VQA system using VisualBERT technique</figDesc><graphic coords="4,72.00,489.60,457.20,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,72.00,380.88,446.40,194.40"><head></head><label></label><figDesc></figDesc><graphic coords="5,72.00,380.88,446.40,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.05,673.43,408.33,86.42"><head>Table 1</head><label>1</label><figDesc>Dataset description for MEDVQA-GI 2023 task</figDesc><table coords="2,77.45,707.98,402.93,51.87"><row><cell>Task</cell><cell>Input/Output</cell><cell>Training Set</cell><cell>Test Set</cell></row><row><cell>VQA/ VQG</cell><cell>Image</cell><cell>2000</cell><cell>1949</cell></row><row><cell></cell><cell>Mask</cell><cell>500</cell><cell>-</cell></row><row><cell></cell><cell>QA pairs</cell><cell>36000</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.05,74.51,442.37,435.74"><head>Table 2</head><label>2</label><figDesc>Question and its frequent answers in MEDVQA-GI 2023 task</figDesc><table coords="3,77.45,109.04,436.97,401.21"><row><cell>Q.No</cell><cell>Questions</cell><cell>Frequent Answers</cell><cell>Categories</cell></row><row><cell>1</cell><cell>Are there any anatomical</cell><cell>Z-line, Not relevant,</cell><cell>Classification type QA-pairs</cell></row><row><cell></cell><cell>landmarks in the image?</cell><cell>Cecum, No</cell><cell></cell></row><row><cell>2</cell><cell>How many findings are present?</cell><cell>0, 1, 2</cell><cell>Numeric oriented QA-pairs</cell></row><row><cell>3</cell><cell>How many instruments are in the</cell><cell>0, 1</cell><cell>Numeric oriented QA-pairs</cell></row><row><cell></cell><cell>image?</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="2">How many polyps are in the image? 0, 1</cell><cell>Numeric oriented QA-pairs</cell></row><row><cell>5</cell><cell cols="2">Is there a green/black box artefact? Yes, No</cell><cell>Color oriented QA-pairs</cell></row><row><cell>6</cell><cell>Is there text?</cell><cell>Yes, No</cell><cell>Classification type QA-pairs</cell></row><row><cell>7</cell><cell>Are there any abnormalities in the</cell><cell>Polyps, Oesophagitis,</cell><cell>Classification type QA-pairs</cell></row><row><cell></cell><cell>image?</cell><cell>Ulcerative colitis, No</cell><cell></cell></row><row><cell>8</cell><cell>Is this finding easy to detect?</cell><cell>Yes, No</cell><cell>Location oriented QA-pairs</cell></row><row><cell>9</cell><cell>What color is the anatomical</cell><cell>Not relevant, Pink</cell><cell>Color oriented QA-pairs</cell></row><row><cell></cell><cell>landmark?</cell><cell></cell><cell></cell></row><row><cell>10</cell><cell>Are there any instruments in the</cell><cell>Not relevant, Tube,</cell><cell>Classification type QA-pairs</cell></row><row><cell></cell><cell>image?</cell><cell>No</cell><cell></cell></row><row><cell>11</cell><cell>Have all polyps been removed?</cell><cell>Not relevant, No</cell><cell>Location oriented QA-pairs</cell></row><row><cell>12</cell><cell>What type of polyp is present?</cell><cell>Not irrelevant, Paris</cell><cell>Classification type QA-pairs</cell></row><row><cell></cell><cell></cell><cell>ip and Paris iia</cell><cell></cell></row><row><cell>13</cell><cell>What type of procedure is the</cell><cell>Colonoscopy</cell><cell>Classification type QA-pairs</cell></row><row><cell></cell><cell>image taken from?</cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>Where in the image is the</cell><cell>Centre, Not relevant</cell><cell>Location oriented QA-pairs</cell></row><row><cell></cell><cell>anatomical landmark?</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell>Where in the image is the</cell><cell>Centre, Not relevant</cell><cell>Location oriented QA-pairs</cell></row><row><cell></cell><cell>instrument?</cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>Where exactly in the image is the</cell><cell></cell><cell></cell></row><row><cell></cell><cell>polyp located?</cell><cell></cell><cell></cell></row><row><cell>17</cell><cell>What color is the abnormality?</cell><cell>Pink, red, white; Pink,</cell><cell>Color oriented QA-pairs</cell></row><row><cell></cell><cell></cell><cell>red; Not relevant</cell><cell></cell></row><row><cell>18</cell><cell>Have all polyps been removed?</cell><cell>Not relevant, No</cell><cell>Location oriented QA-pairs</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>Our profound gratitude to <rs type="institution">Sri Sivasubramaniya Nadar College</rs> <rs type="affiliation">of Engineering, Department of CSE</rs>, for allowing us to utilize the <rs type="institution">High Performance Computing Laboratory</rs> and GPU Server for the execution of this challenge successfully.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For medical VQG model generation, Category based Medical Visual Question Generation (CMVQG) approach is used and it is shown in Figure <ref type="figure" coords="5,327.73,213.52,4.27,10.04">4</ref>. In the proposed CMVQG approach, the Convolutional Neural Network (CNN), Multi-Layer Perceptron (MLP) and Long Short Term Memory (LSTM) are used in the training phase. Because, CNN is capable of extracting the image features and to learn the internal representation of an image. MLP remembers the pattern in the sequential data and is used to extract the text features from the given answers, questions and categories with respect to an image. Finally, LSTM handles long term dependency for extended period of time. Following this, two encoders are used to generate latent encoding from the features of both image as well as text. Later, both the generated latent encoding is concatenated by passing it to the weighted MLP which generates the corresponding latent representation. This concatenated latent representation acts as a backbone that contains the significant information for question generation. The final model is generated by passing this concatenated latent representation to the LSTM and it generates the question as a sequence of words based on the previous words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>The hardware and software required for the implementation of VQA and VQG models includes, (i). Intel i5 processor with NVIDIA GeForce Ti 4800, 4.3GHZ clock speed, 16GB RAM, Graphical Processing Unit and 2TB disk space, (ii). Linux -Ubuntu 20.04 operating system, Python 3.7 package with required libraries like tensorflow, torch, sklearn, nltk, pickle, pandas, etc.</p><p>The three VQA models and one VQG model are created and validated using MEDVQA-GI 2023 dataset. The VQA models are initially is trained for 20 epochs, and finally for an additional 20 epochs starting from the checkpoint with the lowest validation loss with an learning rate of 5×10-5. Due to limitations of the computational resources available unable to fine tune the model using self-critical sequence training. Then the performance of VQA models are analyzed for each question and it is given in Table <ref type="table" coords="6,111.60,74.15,5.58,10.04">4</ref> and<ref type="table" coords="6,138.92,74.15,4.27,10.04">5</ref>. In Table <ref type="table" coords="6,189.99,74.15,4.38,10.04">4</ref>, it has been inferred that the overall accuracy is 0.471. In addition to this, the classification type QA-pairs attained an highest accuracy of 0. 956. The highest, lowest and overall accuracy for each category is given in Table <ref type="table" coords="6,269.26,101.87,5.58,10.04">5</ref> for better understanding. From Table <ref type="table" coords="6,143.67,635.99,4.13,10.04">4</ref>, it has been inferred that, the VisualBERT and ViT maintains the reasonable accuracy for all types of QA-pairs and hence the accuracy ranges from 40% to 60%. But SegFormer and ViT (for classification Type QA-pairs) are question specific and hence its attains the highest accuracy of 95% as well as the lowest accuracy of 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This research experimented the category based approach to solve VQA and VQG tasks of ImageCLEF MEDVQA-GI 2023. For this task, three VQA models such as ViT, SegFormer, VisualBERT and CMVQG are developed and validated. From the results of the proposed models, it has been inferred that SegFormer and ViT are more problem specific and hence the overall performance is 52.2% and 53.0% respectively which will be improved by choosing the appropriate QA-pairs categories with respect to the medical image. On the other hand, VisualBERT are task generic so it performs reasonably better for mostly all VQA datasets and it ranges from 48.1% to 62.4%. In the future work, the performance can be improved by creating medical related transformer based models. The overall performance can be improved by concentrating on the abnormality type questions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,108.07,298.16,415.39,10.04;7,108.07,310.76,415.98,10.04;7,108.07,323.72,76.01,10.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,108.07,310.76,367.40,10.04">Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,496.75,310.76,27.30,10.04">CLEF</title>
		<imprint>
			<date type="published" when="2018-09">2018, September</date>
		</imprint>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="7,108.07,336.32,415.30,10.04;7,108.07,348.95,415.53,10.04;7,108.07,361.55,136.97,10.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,135.76,348.95,178.34,10.04">Overview of ImageCLEFcoral 2019 task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,332.86,348.95,134.20,10.04;7,108.07,361.55,132.27,10.04">CEUR Workshop Proceedings</title>
		<imprint>
			<biblScope unit="volume">2380</biblScope>
			<date type="published" when="2019-07">2019, July</date>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="7,108.07,374.15,415.26,10.04;7,108.07,386.75,415.78,10.04;7,108.07,399.35,415.05,10.04;7,108.07,412.33,204.39,10.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,108.07,386.75,415.78,10.04;7,108.07,399.35,84.32,10.04">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.18,399.35,310.94,10.04;7,108.07,412.33,94.23,10.04">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2021-09">2021. September 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,424.93,415.26,10.04;7,108.07,437.53,415.40,10.04;7,108.07,450.13,415.05,10.04;7,108.07,462.76,204.39,10.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,108.07,437.53,415.40,10.04;7,108.07,450.13,84.32,10.04">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.18,450.13,310.94,10.04;7,108.07,462.76,94.23,10.04">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2021-09">2021. September 2021</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,475.74,415.10,9.07;7,108.07,487.26,415.24,9.07;7,108.07,498.78,415.05,9.07;7,108.07,510.30,415.46,9.07;7,108.07,521.84,415.33,9.07;7,108.07,533.36,415.23,9.07;7,108.07,544.52,415.60,9.07;7,108.07,556.04,415.20,9.07;7,108.07,567.58,415.41,9.07;7,108.07,579.10,415.10,9.07;7,108.07,590.62,86.36,9.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,386.37,544.52,108.10,9.07;7,108.07,556.04,415.20,9.07;7,108.07,567.58,104.68,9.07">Multimedia Retrieval in Medical, SocialMedia and Recommender Systems Applications Experimental IR Meets Multilinguality</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Druagulinescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Wai</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Griffin</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meliha</forename><surname>Yetisgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Ruckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brungel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Papachrysos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Schöler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra-Georgiana</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmedkhan</forename><surname>Radzhabov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioan</forename><surname>Coman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Manguinhas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,222.62,567.58,300.86,9.07;7,108.07,579.10,174.63,9.07">Multimodality, and Interaction, Proceedings of the 14th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,311.95,579.10,205.76,9.07">Springer Lecture Notes in Computer Science LNCS}</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Overview of ImageCLEF</note>
</biblStruct>

<biblStruct coords="7,108.07,603.22,415.38,9.07;7,108.07,614.63,415.39,9.17;7,108.07,626.29,415.32,9.07;7,108.07,637.81,111.57,9.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,166.06,614.63,357.40,9.17;7,108.07,626.29,86.46,9.07">Overview of ImageCLEFmedical 2023 -Medical Visual Question Answering for Gastrointestinal Tract</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Storås</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,202.32,626.29,240.55,9.07">CLEF2023 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">September 18-21, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,650.03,415.41,10.04;7,108.07,662.63,413.78,10.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,295.07,650.03,228.41,10.04;7,108.07,662.63,158.38,10.04">A comprehensive interpretation for medical VQA: Datasets, techniques, and challenges</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Noor Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,274.13,662.63,169.16,10.04">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct coords="7,108.07,675.23,415.29,10.04;7,108.07,687.85,407.30,10.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,440.43,675.23,82.93,10.04;7,108.07,687.85,49.40,10.04">A survey on vision transformer</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,164.98,687.85,280.31,10.04">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.07,700.45,414.62,10.04;7,108.07,713.05,415.43,10.04;7,108.07,725.65,225.38,10.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,470.99,700.45,51.69,10.04;7,108.07,713.05,319.86,10.04">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,436.22,713.05,87.28,10.04;7,108.07,725.65,139.96,10.04">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,111.27,738.27,411.98,10.04;7,109.51,751.23,348.97,10.04" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,413.73,738.27,109.52,10.04;7,109.51,751.23,191.93,10.04">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
