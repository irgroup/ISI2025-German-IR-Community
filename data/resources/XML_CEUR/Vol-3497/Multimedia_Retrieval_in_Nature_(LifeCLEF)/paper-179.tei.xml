<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,352.48,15.42;1,89.29,106.66,95.46,15.43">Entropy-guided Open-set Fine-grained Fungi Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,48.85,11.96"><forename type="first">Huan</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.30,134.97,48.67,11.96"><forename type="first">Han</forename><surname>Jiang</surname></persName>
							<email>hjiang12@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.13,134.97,50.25,11.96"><forename type="first">Wang</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.54,134.97,58.65,11.96"><forename type="first">Meng</forename><surname>Meng</surname></persName>
							<email>meng18@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.71,134.97,74.73,11.96"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tzzhang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Deep Space Exploration Lab</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,352.48,15.42;1,89.29,106.66,95.46,15.43">Entropy-guided Open-set Fine-grained Fungi Recognition</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7E8659258556EF28493983932C13B48D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fungi Recognition</term>
					<term>Open-set Recognition</term>
					<term>Fine-grained Recognition</term>
					<term>Entropy-guided Method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>FungiCLEF 2023 competition aims at automatically recognizing different species of fungi, which is a challenging open-set, fine-grained task. In this paper, we propose an entropy-guided method for open-set fine-grained fungi recognition. To deal with small inter-class variations in fine-grained recognition, we adopt the MetaFormer as our baseline, which can use meta-information to help differentiate similarlylooking species. Besides, we utilize a Seesaw loss to balance the training process between head classes and tail classes. To handle with the challenge of open-set recognition, we propose an entropy-guided method to discriminate unknown classes, since the entropy can measure the uncertainty of class predictions. By combining these techniques, our method achieves superior F1 Scores, specifically 58.95% on the public test set and 58.36% on the private test set, which ranks 1st place in FungiCLEF 2023 competition. Our code will be available at https://github.com/RenHuan1999/FungiCLEF2023-UstcAIGroup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>FungiCLEF 2023 is a competition held jointly as part of the LifeCLEF 2023 <ref type="bibr" coords="1,418.36,414.45,12.78,10.91" target="#b0">[1]</ref> lab 1 of the CLEF 2023 conference, and of the FGVC10 workshop 2 organized in conjunction with CVPR 2023 conference. The competition aims to automatically identify and classify fungi species, which helps raise awareness about fungi biodiversity and assists ecologists in species identification in the wild <ref type="bibr" coords="1,127.39,468.65,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,141.16,468.65,7.49,10.91" target="#b2">3]</ref>. The training data for FungiCLEF 2023 comes mainly from the Danish Fungi 2020 dataset <ref type="bibr" coords="1,124.33,482.20,12.99,10.91" target="#b3">[4]</ref> which includes rich metadata such as habit, substrate, time etc. The competition also releases a validation set containing some "Unknown Classes" which are never seen during training. In practice, fungi recognition needs to capture fine-grained visual differences within subordinate categories and identify both the known classes and unknown species. Thus, it is considered as a fine-grained, open-set recognition task.</p><p>Fine-grained recognition, as opposed to generic object recognition, is a challenging task due to small inter-class variations and large intra-class variations <ref type="bibr" coords="2,366.77,86.97,11.54,10.91" target="#b4">[5]</ref>. To handle small inter-class variations, existing methods can be roughly grouped into two categories, including part-based methods <ref type="bibr" coords="2,131.94,114.06,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,147.15,114.06,9.03,10.91" target="#b6">7]</ref> and attention-based methods <ref type="bibr" coords="2,297.41,114.06,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,312.62,114.06,7.65,10.91" target="#b8">9]</ref>, which make the network focus on the most class-discriminative regions. However, these methods ignore additional meta-information that is beneficial for recognition. For example, habitat and substrate are essential sources of information that help differentiate similarly-looking species. Thus, to utilize meta-information effectively for accurate fine-grained recognition, we take MetaFormer <ref type="bibr" coords="2,398.39,168.26,17.81,10.91" target="#b9">[10]</ref> as a strong baseline, which is a hybrid network with convolutions to downsample the image and Transformers to fuse visual and meta information. Besides, by analyzing the dataset, we observe a long-tailed category distribution problem. Thus, we adopt a Seesaw loss <ref type="bibr" coords="2,353.43,208.91,17.76,10.91" target="#b10">[11]</ref> to balance the training process between head classes and tail classes.</p><p>In open-set recognition, the training phase typically involves a set of known classes, and the classifier learns to distinguish and classify instances belonging to these classes <ref type="bibr" coords="2,441.36,249.56,16.26,10.91" target="#b11">[12]</ref>. However, during the testing phase, the classifier encounters instances that may belong to either the known classes or unknown classes. The classifier needs to make predictions for the known classes while having the ability to identify instances from unknown classes and assign them a separate "unknown" label. To achieve open-set recognition, existing methods focus on developing openset classifiers <ref type="bibr" coords="2,152.10,317.30,16.55,10.91" target="#b12">[13,</ref><ref type="bibr" coords="2,171.56,317.30,12.42,10.91" target="#b13">14]</ref>, but ignore the design of metrics for identifying known and unknown classes. The entropy <ref type="bibr" coords="2,188.60,330.85,18.07,10.91" target="#b14">[15]</ref> can measure the uncertainty of class predictions for the current input sample by a classification model. Based on this uncertainty, unknown classes can be effectively identified since the uncertainty is high for unknown classes. Therefore, we propose an entropy-guided unknown identifier for accurate open-set recognition.</p><p>The major contributions of this work can be summarized as follows: <ref type="bibr" coords="2,393.99,385.05,11.34,10.91" target="#b0">(1)</ref> We utilize MetaFormer as a powerful baseline for fine-grained recognition and employ the Seasaw loss to address the issue of long-tail distribution. <ref type="bibr" coords="2,229.33,412.15,11.81,10.91" target="#b1">(2)</ref> We propose a novel entropy-guided open-set recognition method, which is highly effective in addressing open-set recognition challenges. (3) Extensive experimental results demonstrate that the proposed method performs favorably against stateof-the-art methods, and our method ranks 1st place in FungiCLEF 2023 competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained Vision Classification</head><p>Vision-driven method. Current fine-grained vision classification methods can be categorized into two directions: part-based methods <ref type="bibr" coords="2,274.18,545.94,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,288.53,545.94,12.59,10.91" target="#b15">16,</ref><ref type="bibr" coords="2,303.99,545.94,14.11,10.91" target="#b16">17]</ref> and attention-based methods <ref type="bibr" coords="2,455.87,545.94,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,470.22,545.94,7.65,10.91" target="#b8">9]</ref>. Partbased methods try to learn discriminative parts to distinguish different species. Zhang et al. <ref type="bibr" coords="2,106.46,573.04,18.06,10.91" target="#b15">[16]</ref> propose two deformable part descriptors to pool representations across pose and viewpoint, in turn facilitating fine-grained recognition and attribute prediction. Ge et al. <ref type="bibr" coords="2,491.76,586.59,11.57,10.91" target="#b5">[6]</ref>, part models are built in a weakly supervised manner to retrieve information hidden in the object proposals. Another line of work attempts to introduce attention mechanisms to boost image recognition accuracy. RA-CNN <ref type="bibr" coords="2,227.60,627.23,12.70,10.91" target="#b7">[8]</ref> propose a recurrent attention convolutional neural network, which recursively learns crucial region attention and region-based feature representation at multiple scales. MA-CNN <ref type="bibr" coords="2,206.71,654.33,12.91,10.91" target="#b8">[9]</ref> jointly learns part proposals and the feature representations on each part with a multi-attention convolutional neural network. TransFG <ref type="bibr" coords="2,425.37,667.88,18.07,10.91" target="#b17">[18]</ref> integrates all raw attention weights of the Transformer into an attention map to select discriminative image patches. However, these methods ignore some meta-information, making it difficult to recognize vision-similarly species.</p><p>Multi-modality-driven method. Besides visual information, some works take advantage of additional meta-information (e.g., spatio-temporal prior, attribute, and text description). Tang et al. <ref type="bibr" coords="3,115.53,169.92,18.07,10.91" target="#b18">[19]</ref> first introduce additional information and naturally incorporate these features to make final predictions. Dynamic MLP <ref type="bibr" coords="3,262.58,183.47,18.00,10.91" target="#b19">[20]</ref> is an efficient structure that exploit the additional information as adaptive perceptron weights to interact with vision features. MetaFormer <ref type="bibr" coords="3,488.04,197.02,17.94,10.91" target="#b9">[10]</ref> designs a hybrid structure backbone including convolution and Transformer to fuse vision and meta information to establish joint representations. We choose MetaFormer as our baseline in this competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Open-set Recognition</head><p>The objective of open-set recognition is to classify the known classes during the training process and to identify the unknown classes which are absent in the training set while keep the ability for known classification during the test process. Scheirer et al. <ref type="bibr" coords="3,372.56,314.49,18.00,10.91" target="#b20">[21]</ref> first present the open-set classification problem and design open set classifiers under the one-vs-set setting to balance the unknown classes. Jain et al. <ref type="bibr" coords="3,214.08,341.59,17.84,10.91" target="#b21">[22]</ref> calibrate SVM decision scores to posterior probabilities under the multi-class classifier setting. Another pipeline <ref type="bibr" coords="3,314.52,355.14,16.45,10.91" target="#b22">[23,</ref><ref type="bibr" coords="3,333.71,355.14,14.04,10.91" target="#b23">24]</ref> utilizes uncertainty to measure the feature difference between unknown and known objects. Denouden et al. <ref type="bibr" coords="3,417.11,368.69,17.88,10.91" target="#b22">[23]</ref> incorporate the Mahalanobis distance in latent space to better capture these out-of-distribution samples. Liu et al. <ref type="bibr" coords="3,102.70,395.79,17.88,10.91" target="#b23">[24]</ref> propose an energy-bounded learning objective to fine-tune the network and design an energy score to better distinguish unknown classes. In this work, we present to use entropy, a simple but effective way to measure uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fungi Species Classification</head><p>The 1st team's paper <ref type="bibr" coords="3,186.28,472.61,18.07,10.91" target="#b24">[25]</ref> in FungiCLEF2022 <ref type="bibr" coords="3,293.05,472.61,18.07,10.91" target="#b25">[26]</ref> written by Xiong et al. use an ensemble of MetaFormer <ref type="bibr" coords="3,146.92,486.16,17.92,10.91" target="#b9">[10]</ref> and ConvNext <ref type="bibr" coords="3,234.55,486.16,17.92,10.91" target="#b26">[27]</ref> networks. USTC-IAT-United <ref type="bibr" coords="3,384.68,486.16,17.92,10.91" target="#b27">[28]</ref> ensemble several CNN and Transformer architectures, and explore the impact of data augmentation techniques and loss functions. Shen et al. <ref type="bibr" coords="3,185.42,513.26,17.87,10.91" target="#b28">[29]</ref> design a novel architecture combined with large kernel convolution and vision Transformer. However, these methods ignore the metrics for identifying known and unknown classes, and we design an entropy-guided unknown identifier to recognize unknown classes more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Design</head><p>Deep learning model. Figure <ref type="figure" coords="3,242.46,633.50,5.17,10.91" target="#fig_0">1</ref> shows the overall framework of our model. We employ MetaFormer <ref type="bibr" coords="3,148.40,647.05,18.07,10.91" target="#b9">[10]</ref> as a strong baseline for fine-grained visual classification. Metaformer is a hybrid framework where the first three stages are convolution blocks and the last two stages </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Detail settings of MetaFormer-0 and MetaFormer-2. L represents the number of blocks, and C represents the channels of hidden features.</p><formula xml:id="formula_0" coords="4,125.15,346.41,342.48,33.57">Stages S0 S1 S2 S3 S4 MetaFormer-0 L=3 C=64 L=2 C=96 L=3 C=192 L=5 C=384 L=2 C=768 MetaFormer-2 L=3 C=128 L=2 C=128 L=3 C=256 L=5 C=512 L=2 C=1024</formula><p>are Transformer blocks. We combine two series of MetaFormer, namely MetaFormer-0 and MetaFormer-2. The detailed settings are shown in Table <ref type="table" coords="4,338.28,417.70,3.69,10.91">1</ref>. In addition, we also try some vision classification models, like InternImage-L <ref type="bibr" coords="4,273.67,431.25,16.31,10.91" target="#b29">[30]</ref>. However, due to the memory limits, we do not choose it as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta information.</head><p>In order to classify the similarly-looking species, we make full use of extra meta-information to assist final decision. The meta-information and appearance information are simply fused by relative attention <ref type="bibr" coords="4,266.89,500.62,18.07,10.91" target="#b30">[31]</ref> in Transformer layers. Intuitively, we consider geographical, living habit and temporal information including Observe date (month and day), countryCode, Substrate and Habitat. For temporal information, we map the Observe date</p><formula xml:id="formula_1" coords="4,89.29,539.47,325.87,17.85">[𝑚𝑜𝑛𝑡ℎ, 𝑑𝑎𝑦] into [sin (︀ 2𝜋𝑚𝑜𝑛𝑡ℎ 12 )︀ , cos (︀ 2𝜋𝑚𝑜𝑛𝑡ℎ 12 )︀ , sin (︁ 2𝜋𝑑𝑎𝑦 31 )︁ , cos (︁<label>2𝜋𝑑𝑎𝑦 31</label></formula><p>)︁ ]. For geographical and living habit information, there are 34, 32 and 31 categories for countryCode, Substrate and Habitat, respectively. We use one-hot encoding to encode meta-information according to their meta category. Additionally, we employ a non-linear embedding technique to map the different meta-embeddings into a C-dimensional space, which is consistent with the dimension of the image features.</p><p>Long-tailed Distribution. As shown in Figure <ref type="figure" coords="4,319.13,642.48,3.81,10.91" target="#fig_1">2</ref>, the training set provided by the Fungi-CLEF2023 challenge has a long-tailed class distribution. Among them, the most frequent species has 1913 images while the least frequent species has only 31 images. The imbalanced class distribution will make the training process dominated by head categories, and further lead to misclassification for tail categories. To cope with this challenge, we adopt Seesaw loss <ref type="bibr" coords="5,487.91,319.23,18.07,10.91" target="#b10">[11]</ref> instead of cross entropy loss. Seesaw Loss can dynamically decrease the gradients of negative samples for each category by modifying the cross entropy loss as:</p><formula xml:id="formula_2" coords="5,233.18,370.13,273.46,68.16">ℒ 𝑆𝑒𝑒𝑠𝑎𝑤 (𝑙) = - 𝐶 ∑︁ 𝑖=1 𝑦 𝑖 log 𝑝 𝑖 , 𝑝 𝑖 = 𝑒 𝑙 𝑖 ∑︀ 𝐶 𝑗̸ =𝑖 𝑆 𝑖𝑗 𝑒 𝑙 𝑗 + 𝑒 𝑙 𝑖 ,<label>(1)</label></formula><p>where 𝑙 represents the output logits of the model, 𝑦 is the one-hot class label and 𝑆 is a tunable balancing factor across classes. For more details, please refer to <ref type="bibr" coords="5,373.18,463.27,16.25,10.91" target="#b10">[11]</ref>.</p><p>Data augmentation. We use a composed sequence of some common augmentation techniques to improve the results. In the training process, a random cropping is first performed on the image, where the size of the cropped region is randomly chosen between 8% and 100% of the original image size. Afterward, the slice is resized by the bicubic interpolation method and flipped horizontally with a probability of 50 %. Additionally, we adopt RandAugment <ref type="bibr" coords="5,488.13,546.23,17.86,10.91" target="#b31">[32]</ref> following the setting of Swin Transformer <ref type="bibr" coords="5,279.18,559.77,16.22,10.91" target="#b30">[31]</ref>. This includes both photometric augmentation (Brightness, Contrast, Color, Sharpness, etc.) and geometric augmentation (Shear, Rotate, Translate, etc.). Finally, a random erasing <ref type="bibr" coords="5,234.86,586.87,18.06,10.91" target="#b32">[33]</ref> is used to mask out a random region of images with a probability of 25%. It is worth noting that Mixup <ref type="bibr" coords="5,319.59,600.42,18.07,10.91" target="#b33">[34]</ref> and Cutmix <ref type="bibr" coords="5,397.99,600.42,18.07,10.91" target="#b34">[35]</ref> are not used in the pre-processing pipeline. During the inference process, we use multi scale &amp; ten crop to perform test time augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Entropy-guided Unknown Identifier</head><p>In the open-set scenario, the model needs not only to correctly classify between training known categories, but also to identify whether an image is from a previously unseen class, considered an "unknown" category. While existing literature proposes several classical approaches to identify the unknown, such as MSP <ref type="bibr" coords="6,208.55,148.18,21.32,10.91" target="#b12">[13,</ref><ref type="bibr" coords="6,232.95,148.18,12.42,10.91" target="#b35">36]</ref>, MLS <ref type="bibr" coords="6,268.08,148.18,20.56,10.91" target="#b36">[37]</ref>, we introduce a more effective entropy-guided unknown identifier. We next briefly review the previous approaches and then present our method.</p><p>Maximum Softmax Probability (MSP). Given a network consisting of a backbone 𝑔 and a classifier ℎ, the output logits of an image 𝑥 is denoted as 𝑙(𝑥) = ℎ(𝑔(𝑥)). The predicted probabilities 𝑝(𝑥) that sum to one can then be derived by applying the softmax to logits. The model is trained for closed-set classification, specifically, using the cross-entropy loss between the true class labels 𝑦 and the predicted probabilities 𝑝(𝑥). During testing, the maximum softmax probability 𝑝 𝑚𝑎𝑥 (𝑥) = max(𝑝(𝑥)) is taken for open-set recognition <ref type="bibr" coords="6,382.73,271.79,17.74,10.91" target="#b12">[13,</ref><ref type="bibr" coords="6,402.85,271.79,12.23,10.91" target="#b35">36]</ref>. If 𝑝 𝑚𝑎𝑥 (𝑥) is above the threshold 𝜏 , the image is identified as the corresponding known category, otherwise it is considered to be the unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Logit Score (MLS).</head><p>Although most of the literature adopts MSP as the baseline, Vaze et al. <ref type="bibr" coords="6,138.57,341.19,16.31,10.91" target="#b36">[37]</ref> suggest using the maximum logit score 𝑙 𝑚𝑎𝑥 (𝑥) = max(𝑙(𝑥)) instead, and similarly, using threshold 𝜏 for 𝑙 𝑚𝑎𝑥 (𝑥) to determine known/unknown categories. It has been observed that open-set samples tend to have lower feature magnitudes than closedset ones <ref type="bibr" coords="6,123.88,381.84,18.76,10.91" target="#b37">[38,</ref><ref type="bibr" coords="6,145.65,381.84,12.42,10.91" target="#b38">39]</ref>. Since the softmax operation normalizes out most of the feature magnitude information in the logits, they indicate that using the raw logits would lead to better open set detection results.</p><p>Entropy. We first try to adopt the MLS as our baseline. To determine the optimal threshold 𝜏 , we plot the distribution of the maximum logit values, as shown in Figure <ref type="figure" coords="6,438.14,451.25,3.84,10.91" target="#fig_2">3</ref>(a). As can be seen in the third column, there is no clear boundary for the distribution on the test set, so the threshold 𝜏 can only be set based on the prior information about the number of the unknown samples <ref type="bibr" coords="6,121.79,491.90,18.57,10.91" target="#b39">[40]</ref>. Even with additional training on the validation set, the distribution boundary on the test set is still not obvious, as shown in the fourth column. We therefore conclude that the open-set detection performance is very sensitive to the choice of hyperparameter 𝜏 , which is not robust enough. To solve this problem, we propose an entropy-guided unknown identifier. Specifically, we first calculate the entropy of the predicted probabilities, defined as:</p><formula xml:id="formula_3" coords="6,231.77,569.90,274.87,33.58">𝑒(𝑥) = - 𝐶 ∑︁ 𝑐=1 𝑝 𝑐 (𝑥) log 𝑝 𝑐 (𝑥),<label>(2)</label></formula><p>which is then used for open-set recognition with threshold 𝜏 . The entropy measures the uncertainty of the predictions of the classifier on the input samples. In general, the model is more confident for the known categories, corresponding to a lower entropy. Whereas for the unknown categories the uncertainty is higher and hence the entropy will be higher. Thus, we can effectively distinguish between known/unknown categories based on entropy. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Important Strategies</head><p>Training with the validation set additionally. Since the final evaluation is on the test set, it is a common trick to additionally use the data from the validation set for training. For this competition this trick is helpful because the validation set also contains some samples from the unknown category. With these unknown samples, we can estimate the data distribution of unknown categories and achieve better known/unknown separation. Specifically, for the unknown samples in the validation set, we constrain the known classifier to yield a uniform distribution for them, with the target in the corresponding cross-entropy loss formulated as 𝑦 ˆ= 1 𝐶 1 𝐶 , where 1 𝐶 denote vectors of ones in dimension of 𝐶 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced identification of poisonous species.</head><p>There are some poisonous species of fungi that exist in nature and are dangerous if eaten by mistake. For these poisonous categories, the model should try to identify them, so there should also be an edible/poisonous identification. If a poisonous observation is misclassified as edible, the corresponding cost should increase.</p><p>To enhance the identification of poisonous species, we design a poisonous classification loss. Specifically, the set of poisonous categories is denoted by 𝒞 𝑝𝑜𝑖 , and the set of remaining edible species is denoted by 𝒞 𝑒𝑑𝑖 . For those poisonous samples, we index out and then average the top-𝑘 of the classification logits corresponding to 𝒞 𝑝𝑜𝑖 , as well as the top-𝑘 corresponding to 𝒞 𝑒𝑑𝑖 . A poisonous/edible classification loss is then used as an additional supervision. The whole process is formulated as:</p><formula xml:id="formula_4" coords="7,162.93,651.83,343.71,15.73">𝑙 𝑠𝑒𝑡 (𝑥) = mean (︁ top-k (︀ {𝑙 𝑐 (𝑥)|𝑐 ∈ 𝒞 𝑠𝑒𝑡 } )︀ )︁ , 𝑠𝑒𝑡 ∈ {𝑝𝑜𝑖, 𝑒𝑑𝑖},<label>(3)</label></formula><p>ℒ 𝑝𝑜𝑖 (𝑥) = -log 𝑒 𝑙 𝑝𝑜𝑖 (𝑥) 𝑒 𝑙 𝑝𝑜𝑖 (𝑥) + 𝑒 𝑙 𝑒𝑑𝑖 (𝑥) , 𝑦(𝑥) ∈ 𝒞 𝑝𝑜𝑖 .</p><p>(4)</p><p>Model ensemble. Model ensemble is a well proven trick in competitions. However, the organizers of this competition claim that large models are less practical and therefore limit the maximum model size to 1 GB. Within the limitation of the model size, we experiment with ensemble of the MetaFormer-0 (150 MB) and MetaFormer-2 (393 MB) models. The ensemble model (543 MB) can further improve performance while meeting the model size limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset. The training data for the FungiCLEF2023 challenge is primarily derived from the Danish Fungi 2020 dataset <ref type="bibr" coords="8,210.57,273.76,12.98,10.91" target="#b3">[4]</ref>, which contains 295,938 training images belonging to 1,604 categories. Besides the observed images, a wealth of metadata is also provided, such as habitat, substrate, time, location, country, etc. The validation set contains 60,832 images from 30,131 observations belonging to 2,713 categories, where multiple images may be taken for observations of the same object. The public test set contains 60,225 images from 30,130 observations and has a similar data distribution to the validation set, with an unknown number of categories.</p><p>To ensure the fairness of the competition and prevent participants from overfitting to the leaderboard, there is another private test set that has not been released by the organizers. The final performance evaluation is conducted on the test set.</p><p>Implementation Details. We employ the pre-trained MetaFormer <ref type="bibr" coords="8,405.27,410.91,21.15,10.91" target="#b9">[10]</ref> as our backbone network, fine-tune it and learn a set of meta tokens and a classification head. Our model is trained using the AdamW <ref type="bibr" coords="8,203.01,438.01,24.78,10.91" target="#b40">[41]</ref> optimizer with a cosine decay learning rate scheduler. The learning rate is initialized to 5𝑒 -5 and the weight decay is 0.05. We follow most of the data augmentation and regularization strategies from Swin Transformer <ref type="bibr" coords="8,394.38,465.11,20.05,10.91" target="#b30">[31]</ref> in our training. We conduct all the experiments with NVIDIA RTX 3090 (24G) GPU. The threshold 𝜏 for the entropyguided unknown identifier is set to 4, and the 𝑘 in Equation ( <ref type="formula" coords="8,361.34,492.21,3.86,10.91" target="#formula_4">3</ref>) is 5 in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>The FungiCLEF2023 challenge uses five evaluation metrics to measure performance from different perspectives, including F 1 -Score (higher is better) and four costs (lower is better). Note that during evaluation, all categories not present in the training set are treated as a single "unknown" category, indexed by "-1". Given prediction 𝑞(𝑥 𝑖 ) over observation 𝑥 𝑖 , ground-truth labels 𝑦 𝑖 , and a cost function 𝑊 𝑛 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )), the 𝑛-th cost is calculated by averaging the cost function over all observations:</p><formula xml:id="formula_5" coords="8,227.83,631.04,278.81,33.71">𝐶𝑜𝑠𝑡 𝑛 = 1 𝑁 𝑁 ∑︁ 𝑖=1 𝑊 𝑛 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )).<label>(5)</label></formula><p>𝐹 1 -Score. The 𝐹 1 score for the 𝑐-th category is the harmonic mean of the precision and recall, which is formulated as:</p><formula xml:id="formula_6" coords="9,184.20,116.35,322.44,29.63">𝐹 𝑐 1 = 2 • 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑐 • 𝑟𝑒𝑐𝑎𝑙𝑙 𝑐 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑐 + 𝑟𝑒𝑐𝑎𝑙𝑙 𝑐 = 2𝑇 𝑐 𝑝 2𝑇 𝑐 𝑝 + 𝐹 𝑐 𝑝 + 𝐹 𝑐 𝑛 ,<label>(6)</label></formula><p>where 𝑇 𝑝 , 𝐹 𝑝 , 𝐹 𝑛 represent true-positive, false-positive and false-negative, respectively. The final macro 𝐹 1 -Score is calculated by averaging the 𝐹 1 scores over all categories:</p><formula xml:id="formula_7" coords="9,245.98,189.79,260.66,33.58">macro 𝐹 1 = 1 𝐶 𝐶 ∑︁ 𝑐=1 𝐹 𝑐 1 .<label>(7)</label></formula><p>Track1: Standard Classfication with "unknown" category. The standard classification cost is obtained by averaging the predicted category error over all instances, and the cost function is defined as:</p><formula xml:id="formula_8" coords="9,219.31,275.39,287.33,29.86">𝑊 1 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )) = {︃ 0, 𝑦 𝑖 = 𝑞(𝑥 𝑖 ) 1, 𝑦 𝑖 ̸ = 𝑞(𝑥 𝑖 ) .<label>(8)</label></formula><p>Track2: Cost for confusing edible species for poisonous and vice versa. There is a function 𝑑 that indicates poisonous species as 𝑑(𝑦) = 1 if species 𝑦 is poisonous and 0 otherwise. The cost function for poisonous/edible confusion is formulated as:</p><formula xml:id="formula_9" coords="9,172.06,371.77,334.58,46.09">𝑊 2 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )) = ⎧ ⎪ ⎨ ⎪ ⎩ 0, 𝑑(𝑦 𝑖 ) = 𝑑(𝑞(𝑥 𝑖 )) 𝑐 𝑃 𝑆𝐶 , 𝑑(𝑦 𝑖 ) = 1 and 𝑑(𝑞(𝑥 𝑖 )) = 0 𝑐 𝐸𝑆𝐶 , 𝑑(𝑦 𝑖 ) = 0 and 𝑑(𝑞(𝑥 𝑖 )) = 1 ,<label>(9)</label></formula><p>where 𝑐 𝐸𝑆𝐶 = 1 and 𝑐 𝑃 𝑆𝐶 = 100 because it is more dangerous for a poisonous observation to be misclassified as edible.</p><p>Track3: A user-focused loss composes of both the classification error and the poisonous/edible confusion. The cost function is given as:</p><formula xml:id="formula_10" coords="9,190.91,504.17,315.73,13.13">𝑊 3 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )) = 𝑊 1 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )) + 𝑊 2 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )).<label>(10)</label></formula><p>Track4: Cost for missing "unknown" species is higher; misclassifying for "unknown" is cheaper than confusing species. The cost function is a variant of the standard classification cost, increasing the weight of the cost of missing an unknown category and decreasing the weight of the cost of wrongly detecting an unknown category, which is written as:</p><formula xml:id="formula_11" coords="9,152.14,594.75,354.50,62.92">𝑊 4 (𝑦 𝑖 , 𝑞(𝑥 𝑖 )) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ 0, 𝑦 𝑖 = 𝑞(𝑥 𝑖 ) 𝛼, 𝑦 𝑖 = -1 and 𝑞(𝑥 𝑖 ) ̸ = -1 𝛽, 𝑦 𝑖 ̸ = -1 and 𝑞(𝑥 𝑖 ) = -1 1, 𝑦 𝑖 ̸ = 𝑞(𝑥 𝑖 ) and 𝑦 𝑖 ̸ = -1 and 𝑞(𝑥 𝑖 ) ̸ = -1 ,<label>(11)</label></formula><p>where 𝛼 = 10 and 𝛽 = 0.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison on the Leaderboard</head><p>Table <ref type="table" coords="10,115.64,325.67,5.04,10.91" target="#tab_0">2</ref> and Table <ref type="table" coords="10,169.14,325.67,5.04,10.91" target="#tab_1">3</ref> show the performance of our team compared to other teams on the leaderboard of FungiCLEF 2023 competition. The leaderboards are divided into public and private test sets, in both of which we win 1st place, outperforming the 2nd place by a significant margin. For example, in the public leaderboard in Table <ref type="table" coords="10,297.24,366.32,3.66,10.91" target="#tab_0">2</ref>, our 𝐹 1 -Score exceeds that of the 2nd place by more than 2%, indicating that our method can better solve the open-set fine-grained recognition problem. On both the standard classification cost represented by Track1 and the unknown classification cost represented by Track4, our cost is reduced by nearly half compared to the 2nd place, demonstrating that our method is better at identifying unknown species. Our method also achieves a lower cost on the poisonous classification cost denoted by Track2, indicating that our method discriminates the poisonous/edible category better. Similar performance gains can be observed in the private leaderboard in Table <ref type="table" coords="10,319.90,461.16,3.74,10.91" target="#tab_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To analyze the impact of each design, we conduct a series of ablation studies on the public test set, as detailed below.</p><p>Model architecture. Table <ref type="table" coords="10,226.39,553.19,5.16,10.91" target="#tab_2">4</ref> shows the impact of the model architecture on performance. The comparison between ❶ and ❷ reveals that the accumulated steps in the training process does not have a significant impact on performance. Certain performance gains are achieved by performing model ensemble for ❶ and ❷. InternImage <ref type="bibr" coords="10,346.31,593.84,19.01,10.91" target="#b29">[30]</ref>, a recent model that uses pure images to classify, performs worse compared to MetaFormer, indicating the importance of metainformation for fine-grained classification. By performing model ensemble on MetaFormer and InternImage, performance is higher than ❸, indicating that ensemble is more effective with different model architectures. However, in our final submission, we do not employ InternImage due to the model size limit. Training data. Table <ref type="table" coords="11,196.88,419.91,5.09,10.91" target="#tab_3">5</ref> shows the impact of different training data sources on performance.</p><p>As can be seen from the table, the additional use of the validation set for training can lead to performance gains on the Track1-Track4 metrics. It can also be seen in Figure <ref type="figure" coords="11,431.82,447.01,4.97,10.91" target="#fig_2">3</ref> that adding the validation set for training can improve the discrepancy between known/unknown categories on the validation set and further generalize to the test set for better open-set recognition.</p><p>Unknown identifier. Table <ref type="table" coords="11,227.79,502.87,5.09,10.91" target="#tab_4">6</ref> demonstrates the impact of different unknown identification methods on performance. As can be seen from the table, the proposed entropy-guided unknown identifier can lead to a significant performance improvement, especially for the identification of unknown categories, as indicated by Track1 and Track4. Furthermore, as shown in Figure <ref type="figure" coords="11,499.57,543.52,3.81,10.91" target="#fig_2">3</ref>, entropy is more discriminative for known/unknown categories than MLS, indicating that entropy is more suitable for open-set recognition. However, it is worth noting that when utilizing entropy, Track2 (poisonous identification) exhibits a higher value compared to MLS. This discrepancy arises due to the fact that the "poisonous identification" task only focuses on "known categories", while entropy encourages the model to identify more samples as unknown classes. As a result, the model's performance in the "poisonous identification" task for known categories may be compromised. To address this issue and enhance the identification of poisonous species, we have designed a specific poisonous/edible classification loss, which is elaborated upon in detail in Section 3.3. Poisonous/edible identification. Table <ref type="table" coords="12,283.39,280.17,5.03,10.91" target="#tab_5">7</ref> demonstrates the impact of the poisonous/edible classification loss ℒ 𝑝𝑜𝑖 on performance. As can be seen from the table, Track2 is significantly reduced with the incorporation of explicit poisonous/edible classification supervision, indicating a stronger identification of poisonous categories.</p><p>Submitted models. Table <ref type="table" coords="12,217.10,349.41,4.97,10.91" target="#tab_6">8</ref> shows a comparison among the performance of the three submitted models on the public test set. Within the limitation of the maximum model size of 1GB, we employ the ensemble of MetaFormer-0 and MetaFormer-2, yielding further performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an entropy-guided method for open-set fine-grained fungi classification. The proposed method is based on a modern model MetaFormer, which can utilize meta-information to differentiate similarly-looking species for accurate fine-grained recognition. We also explore the Seesaw loss for long-tail recognition. More importantly, we propose an entropy-guided unknown identifier to discriminate unknown classes with the support of open-set scenarios. Extensive results demonstrate the effectiveness of the proposed method, and our method ranks 1st place in Fungi2023 competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,251.61,416.87,8.93;4,89.29,263.62,416.70,8.87;4,89.29,275.57,100.68,8.87;4,366.19,168.61,90.72,69.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our framework. We use images and meta data as input and train the MetaFormer model with Seesaw loss. During testing, the proposed entropy-guided unknown identifier is employed for open-set recognition.</figDesc><graphic coords="4,366.19,168.61,90.72,69.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,268.07,270.04,8.93;5,89.29,84.19,416.71,171.32"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Class distribution of the training set of FungiCLEF2023.</figDesc><graphic coords="5,89.29,84.19,416.71,171.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,257.19,416.69,8.93;7,89.29,269.20,237.30,8.87;7,108.74,177.01,94.23,72.23"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of the maximum logit values and the entropies on the validation and test set. The sources of training data are noted in parentheses.</figDesc><graphic coords="7,108.74,177.01,94.23,72.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,90.49,377.75,85.18"><head>Table 2</head><label>2</label><figDesc>Public leaderboard of FungiCLEF2023 competition. We rank 1st place.</figDesc><table coords="10,128.53,118.31,338.21,57.35"><row><cell>Rank</cell><cell>Team Name</cell><cell cols="5">𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell>1</cell><cell>meng18</cell><cell>58.95</cell><cell>0.2072</cell><cell>0.1742</cell><cell>0.3814</cell><cell>1.4762</cell></row><row><cell>2</cell><cell>stefanwolf</cell><cell>56.27</cell><cell>0.3528</cell><cell>0.2133</cell><cell>0.5662</cell><cell>2.9296</cell></row><row><cell>3</cell><cell>word2vector</cell><cell>55.46</cell><cell>0.3519</cell><cell>0.2561</cell><cell>0.6080</cell><cell>2.8167</cell></row><row><cell>4</cell><cell cols="2">SSSAMMMM 52.76</cell><cell>0.4124</cell><cell>0.3270</cell><cell>0.7395</cell><cell>3.3302</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,195.57,377.75,85.18"><head>Table 3</head><label>3</label><figDesc>Private leaderboard of FungiCLEF2023 competition. We rank 1st place.</figDesc><table coords="10,128.53,223.39,338.21,57.35"><row><cell>Rank</cell><cell>Team Name</cell><cell cols="5">𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell>1</cell><cell>meng18</cell><cell>58.36</cell><cell>0.2409</cell><cell>0.1269</cell><cell>0.3702</cell><cell>1.771</cell></row><row><cell>2</cell><cell>stefanwolf</cell><cell>55.31</cell><cell>0.3473</cell><cell>0.1904</cell><cell>0.556</cell><cell>1.9045</cell></row><row><cell>3</cell><cell>word2vector</cell><cell>54.34</cell><cell>0.3601</cell><cell>0.2324</cell><cell>0.6034</cell><cell>2.9269</cell></row><row><cell>4</cell><cell cols="2">SSSAMMMM 51.67</cell><cell>0.4408</cell><cell>0.3264</cell><cell>0.7673</cell><cell>3.6493</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.98,90.49,417.00,121.20"><head>Table 4</head><label>4</label><figDesc>Ablation studies about different model architectures. Accu4 means that the accumulated steps is set to 4 during training. We use the maximum logit scores to identify the unknown and do not use additional validation set for training.</figDesc><table coords="11,123.08,142.22,349.12,69.47"><row><cell>Model Architecture</cell><cell cols="5">𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell cols="2">❶ MetaFormer-2 (accu4) 55.94</cell><cell>0.3530</cell><cell>0.2160</cell><cell>0.5690</cell><cell>2.9591</cell></row><row><cell cols="2">❷ MetaFormer-2 (accu8) 55.97</cell><cell>0.3550</cell><cell>0.2398</cell><cell>0.5948</cell><cell>2.9412</cell></row><row><cell>❸ Ensemble (❶ + ❷)</cell><cell>56.68</cell><cell>0.3499</cell><cell>0.2263</cell><cell>0.5762</cell><cell>2.9283</cell></row><row><cell>❹ InternImage</cell><cell>54.32</cell><cell>0.3598</cell><cell>0.2265</cell><cell>0.5863</cell><cell>2.9543</cell></row><row><cell>❺ Ensemble (❶ + ❹)</cell><cell>56.50</cell><cell>0.3486</cell><cell>0.1962</cell><cell>0.5448</cell><cell>2.9159</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,88.98,231.43,417.25,71.01"><head>Table 5</head><label>5</label><figDesc>Ablation studies about different sources of training data. We use the maximum logit scores to identify the unknown with MetaFormer-2.</figDesc><table coords="11,145.17,268.99,304.94,33.45"><row><cell cols="6">Training Data 𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell>train</cell><cell>55.94</cell><cell>0.3530</cell><cell>0.2160</cell><cell>0.5690</cell><cell>2.9591</cell></row><row><cell>train+val</cell><cell>54.98</cell><cell>0.3381</cell><cell>0.1896</cell><cell>0.5277</cell><cell>2.8853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.98,322.34,417.00,73.23"><head>Table 6</head><label>6</label><figDesc>Ablation studies about different unknown identifier. MLS indicates the Maximum Logit Score. We use additional validation set for training MetaFormer-2.</figDesc><table coords="11,133.41,362.11,328.45,33.45"><row><cell cols="6">Unknown Identifier 𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell>MLS</cell><cell>54.98</cell><cell>0.3381</cell><cell>0.1896</cell><cell>0.5277</cell><cell>2.8853</cell></row><row><cell>entropy</cell><cell>57.84</cell><cell>0.2101</cell><cell>0.2544</cell><cell>0.4645</cell><cell>1.4815</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.98,90.49,417.00,73.38"><head>Table 7</head><label>7</label><figDesc>Ablation studies about the enhanced identification of poisonous species. We employ the entropy-guided unknown identifier and use additional validation set for training MetaFormer-0.</figDesc><table coords="12,165.13,130.26,265.01,33.60"><row><cell cols="6">ℒ 𝑝𝑜𝑖 F1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell>✗</cell><cell>57.80</cell><cell>0.2088</cell><cell>0.2865</cell><cell>0.4953</cell><cell>1.4256</cell></row><row><cell>✓</cell><cell>58.11</cell><cell>0.2069</cell><cell>0.2067</cell><cell>0.4136</cell><cell>1.3936</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,88.99,183.27,377.79,73.23"><head>Table 8</head><label>8</label><figDesc>Performance comparison of the final submitted models on the public test set.</figDesc><table coords="12,128.49,211.09,338.30,45.41"><row><cell>Submitted Model</cell><cell cols="5">𝐹 1 (↑) Track1 (↓) Track2 (↓) Track3 (↓) Track4 (↓)</cell></row><row><cell cols="2">MetaFormer-0 (150M) 58.11</cell><cell>0.2069</cell><cell>0.2067</cell><cell>0.4136</cell><cell>1.3936</cell></row><row><cell cols="2">MetaFormer-2 (393M) 57.63</cell><cell>0.2123</cell><cell>0.1943</cell><cell>0.4066</cell><cell>1.4984</cell></row><row><cell>Ensemble (543M)</cell><cell>58.95</cell><cell>0.2072</cell><cell></cell><cell>0.3814</cell><cell>1.4762</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,574.74,395.01,10.91;12,112.66,588.29,394.62,10.91;12,112.66,601.84,337.99,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,179.38,588.29,307.15,10.91">Lifeclef 2023 teaser: Species identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Moussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,601.84,207.71,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,615.39,393.33,10.91;12,112.66,628.93,393.53,10.91;12,112.30,642.48,124.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,375.95,615.39,130.03,10.91;12,112.66,628.93,33.31,10.91">Fungi recognition: A practical use case</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,167.21,628.93,338.98,10.91;12,112.30,642.48,26.65,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,656.03,393.33,10.91;12,112.66,669.58,297.83,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,433.00,656.03,72.99,10.91;12,112.66,669.58,191.96,10.91">Automatic fungi recognition: Deep learning meets mycology</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,313.60,669.58,34.15,10.91">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">633</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,394.53,10.91;13,112.66,100.52,393.33,10.91;13,112.66,114.06,392.48,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,112.66,100.52,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,420.58,100.52,85.40,10.91;13,112.66,114.06,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,393.33,10.91;13,112.66,141.16,393.53,10.91;13,112.30,154.71,124.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,286.77,127.61,219.21,10.91;13,112.66,141.16,50.14,10.91">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,189.82,141.16,316.37,10.91;13,112.30,154.71,26.65,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,168.26,393.33,10.91;13,112.66,181.81,393.33,10.91;13,112.66,195.36,283.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,209.69,168.26,296.30,10.91;13,112.66,181.81,173.82,10.91">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,309.54,181.81,196.44,10.91;13,112.66,195.36,185.04,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,393.33,10.91;13,112.66,222.46,393.32,10.91;13,112.66,236.01,265.37,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,315.63,208.91,190.36,10.91;13,112.66,222.46,205.83,10.91">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,341.36,222.46,164.62,10.91;13,112.66,236.01,106.61,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,249.56,393.33,10.91;13,112.66,263.11,393.33,10.91;13,112.66,276.66,283.16,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,215.77,249.56,290.22,10.91;13,112.66,263.11,193.71,10.91">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,329.54,263.11,176.45,10.91;13,112.66,276.66,185.04,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,290.20,393.53,10.91;13,112.66,303.75,393.32,10.91;13,112.66,317.30,172.27,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,247.87,290.20,258.32,10.91;13,112.66,303.75,135.52,10.91">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,271.13,303.75,234.85,10.91;13,112.66,317.30,74.38,10.91">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,330.85,393.53,10.91;13,112.66,344.40,288.50,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="13,307.65,330.85,198.53,10.91;13,112.66,344.40,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,357.95,394.53,10.91;13,112.66,371.50,393.58,10.91;13,112.66,385.05,351.04,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,112.66,371.50,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,370.86,371.50,135.39,10.91;13,112.66,385.05,252.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,398.60,394.53,10.91;13,112.66,412.15,394.53,10.91;13,112.66,425.70,90.72,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,345.14,398.60,157.76,10.91">Towards open world object detection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,126.92,412.15,375.69,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5830" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,439.25,393.33,10.91;13,112.66,452.79,351.04,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,222.79,439.25,147.79,10.91">Towards open set deep networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,396.22,439.25,109.77,10.91;13,112.66,452.79,252.92,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,466.34,393.33,10.91;13,112.66,479.89,237.70,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Demyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07418</idno>
		<title level="m" coord="13,302.81,466.34,203.17,10.91;13,112.66,479.89,55.98,10.91">Generative openmax for multi-class open set classification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,493.44,393.54,10.91;13,112.66,506.99,260.75,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,329.64,493.44,176.55,10.91;13,112.66,506.99,51.94,10.91">Entropic uncertainty relations and their applications</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tomamichel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,172.86,506.99,122.60,10.91">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">15002</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,520.54,393.32,10.91;13,112.66,534.09,393.33,10.91;13,112.66,547.64,176.26,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,304.77,520.54,201.21,10.91;13,112.66,534.09,154.41,10.91">Deformable part descriptors for fine-grained recognition and attribute prediction</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,289.38,534.09,216.61,10.91;13,112.66,547.64,88.51,10.91">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,561.19,395.17,10.91;13,112.66,574.74,394.53,10.91;13,112.66,588.29,382.13,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,326.73,561.19,181.11,10.91;13,112.66,574.74,64.53,10.91">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,203.39,574.74,262.51,10.91">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,601.84,393.53,10.91;13,112.66,615.39,393.33,10.91;13,112.28,628.93,230.95,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,403.71,601.84,102.49,10.91;13,112.66,615.39,182.95,10.91">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,321.37,615.39,184.61,10.91;13,112.28,628.93,92.47,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="852" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,642.48,393.32,10.91;13,112.66,656.03,394.53,10.91;13,112.66,669.58,90.72,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,345.50,642.48,160.47,10.91;13,112.66,656.03,68.28,10.91">Improving image classification with location context</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,203.40,656.03,299.67,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1008" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,395.17,10.91;14,112.66,100.52,394.62,10.91;14,112.66,114.06,394.53,10.91;14,112.66,127.61,100.87,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,406.44,86.97,101.39,10.91;14,112.66,100.52,372.75,10.91">Dynamic mlp for finegrained image classification by leveraging geographical and temporal information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,114.06,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10945" to="10954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,141.16,394.52,10.91;14,112.66,154.71,374.67,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,378.28,141.16,124.49,10.91">Toward open set recognition</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,112.66,154.71,280.59,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,168.26,395.17,10.91;14,112.66,181.81,394.53,10.91;14,112.66,195.36,330.79,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,266.93,168.26,240.90,10.91;14,112.66,181.81,29.47,10.91">Multi-class open set recognition using probability of inclusion</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,163.82,181.81,248.10,10.91">Computer Vision-ECCV 2014: 13th European Conference</title>
		<title level="s" coord="14,212.03,195.36,90.09,10.91">Proceedings, Part III</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="393" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,395.17,10.91;14,112.66,222.46,393.60,10.91;14,112.66,236.01,146.44,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denouden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Abdelzad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vernekar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02765</idno>
		<title level="m" coord="14,444.23,208.91,63.60,10.91;14,112.66,222.46,360.88,10.91">Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,249.56,393.33,10.91;14,112.66,263.11,277.05,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,258.63,249.56,185.40,10.91">Energy-based out-of-distribution detection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,452.24,249.56,53.75,10.91;14,112.66,263.11,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21464" to="21475" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,393.53,10.91;14,112.66,290.20,393.58,10.91;14,112.33,303.75,29.19,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,407.33,276.66,98.86,10.91;14,112.66,290.20,275.32,10.91">An empirical study for fine-grained fungi recognition with transformer and convnet</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,397.67,290.20,108.57,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,317.30,393.33,10.91;14,112.66,330.85,393.33,10.91;14,112.66,344.40,57.08,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,346.45,317.30,159.53,10.91;14,112.66,330.85,206.01,10.91">Overview of fungiclef 2022: Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,340.27,330.85,165.72,10.91;14,112.66,344.40,26.38,10.91">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,394.61,10.91;14,112.66,371.50,394.53,10.91;14,112.66,385.05,100.87,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,383.60,357.95,103.81,10.91">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,371.50,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,398.60,393.33,10.91;14,112.66,412.15,303.33,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,477.63,398.60,28.36,10.91;14,112.66,412.15,157.33,10.91">Bag of tricks and a strong baseline for fgvc</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,278.16,412.15,105.90,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,425.70,393.53,10.91;14,112.66,439.25,238.10,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,229.73,425.70,276.46,10.91;14,112.66,439.25,92.18,10.91">When large kernel meets vision transformer: A solution for snakeclef &amp; fungiclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,212.94,439.25,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,452.79,394.53,10.91;14,112.66,466.34,394.53,10.91;14,112.66,479.89,394.53,10.91;14,112.66,493.44,100.87,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,112.66,466.34,390.00,10.91">Internimage: Exploring large-scale vision foundation models with deformable convolutions</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,126.92,479.89,375.69,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14408" to="14419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,506.99,393.33,10.91;14,112.39,520.54,393.60,10.91;14,112.66,534.09,243.85,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="14,370.89,506.99,135.10,10.91;14,112.39,520.54,181.37,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.54,520.54,188.44,10.91;14,112.66,534.09,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,547.64,395.17,10.91;14,112.66,561.19,393.33,10.91;14,112.66,574.74,317.74,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,302.02,547.64,205.81,10.91;14,112.66,561.19,172.74,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,308.62,561.19,197.37,10.91;14,112.66,574.74,229.32,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,588.29,394.62,10.91;14,112.66,601.84,394.53,10.91;14,112.41,615.39,27.76,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,322.30,588.29,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,601.84,265.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,628.93,395.17,10.91;14,112.66,642.48,197.93,10.91" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="14,331.02,628.93,176.81,10.91;14,112.66,642.48,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,656.03,393.32,10.91;14,112.66,669.58,393.32,10.91;15,112.66,86.97,233.71,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="14,329.08,656.03,176.90,10.91;14,112.66,669.58,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.28,669.58,188.70,10.91;15,112.66,86.97,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,100.52,393.33,10.91;15,112.66,114.06,394.53,10.91;15,112.66,127.61,55.16,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="15,330.33,100.52,175.66,10.91;15,112.66,114.06,28.98,10.91">Open set learning with counterfactual images</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,165.11,114.06,311.27,10.91">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="613" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,141.16,393.53,10.91;15,112.66,154.71,244.42,10.91" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06207</idno>
		<title level="m" coord="15,292.43,141.16,213.76,10.91;15,112.66,154.71,61.95,10.91">Open-set recognition: A good closed-set classifier is all you need</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,168.26,393.33,10.91;15,112.66,181.81,220.89,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="15,286.82,168.26,151.58,10.91">Reducing network agnostophobia</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,449.65,168.26,56.34,10.91;15,112.66,181.81,176.09,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,393.33,10.91;15,112.66,208.91,393.97,10.91;15,112.66,222.46,48.96,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="15,276.95,195.36,229.03,10.91;15,112.66,208.91,50.09,10.91">Adversarial reciprocal points learning for open set recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,171.28,208.91,292.25,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="8065" to="8081" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.01,393.33,10.91;15,112.66,249.56,393.33,10.91;15,112.66,263.11,106.95,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="15,372.47,236.01,133.52,10.91;15,112.66,249.56,254.60,10.91">1st place solution for fungiclef 2022 competition: Fine-grained open-set fungi recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,375.57,249.56,130.42,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,276.66,393.33,10.91;15,112.66,290.20,107.17,10.91" xml:id="b40">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="15,238.15,276.66,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
