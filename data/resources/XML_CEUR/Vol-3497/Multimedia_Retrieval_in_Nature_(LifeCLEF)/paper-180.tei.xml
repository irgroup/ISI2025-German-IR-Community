<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.58,15.42;1,89.29,106.66,335.37,15.42;1,89.29,128.58,106.29,15.43;1,89.29,150.91,232.31,11.96">Metaformer Model with ArcFaceLoss and Contrastive Learning for SnakeCLEF2023 Fine-Grained Classification Notebook for the &lt;LifeCLEF&gt; Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,62.36,11.96"><forename type="first">Zhennan</forename><surname>Shi</surname></persName>
							<email>zhennanshi@bistu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Information Science and Technology University</orgName>
								<address>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,164.29,176.82,72.83,11.96"><forename type="first">Huazhen</forename><surname>Chen</surname></persName>
							<email>huazhenchen@tju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.77,176.82,50.97,11.96"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>liu.chang.cn@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Information Science and Technology University</orgName>
								<address>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.72,176.82,37.65,11.96"><forename type="first">Jun</forename><surname>Qiu</surname></persName>
							<email>qiu.jun.cn@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Information Science and Technology University</orgName>
								<address>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.58,15.42;1,89.29,106.66,335.37,15.42;1,89.29,128.58,106.29,15.43;1,89.29,150.91,232.31,11.96">Metaformer Model with ArcFaceLoss and Contrastive Learning for SnakeCLEF2023 Fine-Grained Classification Notebook for the &lt;LifeCLEF&gt; Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E92907BEB4A4CBCEB23665B9131F92BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-Grained Visual Classification</term>
					<term>SnakeCLEF2023</term>
					<term>Long Tail Distribution</term>
					<term>Multimodal Backbone</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-Grained Visual Classification (FGVC) has always been a significant direction in computer vision. This paper describes our solution for the SnakeCLEF2023 competition. Firstly, we employ the MetaFormer architecture to process both the meta information and image information of the data. Secondly, we utilize ArcFace loss to address the issue of imbalanced data distribution. Next, we leverage the SimCLR contrastive learning method to allow the model to fully utilize the information from the dataset. Lastly, we employ data preprocessing techniques to enhance accuracy. Our approach achieved 88.30% on the private-score-track1 and 1613 on the private-score-track2, securing the third position.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human eye is an extraordinary organ capable of not only distinguishing broad categories of objects such as bikes, cats, and dogs but also further classifying them into specific subcategories like Garfield cats, Tabby cats, British Shorthair blue cats, and so on. The process of distinguishing different subcategories of cats within the broader category of cats is known as fine-grained visual classification (FGVC). After the rapid development of computer vision, people have attempted to use computer vision instead of human eyes for fine-grained visual classification.</p><p>FGVC has applications in our daily lives, industries, and businesses. For example, when taking a photograph of a bird, this technology can be utilized to identify the species of the bird <ref type="bibr" coords="1,110.49,556.32,11.58,10.91" target="#b0">[1]</ref>. When capturing an image of a car, this technology can be utilized to determine its brand, model, production year, and other relevant details <ref type="bibr" coords="1,340.61,569.87,11.29,10.91" target="#b1">[2]</ref>. FGVC technology is continuously evolving and holds the potential for even more applications in the future. FGVC faces several challenges. Challenge 1: Large intra-class variations. Individuals within the same class can exhibit significant differences in appearance. As shown in Figure <ref type="figure" coords="2,457.24,293.65,3.66,10.91" target="#fig_1">2</ref>, the adult and sub-adult plumages of the Red-crowned Crane have distinct coloration <ref type="bibr" coords="2,427.88,307.20,13.23,10.91" target="#b2">[3]</ref>. Challenge 2: Small inter-class variations. Individuals within the different classes are very similar or closely related in certain aspects. For example, the Coral Snake and the Milk Snake have striking similarities in their appearance, both having bodies with black, red, and yellow bands. Their colors and patterns are almost identical, except for the arrangement of bands. This competition is the SnakeCLEF2023 <ref type="bibr" coords="2,185.67,374.95,15.96,10.91" target="#b3">[4]</ref> competition in LifeCLEF2023 <ref type="bibr" coords="2,332.34,374.95,14.76,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,349.84,374.95,7.63,10.91" target="#b5">6]</ref>, which focuses on snake species classification, both humans and machines face difficulties due to the large intra-class variations and small inter-class variations in snake appearances. To distinguish them, it is necessary to learn features from the head shape, body shape, appearance, skin texture, and eye structure <ref type="bibr" coords="2,492.59,415.59,11.31,10.91" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>FGVC methods can be divided into two categories: methods that only use image information and methods that incorporate meta information, such as geographic location, gender, and shape.</p><p>Using only image information. The mentioned network structures proposed various approaches for image recognition. CMAL-Net <ref type="bibr" coords="3,301.24,151.93,12.99,10.91" target="#b7">[8]</ref> introduced a cross-layer mutual attention learning network that enabled the model to focus on discriminative regions. Yong Hou et al. <ref type="bibr" coords="3,493.30,165.48,12.68,10.91" target="#b8">[9]</ref> proposed a multilayer feature descriptors fusion CNN model that considered both second-order and first-order local feature descriptors from different layers. P-CNN <ref type="bibr" coords="3,399.83,192.57,17.95,10.91" target="#b9">[10]</ref> employed a system consisting of three modules: Squeeze-and-Excitation (SE) block, Part Localization Network (PLN), and Part Classification Network (PCN) to enhance fine-grained classification performance. RA-CNN <ref type="bibr" coords="3,129.92,233.22,17.76,10.91" target="#b10">[11]</ref> proposed a novel recurrent attention convolutional neural network that recursively learned discriminative region attention and region-based feature representation at multiple scales. MRA-CNN <ref type="bibr" coords="3,176.25,260.32,18.06,10.91" target="#b11">[12]</ref> improved RA-CNN by incorporating associations between multiple feature regions. It also introduced a feature scale-dependent (FSD) algorithm to select optimal features as input for the classifier. These network structures proposed different approaches and techniques.</p><p>Moving on to snake image recognition, Amiza Amir et al. <ref type="bibr" coords="3,368.75,314.52,18.07,10.91" target="#b12">[13]</ref> proposed an image-based method for the automatic identification of snake species, achieving an accuracy of 87%. However, it was limited to identifying only 22 species of snakes. Z. Yang et al. <ref type="bibr" coords="3,406.64,341.62,18.07,10.91" target="#b13">[14]</ref> proposed using a detection network to identify the snake's area before classifying its species. Due to limited training data, it could only distinguish 11 snake species. Patel et al. <ref type="bibr" coords="3,376.12,368.71,17.75,10.91" target="#b14">[15]</ref> modified and successfully implemented four region-based convolutional neural network (R-CNN) architectures for image classification, achieving an overall accuracy rate of around 75%.</p><p>Using mata information. Incorporating meta-information has proven effective. Zhai et al. <ref type="bibr" coords="3,89.29,422.91,18.04,10.91" target="#b15">[16]</ref> proposed a joint graph regularized heterogeneous metric learning (JGRHML) algorithm that integrated the structure of different media using joint graph regularization. Geo-Aware <ref type="bibr" coords="3,488.22,436.46,17.76,10.91" target="#b16">[17]</ref> systematically investigated various ways of incorporating geolocation information into finegrained image classification, such as geolocation priors, post-processing, or feature modulation. CVL <ref type="bibr" coords="3,112.94,477.11,18.06,10.91" target="#b17">[18]</ref> proposed a two-stream model combining vision and language (CVL) for learning latent semantic representations. The visual stream learned deep features using convolutional neural networks, while the language stream utilized natural language descriptions to indicate distinctive parts or features of each image. The language stream provided a flexible and compact encoding method for salient visual aspects, aiding in the discrimination of subcategories. These models incorporated meta-information in different ways.</p><p>Regarding snake image recognition with meta-information, Bloch L et al. <ref type="bibr" coords="3,415.14,558.40,17.76,10.91" target="#b18">[19]</ref> utilized YOLOv5 as a detection network and incorporated meta-information such as geographic information for snake classification. However, the issue of imbalanced datasets remained unresolved. I Bolon et al. <ref type="bibr" coords="3,115.28,599.05,18.07,10.91" target="#b19">[20]</ref> used Vision Transformer as the backbone and integrated geographic information through binary masking.</p><p>In summary, these network structures and methods provide insights for improving image recognition and snake species classification. However, each method has its strengths and limitations. Some methods excel in snake species classification accuracy but are limited by imbalanced data, while others can incorporate rich meta-information but may require more computational resources. Therefore, it is crucial to consider these pros and cons and choose the appropriate method based on specific requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>Through analyzing the sample distribution of different categories in the dataset, we discovered a significant class imbalance issue in the dataset. Some classes have as many as 2000 samples, while others have only a few samples. The distribution of images for each category is shown in Figure <ref type="figure" coords="4,119.95,220.76,3.68,10.91" target="#fig_2">3</ref>, which exhibits a long-tail distribution pattern. Furthermore, we compared the dataset with the dataset from the previous year and found an increase of over 200 snake species in Table <ref type="table" coords="4,116.92,247.86,3.81,10.91" target="#tab_0">1</ref>. However, the training set has fewer samples this year, with a reduction of 100000 samples. The increased number of classes and the decreased training samples further increase the difficulty of this year's task. Evaluation Metric:This year, the organizers calculated various metrics. First, they calculated the standard Acc and macro-averaged ğ¹ 1. In addition, they calculated the toxicant confusion error, which is the number of samples that confused toxicants as harmless divided by the number of toxicants in the test set.</p><p>First consider a function ğ‘ such ğ‘(ğ‘ ) = 1 if species ğ‘  is venonmous, otherwise ğ‘(ğ‘ ) = 0. For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>a correct species ğ‘¦ and predicted species ğ‘¦ Ë†, the ğ‘™ğ‘œğ‘ ğ‘ ğ¿(ğ‘¦, ğ‘¦ Ë†) is given as follows:</p><formula xml:id="formula_0" coords="5,175.62,271.41,331.02,65.70">ğ¿(ğ‘¦, ğ‘¦ Ë†) = â§ âª âª âª âª â¨ âª âª âª âª â© 0 if ğ‘¦ = ğ‘¦ 1 if ğ‘¦ Ì¸ = ğ‘¦ Ë†and ğ‘(ğ‘¦) = 0 and ğ‘(ğ‘¦) = 0 2 if ğ‘¦ Ì¸ = ğ‘¦ Ë†and ğ‘(ğ‘¦) = 0 and ğ‘(ğ‘¦ Ë†) = 1 2 if ğ‘¦ Ì¸ = ğ‘¦ Ë†and ğ‘(ğ‘¦) = 1 and ğ‘(ğ‘¦ Ë†) = 1 5 if ğ‘¦ Ì¸ = ğ‘¦ Ë†and ğ‘(ğ‘¦) = 1 and ğ‘(ğ‘¦ Ë†) = 0 (1)</formula><p>The challenge meteric private-score-track2 is sum of ğ¿ over all test observations:</p><formula xml:id="formula_1" coords="5,202.28,373.24,304.35,25.43">ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ -ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ -ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘˜2 = âˆ‘ï¸ ğ‘– ğ¿ (ğ‘¦ ğ‘– , ğ‘¦ Ë†ğ‘–)<label>(2)</label></formula><p>The metric private-score-track1 is a weighted average between the macro ğ¹ 1-score and the weighted accuracies of different types of confusion.</p><formula xml:id="formula_2" coords="5,102.71,447.94,403.93,50.00">ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ -ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ -ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘˜1 = (ğ‘¤ 1 ğ¹ 1 + ğ‘¤ 2 (100 -ğ‘ƒ 1 ) + ğ‘¤ 3 (100 -ğ‘ƒ 2 ) + ğ‘¤ 4 (100 -ğ‘ƒ 3 ) + ğ‘¤ 5 (100 -ğ‘ƒ 4 )) / 5 âˆ‘ï¸ ğ‘– ğ‘¤ ğ‘–<label>(3)</label></formula><p>where ğ‘¤ 1 = 1.0, ğ‘¤ 2 = 1.0, ğ‘¤ 3 = 2.0, ğ‘¤ 4 = 5.0, ğ‘¤ 5 = 2.0, are the weights of individual terms. ğ¹ 1 is the macro ğ¹ 1-score, ğ‘ƒ 1 is the percentage of wrongly classified harmless species as another harmless species, ğ‘ƒ 2 is the percentage of wrongly classified harmless species as another venomous species, ğ‘ƒ 3 is the percentage of wrongly classified venomous species as another harmless species, and ğ‘ƒ 4 is the percentage of wrongly classified venomous species as another venomous species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>For the competition, we utilized the MetaFormer <ref type="bibr" coords="5,315.46,626.38,17.76,10.91" target="#b20">[21]</ref> architecture, which is a useful network architecture for computer vision tasks. MetaFormer has designed a five-stage network structure. The first stage S0 is a simple three-layer convolutional structure. S1 and S2 are MBConv blocks <ref type="bibr" coords="5,89.29,667.03,17.76,10.91" target="#b21">[22]</ref> with squeeze-excitation. MBConv blocks are based on an inverted residual mechanism and a bottleneck design. The inverted residual structure is designed to provide higher nonlinear representation capability while keeping the model lightweight, and the bottleneck uses smaller intermediate layers to reduce computation. S3 and S4 are Transformer blocks with relative position bias. This bias alleviates the problem that the order of the tokens in the input sequence cannot be used in the self-attention operation.</p><p>We modify the input meta information of MetaFormer. Specifically, the meta information is modified to include the code, endemic, and binomial names. The workflow of MetaFormer is shown in Figure <ref type="figure" coords="6,164.29,181.81,4.23,10.91" target="#fig_3">4</ref> <ref type="bibr" coords="6,168.52,181.81,16.92,10.91" target="#b22">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Long-Tailed loss</head><p>In order to address the issue of imbalanced sample distribution in the dataset, we employed a long-tail loss function. There are several commonly used loss functions to cope with the class imbalance problem. ArcFace loss <ref type="bibr" coords="6,233.76,490.58,17.76,10.91" target="#b23">[24]</ref> is designed for face recognition tasks. Other loss functions include the Class-Balanced Loss <ref type="bibr" coords="6,235.69,504.13,16.34,10.91" target="#b24">[25]</ref>, which calculates a small neighborhood associated with each sample for computation. Seesaw Loss <ref type="bibr" coords="6,278.83,517.68,17.78,10.91" target="#b25">[26]</ref> mitigates the risk of increased misclassification due to gradient attenuation in negative samples. Equalization Loss v2 <ref type="bibr" coords="6,407.41,531.23,18.07,10.91" target="#b26">[27]</ref> discovers a novel gradient-guided reweighting mechanism.</p><p>Compared with other loss functions, ArcFace Loss optimizes the measure of feature space by introducing angle cosine values, so that the angles between feature vectors can reflect the similarity between samples. By normalizing the feature vectors and introducing an adjustable parameter, ArcFace Loss enhances the distinguishability of the features and reduces the difference in magnitude of the feature vectors. These features make ArcFace Loss a good performer in fine-grained classification tasks, so we adopt it as our loss function. ArcFace loss's formula is as follows:</p><formula xml:id="formula_3" coords="7,159.26,108.32,347.38,35.56">ğ¿ arcface (ğœƒ) = - 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 log ğ‘’ ğ‘ (cos(ğœƒğ‘¦ ğ‘– +ğ‘š)) ğ‘’ ğ‘ (cos(ğœƒğ‘¦ ğ‘– +ğ‘š)) + âˆ‘ï¸€ ğ‘› ğ‘—=1,ğ‘—Ì¸ =ğ‘¦ ğ‘– ğ‘’ ğ‘  cos ğœƒ ğ‘— (4)</formula><p>where ğ‘ represents the number of samples, ğ‘› represents the number of classes, ğ‘¦ ğ‘– is the true class of the ğ‘–-th sample, ğœƒ ğ‘¦ ğ‘– is the angle between the feature vector of the ğ‘–-th sample and its true class, and ğœƒ ğ‘— is the angle between the feature vector of the ğ‘–-th sample and the ğ‘—-th class. ğ‘  represents the scale parameter. ğ‘š represents the margin parameter, which is the inter-class distance. In both the numerator and denominator, a margin is added to each individual term, represented by ğ›¼ and ğ›½ in Figure <ref type="figure" coords="7,232.05,222.23,3.66,10.91" target="#fig_4">5</ref>. By calculating the loss with this margin, the margin increases gradually, resulting in the compression of the region for each class. This effectively enlarges the inter-class distance while reducing the intra-class distance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SimCLR self-supervised learning</head><p>SimCLR <ref type="bibr" coords="7,138.65,603.26,17.80,10.91" target="#b27">[28]</ref> is a self-supervised learning framework that has gained significant attention in the field of computer vision. The main goal of SimCLR is to learn meaningful representations of unlabeled data by maximizing agreement between different augmented views of the same image while minimizing agreement between views of different images. The framework's workflow is shown in Figure <ref type="figure" coords="7,163.19,657.46,3.67,10.91" target="#fig_5">6</ref>. By maximizing the similarity between the representations of positive image pairs, SimCLR encourages the model to learn meaningful and discriminative features of images.</p><p>We also adopted the InfoNCE <ref type="bibr" coords="8,235.17,338.25,17.96,10.91" target="#b28">[29]</ref> loss function, commonly used in contrastive learning. It places positive samples in the numerator and negative samples in the denominator, aiming to maximize the similarity of positive samples while minimizing the similarity of negative samples. The formula for InfoNCE is expressed as follows:</p><formula xml:id="formula_4" coords="8,225.99,402.34,280.65,28.96">ğ¿ ğ‘ = -log exp (ğ‘ â€¢ ğ‘˜ + /ğœ ) âˆ‘ï¸€ ğ‘˜ ğ‘–=0 exp (ğ‘ â€¢ ğ‘˜ ğ‘– /ğœ )<label>(5)</label></formula><p>where ğ‘˜ + represents the positive samples for ğ‘ while the remaining ğ‘˜ denotes the negative samples. ğœ refers to the temperature hyperparameter, which we set to 0.25 in our subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Pre-post-process</head><p>Data Preprocessing: In order to enhance the generalization ability of the method, we applied image augmentation techniques to the input images, including resizing and center cropping. We also introduced random vertical flipping, random horizontal flipping, and random 45-degree rotation to augment the training dataset.</p><p>Data Postprocessing: During the testing phase, we employed the Test Time Augmentation (TTA) strategy. TTA augments the input test data with operations such as expansion, flipping, and rotation to obtain a set of data for an image and take the mean value of the final predictions. For the csv file of the test dataset, each observation-id corresponds to multiple images, and the model predicts one class for each image, so there may be multiple different predicted classes for each observation id. For each observation id, we select the class with the most occurrences as the final prediction result. We also adopt an integrated learning strategy to improve the accuracy by fusing the results of MetaFormer-0, Metaformer-2, and Metaformer-2 with SimCLR three models. For each observation-id, we select the class with the most occurrences from the csv files generated by the three models as the final prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We conducted all the experiments with one NVIDIA GeForce RTX 3090. We used the AdamW <ref type="bibr" coords="9,119.89,193.06,24.78,10.91" target="#b29">[30]</ref> optimizer with a weight decay of 0.05 and a base learning rate of 5e-5. The batch size is determined by the maximum number that the GPU can handle, usually an integer multiple of 2. We use the batch size of 22. And the initial learning rate is modified according to batch size (That is, the learning rate multiplied by the batch size multiplied by the number of GPUs divided by 512.). Also, set the number of training epochs to 100. During training, we used data augmentation and rotation. We also use the CosineLRScheduler of the timm library to modify the learning rate. At first the learning rate increases from the warmup learning rate 5e-8 to the base learning rate, and then enters the cosine annealing phase, where the learning rate is adjusted by the cosine function, decreasing with increasing epochs until it decreases to min learning rate 5e-7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>Firstly, we attempted the approach based on EfficientNet <ref type="bibr" coords="9,361.89,364.14,16.41,10.91" target="#b30">[31]</ref>, the loss function used was CrossEntropy Loss. However, we found the model couldn't incorporate meta information, resulting in lower accuracy than expected. Therefore, we switched to the MetaFormer model. Comparison of above two models is shown in Table <ref type="table" coords="9,317.22,404.79,3.66,10.91">2</ref>. The results demonstrated the superiority of the MetaFormer backbone. Thus, we use MetaFormer-2 as the backbone of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results of different models. Accuracy is the number of samples correctly predicted in the validation dataset divided by the total number of samples in the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy Size of image Parameters</head><p>EfficientNet-B7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.675">384Ã—384 66M</head><p>MetaFormer-0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.726">384Ã—384 28M</head><p>MetaFormer-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.764">384Ã—384 81M</head><p>To further enhance accuracy, we employed the SimCLR contrastive learning method to train the model and fine-tuned it with an input of size 512x512. As shown in Table <ref type="table" coords="9,443.48,570.90,3.81,10.91">3</ref>, our highest accuracy was achieved using the SimCLR contrastive learning method. The results show the effectiveness of the contrastive learning method SimCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented our solution for the snakeCLEF2023 competition. We adopted the MetaFormer architecture to incorporate effective meta information, utilized the ArcFace loss Table <ref type="table" coords="10,116.06,90.49,5.12,8.93">3</ref> Results of SimCLR and input of size 512x512. We found that using 512Ã—512 large size fine tuning does not work very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy Epochs</head><p>SimCLR+MetaFormer-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.838">100</head><p>MetaFormer-2 0.734 100 function to address the issue of long-tail data distribution, employed the SimCLR contrastive learning method with pre-trained models to improve accuracy, and applied data augmentation techniques to enhance the model's generalization ability.</p><p>Our solution achieved 88.30% on the private-score-track1 and and 1613 on the private-score-track2, securing the third position. Due to time and resource constraints, we were unable to further explore the long-tail loss function and new contrastive learning methods. However, the results demonstrate the effectiveness of our approach and highlight the potential for further improvements. Future work could involve the following aspects: 1) Exploring or designing new long-tail loss functions. 2) Investigating other contrastive learning methods, such as the MAE <ref type="bibr" coords="10,89.29,320.96,17.91,10.91" target="#b31">[32]</ref> pre-training method, to further enhance the performance of pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,229.66,416.69,9.96;2,89.29,241.61,413.53,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General image classification vs FGVC. We can see that general image classification focuses on distinguishing broad classes of species, while FGVC focuses on subtle differences between classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,570.96,418.23,9.96;2,88.99,582.92,416.99,9.96;2,89.29,594.87,87.45,9.96;2,108.02,450.79,60.78,75.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Adult Red-crowned Crane, (b) Sub-adult Red-crowned Crane, (c) Coral Snake, (d) Milk Snake. (a) and (b) show the problem of large intra-class variations. (c) and (d) show the problem of small inter-class variations.</figDesc><graphic coords="2,108.02,450.79,60.78,75.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,503.41,416.70,9.96;4,89.29,515.36,416.70,9.96;4,89.29,527.32,78.08,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples count, with the y-axis representing the number of samples and the x-axis representing the categories in descending order of quantity. It can be seen that the dataset sample numbers are highly unbalanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,337.00,416.69,9.96;6,89.29,348.95,416.69,9.96;6,89.29,360.91,416.70,9.96;6,89.29,372.86,416.87,9.96;6,89.29,384.82,416.69,9.96;6,89.29,396.77,174.57,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The model employs convolutional layers to extract visual features and then transforms the image features into visual tokens through Patch Embedding. The Code, Endemic, and Binomial name meta information are one-hot encoded and passed through a Non-Linear Embedding layer to obtain meta tokens. The visual tokens, meta tokens, and class tokens are fused using Relative Transformer Layers. The fused tokens are continuously aggregated in the following attention blocks. The final output class token is used for category prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,284.09,294.50,22.55,8.92;7,121.42,514.86,384.56,9.96;7,89.29,526.82,416.69,9.96;7,89.29,538.77,21.66,9.96"><head>result 5 :</head><label>5</label><figDesc>The results of ArcFace loss on the MNIST dataset have shown significant improvements in classification performance. Each point in the figure represents a sample, and each color represents a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,224.33,418.35,9.96;8,89.29,236.28,289.50,10.32;8,381.78,236.28,26.67,10.32;8,409.35,236.28,96.64,9.96;8,89.29,248.24,254.78,10.32;8,347.07,248.24,26.75,10.32;8,377.21,248.24,128.94,9.96;8,89.29,260.19,416.70,9.96;8,89.29,272.81,7.45,9.65;8,99.73,272.15,25.86,10.32;8,126.50,272.15,192.36,9.96"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The framework of SimCLR involves the following steps:1) Take an input image and apply random transformations to generate two augmented images, denoted as ğ‘¥ ğ‘– and ğ‘¥ ğ‘— . 2) Pass the augmented images through an encoder to obtain image representations, â„ ğ‘– and â„ ğ‘— respectively. 3) Use a non-linear fully connected layer to map the data to the representation space, ğ‘§. 4) Maximize the similarity between ğ‘§ ğ‘– and ğ‘§ ğ‘— , the representations of the positive image pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,417.00,129.29"><head>Table 1</head><label>1</label><figDesc>Compositions of the 2022 and 2023 datasets were compared.We can find that the number of training samples was much reduced.</figDesc><table coords="5,95.27,133.12,406.28,86.65"><row><cell></cell><cell>2022</cell><cell>2023</cell></row><row><cell>Class</cell><cell>1572</cell><cell>1785</cell></row><row><cell>Train samples</cell><cell>Around 270000</cell><cell>Around 180000</cell></row><row><cell>Test samples</cell><cell>Around 48000</cell><cell>Around 14000</cell></row><row><cell cols="3">Meta information Endemic, Binomial name, Country, Code Endemic, Binomial name, Code</cell></row><row><cell>Image size</cell><cell cols="2">240Ã—240, 500Ã—500, Original Image</cell></row><row><cell>Image dimension</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://github.com/BAOfanTing/</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,379.59,395.17,10.91;10,112.66,393.14,393.33,10.91;10,112.33,406.69,29.19,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miyaguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cheungvivatpant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dudley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Swain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04805</idno>
		<title level="m" coord="10,407.69,379.59,100.15,10.91;10,112.66,393.14,235.92,10.91">Motif mining and unsupervised representation learning for birdclef 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,420.24,395.16,10.91;10,112.66,433.79,393.33,10.91;10,112.66,447.34,394.52,10.91;10,112.66,460.89,80.57,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,245.63,420.24,262.19,10.91;10,112.66,433.79,230.93,10.91">Effective vehicle classification and re-identification on stanford cars dataset using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cynthia Sherin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jayavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,365.94,433.79,140.05,10.91;10,112.66,447.34,347.85,10.91">Proceedings of 3rd International Conference on Artificial Intelligence: Advances and Applications: ICAIAA 2022</title>
		<meeting>3rd International Conference on Artificial Intelligence: Advances and Applications: ICAIAA 2022</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,474.43,393.33,10.91;10,112.26,487.98,394.93,10.91;10,112.66,501.53,169.64,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,312.24,474.43,193.74,10.91;10,112.26,487.98,386.66,10.91">A multi-scale approach to investigating the wintering habitat selection of red-crowned cranes in the yancheng nature reserve, china</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,501.53,124.84,10.91">Pakistan Journal of Zoology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,515.08,395.17,10.91;10,112.66,528.63,393.33,10.91;10,112.66,542.18,107.76,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,321.04,515.08,186.79,10.91;10,112.66,528.63,191.50,10.91">Overview of snakeclef 2023: Snake identification in medically important scenarios</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,328.87,528.63,177.11,10.91;10,112.66,542.18,77.06,10.91">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,555.73,394.53,10.91;10,112.66,569.28,395.01,10.91;10,112.30,582.83,393.68,10.91;10,112.66,596.38,394.53,10.91;10,112.66,609.93,393.33,10.91;10,112.66,623.48,289.43,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,383.16,582.83,122.82,10.91;10,112.66,596.38,171.35,10.91">Lifeclef 2023 teaser: Species identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Moussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.71,609.93,113.28,10.91;10,112.66,623.48,38.01,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,637.02,394.53,10.91;10,112.66,650.57,394.53,10.91;10,112.66,664.12,393.33,10.91;11,112.66,86.97,395.17,10.91;11,112.66,100.52,394.52,10.91;11,112.33,114.06,329.01,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,112.66,664.12,393.33,10.91;11,112.66,86.97,135.53,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,271.76,86.97,236.08,10.91;11,112.66,100.52,389.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">September 18-23, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.53,10.91;11,112.39,141.16,394.89,10.91;11,112.66,154.71,394.52,10.91;11,112.66,168.26,123.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,406.80,127.61,99.40,10.91;11,112.39,141.16,214.27,10.91">A cnn based model for venomous and non-venomous snake classification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">I</forename><surname>Progga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rezoana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">U</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,347.29,141.16,159.99,10.91;11,112.66,154.71,171.39,10.91">Applied Intelligence and Informatics: First International Conference, AII 2021</title>
		<meeting><address><addrLine>Nottingham, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">July 30-31, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="216" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.53,10.91;11,112.66,195.36,393.32,10.91;11,112.33,208.91,62.36,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,267.15,181.81,239.04,10.91;11,112.66,195.36,277.42,10.91">Learn from each other to classify better: Cross-layer mutual attention learning for fine-grained visual classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,398.09,195.36,89.80,10.91">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">109550</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,393.32,10.91;11,112.66,236.01,393.33,10.91;11,112.66,249.56,69.96,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,345.48,222.46,160.50,10.91;11,112.66,236.01,203.17,10.91">Multilayer feature descriptors fusion cnn models for fine-grained visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,324.20,236.01,181.79,10.91">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1897</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,263.11,393.33,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,135.63,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,291.95,263.11,214.04,10.91;11,112.66,276.66,161.37,10.91">P-cnn: Part-based convolutional neural networks for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,282.31,276.66,223.68,10.91;11,112.66,290.20,51.70,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,303.75,393.33,10.91;11,112.66,317.30,393.32,10.91;11,112.66,330.85,276.41,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,215.77,303.75,290.22,10.91;11,112.66,317.30,194.88,10.91">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,330.89,317.30,175.09,10.91;11,112.66,330.85,178.50,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,395.17,10.91;11,112.66,357.95,251.08,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,223.37,344.40,284.46,10.91;11,112.66,357.95,16.17,10.91">Combining multi-feature regions for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fayou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,136.80,357.95,153.15,10.91">Int. J. Image Graph. Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,394.61,10.91;11,112.66,398.60,393.33,10.91;11,112.66,412.15,141.85,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,336.92,371.50,169.07,10.91;11,112.66,385.05,148.83,10.91">Image classification for snake species using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,283.07,385.05,224.20,10.91;11,112.66,398.60,393.33,10.91;11,112.66,412.15,21.62,10.91">Computational Intelligence in Information Systems: Proceedings of the Computational Intelligence in Information Systems Conference (CIIS 2016)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,425.70,361.77,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="11,201.80,425.70,240.71,10.91">Snake detection and classification using deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,439.25,393.32,10.91;11,112.66,452.79,393.33,10.91;11,112.41,466.34,60.01,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,446.56,439.25,59.43,10.91;11,112.66,452.79,348.36,10.91">Revealing the unknown: real-time recognition of galÃ¡pagos snake species using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,469.28,452.79,36.71,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,479.89,393.53,10.91;11,112.66,493.44,394.53,10.91;11,112.39,506.99,141.72,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,219.81,479.89,286.38,10.91;11,112.66,493.44,92.27,10.91">Heterogeneous metric learning with joint graph regularization for cross-media retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,227.52,493.44,275.48,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1198" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,520.54,393.33,10.91;11,112.66,534.09,393.33,10.91;11,112.66,547.64,262.63,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,458.38,520.54,47.61,10.91;11,112.66,534.09,171.00,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,309.82,534.09,196.16,10.91;11,112.66,547.64,194.36,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,561.19,394.62,10.91;11,112.66,574.74,394.53,10.91;11,112.66,588.29,65.30,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,182.67,561.19,304.38,10.91">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,574.74,364.29,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,601.84,394.53,10.91;11,112.66,615.39,360.17,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="11,357.11,601.84,150.07,10.91;11,112.66,615.39,328.25,10.91">Combination of object detection, geospatial data, and feature concatenation for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>BÃ¶ckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bracke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,628.93,393.33,10.91;11,112.66,642.48,393.32,10.91;11,112.66,656.03,386.02,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,453.38,628.93,52.61,10.91;11,112.66,642.48,393.32,10.91;11,112.66,656.03,146.21,10.91">An artificial intelligence model to identify snakes from across the world: Opportunities and challenges for global health and herpetology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,267.43,656.03,143.35,10.91">PLoS neglected tropical diseases</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">10647</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,393.53,10.91;12,112.66,86.97,288.50,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="11,307.65,669.58,198.53,10.91;12,112.66,86.97,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,100.52,393.33,10.91;12,112.66,114.06,393.33,10.91;12,112.66,127.61,182.19,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,368.79,100.52,137.20,10.91;12,112.66,114.06,98.34,10.91">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,234.40,114.06,271.58,10.91;12,112.66,127.61,84.28,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,393.53,10.91;12,112.66,154.71,316.84,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11637</idno>
		<title level="m" coord="12,329.89,141.16,176.30,10.91;12,112.66,154.71,134.65,10.91">Explored an effective methodology for fine-grained snake recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.66,181.81,393.33,10.91;12,112.66,195.36,147.08,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,276.47,168.26,229.52,10.91;12,112.66,181.81,48.67,10.91">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.24,181.81,321.74,10.91;12,112.66,195.36,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,393.53,10.91;12,112.66,222.46,393.33,10.91;12,112.66,236.01,147.08,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,305.50,208.91,200.69,10.91;12,112.66,222.46,44.86,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,180.82,222.46,325.16,10.91;12,112.66,236.01,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,394.53,10.91;12,112.66,263.11,393.58,10.91;12,112.66,276.66,341.92,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,112.66,263.11,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,370.86,263.11,135.39,10.91;12,112.66,276.66,244.01,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,393.33,10.91;12,112.66,303.75,393.53,10.91;12,112.39,317.30,231.07,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,270.42,290.20,235.56,10.91;12,112.66,303.75,133.59,10.91">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.92,303.75,237.27,10.91;12,112.39,317.30,133.15,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,330.85,393.33,10.91;12,112.66,344.40,394.53,10.91;12,112.39,357.95,141.72,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,366.02,330.85,139.97,10.91;12,112.66,344.40,92.27,10.91">Deep graph clustering via dual correlation reduction</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.03,344.40,274.97,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7603" to="7611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,371.50,394.53,10.91;12,112.66,385.05,173.79,10.91" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="12,248.09,371.50,254.55,10.91">Representation learning with contrastive predictive coding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,398.60,393.33,10.91;12,112.66,412.15,107.17,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="12,238.15,398.60,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,425.70,394.53,10.91;12,112.66,439.25,346.82,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="12,178.42,425.70,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.29,439.25,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,452.79,393.33,10.91;12,112.66,466.34,393.32,10.91;12,112.66,479.89,159.65,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="12,330.08,452.79,175.91,10.91;12,112.66,466.34,34.52,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,170.26,466.34,335.73,10.91;12,112.66,479.89,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
