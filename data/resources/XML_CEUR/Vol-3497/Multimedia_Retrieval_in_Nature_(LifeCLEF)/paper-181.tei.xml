<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,380.08,15.42;1,89.29,128.58,81.48,15.43;1,89.29,150.91,219.16,11.96">Leverage Samples with Single Positive Labels to Train CNN-based Models For Multi-label Plant Species Prediction Notebook for the LifeCLEF Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,80.23,11.96"><forename type="first">Huy</forename><surname>Quang Ung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KDDI Research, Inc</orgName>
								<address>
									<settlement>Fujimino</settlement>
									<region>Saitama</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.15,176.82,74.92,11.96"><forename type="first">Ryoichi</forename><surname>Kojima</surname></persName>
							<email>kojima@kddi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">KDDI Research, Inc</orgName>
								<address>
									<settlement>Fujimino</settlement>
									<region>Saitama</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.07,176.82,64.19,11.96"><forename type="first">Shinya</forename><surname>Wada</surname></persName>
							<email>wada@kddi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">KDDI Research, Inc</orgName>
								<address>
									<settlement>Fujimino</settlement>
									<region>Saitama</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,380.08,15.42;1,89.29,128.58,81.48,15.43;1,89.29,150.91,219.16,11.96">Leverage Samples with Single Positive Labels to Train CNN-based Models For Multi-label Plant Species Prediction Notebook for the LifeCLEF Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A4904F622FF3A4C933A4E72466F87AC3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Single positive label</term>
					<term>CNNs</term>
					<term>three-step training strategy</term>
					<term>late fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the geographical distribution of plant species is useful in many scenarios related to biodiversity management and conservation. By associating plant species occurrences with environmental features of each location, we can model the relationship between an environment and the species. However, the cost of multi-label plant species annotation for a large dataset is expensive and time consuming, so it may only be possible to obtain a single positive label for each location. This type of dataset is provided in the GeoLifeCLEF 2023 competition, where learning multi-labels from single positive labels is the main challenge. In this report, we present our proposed models that achieved the best performance in GeoLifeCLEF 2023. We proposed several CNN-based models and a training strategy for learning samples with single positive labels. We conducted experiments to show the effectiveness of our method compared to a simple baseline on the provided dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modelling species distribution is an essential task for monitoring and making conservation decisions for a wide variety of species. Nowadays, the research community has generated millions of geolocated species observations every year, covering tens of thousands of species, which is a good opportunity to apply machine learning and deep learning based models. A common approach is to build a species distribution model (SDM) <ref type="bibr" coords="1,381.89,497.54,11.47,10.91" target="#b0">[1]</ref>, which uses the environmental variables of the location (e.g. temperature, elevation, land cover, soil, etc.) to predict the presence of species at that location.</p><p>Following the ongoing series of the GeoLifeCLEF competitions <ref type="bibr" coords="1,375.06,538.19,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,388.72,538.19,7.42,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,398.58,538.19,7.42,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,408.44,538.19,7.42,10.91" target="#b4">5,</ref><ref type="bibr" coords="1,418.29,538.19,7.49,10.91" target="#b5">6]</ref>, the GeoLifeCLEF 2023 <ref type="bibr" coords="1,112.45,551.74,11.49,10.91" target="#b6">[7]</ref>, which is a part of LifeCLEF 2023 <ref type="bibr" coords="1,278.67,551.74,11.49,10.91" target="#b7">[8]</ref>, aims to predict the presence of plant species at a given location and their change at different timestamps, providing a large scale dataset of raster and time series based variables. The goal of GeoLifeCLEF 2023 is to predict multi-label plant species for each location, while the last GeoLifeCLEF 2022 is to predict only a single label species. Furthermore, the main challenge of GeoLifeCLEF 2023 is that 98 percentages of the samples provided have only single positive labels. Other difficulties include the long-tail distribution of plant species, large-scale multi-modal learning, and a variety of plant species classes.</p><p>Convolutional Neural Networks (CNNs) have achieved great performance in computer vision in recent years. In the GeoLifeCLEF 2022 competition, several CNN-based SDM models have been proposed for learning raster-based variables. However, it is difficult to train CNN-based models by samples with single positive labels <ref type="bibr" coords="2,293.64,181.81,11.35,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,307.72,181.81,12.54,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,322.99,181.81,14.02,10.91" target="#b10">11]</ref> for multi-label prediction task. Single positive labels for multi-label prediction task are considered as noisy labels due to lack of other positive labels <ref type="bibr" coords="2,155.17,208.91,11.43,10.91" target="#b8">[9]</ref>.</p><p>In this technical report, we present several CNN-based models for multi-label plant species prediction on the provided dataset of GeoLifeCLEF 2023. In addition, we introduce an efficient three-step training strategy for leveraging samples with single positive labels to train our proposed CNN-based model. We also present detailed experiments of our proposed method.</p><p>The remaining of this report is organized as follows. Section 2 presents related work of this report. Section 3 describes the provided dataset and our main task in details. Section 4 describes our preprocessing steps. Section 5 introduces our proposed method. Section 6 presents detailed experiments and results. Finally, section 7 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-label learning from single positive labels</head><p>To the best of our knowledge, there has been no study of learning multi-label prediction from single positive labels for plant species prediction. However, Cole et al. <ref type="bibr" coords="2,416.48,410.45,13.00,10.91" target="#b8">[9]</ref> pointed out this problem for the field of computer vision and proposed a method for estimating unobserved labels. They also considered possible methods such as label smoothing to reduce the negative impacts of negative labels assumed. Zhou et al. <ref type="bibr" coords="2,310.75,451.09,18.07,10.91" target="#b9">[10]</ref> introduced a pseudo-labelling method for labelling unobserved positive labels and applied Expectation Maximization loss in their training phase. Xie et al. <ref type="bibr" coords="2,204.17,478.19,18.07,10.91" target="#b10">[11]</ref> also proposed a pseudo-labelling method and a regularization term to address this problem. Although these methods showed their effectiveness on learning multi-labels from single positive labels, those were only experimented on benchmarks with less than 200 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Binary cross-entropy loss</head><p>First, let define a setting for a multi-label prediction. We assume that each input ğ‘¥ from ğ’³ corresponds to a vector label y from the label space ğ‘Œ = {ğ‘¦ ğ‘– } ğ‘–âˆˆ[1,ğ¿] âˆˆ {0, 1} ğ¿ , where L is the number of classes (i.e. species), an entry ğ‘¦ ğ‘– = 1 if the ğ‘–-th class is relevant to ğ‘¥ (i.e. the species ğ‘– is present at the location encoded by ğ‘¥) and ğ‘¦ ğ‘– = 0 otherwise (i.e. the species is absent). The main objective is to find a function ğ‘“ : ğ’³ â†’ ğ’´ that predicts the labels for each ğ‘¥.</p><p>The binary cross-entropy (BCE) loss is one of the most common loss for multi-label learning <ref type="bibr" coords="2,106.60,649.86,16.39,10.91" target="#b11">[12]</ref>. For an observed data point (ğ‘¥ ğ‘› , ğ‘¦ ğ‘› ) including full positive and negative classes, the BCE loss is calculated as follows:</p><formula xml:id="formula_0" coords="3,149.83,108.32,356.81,33.71">â„’ ğµğ¶ğ¸ (ğ‘“ ğ‘› , ğ‘¦ ğ‘› ) = - 1 ğ¿ ğ¿ âˆ‘ï¸ ğ‘–=1 [1 [ğ‘¦ ğ‘›ğ‘– =1] log(ğ‘“ ğ‘›ğ‘– ) + 1 [ğ‘¦ ğ‘›ğ‘– =0] log(1 -ğ‘“ ğ‘›ğ‘– )]<label>(1)</label></formula><p>where</p><formula xml:id="formula_1" coords="3,120.03,154.27,88.62,10.63">ğ‘“ ğ‘› = ğ‘“ (ğ‘¥ ğ‘› ) âˆˆ [0, 1]</formula><p>is the model predicted probability of presence for species i under input ğ‘¥ ğ‘› , and 1 <ref type="bibr" coords="3,158.05,172.04,7.06,6.99">[.]</ref> denotes the indicator function, i.e., 1 ğ‘˜ = 1 if the assertion ğ‘˜ is verified, or 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Assume negative loss</head><p>Suppose that we have partially observed data (ğ‘¥ ğ‘› , ğ‘§ ğ‘› ), the label space is ğ’µ = {ğ‘§ ğ‘– } ğ‘–=1,ğ¿ = {1, 0, âˆ…} ğ¿ and suppose that all of the observed labels are positive i.e., if ğ‘§ ğ‘›ğ‘– is known, then ğ‘§ ğ‘›ğ‘– = 1. Here, we can formulate a loss function with the positive term of 1 [ğ‘¦ ğ‘›ğ‘– =1] log(ğ‘“ ğ‘›ğ‘– ). For unobserved labels, we cannot simply ignore them in the loss function since it could cause that the trained model always results positive classes. The simple approach is to assume unobserved labels are negative. This "assume negative" (AN) loss is calculated as follows:</p><formula xml:id="formula_2" coords="3,154.79,323.81,351.85,33.71">â„’ ğ´ğ‘ (ğ‘“ ğ‘› , ğ‘§ ğ‘› ) = - 1 ğ¿ ğ¿ âˆ‘ï¸ ğ‘–=1 [1 [ğ‘§ ğ‘›ğ‘– =1] log(ğ‘“ ğ‘›ğ‘– ) + 1 [ğ‘§ ğ‘›ğ‘– Ì¸ =1] log(1 -ğ‘“ ğ‘›ğ‘– )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and dataset</head><p>The final goal of GeoLifeCLEF 2023 is to predict the set of plant species presence in a given location and time using various related features: images and time-series data captured from a satellite, time-series climatic data, and other rasterized environmental data: land cover, human footprint, bioclimatic and soil variables. In this technical report, we present rasterbased variables used in our method, i.e., satellite raster images, aggregated human footprint rasters, bioclimatic rasters, and soil-grid rasters. Other variables are described in detail at this competition's homepage 1 .</p><p>â€¢ Satellite raster images: There are RGB and Near Infra-Red (NIR) captured over a square area of 1280 meter Ã— 1280 meter. They are formatted in a size of 128 Ã— 128 pixels. An example is shown in Figure <ref type="figure" coords="3,241.40,534.42,3.74,10.91" target="#fig_0">1</ref> silt, cation exchange capacity, and coarse fragments. These properties were measured from 5 to 15-centimeter depth. Their resolution is 1 kilometer per pixel.</p><p>This competition provides a large-scale training set of about 5 million samples of plant occurrences in Europe. This training set, where only a single positive class is labeled, is socalled presence-only data (PO in short). A validation set consists of 5,948 samples with all the present species (multi-labels of both positive and negative classes), which is so-called presenceabsence data (PA in short). Here, we used a part of the PA data to train our models. A testing set consists of 22,404 samples. The testing set is implicitly divided into a public set and a private set for evaluation. The total number of plant species is 10,039 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Preprocessing data</head><p>For preprocessing data, we used the source code <ref type="foot" coords="4,301.72,431.68,3.71,7.97" target="#foot_0">2</ref> provided by the organizers. The raster-based variables, which consist of bioclimatic rasters, summarized human footprint rasters, and soi-grid rasters are formed into the size of 128 Ã— 128 pixels (the same as the sizes of satellite raster images). The resolutions of them are the same as the above-mentioned. Each satellite raster image and raster-based variable are applied the standard normalization before inputting to our models, where their average and standard deviation values are calculated on training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proposed method</head><p>This section presents the architectures of our proposed models, a method for combining them, and an efficient training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model architectures</head><p>To address this multi-label plant species prediction, we experimented with three CNN-based models with the ResNet <ref type="bibr" coords="4,197.91,636.24,17.90,10.91" target="#b12">[13]</ref> backbone, i.e., BioResNet50, FusResNet34, and FusResNet50. The overview of three proposed models is shown in Figure <ref type="figure" coords="4,333.86,649.79,3.74,10.91" target="#fig_1">2</ref>.</p><p>The BioResNet50 model with the ResNet50 backbone receives bioclimatic rasters as the input. We use available 19 channels of bioclimatic rasters. The FusResNet34 model with three branches of the ResNet34 backbone is a multi-modal late fusion network, which combines bioclimatic rasters (19 channels), satellite imagery (3-channel RGB and 1-channel NIR), a human footprint raster of summary version (1 channel), and soil rasters (9 channels). In FusResNet34, we change the ResNet34 backbone to the ResNet50 one and obtain the FusResNet50 model. Those models are trained in an end-to-end manner.</p><p>We simply combine three proposed models by calculating the average output of them. Figure <ref type="figure" coords="5,501.01,181.81,4.97,10.91" target="#fig_2">3</ref> illustrates our ensemble method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Three-step training strategy</head><p>Training our CNN-based models simultaneously using both the PO and PA data (shown in Figure <ref type="figure" coords="5,120.48,258.64,3.95,10.91" target="#fig_3">4</ref>(a)) is not an effective method due to the negative impact of assuming negative labels of PO (shown in our experiment). We propose an efficient training strategy that can improve the performance using the samples with single positive labels (the PO data). Our method is a three-step training strategy as shown in Figure <ref type="figure" coords="5,301.31,299.28,15.59,10.91" target="#fig_3">4(b)</ref>.</p><p>First, we train our models on the PA data only, using the BCE loss, since PA has fully observed labels. This is a kind of warming-up step for multi-label prediction. Secondly, we continue to train the models obtained in the first step using only the PA data with the cross-entropy (CE) loss <ref type="bibr" coords="5,109.71,353.48,16.41,10.91" target="#b13">[14]</ref>. The CE loss is one of the most common losses for multi-class classification tasks, where the objective of this task is to obtain a single class per an input sample. In this step, we expect that our models can learn specific features for each class since PO has only single positive labels. Finally, we continue training the models obtained in the second step using only the PA data with the BCE loss. Here, the models were able to adjust their activation units for the multi-label prediction and utilize the specific features learned in the second step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>This section presents experiment settings and shows the effectiveness of our models and training strategy. In addition, we perform an ablation study for our proposed training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental settings</head><p>We divided the training and validation set as described in Table <ref type="table" coords="5,376.04,543.13,3.77,10.91" target="#tab_0">1</ref>. Our baseline model was to train a CNN-based model simultaneously using both PO and PA with the AN loss as described in Figure <ref type="figure" coords="5,131.96,570.23,15.06,10.91" target="#fig_3">4(a)</ref>.</p><p>We implemented our models using the PyTorch <ref type="bibr" coords="5,321.02,583.78,18.07,10.91" target="#b14">[15]</ref> framework. The detailed settings of our proposed models and the baseline are described in Table <ref type="table" coords="5,370.60,597.33,3.81,10.91" target="#tab_1">2</ref>. The pre-trained weights of ResNet-34 and ResNet-50 on ImageNet <ref type="bibr" coords="5,261.25,610.87,17.76,10.91" target="#b12">[13]</ref> were used to initialize the backbones of our models in the training phase of Step 1 and our baseline case. Due to resources limitation and the time-consuming of the training phase, we only trained our models by 10 and 20 epochs in Step 2 and the baseline, respectively. In steps 1 and 3, we stopped the training phase at epoch 30th due to time constraints, while the validation loss was slightly improved. However, the results could  be improved if we continue training the models in further epochs. In the inference step, given an input, the models will output the top-20 species with the highest probabilities for evaluation.</p><p>The GeoLifeCLEF 2023 competition used the micro F1-score (â†‘) for evaluation. The micro F1-score (denoted as MF1-score) is calculated as follows:</p><formula xml:id="formula_3" coords="6,213.96,591.44,292.68,33.71">ğ¹ 1 = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 ğ‘‡ ğ‘ƒ ğ‘— ğ‘‡ ğ‘ƒ ğ‘— + (ğ¹ ğ‘ƒ ğ‘— + ğ¹ ğ‘ ğ‘— )/2<label>(3)</label></formula><p>where ğ‘‡ ğ‘ƒ ğ‘— , ğ¹ ğ‘ƒ ğ‘— , and ğ¹ ğ‘ ğ‘— are the true positive, the false positive, and the false negative of the ğ‘—-th input sample, respectively. ğ‘ is the number of samples for evaluation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison among proposed models</head><p>This section presents a comparison among our proposed models and the baseline method which is simultaneously trained on PO and PA using the AN loss. We only implemented the baseline BioResNet50 (denoted as BioResNet50-base) to compare with our BioResNet50 trained by our training strategy. In addition, we tried to train the FusResNet50 in step 2 by 20 epochs (denoted as FusResNet50*) to observe the performance. Table <ref type="table" coords="7,129.93,625.39,5.17,10.91" target="#tab_2">3</ref> shows our experimental results. Our BioResNet50 significantly outperforms BioResNet50-base, indicating the effectiveness of our three-step training strategy. Among our proposed models, the multi-modal FusResNet34 and FusResNet50 models achieve better performance than the BioResNet50. The MF1-score values of FusResNet50* are slightly lower  than those of FusResNet50, indicating that further training of the model in step 2 could not improve the performance. The combination of BioResNet50, FusResNet34, and FusResNet50 by the ensemble method achieves the best performance of 0.276 and 0.270 on the public and private testing sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation study for our proposed training strategy</head><p>We conducted an ablation study for our proposed training strategy using the FusResNet50 model. Table <ref type="table" coords="8,149.24,525.94,5.08,10.91" target="#tab_3">4</ref> presents the detailed results. Overall, applying the three-step training strategy achieves the best performance of around 0.25 on both public and private testing sets. The performance of step 1 alone is around 0.2 on these two testing sets. Without step 3, step 2 alone and the combination of step 1 and step 2 achieve significantly lower performance. Without step 1, the performance of step 2 and step 3 alone is lower 0.03 MF1-core points than that of the combination of these three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This technical report presents working notes on the GeoLifeCLEF 2023 competition. For multilabel plant species prediction, we presented our proposed CNN-based models, i.e., BioResNet50, FusResNet34, and FusResNet50. In addition, we presented a three-step training strategy to improve the prediction performance from learning samples with single positive labels. Our experiments show that FusResNet34 and FusResNet50 achieved the best and comparable performance of around 0.25 on both public and private testing sets. Furthermore, we have also shown the effectiveness of our three-step training strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,219.15,321.30,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of RGB and NIR at (Latitude=43.153, Longtitude=6.080).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,366.99,224.82,8.93;6,126.16,84.19,342.96,270.24"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of proposed models' architectures.</figDesc><graphic coords="6,126.16,84.19,342.96,270.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,489.76,118.01,8.93;6,153.09,393.85,289.09,83.35"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ensemble method.</figDesc><graphic coords="6,153.09,393.85,289.09,83.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,243.48,158.86,8.93;7,124.68,84.19,345.93,146.73"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three-step training strategy.</figDesc><graphic coords="7,124.68,84.19,345.93,146.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,276.63,319.76,81.83"><head>Table 1</head><label>1</label><figDesc>Settings for training and validation sets in training phases.</figDesc><table coords="7,186.52,308.25,222.24,50.21"><row><cell>Subset</cell><cell cols="2">Step 1&amp;3 PO PA</cell><cell cols="2">Step 2 PO PA</cell><cell cols="2">Baseline PO PA</cell></row><row><cell>Training set</cell><cell>-</cell><cell cols="2">80% 98%</cell><cell>-</cell><cell cols="2">100% 80%</cell></row><row><cell>Validation set</cell><cell>-</cell><cell cols="2">20% 2%</cell><cell>-</cell><cell>0%</cell><cell>20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,380.72,334.43,129.65"><head>Table 2</head><label>2</label><figDesc>Hyper-parameters of our proposed models and the baseline model.</figDesc><table coords="7,171.85,412.33,251.57,98.03"><row><cell>Hyper-parameters</cell><cell>Step 1&amp;3</cell><cell cols="2">Step 2 Baseline</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>96</cell><cell>96</cell></row><row><cell>Optimizer</cell><cell>Adam [13]</cell><cell cols="2">Adam Adam</cell></row><row><cell>Learning rate (Lr)</cell><cell>0.001</cell><cell>0.003</cell><cell>0.003</cell></row><row><cell>Lr scheduler</cell><cell>MultiStep,</cell><cell>-</cell><cell>MultiStep,</cell></row><row><cell></cell><cell>epoch 15 and 20</cell><cell></cell><cell>epoch 15</cell></row><row><cell>Lr decay rate</cell><cell>0.1</cell><cell>-</cell><cell>0.1</cell></row><row><cell cols="2">Maximum #epochs 30</cell><cell>10</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,90.49,402.88,156.90"><head>Table 3</head><label>3</label><figDesc>Performance of proposed models on the testing set.</figDesc><table coords="8,103.40,121.83,388.47,125.55"><row><cell>Models</cell><cell cols="2">One-step training #Epochs in step 2</cell><cell cols="2">MF1-score â†‘ Public Private</cell></row><row><cell>BioResNet50-base</cell><cell></cell><cell>-</cell><cell>0.060</cell><cell>0.058</cell></row><row><cell>BioResNet50</cell><cell>-</cell><cell>10</cell><cell>0.243</cell><cell>0.239</cell></row><row><cell>FusResNet34</cell><cell>-</cell><cell>10</cell><cell>0.254</cell><cell>0.249</cell></row><row><cell>FusResNet50</cell><cell>-</cell><cell>10</cell><cell>0.254</cell><cell>0.249</cell></row><row><cell>FusResNet50*</cell><cell>-</cell><cell>20</cell><cell>0.248</cell><cell>0.242</cell></row><row><cell>Ensemble method: BioResNet50 + FusResNet34 + FusResNet50</cell><cell>-</cell><cell>-</cell><cell>0.276</cell><cell>0.270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.98,269.65,297.63,125.68"><head>Table 4</head><label>4</label><figDesc>Ablation study of FusResNet50 on the training strategy.</figDesc><table coords="8,208.66,300.99,177.95,94.33"><row><cell cols="2">Step 1 Step 2 Step 3</cell><cell cols="2">MF1-score â†‘ Public Private</cell></row><row><cell>-</cell><cell>-</cell><cell>0.200</cell><cell>0.196</cell></row><row><cell>-</cell><cell>-</cell><cell>0.059</cell><cell>0.058</cell></row><row><cell></cell><cell>-</cell><cell>0.073</cell><cell>0.073</cell></row><row><cell>-</cell><cell></cell><cell>0.226</cell><cell>0.221</cell></row><row><cell></cell><cell></cell><cell cols="2">0.254 0.249</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,92.57,671.04,120.51,8.97"><p>https://github.com/plantnet/GLC</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,199.79,393.33,10.91;9,112.66,213.34,393.98,10.91;9,112.66,226.89,38.81,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,217.65,199.79,288.33,10.91;9,112.66,213.34,221.76,10.91">Species distribution models: ecological explanation and prediction across space and time, Annual review of ecology</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Elith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Leathwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,342.17,213.34,120.72,10.91">evolution, and systematics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="677" to="697" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,240.44,394.62,10.91;9,112.66,253.99,393.33,10.91;9,112.66,267.54,220.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,374.22,240.44,133.06,10.91;9,112.66,253.99,171.09,10.91">Overview of geolifeclef 2018: location-based species recommendation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">P</forename><surname>Monestiez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,307.12,253.99,198.87,10.91;9,112.66,267.54,128.70,10.91">Working Notes of CLEF 2018-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,281.08,393.33,10.91;9,112.66,294.63,395.17,10.91;9,112.66,308.18,293.43,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,312.15,281.08,193.84,10.91;9,112.66,294.63,245.13,10.91">Overview of geolifeclef 2019: plant species prediction using environment and animal occurrences</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,383.45,294.63,124.38,10.91;9,112.66,308.18,201.59,10.91">CLEF 2019 Working Notes-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,321.73,393.33,10.91;9,112.66,335.28,308.96,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<title level="m" coord="9,449.49,321.73,56.49,10.91;9,112.66,335.28,224.13,10.91">Overview of lifeclef location-based species prediction task 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>geolifeclef</note>
</biblStruct>

<biblStruct coords="9,112.66,348.83,393.33,10.91;9,112.66,362.38,393.58,10.91;9,112.66,375.93,385.44,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,399.64,348.83,106.34,10.91;9,140.40,362.38,313.90,10.91">Predicting species distribution from 2 million remote sensing images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,481.58,362.38,24.66,10.91;9,112.66,375.93,225.57,10.91">CLEF 2021-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2936. 2021</date>
			<biblScope unit="page" from="1451" to="1462" />
		</imprint>
	</monogr>
	<note>Overview of geolifeclef</note>
</biblStruct>

<biblStruct coords="9,112.66,389.48,394.61,10.91;9,112.66,403.03,393.33,10.91;9,112.66,416.58,395.00,10.91;9,112.41,430.13,48.96,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,381.26,389.48,126.01,10.91;9,112.66,403.03,393.33,10.91;9,112.66,416.58,16.81,10.91">Overview of geolifeclef 2022: Predicting species presence from multi-modal remote sensing, bioclimatic and pedologic data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,152.04,416.58,248.92,10.91">CLEF 2022-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1940" to="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,443.67,395.17,10.91;9,112.66,457.22,393.33,10.91;9,112.66,470.77,393.33,10.91;9,112.66,484.32,107.76,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,412.10,443.67,95.73,10.91;9,112.66,457.22,393.33,10.91;9,112.66,470.77,101.46,10.91">Overview of GeoLife-CLEF 2023: Species presence prediction based on occurrences data and high-resolution remote sensing images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,239.14,470.77,266.84,10.91;9,112.66,484.32,77.06,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,497.87,394.53,10.91;9,112.66,511.42,394.53,10.91;9,112.66,524.97,393.33,10.91;9,112.66,538.52,395.17,10.91;9,112.66,552.07,394.52,10.91;9,112.33,565.62,329.01,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,112.66,524.97,393.33,10.91;9,112.66,538.52,135.53,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.76,538.52,236.08,10.91;9,112.66,552.07,389.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">September 18-23, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,579.17,393.32,10.91;9,112.66,592.72,393.32,10.91;9,112.66,606.27,193.87,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,394.81,579.17,111.17,10.91;9,112.66,592.72,91.42,10.91">Multi-label learning from single positive labels</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,226.77,592.72,279.21,10.91;9,112.66,606.27,105.89,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,619.81,395.17,10.91;9,112.66,633.36,393.33,10.91;9,112.66,646.91,394.53,10.91;9,112.66,660.46,55.16,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,331.21,619.81,176.62,10.91;9,112.66,633.36,172.88,10.91">Acknowledging the unknown for multilabel learning with single positive labels</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,307.21,633.36,198.78,10.91;9,112.66,646.91,48.13,10.91">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="volume">XXIV</biblScope>
			<biblScope unit="page" from="423" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,393.33,10.91;10,112.26,100.52,394.38,10.91;10,112.41,114.06,59.11,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,254.67,86.97,251.32,10.91;10,112.26,100.52,112.87,10.91">Label-aware global consistency for multi-label learning with single positive labels</title>
		<author>
			<persName coords=""><forename type="first">M.-K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,233.00,100.52,231.56,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18430" to="18441" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.26,141.16,393.72,10.91;10,112.66,154.71,172.05,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,263.52,127.61,242.46,10.91;10,112.26,141.16,79.25,10.91">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,214.21,141.16,291.77,10.91;10,112.66,154.71,84.28,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,395.17,10.91;10,112.66,181.81,395.01,10.91;10,112.41,195.36,38.81,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,259.74,168.26,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,488.38,168.26,19.45,10.91;10,112.66,181.81,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,395.17,10.91;10,112.66,222.46,122.01,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,160.66,208.91,79.89,10.91">Rational decisions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,248.86,208.91,258.97,10.91;10,112.66,222.46,38.07,10.91">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.53,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,350.57,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,261.26,249.56,244.72,10.91;10,112.66,263.11,67.64,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,188.20,263.11,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
