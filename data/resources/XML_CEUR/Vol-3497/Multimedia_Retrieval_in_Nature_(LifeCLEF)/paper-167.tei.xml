<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,378.57,15.42;1,89.29,106.66,226.97,15.42">Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale</title>
				<funder ref="#_cBPGkEx">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_jgYyhBs #_jCbZJkp">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,64.34,11.96"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier, Occitanie</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.28,134.97,67.32,11.96"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier, Occitanie</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.59,134.97,52.66,11.96"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,378.57,15.42;1,89.29,106.66,226.97,15.42">Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3FE363411545F4CDD857D03B683275B3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>fine-grained classification</term>
					<term>species identification</term>
					<term>biodiversity informatics</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The world is estimated to be home to over 300,000 species of vascular plants. In the face of the ongoing biodiversity crisis, expanding our understanding of these species is crucial for the advancement of human civilization, encompassing areas such as agriculture, construction, and pharmacopoeia. However, the labor-intensive process of plant identification undertaken by human experts poses a significant obstacle to the accumulation of new data and knowledge. Fortunately, recent advancements in automatic identification, particularly through the application of deep learning techniques, have shown promising progress. Despite challenges posed by data-related issues such as a vast number of classes, imbalanced class distribution, erroneous identifications, duplications, variable visual quality, and diverse visual contents (such as photos or herbarium sheets), deep learning approaches have reached a level of maturity which gives us hope that in the near future we will have an identification system capable of accurately identifying all plant species worldwide. The PlantCLEF2023 challenge aims to contribute to this pursuit by addressing a multi-image (and metadata) classification problem involving an extensive set of classes (80,000 plant species). This paper provides an overview of the challenge's resources and evaluations, summarizes the methods and systems employed by participating research groups, and presents an analysis of key findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The world is home to an estimated 300,000 species of vascular plants, and the discovery and description of new plant species continue to occur each year <ref type="bibr" coords="1,363.09,491.74,11.49,10.91" target="#b0">[1]</ref>. The remarkable diversity of plants has played a pivotal role in the advancement of human civilization, providing resources such as food, medicine, building materials, recreational opportunities, and genetic reservoirs <ref type="bibr" coords="1,89.29,532.39,11.47,10.91" target="#b1">[2]</ref>. Moreover, plant diversity plays a crucial role in maintaining the functioning and stability of ecosystems <ref type="bibr" coords="1,157.39,545.94,11.58,10.91" target="#b1">[2]</ref>. However, our understanding of plant species remains limited. For the majority of species, we lack knowledge about their specific roles within ecosystems and their potential utility to humans. Additionally, information regarding the geographic distribution and population abundance of most species remains scarce <ref type="bibr" coords="1,349.33,586.58,11.43,10.91" target="#b2">[3]</ref>.</p><p>Over the past two decades, the biodiversity informatics community has made significant efforts to develop global initiatives, digital platforms, and tools to facilitate the organization, sharing, visualization, and analysis of biodiversity data <ref type="bibr" coords="2,347.34,114.06,11.49,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,362.41,114.06,7.65,10.91" target="#b4">5]</ref>. Nonetheless, the process of systematic plant identification poses a significant obstacle to the aggregation of new data and knowledge at the species level. Botanists, taxonomists, and other plant experts spend substantial time and energy on species identification, which could be better utilized in analyzing the collected data.</p><p>As previously discussed by <ref type="bibr" coords="2,227.23,181.81,11.58,10.91" target="#b5">[6]</ref>, the routine identification of previously described species shares similarities with other human activities that have successfully undergone automation. In recent years, automated identification has made significant advancements, particularly due to the development of deep learning techniques, thanks to the rise of Convolutional Neural Networks (CNNs) <ref type="bibr" coords="2,169.22,236.01,11.28,10.91" target="#b6">[7]</ref>. The long-term evaluation of automated plant identification, conducted as part of the LifeCLEF initiative <ref type="bibr" coords="2,224.81,249.56,11.40,10.91" target="#b7">[8]</ref>, demonstrates the impact of CNNs on performance within a few years. In 2011, the best evaluated system achieved a mere 57% accuracy on a straightforward classification task involving only 71 species captured under highly uniform conditions (scans or photos of leaves on a white background). In contrast, by 2017, the best CNN achieved an 88.5% accuracy on a far more complex task encompassing 10,000 plant species, characterized by imbalanced, heterogeneous, and noisy visual data <ref type="bibr" coords="2,330.76,317.30,11.58,10.91" target="#b8">[9]</ref>. Moreover, in 2018, the best system outperformed five out of nine specialists in re-identifying a subset of test images <ref type="bibr" coords="2,450.57,330.85,16.25,10.91" target="#b9">[10]</ref>.</p><p>Existing plant identification applications, due to their growing popularity, present opportunities for high-throughput biodiversity monitoring and the accumulation of specific knowledge <ref type="bibr" coords="2,89.29,371.50,16.30,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,108.19,371.50,12.50,10.91" target="#b11">12,</ref><ref type="bibr" coords="2,123.29,371.50,12.23,10.91" target="#b12">13]</ref>. However, they often face the challenge of being restricted to specific regional floras or limited to the most common species. With an increasing number of species exhibiting a transcontinental range, such as naturalized alien species <ref type="bibr" coords="2,346.07,398.60,18.04,10.91" target="#b13">[14]</ref> or cultivated plants, relying on regional floras for identification becomes less reliable. Conversely, focusing solely on the most prevalent species disregards the broader implications for biodiversity.</p><p>To address these challenges, during two years of competition, the PlantCLEF 2022 and 2023 challenges introduced a multi-image (and metadata) classification problem involving an extensive number of classes, specifically 80,000 plant species. Convolutional Neural Networks (CNNs) and the recent Vision Transformers (ViTs) techniques emerge as the most promising solutions for tackling such large-scale image classification tasks. However, previous studies had not reported image classification results of this magnitude, regardless of whether the entities were biological or not. This paper presents the challenge's resources and evaluations, summarizes the approaches and systems employed by participating research groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>To thoroughly evaluate the aforementioned scenario on a large scale and in realistic conditions, two distinct training datasets were developed and shared: the "trusted" dataset and the "web" dataset. These datasets encompassed a total of 4 million images across 80,000 plant species, sourced from various origins. "Trusted" training set: this training dataset is based on a carefully curated selection of more than 2.9M images covering 80k plant species aggregated, shared and collected mainly by GBIF (Global Biodiversity Information Facility). This type of data is aggregated from academic sources such as museums, universities, and national institutions, as well as collaborative platforms like inaturalist, and Pl@ntNet, implying a fairly high certainty of determination quality. We initially formed an extensive dataset using the GBIF portal, which includes nearly 16 million occurrences of vascular plants (Tracheophyta) comprising ferns, conifers, and flowering modern plants <ref type="bibr" coords="3,202.46,168.26,16.30,10.91" target="#b14">[15]</ref>. This initial selection, however, exhibited significant imbalance, with some species having tens of thousands of images while others had only one. To ensure class equilibrium and prevent dataset inflation, we limited the number of images per species to approximately 100. The selected images focus on views that are optimal for plant identification, such as close-ups of flowers, fruits, leaves, and trunks. "Web" training set: in contrast, the "web" training dataset was compiled from a collection of web images obtained from search engines like Google and Bing. This initial collection contained millions of images, but it suffered from significant errors in species identification, a high presence of duplicate images, and a large number of images that were less suitable for visual plant identification, such as herbarium images, landscapes, microscopic views, and unrelated subjects. To address these issues, a semi-automatic revision process was conducted to minimize the number of irrelevant images and maximize the inclusion of close-ups of relevant plant features. The "web" dataset ultimately consisted of approximately 1.1 million images, covering around 57,000 plant species.</p><p>Test set: For the evaluation of the models, a separate test set was constructed using multi-image plant observations collected on the Pl@ntNet platform throughout 2021, ensuring that they were not present in the training datasets. Only observations with a high confidence score, determined through the collaborative review process on Pl@ntNet, were selected for the challenge, ensuring a high level of determination quality. The review process involved individuals with varying levels of expertise, ranging from beginners to world-leading experts, with different weights given to their judgments. The test set consisted of approximately 27,000 plant observations, comprising around 55,000 images related to approximately 7,300 plant species.</p><p>Table1 presents various statistics about the three datasets. One notable observation is the significant difference in the number of species between the training sets and the test set. This difference primarily stems from the challenge of collecting a large amount of expert-verified data from botanists on such a scale. However, this difference aligns with the realistic scenario faced by automatic identification systems like Pl@ntNet and Inaturalist. These systems need to be capable of recognizing a wide range of species without prior knowledge of which species will be frequently requested or completely overlooked. This characteristic reflects the goal of these systems to identify as many species as possible and adapt to unpredictable user requests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description</head><p>The challenge was hosted during two years as two rounds in the AICrowd plateform <ref type="foot" coords="4,458.99,210.62,3.71,7.97" target="#foot_0">1</ref> . The task was evaluated as a plant species retrieval task based on multi-image plant observations from the test set. The goal was to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. During the first year of competition in 2022, the participants had access to the training set in mid-February 2022, the test set was published 6 weeks later in early April, and the round of submissions was then open during 5 weeks. During the second round in 2023, the training and test data remained exactly the same (the ground truth on the test set being kept secret). The submission system remained open from mid-March to mid-May.</p><p>The metric used for the evaluation of the task is the Macro Average (by species) Mean Reciprocal Rank (MA-MRR). The Mean Reciprocal Rank (MRR) is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:</p><formula xml:id="formula_0" coords="4,247.75,412.32,255.03,33.71">𝑀 𝑅𝑅 = 1 𝑂 𝑂 ∑︁ 𝑖=1 1 rank 𝑖 (<label>1</label></formula><formula xml:id="formula_1" coords="4,502.78,423.22,3.86,10.91">)</formula><p>where 𝑂 is the total number of plant observations (query occurrences) in the test set and rank 𝑖 is the rank of the correct species of the plant observation 𝑖.</p><p>However, the Macro-Average version of the MRR (average MRR per species in the test set) was used because of the long tail of the data distribution to rebalance the results between underand over-represented species in the test set:</p><formula xml:id="formula_2" coords="4,214.52,548.56,288.27,35.12">𝑀 𝐴 -𝑀 𝑅𝑅 = 1 𝑆 𝑆 ∑︁ 𝑗=1 1 𝑂 𝑗 𝑂 𝑗 ∑︁ 𝑖=1 1 rank 𝑖 (<label>2</label></formula><formula xml:id="formula_3" coords="4,502.78,560.86,3.86,10.91">)</formula><p>where 𝑆 is the total number of species in the test set, 𝑂 𝑗 is the number of plant observations related to a species 𝑗.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Participants and methods</head><p>During the two years of the challenge, a total of 195 people expressed an interest in signing up for the challenge. Among this large raw audience, 8 research groups finally succeeded in submitting run files (8 the first year and 3 the second year). Details of the used methods and evaluated systems during the 2023 round are synthesized below and further developed in the working notes of the participants (Mingle Xu <ref type="bibr" coords="5,296.98,165.48,16.41,10.91" target="#b15">[16]</ref>, Neuon AI <ref type="bibr" coords="5,362.33,165.48,18.03,10.91" target="#b16">[17]</ref>. Table <ref type="table" coords="5,415.51,165.48,5.17,10.91">2</ref> reports the results while describing in various columns the main characteristics that distinguish each method from the others: type of architecture, training set used, pre-training method, taxonomic levels used. Complementary, the following paragraphs give a few more details about the methods and the overall strategy employed by each participant (the paragraphs are sorted in descending order of the best score obtained by each team).</p><p>Mingle Xu, South Korea, 9 runs, <ref type="bibr" coords="5,269.04,261.26,18.14,9.76" target="#b15">[16]</ref>: the team's work is founded on the utilization of a Vision Transformer (ViT) that has been pre-trained using a Self Supervised Learning (SSL) technique, which is a recent and increasingly popular approach in the field of computer vision. This approach is quite disruptive as it deviates from the traditional Supervised Transfer Learning (STL) method. Typically, in a STL approach, a neural network is initially trained from scratch using labeled data for a classification task on a generic dataset such as ImageNet 1k or 22k, and the network is then subsequently fine-tuned on a specific dataset that possesses a distinct set of labels. In contrast, Self Supervised Learning (SSL) methods operate without the need for labeled data. The premise is that a network pre-trained with an SSL method can extract superior features that exhibit improved generalization abilities. These extracted features can subsequently be fine-tuned in a supervised manner, enabling their effective utilization for diverse downstream tasks, including image classification and object detection. In contemporary times, the primary focus of state-of-the-art methods lies not in devising new architectures, as ViT has emerged as the default choice. Instead, the emphasis is on determining the optimal SSL approach for pre-training a ViT model. As an example, in the previous year, the team led by Mingle Xu achieved the best results using a pre-trained ViT-large model based on a Masked Auto-Encoder (MAE) approach <ref type="bibr" coords="5,233.25,477.11,16.41,10.91" target="#b17">[18]</ref>. They obtained a remarkable MA-MRR score of 0.64079 (post-challenge). The concept of MAE draws inspiration from the successful masked language modeling technique commonly used in Natural Language Processing, notably popularized by BERT <ref type="bibr" coords="5,117.21,517.76,16.19,10.91" target="#b18">[19]</ref>. The process of masking data was challenging to apply to CNN-based architectures, whereas it becomes relatively straightforward with vision transformers since they operate internally using visual patches or "tokens" along with positional embedding. MAE shares similarities with BEIT <ref type="bibr" coords="5,194.43,558.40,16.41,10.91" target="#b19">[20]</ref>, wherein the self-supervised task involves training a backbone vision transformer to predict missing tokens from partially masked images.</p><p>During this year's participation, Mingle Xu explored declinations of runs based on the visioncentric foundation model EVA <ref type="bibr" coords="5,225.91,599.05,16.20,10.91" target="#b20">[21]</ref>, that was the state-of-the-art position during the challenge in the first quarter of 2023. EVA is a pretraining strategy that combines CLIP <ref type="bibr" coords="5,424.25,612.60,19.73,10.91" target="#b21">[22]</ref> and MVP <ref type="bibr" coords="5,481.40,612.60,21.01,10.91" target="#b22">[23]</ref>. CLIP maximizes the relationships between paired text and images, while MVP integrates MAE and CLIP to enhance pretraining. MVP freezes the CLIP image encoder and trains the vision part using a loss function that minimizes the distance between frozen CLIP features and vision model features. EVA scales up MVP by using larger models and more datasets, resulting in improved performance across various tasks. Overall, EVA leverages multimodal information and scalability for better semantic learning.</p><p>Mingle Xu conducted an investigation into the finetuning of pre-trained EVA models using various approaches. This included species ablations with limited image data (runs 1, 2, 4, 6), augmenting the "trusted" training set with additional images from the "web" training set (runs 8, 9, 10), starting from a self-supervised learning (SSL)-only pre-trained model (run 3), or employing intermediate supervised finetuning on ImageNet 22k (all other runs). The best run (MingleXuRun 8) reached an impressive MA-MRR of 0.67395.</p><p>Neuon AI, Malaysia, 10 runs, <ref type="bibr" coords="6,247.82,209.85,18.14,9.76" target="#b16">[17]</ref>: this participant used various ensembles of models finetuned using most of the time all the training dataset available ("trusted" and "web") training datasets, mainly based on the Inception-ResNet-v2 architectures <ref type="bibr" coords="6,437.49,236.01,18.07,10.91" target="#b23">[24]</ref> (and on a Inception-v4 to a lesser extent). All the models are directly finetuned CNNs but as a multi-task classification related to five taxonomy levels (species, genus, family, order and "class" in the botanical sense), instead of the default species level. All the runs were finetuned . They then explored various ways to improve the performances: more data augmentation, a balanced batching method, a multi-organ and single-organ training scheme, and finally a feature embedding comparison instead of the traditional softmax function. The same data augmentation techniques were applied for all runs and included random cropping, horizontal flipping, color distortion, bi-cubic resizing, random hue and random contrast. The balanced batching method consisted to limit the selection of training images for a species in an epoch to a maximum of 16 samples in order to avoid any bias towards any particular species and preventing poor performance on underrepresented species (runs 2 and 4). A multi-organ training scheme was used for run 4: the approach involved training multiple models on smaller sub-datasets that exclusively consisted of images tagged with either the Flower, Bark, Fruit, Habit, or Leaf tag. The feature embedding comparison was used in runs 3, 6 and 8. It relies on calculating distances between test and training images, utilizing cosine similarity applied to the feature vectors of a single test image and all the training images. The feature embedding are directly the features extracted from the model before the fully-connected last layer. Distances scores are transformed into probabilities using Inverse Distance Weighting, allowing for class ranking, and the class with the highest probability is finally representing the most confident prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We report in Figure <ref type="figure" coords="6,181.84,552.07,5.17,10.91" target="#fig_0">1</ref> the performance achieved by the collected runs. Table <ref type="table" coords="6,441.35,552.07,5.17,10.91">2</ref> provides the results achieved by each run as well as a brief synthesis of the methods used in each of them.</p><p>ViT SSL are better than CNN STL: The most impressive outcomes were achieved by vision transformer-based approaches, particularly the vision-centric foundation model EVA <ref type="bibr" coords="6,89.29,619.81,16.41,10.91" target="#b20">[21]</ref>, that was the state-of-the-art position during the challenge in the first quarter of 2023. While CNN-based approaches also produced respectable results, with a maximum MA-MRR of 0.61813 (NeuonAIRun9), they still fell notably short of the highest score attained by an EVA approach. The best EVA approach, achieved a remarkable MA-MRR of 0.67395 (MingleXuRun8). holistic approach ensures a more promising understanding of plant biodiversity for effective monitoring and conservation efforts in the future. It is a reminder that comprehensive and inclusive datasets are essential for accurate and reliable analysis in the field of plant biodiversity.</p><p>Combining models dedicated to specific organs deteriorates results: Intuitively, one might say that it's interesting to specialize models on organ-based learning subsets, as botanists eventually learn to analyze organ structure and appearance independently and separately. However, a noteworthy observation is that one of the poorest results in the challenge emerged when combining models trained on specific organ sub-datasets (NeuonAIRun4 with a MA-MRR of 0.33926). A possible explanation for this outcome can be attributed to the loss of a significant number of species per organ. Statistics reported by the authors indicate that by focusing solely on fruits, for instance, the resulting dataset encompasses only approximately 27,000 species, significantly lower than the total 80,000 species available. This reduction in species coverage likely hampers the model's ability to generalize and accurately classify a broader range of plant organisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presented the overview and the results of the LifeCLEF 2023 plant identification challenge following the 12 previous ones conducted within CLEF evaluation forum. This year the task was performed for the second year on the biggest plant images dataset ever published in the literature. This dataset was composed of two distinct sources: a trusted set built from the GBIF and a noisy web dataset totaling both 4M images and covering 80k species.</p><p>The main conclusion of our evaluation is that vision transformers performed definitely better than convolutional neural networks, especially when this type of models are pre-trained with a Self-Supervised Learning. Furthermore, an important lesson we have learned is the significance of maximizing the number of images, including those obtained from the web, despite the possibility of errors. It is crucial not to limit the size of the dataset based on organ types or assume that a certain number of training images is too small to be included in the test set. By incorporating a larger and more diverse set of images, we enhance the model's ability to capture a wider range of plant variations and improve its overall performance.</p><p>However, training those models requires more computational resources that only participants with access to large computational clusters can afford. For instance, the winning team Mingle Xu indicate that, they need to use 16 RTX 3090 GPUs for almost three months for training all the models. We are aware that this is not fair to other teams who do not have enough GPUs, and that it considerably limits the participation of other teams. However, we hope that the challenge and results presented in this article will highlight future research directions for solving key species identification problems across all kingdoms and advancing AI in general for biodiversity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,353.42,416.69,8.93;8,99.71,84.19,395.86,256.66"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scores achieved by all systems evaluated within the plant identification task of LifeCLEF 2023</figDesc><graphic coords="8,99.71,84.19,395.86,256.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,399.97,73.22"><head>Table 1</head><label>1</label><figDesc>Statistics of the LifeCLEF 2023 Plant Identification Task: "n/s" means not specified</figDesc><table coords="4,103.83,118.58,385.14,45.13"><row><cell>Dataset</cell><cell cols="6">Images Observations Classes (species) Genera Families Orders</cell></row><row><cell cols="2">Train "trusted" 2,886,761</cell><cell>n/s</cell><cell>80,000</cell><cell>9,603</cell><cell>483</cell><cell>84</cell></row><row><cell cols="2">Train "web" 1,071,627</cell><cell>n/s</cell><cell>57,314</cell><cell>8,649</cell><cell>479</cell><cell>84</cell></row><row><cell>Test</cell><cell>55,307</cell><cell>26,869</cell><cell>7,339</cell><cell>2,527</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,670.95,218.21,8.97"><p>https://www.aicrowd.com/challenges/lifeclef-2022-23-plant</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgments</head><p>The research described in this paper was partly funded by the <rs type="funder">European Commission</rs> via the <rs type="projectName">GUARDEN</rs> and <rs type="projectName">MAMBO</rs> projects, which have received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon Europe research and innovation program</rs> under grant agreements <rs type="grantNumber">101060693</rs> and <rs type="grantNumber">101060639</rs>. The opinions expressed in this work are those of the authors and are not necessarily those of the GUARDEN or MAMBO partners or the <rs type="institution">European Commission</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cBPGkEx">
					<orgName type="project" subtype="full">GUARDEN</orgName>
				</org>
				<org type="funded-project" xml:id="_jgYyhBs">
					<idno type="grant-number">101060693</idno>
					<orgName type="project" subtype="full">MAMBO</orgName>
					<orgName type="program" subtype="full">Horizon Europe research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_jCbZJkp">
					<idno type="grant-number">101060639</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The noisy web training dataset helps: incorporating the comprehensive PlantCLEF training dataset, which includes both the trusted and web datasets, yielded notable benefits despite the extended training duration and the inherent residual noise present in the web dataset. The inclusion of the web training dataset led to a significant improvement in performance, as evidenced by the MA-MRR reaching 0.67395 (MingleXuRun8), surpassing the maximum of 0.65035 (MingleXuRun5) achieved without its incorporation. However, it's possible that the web dataset has been well curated and that the noise level isn't as high as one might think.</p><p>Species ablation was not relevant: the reduction of the training set by removing the classes with the fewest images (MingleXuRun1-4-2-6 vs 5) implies a significant drop in performance. This observation highlights a crucial point: the presence of a direct correlation between training and test data is not always guaranteed. It underlines the importance of including all classes, including those associated with uncommon species, to meet the challenge of monitoring plant biodiversity. By including a diverse range of classes, even those associated with less common species, we can better grasp the true extent and variability of plant life. This</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,461.65,393.32,10.91;9,112.66,475.20,210.15,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,252.32,461.65,253.66,10.91;9,112.66,475.20,67.68,10.91">The number of known plants species in the world and its annual increase</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Christenhusz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Byng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,188.58,475.20,45.23,10.91">Phytotaxa</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,488.75,395.17,10.91;9,112.66,502.30,394.53,10.91;9,112.66,515.85,22.69,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Bunker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hector</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Loreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Perrings</surname></persName>
		</author>
		<title level="m" coord="9,375.53,488.75,132.29,10.91;9,112.66,502.30,326.98,10.91">Biodiversity, ecosystem functioning, and human wellbeing: an ecological and economic perspective</title>
		<imprint>
			<publisher>OUP Oxford</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,529.40,294.72,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,159.47,529.40,117.29,10.91">Plant extinctions take time</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cronk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,285.12,529.40,33.25,10.91">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="446" to="447" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,542.95,394.53,10.91;9,112.66,556.50,393.33,10.91;9,112.66,570.05,324.43,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,312.74,556.50,193.25,10.91;9,112.66,570.05,173.85,10.91">The encyclopedia of life v2: providing global access to knowledge about life on earth</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Lans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Walley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Hammock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Goddard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Studer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,294.71,570.05,110.45,10.91">Biodiversity data journal</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,583.60,265.66,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,181.16,583.60,57.06,10.91">What if gbif?</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,246.26,583.60,48.12,10.91">BioScience</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="717" to="717" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,597.15,393.33,10.91;9,112.33,610.69,394.31,10.91;9,112.66,624.24,38.81,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,239.49,597.15,196.03,10.91">Automated species identification: why not?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,444.78,597.15,61.21,10.91;9,112.33,610.69,343.95,10.91">Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,637.79,330.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,255.76,637.79,60.92,10.91">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.99,637.79,28.99,10.91">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,651.34,394.53,10.91;9,112.66,664.89,393.33,10.91;10,112.66,86.97,393.32,10.91;10,112.66,100.52,221.54,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,264.60,664.89,241.39,10.91;10,112.66,86.97,226.12,10.91">Biodiversity information retrieval through large scale content-based identification: a long-term evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,361.61,86.97,144.37,10.91;10,112.66,100.52,90.51,10.91">Information Retrieval Evaluation in a Changing World</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="389" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,395.17,10.91;10,112.66,127.61,393.33,10.91;10,112.66,141.16,299.41,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,238.38,114.06,269.44,10.91;10,112.66,127.61,175.37,10.91">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,310.69,127.61,195.30,10.91;10,112.66,141.16,147.84,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2017</note>
</biblStruct>

<biblStruct coords="10,112.66,154.71,395.17,10.91;10,112.66,168.26,393.32,10.91;10,112.66,181.81,393.33,10.91;10,112.66,195.36,183.71,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,242.38,154.71,265.45,10.91;10,112.66,168.26,393.32,10.91;10,112.66,181.81,18.64,10.91">Overview of expertlifeclef 2018: how far automated identification systems are from the best experts? lifeclef experts vs. machine plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,177.97,181.81,328.02,10.91;10,112.66,195.36,26.38,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">2018. Sep. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2018</note>
</biblStruct>

<biblStruct coords="10,112.66,208.91,235.92,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,160.73,208.91,44.07,10.91">inaturalist</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,212.74,208.91,62.05,10.91">Science Scope</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,395.17,10.91;10,112.66,236.01,369.67,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,343.15,222.46,164.68,10.91;10,112.66,236.01,148.06,10.91">Automated plant species identification-trends and future directions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rzanny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,268.88,236.01,125.54,10.91">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1005993</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.66,263.11,380.01,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,381.43,249.56,124.56,10.91;10,112.66,263.11,58.79,10.91">Pl@ ntnet app in the era of deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.39,263.11,268.25,10.91">ICLR: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,394.52,10.91;10,112.66,290.20,395.01,10.91;10,112.66,303.75,83.56,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,236.70,290.20,224.96,10.91">The global naturalized alien flora (glonaf) database</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Kleunen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pyšek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kreft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pergl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Weigelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dullinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lenzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,470.10,290.20,32.88,10.91">Ecology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,394.04,10.91;10,112.66,330.85,346.69,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Occdownload</forename><surname>Gbif</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Org</surname></persName>
		</author>
		<idno type="DOI">10.15468/DL.EJ7KN5</idno>
		<ptr target="https://www.gbif.org/occurrence/download/0105549-210914110416597.doi:10.15468/DL.EJ7KN5" />
		<title level="m" coord="10,232.43,317.30,101.48,10.91">Occurrence download</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,393.33,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,328.66,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,321.01,344.40,184.98,10.91;10,112.66,357.95,332.72,10.91">A bigger training dataset contributes more than advanced pretraining methods for plant identification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,467.81,357.95,38.18,10.91;10,112.66,371.50,297.96,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Plantclef</note>
</biblStruct>

<biblStruct coords="10,112.66,385.05,393.33,10.91;10,112.66,398.60,393.33,10.91;10,112.66,412.15,124.68,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,256.94,385.05,249.05,10.91;10,112.66,398.60,103.93,10.91">Deep learning for large-scale plant classification: Neuon submission to plantclef</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,260.89,398.60,245.10,10.91;10,112.66,412.15,93.98,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,425.70,393.33,10.91;10,112.66,439.25,393.32,10.91;10,112.66,452.79,159.65,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,330.08,425.70,175.91,10.91;10,112.66,439.25,34.52,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,170.26,439.25,335.73,10.91;10,112.66,452.79,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.33,10.91;10,112.66,479.89,363.59,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,466.34,152.55,10.91;10,112.66,479.89,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,493.44,393.33,10.91;10,112.66,506.99,107.17,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="10,226.97,493.44,203.67,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,389.78,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07636</idno>
		<title level="m" coord="10,441.57,520.54,64.42,10.91;10,112.66,534.09,260.29,10.91">Eva: Exploring the limits of masked visual representation learning at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,394.53,10.91;10,112.66,561.19,393.33,10.91;10,112.66,574.74,395.01,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,237.25,561.19,268.73,10.91;10,112.66,574.74,48.76,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,183.64,574.74,197.17,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,394.61,10.91;10,112.66,601.84,318.34,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,283.85,588.29,204.01,10.91">Mvp: Multimodality-guided visual pre-training</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,601.84,187.84,10.91">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="337" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,615.39,393.33,10.91;10,112.66,628.93,337.28,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m" coord="10,307.29,615.39,198.70,10.91;10,112.66,628.93,155.17,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
