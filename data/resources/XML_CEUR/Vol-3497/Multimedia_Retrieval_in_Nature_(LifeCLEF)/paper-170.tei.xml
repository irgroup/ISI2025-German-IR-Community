<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.97,101.58,417.02,15.42;1,89.29,123.50,393.77,15.42">Joint Feature Learning of Image Data with Embedded Metadata to Leverage Snake Species Classification</title>
				<funder>
					<orgName type="full">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,151.81,83.24,11.96"><forename type="first">Benjamin</forename><surname>Bracke</surname></persName>
							<email>benjamin.bracke002@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.18,151.81,133.12,11.96"><forename type="first">Mohammadreza</forename><surname>Bagherifar</surname></persName>
							<email>mohammadreza.bagherifar002@stud.fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.95,151.81,62.76,11.96"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<email>louise.bloch@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence in Medicine (IKIM)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,165.76,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science (FHDO)</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<addrLine>Emil-Figge-Stra√üe 42</addrLine>
									<postCode>44227</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.97,101.58,417.02,15.42;1,89.29,123.50,393.77,15.42">Joint Feature Learning of Image Data with Embedded Metadata to Leverage Snake Species Classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D94981AA5F4B863C1F38686030027E52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake species identification</term>
					<term>multimodal model architecture</term>
					<term>joint feature learning</term>
					<term>metadata embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic identification of snake species from non-standard photos is an important task to improve medical treatment of snakebites. To address this problem, the SnakeCLEF 2023 competition provides a large data set of photos and metadata information for 1,784 snake species. This paper describes the FHDO Biomedical Computer Science Group's (BCSG) participation in this competition. Through a series of experiments investigating the effects of pre-trained feature extractors, image sizes, metadata integrations, class balance learning and multiple instance pooling methods, a proposed model architecture for joint feature learning of image data and embedded metadata is presented to improve classification of snake species. With this proposal, the best model achieved a macro ùêπ1-Score of 81.90 % and challenge-specific metrics of 90.09 % Track 1 and 1, 149 Track 2 on the challenge public test data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents the participation of the University of Applied Sciences and Arts Dortmund (FHDO) Biomedical Computer Science Group (BCSG) at the Conference of Labs of the Evaluation Forum (CLEF) 2023 1 SnakeCLEF <ref type="bibr" coords="1,238.82,485.29,13.00,10.91" target="#b0">[1]</ref> challenge 2 for snake species identification. The code to reproduce the participation is available online on the HuggingFace platform 3 .</p><p>The SnakeCLEF 2023 Challenge is one of five data-driven challenges of the LifeCLEF 2023 <ref type="bibr" coords="1,495.94,512.39,11.24,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,89.29,525.93,7.43,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,98.81,525.93,8.88,10.91" target="#b3">4]</ref> research platform focusing on automated species identification. Specifically, this year's challenge aims to provide data-driven analysis to improve snake species identification, with a focus on accurate identification of venomous and non-venomous snakes based on non-standardised photographs. As the annual mortality from snakebites is between 81,000 and 138,000 people <ref type="bibr" coords="2,492.15,103.81,11.28,10.91" target="#b4">[5]</ref>, identifying snake species could help to administer the correct antivenom <ref type="bibr" coords="2,418.74,117.35,12.91,10.91" target="#b5">[6]</ref> and thus reduce the number of victims.</p><p>Compared to the FHDO-BCSG team's participation in SnakeCLEF 2022 <ref type="bibr" coords="2,418.93,144.45,12.84,10.91" target="#b6">[7]</ref> and before <ref type="bibr" coords="2,479.94,144.45,13.15,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,495.83,144.45,7.57,10.91" target="#b8">9]</ref>, the previously proposed workflow for snake species identification based on object detection is abandoned and a new approach is introduced, which focuses more on multimodality to combine the provided image data with the also provided metadata for snake species identification.</p><p>The paper is structured as follows: In Section 2 the related work in this research area is described. Section 3 summarises the SnakeCLEF 2023 data set and Section 4 describes the new proposed method for snake species identification. Section 5 shows ablations studies as well as the results obtained using the proposed method. Finally, the results are summarised and concluded in Section 6 and Section 7 gives an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatic snake species classification has a long history, starting with classical ML models. For example, an approach based on manually extracted taxonomic features was implemented in <ref type="bibr" coords="2,101.58,338.57,18.07,10.91" target="#b9">[10]</ref> to distinguish between six species. However, manual feature extraction is a tedious task, so field-based approaches have been developed that collect unstructured photographs and extract textural <ref type="bibr" coords="2,160.26,365.67,17.91,10.91" target="#b10">[11]</ref> or deep learning features from snake images.</p><p>Recently, most of the published studies <ref type="bibr" coords="2,277.68,379.22,16.47,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,296.89,379.22,12.56,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,312.19,379.22,12.56,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,327.49,379.22,12.57,10.91" target="#b14">15,</ref><ref type="bibr" coords="2,342.80,379.22,12.57,10.91" target="#b15">16,</ref><ref type="bibr" coords="2,358.10,379.22,14.06,10.91" target="#b16">17]</ref> focus on deep learning-based approaches. Some of these studies were designed as object detection tasks.</p><p>For example, different deep learning-based object detection methods were compared to each other in <ref type="bibr" coords="2,128.23,419.87,16.41,10.91" target="#b11">[12]</ref>. The data set which was extracted from ImageNet-1k <ref type="bibr" coords="2,393.00,419.87,18.07,10.91" target="#b17">[18]</ref> and augmented by a Google Image search included 1,027 images of eleven Australian species. The least frequent class contained 60 images. The best mean Average Precision (mAP) was achieved for a Faster Region-Based Convolutional Neural Network (Faster RCNN) <ref type="bibr" coords="2,358.12,460.51,17.81,10.91" target="#b18">[19]</ref> with a ResNet <ref type="bibr" coords="2,442.92,460.51,17.81,10.91" target="#b19">[20]</ref> backbone.</p><p>A similar approach <ref type="bibr" coords="2,188.25,474.06,17.93,10.91" target="#b12">[13]</ref> used Faster RCNN with different detection layers. The data set collected from three data sources contained 250 images of nine species occurring on the Gal√°pagos Islands, Ecuador. To collect the data set, two internet searches were performed on the Google and Flickr platforms, and an image data set provided by the Ecuadorian Institution of Tropical Herping <ref type="foot" coords="2,126.91,526.50,3.71,7.97" target="#foot_0">4</ref> were accessed. Similar to the previously described method, the ResNet backbone achieved the best accuracy of 75 %.</p><p>Other studies have performed classification tasks. For example, the performances of three deep learning networks, namely VGG16 <ref type="bibr" coords="2,276.07,568.91,16.41,10.91" target="#b20">[21]</ref>, DenseNet161 <ref type="bibr" coords="2,363.21,568.91,16.41,10.91" target="#b21">[22]</ref>, and MobileNetV2 <ref type="bibr" coords="2,470.52,568.91,18.07,10.91" target="#b22">[23]</ref> are compared in <ref type="bibr" coords="2,147.12,582.46,16.19,10.91" target="#b13">[14]</ref>. The data set contained 3,050 images of 28 species. An accuracy of 72 % was achieved for the test data set and the DenseNet161 architecture.</p><p>A deep Siamese network <ref type="bibr" coords="2,211.43,609.55,17.79,10.91" target="#b23">[24]</ref> for one-shot learning was developed in <ref type="bibr" coords="2,406.21,609.55,16.13,10.91" target="#b24">[25]</ref>. The network was trained on 200 images from the World Health Organization (WHO) venomous snake database <ref type="foot" coords="2,501.09,621.35,3.71,7.97" target="#foot_1">5</ref> . This data set contained three to 16 images per class.</p><p>Although the previously described methods each examined less than 30 distinguishable species, more than 600 of the world's 3,700 snake species are medically relevant <ref type="bibr" coords="3,446.73,117.35,16.25,10.91" target="#b25">[26]</ref>.</p><p>The SnakeCLEF challenge <ref type="bibr" coords="3,217.53,130.90,16.35,10.91" target="#b26">[27,</ref><ref type="bibr" coords="3,236.60,130.90,13.98,10.91" target="#b25">26]</ref> overcomes this disadvantage by providing a more diverse data set with images of more than 1,000 species. It addresses the problems of high intra-class variance and low inter-class variance as well as the long-tail distribution of snake images. Since snake species strongly vary across countries, the data set also includes location metadata. Several deep learning approaches have been successfully submitted in previous rounds of this challenge.</p><p>In SnakeCLEF 2020 <ref type="bibr" coords="3,185.72,212.20,16.08,10.91" target="#b27">[28]</ref>, the winning approach <ref type="bibr" coords="3,305.93,212.20,17.76,10.91" target="#b28">[29]</ref> used a ResNet architecture pre-trained on ImageNet-21k <ref type="bibr" coords="3,153.32,225.75,17.76,10.91" target="#b29">[30]</ref> and achieved a macro-averaging ùêπ 1 -Score of 62.54 %. The FHDO-BCSG <ref type="bibr" coords="3,493.30,225.75,12.68,10.91" target="#b7">[8]</ref> combined object detection and image classification using a Mask-RCNN <ref type="bibr" coords="3,406.06,239.30,17.76,10.91" target="#b30">[31]</ref> instance detection framework and an EfficientNet-B4 <ref type="bibr" coords="3,249.47,252.85,18.07,10.91" target="#b31">[32]</ref> classification model. This method reached a macroaveraging ùêπ 1 -Score of 40.35 %. In post competition experiments, the score was optimized to 59.40 %.</p><p>The winning approach <ref type="bibr" coords="3,199.57,293.49,17.76,10.91" target="#b32">[33]</ref> of SnakeCLEF 2021 combined object detection with an EfficientDet-D1 <ref type="bibr" coords="3,104.83,307.04,17.97,10.91" target="#b33">[34]</ref> model, and an EfficientNet-B0 classifier as well as likelihood weighting to fuse image and location information. The best model reached a macro-averaging ùêπ 1 -Score of 90.30 %.</p><p>Experiments with several Convolutional Neural Network (CNN) architectures were presented in <ref type="bibr" coords="3,101.59,347.69,16.41,10.91" target="#b34">[35]</ref>. The best ùêπ 1 -Score of 83.00 % was obtained for an ensemble model combining two ResNeSt <ref type="bibr" coords="3,127.39,361.24,17.76,10.91" target="#b35">[36]</ref> models with a ResNet <ref type="bibr" coords="3,242.34,361.24,16.09,10.91" target="#b19">[20]</ref>, and a ResNeXt <ref type="bibr" coords="3,329.73,361.24,17.75,10.91" target="#b36">[37]</ref> model. The ensemble was generated by a majority voting of the top 1 predictions of the individual models.</p><p>The FHDO-BCSG <ref type="bibr" coords="3,179.44,388.34,12.68,10.91" target="#b8">[9]</ref> expanded the SnakeCLEF 2020 workflow by combining object detection with EfficientNets and Vision Transformers (ViTs) <ref type="bibr" coords="3,325.24,401.89,16.41,10.91" target="#b37">[38]</ref>. The best model was an ensemble averaging the model predictions of an EfficientNet-B4 model and ViTs. This submission obtained a macro-averaging ùêπ 1 -Score of 78.75 %.</p><p>After the challenge, the organizers published an approach <ref type="bibr" coords="3,353.43,442.54,17.76,10.91" target="#b16">[17]</ref> that was trained on a subset of the challenge data set and evaluated on the official test set. In the work, ViTs were trained using a two-step approach. First, the model is trained with cross entropy loss on the training data set. Second, the resulting model was fine-tuned with focal loss <ref type="bibr" coords="3,358.58,483.18,18.06,10.91" target="#b38">[39]</ref> to improve performance for rare species. The model achieved a macro-averaging ùêπ 1 -Score of 92.20 %.</p><p>In the SnakeCLEF 2022 <ref type="bibr" coords="3,204.20,510.28,17.80,10.91" target="#b26">[27]</ref> challenge, most teams focused on the combination of image and metadata as well as strategies that solve the problem of long-tail distributions. The approach <ref type="bibr" coords="3,488.22,523.83,17.76,10.91" target="#b39">[40]</ref> that produced the best results was an ensemble of different model architectures, namely Con-vNeXt <ref type="bibr" coords="3,120.64,550.93,16.34,10.91" target="#b40">[41]</ref>, VOLO <ref type="bibr" coords="3,174.66,550.93,16.34,10.91" target="#b41">[42]</ref>, CoLKANet <ref type="bibr" coords="3,249.88,550.93,16.34,10.91" target="#b39">[40]</ref>, SwinTransformer <ref type="bibr" coords="3,354.94,550.93,16.34,10.91" target="#b42">[43]</ref>, and ViT. The CoLKANet is a newly developed architecture that combines large kernel attention layers and self attention layers. The model performance was improved by different strategies to enhance the robustness, e.g. the use of TrivialAugment <ref type="bibr" coords="3,223.04,591.58,16.09,10.91" target="#b43">[44]</ref>, Test-time Augmentation (TTA) <ref type="bibr" coords="3,383.00,591.58,16.30,10.91" target="#b44">[45,</ref><ref type="bibr" coords="3,401.63,591.58,12.23,10.91" target="#b45">46]</ref>, pseudo labelling for rare classes, or Exponential Moving Average (EMA). The ensemble achieved an macro averaging ùêπ 1 -Score of 85.40 % on the private test set.</p><p>The second place team <ref type="bibr" coords="3,209.88,632.22,18.07,10.91" target="#b46">[47]</ref> trained models based on the ViT architecture. An effective logit adjustment loss (ELAL) <ref type="bibr" coords="3,225.68,645.77,18.07,10.91" target="#b46">[47]</ref> which combines the logit adjustment loss <ref type="bibr" coords="3,444.92,645.77,18.06,10.91" target="#b47">[48]</ref> with the class-balanced loss <ref type="bibr" coords="3,177.01,659.32,18.04,10.91" target="#b48">[49]</ref> was developed to increase the relative margin between logits of rare and common labels. This loss improved the classification, especially for the rare classes. The final model was an ensemble of two ViT-L models and one ViT-H model and reached a macro ùêπ 1 -Score of 84.57 % for the private test set.</p><p>The third place of the challenge and a macro ùêπ 1 -Score of 82.65 % for the private test set was reached by <ref type="bibr" coords="4,139.90,144.45,16.13,10.91" target="#b49">[50]</ref>. The approach combines supervised and unsupervised training on the training, validation, and test sets using the Simple Framework for Contrastive Learning (SimCLR) <ref type="bibr" coords="4,488.01,158.00,17.97,10.91" target="#b50">[51]</ref> with the MetaFormer <ref type="bibr" coords="4,185.47,171.55,17.82,10.91" target="#b51">[52]</ref> architecture that combines image data and metadata, TTA, and logit adjustments to reduce the impact of the long-tailed class distribution. The final model is an ensemble combining seven MetaFormer models trained for different epochs and with different hyperparameters.</p><p>During the participation in the previous SnakeCLEF challenge, the FHDO-BCSG <ref type="bibr" coords="4,451.25,225.75,12.68,10.91" target="#b6">[7]</ref> extended their previous workflow by using object detection with YOLOv5 <ref type="bibr" coords="4,382.80,239.30,16.41,10.91" target="#b52">[53]</ref>, feature concatenation, and multiplication with prior probabilities. The final ensemble that combines seven models with different architectures (EfficienNet, EfficientNet-v2 <ref type="bibr" coords="4,337.86,266.40,16.14,10.91" target="#b53">[54]</ref>, and ConvNeXt) reached a macro-ùêπ 1 -Score of 70.80 % on the private test set.</p><p>In this work, the previously developed workflow <ref type="bibr" coords="4,320.76,293.49,11.40,10.91" target="#b6">[7,</ref><ref type="bibr" coords="4,334.88,293.49,7.49,10.91" target="#b7">8,</ref><ref type="bibr" coords="4,345.09,293.49,8.98,10.91" target="#b8">9]</ref> has been completely revised. The new workflow which is presented in this paper, focuses more on training of multimodal models that combine image data with tabular metadata. The distribution of images per snake species is highly imbalanced. The most frequent species was the Natrix natrix containing 2,079 images in the training and validation sets. For six species only three images were collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SnakeCLEF 2023 Data Set</head><p>In addition to the photographs, metadata that provides information about the country (code), and if the species is endemic (endemic) is available. The data was collected in 214 countries with Mexico ("MX") being the most frequent country (21,002 images; 10.70 %). For 9,730 images (4.96 %) no information about the code was available. <ref type="bibr" coords="4,342.22,596.01,49.21,10.91">29,198 (14.</ref>87 %) of the images show endemic snakes. An additional table is available that contains information if the species is venomous or not. 285 (15.97 %) species in the data set are venomous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>The participation of FHDO-BCSG team in SnakeCLEF 2022 <ref type="bibr" coords="5,350.39,128.12,12.71,10.91" target="#b6">[7]</ref> and before <ref type="bibr" coords="5,410.61,128.12,12.96,10.91" target="#b7">[8,</ref><ref type="bibr" coords="5,426.29,128.12,8.89,10.91" target="#b8">9]</ref> focused heavily on an object detection based snake species identification workflow where the image data was first cropped to a specific region of interest where the snake is pictured and subsequently classified. The proposed method for this year's participation abandons this workflow and focuses more on a multimodal model that combines the provided image data with the also provided metadata for snake species identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ConvNeXt</head><p>For feature extraction from image data, the proposed method relies on highly optimised and pre-trained CNNs. Specifically, it uses a ConvNeXt V2 <ref type="bibr" coords="5,334.02,259.14,17.95,10.91" target="#b54">[55]</ref> base model with 89M. parameters. The ConvNeXt architecture <ref type="bibr" coords="5,211.54,272.69,17.76,10.91" target="#b40">[41]</ref> is a state-of-the-art approach to modernising the most standard CNN architecture (ResNet50) towards the design choices of the popular Vision Transformers models. Therefore, the authors of ConvNeXt conducted several experiments to discover the key components that lead to the performance differences. A key component was changing the multi-stage macro design of the architecture to reduce the stage computation ratio and changing the stem to a simpler "patchify" layer similar to ViT <ref type="bibr" coords="5,326.88,340.44,16.41,10.91" target="#b40">[41]</ref>. Other changes included the use of inverted bottleneck blocks with depth-wise convolution, a larger kernel size, and an increased network width to the same number of channels as the Swin-Transformer <ref type="bibr" coords="5,415.36,367.53,16.22,10.91" target="#b40">[41]</ref>. ConvNeXt also adopted some features of the micro-scale architecture of transformers, such as replacing the Rectified Linear Units (ReLU) activation function with its smoother Gaussian Error Linear Unit (GELU) <ref type="bibr" coords="5,125.06,408.18,18.03,10.91" target="#b55">[56]</ref> variant, using fewer normalization layers, and replacing BatchNorm layers with simple Layer-Normalization <ref type="bibr" coords="5,215.30,421.73,16.15,10.91" target="#b40">[41]</ref>. Other performance differences resulted from similar training techniques as for ViT, e.g., the use of the AdamW <ref type="bibr" coords="5,319.62,435.28,18.07,10.91" target="#b56">[57]</ref> optimizer, extended training epochs, heavy data augmentation including CutMix <ref type="bibr" coords="5,282.57,448.83,16.09,10.91" target="#b57">[58]</ref>, RandAugment <ref type="bibr" coords="5,370.89,448.83,16.09,10.91" target="#b58">[59]</ref>, Random erasing <ref type="bibr" coords="5,467.10,448.83,16.08,10.91" target="#b59">[60]</ref>, and label smoothing <ref type="bibr" coords="5,165.19,462.38,16.42,10.91" target="#b40">[41]</ref>. Further improvements to the ConvNeXt architecture have been made by <ref type="bibr" coords="5,102.40,475.93,17.76,10.91" target="#b54">[55]</ref> by adding Global Response Normalisation (GRN) layers to improve inter-channel feature competition, as well as using self-supervised learning techniques such as masked autoencoders. This co-design of architectural improvements and self-supervised learning techniques results in the so-called ConvNeXt V2 model family, which further improves the performance of pure ConvNets. There are different ConvNeXt V2 variants T/S/B/L, which differ only in the number of channels and the number of blocks in each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Leveraging Metadata Information</head><p>As mentioned in Section 3, the data set contains additional metadata including region and endemic information for most of the images. The idea is to use the metadata information as additional features to leverage model performance in snake species identification. This requires a multimodal model architecture that can handle both image data and structural metadata. Classifier: 1x Linear Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Snake Species classification</head><p>Metadata: regional prior distribution of snake species snake region prior distribution 3. Weight predictions with regional prior distributions of snake species Figure 1: Schematic representation of the proposed method, which incorporates regional metadata by multiplying it with regional prior probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Multiplication with Regional Prior Probabilities</head><p>A successful approach in recent years of the snake challenge has been to multiply the snake species prediction probabilities of the image classification model by the regional prior probabilities of the snake species, as visualised in Figure <ref type="figure" coords="6,294.16,358.42,3.66,10.91">1</ref>. This requires estimating the prior probabilities from the relative frequency distribution of observations per snake species and region of the training data set. Two strategies can be used to combine the region and image information. First, the raw regional prior probabilities can be multiplied by the snake species prediction probabilities of the image classification model. The second strategy is to multiply by a binarised version of the prior probabilities so that it acts as a filter mask, filtering out those snake species predictions of the model that do not occur in that region with respect to a given region code in the training data set. For images with missing regional information, as well as for regions not available in the training data set, the prior probability of the "unknown" class can be used.</p><p>Unfortunately, this approach only applies to the regional metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Joint Feature Learning with Embedded Metadata</head><p>A more advanced method of utilising metadata is to use them as additional features alongside the image data and let the model learn how to incorporate them into the identification of snake species, as visualised in Figure <ref type="figure" coords="6,233.33,556.79,3.81,10.91" target="#fig_1">2</ref>. This raises the problem of how to represent the discrete, nominal metadata in such a way that the model can utilise them as features, since neural network models usually assume numerical, continuous values as input.</p><p>Embedding layers , commonly used in natural language processing (NLP) tasks for word embedding, is the proposed solution to embed the metadata as a learnable numerical vector representation. Embedding layers (in particular, the nn.Embedding  tokens (e.g., words or country codes) to continuous embedding vector representations in a highdimensional space. To do this, they internally map index values provided by a fixed vocabulary of input tokens to a weight matrix of learnable parameters. During training, the parameters of this weight matrix are updated using backpropagation to minimise the loss function and optimise model performance. With this approach, the representation of the embedding vector of a given input token is ideally optimised to best represent the meaning or context of the input token with respect to the specific training task of the model. In order to embed the provided metadata of the SnakeCLEF 2023 data set, a fixed vocabulary of input tokens must be defined that maps the unique and alphabetically sorted country codes to integer index values, i.e. {"DE": 0, "US": 1 . . . "unknown": 212}. Similarly, a vocabulary of input tokens for the endemic metadata {false: 0, true: 1} must be defined. During training, these predefined tokens are used as input to the embedding layers, which learn a numerical vector representation (embedding) of dimensions 64 for each code and endemic token (manually and heuristically defined so that the combined metadata dimensions are about 1/10 of the image feature dimensions).</p><p>Metadata Model is used to concatenate the individual embedding representations of code and endemic metadata and to learn a joint representation of both metadata resources. It is a small neural network consisting of two linear layers, both with 128 dimensions, with GELU activation <ref type="bibr" coords="7,136.30,590.47,17.91,10.91" target="#b55">[56]</ref> and layer normalisation <ref type="bibr" coords="7,265.83,590.47,16.25,10.91" target="#b60">[61]</ref>, connected by a drop-out layer. <ref type="bibr" coords="7,248.34,619.23,16.41,10.91" target="#b61">[62]</ref>, is used to concatenate the features of the metadata model with the features of the image feature extractor model. Subsequently, the joint feature representation is passed to a single linear layer of dimension 1784 with softmax activation, which serves as the classifier to predict the snake species. To prevent overfitting, the classifier model is preceded by a drop-out layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Fusion approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Instance Learning (MIL) Methods</head><p>As described in Section 3, some of the snake observations in the data set may contain more than one image, which makes it necessary to combine the model predictions into one common prediction per observation. This problem is called Multi-Instance Learning (MIL) and there are two different ways to aggregate the results for the instances of the model (MIL pooling): the instance-level approach and the bag-level approach <ref type="bibr" coords="8,336.62,407.66,16.21,10.91" target="#b62">[63]</ref>. The main difference between the instance-level approach and the bag-level approach is the operational space of MIL-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Instance-Level MIL Pooling</head><p>In the instance-level approach <ref type="bibr" coords="8,220.31,470.39,16.08,10.91" target="#b62">[63]</ref>, the model architecture requires an instance-level classifier, i.e. the prediction probabilities of the snake species are provided for each instance. The subsequent MIL pooling combines all prediction probabilities of the snake species for each instance into a global prediction. The proposed method implements mean MIL pooling, which simply averages the prediction probabilities of the classifier output for the snake species across all instances. Another method is weighted average MIL pooling, where the contribution of each instance is weighted differently when averaging the prediction probabilities for the snake species. The normalised values of the highest prediction probability of the model for a snake type are used as the weighting of the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Bag-Level MIL Pooling</head><p>The bag-level approach <ref type="bibr" coords="8,198.91,641.52,18.06,10.91" target="#b62">[63]</ref> works with the feature space (embeddings) and MIL pooling is used to aggregate the embeddings of all instances to obtain a global embedding representation (bag embedding). This bag-level embedding is subsequently classified by a bag-level classifier, and a joint prediction of the snake species is obtained including all instances.</p><p>The proposed method implements the bag-level approach with an attention model for MIL pooling, as visualised in Figure <ref type="figure" coords="9,230.16,117.35,3.75,10.91" target="#fig_2">3</ref>. Attention MIL pooling <ref type="bibr" coords="9,344.56,117.35,17.93,10.91" target="#b62">[63]</ref> is more flexible than, e.g., mean MIL pooling and can adapt the pooling to the task and the provided data and provides better interpretability. Essentially, attention MIL pooling applies a weighted average MIL pooling to the feature space of all instances, with the weights being determined by a small learnable neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Test Time Augmentation</head><p>The proposed method uses Test Time Augmentation (TTA) <ref type="bibr" coords="9,352.63,221.28,16.37,10.91" target="#b44">[45,</ref><ref type="bibr" coords="9,371.75,221.28,13.99,10.91" target="#b45">46]</ref> to make model predictions more robust, where multiple augmented versions of an image are presented to the model during inference. The TTA augmentation pipeline consists of resizing the image to 1.25 times the input size of the image feature extraction model and then using the FiveCrops augmentation method to obtain five different cropped representations of the same image, i.e. crops from the four corners and a central crop. As with multi-instance learning, the model predictions for each crop must then be aggregated into a common prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Class Imbalance Learning Methods</head><p>As mentioned in Section 3, the distribution of images per snake species in the data set is highly imbalanced. Several approaches have been developed in the past to improve deep learning on imbalanced data sets, i.e using specialised loss functions with class imbalance weighting techniques that are used in the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Focal Loss</head><p>Focal Loss <ref type="bibr" coords="9,137.04,442.67,17.76,10.91" target="#b38">[39]</ref> is such a specialised loss function that applies a dynamic scaling term (1 -ùëù ùë° ) ùõæ to the standard cross-entropy loss function. The scaling factor decays to zero as the confidence of the correctly predicted class ùëù ùë° &gt; 0.5 increases, which automatically down-weights the contribution of easily classifiable examples during training. Vice-versa, the scaling factor increases as the confidence of the correctly predicted class ùëù ùë° &lt; 0.5 decreases, shifting the focus more towards the hard to classify samples during training. With the hyperparameter ùõæ &gt; 0 the strength of the scaling term can be set exponentially.</p><p>In addition, the focal loss can also be combined with a class weighting factor that differently weights the contribution of classes to the loss, allowing to handle the class imbalance problem. Different loss weighting techniques are tried to handle class imbalance, such as inverse class frequency and the so-called effective number of samples technique <ref type="bibr" coords="9,388.10,578.17,16.23,10.91" target="#b48">[49]</ref>. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1 -ùõΩ ùëõ )/(1 -ùõΩ) (its inverse defines the loss weighting term), with ùëõ the number of samples per snake species and ùõΩ ‚àà [0, 1) being a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">ArcFace Loss</head><p>ArcFace loss or Additive Angular Margin Loss <ref type="bibr" coords="9,302.41,668.54,18.07,10.91" target="#b63">[64]</ref> focuses on learning more discriminative features to enforce higher similarity for intra-class samples and greater diversity for inter-class  <ref type="foot" coords="10,263.40,308.13,3.24,6.21" target="#foot_3">8</ref>5</p><p>samples, i.e. to generate margin as Support Vector Machines do. It addresses the problem of softmax-based loss functions, such as focal loss, which do not explicitly optimise feature embeddings, resulting in a performance gap for large intra-class variation. ArcFace loss achieves this by learning a projection of features that distributes them on a circular shape, the hypersphere, so that the prediction depends only on the angle between them. Adding a margin to the angle as a penalty increases the distance between classes with similar feature embeddings. The authors introduced the ArcFace loss for deep face recognition and found that it could maximise the decision boundaries between similar looking faces, making it easier to distinguish them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>In order to test the influence of the different proposed methods and come up with a final solution, several ablation studies were conducted, which are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">General Experiment Conditions</head><p>All experiments are implemented in Python 3.8.13 using several Python packages, but mainly the Python library timm 0.9.2 <ref type="bibr" coords="10,228.07,575.66,18.07,10.91" target="#b64">[65]</ref> for pre-trained image classification models and PyTorch 1.13 <ref type="bibr" coords="10,110.73,589.21,18.07,10.91" target="#b65">[66]</ref> for training and inference. All experiments use similar base hyperparameters as summarised in Table <ref type="table" coords="10,185.81,602.76,3.79,10.91" target="#tab_3">1</ref>, while experiment-specific hyperparameters will be mentioned in the following subsections. In order to achieve high batch sizes, mixed precision <ref type="bibr" coords="10,428.76,616.31,17.95,10.91" target="#b66">[67]</ref> and gradient accumulation are used. The full-sized version of the SnakeCLEF 2023 data set (training + additional) is used for training. Since some images are missing from the full-sized version of the training data set, a total of 332 images of the smaller "medium" version is added to the training data set. However, a total of 85 images are still missing.</p><p>All models are trained with the same image augmentation pipeline, which consists of first resizing the image to 1.25 times (at smallest image size, keeping the aspect ratio the same) the target model input sizes, then randomly flipping the image vertically and horizontally with probability 0.5. Subsequently, a random squared crop of the target model input sizes is performed, followed by RandAugment <ref type="bibr" coords="11,259.48,185.10,17.76,10.91" target="#b58">[59]</ref> with ùëõ = 2 and ùëö = 7, and normalising the images with ImageNet means (0.485, 0.456, 0.406) and standard deviations (0.229, 0.224, 0.225).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">General Validation Conditions</head><p>Validation is performed on the full-sized version of the SnakeCLEF 2023 validation data set using an EMA model (exponential moving average of model parameters) that is updated at each training step with a decay rate of 0.9998.</p><p>Also for validation, TTA is used, i.e. each image is first scaled to 1.25 times (at smallest image size, keeping the aspect ratio the same) the target model input sizes, followed by five square crops of the target model input sizes (4 corner crops, 1 centre crop). Unless otherwise stated, the class predictions of the model for each TTA instance are pooled by mean MIL pooling to obtain a combined snake species prediction of the model. The same applies to combining class predictions of the model for multiple images of the same snake observation. This also means that all metrics for the validation results are gathered on the mean MIL pooled data.</p><p>Multiple metrics are taken into account for evaluating the experiment results. First the macro-ùêπ 1 -Score as well as two custom metrics provided by the challenge organizers, namely "Challenge Track 2" Equation 2 and "Challenge Track 1" Equation <ref type="formula" coords="11,383.93,410.97,3.74,10.91" target="#formula_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùêø(ùë¶, ùë¶</head><formula xml:id="formula_0" coords="11,118.54,445.59,388.10,110.43">^) = ‚éß ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é® ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é© 0 if ùë¶ = ùë¶ 1 if ùë¶ Ã∏ = ùë¶ ^and ùëù(ùë¶) = 0 and ùëù(ùë¶ ^) = 0 2 if ùë¶ Ã∏ = ùë¶ ^and ùëù(ùë¶) = 0 and ùëù(ùë¶ ^) = 1 2 if ùë¶ Ã∏ = ùë¶ ^and ùëù(ùë¶) = 1 and ùëù(ùë¶ ^) = 1 5 if ùë¶ Ã∏ = ùë¶ ^and ùëù(ùë¶) = 1 and ùëù(ùë¶ ^) = 0 (1) L = ‚àëÔ∏Å ùëñ ùêø (ùë¶ ùëñ , ùë¶ ^ùëñ)<label>(2)</label></formula><formula xml:id="formula_1" coords="11,116.61,560.28,390.03,28.57">ùëÄ = (ùë§ 1 ùêπ 1 + ùë§ 2 (100 -ùëÉ 1 ) + ùë§ 3 (100 -ùëÉ 2 ) + ùë§ 4 (100 -ùëÉ 3 ) + ùë§ 5 (100 -ùëÉ 4 )) ‚àëÔ∏Ä 5 ùëñ ùë§ ùëñ<label>(3)</label></formula><p>The motivation of "Challenge Track 2" is to penalise misclassifications of venomous species with harmless ones, but not vice versa, as based on this prediction a possible anti-venom might not be injected to the victim. Therefore, different weights for misclassifications are defined in Equation <ref type="formula" coords="11,181.59,639.38,3.81,10.91">1</ref>, where ùëù(ùë†) = 1 if species ùë† is venomous, otherwise ùëù(ùë†) = 0, as well as ùë¶ for ground truth species and ùë¶ ^for predicted species. The metric "Challenge Track 1" is a weighted average of the overall macro ùêπ 1 -Score and the weighted accuracies of different types of snake species confusion, where ùë§ 1 = 1, ùë§ 2 = 1, ùë§ 3 = 2, ùë§ 4 = 5, ùë§ 5 = 2 are the weights of individual misclassifications as in Equation <ref type="formula" coords="12,337.47,233.07,3.81,10.91">1</ref>. ùêπ 1 is the macro ùêπ 1 -Score, ùëÉ 1 is the percentage of harmless species misclassified as another harmless species, ùëÉ 2 is the percentage of harmless species misclassified as another venomous species, ùëÉ 3 is the percentage of venomous species misclassified as another harmless species, and ùëÉ 4 is the percentage of venomous species misclassified as another venomous species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment: iNaturalist21 Pre-Training</head><p>First, the influence of different pre-trained CNN feature extractor models fine-tuned with the SnakeCLEF 2023 data set is observed. For this purpose, a ConvNeXt V2 base model pre-trained with ImageNet21k ("convnextv2_base.fcmae_ft_in22k_in1k_384") from the Python library timm 0.9.2 <ref type="bibr" coords="12,111.63,377.65,17.76,10.91" target="#b64">[65]</ref> is compared with a pre-trained iNaturalist21 <ref type="bibr" coords="12,328.77,377.65,17.76,10.91" target="#b67">[68]</ref> ConvNeXt V2 base model (provided by ourselves). The pre-training was performed on the iNaturalist21 data set for 10 epochs with an image size of 384 √ó 384 px and normal cross entropy as loss function. Fine-tuning for both models was then performed for 30 epochs with an image size of 384 √ó 384 px and normal cross entropy as the loss function. In order to ensure fairness to other challenge participants, the model weights were published during the course of the challenge <ref type="foot" coords="12,379.79,443.64,3.71,7.97" target="#foot_4">9</ref> .</p><p>The obtained validation results for this experiment are summarised in Table <ref type="table" coords="12,432.45,458.94,3.66,10.91" target="#tab_4">2</ref>. The validation results show that pre-training on the iNaturalist21 data set leads to an improved downstream snake species classification of about 2.4 % points macro ùêπ 1 -Score when fine-tuned with the challenge data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiment: Influence of Image Size</head><p>The second experiment focuses on the influence of different image sizes on the snake species classification result. Specifically, the pre-trained iNaturalist21 model fine-tuned in the previous experiment was further tuned for 10 more epochs using image sizes of 464 √ó 464 px, 544 √ó 544 px, and 624 √ó 624 px.</p><p>The validation results obtained for this experiment are summarised in Table <ref type="table" coords="12,432.45,603.51,3.66,10.91" target="#tab_5">3</ref>. The validation results show that increased image sizes generally improve the snake species classification result. The biggest gain is seen between image sizes of 384 px and 464 px, of about 1.8 % points macro ùêπ 1 -Score. However, this comes at the cost of increased training time per epoch. Based on these results, it was investigated whether it would be useful to fine-tune a new model from "the beginning", using the good performing image size of 544 px (taking into account the training time and the improvement in classification results). Therefore, the pre-trained iNaturalist21 model was fine-tuned for 40 epochs (for reasons of comparability, as the effective fine-tuning epochs of the previous results were 30 epochs of the first experiment + 10 epochs of the second experiment) with the selected image size of 544 px and normal cross entropy as the loss function.</p><p>The validation results obtained (Table <ref type="table" coords="13,270.53,392.17,4.15,10.91" target="#tab_5">3</ref>) show that this approach could further improve the snake species classification result by about 2.4 % points macro ùêπ 1 -Score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experiment: Leveraging Classification Results with Metadata</head><p>The third experiment focuses on the use of the provided metadata, such as endemic and regional code information, in addition to the image data to improve snake species classification. Specifically, the influences of the approaches described in the previous Section 4.2 are compared, such as multiplying model class prediction distributions with regional prior probabilities as well as joint feature learning using embedded endemic and regional code metadata.</p><p>As the first approach does not require any re-training, the obtained model from the second experiment is used, but its class prediction outputs are weighted with the previously defined regional prior probabilities of snake species. This approach leverages the snake species classification by about 9.3 % points macro ùêπ 1 -Score compared to the same model of the second experiment (Table <ref type="table" coords="13,172.72,577.39,4.16,10.91" target="#tab_6">4</ref>) without regional prior probability weighting.</p><p>As the model architecture of the second approach differs with embedding layers, metadata model and classifier of the joint feature modalities, a new model needs to be trained for 40 epochs using the selected image size of 544 px from second experiment and normal cross entropy as loss function. This approach also leverages the snake species classification by about 10 % points macro ùêπ 1 -Score (Table <ref type="table" coords="13,220.16,645.14,4.08,10.91" target="#tab_6">4</ref>) and even marginally outperforms the first mentioned approach of metadata incorporation. Thus, this approach is continued in the following experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Experiment: Class Imbalance Learning</head><p>The fourth experiment focuses on the strong class imbalance present in the challenge data set and investigates whether the use of specialised loss functions with class balance weighting terms, mentioned in Section 4.5, can improve snake species classification. Specifically, the influences of using focal loss with weak focal value ùõæ = 0. The validation results obtained (Table <ref type="table" coords="14,265.26,434.73,4.08,10.91" target="#tab_8">5</ref>) show that using focal loss with different class balance weighting terms has only a marginal effect on snake species classification results. In general, a weak focal value ùõæ = 0.5 slightly outperforms a strong focal value ùõæ = 2.0, and using the "effective number of samples" formula as class balance term slightly outperforms the inverse class frequency class balance term. However, the best combination of focal loss with ùõæ = 0.5 and "effective number of samples" as the class balance term only improves the snake species classification results by about 1.1 % points macro ùêπ 1 -Score compared to normal cross entropy.</p><p>In another experiment, the best combination of focal loss and class weighting term is then combined with the ArcFace loss, which should force the model to learn a better feature representation of the intermediate feature embedding before the classifier. As before, the new model is fine-tuned for 25 epochs under the combined influence of the aforementioned loss functions, using the parameter checkpoint of epoch 15 of the joint feature learning model from the previous experiment as weight initialisation.</p><p>Validation results (Table <ref type="table" coords="14,215.84,610.87,4.25,10.91" target="#tab_8">5</ref>) show that this approach further improves the snake species classification result by a substantial amount of about 3.1 % points macro ùêπ 1 -Score compared to normal cross entropy, making it the best performing model of all experiments, which will be continued in the following experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Experiment: Influence of MIL-Pooling Operators</head><p>The fifth experiment focuses on the influence of different MIL pooling methods, either for pooling the model predictions for different TTA instances of the same image, or for multiple image instances of the same snake observation in the data set. As mentioned earlier in this section, in the previous experiments simple mean MIL pooling was used for aggregating TTA instances and image instances of snake observations. This experiment investigates the influence of using different MIL-Pooling methods described in Section 4.3 on the snake species classification results. For the investigated classical MIL pooling methods, no re-training of the model is required, so the best model from the previous experiment is used.</p><p>The validation results (Table <ref type="table" coords="15,231.13,487.17,4.21,10.91" target="#tab_10">6</ref>) show that applying different combinations of classical MIL pooling operators has only a marginal impact on the snake species classification results, as indicated by the macro ùêπ 1 -Score. However, these results show that the combination of using the weighted average MIL pooling operator for pooling TTA instances and using a simple mean MIL pooling operator for different snake instances of the same observation gives the best results when it comes to more costly errors, as indicated by the "Challenge Track 2" metric.</p><p>Furthermore, two different approaches of attention-based MIL pooling are investigated. For both approaches, the best performing model from the previous experiment is further fine-tuned for 10 epochs with an attention module integrated into the model architecture as described in Section 4.3.2. In the first approach, attention is used to pool TTA instances in combination with a classical mean MIL pooling for instances containing the same snake observations. The difference with the second approach is that attention is used to pool both TTA instances and instances of the same snake observations in one step. Due to the VRAM limitations of the GPU, this requires limiting the total number instances, i.e. a maximum of 100 instances are used.</p><p>The validation results (Table <ref type="table" coords="15,228.78,676.85,4.13,10.91" target="#tab_10">6</ref>) show that both attention-based MIL pooling approaches can  For the example on the top row, attention pooling helped to guide the classifier to more relevant instances where the actual snake is well represented, making the global prediction correct and prevents misclassification. For the example on the bottom row, attention pooling is similar to mean MIL pooling where all TTA instances represent the snake well. reduce the more costly errors, as indicated by the "Challenge Track 2" metric, making these two approach the best performing models so far. Figure <ref type="figure" coords="17,336.34,560.19,5.00,10.91" target="#fig_4">4</ref> shows two examples of the influence of attention-based pooling of TTA instances of the model. For the example in the top row, attention pooling helped to guide the classifier to the more relevant instances where the actual snake is well represented, making the combined prediction from all TTA instances correct and thus preventing misclassification. For the example in the bottom row, attention-based pooling is similar to mean MIL pooling where all TTA instances represent the snake well. Figure <ref type="figure" coords="17,500.81,627.94,5.17,10.91" target="#fig_3">5</ref> shows the influence of attention-based pooling of TTA instances + instances of the same snake observation. In this example, the attention module also helped to guide the classifier to the more relevant image instances where the actual snake is well represented and reduces the influence of the "junk" instances such as the road images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Final Models + Results on Testdata</head><p>As a result of these experiments, a total of three final models were submitted. All models share the same joint feature extractor with embedded metadata trained using the best performing class imbalance learning method of the fourth experiment (Focal loss ùõæ = 0.5 with "effective num. of samples" class weighting term + ArcFace loss). For submission, this model was further fine-tuned for 20 epochs using the combined training and validation data sets of the challenge. The final models differ only in the way the TTA instances and instances of the same snake observations are pooled, as mentioned in the previous experiment. For the submission models that contain attention-based pooling, the attention modules as well as the classifiers of the models were also retrained for 10 epochs using the combined training and validation data sets of the challenge.</p><p>The following Table <ref type="table" coords="18,191.49,537.97,5.03,10.91" target="#tab_11">7</ref> shows the results of the final models on the public challenge test data set compared to the submitted models further fine-tuned on the challenge training + validation data sets. In general, the test results show that further fine-tuning with the training + validation data sets could only improve the snake species classification results very marginally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.">Error Analysis in Detail</head><p>This section presents a comprehensive error analysis of a model based on the method proposed in section 4.2.2. It should be noted that the model used for this analysis is none of the submitted models, but is very similar to the Model  + mean MIL pooling of image instances with the same observation ID). However, due to the similarity of the models, this analysis should still be useful to evaluate and identify shortcomings and limitations of the proposed method.</p><p>In general the used model achieved an accuracy score of approximately 83 % by correctly predicting 11,807 images from the validation data set. However, it misclassified 2,305 images, indicating a considerable error rate. Furthermore, the analysis focused on rare snake species classes in the validation data set, defined as classes with fewer than 10 samples, totaling 1,115 classes. Figure <ref type="figure" coords="19,157.58,460.37,5.17,10.91" target="#fig_7">6</ref> represents the relationship between the number of training images and the corresponding ùêπ 1 -Score per species class and it indicates that a considerable number of classes have a relatively small number of training images. The figure suggests that increasing the number of training images for each species has a positive impact on the ùêπ 1 -Score. When the number of training images is higher, the model's performance, as measured by the ùêπ 1 -Score, tends to improve. The fact that most of the plot is in the upper part of the graph and falls within the range of 0.6 to 1 indicates that the model achieves relatively high accuracy and precision in identifying and classifying the species under consideration. Species exhibiting an ùêπ 1 -Score below 0.5 are categorized as rare classes. However, there are certain extremely rare classes that have achieved a remarkable ùêπ 1 -Score of 1.0. This pattern suggests that sufficient training data is crucial for achieving accurate and reliable results. The results revealed that approximately 21 % of the errors were attributed to these rare classes. However, the rare classes also exhibited good performance, accounting for 42 % of the overall correct predictions. This finding suggests that although rare classes may contribute to errors, their presence does not necessarily indicate poor model performance.</p><p>To gain deeper insights into the incorrect predictions, a manual examination was conducted on 500 randomly chosen misclassified images. One common reason was attributed to low quality images (137 images, about 27 % of the whole dataset), which exhibited issues such as low resolution, compression artifacts, or blurred areas that could have adversely affected the accuracy of the model's predictions (Figure <ref type="figure" coords="20,239.43,457.79,7.71,10.91" target="#fig_8">7a</ref>). Another prevalent reason was the presence of a complex background in some images (87 images, about 17 % of the whole dataset), where visually cluttered backgrounds or multiple objects made it challenging for the model to accurately identify the snake object of interest (mostly the trees, branches and leaves) (Figure <ref type="figure" coords="20,414.48,498.44,8.21,10.91" target="#fig_8">7b</ref>). Additionally, in certain cases, the snake's colors were almost the same as the background, resulting in the snake blending in with the surroundings, making it difficult for the model to distinguish it accurately (87 images, about 17 % of the whole dataset) (Figure <ref type="figure" coords="20,337.57,539.09,7.85,10.91" target="#fig_8">7c</ref>). Furthermore, poor lighting was identified as a factor in some incorrect predictions (62 images, about 12 % of the whole dataset), where images had uneven lighting, shadows, or overexposure, which could have impacted the model's ability to accurately detect the snake (Figure <ref type="figure" coords="20,326.25,579.74,8.09,10.91" target="#fig_8">7d</ref>). While these reasons accounted for a sizable portion of the incorrect predictions, there were also cases where no specific reason could be identified for the incorrect prediction (39 images, about 8 % of the whole dataset) (Figure <ref type="figure" coords="20,89.04,620.38,7.78,10.91" target="#fig_8">7e</ref>). Another notable observation from the analysis is that approximately 21 % of the incorrect predictions (106 images) were attributed to the snake not being completely captured within the image. In some cases, the head of the snake was not visible in the image, while in other cases, the body was missing. This indicates that the partial presence or absence of the snake within the image played a notable role in contributing to the incorrect predictions made by the model (Figure <ref type="figure" coords="21,170.76,103.81,7.47,10.91" target="#fig_8">7f</ref>). The incomplete presence of the snake in the image can pose challenges for the model in accurately detecting and classifying the snake object, as the model may rely on the complete representation of the snake's features, such as head shape, body pattern, and other visual cues, to make accurate predictions. When crucial parts of the snake are missing in the image, the model's ability to correctly classify the snake can be compromised. In the analysis of the dataset, it was also observed that some images had multiple reasons for incorrect predictions. For example, some images exhibited both low quality and poor lighting issues simultaneously (24 images, about 5 % of the whole dataset), while others had both complex background and color similarity with the background concerns (14 images, about 3 % of the whole dataset). This suggests that certain images may have had multiple factors contributing to the incorrect predictions, making it more challenging for the model to accurately classify the snake object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work presents a multimodal deep learning based model for snake species identification using image data in combination with additional metadata, including regional and endemic information. The presented model architecture allows joint feature learning of both modalities, obtained from a ConvNeXt V2 base image feature extractor CNN and a custom model that embeds the provided metadata, in an intermediate feature concatenation approach.</p><p>Subsequent ablation studies have investigated the influence of selected hyperparameters as well as deep learning techniques to further improve the proposed method. The results of the ablation studies showed, that pre-training on large fine-grained data sets, such as iNaturalist21, as well as using large image sizes could improve the downstream fine-tuning of the image feature extractor CNN. Furthermore, the results showed that the identification of snake species can be leveraged when additional metadata is considered. The proposed joint feature model proved to be a good approach that could even outperform other approaches considered in previous SnakeCLEF challenges, like weighting model outputs with regional prior distributions of snake species. The problem of the highly class imbalanced challenge data set was addressed by using the focal loss function with class balance re-weighting terms. The obtained results showed that this approach only marginally affected the snake species identification performance. The considered ArcFace loss that directly optimise the joint modality feature embedding to enforce higher similarity for intra-class samples and greater diversity for inter-class samples, proved to be a much better approach to improve snake species identification performance. Further refinements in terms of less costly snake species identification errors were achieved by integrating learnable attention-based MIL pooling over classical non-trainable operators into the model architecture to pool both TTA instances as well as instances of the same snake observation. Increasing the size of the training data set by including the validation data set only improved snake species identification very slightly, resulting in the best performing model achieving a macro ùêπ 1 -Score of 81.9 % and challenge-specific metrics of 95.09 % Track 1 and 1149 Track 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Further Research</head><p>The proposed method offers several possibilities for further research. These generally include experiments of different learning techniques with better optimised hyperparameters or adjustments to the proposed model architecture (i.e. metadata model size or embedding layer sizes) that were not explicitly considered in the conducted ablation studies.</p><p>A more specific approach for future work would be a two-stage snake species classification process that first predicts whether the provided snakes in the image instances belong to a venomous or non-venomous species, as this information is also provided by the challenge data set. This 'venomous' information could then also be embedded as additional metadata using the proposed model architecture to predict the actual snake species. This approach may be able to further reduce the challenge specific metrics as they explicitly account for the confusion between venomous and non-venomous species.</p><p>Since the considered ArcFace loss performed well by enforcing higher embedding similarity for intra-class samples and higher diversity for inter-class samples, it may be worthwhile for future work to test deep learning methods that follow a similar direction. These would include unsupervised pre-training methods using contrastive loss techniques such as in SimCLR <ref type="bibr" coords="22,486.81,317.81,16.30,10.91" target="#b50">[51]</ref>, or simpler approaches such as Bootstrap Your Own Latent (BYOL) <ref type="bibr" coords="22,386.34,331.36,16.25,10.91" target="#b68">[69]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,284.04,416.69,8.93;7,89.29,296.05,104.28,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic representation of the proposed method for joint feature learning of image data and embedded metadata.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,283.96,416.69,8.93;8,89.29,295.97,418.22,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic representation of the proposed method with joint feature learning of embedded metadata as well as attention-based bag-level MIL pooled image data of all TTA-, observation instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,358.21,339.89,148.05,10.91;14,89.29,353.44,416.69,10.91;14,89.29,366.99,416.70,10.91;14,89.29,380.54,416.70,10.91;14,89.29,394.09,416.90,10.91;14,89.29,407.64,416.69,10.91;14,89.29,421.19,27.65,10.91"><head>5</head><label>5</label><figDesc>and strong focal value ùõæ = 2.0 in combination with different class balance weighting terms obtained from the inverse class frequency or "effective number of samples" formula are investigated. A parameter checkpoint from epoch 15 of the joint feature learning model from the previous experiment (trained with normal cross-entropy) is used as weight initialisation. The model is then fine-tuned further for 25 epochs under the influence of the mentioned focal loss function and class balance weighting terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,89.29,561.06,416.69,8.93;16,89.29,573.06,416.69,8.87;16,89.29,585.02,416.69,8.87;16,88.93,596.97,375.42,8.87;16,89.29,608.93,416.69,8.87;16,89.29,620.88,416.69,8.87;16,89.29,632.84,416.70,8.87;16,88.93,644.79,201.87,8.87;16,90.13,334.71,412.53,218.92"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prediction examples of a model with attention-based pooling of TTA instances. "instance pred. " refers to the model prediction for each individual image instance. "Global model prediction" refers to the overall model prediction obtained after pooling of TTA instances. "attention weight" refers to the weight that the attention module assigns to the specific instances for TTA instance pooling.For the example on the top row, attention pooling helped to guide the classifier to more relevant instances where the actual snake is well represented, making the global prediction correct and prevents misclassification. For the example on the bottom row, attention pooling is similar to mean MIL pooling where all TTA instances represent the snake well.</figDesc><graphic coords="16,90.13,334.71,412.53,218.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="17,89.29,401.44,416.69,8.93;17,89.29,413.45,416.69,8.87;17,89.29,425.40,416.69,8.87;17,89.29,437.36,416.69,8.87;17,89.29,449.31,416.87,8.87;17,89.29,461.27,416.69,8.87;17,89.29,473.22,125.54,8.87;17,89.29,485.18,416.69,8.87;17,88.93,497.13,417.05,8.87;17,89.29,509.09,52.04,8.87;17,90.13,101.03,412.53,292.99"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Prediction examples of a model with attention-based pooling of TTA instances + multiple instances of the same snake observation. "Instance prediction" refers to the model prediction for each individual image instance. "Global model prediction" refers to the overall model prediction obtained after pooling of TTA instances and instances of the same snake observation. "Attention weight" refers to the weight that the integrated attention module of the model assigns to the specific instances for instance pooling. This value allows to interpret the influence of the individual instances on the overall global prediction of the model.In this example, the attention module helped to guide the classifier to the more relevant image instances where the actual snake is well represented and reduces the influence of the "junk" instances such as the road images.</figDesc><graphic coords="17,90.13,101.03,412.53,292.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="18,273.35,655.44,232.64,10.91;19,187.56,282.99,4.60,13.68;19,212.04,282.99,13.80,13.68;19,241.12,282.99,13.80,13.68;19,270.20,282.99,132.42,13.68;19,251.65,292.88,97.28,13.68;19,162.63,268.03,11.50,13.68;19,162.63,239.42,11.50,13.68;19,162.63,210.82,11.50,13.68;19,162.63,182.22,11.50,13.68;19,162.63,153.62,11.50,13.68;19,162.63,125.02,11.50,13.68;19,147.32,188.86,13.68,31.64"><head></head><label></label><figDesc>1. mentioned previously (w. avg. TTA MIL pooling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="19,89.29,315.75,416.69,9.65;19,89.29,327.98,73.26,8.87"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of ùêπ 1 -Score per snake species class in relation to the number of training images per snake species.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="20,89.29,381.14,416.69,8.93;20,89.29,393.14,142.89,8.87;20,90.13,101.03,412.53,273.52"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Difficult snake image examples from the validation dataset that were misclassified by the model assuming the reasons given.</figDesc><graphic coords="20,90.13,101.03,412.53,273.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.97,379.22,418.70,132.85"><head></head><label></label><figDesc>The SnakeCLEF 2023 data set included 196,332 images of 111,215 observations and 1,784 species. The training data set contains 168,144 (85.64 %) images of 95,588 (85.95 %) observations and consists of two data sources. The first one originates from the iNaturalist platform6 and includes</figDesc><table coords="4,88.97,419.87,417.23,92.21"><row><cell>154,301 (91.77 % of training data set) images of 85,843 (89.81 % of training data set) observations</cell></row><row><cell>and 1,784 (100.00 %) species. To add images of rare species, additionally, data from Herpmapper</cell></row><row><cell>is added to the training data set. This data source includes 13,843 (8.23 % of training data</cell></row><row><cell>set) images of 9,745 (10,19 % of training data set) observations and 889 (49,83 %) species. The</cell></row><row><cell>validation data set includes 14,117 (7.19 %) images of 7,816 (7.03 %) observations and 1,599</cell></row><row><cell>(89.63 %) species. The remaining 14,071 (7.17 %) images of 7,811 (7.02 %) observations were used</cell></row><row><cell>as a test set.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,89.29,637.99,416.69,26.21"><head></head><label></label><figDesc>7 provided by the PyTorch framework) generally serve as lookup tables to learn a mapping from arbitrary discrete input</figDesc><table coords="7,166.22,103.32,256.43,169.19"><row><cell cols="3">1. Extract image features</cell><cell cols="4">3. Conatenate features</cell><cell cols="2">4. Snake Species</cell></row><row><cell></cell><cell>with CNN</cell><cell></cell><cell cols="3">of both modalites</cell><cell></cell><cell cols="2">classification</cell></row><row><cell cols="2">2. Embed metadata</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Code Token ÔøΩ Image Data 3x544x544 1x64 Metadata: Endemic Token ÔøΩ 1x64</cell><cell>Model ConvNeXt V2 Base Metadata</cell><cell cols="2">1x1024 1x128</cell><cell>ConcatenaÔøΩon</cell><cell>1x1152</cell><cell>Classifier: 1x Linear Layer</cell><cell>1x1784 softmax-activation</cell><cell>Snake species predictions</cell></row><row><cell cols="2">Learned embedding representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.99,107.32,302.05,211.33"><head>Table 1</head><label>1</label><figDesc>Base configuration of hyperparameters for all experiments.</figDesc><table coords="10,198.26,138.89,192.79,179.76"><row><cell>Hyperparameter</cell><cell>Base Config</cell></row><row><cell>batch size</cell><cell>128</cell></row><row><cell>optimizer</cell><cell>AdamW ùõΩ 1 = 0.9, ùõΩ 2 = 0.999</cell></row><row><cell cols="2">CNN feature extractor</cell></row><row><cell>lr</cell><cell>1e-5</cell></row><row><cell>layer-wise lr decay</cell><cell>0.85</cell></row><row><cell>stochastic depth</cell><cell>0.2</cell></row><row><cell>weight decay</cell><cell>1e-8</cell></row><row><cell cols="2">classifier/ embedding layer/ attention module</cell></row><row><cell>lr</cell><cell>1e-4</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>warmup epochs</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,88.97,107.32,417.01,99.04"><head>Table 2</head><label>2</label><figDesc>Validation results of the first experiment comparing different pre-trained image feature extraction CNNs (ImageNet21k vs. iNaturalist21), which were subsequently fine-tuned with the challenge data set.</figDesc><table coords="12,134.98,150.85,325.31,55.52"><row><cell>Pre-Train</cell><cell></cell><cell>Validation Metrics</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Macro ùêπ 1 -Score Challenge Track 1 Challenge Track 2</cell></row><row><cell>ImageNet21k</cell><cell>59.00 %</cell><cell>87.87 %</cell><cell>2786</cell></row><row><cell>iNaturalist21</cell><cell>61.39 %</cell><cell>88.46 %</cell><cell>2612</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,88.97,107.32,416.64,163.29"><head>Table 3</head><label>3</label><figDesc>Validation results of the second experiment comparing different fine-tuning image sizes.</figDesc><table coords="13,89.66,137.29,415.95,133.33"><row><cell>Model Input</cell><cell>Training Time</cell><cell>10</cell><cell>Validation Metrics</cell><cell></cell></row><row><cell>Image Size</cell><cell>per Epoch</cell><cell cols="3">Macro ùêπ 1 -Score Challenge Track 1 Challenge Track 2</cell></row><row><cell></cell><cell cols="3">model of experiment 1 fine-tuned + 10 epochs</cell><cell></cell></row><row><cell>384 √ó 384 px</cell><cell>3700 sec.</cell><cell>61.39 %</cell><cell>88.46 %</cell><cell>2612</cell></row><row><cell>464 √ó 464 px</cell><cell>5300 sec.</cell><cell>63.18 %</cell><cell>89.47 %</cell><cell>2380</cell></row><row><cell>544 √ó 544 px</cell><cell>7400 sec.</cell><cell>63.51 %</cell><cell>89.70 %</cell><cell>2330</cell></row><row><cell>624 √ó 624 px</cell><cell>9700 sec.</cell><cell>64.20 %</cell><cell>89.75 %</cell><cell>2307</cell></row><row><cell></cell><cell></cell><cell cols="2">new model trained for 40 epochs</cell><cell></cell></row><row><cell>544 √ó 544 px</cell><cell>7400 sec.</cell><cell>65.95 %</cell><cell>90.17 %</cell><cell>2197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,88.97,107.32,417.01,67.95"><head>Table 4</head><label>4</label><figDesc>Validation results of the third experiment comparing different proposed approaches to incorporate metadata into the model architecture.</figDesc><table coords="14,143.12,148.62,248.74,26.65"><row><cell>Metadata</cell><cell>Validation Metrics</cell></row><row><cell>Approach</cell><cell>Macro ùêπ 1 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,108.81,165.84,377.65,86.13"><head>Score Challenge Track 1 Challenge Track 2</head><label></label><figDesc></figDesc><table coords="14,108.81,183.32,346.08,68.64"><row><cell>No Metadata incorp.</cell><cell>65.95 %</cell><cell>90.17 %</cell><cell>2197</cell></row><row><cell>Multiply with regional prior dist.</cell><cell>75.27 %</cell><cell>93.25 %</cell><cell>1511</cell></row><row><cell>Joint Feature Learning with Embedded Metadata</cell><cell>76.04 %</cell><cell>93.54 %</cell><cell>1427</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="15,88.97,107.32,417.00,70.17"><head>Table 5</head><label>5</label><figDesc>Validation results of the fourth experiment comparing different loss functions and class balance terms to handle the strong class imbalance of the challenge data set.</figDesc><table coords="15,110.71,150.85,292.49,26.65"><row><cell>Loss</cell><cell>Class</cell><cell>Validation Metrics</cell></row><row><cell>Function</cell><cell>Weighting</cell><cell>Macro ùêπ 1 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="15,97.47,168.06,400.33,163.03"><head>Score Challenge Track 1 Challenge Track 2</head><label></label><figDesc></figDesc><table coords="15,97.47,185.54,368.76,145.55"><row><cell>CE</cell><cell>-</cell><cell>76.04 %</cell><cell>93.54 %</cell><cell>1427</cell></row><row><cell>Focal Loss ùõæ = 0.5</cell><cell>inverse class freq.</cell><cell>76.58 %</cell><cell>93.27 %</cell><cell>1482</cell></row><row><cell>Focal Loss ùõæ = 2.0</cell><cell>inverse class freq.</cell><cell>75.93 %</cell><cell>93.16 %</cell><cell>1518</cell></row><row><cell>Focal Loss ùõæ = 0.5</cell><cell>effectiv num. samples</cell><cell>77.15 %</cell><cell>93.58 %</cell><cell>1425</cell></row><row><cell>Focal Loss ùõæ = 2.0</cell><cell>effectiv num. samples</cell><cell>76.64 %</cell><cell>93.34 %</cell><cell>1466</cell></row><row><cell>ArcFace Loss + Focal Loss ùõæ = 0.5</cell><cell>effectiv num. samples</cell><cell>79.10 %</cell><cell>93.85 %</cell><cell>1373</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="16,88.97,107.32,417.32,211.51"><head>Table 6</head><label>6</label><figDesc>Validation results of the fifth experiment comparing different MIL pooling techniques to pool TTA instances as well as instances of the same snake observation. "mean" refers to mean and "w. avg. " refers to the weighted average MIL pooling on the instance-level as described in Section 4.3.1. "attention" refers to bag-level MIL pooling described in Section 4.3.2.</figDesc><table coords="16,89.76,174.76,415.76,144.08"><row><cell>TTA Instance</cell><cell>Snake Instance</cell><cell cols="2">Validation Metrics</cell><cell></cell></row><row><cell>MIL-Pooling</cell><cell>MIL-Pooling</cell><cell cols="3">Macro ùêπ 1 -Score Challenge Track 1 Challenge Track 2</cell></row><row><cell></cell><cell></cell><cell>classic MIL pooling methods</cell><cell></cell><cell></cell></row><row><cell>mean</cell><cell>mean</cell><cell>79.10 %</cell><cell>93.85 %</cell><cell>1373</cell></row><row><cell>mean</cell><cell>w. avg.</cell><cell>79.17 %</cell><cell>93.85 %</cell><cell>1374</cell></row><row><cell>w. avg.</cell><cell>mean</cell><cell>79.53 %</cell><cell>94.07 %</cell><cell>1325</cell></row><row><cell>w. avg.</cell><cell>w. avg.</cell><cell>79.57 %</cell><cell>94.05 %</cell><cell>1330</cell></row><row><cell></cell><cell cols="3">attention-based MIL pooling methods</cell><cell></cell></row><row><cell>attention</cell><cell>mean</cell><cell>80.01 %</cell><cell>94.12 %</cell><cell>1305</cell></row><row><cell cols="2">attention</cell><cell>79.78 %</cell><cell>94.16 %</cell><cell>1297</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="18,88.93,107.32,417.07,248.13"><head>Table 7</head><label>7</label><figDesc>Results of the final selected models on the challenge public test data set compared to the submitted models that were further fine-tuned using the training + validation data sets. TTA, MIL pooling methods of the models are given in brackets. "mean" refers to mean MIL pooling and "w. avg." refers to the weighted average MIL pooling of the class probability distribution output by the model as described in Section 4.3.1. "attention" refers to the feature embedding level MIL pooling described in Section 4.3.2.</figDesc><table coords="18,89.29,184.85,416.71,170.60"><row><cell>Final Model</cell><cell>Fine-Tuned w/</cell><cell></cell><cell>Test Metrics 11</cell><cell></cell></row><row><cell>(TTA + MIL pooling)</cell><cell>Validation Data</cell><cell cols="3">Macro ùêπ 1 -Score Challenge Track 1 Challenge Track 2</cell></row><row><cell>Model 1. (w.avg + mean)</cell><cell>no</cell><cell>81.49 %</cell><cell>94.92 %</cell><cell>1187</cell></row><row><cell>Model 2. (attention + mean)</cell><cell>no</cell><cell>81.37 %</cell><cell>94.90 %</cell><cell>1194</cell></row><row><cell>Model 3. (attention)</cell><cell>no</cell><cell>80.55 %</cell><cell>94.80 %</cell><cell>1185</cell></row><row><cell>Model 1. (w.avg + mean)</cell><cell>yes</cell><cell>81.90 %</cell><cell>95.09 %</cell><cell>1149</cell></row><row><cell>Model 2. (attention + mean)</cell><cell>yes</cell><cell>81.39 %</cell><cell>94.95 %</cell><cell>1187</cell></row><row><cell>Model 3. (attention)</cell><cell>yes</cell><cell>80.97 %</cell><cell>94.96 %</cell><cell>1172</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,108.66,661.70,298.39,8.97"><p>Tropical Herping: https://www.tropicalherping.com/, [Last accessed: 2023-06-01].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,108.50,672.66,397.48,8.97;2,89.29,683.62,78.72,8.97"><p>WHO Snakebite Data Information Portal: https://snbdatainfo.who.int/?page=Information#tab=tab_3, [Last accessed: 2023-06-01]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="4,108.93,683.54,252.29,8.97"><p>iNaturalist: https://www.inaturalist.org/, [Last accessed: 2023-06-01].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="10,108.66,683.61,213.19,8.97"><p>Training epochs with parameter-frozen feature extractors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4" coords="12,108.93,683.59,325.60,8.97"><p>https://huggingface.co/BBracke/convnextv2_base.inat21_384, [Last accessed: 2023-06-01]</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Louise Bloch</rs> was partially funded by a PhD grant from <rs type="funder">University of Applied Sciences and Arts Dortmund, Dortmund, Germany</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="22,112.66,462.16,393.33,10.91;22,112.66,475.71,393.33,10.91;22,112.66,489.26,107.76,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="22,342.27,462.16,163.72,10.91;22,112.66,475.71,201.93,10.91">Overview of snakeclef 2023: Snake identification in medically important scenarios</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,337.00,475.71,168.99,10.91;22,112.66,489.26,77.06,10.91">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,502.81,394.53,10.91;22,112.66,516.35,394.53,10.91;22,112.66,529.90,393.33,10.91;22,112.66,543.45,393.33,10.91;22,112.66,557.00,393.33,10.91;22,112.66,570.55,394.52,10.91;22,112.66,584.10,128.78,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,194.37,529.90,311.62,10.91;22,112.66,543.45,258.38,10.91">Overview of LifeCLEF 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,400.42,543.45,105.56,10.91;22,112.66,557.00,393.33,10.91;22,112.66,570.55,188.85,10.91">Proceedings of the 13th International Conference of the CLEF Association (CLEF</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="257" to="285" />
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="22,112.66,597.65,394.53,10.91;22,112.66,611.20,395.01,10.91;22,112.30,624.75,393.68,10.91;22,112.66,638.30,394.53,10.91;22,112.66,651.85,393.33,10.91;22,112.66,665.40,289.43,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,383.16,624.75,122.82,10.91;22,112.66,638.30,171.35,10.91">Lifeclef 2023 teaser: Species identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Moussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,392.71,651.85,113.28,10.91;22,112.66,665.40,38.01,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,103.81,394.53,10.91;23,112.66,117.35,394.53,10.91;23,112.66,130.90,393.33,10.91;23,112.66,144.45,395.16,10.91;23,112.66,158.00,394.52,10.91;23,112.33,171.55,329.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="23,112.66,130.90,393.33,10.91;23,112.66,144.45,135.53,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,271.76,144.45,236.07,10.91;23,112.66,158.00,389.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">September 18-23, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,185.10,394.53,10.91;23,112.66,198.65,397.48,10.91;23,112.66,214.64,43.94,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,112.66,198.65,100.79,10.91">Snakebite envenoming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Guti√©rrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Calvete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Warrell</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrdp.2017.63</idno>
	</analytic>
	<monogr>
		<title level="j" coord="23,224.02,198.65,147.96,10.91">Nature Reviews Disease Primers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,225.75,394.52,10.91;23,112.33,239.30,394.85,10.91;23,112.33,252.85,226.75,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,112.33,239.30,390.47,10.91">The urgent need to develop novel strategies for the diagnosis and treatment of snakebites</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Layfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vallance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Trim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaiyapuri</surname></persName>
		</author>
		<idno type="DOI">10.3390/toxins11060363</idno>
	</analytic>
	<monogr>
		<title level="j" coord="23,112.33,252.85,29.72,10.91">Toxins</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,266.40,394.53,10.91;23,112.66,279.94,394.52,10.91;23,112.66,293.49,395.17,10.91;23,112.66,307.04,394.53,10.91;23,112.66,320.59,395.01,10.91;23,112.66,334.14,221.61,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,354.94,266.40,152.24,10.91;23,112.66,279.94,322.27,10.91">Combination of object detection, geospatial data, and feature concatenation for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>B√∂ckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bracke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-158.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="23,291.41,293.49,216.42,10.91;23,112.66,307.04,206.61,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="23,211.64,320.59,155.18,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1982" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,347.69,394.53,10.91;23,112.66,361.24,393.32,10.91;23,112.66,374.79,393.33,10.91;23,112.41,388.34,394.77,10.91;23,112.33,401.89,372.49,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="23,246.68,361.24,259.30,10.91;23,112.66,374.79,273.70,10.91">Combination of image and location information for snake species identification using object detection and EfficientNets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_201.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="23,409.33,374.79,96.66,10.91;23,112.41,388.34,278.59,10.91">Working Notes of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,415.44,395.17,10.91;23,112.66,428.99,393.33,10.91;23,112.66,442.54,394.52,10.91;23,112.66,456.08,310.80,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="23,225.36,415.44,282.47,10.91;23,112.66,428.99,188.60,10.91">EfficientNets and Vision Transformers for snake species identification using image and location information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-126.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="23,322.64,428.99,183.35,10.91;23,112.66,442.54,184.84,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1477" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,469.63,393.33,10.91;23,112.66,483.18,393.58,10.91;23,112.33,496.73,200.73,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="23,311.94,469.63,194.05,10.91;23,112.66,483.18,137.07,10.91">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13673-014-0003-0</idno>
	</analytic>
	<monogr>
		<title level="j" coord="23,257.77,483.18,240.64,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,510.28,393.33,10.91;23,112.66,523.83,393.33,10.91;23,112.66,537.38,395.17,10.91;23,112.41,550.93,394.78,10.91;23,112.66,564.48,254.33,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="23,320.46,510.28,185.53,10.91;23,112.66,523.83,122.32,10.91">Image classification for snake species using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48517-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="23,452.94,523.83,53.05,10.91;23,112.66,537.38,363.35,10.91">Proceedings of the Computational Intelligence in Information Systems Conference (CIIS 2016)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Phon-Amnuaisuk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T.-W</forename><surname>Au</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Omar</surname></persName>
		</editor>
		<meeting>the Computational Intelligence in Information Systems Conference (CIIS 2016)<address><addrLine>Brunei; Brunei Darussalam; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,578.03,393.33,10.91;23,112.66,591.58,393.33,10.91;23,111.46,605.13,393.45,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="23,198.76,578.03,232.27,10.91">Snake detection and classification using deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<idno type="DOI">10.24251/hicss.2021.148</idno>
	</analytic>
	<monogr>
		<title level="m" coord="23,452.94,578.03,53.05,10.91;23,112.66,591.58,338.90,10.91;23,120.16,605.13,45.78,10.91">Proceedings of the 54th Hawaii International Conference on System Sciences (HICSS 2021)</title>
		<meeting>the 54th Hawaii International Conference on System Sciences (HICSS 2021)<address><addrLine>Maui, Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
	<note>2021-01-08</note>
</biblStruct>

<biblStruct coords="23,112.66,618.67,393.33,10.91;23,112.66,632.22,393.32,10.91;23,112.41,645.77,194.44,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="23,444.88,618.67,61.11,10.91;23,112.66,632.22,348.64,10.91">Revealing the unknown: Real-time recognition of gal√°pagos snake species using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname></persName>
		</author>
		<idno type="DOI">10.3390/ani10050806</idno>
	</analytic>
	<monogr>
		<title level="j" coord="23,469.50,632.22,36.48,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,659.32,393.33,10.91;23,112.66,672.87,393.33,10.91;24,112.66,103.81,397.48,10.91;24,112.66,119.80,73.61,7.90" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="23,367.69,659.32,138.30,10.91;23,112.66,672.87,50.14,10.91">Snake species identification and recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vasmatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumbla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pimpalkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/IBSSC51096.2020.9332218</idno>
	</analytic>
	<monogr>
		<title level="m" coord="23,186.15,672.87,285.95,10.91">Proceedings of the IEEE Bombay Section Signature Conference</title>
		<meeting>the IEEE Bombay Section Signature Conference<address><addrLine>IBSSC; Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,130.90,394.52,10.91;24,112.66,144.45,393.33,10.91;24,112.33,158.00,393.65,10.91;24,112.66,171.55,363.70,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="24,279.77,130.90,222.55,10.91">Snake image classification using Siamese networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1145/3338472.3338476</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,127.57,144.45,378.42,10.91;24,112.33,158.00,54.73,10.91">Proceedings of the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)</title>
		<meeting>the 3rd International Conference on Graphics and Signal Processing (ICGSP 2019)<address><addrLine>Hong Kong, Hong Kong; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,185.10,393.33,10.91;24,112.66,198.65,393.33,10.91;24,112.66,212.20,394.52,10.91;24,112.66,225.75,394.53,10.91;24,112.66,239.30,287.08,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="24,310.02,185.10,195.96,10.91;24,112.66,198.65,162.02,10.91">Image-based classification of snake species using convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
		<idno type="DOI">10.1109/isriti48646.2019.9034633</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,302.18,198.65,203.81,10.91;24,112.66,212.20,330.00,10.91">Proceedings of the International Seminar on Research of Information Technology and Intelligent Systems (ISRITI 2019)</title>
		<meeting>the International Seminar on Research of Information Technology and Intelligent Systems (ISRITI 2019)<address><addrLine>Yogyakarta, Indonesia</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,252.85,393.33,10.91;24,112.66,266.40,393.32,10.91;24,112.66,279.94,395.01,10.91;24,112.66,293.49,185.12,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,453.38,252.85,52.61,10.91;24,112.66,266.40,393.32,10.91;24,112.66,279.94,152.50,10.91">An artificial intelligence model to identify snakes from across the world: Opportunities and challenges for global health and herpetology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pntd.0010647</idno>
	</analytic>
	<monogr>
		<title level="j" coord="24,277.30,279.94,157.75,10.91">PLOS Neglected Tropical Diseases</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,307.04,393.32,10.91;24,112.66,320.59,393.33,10.91;24,112.66,334.14,393.33,10.91;24,112.66,347.69,397.48,10.91;24,112.66,363.68,43.94,7.90" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,341.73,307.04,164.25,10.91;24,112.66,320.59,65.12,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,200.70,320.59,305.29,10.91;24,112.66,334.14,109.45,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)<address><addrLine>Miami Beach, Florida, US</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,374.79,393.32,10.91;24,112.66,388.34,393.33,10.91;24,112.66,401.89,267.33,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,261.44,374.79,244.54,10.91;24,112.66,388.34,108.39,10.91">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j" coord="24,228.73,388.34,277.26,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,415.44,395.16,10.91;24,112.66,428.99,394.61,10.91;24,112.66,442.54,393.33,10.91;24,112.66,456.08,301.22,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,259.80,415.44,203.34,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,488.38,415.44,19.44,10.91;24,112.66,428.99,390.35,10.91">Proceesings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<meeting>eesings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)<address><addrLine>Las Vegas, Nevada, US</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,469.63,393.33,10.91;24,112.66,483.18,393.33,10.91;24,112.66,496.73,395.17,10.91;24,112.66,510.28,155.75,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="24,247.99,469.63,258.00,10.91;24,112.66,483.18,50.14,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,316.83,483.18,189.15,10.91;24,112.66,496.73,291.76,10.91">Conference Track Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2015" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,523.83,393.33,10.91;24,112.66,537.38,393.33,10.91;24,112.66,550.93,395.01,10.91;24,112.66,564.48,143.58,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="24,359.69,523.83,146.30,10.91;24,112.66,537.38,39.98,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,182.56,537.38,323.43,10.91;24,112.66,550.93,107.75,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,578.03,393.33,10.91;24,112.66,591.58,393.53,10.91;24,112.30,605.13,394.88,10.91;24,112.66,618.67,266.52,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="24,399.80,578.03,106.19,10.91;24,112.66,591.58,141.79,10.91">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
	</analytic>
	<monogr>
		<title level="m" coord="24,278.69,591.58,227.50,10.91;24,112.30,605.13,193.13,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)<address><addrLine>Salt Lake City, Utah, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,112.66,632.22,393.33,10.91;24,112.66,645.77,393.33,10.91;24,112.66,659.32,394.53,10.91;24,112.66,672.87,394.03,10.91;25,112.66,103.81,224.56,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="24,341.94,632.22,164.05,10.91;24,112.66,645.77,119.27,10.91">Signature verification using a Siamese time delay neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>S√§ckinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="24,449.66,645.77,56.33,10.91;24,112.66,659.32,236.09,10.91">Advances in Neural Information Processing Systems (NIPS 1993)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<meeting><address><addrLine>Denver, Colorado, US</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,117.35,395.16,10.91;25,112.66,130.90,393.32,10.91;25,112.66,144.45,395.00,10.91;25,112.66,158.00,276.83,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="25,278.72,117.35,229.10,10.91;25,112.66,130.90,25.23,10.91">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,160.76,130.90,345.22,10.91;25,112.66,144.45,149.47,10.91">Proceedings of the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the Deep Learning workshop of the International Conference on Machine Learning (ICML 2015)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2015" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,171.55,394.53,10.91;25,112.66,185.10,393.33,10.91;25,112.66,198.65,393.33,10.91;25,112.66,212.20,268.68,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="25,112.66,185.10,393.33,10.91;25,112.66,198.65,281.21,10.91">Supervised learning computer vision benchmark for snake species identification from photographs: Implications for herpetology and global health</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salath√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2021.582110</idno>
	</analytic>
	<monogr>
		<title level="j" coord="25,407.34,198.65,98.64,10.91;25,112.66,212.20,51.98,10.91">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,225.75,393.33,10.91;25,112.66,239.30,393.33,10.91;25,112.66,252.85,394.52,10.91;25,112.66,266.40,70.76,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="25,292.93,225.75,213.05,10.91;25,112.66,239.30,172.54,10.91">Overview of SnakeCLEF 2022: Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,309.75,239.30,196.24,10.91;25,112.66,252.85,185.78,10.91">Working Notes of the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1957" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,279.94,393.59,10.91;25,112.66,293.49,393.33,10.91;25,112.66,307.04,394.53,10.91;25,112.33,320.59,372.49,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="25,380.27,279.94,125.97,10.91;25,112.66,293.49,252.28,10.91">Overview of the SnakeCLEF 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De Casta√±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharada</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_258.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,395.53,293.49,110.46,10.91;25,112.66,307.04,271.29,10.91">Proceedings of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting>the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,334.14,394.61,10.91;25,112.66,347.69,395.17,10.91;25,112.66,361.24,394.03,10.91;25,112.66,374.79,66.21,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="25,195.64,334.14,286.96,10.91">Impact of pretrained networks for snake species classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_194.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,112.66,347.69,363.88,10.91">Proceedings of the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<meeting>the 11th Conference and Labs of the Evaluation Forum (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">194</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,388.34,393.33,10.91;25,112.66,401.89,393.33,10.91;25,112.33,415.44,395.50,10.91;25,112.66,428.99,375.74,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="25,353.22,388.34,152.77,10.91;25,112.66,401.89,28.26,10.91">ImageNet-21k pretraining for the masses</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Zkj_VcZ6ol" />
	</analytic>
	<monogr>
		<title level="m" coord="25,163.39,401.89,342.60,10.91;25,112.33,415.44,247.57,10.91;25,474.63,415.44,33.21,10.91;25,112.66,428.99,71.10,10.91">Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 1)</title>
		<meeting>the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 1)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Onlineonly Conference</note>
</biblStruct>

<biblStruct coords="25,112.66,442.54,393.33,10.91;25,112.66,456.08,394.53,10.91;25,112.30,469.63,395.36,10.91;25,112.66,483.18,143.58,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.322</idno>
		<title level="m" coord="25,395.24,442.54,110.75,10.91;25,112.66,456.08,271.02,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV 2017)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV 2017)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,496.73,394.52,10.91;25,112.66,510.28,393.33,10.91;25,112.66,523.83,394.53,10.91;25,112.39,537.38,371.49,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="25,178.09,496.73,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="25,293.89,510.28,212.10,10.91;25,112.66,523.83,151.51,10.91;25,322.62,523.83,54.73,10.91">Proceedings of the 36th International Conference on Machine Learning (ICML 2019)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning (ICML 2019)<address><addrLine>Long Beach, California, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>-2019-06-15</note>
</biblStruct>

<biblStruct coords="25,112.66,550.93,393.33,10.91;25,112.66,564.48,393.33,10.91;25,112.66,578.03,395.01,10.91;25,112.66,591.58,217.35,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="25,202.64,550.93,303.34,10.91;25,112.66,564.48,88.24,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-127.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,222.23,564.48,283.76,10.91;25,112.66,578.03,199.82,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2021): 2021-09-21 -2021-09-24</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1499" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,605.13,394.61,10.91;25,112.66,618.67,395.16,10.91;25,112.66,632.22,394.53,10.91;25,112.41,645.77,394.11,10.91;25,112.66,659.32,290.74,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="25,245.07,605.13,236.65,10.91">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="25,112.66,618.67,395.16,10.91;25,112.66,632.22,309.77,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020): 2020-06-14 -2020-06-19</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020): 2020-06-14 -2020-06-19</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
	<note>Online-only conference</note>
</biblStruct>

<biblStruct coords="25,112.66,672.87,393.33,10.91;26,112.66,103.81,393.33,10.91;26,112.66,117.35,395.01,10.91;26,112.66,130.90,217.35,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="25,302.26,672.87,203.73,10.91;26,112.66,103.81,71.13,10.91">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-128.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="26,207.06,103.81,298.92,10.91;26,112.66,117.35,199.82,10.91">Working Notes of the 12th Conference and Labs of the Evaluation Forum (CLEF 2021): 2021-09-21 -2021-09-24</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1512" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,144.45,394.53,10.91;26,112.66,158.00,393.33,10.91;26,112.66,171.55,395.01,10.91;26,112.66,185.10,191.05,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="26,190.58,158.00,150.93,10.91">ResneSt: Split-attention networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw56347.2022.00309</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,390.89,158.00,115.10,10.91;26,112.66,171.55,288.41,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="2736" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,198.65,393.32,10.91;26,112.66,212.20,393.33,10.91;26,112.66,225.75,395.01,10.91;26,112.66,239.30,143.58,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="26,301.73,198.65,204.25,10.91;26,112.66,212.20,68.94,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,204.45,212.20,301.54,10.91;26,112.66,225.75,107.67,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,252.85,395.16,10.91;26,112.66,266.40,393.33,10.91;26,112.26,279.94,393.72,10.91;26,112.66,293.49,395.17,10.91;26,112.66,307.04,378.80,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="26,399.68,266.40,106.31,10.91;26,112.26,279.94,218.27,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="26,351.23,279.94,154.75,10.91;26,112.66,293.49,238.73,10.91;26,473.33,293.49,34.49,10.91;26,112.66,307.04,68.94,10.91">Proceedings of the 9th International Conference on Learning Representations (ICLR 2021)</title>
		<meeting>the 9th International Conference on Learning Representations (ICLR 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Onlineonly conference</note>
</biblStruct>

<biblStruct coords="26,112.66,320.59,394.61,10.91;26,112.66,334.14,395.01,10.91;26,112.66,347.69,143.58,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="26,327.53,320.59,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,136.67,334.14,269.07,10.91">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,361.24,393.53,10.91;26,112.66,374.79,394.53,10.91;26,112.66,388.34,393.33,10.91;26,112.66,401.89,393.32,10.91;26,112.66,415.44,394.67,10.91;26,112.41,428.99,35.59,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="26,229.73,361.24,276.46,10.91;26,112.66,374.79,115.37,10.91">When large kernel meets vision transformer: A solution for SnakeCLEF &amp; FungiCLEF</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-175.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="26,112.66,388.34,393.33,10.91;26,112.66,401.89,26.91,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="26,432.99,402.90,72.99,9.72;26,112.66,415.44,75.21,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="2199" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,442.54,394.61,10.91;26,112.66,456.08,394.52,10.91;26,112.66,469.63,268.75,10.91" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="26,380.81,442.54,106.77,10.91">A ConvNet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52688.2022.01167</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,136.00,456.08,340.29,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="11966" to="11976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,483.18,394.52,10.91;26,112.66,496.73,397.48,10.91;26,112.36,512.72,138.90,7.90" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="26,297.23,483.18,205.46,10.91">VOLO: Vision outlooker for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2022.3206108</idno>
	</analytic>
	<monogr>
		<title level="j" coord="26,112.66,496.73,295.55,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,523.83,395.17,10.91;26,112.66,537.38,393.33,10.91;26,112.66,550.93,397.48,10.91;26,112.66,566.92,32.07,7.90" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="26,378.80,523.83,129.03,10.91;26,112.66,537.38,193.01,10.91">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00986</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,350.74,537.38,155.25,10.91;26,112.66,550.93,124.61,10.91">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,578.03,394.53,10.91;26,112.66,591.58,394.53,10.91;26,112.41,605.13,205.46,10.91" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="26,202.42,578.03,299.84,10.91">TrivialAugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00081</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,150.32,591.58,283.67,10.91">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,618.67,393.32,10.91;26,112.66,632.22,393.33,10.91;26,112.66,645.77,393.33,10.91;26,112.33,659.32,394.36,10.91;26,112.66,672.87,387.22,10.91" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="26,295.19,618.67,210.79,10.91;26,112.66,632.22,72.47,10.91">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="26,462.03,632.22,43.95,10.91;26,112.66,645.77,247.26,10.91">Advances in Neural Information Processing Systems (NIPS 2012)</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, US</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,103.81,394.53,10.91;27,112.28,117.35,393.70,10.91;27,112.66,130.90,394.53,10.91;27,112.66,144.45,317.33,10.91" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="27,178.57,117.35,137.02,10.91">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,337.26,117.35,168.72,10.91;27,112.66,130.90,242.50,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)<address><addrLine>Boston, Massachusetts, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,158.00,393.32,10.91;27,112.66,171.55,393.33,10.91;27,112.14,185.10,395.05,10.91;27,112.66,198.65,394.53,10.91;27,112.66,212.20,315.05,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="27,276.64,158.00,229.34,10.91;27,112.66,171.55,60.16,10.91">Solution for SnakeCLEF 2022 by tackling long-tailed categorization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-180.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="27,425.17,171.55,80.82,10.91;27,112.14,185.10,325.72,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="27,316.03,198.65,148.41,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,225.75,393.33,10.91;27,112.66,239.30,394.61,10.91;27,112.66,252.85,208.58,10.91" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="27,411.38,225.75,94.60,10.91;27,112.66,239.30,71.51,10.91">Long-tail learning via logit adjustment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=37nvvqkCo5" />
	</analytic>
	<monogr>
		<title level="m" coord="27,207.69,239.30,244.11,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,266.40,393.53,10.91;27,112.66,279.94,393.33,10.91;27,112.33,293.49,292.33,10.91" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="27,305.50,266.40,200.69,10.91;27,112.66,279.94,44.79,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00949</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,203.72,279.94,302.27,10.91;27,112.33,293.49,30.22,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,307.04,393.32,10.91;27,112.66,320.59,393.33,10.91;27,112.33,334.14,393.65,10.91;27,112.66,347.69,393.32,10.91;27,112.66,361.24,394.67,10.91;27,112.66,374.79,88.91,10.91" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="27,295.02,307.04,210.96,10.91;27,112.66,320.59,169.77,10.91">Solutions for fine-grained and long-tailed snake species recognition in SnakeCLEF 2022</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3180/paper-183.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="27,143.79,334.14,362.19,10.91;27,112.66,347.69,78.73,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="27,480.05,348.70,25.94,9.72;27,112.66,361.24,122.91,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5th -to -8th, 2022. 2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="2291" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,388.34,393.33,10.91;27,112.66,401.89,393.33,10.91;27,112.33,415.44,178.93,10.91" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="27,340.94,388.34,165.05,10.91;27,112.66,401.89,151.46,10.91">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,290.28,401.89,215.70,10.91;27,112.33,415.44,29.23,10.91">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,428.99,393.61,10.91;27,112.26,442.54,393.73,10.91;27,112.66,456.08,394.53,10.91;27,112.66,469.63,118.49,10.91" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="27,401.73,428.99,104.54,10.91;27,112.26,442.54,105.22,10.91">MetaFormer is actually what you need for vision</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,238.60,442.54,267.38,10.91;27,112.66,456.08,165.40,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)<address><addrLine>New Orleans, Louisiana, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,483.18,393.33,10.91;27,112.66,496.73,393.32,10.91;27,112.66,510.28,395.01,10.91;27,112.41,523.83,179.18,10.91" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="27,325.07,483.18,180.92,10.91;27,112.66,496.73,67.70,10.91">You Only Look Once: Unified, real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,202.98,496.73,303.00,10.91;27,112.66,510.28,114.71,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)<address><addrLine>Las Vegas, Nevada, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,537.38,393.33,10.91;27,112.33,550.93,394.94,10.91;27,112.66,564.48,393.33,10.91;27,112.66,578.03,394.03,10.91;27,112.66,591.58,82.84,10.91" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="27,178.19,537.38,221.10,10.91">EfficientNetV2: smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/tan21a/tan21a.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="27,141.32,550.93,361.69,10.91;27,403.69,565.49,102.30,9.72;27,112.66,579.04,76.47,9.72">Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning (ICML 2021)<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="27,112.66,605.13,393.33,10.91;27,112.66,618.67,354.81,10.91" xml:id="b54">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00808v1</idno>
		<title level="m" coord="27,381.62,605.13,124.37,10.91;27,112.66,618.67,212.68,10.91">ConvNeXt V2: Co-designing and scaling convnets with masked autoencoders</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,632.22,393.33,10.91;27,112.66,645.77,375.69,10.91" xml:id="b55">
	<monogr>
		<title level="m" type="main" coord="27,224.60,632.22,281.39,10.91;27,112.66,645.77,74.22,10.91">Bridging nonlinearities and stochastic regularizers with Gaussian error linear units</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR abs/1606.08415</idno>
		<ptr target="http://arxiv.org/abs/1606.08415" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,659.32,393.33,10.91;27,112.66,672.87,395.16,10.91;28,112.66,103.81,393.33,10.91;28,112.66,117.35,58.17,10.91" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="27,221.94,659.32,176.13,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m" coord="27,421.66,659.32,84.32,10.91;27,112.66,672.87,395.16,10.91;28,112.66,103.81,8.50,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2019) 2019-05-06 -2019-05-09</title>
		<meeting>the International Conference on Learning Representations (ICLR 2019) 2019-05-06 -2019-05-09<address><addrLine>New Orleans, Louisiana, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,130.90,393.33,10.91;28,112.66,144.45,393.32,10.91;28,112.66,158.00,394.53,10.91;28,112.66,171.55,248.89,10.91" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="28,328.85,130.90,177.14,10.91;28,112.66,144.45,181.87,10.91">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00612</idno>
	</analytic>
	<monogr>
		<title level="m" coord="28,317.15,144.45,188.84,10.91;28,112.66,158.00,205.78,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV 2019)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV 2019)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,185.10,395.16,10.91;28,112.66,198.65,393.32,10.91;28,112.66,212.20,395.17,10.91;28,112.66,225.75,360.80,10.91" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="28,300.35,185.10,207.47,10.91;28,112.66,198.65,172.74,10.91">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00359</idno>
	</analytic>
	<monogr>
		<title level="m" coord="28,308.62,198.65,197.35,10.91;28,112.66,212.20,239.75,10.91;28,473.58,212.20,34.26,10.91;28,112.66,225.75,68.94,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020)</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition (CVPR 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3008" to="3017" />
		</imprint>
	</monogr>
	<note>Onlineonly conference</note>
</biblStruct>

<biblStruct coords="28,112.66,239.30,395.17,10.91;28,112.66,252.85,394.52,10.91;28,112.66,266.40,378.97,10.91" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="28,305.97,239.30,154.26,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.7000</idno>
	</analytic>
	<monogr>
		<title level="m" coord="28,469.02,239.30,38.81,10.91;28,112.66,252.85,278.40,10.91">Proceedings of the 34 Conference on Artificial Intelligence (AAAI 2020)</title>
		<meeting>the 34 Conference on Artificial Intelligence (AAAI 2020)<address><addrLine>New York, New York, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,279.94,363.78,10.91" xml:id="b60">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m" coord="28,258.26,279.94,88.05,10.91">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,293.49,393.33,10.91;28,112.66,307.04,393.33,10.91;28,112.66,320.59,397.48,10.91;28,112.36,336.58,140.39,7.90" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="28,346.63,293.49,159.35,10.91;28,112.66,307.04,315.59,10.91">Early, intermediate and late fusion strategies for robust deep learning-based multimodal action recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Boulahia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amamra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Madi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Daikh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-021-01249-8</idno>
		<ptr target="https://doi.org/10.1007%2Fs00138-021-01249-8.doi:10.1007/s00138-021-01249-8" />
	</analytic>
	<monogr>
		<title level="j" coord="28,436.61,307.04,69.37,10.91;28,112.66,320.59,76.64,10.91">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,347.69,394.61,10.91;28,112.66,361.24,362.09,10.91" xml:id="b62">
	<analytic>
		<title level="a" type="main" coord="28,271.88,347.69,215.42,10.91">Attention-based deep multiple instance learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,112.66,361.24,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,374.79,393.33,10.91;28,112.66,388.34,393.33,10.91;28,112.41,401.89,272.78,10.91" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="28,340.73,374.79,165.26,10.91;28,112.66,388.34,106.18,10.91">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2021.3087709</idno>
	</analytic>
	<monogr>
		<title level="j" coord="28,226.60,388.34,279.39,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="5962" to="5979" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,415.44,346.44,10.91" xml:id="b64">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
	</analytic>
	<monogr>
		<title level="j" coord="28,176.94,415.44,99.63,10.91">PyTorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,428.99,394.53,10.91;28,112.66,442.54,394.52,10.91;28,112.66,456.08,394.53,10.91;28,112.66,469.63,394.53,10.91;28,112.66,483.18,395.17,10.91;28,112.66,496.73,394.53,10.91;28,112.66,510.28,394.67,10.91;28,112.66,523.83,226.69,10.91" xml:id="b65">
	<analytic>
		<title level="a" type="main" coord="28,369.79,456.08,137.40,10.91;28,112.66,469.63,178.35,10.91">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="28,292.30,483.18,215.52,10.91;28,112.66,496.73,85.14,10.91">Advances in Neural Information Processing Systems (Neurips 2019)</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,537.38,394.53,10.91;28,112.66,550.93,393.33,10.91;28,112.66,564.48,395.17,10.91;28,112.66,578.03,395.00,10.91" xml:id="b66">
	<analytic>
		<title level="a" type="main" coord="28,280.58,550.93,111.74,10.91">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1gs9JgRZ" />
	</analytic>
	<monogr>
		<title level="m" coord="28,420.18,550.93,85.80,10.91;28,112.66,564.48,291.76,10.91">Proceedings of the International Conference on Learning Representations (ICLR 2018)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2018)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,591.58,394.04,10.91;28,112.66,605.13,133.95,10.91" xml:id="b67">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/inaturalist-2021" />
		<title level="m" coord="28,215.29,591.58,136.38,10.91">iNat challenge 2021 -FGVC8</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,112.66,618.67,394.53,10.91;28,112.66,632.22,393.33,10.91;28,112.66,645.77,393.32,10.91;28,112.66,659.32,216.98,10.91" xml:id="b68">
	<analytic>
		<title level="a" type="main" coord="28,461.78,632.22,44.21,10.91;28,112.66,645.77,271.67,10.91">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName coords=""><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Altch√®</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,407.39,645.77,98.59,10.91;28,112.66,659.32,186.99,10.91">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
