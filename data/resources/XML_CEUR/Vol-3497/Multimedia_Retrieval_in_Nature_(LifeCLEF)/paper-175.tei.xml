<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,440.21,17.04;1,72.02,96.20,250.01,17.04">Bird Species Recognition using Convolutional Neural Networks with Attention on Frequency Bands</title>
				<funder ref="#_TE72cjk">
					<orgName type="full">BMEL (Bundesministerium für Ernährung und Landwirtschaft)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.02,141.82,70.90,10.80"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn.berlin</email>
							<affiliation key="aff0">
								<orgName type="institution">Museum für Naturkunde Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,440.21,17.04;1,72.02,96.20,250.01,17.04">Bird Species Recognition using Convolutional Neural Networks with Attention on Frequency Bands</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A5B242F53649E47AE6A23AD9B9B9E91</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Species Recognition</term>
					<term>Biodiversity Assessment</term>
					<term>Soundscapes</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Data Augmentation</term>
					<term>Kaggle Competition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a deep learning approach for recognizing bird species in soundscape recordings using Convolutional Neural Networks (CNNs). The proposed method extends CNNs with a classification head that incorporates attention on frequency bands. The models are trained on a large dataset of bird sounds and employ various data augmentation techniques to improve performance and address the domain shift between training and test data. The effectiveness of the approach is evaluated in the BirdCLEF 2023 competition, hosted on Kaggle, where it achieves a macro-averaged mean average precision (cmAP) of 76.3 % on the official test set. This performance positions the method among the top 3 systems to accurately identify birds in wildlife monitoring recordings around Northern Mount Kenya.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2023 competition focuses on recognizing vocalizing birds in Eastern African soundscape recordings. The main challenges to address in this year's task edition include the imbalance in the number of training files per species, the domain shift between training and test data and the time limit of two hours to identify all birds in a large set of diverse soundscape recordings spanning over 30 hours.</p><p>The machine learning algorithms developed within the scope of this competition will help researchers to conduct pilot projects in selected areas of Northern Mount Kenya and evaluate the impact of different management strategies and degradation levels on bird biodiversity in rangeland systems. By accurately monitoring the effects of restoration efforts on biodiversity, they aim to establish financial mechanisms for widespread landscape restoration and protection. The advancements of systems for automated bird recognition will facilitate more effective evaluation of threats and adjustments to conservation actions, benefiting avian populations and supporting long-term sustainability.</p><p>Further details about the BirdCLEF 2023 task are given in [1], [2] and <ref type="bibr" coords="1,405.66,621.12,11.72,9.94" target="#b0">[3]</ref>. The task is part of the LifeCLEF 2023 evaluation campaign <ref type="bibr" coords="1,237.53,633.84,12.65,9.94">[4,</ref><ref type="bibr" coords="1,250.18,633.84,8.43,9.94" target="#b1">5]</ref> and the Conference and Labs of the Evaluation Forum <ref type="bibr" coords="1,499.42,633.84,11.94,9.94" target="#b2">[6,</ref><ref type="bibr" coords="1,511.36,633.84,7.96,9.94" target="#b3">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation</head><p>The implementation of the machine learning based system for bird species recognition presented in this paper builds upon solutions for previous BirdCLEF competitions and similar tasks <ref type="bibr" coords="2,458.26,118.36,12.98,9.94" target="#b4">[8,</ref><ref type="bibr" coords="2,471.24,118.36,8.66,9.94" target="#b5">9,</ref><ref type="bibr" coords="2,479.90,118.36,12.98,9.94" target="#b6">10,</ref><ref type="bibr" coords="2,492.88,118.36,12.98,9.94" target="#b7">11,</ref><ref type="bibr" coords="2,505.87,118.36,12.98,9.94" target="#b8">12]</ref>. Further details on own past developments and implementation methods can be found for example in <ref type="bibr" coords="2,72.02,143.68,16.99,9.94" target="#b9">[13]</ref>, <ref type="bibr" coords="2,95.90,143.68,18.31,9.94" target="#b10">[14]</ref> and <ref type="bibr" coords="2,135.61,143.68,16.90,9.94" target="#b11">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Preparation</head><p>The official BirdCLEF 2023 training dataset consists of 16940 audio recordings provided by Xenocanto <ref type="bibr" coords="2,99.02,214.12,16.89,9.94">[16]</ref>, covering 264 different bird species. To address the class imbalance and limited number of training files per species, the dataset is extended with recordings from previous competitions <ref type="bibr" coords="2,72.02,239.44,18.37,9.94" target="#b12">[17,</ref><ref type="bibr" coords="2,90.40,239.44,13.78,9.94">18,</ref><ref type="bibr" coords="2,104.18,239.44,13.78,9.94" target="#b13">19,</ref><ref type="bibr" coords="2,117.96,239.44,13.78,9.94">20]</ref> and additional files from Xeno-canto (XC). Furthermore, soundscapes (SC) without bird activity from the DCASE 2018 Bird Detection Task <ref type="bibr" coords="2,308.58,252.04,18.50,9.94">[21,</ref><ref type="bibr" coords="2,327.07,252.04,13.87,9.94" target="#b14">22]</ref> and other sources <ref type="bibr" coords="2,425.89,252.04,18.40,9.94">[23]</ref> are included as a 'nocall' class and for noise augmentation.</p><p>Table <ref type="table" coords="2,114.36,277.39,5.52,9.94" target="#tab_0">1</ref> gives an overview on the individual datasets utilized. The extended dataset encompasses a total of 659 classes, comprising 264 species from 2023, additional 394 species from previous years and the 'nocall' class. All in all, 141580 files are collected for training and augmentation with a total accumulated duration of approximately 78 days. The original training set consists of audio files from Xeno-canto only, which were resampled to 32 kHz, converted to mono and compressed to lossy Ogg format. To ensure consistent sampling rates and prevent resampling during training, files from other sources are also converted to 32 kHz. Additional files obtained from Xeno-canto [24] are converted to lossless FLAC format without mono mixing. The duration of each file is added to the training metadata to enable fast access of short audio segments within files at random position during training. Furthermore, the energy of 5-second segments, shifted by one second, is calculated for each file using the root-mean-square (RMS) of the signal amplitude in each segment. This information is later used to weight the selection of audio chunks and increase the probability of finding segments with bird activity.</p><p>Xeno-canto files are weakly labeled, meaning that there is no precise information on the presence or absence of the labeled bird within the recording. However, there is typically a high probability of hearing the labeled bird at the beginning of each audio file, as recordists often trim their recordings accordingly before uploading them. To exploit this characteristic, the first 10 seconds of each recording are duplicated and also added to the training set.</p><p>For training and cross-validation, the entire dataset is split into 8 stratified randomized folds, ensuring that the primary species used in the 2021 and 2023 BirdCLEF editions are proportionally represented in each fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Engineering</head><p>The models are trained on 5-second audio chunks represented as spectrograms. The raw 1D audio signal is converted to a 2D log Mel spectrogram image using the melspectrogram and power_to_db functions of the librosa python library <ref type="bibr" coords="3,241.97,131.08,18.32,9.94" target="#b15">[25]</ref> with the following parameters:</p><formula xml:id="formula_0" coords="3,104.18,155.90,95.91,113.91">• sr = 32000 • n_fft = 2048 • hop_length = 512 • n_mels = 128 • fmin = 40 • fmax = 15000 • power = 2.0 • ref = np.max • top_db = 100</formula><p>The resulting spectrogram image is then normalized to the unsigned integer range of 0 to 255, resized to a resolution of 312x128 pixels and converted to a 3-channel RGB image. This preprocessing yield to create images from audio close to the input format most Convolutional Neural Networks are original designed for and pretrained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Methods</head><p>The training process makes use of various tools and libraries. PyTorch <ref type="bibr" coords="3,403.76,380.35,18.46,9.94" target="#b16">[26]</ref> is utilized as the main framework, along with additional libraries such as timm <ref type="bibr" coords="3,337.97,393.07,18.46,9.94" target="#b17">[27]</ref> for CNN backbones and pretrained weights, SoundFile [28], librosa <ref type="bibr" coords="3,215.87,405.67,18.38,9.94" target="#b15">[25]</ref> and SciPy [29] for audio and signal processing, Audiomentations <ref type="bibr" coords="3,72.02,418.39,18.44,9.94">[30]</ref> for data augmentation and scikit-learn <ref type="bibr" coords="3,262.83,418.39,18.34,9.94" target="#b18">[31]</ref> for calculating metrics and creating training/validation data splits.</p><p>All models adopt a common architecture consisting of a CNN backbone pretrained on ImageNet <ref type="bibr" coords="3,72.02,456.33,18.44,9.94" target="#b19">[32]</ref> as a feature extractor, combined with a custom classification head. The classification head is designed based on a modified Sound Event Detection (SED) head, which incorporates attention on frequency bands. This modification aims to leverage the fact, that birds in soundscapes usually occupy species specific frequency ranges. In the original SED architecture <ref type="bibr" coords="3,395.25,494.25,17.86,9.94" target="#b20">[33,</ref><ref type="bibr" coords="3,413.12,494.25,13.40,9.94">34,</ref><ref type="bibr" coords="3,426.51,494.25,13.40,9.94" target="#b21">35,</ref><ref type="bibr" coords="3,439.91,494.25,13.40,9.94">36]</ref>, feature maps representing frequency bands are aggregated via mean pooling and attention is applied on features representing time frames only. By applying attention to frequency bands instead, the model can better differentiate species vocalizing simultaneously but with different pitches. The modification involves a simple step of rotating the spectrogram image by 90 degrees before feeding it into the original SED network. For feature encoding, EfficientNet CNNs of the first and second generation <ref type="bibr" coords="3,472.57,557.49,18.54,9.94" target="#b22">[37,</ref><ref type="bibr" coords="3,491.11,557.49,13.91,9.94" target="#b23">38]</ref> are employed (mainly timm's tf_efficientnet_b0_ns and tf_efficientnetv2_s_in21k). Figure <ref type="figure" coords="3,456.69,570.21,5.52,9.94" target="#fig_0">1</ref> illustrates the variation in class activation outputs between the original and modified SED architecture when presented with an input containing multiple bird species. In this simplified example, species c2 is better detected with frequency attention, because class activation doesn't interfere with other species, like it would for example with species c0 in the case of using time attention.</p><p>The training process involves multiple steps. Initially, the models are trained on all 8 cross-validation folds. During training, 5-second audio chunks are randomly selected from each file, either without weighting or weighted by signal energy to increase the likelihood of capturing segments with bird activity. The trained models are then used to create pseudo labels for successive 5-second intervals in all training files. This enables the selection of chunks in subsequent training steps not only based on signal energy weighting but also weighted by the probability of the primary (foreground) species assigned to the file. Pseudo labels are also used to add additional labels of possible background species to selected audio chunks during training. Up to 8 pseudo labels are added, depending on species probabilities, either as hard labels (if the species probability is above 0.8) or as soft labels using the probability derived from the pseudo label. If provided by Xeno-canto recordists, background species are included as soft target labels with a value of 0.3. For inference, predictions are reduced to the 2023 species set (264 classes) and multiple models are ensembled by averaging their predictions without weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Data Augmentation</head><p>Several data augmentation techniques are applied during training, especially to address the challenges posed by weak and noisy labels, as well as to compensate for the domain shift between training and test data. Many of these techniques have been successfully employed in previous approaches. For a more detailed description of each method and its impact on model performance, please refer to <ref type="bibr" coords="4,140.59,465.09,18.50,9.94" target="#b9">[13]</ref> and <ref type="bibr" coords="4,182.74,465.09,16.94,9.94" target="#b11">[15]</ref>. Here is a concise overview of the augmentation methods used in this competition:</p><p>• Random cyclic shift of the audio signal A novel augmentation method introduced in this year's task is reverb augmentation. In soundscapes, recordings often capture birds from a large distance, resulting in weaker sounds with more reverb and attenuated high frequencies compared to the typically cleaner sounds in Xeno-canto files, where the microphone is directly targeted at the bird. To address this difference between training and test data, reverb is added to the training files using impulse responses recorded from the Valhalla Vintage Verb audio plugin <ref type="bibr" coords="4,128.90,720.24,16.90,9.94">[45]</ref>. During training, randomly selected impulse responses are convolved with the original audio signal with a probability of 20%, employing a dry/wet mix control that ranges from 0.2 (almost dry signal) to 1.0 (only reverb). Figure <ref type="figure" coords="4,260.81,745.56,5.52,9.94" target="#fig_1">2</ref> provides examples, illustrating the influence of reverb augmentation on the resulting spectrogram image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The approach described in this paper secured the 3rd place among a total of 1189 participating teams and the 1 st place on public leaderboard, representing a smaller subset of the test data. Final scores on private and public leaderboard (LB) and ranking of the top 10 teams are presented in Table <ref type="table" coords="5,497.70,395.23,4.14,9.94" target="#tab_2">2</ref>. By combining several diverse models (including different CNN backbones, training hyperparameters and dataset folds) a macro-averaged mean average precision score (cmAP) of 76.3 % was achieved on the complete test set (see team 'adsr' in Table <ref type="table" coords="5,256.94,433.17,3.97,9.94" target="#tab_2">2</ref>).</p><p>Due to differences in Kaggle's hardware (particularly CPU types) used to run inference notebooks, the number of models that could be ensembled to identify all birds in the test set in the given time varied. To prevent submission errors, a timer was implemented in the notebook to ensure completion within the 2-hour limit. If the timer reached approximately 118 minutes, inferencing was halted and results were collected for all models and predicted file parts up to that point. Predictions from unfinished models or file parts were masked before averaging. This approach makes it difficult to determine the exact number of models that can be ensembled. Initially, only 3 models could be ensembled without risking timeouts. Later, inference speed was prioritized over model diversity by using models with the same inputs (consistent FFT size, number of Mel bands, etc.). This allowed pre-calculation and saving of Mel spectrogram images to memory for all files in advance, which were then reused for each model. Additionally, models were converted to TorchScript and the preprocessing of test files was parallelized using multiple CPU threads. With these optimizations, at least 7 models could be ensembled, depending on the backbone architecture, without setting a timer (e.g., 4 EfficientNetB0 + 3 EfficientNetV2s).</p><p>A fairly good cmAP of 74.2% on the complete test set (Kaggle's private leaderboard) can be achieved with a single, small and very fast EfficientNetB0-based model (see M6 in Table <ref type="table" coords="5,470.52,623.04,3.99,9.94" target="#tab_3">3</ref>). The best single model, which utilizes a ResNet50 backbone, achieves a score of 74.8% on the private leaderboard (M8 in Table <ref type="table" coords="5,143.84,648.24,4.02,9.94" target="#tab_3">3</ref>). On public leaderboard, the highest score is achieved by a model with an EfficientNetV2s backbone (M9 in Table <ref type="table" coords="5,257.45,660.96,4.00,9.94" target="#tab_3">3</ref>). The overall best system was not submitted for the final ranking. It achieves a cmAP score of 76.4 % with an ensemble of 8 models (5 EfficientNetB0 + 3 EfficientNetV2s). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Study</head><p>Table <ref type="table" coords="6,115.34,328.27,5.52,9.94" target="#tab_3">3</ref> illustrates the contributions of important aspects and novel approaches described in this paper on model performance. Model M1 serves as a baseline for comparison, utilizing an EfficientNetB0 backbone with the modified SED head mentioned earlier. Initially, audio chunks were selected without any weighting and only official competition data from this and previous years were used for training. As the model progressed from M1 to M2 and M4, by incorporating pseudo labels and adopting weighted audio chunk selection, there was a noticeable increase in performance. The use of EfficientNetV2s backbones improved the score on the public but not necessarily on the private leaderboard (M3 vs. M2 &amp; M9 vs. M6). The inclusion of additional files from Xeno-canto in the training data slightly contributed to score improvement (M5 vs. M4). Notably, the introduction of reverb augmentation significantly enhanced performance and proved to be an effective method to compensate for the domain shift between Xeno-canto files and soundscapes (M6 vs. M5). The modified SED version, with attention on frequency bands, surpassed the original SED architecture, which focused on time frames (M6 vs. M7). Although ResNet-based models achieved commendable scores (M8), they were not included in the final ensembles due to less favorable tradeoffs between model accuracy and inference time compared to EfficientNet-based architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The BirdCLEF 2023 task introduced some interesting and welcome changes compared to previous years. The competition's focus on cmAP as the evaluation metric eliminated the need for threshold tuning, while the inference time constraint encouraged the development of efficient models with a good balance between accuracy and speed.</p><p>While this paper discusses successful approaches, several other methods were explored but didn't yield satisfactory results. These included for example experimenting with different loss functions, creating model soups <ref type="bibr" coords="7,172.04,181.72,16.97,9.94" target="#b24">[46]</ref>, applying knowledge distillation, finetuning models with data containing only species of the test set or further optimizing CPU inference by converting models to ONNX <ref type="bibr" coords="7,492.99,194.32,18.51,9.94">[47]</ref> or <ref type="bibr" coords="7,72.02,206.92,74.83,9.94">OpenVINO [48]</ref> formats. Advanced postprocessing techniques, such as adjusting probabilities in neighboring audio segments or weighting model predictions in the ensemble, also did not lead to further score improvements.</p><p>Modifying the original SED architecture to incorporate frequency instead of time attention proved effective in recognizing birds in soundscapes, where multiple species vocalize at the same time but in different frequency ranges. However, the frequency dimensionality of the SED output is much smaller than the frequency resolution of the input spectrogram due to multiple max pooling operations in the CNN encoder. Increasing the frequency resolution by adjusting preceding layers in the feature encoder could further improve identification performance for recordings with a high degree of overlapping sounds. Exploring models that combine time and frequency attention within the same network would be also a promising avenue for future research. In addition to pure classification, such models would allow to annotate individual sound events in both time and frequency within the spectrogram.</p><p>The performance of models heavily relies on the quantity and quality of the training data. If the model is deployed to identify birds in soundscapes, the training data should be representative of that scenario. But unlike recordings with only a few birds and good signal to noise ratio, annotating soundscapes can be very time-consuming and requires expert knowledge. In addition to mixing audio segments with different sounds and noise characteristics, incorporating reverb into the audio signal can help to bridge the gap between clean recordings and soundscapes. If a system is designed to identify birds in a specific area or habitat it might be worth to create impulse responses of the target location and use those for reverb augmentation during training to simulate the characteristics of sound propagation in that area.</p><p>A model similar to the ones developed for this competition, trained to identify European bird species, is available at https://code.naturkundemuseum.berlin/tsa/birdid-europe254-2103. The model adopts the modified SED architecture and many of the training methods described in this paper. It has already been successfully implemented in various projects to assess avian biodiversity <ref type="bibr" coords="7,398.71,510.57,17.55,9.94" target="#b25">[49,</ref><ref type="bibr" coords="7,419.26,510.57,14.07,9.94">50,</ref><ref type="bibr" coords="7,433.33,510.57,14.07,9.94" target="#b26">51,</ref><ref type="bibr" coords="7,447.39,510.57,14.07,9.94">52]</ref> and is part of Naturblick [53], a mobile application for discovering and learning about nature in urban areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.02,254.21,403.14,11.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of SED output (class activation) using either time or frequency attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.02,297.53,450.63,11.04;5,87.70,185.15,135.72,96.05"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Reverb augmentation examples (left: original, mid.: dry/wet mix 0.5, right: dry/wet mix 0.8)</figDesc><graphic coords="5,87.70,185.15,135.72,96.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.02,340.13,404.53,149.54"><head>Table 1 :</head><label>1</label><figDesc>Datasets used for training and augmentation</figDesc><table coords="2,122.66,369.17,353.90,120.50"><row><cell cols="2">ID Name</cell><cell># Classes</cell><cell># Files</cell><cell>accum. Duration</cell></row><row><cell>1</cell><cell>BirdCLEF 2023 XC</cell><cell>264</cell><cell>16940</cell><cell>8d 00h 24m 02s</cell></row><row><cell>2</cell><cell>BirdCLEF 2023 XC ext. 2</cell><cell>264</cell><cell>32729</cell><cell>21d 02h 38m 33s</cell></row><row><cell>3</cell><cell>BirdCLEF 2020/21 XC</cell><cell>397</cell><cell>62874</cell><cell>40d 22h 29m 56s</cell></row><row><cell>4</cell><cell>BirdCLEF 2020/21 XC ext.</cell><cell>182</cell><cell>6941</cell><cell>3d 05h 08m 04s</cell></row><row><cell>5</cell><cell>BirdCLEF 2020/21 SC</cell><cell>49</cell><cell>20</cell><cell>03h 20m 00s</cell></row><row><cell>6</cell><cell>BirdCLEF 2019 SC</cell><cell>69</cell><cell>64</cell><cell>2d 02h 17m 11s</cell></row><row><cell>7</cell><cell>DCASE 2018</cell><cell>1</cell><cell>22012</cell><cell>2d 13h 08m 40s</cell></row><row><cell></cell><cell>Total</cell><cell cols="2">659 141580</cell><cell>78d 01h 26m 26s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,104.18,515.35,417.01,126.75"><head>•</head><label></label><figDesc>Application of audio signal filter with random transfer function • Mixup in time domain by adding chunks of same species, random species and nocall/noise • Random gain adjustment of signal amplitude for individual chunks before mixing • Random gain adjustment for the mixed signal • Pitch shift and time stretch (both local and global in time and frequency domain) • Addition of Gaussian/pink/brown noise • Insertion of short noise bursts • Addition of reverb (see remarks below) • Utilization of different interpolation filters for spectrogram resizing • Application of color jitter (brightness, contrast, saturation, hue) to the spectrogram image</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.02,74.16,446.72,190.37"><head>Table 2 :</head><label>2</label><figDesc>Competition results of the top 10 teams (with solution of team adsr describes in this paper)</figDesc><table coords="6,84.38,103.22,425.94,161.31"><row><cell cols="2">Rank Team Name on Kaggle</cell><cell>cmAP [%]</cell><cell>cmAP [%]</cell></row><row><cell></cell><cell></cell><cell>(priv. LB)</cell><cell>(publ. LB)</cell></row><row><cell>1</cell><cell>Volodymyr</cell><cell>76.392</cell><cell>84.444</cell></row><row><cell>2</cell><cell>griffith</cell><cell>76.369</cell><cell>84.292</cell></row><row><cell>3</cell><cell>adsr</cell><cell>76.309</cell><cell>84.735</cell></row><row><cell>4</cell><cell>atfujita</cell><cell>75.688</cell><cell>84.096</cell></row><row><cell>5</cell><cell>Yevhenii Maslov</cell><cell>75.498</cell><cell>83.847</cell></row><row><cell>6</cell><cell>anonamename</cell><cell>75.384</cell><cell>83.391</cell></row><row><cell>7</cell><cell>MSU+YSDA+HSE</cell><cell>75.347</cell><cell>83.442</cell></row><row><cell>8</cell><cell>furu-nag</cell><cell>75.285</cell><cell>83.735</cell></row><row><cell>9</cell><cell>Synergy</cell><cell>75.201</cell><cell>83.474</cell></row><row><cell>10</cell><cell>LeonShangguan</cell><cell>74.962</cell><cell>83.181</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.02,555.43,438.30,175.95"><head>Table 3 :</head><label>3</label><figDesc>Influence of individual methods and network architectures on model performance</figDesc><table coords="6,90.74,584.47,79.20,11.04"><row><cell>ID</cell><cell>Description</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,75.50,750.72,393.35,7.24"><p>This dataset also includes files from the official competition dataset (BirdCLEF 2023 XC) but with different preprocessing</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>I would like to thank <rs type="person">Sohier Dane</rs>, <rs type="person">Stefan Kahl</rs>, <rs type="person">Tom Denton</rs>, <rs type="person">Holger Klinck</rs> and all involved institutions (<rs type="affiliation">Kaggle</rs>, <rs type="affiliation">Chemnitz University of Technology, Google Research</rs>, <rs type="person">K. Lisa Yang Center</rs> for <rs type="affiliation">Conservation Bioacoustics at the Cornell Lab of Ornithology, LifeCLEF, NATURAL STATE, OekoFor GbR</rs> and Xeno-canto) for organizing this competition. I also want to thank the <rs type="institution">Museum für Naturkunde</rs> and the team of the <rs type="institution">Animal Sound Archive Berlin</rs> [54] in particular <rs type="person">Karl-Heinz Frommolt</rs>, <rs type="person">Olaf Jahn</rs> and <rs type="person">Benjamin Werner</rs> for supporting my work. The research was partly funded by the <rs type="funder">BMEL (Bundesministerium für Ernährung und Landwirtschaft)</rs> within the project "<rs type="projectName">Machbarkeitsstudie -Integration (bio-)akustischer Methoden zur Quantifizierung biologischer Vielfalt in das Waldmonitoring</rs>" (FKZ: <rs type="grantNumber">2221NR050B</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TE72cjk">
					<idno type="grant-number">2221NR050B</idno>
					<orgName type="project" subtype="full">Machbarkeitsstudie -Integration (bio-)akustischer Methoden zur Quantifizierung biologischer Vielfalt in das Waldmonitoring</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,108.02,140.10,415.10,8.96;8,108.02,151.62,415.04,8.96;8,108.02,163.04,317.02,9.06" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,139.39,151.62,363.78,8.96">Overview of BirdCLEF 2023: Automated bird species identification in Eastern Africa</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Reers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cherutich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.02,163.04,317.02,9.06">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,186.18,414.65,8.96;8,108.02,197.60,414.84,9.06;8,108.02,209.10,414.59,8.96;8,108.02,220.62,415.00,8.96;8,108.02,232.14,281.92,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,264.08,209.10,258.52,8.96;8,108.02,220.62,243.03,8.96">Overview of LifeCLEF 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcos</forename><forename type="middle">D</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,370.44,220.62,152.58,8.96;8,108.02,232.14,212.91,8.96">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,243.66,415.07,8.96;8,108.02,255.21,136.81,8.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,349.34,243.66,173.75,8.96;8,108.02,255.21,136.81,8.96">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,266.61,414.75,8.96;8,108.02,278.13,288.15,8.96;8,460.58,278.13,62.07,8.96;8,108.02,289.65,381.04,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,238.89,278.13,157.28,8.96;8,460.58,278.13,58.26,8.96">Experimental IR Meets Multilinguality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.02,289.65,324.30,8.96">Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<meeting>the Fourteenth International Conference of the CLEF Association<address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,301.17,414.45,8.96;8,108.02,312.69,221.17,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,319.61,301.17,202.86,8.96;8,108.02,312.69,76.44,8.96">Audio based bird species identification using deep learning techniques</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sprengel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,204.61,312.69,120.30,8.96">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,324.21,173.21,8.96;8,301.63,324.21,221.31,8.96;8,108.02,335.61,271.44,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,333.71,324.21,189.23,8.96;8,108.02,335.61,126.12,8.96">Large-Scale Bird Sound Classification using Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hussein</forename><forename type="middle">H</forename><surname>Et</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,254.89,335.61,120.29,8.96">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,347.13,414.54,8.96;8,108.02,358.65,17.81,8.96;8,143.50,358.65,38.29,8.96;8,199.46,358.65,25.53,8.96;8,242.51,358.65,43.25,8.96;8,303.44,358.65,46.11,8.96;8,367.21,358.65,70.80,8.96;8,455.65,358.65,18.57,8.96;8,491.87,358.65,30.79,8.96;8,108.02,370.17,200.93,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,212.01,347.13,292.90,8.96">Two Convolutional Neural Networks for Bird Detection in Audio Signals</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<idno type="DOI">10.23919/EUSIPCO.2017.8081512</idno>
		<ptr target="https://doi.org/10.23919/EUSIPCO.2017.8081512" />
	</analytic>
	<monogr>
		<title level="m" coord="8,108.02,358.65,17.81,8.96;8,143.50,358.65,38.29,8.96;8,199.46,358.65,25.53,8.96;8,242.51,358.65,43.25,8.96;8,303.44,358.65,46.11,8.96;8,367.21,358.65,65.74,8.96">25th European Signal Processing Conference (EUSIPCO2017)</title>
		<meeting><address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,381.69,415.12,8.96;8,108.02,393.21,272.45,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,222.33,381.69,300.81,8.96;8,108.02,393.21,127.00,8.96">Audio bird classification with inception-v4 extended with time and timefrequency attention mechanisms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,255.90,393.21,120.29,8.96">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,404.51,414.89,9.06;8,108.02,416.13,411.97,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,369.47,404.61,153.44,8.96;8,108.02,416.13,252.01,8.96">Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pamuła</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,380.18,416.13,139.82,8.96">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,427.65,389.77,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,183.27,427.65,169.29,8.96">Bird Species Identification in Soundscapes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,373.22,427.65,120.30,8.96">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,439.19,414.67,8.96;8,108.02,450.71,414.50,8.96;8,108.02,462.11,305.65,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,186.14,439.19,275.91,8.96">Acoustic Bird Detection with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,174.51,450.71,348.00,8.96;8,108.02,462.11,41.65,8.96">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</editor>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2018 Workshop<address><addrLine>DCASE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="143" to="147" />
		</imprint>
		<respStmt>
			<orgName>Tampere University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,473.63,414.60,8.96;8,108.02,485.15,138.20,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,183.78,473.63,334.25,8.96">Audio-based Bird Species Identification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,121.66,485.15,120.30,8.96">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,508.19,414.77,8.96;8,108.02,519.61,415.21,9.06;8,108.02,531.11,136.81,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,427.96,508.19,94.83,8.96;8,108.02,519.71,211.07,8.96">Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,340.97,519.61,182.26,9.06;8,108.02,531.11,136.81,8.96">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,554.15,414.65,8.96;8,108.02,565.57,415.10,9.06;8,108.02,577.19,185.36,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,472.23,554.15,50.44,8.96;8,108.02,565.67,263.87,8.96">Overview of BirdCLEF 2021: Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.50,565.57,130.62,9.06;8,108.02,577.19,185.36,8.96">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,611.56,414.58,9.05;8,108.02,623.18,414.89,8.96;8,108.02,634.70,266.35,8.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,369.15,611.56,153.44,9.05;8,108.02,623.18,249.21,8.96">Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pamuła</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1807.05812</idno>
		<idno>doi:</idno>
		<ptr target="10.48550/arXiv.1807.05812" />
	</analytic>
	<monogr>
		<title level="m" coord="8,378.90,623.18,139.84,8.96">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,669.14,414.61,8.96;8,108.02,680.56,414.76,9.05;8,108.02,692.18,114.78,8.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,138.70,680.56,198.42,9.05">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,358.28,680.56,164.50,9.05;8,108.02,692.18,42.14,8.96">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,703.70,293.37,8.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,199.10,703.70,146.90,8.96">Automatic differentiation in PyTorch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,366.77,703.70,34.62,8.96">NIPS-W</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.02,715.22,415.24,8.96;8,108.02,726.62,181.19,8.96" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target=":10.5281/zenodo.4414861" />
		<title level="m" coord="8,190.48,715.22,91.75,8.96">PyTorch Image Models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">GitHub repository</note>
</biblStruct>

<biblStruct coords="9,108.02,86.10,395.26,8.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,202.69,86.10,164.63,8.96">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,374.25,86.10,25.48,8.96">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,97.50,414.60,8.96;9,108.02,109.02,248.38,8.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,199.00,97.50,213.50,8.96">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,436.40,97.50,86.22,8.96;9,108.02,109.02,166.58,8.96">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,120.54,415.05,8.96;9,108.02,132.06,414.66,8.96;9,108.02,143.58,104.69,8.96" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="9,490.65,120.54,32.42,8.96;9,108.02,132.06,339.85,8.96">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,108.02,166.50,414.93,8.96;9,108.02,178.02,414.56,8.96;9,108.02,189.54,339.34,8.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,290.91,166.50,232.03,8.96;9,108.02,178.02,52.76,8.96">Sound Event Classification and Detection with Weakly Labeled Data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fayek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tourbabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.87,178.02,349.72,8.96;9,108.02,189.54,100.90,8.96">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019)<address><addrLine>New York University, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,212.58,414.57,8.96;9,108.02,224.10,208.94,8.96" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="9,205.80,212.58,312.20,8.96">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1905.11946</idno>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,235.50,415.14,8.96;9,108.02,247.02,112.94,8.96" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="9,204.94,235.50,215.24,8.96">EfficientNetV2: Smaller Models and Faster Training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2104.00298</idno>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,350.61,415.06,8.96;9,108.02,362.01,415.06,8.96;9,108.02,373.53,112.94,8.96" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="9,302.52,350.61,220.56,8.96;9,108.02,362.01,281.09,8.96">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.05482</idno>
		<idno type="arXiv">arXiv:2203.05482</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,408.09,415.08,8.96;9,108.02,419.61,415.12,8.96;9,108.02,431.01,111.26,8.96" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="9,380.04,408.09,143.05,8.96;9,108.02,419.61,151.05,8.96">Towards a multisensor station for automated biodiversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Wägele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Bourlat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.baae.2022.01.003</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,293.32,419.61,130.52,8.96">Basic and Applied Ecology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="105" to="138" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.02,454.07,414.46,8.96;9,108.02,465.59,414.72,8.96;9,108.02,477.11,138.22,8.96" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="9,341.20,454.07,181.28,8.96;9,108.02,465.59,250.28,8.96">Evaluation of acoustic pattern recognition of nightingale (Luscinia megarhynchos) recordings by citizens</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stehle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khorramshahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sturm</surname></persName>
		</author>
		<idno type="DOI">10.3897/rio.6.e50233</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,382.93,465.59,127.23,8.96">Research Ideas and Outcomes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">50233</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
