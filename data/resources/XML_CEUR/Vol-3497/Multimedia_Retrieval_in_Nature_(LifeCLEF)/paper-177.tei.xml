<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,380.53,15.42;1,88.69,106.66,293.23,15.42">Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,98.65,11.96"><forename type="first">Anthony</forename><surname>Miyaguchi</surname></persName>
							<email>acmiyaguchi@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.16,134.97,71.10,11.96"><forename type="first">Nathan</forename><surname>Zhong</surname></persName>
							<email>nathanzhong@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.90,134.97,80.67,11.96"><forename type="first">Murilo</forename><surname>Gustineli</surname></persName>
							<email>murilogustineli@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.57,134.97,68.26,11.96"><forename type="first">Chris</forename><surname>Hayduk</surname></persName>
							<email>chayduk3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,380.53,15.42;1,88.69,106.66,293.23,15.42">Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">60FAF58701F418A9C0FCD6A37DF21714</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transfer Learning</term>
					<term>Dataset Annotation</term>
					<term>BirdNET</term>
					<term>Bird-MixIT</term>
					<term>CEUR-WS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present working notes on transfer learning with semi-supervised dataset annotation for the Bird-CLEF 2023 competition, focused on identifying African bird species in recorded soundscapes. Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition. We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning. Our experiments involve various models and feature engineering approaches to maximize performance on the competition leaderboard. The results demonstrate the effectiveness of our approach in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2023 competition <ref type="bibr" coords="1,241.84,379.58,12.99,10.91" target="#b0">[1]</ref> focuses on classifying bird species in 10-minute-long soundscapes recorded in various parts of Africa as part of the LifeCLEF lab <ref type="bibr" coords="1,434.90,393.13,11.58,10.91" target="#b1">[2]</ref>. We need to label each 5-second interval in the test soundscape with the probability that each 264 target species is present. The training dataset comprises 16,941 tracks spanning 192 hours of audio tagged by the label of the target species present. While we have metadata about the species in the tracks, we do not have concrete labels on which audio sections contain calls. The dataset also poses significant challenges given the species distribution, with many having only one or two examples.</p><p>We focus our efforts on utilizing existing off-the-shelf models as the basis for our classification models. We hypothesize that we can reduce environmental noise and cross-talk using a soundseparation model and re-purpose knowledge in the embedding space learned by a domainspecific convolution neural network. These models can address some of the most significant impediments in the competition, including the need for labeled data through semi-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Embedding Space and Transfer Learning</head><p>BirdNET <ref type="bibr" coords="2,130.11,111.28,12.68,10.91" target="#b2">[3]</ref> classifies 48kHz 3-second audio clips into 3337 classes using a convolutional neural network trained on scaled spectrograms computed by the short-time Fourier transform. The classes are primarily composed of bird species but include non-bird classes such as environmental noise or human voices. We obtain embedding tokens by taking the values at the second-to-last layer of the model, preceding a fully connected logit layer. The embedding maps the audio in the time domain into a vector space ℛ 320 that roughly preserves distances between points. We use embedding tokens as features in a supervised machine-learning model to take advantage of the compact representation of the audio data. We show a clear separation between the embedding tokens by running them through a dimensional reduction technique in figure <ref type="figure" coords="2,446.38,219.67,3.81,10.91" target="#fig_3">1</ref>. Clustering supports a hypothesis that we effectively utilize representation learned by BirdNET in new contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>We demonstrate the clustering properties of the BirdNET embeddings by projecting them into ℛ 2 via UMAP <ref type="bibr" coords="2,168.04,503.91,11.83,8.87" target="#b3">[4]</ref> The projection preserves Euclidean distance in 2D space. We take the embedding token across each track with the most significant probability across the BirdNET prediction vector and assign it a positive label. The left plot shows clustering across the seven most common species in the training dataset. The right plot demonstrates a clear separation between the seven most common species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-Supervised Dataset Annotation</head><p>We propose a process to derive an annotated dataset to fit data to traditional supervised learning algorithms. First, we chunk audio within the training examples so that no track lasts 3 minutes by recursively splitting the tracks until they are smaller than our threshold, padded to the nearest third second with additive white noise. Chunking the audio solves the problem of batch processing skew introduced by several examples that are longer than 30 minutes. We assume the upper bound on the track length is sufficient to model temporal dependencies. We use Bird-MixIT <ref type="bibr" coords="3,142.09,528.75,12.82,10.91" target="#b4">[5]</ref> to isolate environmental noise and bird vocalizations. We then process each of the tracks using BirdNET to extract an embedding vector in R 320 and a prediction logit vector in R 3337 for every 3-second interval over a 1-second sliding window. Each interval is labeled with top prediction labels and the energy of the original track. We save the results from each track to disk and consolidate them into a parquet dataset. See table <ref type="table" coords="3,383.37,582.95,4.97,10.91" target="#tab_0">1</ref> for an example row of this process. We only have to pay for the expensive process of running TensorFlow models once by processing the audio training examples before fitting models.</p><p>We had four versions of the embedding dataset (emb). In emb_v1, we encountered missing entries and Docker permission problems. We fixed this in emb_v2 but incorrectly mapped input to the wrong model layer. Our first usable dataset was emb_v3, which fixed previous issues and addressed the skew of long tracks by recursively chunking them. emb_v4 includes all possible 3-second intervals at a 1-second resolution. With the embedding and prediction vectors in a dataset, we apply a series of heuristics and feature engineering for our final models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation and Workflow</head><p>We split our workflow into training and inference. Our training pipeline runs on the Google Cloud Platform (GCP), while inference runs in a Kaggle notebook optimized for offline usage. We implement a shared Python package on GitHub. <ref type="foot" coords="4,342.67,648.41,3.71,7.97" target="#foot_0">1</ref> The package defines environment dependencies to run the training and inference workflows. It contains helper PySpark code <ref type="bibr" coords="5,89.29,100.52,11.58,10.91" target="#b5">[6]</ref>, wrappers around BirdNET, and utilities for manipulating audio samples into matrices representing sliding windows. We also define a workflow package containing Luigi <ref type="bibr" coords="5,461.24,114.06,12.81,10.91" target="#b6">[7]</ref> scripts that implement dataset processing and annotation. We build Docker images for BirdNet and MixIT and integrate modified versions of the canonical inference script into our data pipeline as per figure <ref type="figure" coords="5,147.59,154.71,3.74,10.91" target="#fig_1">3</ref>. The inference pipeline is composed of three notebooks. The package sync notebook downloads the shared Python package with all dependencies into a local directory. The model sync notebook similarly downloads serialized models and weights from object storage. The final inference notebook runs offline after attaching the package and model sync notebooks as data sources. We read the soundscapes, split them into chunks, obtained BirdNET embeddings, and computed predictions submitted to the competition. See the appendix for the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We run experiments on the embedding dataset to maximize our performance on the public leaderboard. The feature engineering process of embedding tokens and prediction logits is part of the model-fitting process. We first focused on a baseline with minimal modifications to the embedding dataset and then worked toward overcoming the idiosyncrasies of the training dataset by more complex feature engineering. The input interval of BirdNET and the output interval of the competition do not match, so we had to consider this difference. The former expects 3-second intervals, while the latter expects 5-second intervals. Our two main approaches are to aggregate the output of models that represent 3-second intervals and to aggregate input to represent 5-second intervals.</p><p>While we use simple measures such as macro-precision and accuracy to assess models against the derived dataset in hyperparameter searches, we note that they did not reflect performance against the public leaderboard. The results of our derived datasets and models are summarized in table <ref type="table" coords="5,125.18,457.22,5.07,10.91" target="#tab_2">3</ref> and<ref type="table" coords="5,152.13,457.22,28.03,10.91" target="#tab_3">table 4</ref>, respectively.  Adds logic to assign the primary label and no-call labels. It uses concatenation instead of interpolation for a 5-second interval token and includes the prediction logits for the current interval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline Model</head><p>The baseline model uses embedding tokens taken from the isolated track source with the highest energy, assuming that the loudest voice in the track is associated with the primary label. We label the tokens according to the primary label if the max probability of the prediction vector exceeds a threshold, e.g., 0.5; otherwise, we label the token as "no-call". We fit the data to logistic regression, multi-layer perceptron (MLP), support vector machine (SVM), and gradient-boosted decision tree (GBDT) classifiers. We use scikit-learn <ref type="bibr" coords="7,333.76,175.28,13.00,10.91" target="#b7">[8]</ref> for the first three of these models and the scikit-learn compatible interface against XGBoost <ref type="bibr" coords="7,351.82,188.83,12.94,10.91" target="#b8">[9]</ref> for the latter. When applicable, we perform a hyper-parameter search using sci-kit-optimize <ref type="bibr" coords="7,362.79,202.38,16.32,10.91" target="#b9">[10]</ref>, which performs sequential optimization using Bayesian methods. We hypothesize that classes cluster together in the low-dimensional embedding learned by BirdNET and that linear models can effectively learn to discriminate between new classes. Our baseline logistic regression classifier trained on the embedding tokens reaches a public/private score of 0.78/0.68, notably better than the starter Kaggle notebook using the Google Research Bird Vocalization Classifier with a score of 0.72/0.61. We find that SVM and GBDT are comparable to the logistic regression and that MLP models require significant tuning to reach good performance.</p><p>GBDT via XGBoost is our preferred model because it trains quickly with a GPU with relatively high predictive performance. While logistic regression has fewer parameters and performs just as well, training can be slow as the number of examples increases. In table 5, logistic regression takes 12x as long to train as XGBoost with GPU-based histogram binning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>A comparison between fit and predict the time for various models fit on a GCP n1-standard-4 compute instance with a Telsa T4 GPU. We fit the post-v7 dataset, which has 255,372 rows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Baseline Binary No-call Model</head><p>We explore and analyze the performance of a binary classifier to further our understanding of the embedding space. While we do not use this model directly in the competition, it helps quantify the quality of our automated labeling process. We construct a binary dataset with positive and negative embedding samples. Positive signifies the presence of a birdcall in the sample, while negative denotes its absence. The distribution of positive and negative samples within the dataset is well-balanced, with 49.7% being positive and 50.3% being negative. About half of the available training data is empty across original and sound-separated tracks. A logistic regression classifier achieved an accuracy of 0.88 on this binary dataset.</p><p>We create a second binary dataset from a subset of the first using the top three most common species, with 49.8% of samples being positive and 50.2% being negative. A logistic regression classifier is trained and predicted 1.0 accuracy on the smaller dataset. We hypothesize that this behavior is due to the large number of species in the dataset. The distribution of some species is highly skewed, as shown in Figure <ref type="figure" coords="8,255.87,154.71,3.74,10.91" target="#fig_2">4</ref>.</p><p>Furthermore, we classified audio embeddings of the freefield1010 background soundscape dataset <ref type="bibr" coords="8,124.44,181.81,18.07,10.91" target="#b10">[11]</ref> using the logistic regression classifier. The classifier predictions are probability scores indicating the presence or absence of birdcalls in each sample. It predicted 77.3% of the samples as having no birdcalls (no-call) and 22.7% as having birdcalls (call). However, if the classifier's predictions were perfect, the no-call percentage should have been 1.0, as the entire dataset consisted only of background noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Interpolated Embedding Models</head><p>We build upon the baseline embedding model by interpolating embedding tokens to synthesize new examples and labels. We assume that positive examples from each class tend to cluster together in the high-dimensional embedding space. We also assume that embedding space takes on a Euclidean geometry or admits some approximation. By interpolating examples, we hypothesize that the resulting coordinate lies between the clusters and, therefore, closer to the decision boundary of the sources.</p><p>We first use interpolation to address the alignment problem between the 3-second interval of BirdNET and the 5-second interval of the competition by generating a new feature. In addition, we also construct a method that generates pairs and triplets of tokens sampled evenly across classes. These interpolated tokens are assigned multiple labels used in a multi-label classifier.</p><p>We experiment with interpolation to add contextual information to each example. For each token in a track from our dataset 𝑣 𝑡 , we generate the token that follows it directly in time 𝑣 𝑡+1 and the token that represents the entire track ∑︀ 𝑛 𝑖=0 𝑣 𝑖 . We generate features by concatenating (⊕) each of these tokens and evaluate performance relative to each other using the same set of labels.</p><p>𝑦 ˆ∼ 𝑀 1 (𝑣 𝑡 )</p><p>(1)</p><formula xml:id="formula_0" coords="9,234.89,222.05,271.10,11.36">𝑦 ˆ∼ 𝑀 2 (𝑣 𝑡 ⊕ 𝑣 𝑡+1 )<label>(2)</label></formula><formula xml:id="formula_1" coords="9,234.89,237.36,271.10,33.71">𝑦 ˆ∼ 𝑀 3 (𝑣 𝑡 ⊕ 𝑛 ∑︁ 𝑖=0 𝑣 𝑖 )<label>(3)</label></formula><formula xml:id="formula_2" coords="9,234.89,273.68,271.10,33.71">𝑦 ˆ∼ 𝑀 4 (𝑣 𝑡 ⊕ 𝑣 𝑡+1 ⊕ 𝑛 ∑︁ 𝑖=0 𝑣 𝑖 )<label>(4)</label></formula><p>Interpolation may provide some value, in particular around multi-labeling. We found that our model trained on a dataset does not perform worse than our baseline model while avoiding issues related to having a small number of training examples for the class. Most models trained using a form of interpolated tokens resulted in lower leaderboard scores. However, these models did not include interpolated pair and triplet tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Concatenated Embedding Model</head><p>Another class of models that handles the time-interval discrepancy involves concatenating the embedding tokens. We must train a classifier that accepts input in ℛ 2𝑥320 . This model performs worse than the interpolated model. The increased dimensional of the underlying feature also increases the model fit time, which leads us to skip augmented feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ensemble Embedding Model</head><p>Our last embedding model uses the best models from our baseline and interpolated embedding experiments. We train an XGBoost model trained on the outputs from the best classifiers. This model is likely affected by the quality of the training dataset and the differences in embedding token semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Probability Logit Model</head><p>Our final experiment uses the outputs of the final logit layer in the BirdNET model to determine a class' presence directly. We generate a probability vector by taking a softmax of the logit layer. We ran into out-of-memory issues on our GPUs fitting XGBoost models using the probability vector in ℛ 3337 and found the scikit-learn logistic regression implementation needed to be faster. We fit the data using a Complement Naive Bayes classifier instead of a GBDT or logistic regression classifier. Naive Bayes assumption works well in this problem, where each feature is treated independently toward the classification goal. It is also swift because it simply computes counts over features. We use the Complement Naive Bayes model to address the heavy skew in the class distribution but also find comparable performance across this family of classifiers. The predictive performance is far worse than the baseline, with a public/private score of 0.71/0.59.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Semi-Supervised Annotation Quality</head><p>The data quality is an aspect of our training dataset that we would like to explore more deeply because the data quality affects the model's quality. We have built a training workflow that enables flexibility in labeling timestamps across the entire training audio to 1-second granularity. While we succeeded in our baseline transfer learning experiment, having a human-labeled test dataset independent of the competition leaderboard would be helpful. We can use ground-truth annotations to test how well an automated labeling process does at assigning primary, secondary, and no-call labels.</p><p>It would also be worth exploring human annotation in the sound separation process. While we have listened to several examples to judge the quality of sound separation, we need a method to quantify quality. In the case of sorting by the highest energy source, we may confuse a source with a significant amount of noise for the true birdcall just by the nature of higher entropy in the source. In the case of sorting by the highest number of matching classifications from an existing model, we may need to produce a classifier that can distinguish between noise and birdcall for rare classes. In this case, we continue to propagate uncertainty into the resulting labels, leading to poor performance, mainly when the number of examples is small.</p><p>We could introduce a metric to measure the resulting separation quality rigorously. We could create many positive birdcall clips and have a human determine which channel mainly contains the primary species. These clips let us see how well our automated channel selection process does at choosing the right channel based on human-generated labels. However, this metric would not allow us to determine the separation quality in degenerate cases where a single separated source contains multiple bird vocalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Audio Source Separation</head><p>The MixIT source separation model plays a significant role in the embedding pipeline used for our experiments. One of the most significant benefits is noise suppression. We also relied on sound-separated channels to generate training examples and semi-supervised labels. Running an ablation study by evaluating the labeling process without access to the sound separation model would have been helpful.</p><p>We wanted to explore the 8-source model, which could have different performance characteristics than the 4-source model. However, we decided to ignore this model because the training examples generally have few distinct vocalizations, and creating more source channels increases the necessary disk space for the intermediate files.</p><p>We are also interested in the effect of the sound separation model during inference. However, we observed that the sound separation stage in training was a significant fraction of the computation budget. There are also issues with different sample rates. BirdNET expects audio sampled at 48kHz, while MixIT expects audio at 32KHz. We did not attempt to integrate the model into this project because it would have put us over the submission time limit and required significant engineering effort to run inside the competition environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Embedding Space and Transfer Learning</head><p>Our experiments with interpolated embeddings in section 5.2 had mixed results with respect to the baseline embedding model. Including embedding context from other time intervals had substantially lower performance than our baseline. The new labeling process may have overshadowed potential positive effects. On the other hand, we saw a slight increase in our model performance when using the mean of pairs and triplets during model fitting. Further research is necessary to determine whether it is valuable to manipulate embeddings similarly to mix up <ref type="bibr" coords="11,134.49,258.64,17.84,10.91" target="#b11">[12]</ref> that interpolates between training examples in the time domain to increase and augment training data.</p><p>We want to experiment with the embeddings from another model, such as the Google Research Bird Vocalization Classifier. This classifier has seen more of the species in this competition than BirdNET. It would be helpful to see how these two embeddings compare, and we could try this out as another feature.</p><p>Sequential models could be helpful in the competition by capturing dynamics and imbuing contextual information between embedding tokens. The simplest model would be an autoregressive linear model using an embedding from a single timestep to predict the next timestep optimized by a squared error loss. We made initial forays into attention-based sequence-tosequence models to address the output time-interval issue, but we needed more time to complete our experimentation. Future work might explore data-driven methods like HAVOC <ref type="bibr" coords="11,475.47,407.68,18.07,10.91" target="#b12">[13]</ref> to analyze the dynamics of birdcall audio and their embeddings.</p><p>We would also like to explore the relationship between the sound-separated tracks and the embedding. The separation model is constrained so that the sum of the sources results in the original track. It would be interesting to verify a relationship between embeddings of various tracks by fitting a predictive model that takes tokens from each source to predict the embedding of the original track. This line of thought does not directly help with model performance on the final task, but it does help understand the nature of the classifier embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In summary, our approach leveraged the embedding space learned by BirdNET to address the representation and labeling challenges in the competition. We developed a pipeline that included sound separation with MixIT, extraction of embedding tokens using BirdNET, and the creation of annotated datasets. Our results showcased the competitive performance of our logistic regression baseline model as an effective method on unseen species and the comparative performance of various feature engineering work. Our approach demonstrated the potential of transfer and semi-supervised learning for bird species classification in soundscapes. for blob in client.list_blobs(bucket, prefix=prefix): name = Path(blob.name).relative_to(relative) name.parent.mkdir(parents=True, exist_ok=True) blob.download_to_filename(name) print(f"downloaded {name}") client = storage.Client(project="birdclef-2023") download(client, "birdclef-2023", "data/models/birdnet-analyzer-pruned", "data") download(client, "birdclef-2023", "data/raw/sound_separation", "data") download(client, "birdclef-2023", "data/models/baseline", "data") download(client, "birdclef-2023", "data/models/ensembles", "data")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Kaggle Inference Sync Notebooks</head><p>A. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,416.23,413.25,8.93;3,88.81,428.23,405.55,8.87;3,89.29,440.19,406.27,8.87;3,89.29,452.14,396.40,8.87;3,89.29,464.10,369.95,8.87;3,89.29,84.19,416.70,312.66"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bird-MixIT has improved the precision of downstream classifiers in experimental settings. We demonstrate separation across entities in the mel-spectrogram of track XC207767 containing a Red-chested Cuckoo. We observe separating three sound signatures into sources 0, 1, and 3. Source 2 is an amalgamation of the other sound signatures, an artifact of the model separating into four channels. An automated process should choose source 3 containing the species of interest.</figDesc><graphic coords="3,89.29,84.19,416.70,312.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,290.02,418.00,8.93;4,89.29,302.03,413.51,8.87;4,89.29,313.98,414.06,8.87;4,89.29,325.94,141.43,8.87;4,89.29,84.19,416.68,186.45"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:We use Luigi to coordinate a processing pipeline spanning days on an n2-standard-16 compute instance. We prevent processing skew across workers by recursively training audio. The audio is then source separated and embedded, resulting in a parquet file per audio chunk. We consolidate the parquet files into the final dataset.</figDesc><graphic coords="4,89.29,84.19,416.68,186.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,486.40,409.82,8.93;8,89.29,498.14,51.16,9.14;8,140.45,496.57,3.97,6.12;8,147.41,498.41,350.62,8.87;8,89.29,510.36,71.20,8.87;8,89.29,257.76,416.69,209.26"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: We demonstrate the clustering properties of the binary no-call embeddings by projecting them into ℛ 2 via UMAP. The clustering compares the binary dataset with the overall species versus the top-3 species.</figDesc><graphic coords="8,89.29,257.76,416.69,209.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,89.29,111.96,240.83,10.71;14,89.29,132.12,53.65,7.62;14,89.29,143.08,385.34,7.62;14,89.69,155.04,283.69,6.49;14,89.69,167.79,405.25,6.49;14,89.69,180.54,395.50,6.49;14,89.29,206.52,230.40,10.71;14,89.29,227.68,157.53,6.49;14,89.29,238.64,118.22,6.49;14,89.29,260.56,229.64,6.49"><head>A. 1 .</head><label>1</label><figDesc>birdnet-transfer-learning-package-sync # %% [code] # Clone the repo and all of the submodules; this way, we can include the extras ! if [[ -d birdclef-2023 ]]; then rm -rf birdclef-2023; fi ! git clone --recurse-submodules https://github.com/dsgt-birdclef/birdclef-2023.git ! pip download -d pip-packages --prefer-binary ./birdclef-2023 tensorflow==2.11.0 A.2. birdnet-transfer-learning-model-sync from google.cloud import storage from pathlib import Path def download(client, bucket, prefix, relative):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,89.29,267.03,416.69,205.49"><head></head><label></label><figDesc></figDesc><graphic coords="2,89.29,267.03,416.69,205.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,403.68,408.67,159.07"><head>Table 1</head><label>1</label><figDesc>Example output row from the embedding dataset (emb_v4). The emb_3 dataset is about 20 GB, and the emb_v4 dataset is about 60 GB.</figDesc><table coords="4,99.10,442.65,394.58,120.10"><row><cell cols="2">column name value</cell></row><row><cell>species</cell><cell>grecor</cell></row><row><cell>track_stem</cell><cell>XC629875_part003</cell></row><row><cell>track_type</cell><cell>source0</cell></row><row><cell>track_name</cell><cell>grecor/XC629875_part003_source0.mp3</cell></row><row><cell>embedding</cell><cell>[0.6731137633323669, 1.1389738321304321, 0.6284520626068115, ...</cell></row><row><cell cols="2">prediction_vec [-8.725186347961426, -7.3204827308654785, -9.82101821899414, ...</cell></row><row><cell>predictions</cell><cell>[{0, 3026, Tetrastes bonasia_Hazel Grouse, hazgro1, 0.02235649898648262}, {1,...</cell></row><row><cell>start_time</cell><cell>75</cell></row><row><cell>energy</cell><cell>0.01598571054637432</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,486.23,409.91,138.24"><head>Table 2</head><label>2</label><figDesc>Example output row from the post v6 dataset on the left and v7 on the right.</figDesc><table coords="5,102.03,514.23,375.40,110.24"><row><cell>column name</cell><cell>value</cell></row><row><cell cols="2">track_stem track_type start_time primary_label metadata_species [ratcis1] XC116777 source1 0 ratcis1 probability 0.348693 embedding [0.64499, 0.45046, 0.36006, ...</cell><cell>column name value track_stem XC213642 track_type original start_time 55 species afmdov1 embedding [1.76885, 0.83265, 0.49710, ...</cell></row><row><cell>next_embedding</cell><cell>[0.86384, 0.81126, 0.26452, ...</cell></row><row><cell>track_embedding</cell><cell>[0.67923, 0.53318, 0.36459, ...</cell></row></table><note coords="5,306.81,599.46,192.09,7.98"><p>prediction_vec [-14.44950, -12.09912, -15.69485, ...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.98,90.49,396.78,266.89"><head>Table 3</head><label>3</label><figDesc>An overview of changes in the post-processed dataset (post).</figDesc><table coords="6,104.96,118.53,380.80,238.85"><row><cell cols="2">Dataset Source</cell><cell>Description</cell></row><row><cell>post v1</cell><cell>emb v3</cell><cell>Simplified dataset where embedding tokens come from the channel</cell></row><row><cell></cell><cell></cell><cell>with the highest number of positive classifications against the base-</cell></row><row><cell></cell><cell></cell><cell>line model. Multi-label generated by averaging pairs and triplets of</cell></row><row><cell></cell><cell></cell><cell>embedding tokens together.</cell></row><row><cell>post v2</cell><cell>emb v4</cell><cell>Tokens are now the average of the first and third tokens of each 5-</cell></row><row><cell></cell><cell></cell><cell>second interval. We generate current, next, and track embeddings</cell></row><row><cell></cell><cell></cell><cell>using only source-separated tracks. Tokens are multi-labeled by the</cell></row><row><cell></cell><cell></cell><cell>primary and secondary species associated with the track.</cell></row><row><cell>post v3</cell><cell>post v2</cell><cell>Augments above but the top-20 tokens in each species are averaged</cell></row><row><cell></cell><cell></cell><cell>against random no-call tokens to simplify train-test splits.</cell></row><row><cell>post v4</cell><cell>emb v4</cell><cell>Same methodology as post v2 and v3. Multi-label is generated by</cell></row><row><cell></cell><cell></cell><cell>confident baseline predictions filtered by plausible primary and sec-</cell></row><row><cell></cell><cell></cell><cell>ondary metadata labels.</cell></row><row><cell>post v5</cell><cell>emb v4</cell><cell>Same as post v4, but it fixes a modulo bug in previous averaged-token</cell></row><row><cell></cell><cell></cell><cell>datasets.</cell></row><row><cell>post v6</cell><cell>emb v4</cell><cell>Drops notion of multi-label prediction. It uses the original track and</cell></row><row><cell></cell><cell></cell><cell>the best source-separated track to increase the number of training</cell></row><row><cell></cell><cell></cell><cell>examples.</cell></row><row><cell>post v7</cell><cell>emb v4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.98,390.24,389.66,256.53"><head>Table 4</head><label>4</label><figDesc>A summary of experiments and their associated datasets.</figDesc><table coords="6,110.61,418.28,368.04,228.49"><row><cell>Model</cell><cell cols="2">Dataset Description</cell><cell>Public</cell><cell>Private</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Score</cell><cell>Score</cell></row><row><cell>Logistic Re-</cell><cell>emb v3</cell><cell>Baseline</cell><cell>0.78541</cell><cell>0.68369</cell></row><row><cell>gression</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell>emb v3</cell><cell>Baseline</cell><cell>0.74014</cell><cell>0.62283</cell></row><row><cell>XGBoost</cell><cell>post v1</cell><cell>Multi-label one-vs-rest strategy. Inter-</cell><cell>0.79068</cell><cell>0.68181</cell></row><row><cell></cell><cell></cell><cell>polated token pairs and triplets.</cell><cell></cell><cell></cell></row><row><cell>XGBoost</cell><cell>post v1</cell><cell>Same as above, but weighted samples</cell><cell>0.78829</cell><cell>0.68053</cell></row><row><cell></cell><cell></cell><cell>and native multi-label training.</cell><cell></cell><cell></cell></row><row><cell>XGBoost</cell><cell>post v5</cell><cell>Current interpolated-token.</cell><cell>0.7692</cell><cell>0.65937</cell></row><row><cell>XGBoost</cell><cell>post v5</cell><cell>Current and track interpolated-token.</cell><cell>0.7489</cell><cell>0.63059</cell></row><row><cell>XGBoost</cell><cell>post v3</cell><cell>Current, next, and track interpolated-</cell><cell>0.75049</cell><cell>0.63877</cell></row><row><cell></cell><cell></cell><cell>token.</cell><cell></cell><cell></cell></row><row><cell>XGBoost</cell><cell>post v3</cell><cell>Current and next interpolated-token.</cell><cell>0.76484</cell><cell>0.65346</cell></row><row><cell>XGBoost</cell><cell>post v7</cell><cell>Current concatenated token.</cell><cell>0.75997</cell><cell>0.64414</cell></row><row><cell>XGBoost</cell><cell>post v4</cell><cell>Ensemble of best logistic regression</cell><cell>0.75091</cell><cell>0.64242</cell></row><row><cell></cell><cell></cell><cell>and boost model.</cell><cell></cell><cell></cell></row><row><cell>Complement</cell><cell>post v7</cell><cell>Current prediction logit softmax vec-</cell><cell>0.71093</cell><cell>0.59652</cell></row><row><cell>Naive Bayes</cell><cell></cell><cell>tor.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,92.57,671.04,212.11,8.97"><p>Implementation at github.com/dsgt-birdclef/birdclef-2023</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to the <rs type="institution">Data Science at Georgia Tech (DS@GT) club</rs> for hosting our Kaggle competition team. Thanks to DS@GT leadership for publicizing recruitment, particularly <rs type="person">Krishi Manek</rs> as the Director of Projects. Thanks to <rs type="person">Erin Middlemas</rs> and <rs type="person">Grant Williams</rs> for their support and engagement as initial team members.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>def run_inference(path, embedding_func, prediction_func, clf, sr=48000, **kwargs): y, sr = librosa.load(path.as_posix(), sr=sr, mono=True) X = slice_seconds(y, sr, seconds=3, step=1) # drop every 4th/5th index, so we're not processing more than we need to # ,First pad the resulting slices by 2 X = np.pad(X, ((0, 2), (0, 0))) # then reshape it X = X.reshape(-1, 5, X.shape[-1]) # Now drop the last 2 seconds of each 5-second frame </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,210.55,394.53,10.91;12,112.66,224.10,393.33,10.91;12,112.66,237.65,393.32,10.91;12,112.66,251.20,61.18,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,203.49,224.10,103.02,10.91;12,336.06,224.10,169.92,10.91;12,112.66,237.65,71.11,10.91">Automated bird species identification in eastern africa</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Reers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cherutich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,191.70,237.65,314.29,10.91;12,112.66,251.20,29.26,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Overview of BirdCLEF</note>
</biblStruct>

<biblStruct coords="12,112.66,264.75,394.53,10.91;12,112.33,278.30,395.33,10.91;12,112.30,291.85,394.97,10.91;12,112.66,305.40,393.33,10.91;12,112.66,318.95,393.33,10.91;12,112.66,332.50,118.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,383.36,291.85,123.91,10.91;12,112.66,305.40,393.33,10.91;12,112.66,318.95,21.15,10.91">Overview of LifeCLEF 2023: evaluation of AI models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,155.82,318.95,350.17,10.91;12,112.66,332.50,44.89,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,346.05,393.33,10.91;12,112.66,359.59,275.66,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,305.12,346.05,200.87,10.91;12,112.66,359.59,89.67,10.91">Birdnet: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,211.05,359.59,99.32,10.91">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,373.14,393.32,10.91;12,112.66,386.69,235.52,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m" coord="12,256.51,373.14,249.47,10.91;12,112.66,386.69,105.51,10.91">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,400.24,393.33,10.91;12,112.66,413.79,394.53,10.91;12,112.66,427.34,287.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,286.68,400.24,219.31,10.91;12,112.66,413.79,76.31,10.91">Improving bird classification with unsupervised sound separation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,215.85,413.79,291.33,10.91;12,112.66,427.34,172.55,10.91">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="636" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,440.89,395.01,10.91;12,112.66,454.44,393.33,10.91;12,112.66,467.99,395.01,10.91;12,112.41,481.54,48.96,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,231.34,454.44,198.25,10.91">Spark sql: Relational data processing in spark</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,452.46,454.44,53.52,10.91;12,112.66,467.99,346.97,10.91">Proceedings of the 2015 ACM SIGMOD international conference on management of data</title>
		<meeting>the 2015 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1383" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,495.09,395.17,10.91;12,112.66,508.64,26.38,10.91" xml:id="b6">
	<analytic>
		<title/>
		<ptr target="https://luigi.readthedocs.io/en/stable/" />
	</analytic>
	<monogr>
		<title level="j" coord="12,112.66,495.09,23.03,10.91">Luigi</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2023-06-07">2023. 2023-06-07</date>
		</imprint>
	</monogr>
	<note>13 documentation</note>
</biblStruct>

<biblStruct coords="12,112.66,522.18,394.53,10.91;12,112.66,535.73,394.53,10.91;12,112.66,549.28,393.32,10.91;12,112.66,562.83,176.63,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,228.10,549.28,182.14,10.91">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,419.22,549.28,86.76,10.91;12,112.66,562.83,82.55,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,576.38,393.33,10.91;12,112.66,589.93,394.52,10.91;12,112.66,603.48,394.04,10.91;12,112.66,617.03,233.99,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,211.45,576.38,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785.doi:10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m" coord="12,421.80,576.38,84.19,10.91;12,112.66,589.93,394.52,10.91;12,112.66,603.48,36.69,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,630.58,394.53,10.91;12,112.66,644.13,395.17,10.91;12,112.66,657.68,395.17,10.91;13,112.66,86.97,394.53,10.91;13,112.28,100.52,394.42,10.91;13,112.66,114.06,224.44,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mechcoder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shcherbatyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vinícius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cereda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schwabedal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hvass-Labs</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Somanyusernamestaken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Es-Tève</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Besson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pfannschmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Linzberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cauet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fabisch</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1207017</idno>
		<idno>0.5.2</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1207017.doi:10.5281/zenodo.1207017" />
		<title level="m" coord="13,167.46,100.52,136.83,10.91">scikit-optimize/scikit-optimize</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,394.61,10.91;13,112.66,141.16,231.90,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.5275</idno>
		<title level="m" coord="13,234.23,127.61,273.05,10.91;13,112.66,141.16,55.03,10.91">An open dataset for research on audio field recording archives: freefield1010</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,154.71,395.17,10.91;13,112.66,168.26,145.71,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="13,330.41,154.71,177.42,10.91;13,112.66,168.26,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,181.81,393.61,10.91;13,112.66,195.36,256.32,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,389.68,181.81,116.59,10.91;13,112.66,195.36,87.80,10.91">Chaos as an intermittently forced linear system</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,209.17,195.36,107.22,10.91">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
