<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,85.02,398.94,15.72">A Deep Learning based Solution to FungiCLEF2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,114.16,47.78,10.93"><forename type="first">Feiran</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,149.42,114.16,54.62,10.93"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<email>wangpeng@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.40,114.16,59.39,10.93"><forename type="first">Yangyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.14,114.16,75.65,10.93"><forename type="first">Chenlong</forename><surname>Duan</surname></persName>
							<email>duancl@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.15,114.16,50.77,10.93"><forename type="first">Zijian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,439.27,114.16,36.21,10.93"><forename type="first">Yong</forename><surname>Li</surname></persName>
							<email>yong.li@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.90,128.10,67.56,10.93"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,85.02,398.94,15.72">A Deep Learning based Solution to FungiCLEF2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7E36A83CF5078468B260247B6108DBB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fungi Species Identification</term>
					<term>Fine-grained image recognition</term>
					<term>Open-Set</term>
					<term>Long-tailed</term>
					<term>Metadata</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The FungiCLEF2023 competition intends to foster the development of advanced algorithms for fungi species identification through the analysis of images and metadata, thereby making notable contributions to biodiversity conservation and human health. To overcome the inherent challenges posed by this competition, this paper introduces a deep learning based approach to solving this problem, which is based on the VOLO [1] backbone architecture with rich data augmentation and the loss function addressing data imbalance issues. We also discuss methods for utilizing metadata to address the issue of open-set recognition under limited model capacity. Our method achieves 54.34% on private leaderboard, which is the third place among the participators. The code is available at https://github.com/xiaoxsparraw/ CLEF2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization represents a fundamental and significant challenge in computer vision and pattern recognition, playing a crucial role in various practical applications <ref type="bibr" coords="1,492.57,397.12,11.32,9.97" target="#b1">[2]</ref>. The FungiCLEF2023 competition, held jointly as part of the LifeCLEF2023 lab in the CLEF2023 conference and the FGVC10 workshop organized in conjunction with the CVPR2023 conference, aims to advance the progress of robust algorithms for the identification of fungi species using image and metadata inputs. Achieving this objective carries profound implications for biodiversity conservation and plays a vital role in preserving human health.</p><p>Previous iterations of this competition have achieved remarkable performances accomplished by deep learning models <ref type="bibr" coords="1,204.65,491.97,11.49,9.97" target="#b2">[3,</ref><ref type="bibr" coords="1,219.28,491.97,7.52,9.97" target="#b3">4,</ref><ref type="bibr" coords="1,229.94,491.97,7.52,9.97" target="#b4">5,</ref><ref type="bibr" coords="1,240.61,491.97,7.52,9.97" target="#b5">6,</ref><ref type="bibr" coords="1,251.27,491.97,7.65,9.97" target="#b6">7]</ref>. In order to augment the practical significance of the competition and effectively address the concerns encountered by developers, scientists, users, and communities, the organizers introduce more constraints. Consequently, the challenges encountered in this year's competition can be summarized as follows:</p><p>• Fine-grained image recognition: Fine-grained image analysis has long presented a challenge within the FGVC workshop, prompting the need for further investigation and research.</p><p>• Open-Set recognition: Open-Set recognition constitutes a critical problem to be addressed in real-world applications. The FungiCLEF challenge's test dataset includes numerous species that do not appear in the training dataset. • Utilization of metadata: The incorporation of metadata assumes a vital role in the classification process, particularly in the identification of fungi species. Such metadata, commonly utilized by individuals in their everyday lives, necessitates utilization of location-based information in deep learning models. • Long-tailed distribution: The prevalence of long-tailed distribution permeates numerous real-world scenarios, including the distribution of fungi species. Managing the challenges posed by the long-tailed nature of fungi species distribution calls for corresponding solutions. • Model size limitation: A strict constraint has been imposed on the model size, limiting it to a maximum of 1GB.</p><p>The challenges mentioned above pose significant difficulties that require to overcome. In light of this, this paper proposes a preliminary solution utilizing deep learning techniques. Our method is based on VOLO <ref type="bibr" coords="2,213.97,305.15,11.58,9.97" target="#b0">[1]</ref>, strong data augmentation techniques are utilized by us too. Besides, seesaw loss <ref type="bibr" coords="2,181.24,318.70,12.83,9.97" target="#b7">[8]</ref> helps to relieve the challenge of long-tailed distribution.</p><p>To recognize the open-set classes in test dataset, a post-processing method is taken by us, which means assign -1 prediction to examples that the model is uncertain about.</p><p>The subsequent sections of this paper offer overview of the key components. Section 2 provides a detailed explanation of the datasets, accompanied by examination of the evaluation metric employed. In Section 3, we discuss our proposed methodologies in detail. Section 4 presents the implementation details, coupled with analysis of the results and findings obtained. Lastly, in Section 5, we conclude this paper by summarizing the key insights derived from the study and discussing potential directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset and Evaluation Metric</head><p>Understanding of datasets and metrics is a fundamental requirement for effectively participating in a competition. In this section, we show our comprehension of the datasets utilized in the competition and provide an overview of the evaluation metrics selected by the competition organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dataset</head><p>The challenge dataset primarily relies on the data sourced from the Danish Fungi 2020 dataset <ref type="bibr" coords="2,492.15,575.74,11.28,9.97" target="#b8">[9]</ref>, comprising 295,938 training images, each corresponding to one of the 1,604 observed species, predominantly found within Denmark. All training samples passed an expert validation process, ensuring high-quality labels. Additionally, the dataset includes observation metadata encompassing details about habitat, substrate, time, location, etc. In conjunction with the training dataset, the organizers have provided a validation set. This set comprises 30,131 observations, encompassing 60,832 images, and spanning across 2,713 species. Importantly, this validation set includes observations collected throughout the entire year, encompassing diverse substrate It is important to acknowledge the presence of a significantly imbalanced long-tailed distribution within the given dataset, as illustrated in Figure <ref type="figure" coords="3,335.60,439.08,3.72,9.97" target="#fig_0">1</ref>. This distribution is characterized by a substantial disparity in the number of instances across different classes, with a few classes having a disproportionately large number of samples compared to the majority of classes. Furthermore, it should be noted that the test set includes multiple out-of-scope classes, which pose an additional challenge in the evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head class Tail class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metric</head><p>The FungiCLEF2023 recognition task incorporates several evaluation metrics, as shown by Equation <ref type="formula" coords="3,131.46,555.94,3.67,9.97" target="#formula_0">1</ref>, which cater to distinct decision problems. The primary objective is to minimize the empirical loss L for decision 𝑞(𝑥) across a set of observations 𝑥 and their corresponding true labels 𝑦. This optimization process is achieved by considering a cost function 𝑊 (𝑦, 𝑞(𝑥)) that quantifies the impact of the decision 𝑞(𝑥) on the true label 𝑦:</p><formula xml:id="formula_0" coords="3,246.59,615.29,260.05,33.71">L = 5 ∑︁ 𝑖=1 𝑊 (𝑘 𝑖 , 𝑞(𝑥 𝑖 )) .<label>(1)</label></formula><p>The first metric is the standard classification error. All species not represented in the training set should correctly be classified as an "unknown" category. The second one is the cost for confusing edible species for poisonous and vice versa. The third one is a user-focused loss composes of both the classification error and the poisonous confusion. The fourth one is the cost for confusing "unknown" species, that missing "unknown" species is higher than misclassifying species. The last one is an increasing the weight of performance on rare species. While improvements in average classification accuracy can be improved by focusing on the more common species, in some applications correct classification of rare species is of high importance. We set the misclassification cost proportional to the inverse frequency of the species in the training dataset. Note that if the species distribution was the same in the test set, this would be equivalent to macro-averaged accuracy; but it is not the case of the FungiCLEF2023 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will introduce our solution to FungiCLEF2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Augmentation</head><p>Data augmentation plays a crucial role in computer vision tasks, and it is an integral part of our methodology. In our method, we employ a set of fundamental image augmentation methods sourced from Albumentations <ref type="bibr" coords="4,264.56,331.81,16.17,9.97" target="#b9">[10]</ref>. These methods encompass a range of transformations, including RandomResizedCrop, Transpose, HorizontalFlip, VerticalFlip, ShiftScaleRotate, RandomBrightnessContrast, PiecewiseAffine, HueSaturationValue, OpticalDistortion, Elastic-Transform, Cutout, and GridDistortion. In addition to these standard augmentation techniques, we also incorporate data mixing augmentation methods, namely Mixup <ref type="bibr" coords="4,424.27,386.01,16.42,9.97" target="#b10">[11]</ref>, CutMix <ref type="bibr" coords="4,486.67,386.01,16.41,9.97" target="#b11">[12]</ref>, TokenMix <ref type="bibr" coords="4,136.33,399.56,16.16,9.97" target="#b12">[13]</ref>, and RandomMix <ref type="bibr" coords="4,234.70,399.56,16.16,9.97" target="#b13">[14]</ref>, during the course of the competition. These data mixing techniques provide effective regularization for the models by blending both images and labels, thereby mitigating the risk of overfitting to the training dataset. This approach helps to enhance the generalization capability of the models and improve their performance on unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone</head><p>Throughout the competition, we conducted exploration of various models, including both classical and state-of-the-art architectures, to develop robust solutions. These models encompassed Convolutional Neural Networks and Vision Transformers, which have demonstrated remarkable performance in computer vision tasks. Notable models employed during the competition include ResNet <ref type="bibr" coords="4,124.69,544.13,16.41,9.97" target="#b14">[15]</ref>, VOLO <ref type="bibr" coords="4,180.27,544.13,11.58,9.97" target="#b0">[1]</ref>, BEiT-v2 <ref type="bibr" coords="4,238.63,544.13,16.41,9.97" target="#b15">[16]</ref>, and ConvNeXt-v2 <ref type="bibr" coords="4,348.29,544.13,16.41,9.97" target="#b16">[17]</ref>. The implementation of these models was facilitated using the timm library <ref type="bibr" coords="4,288.01,557.68,16.09,9.97" target="#b17">[18]</ref>, which offers a versatile framework for model development and evaluation. Through extensive experimentation under various settings, we selected VOLO <ref type="bibr" coords="4,158.67,584.78,12.91,9.97" target="#b0">[1]</ref> as the backbone architecture for our final proposed method. This decision was based on the model's superior performance and its ability to effectively capture relevant visual features and patterns for fungi species recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization Procedure</head><p>Dealing with long-tailed recognition constitutes a significant challenge encountered in the competition. To address this issue, we conducted a thorough investigation of various techniques, leveraging the insights and methods proposed in BagofTricks-LT <ref type="bibr" coords="5,379.80,135.64,16.18,9.97" target="#b18">[19]</ref>. In our final submission, we strategically integrated the seesaw loss <ref type="bibr" coords="5,283.81,149.19,12.97,9.97" target="#b7">[8]</ref> as a fundamental component of our approach. The seesaw loss formulation is expressed as follows:</p><formula xml:id="formula_1" coords="5,227.33,185.55,279.31,66.06">𝐿 seesaw (z) = - 𝐶 ∑︁ 𝑖=1 𝑦 𝑖 log (̂︀ 𝜎 𝑖 ) , with ̂︀ 𝜎 𝑖 = 𝑒 𝑧 𝑖 ∑︀ 𝐶 𝑗̸ =𝑖 𝒮 𝑖𝑗 𝑒 𝑧 𝑗 + 𝑒 𝑧 𝑖 ,<label>(2)</label></formula><p>where z refers to the output obtained from the fully connected layer, 𝐶 represents the total number of classes, and 𝑦 𝑖 denotes the one-hot label of the image under consideration. The hyperparameters 𝒮 𝑖𝑗 are determined, taking into account the distribution characteristics inherent in the dataset. These hyper-parameters play a crucial role in guiding the training process and ensuring the appropriate handling of class imbalance during optimization.</p><p>In addition to the careful selection of loss functions, the choice of an optimizer and an appropriate learning rate decay strategy are crucial factors in the training of our models. For optimization, we utilize the AdamW <ref type="bibr" coords="5,262.84,354.41,18.07,9.97" target="#b19">[20]</ref> optimizer. To further enhance the convergence speed and overall performance, we incorporate the cosine learning rate decay technique <ref type="bibr" coords="5,487.99,367.95,17.99,9.97" target="#b20">[21]</ref> in conjunction with warmup techniques during the training process. This combination of strategies promotes more efficient and effective model convergence, leading to improved training outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Post-processing</head><p>In the context of open-set setting, accurately determining whether a test sample belongs to a known category is important for the deployment of a model. Vaze et al. <ref type="bibr" coords="5,398.31,471.88,17.76,9.97" target="#b21">[22]</ref> have shown that the classifier's ability to make a "none-of-above" decision is closely linked to its accuracy on closedset classes. They achieved state-of-the-art performance on existing open-set recognition (OSR) benchmarks by employing a more robust baseline model. However, in this year's challenge, due to limitations imposed by the model and training data, it has been challenging to train a baseline model in closed-set scenarios. Consequently, our focus has shifted towards exploring the utilization of metadata shown in Tabel 1, as an alternative approach.</p><p>Inspired by Aodha et al. <ref type="bibr" coords="5,213.27,566.72,16.42,9.97" target="#b22">[23]</ref>, we developed a prior model trained on a balanced training set, which we refer to as a Multi-Layer Perceptron (MLP), comprising three fully connected layers and employing dropout regularization. We tried to use the metadata of "countryCode", "level2Name", "Latitude" with "Longitude" and "Habitat" with "MetaSubstrate". For text and number, we utilized the text encoder of CLIP <ref type="bibr" coords="5,290.32,620.92,17.87,9.97" target="#b23">[24]</ref> and [𝑠𝑖𝑛(𝜋𝑥), 𝑐𝑜𝑠(𝜋𝑥)] to map, respectively. However, our attempts revealed that utilizing metadata did not yield improvements in the final outcomes. We attribute this to the insufficiency of the baseline model's robustness and its limited resistance to perturbations. After threshold processing, it still led to confusion The location information of latitude and longitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level2Name</head><p>The position information of the observer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Habitat</head><p>Observations of the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substrate</head><p>Natural substance on which to live, such as bark, soil, etc.</p><p>between unknown and known classes. Therefore, we only use a simple threshold for this year competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will introduce our implementation details and main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>The proposed methodology has been developed using the PyTorch framework <ref type="bibr" coords="6,435.36,345.84,16.15,9.97" target="#b24">[25]</ref>. All models utilized in our approach have pre-training on the ImageNet dataset <ref type="bibr" coords="6,407.08,359.39,16.42,9.97" target="#b25">[26]</ref>, which is readily available within the timm library <ref type="bibr" coords="6,244.68,372.93,16.41,9.97" target="#b17">[18]</ref>. Fine-tuning of these models was performed using 4 Nvidia RTX3090 GPUs. The initial learning rate was set to 2 × 10 -5 , and the total number of training epochs was set to 15, with the first epoch dedicated to warm-up by employing a learning rate of 2 × 10 -7 . For optimal model training, we employed the AdamW optimizer <ref type="bibr" coords="6,488.23,413.58,17.76,9.97" target="#b26">[27]</ref> in conjunction with a cosine learning rate scheduler <ref type="bibr" coords="6,317.82,427.13,16.09,9.97" target="#b20">[21]</ref>, with the weight decay set to 2 × 10 -5 . During inference on the test dataset, test time augmentation was incorporated. Additionally, considering that an observation may consist of multiple images, we adopted a simple averaging approach to obtain a single prediction for each observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>In this section, we present the main findings obtained during the challenge, as depicted in Table <ref type="table" coords="6,115.79,531.06,3.74,9.97" target="#tab_1">2</ref>. The column labeled "Metric" in the table represents the F1 score on the leaderboard. Due to the substantial size of the FungiCLEF dataset and inherent constraints in terms of our energy and hardware, our experimentation on FungiCLEF was limited. Furthermore, certain techniques <ref type="bibr" coords="6,140.75,571.70,18.00,9.97" target="#b27">[28]</ref> that demonstrated effectiveness in SnakeCLEF during our participation were not employed in FungiCLEF. Initially, we began with the implementation of ResNet <ref type="bibr" coords="6,451.15,585.25,16.09,9.97" target="#b14">[15]</ref>, and the outcomes indicated that weight cross entropy loss did not contribute significantly to the task. In our preliminary experiments, the inclusion of metadata yielded performance improvements on the leaderboard. However, in more robust models, the incorporation of metadata failed to deliver performance enhancements.</p><p>We also incorporated the utilization of Seesaw loss <ref type="bibr" coords="6,333.48,653.00,13.00,9.97" target="#b7">[8]</ref> and CutMix <ref type="bibr" coords="6,405.86,653.00,18.07,9.97" target="#b11">[12]</ref> in the FungiCLEF task, which proved to be effective. These techniques alleviated the challenges posed by the  <ref type="bibr" coords="7,183.72,151.49,16.46,8.87" target="#b14">[15]</ref> 224 × 224 40.64 weight CE loss BEiT-v2-B <ref type="bibr" coords="7,184.78,163.45,16.46,8.87" target="#b15">[16]</ref> 224 × 224 50.40 stronger backbone BEiT-v2-B <ref type="bibr" coords="7,184.78,175.40,16.46,8.87" target="#b15">[16]</ref> 224 × 224 51.49 metadata VOLO <ref type="bibr" coords="7,179.58,187.36,11.83,8.87" target="#b0">[1]</ref> 448 × 448 52.65 seesaw loss VOLO <ref type="bibr" coords="7,179.58,199.31,11.83,8.87" target="#b0">[1]</ref> 448 × 448 53.93 seesaw loss + cutmix VOLO <ref type="bibr" coords="7,179.58,211.27,11.83,8.87" target="#b0">[1]</ref> 448 long-tailed distribution and enhanced the generative capability of the models. As shown in Table <ref type="table" coords="7,115.20,273.35,3.66,9.97" target="#tab_1">2</ref>, we observed that as the resolution increased, the metric score plateaued when reaching a resolution of 448. Based on our experiments detailed in Table <ref type="table" coords="7,363.20,286.90,3.66,9.97" target="#tab_1">2</ref>, our final submission employed VOLO <ref type="bibr" coords="7,119.48,300.45,12.84,9.97" target="#b0">[1]</ref> as the backbone model, without the utilization of any metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Fine-grained visual analysis continues to present significant challenges, particularly in the domain of fungi species recognition, where the shared visual characteristics among different species, coupled with their prevalence in humid environments, contribute to the complexity of the task. In our approach, we solely rely on image data for predictions, disregarding the available metadata entirely. The open-set problem and the integration of metadata and visual data present a persistent challenge that demands further research and exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,311.50,416.71,9.21;3,89.29,323.45,416.71,8.87;3,89.29,335.41,231.90,8.87;3,89.29,88.23,416.70,215.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Long-tailed distribution of the FungiCLEF2023 training dataset. The classes are arranged in descending order based on the number of samples in each category, with the color of blue representing the head class and the orange representing the tail class.</figDesc><graphic coords="3,89.29,88.23,416.70,215.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.64,298.92,69.71"><head>Table 1</head><label>1</label><figDesc>Description of the provided metadata.</figDesc><table coords="6,133.42,122.10,254.49,38.25"><row><cell>Tags</cell><cell>Description</cell></row><row><cell>CountryCode</cell><cell>Country information.</cell></row><row><cell>Latitude, Longitude</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.64,333.33,69.71"><head>Table 2</head><label>2</label><figDesc>Results of FungiCLEF.</figDesc><table coords="7,142.47,122.10,279.85,38.25"><row><cell>Backbone</cell><cell cols="2">Resolution Metric (%)</cell><cell>Comments</cell></row><row><cell>ResNet50 [15]</cell><cell>224 × 224</cell><cell>43.19</cell><cell>CE loss</cell></row><row><cell>ResNet50</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,485.45,394.53,9.97;7,112.66,498.99,319.01,9.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,301.13,485.45,201.49,9.97">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,112.66,498.99,287.09,9.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,512.54,395.17,9.97;7,112.66,526.09,393.32,9.97;7,112.28,539.64,247.52,9.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,484.65,512.54,23.18,9.97;7,112.66,526.09,244.38,9.97">Finegrained image analysis with deep learning: A survey</title>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,369.55,526.09,136.43,9.97;7,112.28,539.64,153.45,9.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="8927" to="8948" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,553.19,393.32,9.97;7,112.66,565.74,360.95,10.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,346.39,553.19,159.60,9.97;7,112.66,565.74,214.29,10.97">Overview of fungiclef 2022: Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.78,566.74,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,580.29,393.33,9.97;7,112.66,593.84,311.04,9.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,477.63,580.29,28.36,9.97;7,112.66,593.84,163.50,9.97">Bag of tricks and a strong baseline for FGVC</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,285.87,593.84,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,607.39,393.32,9.97;7,112.66,620.94,338.97,9.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,434.88,607.39,71.10,9.97;7,112.66,620.94,192.77,9.97">Does closed-set training generalize to open-set recognition?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zining</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weiqiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,313.81,620.94,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,633.48,393.33,10.97;7,112.66,648.04,393.32,9.97;7,112.66,661.59,229.85,9.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,444.83,633.48,61.16,10.91;7,112.66,648.04,393.32,9.97;7,112.66,661.59,83.24,9.97">Classification of fungi species: A deep learning based image feature extraction and gradient boosting ensemble approach</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Desingu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Chodisetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bharathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,204.69,661.59,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,86.97,393.33,10.97;8,112.66,101.52,181.25,9.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,203.02,86.97,302.96,10.97;8,112.66,101.52,35.11,9.97">Transformer-based fine-grained fungi classification in an open-set scenario</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,156.09,101.52,105.91,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,115.07,394.52,9.97;8,112.66,128.62,393.58,9.97;8,112.66,142.17,351.04,9.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,112.66,128.62,228.32,9.97">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,370.85,128.62,135.38,9.97;8,112.66,142.17,252.92,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,155.72,394.53,9.97;8,112.66,169.26,393.32,9.97;8,112.66,182.81,394.87,9.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,112.66,169.26,281.25,9.97">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.58,169.26,85.40,9.97;8,112.66,182.81,297.56,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,196.36,394.52,9.97;8,112.28,208.91,370.34,10.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,112.28,208.91,245.27,10.97">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,366.37,209.91,53.51,9.97">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,223.46,395.16,9.97;8,112.66,237.01,309.29,9.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,330.70,223.46,177.12,9.97;8,112.66,237.01,16.17,9.97">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,151.43,237.01,240.50,9.97">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,250.56,393.32,9.97;8,112.66,263.11,393.32,10.97;8,112.66,277.66,297.29,9.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,329.08,250.56,176.90,9.97;8,112.66,263.11,185.21,10.97">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,320.83,264.11,185.15,9.97;8,112.66,277.66,199.17,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,291.21,393.32,9.97;8,112.66,304.76,394.52,9.97;8,112.66,318.31,123.33,9.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,293.68,291.21,212.31,9.97;8,112.66,304.76,166.74,9.97">Tokenmix: Rethinking image mixing for data augmentation in vision transformers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,307.87,304.76,194.78,9.97">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="455" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,331.86,393.32,9.97;8,112.26,345.40,302.07,9.97" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,252.17,331.86,253.81,9.97;8,112.26,345.40,119.14,9.97">Randommix: A mixed sample data augmentation method with multiple mixed modes</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08728</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,358.95,395.16,9.97;8,112.66,372.50,395.00,9.97;8,112.41,386.05,38.81,9.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,251.58,358.95,194.91,9.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.26,358.95,38.56,9.97;8,112.66,372.50,348.71,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,399.60,395.17,9.97;8,112.66,413.15,301.63,9.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,298.15,399.60,209.68,9.97;8,112.66,413.15,119.51,9.97">Beit v2: Masked image modeling with vectorquantized visual tokenizers</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.06366</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,426.70,393.33,9.97;8,112.66,440.25,395.16,9.97;8,112.66,453.80,330.50,9.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,386.45,426.70,119.53,9.97;8,112.66,440.25,208.51,9.97">Convnext v2: Co-designing and scaling convnets with masked autoencoders</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,344.08,440.25,163.74,9.97;8,112.66,453.80,222.23,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="16133" to="16142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,467.35,8.98,9.97;8,139.07,467.35,51.02,9.97;8,211.20,467.35,35.67,9.97;8,264.31,467.35,27.43,9.97;8,309.18,467.35,34.79,9.97;8,365.09,467.35,141.60,9.97;8,112.66,480.90,129.46,9.97" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<title level="m" coord="8,211.20,467.35,35.67,9.97;8,264.31,467.35,27.43,9.97;8,309.18,467.35,29.82,9.97">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,494.45,393.32,9.97;8,112.66,506.99,393.33,10.97;8,112.66,521.54,147.72,9.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,270.40,494.45,235.58,9.97;8,112.66,507.99,154.49,9.97">Bag of tricks for long-tailed visual recognition with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,289.80,506.99,216.19,10.97;8,112.66,521.54,50.10,9.97">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3447" to="3455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,535.09,395.16,9.97;8,112.66,548.64,178.99,9.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,218.88,535.09,171.50,9.97">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,413.17,535.09,94.66,9.97;8,112.66,548.64,148.97,9.97">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,562.19,394.61,9.97;8,112.66,575.74,270.52,9.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,230.71,562.19,251.88,9.97">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,575.74,240.49,9.97">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,588.29,393.53,10.97;8,112.66,602.84,355.77,9.97" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,292.43,588.29,213.77,10.97;8,112.66,602.84,61.95,9.97">Open-set recognition: A good closed-set classifier is all you need</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,197.91,602.84,240.50,9.97">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,615.39,393.33,10.97;8,112.66,628.93,393.52,10.97;8,112.30,643.49,124.54,9.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,259.01,615.39,246.98,10.97;8,112.66,628.93,57.10,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,193.33,629.94,312.85,9.97;8,112.30,643.49,26.65,9.97">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,657.04,394.52,9.97;8,112.66,670.58,393.32,9.97;9,112.66,87.97,394.52,9.97;9,112.66,101.52,22.69,9.97" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="8,237.25,670.58,268.73,9.97;9,112.66,87.97,50.66,9.97">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,186.65,87.97,211.00,9.97">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,115.07,394.52,9.97;9,112.66,128.62,394.52,9.97;9,112.66,142.17,394.52,9.97;9,112.66,155.72,393.32,9.97;9,112.66,169.26,252.86,9.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,371.35,142.17,135.83,9.97;9,112.66,155.72,175.42,9.97">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,310.67,155.72,195.31,9.97;9,112.66,169.26,36.36,9.97">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,182.81,393.33,9.97;9,112.66,196.36,393.32,9.97;9,112.66,209.91,174.72,9.97" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="9,346.64,182.81,159.35,9.97;9,112.66,196.36,67.18,9.97">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,203.77,196.36,302.21,9.97;9,112.66,209.91,86.75,9.97">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,223.46,332.13,9.97" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m" coord="9,219.53,223.46,193.34,9.97">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,237.01,393.32,9.97;9,112.39,250.56,393.59,9.97;9,112.66,264.11,136.29,9.97" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="9,458.50,237.01,47.48,9.97;9,112.39,250.56,229.90,9.97">Watch out venomous snake species: A solution to snakeclef</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.88,250.56,141.10,9.97;9,112.66,264.11,105.59,9.97">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
