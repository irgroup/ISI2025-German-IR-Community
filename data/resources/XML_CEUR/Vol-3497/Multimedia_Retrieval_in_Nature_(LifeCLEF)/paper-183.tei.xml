<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,331.36,15.42;1,89.29,106.66,354.98,15.42;1,89.29,128.58,252.24,15.43">PlantCLEF2023: A Bigger Training Dataset Contributes More than Advanced Pretraining Methods for Plant Identification</title>
				<funder ref="#_2Hz5P72">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_UMYUqzr">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
				<funder ref="#_mHPSdBR">
					<orgName type="full">Korea Smart Farm R&amp;D Foundation (KosFarm)</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, and Forestry</orgName>
					<orgName type="abbreviated">IPET</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Agriculture, Food and Rural Affairs</orgName>
					<orgName type="abbreviated">MAFRA</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Science and ICT (MSIT)</orgName>
				</funder>
				<funder ref="#_PjDrSvg">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
				<funder ref="#_tGT8xdE">
					<orgName type="full">Rural Development Administration</orgName>
					<orgName type="abbreviated">RDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,50.82,11.96"><forename type="first">Mingle</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Core Research Institute of Intelligent Robots</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.02,156.89,51.03,11.96"><forename type="first">Sook</forename><surname>Yoon</surname></persName>
							<email>syoon@mokpo.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Mokpo National University</orgName>
								<address>
									<postCode>58554</postCode>
									<settlement>Jeonnam</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.65,156.89,67.37,11.96"><forename type="first">Chenmou</forename><surname>Wu</surname></persName>
							<email>chenmou@jbnu.ac.kr</email>
							<affiliation key="aff3">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<settlement>Jeonju</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.64,156.89,79.12,11.96"><forename type="first">Jeonghyun</forename><surname>Baek</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Rural Development Administration</orgName>
								<address>
									<postCode>54875</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.52,156.89,73.08,11.96"><forename type="first">Dong</forename><forename type="middle">Sun</forename><surname>Park</surname></persName>
							<email>dspark@jbnu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Core Research Institute of Intelligent Robots</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,331.36,15.42;1,89.29,106.66,354.98,15.42;1,89.29,128.58,252.24,15.43">PlantCLEF2023: A Bigger Training Dataset Contributes More than Advanced Pretraining Methods for Plant Identification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">4D3899C501926AB4147AA4DB6EFAA9BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>multi-observation</term>
					<term>multimodal pretraining</term>
					<term>vision transformer</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Plant identification has received a significant improvement in recent years with the development of deep learning. However, large-scale plant identifications still suffer such as the PlantCLEF2022 challenge, and this paper aims to address this issue in the PlantCLEF2023 challenge. On one hand, we extend an effective strategy, transfer learning, resorting to advanced pretrained methods with multi-modal and self-supervised learning. On the other hand, training dataset size and quality are probed and their impacts are compared to one of the advanced pretrained methods. The experimental results suggest that advanced pretrained methods, bigger datasets, and noisy datasets are beneficial to plant identification. Especially, the performance gain from the first one is inferior to the counterparts of the latter two, with the dataset in the PlantCLEF2023 challenge. For example, employing a noisy dataset together receives 0.0236 gains in performance whereas multimodal and self-supervised pretraining contributes 0.00792 gains. We hope that our results highlight the importance of collecting better datasets for plant identification. Our codes are public at https://github.com/xml94/PlantCLEF2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Plant identification is an essential task in plant science, such as for maintaining biodiversity and finding new plant species, which traditionally requires the involvement of human experts. Because of the limitation of experts, plant identification is often not convenient and expensive. Besides, it is also difficult to integrate the identification with other tasks. Resorting to RGB images and computer vision methods, plant identification has achieved a significant improvement in recent years. A commonly embraced paradigm is taking plant identification from the computer vision community, with a basic inspiration that plants images resemble the images in general computer vision tasks such as ImageNet-1k <ref type="bibr" coords="1,285.10,591.99,11.43,10.91" target="#b0">[1]</ref>.</p><p>Holding the paradigm, the methods and techniques in general computer vision tasks are overwhelmingly borrowed to plant identification. Transfer learning is proven as one of the most successful strategies, which directly adopts the models trained in the datasets from general computer vision tasks. This paper extends this idea one more step by using advanced pretrained methods. To be specific, a ViT-large model pretrained in multi-modal and self-supervised is fine-tuned in the PlantCLEF2023 dataset. Multi-modal refers to using inputs in different modality <ref type="bibr" coords="2,132.29,168.26,11.47,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,146.49,168.26,7.51,10.91" target="#b2">3,</ref><ref type="bibr" coords="2,156.74,168.26,7.64,10.91" target="#b3">4]</ref>, such as text and image <ref type="bibr" coords="2,276.32,168.26,11.56,10.91" target="#b4">[5]</ref>, by which more semantics are learned, whereas self-supervised learning denotes a set of training strategies without using labels. One of the most successful self-supervised learning methods is predicting the masked contents from the original inputs <ref type="bibr" coords="2,156.55,208.91,11.26,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,170.54,208.91,7.44,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,180.71,208.91,7.51,10.91" target="#b2">3]</ref>. Finally, we examine the effectiveness of the pretraining with multi-modal and masked image modeling in the context of plant identification.</p><p>Simultaneously, we aim to point out the heterogeneity between general computer vision and plant identification and argue that plant identification has its specificity <ref type="bibr" coords="2,419.97,249.56,11.58,10.91" target="#b7">[8]</ref>. For example, a plant may be very similar to another plant. However, this heterogeneity should receive more attention ranging from dataset collecting and algorithm designing, although some existing papers cast plant identification into a fine-grained classification. In this paper, we aim to probe the impact of size and quality of training datasets to identify plant species, and compare the impacts to the one with advanced pretraining methods. We emphasize that the comparison is our ultimate objective, rather than probing training dataset alone, in that scaling laws, enlarging capacities of models and sizes of training datasets, already suggests its power in natural language processing <ref type="bibr" coords="2,89.29,357.95,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="2,103.37,357.95,12.55,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,118.65,357.95,14.03,10.91" target="#b10">11]</ref> and computer vision <ref type="bibr" coords="2,229.63,357.95,11.36,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,243.71,357.95,12.32,10.91" target="#b11">12]</ref>.</p><p>Through experiments in PlantCLEF2023, we found that advanced pretraining methods slightly contribute to plant identification, and a bigger dataset and noisy datasets may be more beneficial. As shown in Table <ref type="table" coords="2,174.51,398.60,3.76,10.91">4</ref>, employing noisy datasets together receives 0.0236 gains in performance whereas multimodal and self-supervised pretraining contributes 0.00792 improvements. Based on this observation, we think that collecting a better dataset and analyzing the challenges within a collected dataset should receive more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Material and Evaluation Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>Trusted and Web datasets. The PlantCLEF2023 challenge <ref type="bibr" coords="2,361.68,518.84,16.55,10.91" target="#b12">[13,</ref><ref type="bibr" coords="2,381.01,518.84,14.11,10.91" target="#b13">14]</ref> adopts the same dataset as PlantCLEF2022 and the dataset officially consists of three parts, Trusted, Web, and Test. We summarize some details and our analysis and understanding in Table <ref type="table" coords="2,420.63,545.94,3.80,10.91" target="#tab_0">1</ref>. The Trusted and Web datasets have labels and can be utilized to train proposed models but their reliabilities are different. The Trusted dataset is annotated by experts whereas the Web one is collected from the internet with some other post-processes to improve the quality.</p><p>We aim to analyze the Trusted and Web datasets and found distinct characteristics. First, the two datasets are both in heavy class-imbalance where some classes have much more number of images than other classes as shown in Figure <ref type="figure" coords="2,339.19,627.23,5.02,10.91" target="#fig_1">2</ref> and Table <ref type="table" coords="2,392.48,627.23,5.02,10.91" target="#tab_1">2</ref> and<ref type="table" coords="2,419.19,627.23,3.70,10.91" target="#tab_2">3</ref>, which may result in lower performance for the classes with less number of images <ref type="bibr" coords="2,371.86,640.78,16.09,10.91" target="#b14">[15]</ref>. Second, the datasets have huge intra-class variation <ref type="bibr" coords="2,215.70,654.33,16.30,10.91" target="#b15">[16,</ref><ref type="bibr" coords="2,234.60,654.33,13.95,10.91" target="#b14">15]</ref> in two factors: a plant species has multiple organs, such as flowers and leaves; and images from an organ may have heterogeneity, such as multiscale and growth stage as shown in Figure <ref type="figure" coords="3,238.95,573.22,3.79,10.91" target="#fig_0">1</ref>. Third, the datasets have relatively less number of images averaged by the classes and can be cast as a few-shot classification. For example, the Trusted and Web datasets have approximately 36 and 18 images on average for each class. Through our observation, the difference between the Trusted and Web datasets may be the distribution of the number of images in one species, as shown in Figure <ref type="figure" coords="3,422.32,627.41,3.81,10.91" target="#fig_2">3</ref>. Several random observations suggest that most of the images in the Web dataset are correctly annotated. Therefore, the two datasets are combined as a potential training dataset with much more images and less impact of class imbalance <ref type="bibr" coords="3,243.86,668.06,16.25,10.91" target="#b14">[15]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test dataset and observation</head><p>The Test dataset has no public labels and is formally adopted to evaluate different methods. It has 55,306 images for 26,868 individual plants, termed observations where a plant may have multiple images. According to the official report <ref type="bibr" coords="4,409.60,572.48,16.09,10.91" target="#b17">[18]</ref>, the images cover 7,339 plant species, less than the total numbers in the Trusted and Web datasets, but the exact information is not accessible. The observation-level classification requires the ability to make decisions considering multiple predictions from a model, which may be beneficial to have higher performance to recognize plant species. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation metric</head><p>Macro averaged mean reciprocal rank (MA-MRR) is utilized to evaluate different submissions for the PlantCLEF2022 challenge. The challenge requests a submission with a rank based on the score with a given length for each testing observation, and the rank is thirty for the challenge. Assume there are ğ‘ classes in the testing dataset and class ğ‘› has ğ‘‚ ğ‘› observations. Mathematically, MA-MRR can be formalized as 1,2 :</p><formula xml:id="formula_0" coords="5,218.76,595.35,287.22,33.74">ğ‘€ ğ´ -ğ‘€ ğ‘…ğ‘… = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘›=1 1 ğ‘‚ ğ‘› ğ‘‚ğ‘› âˆ‘ï¸ ğ‘–=1 1 ğ‘Ÿ ğ‘– ,<label>(1)</label></formula><p>where ğ‘Ÿ ğ‘– refers to the rank position of the first relevant ground-truth label for the ğ‘–-th observation from one class. Conceptually, if all of the observations are classified correctly in the first, then MA-MRR is one where the accuracy is hundred percent. In contrast, if MA-MRR is smaller then the model is worse. Intuitively, the observation level enables us to observe different parts of plants, while MA-MRR allows more than one chance to recognize plant species for a specific image, 30 chances in the PlantCLEF2023 challenge. In general, MA-MRR is slightly higher than the actual accuracy, a widely used metric to evaluate image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, a combination method for observation-level image classification is first described. Second, our two main strategies, relevant to and fine-tuning, for the PlantCLEFF2022 challenge introduced. Our results are summarized in Table <ref type="table" coords="6,369.41,460.93,3.74,10.91">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Strategy towards observation</head><p>Let ğ‘Ÿ ğ‘— ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ denote the testing probability score rank of the ğ‘—-th image of one observation. Similarly, ğ‘Ÿ ğ‘— ğ‘ğ‘™ğ‘ğ‘ ğ‘  is the testing class rank with the same length. Besides, ğ‘Ÿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘– and ğ‘Ÿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘– are the ğ‘–-th class-score pair and mean the class and the corresponding probability score, respectively. The rank requires ğ‘Ÿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ &gt; ğ‘Ÿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ if ğ‘ âˆˆ R + &lt; ğ‘ âˆˆ R + . A final desired output {ğ‘Ÿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ , ğ‘Ÿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  }, pair of class and corresponding score can be formulated as</p><formula xml:id="formula_1" coords="6,201.52,586.79,304.46,15.82">{ğ‘Ÿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  , ğ‘Ÿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ } = {(âˆª ğ‘› ğ‘—=1 ({ğ‘Ÿ ğ‘— ğ‘ğ‘™ğ‘ğ‘ ğ‘  , ğ‘Ÿ ğ‘— ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ })),<label>(2)</label></formula><p>where { means an integration function and the observation has ğ‘› corresponding images. âˆª suggests the union of the ğ‘› images. Following our observation in the last year <ref type="bibr" coords="6,295.83,640.97,16.41,10.91" target="#b16">[17]</ref>, the multi-sorted method contributes to a relatively higher performance, where multi means that the final rank pair is from multiple images, rather than a single image, and sorted means that the predicted species with higher Table <ref type="table" coords="7,116.06,90.49,5.12,8.93">4</ref> Our official results, the best showing in the boldface. We fine-tune the models in the PlantCLEF2023 dataset. Except for run 9 and 10, fine-tuned by 10 and 50 epochs respectively, all models are fine-tuned by 100 epochs, following MAE. Same with our last year's experiment <ref type="bibr" coords="7,367.89,126.40,14.79,8.87" target="#b16">[17]</ref>, we executed the experiments with the code of MAE, which means the same training strategies. The Trusted and Web datasets are short as T and W. The number after num suggests the statistics term. For example, T@num36 denotes the images from the classes with exact or more than 36 images. <ref type="bibr" coords="7,345.60,162.27,15.99,8.87">MAE</ref> probabilities after sorting all predictions from all images aligning to the same observation are adopted to evaluate. With the multi-sorted strategy, the scores of all images from the same observation are sorted: ğ‘ ğ‘œğ‘Ÿğ‘¡(âˆª ğ‘› ğ‘—=1 âˆª ğ‘™ ğ‘–=1 ğ‘Ÿ ğ‘— ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘– ) where ğ‘™ is the length of the ranges (30 in the PlantCLEF2023 challenge); and then, after removing duplicates of same classes, the first required length of class-score pair are taken as the final ranking pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Advanced pretraining models</head><p>As mentioned before, the PlantCLEF2023 challenge can be recognized as a few-shot image classification. Even if the Trusted and Web datasets are combined, each class has only about 50 images on average, much less than the counterpart in the ImageNet <ref type="bibr" coords="7,388.35,523.52,12.71,10.91" target="#b0">[1]</ref> dataset. To address this issue, employing a pretrained model is one of the most effective and efficient methods across many different tasks <ref type="bibr" coords="7,179.39,550.62,16.30,10.91" target="#b15">[16,</ref><ref type="bibr" coords="7,197.77,550.62,12.23,10.91" target="#b14">15]</ref>, especially in the eras of large language models such as ChatGPT <ref type="bibr" coords="7,493.30,550.62,12.68,10.91" target="#b6">[7]</ref> and foundation models <ref type="bibr" coords="7,192.27,564.17,11.28,10.91" target="#b1">[2]</ref>. Therefore, we aim to probe the impact of state-of-the-art pretrained models.</p><p>MAE <ref type="bibr" coords="7,125.67,591.26,12.82,10.91" target="#b5">[6]</ref> is self-supervised trained in ImageNet-1k (IN1k), label information not being used, where the input is randomly blocked and the objective is reconstructing the blocked pixels, as shown in Figure <ref type="figure" coords="7,182.11,618.36,3.81,10.91" target="#fig_3">4</ref>. We employed MAE with ViT-large <ref type="bibr" coords="7,361.05,618.36,18.07,10.91" target="#b18">[19]</ref> model in the last year and obtained a first place for the PlantCLEF2022 challenge <ref type="bibr" coords="7,325.72,631.91,16.08,10.91" target="#b16">[17]</ref>. Another widely adopted pretraining strategy involves multi-modal, such as CLIP <ref type="bibr" coords="7,288.17,645.46,11.44,10.91" target="#b4">[5]</ref>, in which the learning loss is to maximize the relationships between paired text and images and minimize the counterparts of the unpaired  <ref type="bibr" coords="8,221.99,269.44,12.14,8.87" target="#b5">[6]</ref>,can be adopted as pre-training method without using annotations. ones. As shown in Figure <ref type="figure" coords="8,205.18,591.42,3.75,10.91" target="#fig_4">5</ref>, this pretraining employs multimodal information simultaneously and thus may learn semantics better than using vision information alone <ref type="bibr" coords="8,416.14,604.97,11.43,10.91" target="#b2">[3]</ref>.</p><p>MVP <ref type="bibr" coords="8,124.90,618.52,12.77,10.91" target="#b2">[3]</ref> integrates the strategies in MAE and CLIP to improve the pretraining performance. As shown in Figure <ref type="figure" coords="8,175.39,632.07,3.66,10.91" target="#fig_5">6</ref>, MVP freezes the image encoder from CLIP and trains the parameters from the vision part. To be specific, the loss function is to minimize the distance between the feature from the frozen CLIP image encoder and the feature from the vision model with randomly  blocked images as inputs. In this way, a vision model in MVP can learn more semantics from the frozen CLIP image encoder that is pretrained in 400 million image-text pairs. Furthermore, as highlighted in Figure <ref type="figure" coords="9,198.32,556.58,3.76,10.91" target="#fig_6">7</ref>, EVA <ref type="bibr" coords="9,230.92,556.58,12.88,10.91" target="#b3">[4]</ref> scales MVP by using bigger models with more parameters and more datasets, by which better performance is achieved across different downstream tasks.</p><p>Inspired by the success of EVA, we extend it from general computer vision tasks to plant species identification on a large scale. Because of the limitation of GPUs, we only consider the scaling of EVA with more datasets and the model architecture is still ViT-large <ref type="bibr" coords="9,432.49,610.77,16.09,10.91" target="#b18">[19]</ref>. Specifically, EVA-L pretrained in ImageNet-21k, rather than ImageNet-1k, is leveraged, which is denoted EVA-IN21k. We also utilized its finetuned version, finetuned once more in ImageNet-21k in a supervised manner, mainly because it has better performance in ImageNet-1k and empirically, a model with higher performance in ImageNet-1k may have a better performance in the target dataset with transfer learning strategy <ref type="bibr" coords="10,265.72,86.97,16.39,10.91" target="#b19">[20]</ref>. This pretrained model is denoted as EVA-IN21k-IN21k.</p><p>As shown in Table <ref type="table" coords="10,190.64,114.06,3.81,10.91">4</ref>, EVA-IN21k pretrained model outperforms MAE-IN1k slightly and EVA-IN21k-IN21k has also a tiny improvement. Our results suggest that better pretrained models and better performance in ImageNet-1k may have superiority in plant identification, same observations in <ref type="bibr" coords="10,185.78,154.71,16.25,10.91" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bigger training dataset</head><p>Considering that the Trusted dataset has 80,000 classes but the Test dataset only has 7,339 classes, training models in the whole Trusted dataset seems a waste of computing resources. A desired strategy is to find the class in the Test dataset and then just use the corresponding images in the Trusted dataset. However, the desired classes are not given. Instead, we assume that the frequencies of plant species in the Trusted and Test datasets are similar. Therefore, we just choose the classes from the Trusted dataset based on the number of samples. For example, those classes having 100 images or more are selected, denoted as T@num100, as shown in Table <ref type="table" coords="10,89.04,299.28,3.74,10.91">4</ref>.</p><p>To our surprise, the test performance monotonically shrinks when the number of images reduces. Especially, training the model in the whole Trusted dataset leads to the best performance. We emphasize that this phenomenon may result from two factors: the number of images and the number of classes. When the number of images increases, some classes in the Test dataset may be included. However, these classes just have a few images, such as less than 7 if the training dataset replaces T@num7 with T.</p><p>We further directly utilized the Web dataset along with the Trusted because of our previous observation that the main difference is the frequency as shown in Figure <ref type="figure" coords="10,411.93,407.68,3.70,10.91" target="#fig_2">3</ref>. The corresponding performance is further improved by 3.6% than using the Trusted alone. As the Web dataset is assumed to have noise, the trained model with the Trusted and Web datasets undergoes fine-tuning once more, 10 and 50 epochs as run ID 9 and 10 respectively. We are shocked that the performance degrades with the fine-tuning process. We conjecture that most of the images in the Web dataset are labeled correctly.</p><p>We emphasize that the experiment to change the training datasets is not trivial in that it resembles the process in real applications. The test scenario is not always fully understood and the training dataset is getting bigger gradually when projects continue. Sometimes, a training dataset may be along with noisy labels, and the strategies to use them should be further considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Transfer learning and its advanced version are slightly beneficial for plant identification. Transfer learning has contributed to many tasks and our work proves that advanced transfer learning, such as foundation models, has an extra contribution. Therefore, utilizing a better pretrained model is one of the potential strategies. But we emphasize that it may also hinder performance such as when the labeled data in the target task is scarce <ref type="bibr" coords="10,471.20,647.09,16.55,10.91" target="#b21">[22,</ref><ref type="bibr" coords="10,491.11,647.09,12.42,10.91" target="#b22">23]</ref>. Simultaneously, foundation models may also suffer when the distributions of the source dataset to pretrain a model and the target dataset is diverse <ref type="bibr" coords="11,329.30,86.97,16.41,10.91" target="#b23">[24]</ref>. As far as we know, the images of PlantCLEF2023 are far different from the datasets, ImageNet-1k and ImageNet-21k, to pretrain MAE and EVA models. New methods to leverage the pretrained model are the potential to make further improvements.</p><p>Collecting a better dataset is still promising for plant identification. In the Plant-CLEF2023 challenge, the Trusted and Web datasets are both in heavy class imbalance and some classes of the datasets have only one or two images. Our experimental results suggest that more data make better performance, even if some noisy data are leveraged directly. Compared to the gains from transfer learning methods and bigger datasets, we believe that collecting a bigger dataset could be more effective.</p><p>Limitations and social impacts. Our experiments have used enormous computing resources although we obtained a decent record for the challenge. To be honest, we used 16 RTX 3090 GPUs for almost three months. It is actually not fair to other teams without enough GPUs. On the other hand, we hope our analysis and understandings are useful for the community, such as dataset analysis and the comparison between transfer learning and bigger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We first analyzed the given datasets in the PlantCLEF2023 challenge and found that class imbalance and huge intra-class variation are the main characteristics. Simultaneously, the challenge can be taken from a few-shot perspective. Furthermore, the Trusted and Web datasets are compared, with an observation that their image distribution is much different and the noise in the Web dataset may be exaggerated. Two strategies are executed for the challenge, different pretrained models and different sizes of the training dataset. Both strategies contribute and, even with noisy data, a bigger training dataset is more beneficial to the challenge. We hope that our results highlight the importance of collecting bigger datasets for plant identification. Simultaneously, we emphasize the heterogeneity between plant identification and general computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,235.45,416.69,8.93;4,89.29,247.46,381.95,8.87;4,407.55,158.37,59.98,58.38"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Images of Aralia nudicaulis L. species from PlantCLEF2022 Trusted dataset. The pictures show huge intra-class variation<ref type="bibr" coords="4,220.18,247.46,15.05,8.87" target="#b16">[17,</ref><ref type="bibr" coords="4,237.72,247.46,11.46,8.87" target="#b15">16,</ref><ref type="bibr" coords="4,251.67,247.46,12.86,8.87" target="#b14">15]</ref> and every triplet suggests an intra-class variation.</figDesc><graphic coords="4,407.55,158.37,59.98,58.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,411.34,418.23,8.93;5,89.29,423.35,416.70,8.87;5,89.29,435.30,416.69,8.87;5,89.29,447.26,416.69,8.87;5,89.29,459.21,170.64,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left up (1): number of images of every class in the Trusted dataset, sorted by their numbers. More than 40,000 classes have less than 20 images. Right up (2): relation between number of images and number of species. Two peaks are around 0 and 100 images in species. Left bottom (3): number of images of every class in the Web dataset, sorted by their numbers. Right bottom (4): relation between number of images and number of species.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,245.95,416.70,8.93;6,89.29,257.95,416.70,8.87;6,89.29,269.91,185.57,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number difference between the same class in the Trusted and Web dataset, sorted by the difference. It suggests a distinct distribution gap between the Trusted and Web datasets that most of the classes have different numbers of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,269.39,418.23,8.93;8,151.80,84.18,291.70,172.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architectures of MAE<ref type="bibr" coords="8,221.99,269.44,12.14,8.87" target="#b5">[6]</ref>,can be adopted as pre-training method without using annotations.</figDesc><graphic coords="8,151.80,84.18,291.70,172.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,89.29,553.82,333.93,8.93;8,98.32,268.14,396.91,280.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training architecture of CLIP [5], trained with paired images and texts.</figDesc><graphic coords="8,98.32,268.14,396.91,280.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,89.29,292.47,385.36,8.93;9,151.80,319.32,291.69,239.99"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architectures of MVP [3] which can be deemed as a combination of MAE and CLIP.</figDesc><graphic coords="9,151.80,319.32,291.69,239.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,89.29,491.88,234.69,8.93"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Highlights of EVA [4], a scaled version of MVP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,412.28,457.38"><head>Table 1</head><label>1</label><figDesc>Details, analysis, and understandings towards the datasets in PlantCLEF2023 challenge.</figDesc><table coords="3,95.67,118.58,405.61,237.21"><row><cell>Dataset</cell><cell>Classes</cell><cell>Images Analysis and understandings</cell></row><row><cell>Trusted</cell><cell cols="2">80,000 2,885,052 It is annotated by human experts and can be regarded as</cell></row><row><cell></cell><cell></cell><cell>reliable. This dataset is in heavy class imbalance, as shown</cell></row><row><cell></cell><cell></cell><cell>in the first row of Figure 2 and Table 2. It owns a huge intra-</cell></row><row><cell></cell><cell></cell><cell>class variation in terms of two aspects: one plant species has</cell></row><row><cell></cell><cell></cell><cell>multiple organs such as flowers and leaves and, as shown</cell></row><row><cell></cell><cell></cell><cell>in Figure 1, one organ has heterogeneity such as leaves in</cell></row><row><cell></cell><cell></cell><cell>multiscale and flowers in the different growth stages.</cell></row><row><cell>Web</cell><cell cols="2">57,314 1,071,627 It is collected from the web and thus may be with noisy</cell></row><row><cell></cell><cell></cell><cell>and even wrong labels. Its class space is a proper set of the</cell></row><row><cell></cell><cell></cell><cell>Trusted dataset with less number of classes. This dataset is</cell></row><row><cell></cell><cell></cell><cell>also in heavy class imbalance, as shown in the second row</cell></row><row><cell></cell><cell></cell><cell>of Figure 2 and Table 3. The Web dataset is in a different</cell></row><row><cell></cell><cell></cell><cell>distribution from the Trusted dataset where the number of</cell></row><row><cell></cell><cell></cell><cell>images for one class is different as shown in Figure 3. We</cell></row><row><cell></cell><cell></cell><cell>randomly looked at several classes in the Web dataset and</cell></row><row><cell></cell><cell></cell><cell>found that most of the images seem in the correct classes</cell></row><row><cell></cell><cell></cell><cell>(we emphasize we are not domain experts and thus our</cell></row><row><cell></cell><cell></cell><cell>judgments may not be correct).</cell></row><row><cell>Trusted + Web</cell><cell cols="2">80,000 3,995,568 This dataset is</cell></row></table><note coords="3,324.96,346.92,174.64,8.87;3,259.64,358.87,239.96,8.87;3,259.64,370.83,241.63,8.87;3,259.64,382.78,239.96,8.87;3,259.64,394.74,241.63,8.87;3,259.64,406.69,161.57,8.87;3,117.22,419.05,16.60,8.87;3,177.40,419.05,20.72,8.87;3,221.93,419.05,278.77,8.87;3,259.64,431.00,239.96,8.87;3,259.64,442.96,239.96,8.87;3,259.41,454.91,240.20,8.87;3,259.64,466.87,241.06,8.87;3,259.64,478.82,239.97,8.87;3,259.64,490.78,115.21,8.87;3,131.54,503.13,79.88,8.87;3,259.64,502.86,239.97,9.14;3,259.64,515.09,239.97,8.87;3,259.64,527.04,241.06,8.87;3,259.64,539.00,239.96,8.87"><p><p><p>the combination of the Trusted and Web datasets. The total number of images is not the summation of the Trusted and Web datasets and we guess that some images are shared in the two datasets. This combined dataset has more images and may mitigate the class-imbalance impact than their individual counterparts. Test 7,339 55,306 The classification should be done at the observation level, rather than the image level, where one observation refers to one actual plant with several pictures taken from different viewpoints. The task requires a strategy to combine the predictions of multiple images for one observation. Besides, the testing dataset only shares part of the plant identity in the trusted training dataset.</p>Shared information</p>Images have similar resolutions, around 450 Ã— 600. Most of the images are collected from real scenarios, rather than laboratories. Class imbalance, huge intra-class variations, and few-shot are the distinct characteristics of the datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,280.56,418.09,122.64"><head>Table 2</head><label>2</label><figDesc>Statistics of the Trusted dataset where the numbers of classes and images are counted. For example, there are 28,681 classes with or more than 36 images, covering 2,381,264 images in total. 36 is the average number of images for the classes.</figDesc><table coords="4,222.75,332.56,147.28,70.64"><row><cell cols="2">Statistic term Classes</cell><cell>Images</cell></row><row><cell>all</cell><cell cols="2">80,000 2,885,052</cell></row><row><cell>&gt;=7</cell><cell cols="2">54,478 2,807,969</cell></row><row><cell>&gt;=36</cell><cell cols="2">28,681 2,381,264</cell></row><row><cell>&gt;50</cell><cell cols="2">24,284 2,194,912</cell></row><row><cell>&gt;=100</cell><cell>9,122</cell><cell>920,774</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,423.10,417.00,97.93"><head>Table 3</head><label>3</label><figDesc>Statistics of the Web dataset where the numbers of classes and images are counted. For example, there are 23,317 classes with or more than 18 images, covering 869,518 images in total. 18 is the average number of images for the classes.</figDesc><table coords="4,222.75,475.10,147.28,45.93"><row><cell cols="2">Statistic term Classes</cell><cell>Images</cell></row><row><cell>all</cell><cell cols="2">57,314 1,071,617</cell></row><row><cell>&gt;=18</cell><cell>23,317</cell><cell>869,518</cell></row><row><cell>&gt;50</cell><cell>4,335</cell><cell>251,206</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,92.57,660.07,192.13,8.97"><p>https://en.wikipedia.org/wiki/Mean_reciprocal_rank</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,92.57,671.03,311.39,8.97"><p>https://androidkt.com/micro-macro-averages-for-imbalance-multiclass-classification/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by <rs type="programName">Basic Science Research Program</rs> through the <rs type="funder">National Research Foundation of Korea (NRF)</rs> funded by the <rs type="funder">Ministry of Education</rs> (No. <rs type="grantNumber">2019R1A6A1A09031717</rs>). This work was supported by the <rs type="funder">Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, and Forestry (IPET)</rs> and <rs type="funder">Korea Smart Farm R&amp;D Foundation (KosFarm)</rs> through the <rs type="programName">Smart Farm Innovation Technology Development Program</rs>, funded by <rs type="funder">Ministry of Agriculture, Food and Rural Affairs (MAFRA)</rs> and <rs type="funder">Ministry of Science and ICT (MSIT)</rs>, <rs type="funder">Rural Development Administration (RDA)</rs> (<rs type="grantNumber">421027-04</rs>). This work was supported by the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korean government (MSIT)</rs> (<rs type="grantNumber">NRF-2021R1A2C1012174</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2Hz5P72">
					<orgName type="program" subtype="full">Basic Science Research Program</orgName>
				</org>
				<org type="funding" xml:id="_UMYUqzr">
					<idno type="grant-number">2019R1A6A1A09031717</idno>
				</org>
				<org type="funding" xml:id="_mHPSdBR">
					<orgName type="program" subtype="full">Smart Farm Innovation Technology Development Program</orgName>
				</org>
				<org type="funding" xml:id="_tGT8xdE">
					<idno type="grant-number">421027-04</idno>
				</org>
				<org type="funding" xml:id="_PjDrSvg">
					<idno type="grant-number">NRF-2021R1A2C1012174</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,111.28,393.33,10.91;12,112.66,124.83,303.45,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,167.07,111.28,223.40,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,434.86,111.28,71.13,10.91;12,112.66,124.83,192.64,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,138.38,394.53,10.91;12,112.48,151.93,393.50,10.91;12,112.66,165.48,211.84,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m" coord="12,299.30,151.93,206.69,10.91;12,112.66,165.48,29.25,10.91">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,179.03,394.61,10.91;12,112.66,192.57,394.53,10.91;12,112.66,206.12,255.84,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,283.85,179.03,204.01,10.91">Mvp: Multimodality-guided visual pre-training</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,192.57,251.03,10.91">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="volume">XXX</biblScope>
			<biblScope unit="page" from="337" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,219.67,394.62,10.91;12,112.66,233.22,393.33,10.91;12,112.66,246.77,395.01,10.91;12,112.41,260.32,59.11,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,112.66,233.22,314.18,10.91">Exploring the limits of masked visual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eva</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,450.78,233.22,55.20,10.91;12,112.66,246.77,344.92,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19358" to="19369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,273.87,394.53,10.91;12,112.66,287.42,393.33,10.91;12,112.66,300.97,395.01,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,237.25,287.42,268.73,10.91;12,112.66,300.97,48.76,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.64,300.97,197.17,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,314.52,393.33,10.91;12,112.66,328.07,393.32,10.91;12,112.66,341.62,159.65,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,330.08,314.52,175.91,10.91;12,112.66,328.07,34.52,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,170.26,328.07,335.73,10.91;12,112.66,341.62,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,355.17,393.32,10.91;12,112.66,368.71,279.89,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yiheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01852</idno>
		<title level="m" coord="12,186.60,355.17,319.38,10.91;12,112.66,368.71,97.30,10.91">Summary of chatgptgpt-4 research and perspective towards the future of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,382.26,393.32,10.91;12,112.33,395.81,394.85,10.91;12,112.66,409.36,22.69,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,145.22,382.26,360.76,10.91;12,112.33,395.81,198.83,10.91">Enhanced Plant Disease Recognition with Limited Training Dataset Using Image Translation and Two-Step Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Jeonbuk National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,112.66,422.91,393.58,10.91;12,112.33,436.46,29.19,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,184.18,422.91,174.02,10.91">Scaling laws for neural language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,450.01,394.53,10.91;12,112.66,463.56,393.33,10.91;12,112.66,477.11,111.85,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,227.86,463.56,278.13,10.91;12,112.66,477.11,79.47,10.91">Scaling laws vs model architectures: How does inductive bias influence scaling?</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,490.66,393.33,10.91;12,112.33,504.21,29.19,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m" coord="12,170.08,490.66,188.36,10.91">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,517.76,393.33,10.91;12,112.66,531.30,107.17,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05442</idno>
		<title level="m" coord="12,203.77,517.76,229.24,10.91">Scaling vision transformers to 22 billion parameters</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,544.85,393.32,10.91;12,112.66,558.40,366.64,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,241.41,544.85,264.57,10.91;12,112.66,558.40,60.93,10.91">Overview of plantclef 2023: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,196.14,558.40,252.47,10.91">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,571.95,393.33,10.91;12,112.66,585.50,395.17,10.91;12,112.66,599.05,393.32,10.91;12,112.66,612.60,336.62,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,161.50,571.95,344.49,10.91;12,112.66,585.50,395.17,10.91;12,112.66,599.05,227.68,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi, in: International conference of the crosslanguage evaluation forum for european languages</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,363.85,599.05,142.14,10.91;12,112.66,612.60,263.46,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,626.15,393.33,10.91;12,112.66,639.70,393.60,10.91;12,112.66,653.25,146.44,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,456.91,626.15,49.07,10.91;12,112.66,639.70,361.97,10.91">Embracing limited and imperfect data: A review on plant stress recognition using deep learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11533</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,666.80,393.32,10.91;13,112.66,86.97,287.81,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,287.06,666.80,218.92,10.91;13,112.66,86.97,125.40,10.91">A comprehensive survey of image augmentation techniques for deep learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,246.37,86.97,89.02,10.91">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">109347</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,100.52,393.32,10.91;13,112.66,114.06,395.17,10.91;13,112.66,127.61,374.31,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,304.44,100.52,201.54,10.91;13,112.66,114.06,205.23,10.91">Transfer learning with self-supervised vision transformer for large-scale plant identification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,340.59,114.06,167.24,10.91;13,112.66,127.61,228.39,10.91">International conference of the crosslanguage evaluation forum for European languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,141.16,393.32,10.91;13,112.66,154.71,394.53,10.91;13,112.66,168.26,90.72,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,241.41,141.16,264.57,10.91;13,112.66,154.71,59.73,10.91">Overview of plantclef 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,194.54,154.71,247.52,10.91">CLEF 2022-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="1916" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,181.81,395.16,10.91;13,112.66,195.36,395.17,10.91;13,112.66,208.91,395.17,10.91;13,112.66,222.46,73.36,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,330.08,195.36,177.76,10.91;13,112.66,208.91,169.53,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,304.90,208.91,202.94,10.91;13,112.66,222.46,43.59,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,393.33,10.91;13,112.66,249.56,394.52,10.91;13,112.66,263.11,22.69,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,248.22,236.01,183.47,10.91">Do better imagenet models transfer better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,452.94,236.01,53.05,10.91;13,112.66,249.56,319.37,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,276.66,393.33,10.91;13,112.26,290.20,264.11,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,266.34,276.66,239.65,10.91;13,112.26,290.20,74.28,10.91">Transfer learning for versatile plant disease recognition with limited data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,194.73,290.20,113.83,10.91">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4506</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,303.75,394.53,10.91;13,112.66,317.30,394.53,10.91;13,112.66,330.85,100.87,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,296.85,303.75,206.18,10.91">Characterizing and avoiding negative transfer</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>PÃ³czos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.24,317.30,375.49,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11293" to="11302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,344.40,393.33,10.91;13,112.28,357.95,113.03,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,277.64,344.40,128.01,10.91">A survey on negative transfer</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,413.58,344.40,92.41,10.91;13,112.28,357.95,81.11,10.91">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,393.33,10.91;13,112.66,385.05,393.33,10.91;13,112.66,398.60,381.98,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,415.04,371.50,90.94,10.91;13,112.66,385.05,183.17,10.91">Surgical fine-tuning improves adaptation to distribution shifts</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tajwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=APuPRxjHvZ" />
	</analytic>
	<monogr>
		<title level="m" coord="13,318.21,385.05,187.78,10.91;13,112.66,398.60,111.77,10.91">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
