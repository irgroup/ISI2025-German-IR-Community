<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.40,85.02,392.24,15.72;1,89.29,106.93,124.74,15.72">Watch out Venomous Snake Species: A Solution to SnakeCLEF2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.07,48.84,10.93"><forename type="first">Feiran</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.77,136.07,55.83,10.93"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<email>wangpeng@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.25,136.07,60.70,10.93"><forename type="first">Yangyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.59,136.07,77.28,10.93"><forename type="first">Chenlong</forename><surname>Duan</surname></persName>
							<email>duancl@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.51,136.07,51.89,10.93"><forename type="first">Zijian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,447.04,136.07,45.94,10.93"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<email>wangfei@ainnovation.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Qingdao AInnovation Technology Group Co,. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,150.02,57.65,10.93"><forename type="first">Faen</forename><surname>Zhang</surname></persName>
							<email>zhangfaen@ainnovation.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Qingdao AInnovation Technology Group Co,. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.59,150.02,37.05,10.93"><forename type="first">Yong</forename><surname>Li</surname></persName>
							<email>yong.li@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.62,150.02,67.56,10.93"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.40,85.02,392.24,15.72;1,89.29,106.93,124.74,15.72">Watch out Venomous Snake Species: A Solution to SnakeCLEF2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">3EC0AEAE89E5BDF082F167FEE8A834B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake Species Identification</term>
					<term>Fine-grained image recognition</term>
					<term>Long-tailed</term>
					<term>Metadata</term>
					<term>SnakeCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SnakeCLEF2023 competition aims to the development of advanced algorithms for snake species identification through the analysis of images and accompanying metadata. This paper presents a method leveraging utilization of both images and metadata. Modern CNN models and strong data augmentation are utilized to learn better representation of images. To relieve the challenge of long-tailed distribution, seesaw loss [1] is utilized in our method. We also design a light model to calculate prior probabilities using metadata features extracted from CLIP [2] in post processing stage. Besides, we attach more importance to venomous species by assigning venomous species labels to some examples that model is uncertain about. Our method achieves 91.31% score of the final metric combined of F1 and other metrics on private leaderboard, which is the 1st place among the participators. The code is available at https://github.com/xiaoxsparraw/CLEF2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization is a well-established and pivotal challenge within the fields of computer vision and pattern recognition, serving as the cornerstone for a diverse array of realworld applications <ref type="bibr" coords="1,175.31,456.50,11.54,9.97" target="#b2">[3]</ref>. The SnakeCLEF2023 competition, co-hosted as an integral part of the LifeCLEF2023 lab within the CLEF2023 conference, and the FGVC10 workshop in conjunction with the esteemed CVPR2023 conference, is geared towards advancing the development of a robust algorithm for snake species identification from images and metadata. This objective holds profound significance in the realm of biodiversity conservation and constitutes a crucial facet of human health preservation.</p><p>In this paper, we introduce a method that addresses the recognition of snake species by leveraging both metadata and images. ConvNeXt-v2 <ref type="bibr" coords="1,338.44,551.34,13.00,9.97" target="#b3">[4]</ref> and CLIP <ref type="bibr" coords="1,402.09,551.34,13.00,9.97" target="#b1">[2]</ref> are used to extract images features and metadata features separately, and the image features and text features are concatenated to be input of MLP classifier, thus getting better representation of examples and recognition results. Seesaw loss <ref type="bibr" coords="2,237.43,87.97,13.00,9.97" target="#b0">[1]</ref> are utilized in our method, thereby alleviating the longtailed distribution problem. Notably, our proposed method takes into careful consideration the critical real-world need to distinguish venomous and harmless snake species by using the Real-World Weighted Cross-Entropy (RWWCE) loss <ref type="bibr" coords="2,297.75,128.62,12.74,9.97" target="#b4">[5]</ref> and post-processing, resulting in exemplary performance surpassing that of other solutions presented in this year's competition. Experiments and competition results show that our method is effective in snake species recognition task.</p><p>The subsequent sections of this paper provide a comprehensive overview of the key aspects. Section 2 introduces the competition challenges and datasets, accompanied by the examination of the evaluation metric utilized. Section 3 describes our proposed methodologies, offering a comprehensive and detailed introduction to the techniques. Section 4 presents the implementation details, alongside a comprehensive analysis of the principal outcomes achieved. Finally, Section 5 concludes this paper by summarizing the key findings and offering future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Competition Description</head><p>Understanding datasets and metrics is an essential requirement for engaging in a machine learning competition. Within this section, we aim to introduce our comprehension of the datasets and provide overview of the evaluation metrics employed by the competition organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Challenges of the Competition</head><p>Past iterations of this competition have witnessed remarkable accomplishments by machine learning models <ref type="bibr" coords="2,165.43,399.56,11.48,9.97" target="#b5">[6,</ref><ref type="bibr" coords="2,179.91,399.56,7.52,9.97" target="#b6">7,</ref><ref type="bibr" coords="2,190.44,399.56,7.52,9.97" target="#b7">8,</ref><ref type="bibr" coords="2,200.96,399.56,7.52,9.97" target="#b8">9,</ref><ref type="bibr" coords="2,211.48,399.56,12.59,9.97" target="#b9">10,</ref><ref type="bibr" coords="2,227.08,399.56,12.42,9.97" target="#b10">11]</ref>. To further enhance the competition's practical relevance and address the exigencies faced by developers, scientists, users, and communities, such as addressing post-snakebite incidents, the organizers have imposed more stringent constraints. The ensuing challenges of this year's competition can be summarized as follows:</p><p>â€¢ Fine-grained image recognition: The domain of fine-grained image analysis has long posed a challenging problem within the FGVC workshop, deserving further investigation and study. â€¢ Utilization of metadata: The incorporation of metadata, particularly pertaining to the geographical distribution of snake species, plays a vital role in their classification. Such metadata is commonly employed by individuals to identify snakes in their daily lives. Hence, utilization of location metadata holds significance and needs careful consideration. â€¢ Long-tailed distribution: Long-tailed distributions are common in real-world scenarios, and the distribution of snake species is no exception. â€¢ Identification of venomous and harmless species: The distinction between venomous and harmless snake species is meaningful, as venomous snake bites lead to large number of death each year. Consequently, leveraging deep learning methodologies to address this problem is of paramount urgency. â€¢ Model size limitation: A strict limitation has been imposed on the model size, constraining it to a maximum of 1GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natrix natrix</head><p>Xenodon pulcher</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head class</head><p>Tail class </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset</head><p>The organizers provide a dataset, consisting of 103,404 recorded snake observations, supplemented by 182,261 high-resolution images. These observations encompass a diverse range of 1,784 distinct snake species and have been documented across 214 geographically varied regions.</p><p>It is worth to note that the provided dataset is in a heavily long-tailed distribution, as shown in Fig. <ref type="figure" coords="3,119.60,458.81,3.72,9.97" target="#fig_0">1</ref>. In this distribution, the most frequently encountered species have 1,262 observations consists of 2,079 accompanying images. However, the least frequently encountered species is captured by a mere 3 observations, showing its exceptional rarity within the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation Metric</head><p>In addition to the conventional evaluation metrics of Accuracy (Acc) and Mean F1-Score, this year's competition incorporates a novel evaluation metric, denoted as "public_score_track1" on the leaderboard. This metric combines the F1-Score with an assessment of the confusion errors related to venomous species. It is calculated as a weighted average, incorporating both the macro F1-score and the weighted accuracy of various types of confusions:</p><formula xml:id="formula_0" coords="3,113.49,607.86,393.15,28.57">ğ‘€ = ğ‘¤ 1 ğ¹ 1 + ğ‘¤ 2 (100 -ğ‘ƒ 1 ) + ğ‘¤ 3 (100 -ğ‘ƒ 2 ) + ğ‘¤ 4 (100 -ğ‘ƒ 3 ) + ğ‘¤ 5 (100 -ğ‘ƒ 4 ) âˆ‘ï¸€ 5 ğ‘– ğ‘¤ ğ‘– ,<label>(1)</label></formula><p>where ğ‘¤ 1 = 1.0, ğ‘¤ 2 = 1.0, ğ‘¤ 3 = 2.0, ğ‘¤ 4 = 5.0, ğ‘¤ 5 = 2.0 are the weights of individual terms. The metric incorporates several percentages, namely ğ¹ 1 representing the macro F1score, ğ‘ƒ 1 denoting the percentage of harmless species misclassified as another harmless species, ğ‘ƒ 2 indicating the percentage of harmless species misclassified as a venomous species, ğ‘ƒ 3 reflecting the percentage of venomous species misclassified as another harmless species, and ğ‘ƒ 4 representing the percentage of venomous species misclassified as another venomous species. This metric is bounded below by 0% and above by 100%. The lower bound is attained when all species are misclassified, including misclassification of harmless species as venomous and vice versa. Conversely, if the F1-score reaches 100%, indicating correct classification of all species, each ğ‘ƒ ğ‘– value must be zero, leading to an overall score of 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we shall introduce the methodologies employed to address the task of snake species classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Preprocessing</head><p>Data preprocessing plays a crucial role in machine learning, as it influences not only the final performance but also the feasibility of problem resolution. Upon obtaining the dataset provided by the competition organizers, several issues emerged. For instance, certain images listed in the metadata CSV file were found to be nonexistent within the corresponding image folders.</p><p>To address this, we generated a new metadata CSV file by eliminating the affected rows from the original file. Additionally, a subset of images within the dataset was found to be corrupted, potentially due to network transmission or other factors. To mitigate this concern, we utilized OpenCV to read the problematic images and subsequently re-wrote them to the file system, thereby solving the corruption issue. The SnakeCLEF dataset includes valuable metadata pertaining to the observation locations. Leveraging this location information is of great significance, as certain snake species inhabit geographically confined areas. However, the metadata presents the location in the form of country or region codes, which cannot be directly utilized as inputs for convolutional neural network (CNN) or Vision Transformer (ViT) <ref type="bibr" coords="4,283.19,467.31,16.09,9.97" target="#b11">[12]</ref>. To address this challenge, we employ CLIP <ref type="bibr" coords="4,493.30,467.31,12.69,9.97" target="#b1">[2]</ref> to extract location features without engaging in fine-tuning. Subsequently, Principal Component Analysis (PCA) <ref type="bibr" coords="4,159.71,494.40,17.91,9.97" target="#b12">[13]</ref> is employed to reduce the dimension of the resulting feature vectors.</p><p>Data augmentation serves as a key technique in computer vision tasks. Within our methodology, we leverage fundamental image augmentation methods from Albumentations <ref type="bibr" coords="4,456.32,521.50,16.19,9.97" target="#b13">[14]</ref>. These methods encompass RandomResizedCrop, Transpose, HorizontalFlip, VerticalFlip, ShiftScaleRotate, RandomBrightnessContrast, PiecewiseAffine, HueSaturationValue, OpticalDistortion, Elas-ticTransform, Cutout, and GridDistortion. Furthermore, we incorporate data mixing augmentation techniques, such as Mixup <ref type="bibr" coords="4,227.69,575.70,16.10,9.97" target="#b14">[15]</ref>, CutMix <ref type="bibr" coords="4,286.37,575.70,16.10,9.97" target="#b15">[16]</ref>, TokenMix <ref type="bibr" coords="4,356.23,575.70,16.09,9.97" target="#b16">[17]</ref>, and RandomMix <ref type="bibr" coords="4,454.00,575.70,16.10,9.97" target="#b17">[18]</ref>, during the course of the competition. These data augmentation methods provide strong regularization to models by softening both images and labels, avoiding the model overfitting in training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>Throughout the competition, we explored various models, including both classical and stateof-the-art architectures, such as Convolutional Neural Networks and Vision Transformers.  <ref type="bibr" coords="5,327.28,286.08,11.96,8.87" target="#b3">[4]</ref> as the backbone, which is made up of 4 stages, feature vector extracted from metadata (ğ‘£ 1 ), original feature vector (ğ‘£ 2 ) and feature vector from middle stage of the backbone (ğ‘£ 3 ) are concatenated to get the final feature vector ğ‘£, a MLP classifier is followed to get the final classification results.</p><p>Models employed during the competition include ResNet <ref type="bibr" coords="5,337.93,360.50,16.09,9.97" target="#b18">[19]</ref>, VOLO <ref type="bibr" coords="5,389.97,360.50,16.09,9.97" target="#b19">[20]</ref>, ConvNeXt <ref type="bibr" coords="5,460.84,360.50,16.09,9.97" target="#b20">[21]</ref>, BEiT-v2 <ref type="bibr" coords="5,102.43,374.05,16.41,9.97" target="#b21">[22]</ref>, EVA <ref type="bibr" coords="5,148.31,374.05,18.06,9.97" target="#b22">[23]</ref> and ConvNeXt-v2 <ref type="bibr" coords="5,253.62,374.05,11.57,9.97" target="#b3">[4]</ref>. The implementation of these models was facilitated using the timm <ref type="bibr" coords="5,160.67,387.60,18.04,9.97" target="#b23">[24]</ref> library. In light of the imposed limitations on model parameters and the consideration of the model representation capabilities, we selected ConvNeXt-v2 <ref type="bibr" coords="5,462.71,401.15,13.00,9.97" target="#b3">[4]</ref> as the backbone architecture in our final method.</p><p>However, relying solely on the visual backbone is insufficient for effectively addressing the task at hand. Given the availability of metadata in the competition and the inherent challenges associated with fine-grained image classification, it becomes necessary to modify the architecture of the vision model to achieve superior performance. The architectural design of the model employed in our final submission is illustrated in Fig. <ref type="figure" coords="5,328.64,482.45,3.74,9.97" target="#fig_1">2</ref>.</p><p>Following the completion of the third stage of ConvNeXt-v2 <ref type="bibr" coords="5,385.59,495.99,11.58,9.97" target="#b3">[4]</ref>, the intermediate-level feature map is combined with the high-level image features after the final stage, along with the metadata features. This concatenation process yields a comprehensive representation that captures both the image and metadata information. To mitigate overfitting, we have incorporated MaxPooling <ref type="bibr" coords="5,146.26,550.19,16.34,9.97" target="#b24">[25]</ref>, BatchNorm <ref type="bibr" coords="5,223.85,550.19,16.34,9.97" target="#b25">[26]</ref>, and Dropout <ref type="bibr" coords="5,307.02,550.19,18.00,9.97" target="#b26">[27]</ref> techniques into our methodology. Once the comprehensive representation is obtained, a classifier comprising two linear layers and ReLU <ref type="bibr" coords="5,116.27,577.29,17.91,9.97" target="#b27">[28]</ref> activation functions follows and generates classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization Procedure</head><p>Addressing long-tailed recognition is another challenge encountered in the competition. To tackle this issue, we extensively explored various techniques implemented in BagofTricks-LT <ref type="bibr" coords="5,104.19,654.12,16.19,9.97" target="#b28">[29]</ref>. In our final submission, we incorporated the seesaw loss <ref type="bibr" coords="5,379.82,654.12,12.78,9.97" target="#b0">[1]</ref> as a key component. The seesaw loss formulation can be expressed as follows:</p><formula xml:id="formula_1" coords="6,227.33,97.24,279.31,66.06">ğ¿ seesaw (z) = - ğ¶ âˆ‘ï¸ ğ‘–=1 ğ‘¦ ğ‘– log (Ì‚ï¸€ ğœ ğ‘– ) , with Ì‚ï¸€ ğœ ğ‘– = ğ‘’ ğ‘§ ğ‘– âˆ‘ï¸€ ğ¶ ğ‘—Ì¸ =ğ‘– ğ’® ğ‘–ğ‘— ğ‘’ ğ‘§ ğ‘— + ğ‘’ ğ‘§ ğ‘– ,<label>(2)</label></formula><p>where z denotes the output obtained from the fully connected layer, ğ¶ represents the total number of classes, and ğ‘¦ ğ‘– corresponds to the one-hot label of the image. The hyper-parameters ğ’® ğ‘–ğ‘— are carefully set based on the distribution characteristics inherent in the dataset.</p><p>Distinguishing between venomous and non-venomous snake species and the consequential assignment of varying costs to different classification errors are of great importance in this year's challenge, as demonstrated by Eq. 1. In accordance with these requirements, loss function that effectively models the real-world costs associated with mislabeling <ref type="bibr" coords="6,405.13,252.54,12.73,9.97" target="#b4">[5]</ref> is utilized by us. To align with this objective, we incorporate the Real-World Weighted Cross-Entropy (RWWCE) loss function <ref type="bibr" coords="6,149.20,279.64,12.84,9.97" target="#b4">[5]</ref> during the final three epochs of training, employing a reduced learning rate.</p><p>In addition to the choice of loss functions, the selection of an optimizer and an appropriate learning rate decay strategy are important in the training of our models. For optimization, we adopt the AdamW optimizer <ref type="bibr" coords="6,217.73,320.29,16.16,9.97" target="#b29">[30]</ref>. To enhance convergence speed and overall performance, we implement cosine learning rate decay <ref type="bibr" coords="6,254.77,333.84,17.75,9.97" target="#b30">[31]</ref> coupled with warmup techniques during the training process. These strategies collectively facilitate more effective and efficient model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Post-processing</head><p>In this year's challenge, the task requires the solution to accurately identify the venomous nature of snakes, particularly focusing on distinguishing the venomous species, with the limited model capacity. It is challenging but fortunately, the organizers provided a metadata repository, with a particular focus on geographical information. In practical contexts, where reliance solely on visual cues may prove insufficient performance on fine-grained classification, the supplementation of geographical details assumes a crucial role in assisting human experts in making judgment. Thus, the integration of geographical information within the metadata exhibits the potential to enhance the decision-making prowess of classification models.</p><p>Inspired by <ref type="bibr" coords="6,152.97,505.51,16.23,9.97" target="#b31">[32]</ref>, assuming the above-mentioned trained model as ğ‘“ , we developed a simple prior model denoted as ğ‘”. This prior model is simple but efficiently, composed of three fully connected layers with non-linear activation function and employed dropout regularization. In the training process of this light model, we adopt the AdamW <ref type="bibr" coords="6,372.01,546.15,18.06,9.97" target="#b29">[30]</ref> optimizer and performed balanced sampling on the training data, to mitigate the impact of the long-tail distribution in the dataset. The objective of this training process was to minimize the following loss function:</p><formula xml:id="formula_2" coords="6,144.78,596.07,361.86,80.37">â„’ ğ‘™ğ‘œğ‘ (x, r, O, ğ‘¦) =ğœ† log (ğ‘  (ğ‘”(x)O :,ğ‘¦ )) + ğ¶ âˆ‘ï¸ ğ‘–=1 ğ‘–Ì¸ =ğ‘¦ log (1 -ğ‘  (ğ‘”(x)O :,ğ‘– )) + ğ¶ âˆ‘ï¸ ğ‘–=1 log (1 -ğ‘  (ğ‘”(r)O :,ğ‘– )) ,<label>(3)</label></formula><p>where the metadata features extracted from CLIP is denoted as x. O is the category embedding matrix, where each column is the prototype of different category, pre-computed by our trained model ğ‘“ , e.g., ConvNeXt-v2 <ref type="bibr" coords="7,219.35,115.07,11.58,9.97" target="#b3">[4]</ref>. Furthermore, r signifies a uniformly random location data point, and ğœ† serves as a hyper-parameter for weighting positive observations. It is important to note that if a category ğ‘¦ has been observed at the spatial location x within the training set, the value of ğ‘  (ğ‘”(x)O :,ğ‘¦ ) should approximate 1. Conversely, if the category has not been observed, the value should approximate 0.</p><p>During the inference stage, our prior model efficiently calculates the prior class embeddings denoted as P. Utilizing the following equation:</p><formula xml:id="formula_3" coords="7,242.97,218.10,263.67,12.74">S â€² = ğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(P) âŠ™ S,<label>(4)</label></formula><p>where S is the prediction score computed by ğ‘“ . We derive the final class scores S â€² by computing the joint probability of predictions from the two models ğ‘“ and ğ‘”. In real-world scenarios, misclassifying a non-venomous snake as venomous carries significant consequences and is deemed unacceptable. To address this concern, we implement a robust post-processing approach.</p><p>When the predicted confidence of an image x is relatively low, we analyze its top-5 predictions.</p><p>If any of these predictions include a venomous class, we classify the image as venomous. This post-processing technique represents a well-considered compromise between precision and recall. Notably, this approach enable us to get the 1st place in the private leaderboard. We firmly believe that this strategy possesses considerable advantages for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will introduce our implementation details and main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>The proposed methodology is developed utilizing the PyTorch framework <ref type="bibr" coords="7,431.36,462.13,16.42,9.97" target="#b32">[33]</ref>. All models employed in our approach have been pre-trained on the ImageNet dataset <ref type="bibr" coords="7,412.11,475.67,16.09,9.97" target="#b33">[34]</ref>, readily available within the timm library <ref type="bibr" coords="7,202.96,489.22,16.41,9.97" target="#b23">[24]</ref>. Fine-tuning of these models was conducted across 4 Nvidia RTX3090 GPUs. The initial learning rate was set to 2 Ã— 10 -5 , and the total number of training epochs was set to 15, with the first epoch dedicated to warm-up, employing a learning rate of 2Ã—10 -7 . To optimize model training, we utilized the AdamW optimizer <ref type="bibr" coords="7,401.10,529.87,17.76,9.97" target="#b29">[30]</ref> in conjunction with a cosine learning rate scheduler <ref type="bibr" coords="7,234.75,543.42,16.34,9.97" target="#b30">[31]</ref>, setting the weight decay to 2 Ã— 10 -5 . During inference on the test dataset, we adopted test time augmentation. Furthermore, considering that an observation may consist of multiple images, we adopted a simple averaging approach to obtain a singular prediction for each observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>In this section, we present our primary findings attained throughout the challenge, as illustrated in Tab. 1. The "Metric" column within the table corresponds to the public track1 metric featured on the leaderboard. As indicated by Tab. 1, the model parameters and image resolution hold crucial significance in image recognition tasks, aligning with conventional expectations. An increase in model parameters and image resolution corresponds to improvement in the public leaderboard score. Furthermore, data augmentation plays as a key factor in enhancing the generalization capacity of models. Notably, CutMix <ref type="bibr" coords="8,212.43,371.55,17.76,9.97" target="#b15">[16]</ref> outperforms alternative data mixing augmentation techniques, such as RandomMix <ref type="bibr" coords="8,181.69,385.10,16.25,9.97" target="#b17">[18]</ref>, based on our experimental observations. Metadata plays a pivotal role in the recognition of snake species, enabling models to acquire enhanced representations of observations and thereby achieve superior classification results. In our experiments, the utilization of metadata facilitated the acquisition of enriched contextual information, leading to improved model performance. Additionally, the incorporation of the Seesaw loss <ref type="bibr" coords="8,143.22,452.85,12.69,9.97" target="#b0">[1]</ref> demonstrated notable efficacy in mitigating the challenges posed by long-tailed distributions, surpassing the conventional CrossEntropy loss. Moreover, the integration of middle-level features proved effective in alleviating the complexities associated with fine-grained image recognition, enabling more precise discrimination between similar snake species.</p><p>Given that the final evaluation metric takes into account the demands of real-world applications and imposes greater penalties for misclassifying a venomous snake species as harmless compared to misclassifying a harmless species as venomous, we place significant emphasis on post-processing techniques. Specifically, when the model exhibits uncertainty in its predictions for a particular observation, we adopt a cautious approach and classify it as a venomous snake species based on the top-5 predictions. This post-processing strategy has proven highly advantageous, leading to substantial improvements in both the public leaderboard and the private test data performance, as evidenced by Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Fine-grained visual analysis holds great practical significance, particularly in accurately discerning the toxicity of snakes within the domain of snake sub-classification. This paper focuses on addressing the snake classification problem by harnessing the valuable metadata present in the dataset for posterior filtering. Additionally, a robust post-processing technique is employed to facilitate toxicity identification. These approaches have culminated in our noteworthy achievement of securing the first-place position in the challenge, attaining an impressive overall evaluation score of 91.31% on the private leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,311.50,416.70,9.21;3,89.29,323.45,416.71,8.87;3,89.29,335.41,262.55,8.87;3,89.29,84.19,416.70,219.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Long-tailed distribution of the SnakeCLEF2023 training dataset. The blue color means head classes, which means most images in the dataset belong to these classes. The orange color means tail classes, which means most classes in the dataset are tail classes.</figDesc><graphic coords="3,89.29,84.19,416.70,219.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,286.08,416.93,9.21;5,89.29,298.04,416.70,9.38;5,89.29,309.72,416.69,9.65;5,89.29,321.95,185.44,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Architecture of our model. Take ConvNeXt-v2<ref type="bibr" coords="5,327.28,286.08,11.96,8.87" target="#b3">[4]</ref> as the backbone, which is made up of 4 stages, feature vector extracted from metadata (ğ‘£ 1 ), original feature vector (ğ‘£ 2 ) and feature vector from middle stage of the backbone (ğ‘£ 3 ) are concatenated to get the final feature vector ğ‘£, a MLP classifier is followed to get the final classification results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,90.64,390.44,199.00"><head>Table 1</head><label>1</label><figDesc>Main results of SnakeCLEF.</figDesc><table coords="8,115.85,119.88,363.58,169.76"><row><cell>Backbone</cell><cell cols="2">Resolution Metric (%)</cell><cell>Comments</cell></row><row><cell>ResNet50 [19]</cell><cell>224 Ã— 224</cell><cell>72.22</cell><cell>baseline</cell></row><row><cell>BEiT-v2-L [22]</cell><cell>224 Ã— 224</cell><cell>82.59</cell><cell>stronger backbone</cell></row><row><cell>BEiT-L [35]</cell><cell>384 Ã— 384</cell><cell>88.74</cell><cell>cutmix</cell></row><row><cell>EVA-L [23]</cell><cell>336 Ã— 336</cell><cell>86.82</cell><cell>cutmix</cell></row><row><cell>Swin-v2-L [36]</cell><cell>384 Ã— 384</cell><cell>88.19</cell><cell>cutmix</cell></row><row><cell>VOLO [20]</cell><cell>448 Ã— 448</cell><cell>88.50</cell><cell>cutmix</cell></row><row><cell cols="2">ConvNeXt-v2-L [4] 384 Ã— 384</cell><cell>88.98</cell><cell>seesawloss + randommix</cell></row><row><cell cols="2">ConvNeXt-v2-L [4] 384 Ã— 384</cell><cell>89.47</cell><cell>seesawloss + cutmix</cell></row><row><cell cols="2">ConvNeXt-v2-L [4] 512 Ã— 512</cell><cell>90.86</cell><cell>seesawloss + cutmix + metadata</cell></row><row><cell cols="2">ConvNeXt-v2-L [4] 512 Ã— 512</cell><cell>91.98</cell><cell>seesawloss + cutmix + metadata + middle-level feature</cell></row><row><cell cols="2">ConvNeXt-v2-L [4] 512 Ã— 512</cell><cell>93.65</cell><cell>seesawloss + cutmix + metadata + middle-level feature + post-processing</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,252.20,394.52,9.97;9,112.66,265.75,393.33,9.97;9,112.66,279.30,297.29,9.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,112.66,265.75,213.88,9.97">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,349.17,265.75,156.81,9.97;9,112.66,279.30,199.17,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,292.85,394.52,9.97;9,112.66,306.40,393.32,9.97;9,112.66,319.95,394.52,9.97;9,112.66,333.50,22.69,9.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,237.25,306.40,268.73,9.97;9,112.66,319.95,50.66,9.97">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,186.65,319.95,211.00,9.97">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,347.05,395.17,9.97;9,112.66,360.60,393.32,9.97;9,112.28,374.15,247.52,9.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,484.65,347.05,23.18,9.97;9,112.66,360.60,244.38,9.97">Finegrained image analysis with deep learning: A survey</title>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,369.55,360.60,136.43,9.97;9,112.28,374.15,153.45,9.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="8927" to="8948" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,387.70,393.33,9.97;9,112.66,401.24,395.00,9.97" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00808</idno>
		<title level="m" coord="9,381.71,387.70,124.27,9.97;9,112.66,401.24,212.60,9.97">ConvNeXt V2: Co-designing and scaling convnets with masked autoencoders</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,414.79,393.32,9.97;9,112.66,428.34,212.57,9.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,194.12,414.79,311.86,9.97;9,112.66,428.34,60.80,9.97">The real-world-weight cross-entropy loss function: Modeling the costs of mislabeling</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wookey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,181.85,428.34,54.37,9.97">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="4806" to="4813" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,441.89,394.61,9.97;9,112.28,454.44,360.54,10.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,358.78,441.89,148.49,9.97;9,112.28,454.44,214.32,10.97">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,335.00,455.44,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,468.99,393.33,9.97;9,112.66,481.54,378.06,10.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,334.93,468.99,171.05,9.97;9,112.66,481.54,231.97,10.97">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,352.90,482.54,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,496.09,393.32,9.97;9,112.66,508.64,313.77,10.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,297.11,496.09,208.87,9.97;9,112.66,508.64,168.02,10.97">Overview of snakeclef 2022: Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,288.60,509.64,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,523.19,394.53,9.97;9,112.66,536.74,393.32,9.97;9,112.66,549.28,393.58,10.97;9,112.33,563.84,29.19,9.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,246.68,536.74,259.30,9.97;9,112.66,549.28,275.68,10.97">Combination of image and location information for snake species identification using object detection and efficientnets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,397.51,550.29,108.73,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,577.38,393.32,9.97;9,112.66,590.93,215.80,9.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,302.26,577.38,203.72,9.97;9,112.66,590.93,69.79,9.97">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,190.64,590.93,105.90,9.97">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,603.48,393.32,10.97;9,112.66,618.03,344.61,9.97" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.01216</idno>
		<title level="m" coord="9,295.02,603.48,210.97,10.97;9,112.66,618.03,162.36,9.97">Solutions for fine-grained and long-tailed snake species recognition in snakeclef 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,631.58,395.16,9.97;9,112.66,645.13,395.17,9.97;10,112.66,87.97,393.32,9.97;10,112.66,101.52,145.90,9.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,330.07,645.13,177.75,9.97;10,112.66,87.97,170.51,9.97">An image is worth 16x16 words: Trans-formers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,291.19,87.97,214.79,9.97;10,112.66,101.52,113.98,9.97">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,115.07,395.17,9.97;10,112.66,128.62,120.40,9.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,248.43,115.07,165.29,9.97">Principal components analysis (PCA)</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>MaÄ‡kiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ratajczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,422.64,115.07,85.19,9.97;10,112.66,128.62,36.47,9.97">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="303" to="342" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,142.17,394.52,9.97;10,112.28,154.71,370.34,10.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,112.28,154.71,245.27,10.97">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,366.37,155.72,53.51,9.97">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,169.26,395.16,9.97;10,112.66,182.81,197.93,9.97" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="10,330.70,169.26,177.12,9.97;10,112.66,182.81,16.17,9.97">Mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,196.36,393.32,9.97;10,112.66,208.91,393.32,10.97;10,112.66,223.46,240.15,9.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,329.08,196.36,176.90,9.97;10,112.66,208.91,181.87,10.97">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.14,209.91,188.84,9.97;10,112.66,223.46,142.26,9.97">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,237.01,393.32,9.97;10,112.66,250.56,394.52,9.97;10,112.66,264.11,123.33,9.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,293.68,237.01,212.31,9.97;10,112.66,250.56,166.74,9.97">Tokenmix: Rethinking image mixing for data augmentation in vision transformers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.87,250.56,194.78,9.97">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="455" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,277.66,393.32,9.97;10,112.26,291.21,302.07,9.97" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,252.17,277.66,253.81,9.97;10,112.26,291.21,119.14,9.97">Randommix: A mixed sample data augmentation method with multiple mixed modes</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08728</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,304.76,395.17,9.97;10,112.66,318.31,395.01,9.97;10,112.41,331.86,38.81,9.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,259.74,304.76,203.37,9.97">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,488.38,304.76,19.45,9.97;10,112.66,318.31,348.39,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,345.40,394.53,9.97;10,112.66,358.95,319.01,9.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,301.13,345.40,201.49,9.97">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.66,358.95,287.09,9.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,372.50,394.61,9.97;10,112.66,386.05,394.52,9.97;10,112.66,399.60,75.45,9.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,383.59,372.50,103.81,9.97">A convnet for the 2020s</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,386.05,364.28,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,413.15,395.17,9.97;10,112.66,426.70,301.63,9.97" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,298.15,413.15,209.68,9.97;10,112.66,426.70,119.51,9.97">Beit v2: Masked image modeling with vectorquantized visual tokenizers</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.06366</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,440.25,394.61,9.97;10,112.66,453.80,393.32,9.97;10,112.66,467.35,394.85,9.97" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,112.66,453.80,305.44,9.97">Exploring the limits of masked visual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eva</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.57,453.80,65.41,9.97;10,112.66,467.35,288.60,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19358" to="19369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,480.90,8.98,9.97;10,139.07,480.90,51.02,9.97;10,211.20,480.90,35.67,9.97;10,264.31,480.90,27.43,9.97;10,309.18,480.90,34.79,9.97;10,365.09,480.90,141.60,9.97;10,112.66,494.45,129.46,9.97" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<title level="m" coord="10,211.20,480.90,35.67,9.97;10,264.31,480.90,27.43,9.97;10,309.18,480.90,29.82,9.97">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,507.99,394.60,9.97;10,112.66,521.54,394.52,9.97;10,112.66,535.09,65.30,9.97" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,300.02,507.99,187.59,9.97">Learning mid-level features for recognition</title>
		<author>
			<persName coords=""><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,521.54,364.28,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,393.33,10.97;10,112.66,562.19,394.52,9.97;10,112.66,575.74,55.16,9.97" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,200.82,548.64,305.16,9.97;10,112.66,562.19,100.53,9.97">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.53,562.19,208.81,9.97">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,589.29,395.17,9.97;10,112.66,602.84,393.32,9.97;10,112.66,616.39,102.10,9.97" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<title level="m" coord="10,471.08,589.29,36.75,9.97;10,112.66,602.84,316.74,9.97">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,629.94,394.62,9.97;10,112.66,643.49,327.84,9.97" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,211.85,629.94,274.64,9.97">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,643.49,207.49,9.97">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,657.04,393.32,9.97;10,112.66,669.58,393.33,10.97;11,112.66,87.97,147.72,9.97" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,270.40,657.04,235.58,9.97;10,112.66,670.58,161.12,9.97">Bag of tricks for long-tailed visual recognition with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.53,669.58,208.46,10.97;11,112.66,87.97,50.10,9.97">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3447" to="3455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,101.52,393.32,9.97;11,112.66,115.07,107.17,9.97" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="11,238.15,101.52,182.94,9.97">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,128.62,393.59,9.97;11,112.66,142.17,146.44,9.97" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m" coord="11,226.91,128.62,243.29,9.97">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,154.71,393.33,10.97;11,112.66,168.26,393.52,10.97;11,112.30,182.81,56.51,9.97" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="11,259.01,154.71,246.98,10.97;11,112.66,168.26,57.10,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,193.33,169.26,312.85,9.97;11,112.30,182.81,26.65,9.97">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,196.36,394.52,9.97;11,112.66,209.91,394.52,9.97;11,112.66,223.46,394.52,9.97;11,112.66,237.01,393.32,9.97;11,112.66,250.56,132.21,9.97" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,371.35,223.46,135.83,9.97;11,112.66,237.01,175.42,9.97">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,310.67,237.01,195.31,9.97;11,112.66,250.56,33.91,9.97">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,264.11,393.33,9.97;11,112.66,277.66,393.32,9.97;11,112.66,291.21,139.36,9.97" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="11,346.64,264.11,159.35,9.97;11,112.66,277.66,65.12,9.97">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,200.70,277.66,305.28,9.97;11,112.66,291.21,51.39,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,304.76,393.59,9.97;11,112.66,318.31,146.44,9.97" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="11,265.50,304.76,204.92,9.97">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,331.86,393.32,9.97;11,112.66,345.40,393.33,9.97;11,112.66,358.95,307.44,9.97" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="11,483.94,331.86,22.04,9.97;11,112.66,345.40,216.29,9.97">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,350.41,345.40,155.57,9.97;11,112.66,358.95,199.17,9.97">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
