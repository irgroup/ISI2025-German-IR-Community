<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,375.66,15.42;1,89.29,106.66,168.76,15.42">Metric Weighted Ensemble Focal Loss for Snake Species Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.87,134.97,61.14,11.96"><forename type="first">Aarti</forename><surname>Balana</surname></persName>
							<email>@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Consultant</orgName>
								<orgName type="institution">WorldQuant Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,375.66,15.42;1,89.29,106.66,168.76,15.42">Metric Weighted Ensemble Focal Loss for Snake Species Identification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">40BE36E5D620F83F70706372C794B2C0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metric-Weighted Ensemble Focal Loss</term>
					<term>Snake Species Identification</term>
					<term>Imbalanced Dataset</term>
					<term>Efficient Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper outlines the participation in the LifeCLEF-SnakeCLEF 2023 challenge for automated identification of snake species. A metric-weighted ensemble loss function was introduced. The proposed loss function was put to test with various deep learning architectures, including Data Efficient Image Transformers (DEiT), EfficientNet and DinoV2. The SnakeCLEF 2023 challenge offers a substantial database comprised of images and associated recording locations for 1,784 snake species, serving as a resource to address this identification problem. The DEiT model trained with proposed loss outperformed the EfficientNetB0 model trained with focal loss by 49% in terms of the F1 score (macro). In the context of the SnakeCLEF2023 challenge, highest F1 score (macro) of 0.36 was achieved. Notably, our experiments forego the usage of metadata such as region or country, focusing solely on image based classification. The development of such a system holds significant potential to reduce snakebite induced mortality and morbidity, aligning closely with global health and biodiversity objectives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Ophidian Envenomation (OE), recognized as the second most fatal neglected tropical disease, presents a significant global health crisis. Each year OE impacts approximately 1.8 to 2.7 million individuals <ref type="bibr" coords="1,162.08,459.46,11.54,10.91" target="#b0">[1]</ref>, predominantly those residing in underdeveloped, remote, and politically marginalized tropical regions. The annual death toll from OE ranges from 81,410 to 137,880, while around 400,000 <ref type="bibr" coords="1,183.25,486.56,12.68,10.91" target="#b0">[1]</ref> survivors endure permanent physical and psychological trauma, underscoring the urgent necessity for focused attention on this health issue. Ophidian envenomation predominantly impacts individuals in Africa, Asia, and Latin America. Asia alone witnesses up to 2 million cases <ref type="bibr" coords="1,184.59,527.21,12.99,10.91" target="#b0">[1]</ref> of snake venom intoxication annually. Concurrently, Africa records an estimated 435,000 to 580,000 <ref type="bibr" coords="1,233.25,540.76,12.95,10.91" target="#b1">[2]</ref> instances each year that necessitate medical intervention due to snake bites. The appropriate administration of antivenoms can be a lifesaver, yet it often depends upon the correct taxonomic identification (family, genus, and species) of the snake in  question. An automated system capable of providing a preliminary identification to healthcare professionals using a low quality photograph could expedite the treatment process.</p><p>The SnakeCLEF2023 challenge <ref type="bibr" coords="2,244.82,289.34,11.48,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,260.05,289.34,7.52,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,271.31,289.34,9.03,10.91" target="#b4">5]</ref> urged participants to experiment upon different deep learning methodologies for an accurate classifier for snake species, adaptable to diverse conditions. This challenge was integrated into the LifeCLEF 2023 research platform, which is dedicated to the automated identification of species <ref type="bibr" coords="2,342.68,329.99,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,357.80,329.99,9.03,10.91" target="#b6">7]</ref> across five unique challenges. The approach for loss function implemented in this paper draws inspiration from the metric, venomous species confusion error, mentioned in the challenge <ref type="bibr" coords="2,381.33,357.09,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,396.34,357.09,7.65,10.91" target="#b8">9]</ref>. The objective of the SnakeCLEF challenge <ref type="bibr" coords="2,187.03,370.64,11.30,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,201.05,370.64,7.45,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,211.22,370.64,7.45,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,221.39,370.64,7.45,10.91" target="#b7">8,</ref><ref type="bibr" coords="2,231.56,370.64,8.03,10.91" target="#b8">9]</ref>is to enhance the robust and accurate identification of snake species from photographs. An operational system proficient in snake species recognition from images could substantially enrich the data on snakebite ecoepidemiology, clinical management (like precise antivenom administration), and patient outcomes <ref type="bibr" coords="2,370.22,411.28,16.48,10.91" target="#b9">[10,</ref><ref type="bibr" coords="2,389.42,411.28,12.57,10.91" target="#b10">11,</ref><ref type="bibr" coords="2,404.72,411.28,12.57,10.91" target="#b11">12,</ref><ref type="bibr" coords="2,420.01,411.28,12.56,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,435.30,411.28,12.36,10.91" target="#b13">14]</ref>. Despite the fact that only about 20% of global snake species are medically significant <ref type="bibr" coords="2,408.54,424.83,16.09,10.91" target="#b11">[12]</ref>, understanding of their epidemiological relevance (like the count of snakebites each species is accountable for in different regions) remains limited. Moreover, the knowledge about the geographical distribution of various snake species is far from complete. It is not uncommon for the majority of images of a particular snake species to come from a small group of countries or even a single country which in turn bias the model and model trained on snake species in one region is not detected in another region because of environmental difference causing the difference in the appearance. On the other hand, several snake species resemble species from other continents, with which they share no overlap in their geographic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The automated identification of snake species utilizing machine learning remains an infrequently explored field, primarily due to the scarcity of annotated image datasets. James et al. <ref type="bibr" coords="2,487.91,605.40,18.07,10.91" target="#b14">[15]</ref> proposed a semi-automated technique that involved extracting taxonomical features from images to discriminate among six different species. Amiza Amir et al. <ref type="bibr" coords="2,405.74,632.50,18.05,10.91" target="#b15">[16]</ref> introduced Image Classification for Snake Species Using Machine Learning Techniques, where they investigated the accuracy of five state-of-the-art machine learning techniques -decision tree J48, nearest neighbors, k-nearest neighbors (k-NN), backpropagation neural network, and naive Bayesfor image-based snake species identification problem. They showed backpropagation neural network and nearest neighbour are highly accurate with greater than 87 % accuracy. Recent studies have employed automated feature extraction strategies, such as texture features <ref type="bibr" coords="3,476.22,127.61,17.82,10.91" target="#b15">[16]</ref> or deep learning methodologies <ref type="bibr" coords="3,217.92,141.16,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="3,231.73,141.16,12.50,10.91" target="#b16">17,</ref><ref type="bibr" coords="3,246.80,141.16,12.50,10.91" target="#b17">18,</ref><ref type="bibr" coords="3,261.87,141.16,13.95,10.91" target="#b18">19]</ref> as manual feature extraction of snake species data is time taking and requires some expertise in the feature engineering and in-depth understanding of the dataset.</p><p>Image-Based Classification of Snake Species Using Convolutional Neural Network (CNN) by Isa Setiawan et al. <ref type="bibr" coords="3,187.19,195.36,18.02,10.91" target="#b19">[20]</ref> experimented with three CNN architectures using a dataset of 415 snake images from five common hazardous venomous snake species in Indonesia where snake images are classified the snake images with a high accuracy of 82%. Patel et al. <ref type="bibr" coords="3,444.54,222.46,17.99,10.91" target="#b16">[17]</ref> deployed deep learning strategies to create a smartphone application that could distinguish images of nine snake species native to the GalÃ¡pagos Islands in Ecuador. They have fused three data sources together resulting a total of 250 images for the training dataset. After testing various model architectures for object detection and image classification, the model based on the Faster Region-based Convolutional Neural Network (Faster R-CNN) <ref type="bibr" coords="3,356.63,290.20,17.76,10.91" target="#b20">[21]</ref> ResNet <ref type="bibr" coords="3,409.85,290.20,17.75,10.91" target="#b21">[22]</ref> demonstrated the highest classification accuracy at 75%. However, the authors emphasized the necessity for larger training samples for further research in this area. Chamath Abeysinghe et al. <ref type="bibr" coords="3,425.87,317.30,17.76,10.91" target="#b22">[23]</ref> experimented classification using Siamese Networks <ref type="bibr" coords="3,257.17,330.85,17.75,10.91" target="#b23">[24]</ref> and explored the applicability of single shot learning techniques along with deep neural networks to solve the snake image classification problem. An artificial intelligence (AI) model to identify snakes from across the world: Opportunities and challenges for global health and herpetology, Isabelle Bolon et al., <ref type="bibr" coords="3,413.01,371.50,16.41,10.91" target="#b24">[25]</ref>, they developed an AI model based on Vision Transformer (ViT) <ref type="bibr" coords="3,317.76,385.05,16.41,10.91" target="#b25">[26]</ref>. Snake Detection and Classification using Deep Learning by Zihan Yang et al. <ref type="bibr" coords="3,278.16,398.60,17.98,10.91" target="#b26">[27]</ref> used different deep learning models, YOLO v3, Tiny YOLO, Faster RCNN ResNet and others for snake detection and classification problem on Australian snake dataset. The dataset used in <ref type="bibr" coords="3,295.12,425.70,18.02,10.91" target="#b26">[27]</ref> paper was a collection from Google Image through web crawling and ImageNet dataset. In SnakeCLEF Challenge 2020, FHDO BCSG <ref type="bibr" coords="3,483.95,439.25,22.03,10.91" target="#b27">[28]</ref> used combination of image and location information for snake species identification using object detection and EfficientNets. In SnakeCLEF Challenge 2022, GG <ref type="bibr" coords="3,401.88,466.34,17.93,10.91" target="#b28">[29]</ref> introduced a novel architecture CoLKA-Net based on <ref type="bibr" coords="3,245.11,479.89,41.74,10.91">VAN [30]</ref> and CoAtNet <ref type="bibr" coords="3,350.53,479.89,16.25,10.91" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>In SnakeCLEF challenge 2023 <ref type="bibr" coords="3,216.99,538.52,11.24,10.91" target="#b4">[5,</ref><ref type="bibr" coords="3,230.10,538.52,7.49,10.91" target="#b8">9]</ref>, a comprehensive developed dataset, which comprises 103,404 snake observations with a total of 182,261 photographs. These images represent 1,784 unique snake species sighted across 214 countries. The data was gathered from online biodiversity platforms, namely iNaturalist and Herpmapper. It is important to note that the provided dataset exhibits a pronounced long-tailed class distribution, with the most frequently observed species, Natrix Natrix, represented by 1,262 observations (encompassing 2,079 images), while the least observed species have merely 3 recorded observations <ref type="bibr" coords="3,331.29,619.81,11.37,10.91" target="#b7">[8]</ref>. This dataset also includes metadata that is country code and name of the species. This dataset is bifurcated into two components, serving as training data where one part includes commonly observed species, while the other contains data pertaining to rarer snake species. Note that in this work, the metadata is not included to maintain simplicity and considering the scenario when metadata might not be available.</p><p>As displayed in Figure <ref type="figure" coords="4,200.12,404.60,3.66,10.91" target="#fig_1">1</ref>, the bar plot representing absolute class frequencies, the distribution of snake species in the dataset exhibits a pronounced imbalance. Despite the considerable size of this dataset, it contains numerous challenging examples that make classification a daunting task. For instance, Figure <ref type="figure" coords="4,203.77,445.25,5.07,10.91" target="#fig_2">2</ref> presents images of snakes demonstrating a significant variation in the surfaces on which they are found and some snake species are less distinguishable w.r.t the surface itself. These variations further complicate the classification process. The mathematical formulation of Focal Loss is -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Metric-Weighted Ensemble Focal Loss</head><formula xml:id="formula_0" coords="5,226.57,109.71,280.07,13.13">ğ¹ ğ¿(ğ‘ ğ‘¡ ) = -ğ›¼ ğ‘¡ (1 -ğ‘ ğ‘¡ ) ğ›¾ log(ğ‘ ğ‘¡ )<label>(1)</label></formula><p>where ğ¹ ğ¿(ğ‘ ğ‘¡ ) presents Focal loss, ğ›¼ ğ‘¡ is a balancing factor (usually set to 0.25) that balances the importance of positive and negative samples ( "positive" and "negative" samples are also often used to refer to true positive and false positive), ğ›¾ is a focusing parameter (usually set to 2) and ğ‘ ğ‘¡ is the model's estimated probability for the true class at observation t. In the observations mentioned in Lin et al., <ref type="bibr" coords="5,195.08,190.18,14.74,10.91" target="#b31">[32]</ref>, when an example is misclassified and ğ‘ ğ‘¡ is small, the modulating factor is near 1 and the loss is unaffected. As ğ‘ ğ‘¡ â†’ 1, the factor goes to 0 and the loss for well-classified examples is down-weighted. Also, the focusing parameter ğ›¾ smoothly adjusts the rate at which easy examples are downweighted.</p><p>Another variation of Focal loss, Equalized focal loss <ref type="bibr" coords="5,336.13,244.38,16.38,10.91" target="#b32">[33]</ref>, on the other hand, was designed to address another imbalance, which is the within-class sample imbalance, a common issue in long-tailed classification problems. In a multi-class classification setting, applying a single modulating factor uniformly across all classes (as done in focal loss) might not be the best approach, especially when there is a large class imbalance. This is because the modulating factor does not take into account the relative frequency of the classes in the dataset. In this type of problem, some classes have a lot more examples than others. Equalized focal loss tries to mitigate the issue by estimating class-wise weights to balance the influence of each class. This makes the model focus more on the under represented classes. The Equalized focal loss for the j-th category as-</p><formula xml:id="formula_1" coords="5,220.28,390.20,286.36,14.58">ğ¸ğ¹ ğ¿(ğ‘ ğ‘¡ ) = -ğ›¼ ğ‘¡ (1 -ğ‘ ğ‘¡ ) ğ›¾ ğ‘— log(ğ‘ ğ‘¡ )<label>(2)</label></formula><p>Here, ğ›¼ ğ‘¡ and ğ‘ ğ‘¡ are identical to those in the original focal loss. The parameter ğ›¾ ğ‘— acts as the focusing factor for the j-th category, paralleling the role of ğ›¾ in traditional focal loss. As mentioned in Lin et al., <ref type="bibr" coords="5,195.17,440.54,14.75,10.91" target="#b31">[32]</ref>, varying values of ğ›¾ correspond to different degrees of imbalance between positive and negative instances. For rare categories positive-negative imbalance, a larger ğ›¾ ğ‘— is employed. In cases of categories with minor imbalance, a smaller ğ›¾ ğ‘— is suitable. In equalised focal loss <ref type="bibr" coords="5,193.53,481.19,16.41,10.91" target="#b32">[33]</ref>, the focusing factor ğ›¾ ğ‘— is decomposed into two parts, namely, a category-agnostic parameter ğ›¾ ğ‘ and a category-specific parameter ğ›¾ ğ‘— ğ‘£ :</p><formula xml:id="formula_2" coords="5,268.48,517.48,238.16,14.19">ğ›¾ ğ‘— = ğ›¾ ğ‘ + ğ›¾ ğ‘— ğ‘£<label>(3)</label></formula><p>Here, ğ›¾ ğ‘ signifies the focusing factor in a balanced data scenario, regulating the fundamental behavior of the classifier. On the other hand, ğ›¾ ğ‘— ğ‘£ â‰¥ 0 is a variable parameter tied to the imbalance degree of the j-th category. It dictates the extent to which the learning focus is on addressing positive-negative imbalance issues.</p><p>In this paper, both loss functions are utilized. As mentioned in Eq.( <ref type="formula" coords="5,389.65,597.95,3.94,10.91" target="#formula_0">1</ref>), focal loss equally treats the learning of all categories with the same modulating factor, so it is element-wise multiplied with the weights obtained by the metric mentioned in <ref type="bibr" coords="5,339.81,625.05,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="5,354.40,625.05,9.03,10.91" target="#b8">9]</ref> for Metric Weighted Ensemble Focal Loss. The metric representation is -</p><formula xml:id="formula_3" coords="6,159.31,97.08,347.33,78.21">ğ¿(ğ‘¦, ğ‘¦ Ë†) = â§ âª âª âª âª âª âª â¨ âª âª âª âª âª âª â© 1 if ğ‘¦ = ğ‘¦ Ë†, 1.5 if ğ‘¦ Ì¸ = ğ‘¦ Ë†ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦) = 0 ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦ Ë†) = 0 2 if ğ‘¦ Ì¸ = ğ‘¦ Ë†ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦) = 0 ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦ Ë†) = 1 2 if ğ‘¦ Ì¸ = ğ‘¦ Ë†ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦) = 1 ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦ Ë†) = 1 5 if ğ‘¦ Ì¸ = ğ‘¦ Ë†ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦) = 1 ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘¦ Ë†) = 0 (4)</formula><p>where p is a function and p(s)=1 if species is venomous, p(s)=0, when it is non-venomous, y is target class and ğ‘¦ Ë†is predicted class. Note that there is a small change done on the original ğ‘œğ‘Ÿğ‘–ğ‘” -ğ¿(ğ‘¦, ğ‘¦ Ë†) metric for the purpose of using it as a weight. After the calculation the final formulation of the proposed Metric-Weighted Ensemble Focal Loss is -</p><formula xml:id="formula_4" coords="6,198.97,251.51,307.66,11.36">ğ‘€ ğ‘Š ğ¸ğ¹ ğ¿ = ğ¸ğ¹ ğ¿(ğ‘ ğ‘¡ ) + ğ¿(ğ‘¦, ğ‘¦ Ë†) âŠ™ ğ¹ ğ¿(ğ‘ ğ‘¡ )<label>(5)</label></formula><p>where MWEFL, Metric-Weighted Ensemble Focal Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sampling Strategy</head><p>In a normal random sampling scenario, every data point in the dataset has an equal chance of being included in a batch. This can cause problems if the dataset is imbalanced, which is our case, meaning some classes have more samples than others. If a model is trained on these batches, it might perform poorly on the under represented classes simply because it didn't see enough of those examples during training. This is where WeightedRandomSampler <ref type="bibr" coords="6,276.10,397.10,17.75,10.91" target="#b33">[34]</ref> is used in this paper. It assigns each data point in the dataset a weight. The probability that a data point gets included in a batch is proportional to its weight. The idea of weighted random sampling is a general statistical technique which uses multinomial distribution for sampling and is not tied to a specific paper or proposal. That said, many papers in machine learning that deal with imbalanced datasets will make use of weighted sampling or similar techniques e.g., Learning from Imbalanced Data by He and Garcia <ref type="bibr" coords="6,487.15,464.85,16.41,10.91" target="#b34">[35]</ref>. Please note that paper, Learning from Imbalanced Data <ref type="bibr" coords="6,336.79,478.40,16.24,10.91" target="#b34">[35]</ref>, discusses a variety of techniques, not just weighted sampling.</p><p>The mathematical steps for the WeightedRandomSampler as mentioned in pytorch are-</p><p>â€¢ Normalize the weights to create a discrete probability distribution-</p><formula xml:id="formula_5" coords="6,244.00,545.51,262.64,27.08">ğ‘ƒ (ğ‘–) = ğ‘¤ ğ‘– âˆ‘ï¸€ ğ‘— ğ‘¤ ğ‘— âˆ€ğ‘— âˆˆ dataset<label>(6)</label></formula><p>â€¢ Then, draw a sample index i from the discrete distribution:</p><formula xml:id="formula_6" coords="6,291.74,605.86,214.90,10.91">ğ‘– âˆ¼ ğ‘ƒ (ğ‘–)<label>(7)</label></formula><p>â€¢ The final output is a sequence of indices, with a total length equal to to the sampled data points. It's worth noting that because this sampling process is inherently probabilistic, different runs will generally yield different results unless a consistent random seed is used.  Before feeding the images into the model, they are pre-processed using a saliency-based object detection method proposed by Rail Chamidullin in 2022, <ref type="bibr" coords="7,339.34,288.38,16.34,10.91">[36,</ref><ref type="bibr" coords="7,358.41,288.38,12.26,10.91" target="#b35">37]</ref>. Given the challenging nature of the dataset, where differentiating a snake from its surrounding environment can be arduous, this process proves beneficial. By eliminating irrelevant sections of the image, the model is more likely to focus on the features of the snake, thus enhancing its performance. Figures <ref type="figure" coords="7,500.81,329.03,5.17,10.91" target="#fig_4">3</ref> and<ref type="figure" coords="7,108.37,342.57,5.05,10.91" target="#fig_5">4</ref> showcase some of the outputs generated by this saliency-based object detection method [36, 37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Exploring Different Architectural Designs Through Experimentation</head><p>In order to address this imabalanced data classification task, a series of experiments were conducted. These experiments included DEiT <ref type="bibr" coords="7,291.01,419.40,16.10,10.91" target="#b36">[38]</ref>, EfficientNets <ref type="bibr" coords="7,372.76,419.40,16.10,10.91" target="#b37">[39]</ref>, and DinoV2 <ref type="bibr" coords="7,450.19,419.40,16.11,10.91" target="#b38">[40]</ref>. Among these, DEiT <ref type="bibr" coords="7,143.44,432.95,17.90,10.91" target="#b36">[38]</ref> emerged as the most proficient when using an input image size of 384 Ã— 384.</p><p>An exploratory analysis involving DinoV2 <ref type="bibr" coords="7,280.01,446.50,17.92,10.91" target="#b38">[40]</ref> was also conducted, primarily focusing on the PCA <ref type="bibr" coords="7,112.55,460.05,17.91,10.91" target="#b39">[41]</ref> features, as exhibited in Figure <ref type="figure" coords="7,272.49,460.05,3.74,10.91" target="#fig_7">5</ref>.</p><p>These figures demonstrate the model's adeptness at learning accurate features. Additionally, DinoV2 <ref type="bibr" coords="7,127.36,487.15,18.06,10.91" target="#b38">[40]</ref> was trained for a single epoch, a process that took approximately two and a half hours, yet yielded a commendable macro F1 score at first pass than other experimented models. This performance surpassed the first epoch results of other models. However, due to the substantial time consumption, this model was not further pursued in the current study. Nonetheless, its promising initial results mark it as a potentially worthwhile experiment for future research. These models were trained using the proposed loss function. Individually focal loss <ref type="bibr" coords="7,108.91,568.44,17.91,10.91" target="#b31">[32]</ref> and equalized focal loss <ref type="bibr" coords="7,237.40,568.44,17.91,10.91" target="#b32">[33]</ref> were also being utilized for training experiments.</p><p>The GPUs utilized for this research were the A6000 48 GiB GPU with 45 GiB RAM from Paperspace and the A100 from Colab. It is significant to mention that due to limited resources, the number of training epochs was kept under 30. Apart from the Metric Weighted Ensemble Focal Loss (MWEFL), other loss functions such as the Equalized Focal Loss <ref type="bibr" coords="7,428.02,622.64,17.71,10.91" target="#b32">[33]</ref>(eFocal) were also explored.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Table <ref type="table" coords="8,116.43,463.00,5.17,10.91" target="#tab_0">1</ref> presents the performance comparison of various model architectures used in the experiments. Each model is evaluated on three distinct metrics -F1 score (macro), snake score (a custom metric employed as a weight in our MWEFL loss function and introduced in the challenge <ref type="bibr" coords="8,135.42,503.65,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="8,149.78,503.65,7.40,10.91" target="#b8">9]</ref>), and top-3 accuracy. The tabulated results provide a clear understanding of the strengths and weaknesses of each architecture in tackling the given classification task. Despite the fact that the highest macro F1-score was achieved by the DEiT model trained using eFocal loss (F1-score (macro) 0.36), the DEiT model trained with MWEFL (F1-score (macro) 0.35) outperformed the others in terms of the lowest snake score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and Future Work</head><p>The research conducted in this study involved numerous experiments, which could be further improved with access to a larger and more balanced dataset and by running more epochs. Specifically, for those species with a smaller number of samples, one could artificially boost their representation through offline augmentation techniques.</p><p>Another promising direction for future work could be to treat this problem as a similarity search task. In this approach, we could construct a database of embeddings for different appearances of all species. Then, for a new image, we calculate the similarity score between the new image and the images in the database. This method could potentially make the model more robust and improve its accuracy by leveraging a more comprehensive understanding of the variability in appearances among species.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,97.36,158.34,171.07,9.96;2,318.18,158.34,187.81,9.96"><head></head><label></label><figDesc>(a) Distribution of Train Dataset Class IDs (b) Distribution of Rare Train Dataset Class IDs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,176.82,416.69,8.93;2,89.29,188.83,416.69,8.87;2,89.29,200.78,416.70,8.87;2,89.29,212.74,416.70,8.87;2,89.29,224.69,322.12,8.87;2,89.29,84.77,187.49,66.69"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Exponentially Decreasing Distribution of Class IDs in the Train Dataset showcases the long-tailed distribution of example classes. The x-axis represents the unique class IDs, while the y-axis represents the frequency of each class ID in the dataset. It can be observed that a small number of classes have a high frequency but the majority of classes have a significantly lower frequency with many appearing just a few times which indicates a significant class imbalance.</figDesc><graphic coords="2,89.29,84.77,187.49,66.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,315.99,416.70,8.93;4,89.29,327.99,416.70,8.87;4,89.29,339.95,88.25,8.87;4,89.29,196.87,125.01,93.76"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The dataset has variety of images which are quite challenging samples. Some snakes in the image are less distinguishable w.r.t the surface itself. These variations further complicate the classification process.</figDesc><graphic coords="4,89.29,196.87,125.01,93.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,89.29,530.97,416.69,10.91;4,89.29,544.52,416.70,10.91;4,89.29,558.07,43.82,10.91;4,100.20,571.62,405.78,10.91;4,89.29,585.17,416.69,10.91;4,89.29,598.72,416.69,10.91;4,89.29,612.27,416.69,10.91;4,89.29,625.82,416.69,10.91;4,89.29,639.36,297.62,11.36;4,386.92,638.14,4.36,6.99;4,394.98,639.36,112.21,10.91;4,88.89,652.91,418.77,10.91"><head>Focal</head><label></label><figDesc>loss and equalized focal loss are often used to handle imbalanced datasets because they are designed to give more weight to hard, misclassified examples and less weight to well-classified examples. Focal loss was introduced in the paper Focal Loss for Dense Object Detection by Lin et al.,[32] as a way to address the class imbalance issue in object detection. It was designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training. The central idea of the focal loss is to reduce the contribution of well classified examples, allowing the training process to focus more on hard examples. This is achieved by adding a modulating factor (1 -ğ‘ ğ‘¡ ) ğ›¾ to the cross entropy loss, with tunable focusing parameter ğ›¾ â‰¥ 0, that decreases the weight of well-classified examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,174.78,205.54,8.93;7,89.29,101.36,187.51,60.86"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Saliency Object Detection -DEiT model</figDesc><graphic coords="7,89.29,101.36,187.51,60.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,318.47,191.95,183.24,8.93;7,318.47,84.19,187.51,95.19"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BoundingBox on Augmented Data</figDesc><graphic coords="7,318.47,84.19,187.51,95.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,89.29,237.23,418.22,8.93;8,89.29,249.23,416.70,8.87;8,89.29,261.19,416.70,8.87;8,88.93,273.14,281.30,8.87;8,135.55,85.54,95.83,126.33"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fig (a) represents Original image of snake on which Dinov2 is executed for feature analysis. Fig (b) represents PCA features components extracted from Dinov2 model exhibiting features learned from the original image on the left side. Here, Dinov2 model feature extraction PCA anaysis exhibits well learned spacial and texture features of the original snake image.</figDesc><graphic coords="8,135.55,85.54,95.83,126.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,306.24,32.19,8.93"><head>Table 1</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,318.25,417.00,93.73"><head></head><label></label><figDesc>Table presents training loss, validation loss, F1 scores (macro), top-3 score of training different models (with pre-trained weights) with proposed loss and individual losses</figDesc><table coords="8,118.89,349.82,357.50,62.16"><row><cell>Model</cell><cell>loss</cell><cell cols="5">train loss val loss snake score mac-f1 score top 3</cell></row><row><cell>DEit</cell><cell>eFocal</cell><cell>0.46</cell><cell>4.59</cell><cell>-</cell><cell>0.36</cell><cell>0.72</cell></row><row><cell>DEit</cell><cell>MWEFL</cell><cell>0.64</cell><cell>8.06</cell><cell>7165</cell><cell>0.35</cell><cell>0.72</cell></row><row><cell>DinoV2</cell><cell>Focal</cell><cell>2.04</cell><cell>3.44</cell><cell>15666</cell><cell>0.15</cell><cell>0.45</cell></row><row><cell>EfficientNet(b0)</cell><cell>Focal</cell><cell>0.03</cell><cell>3.18</cell><cell>-</cell><cell>0.24</cell><cell>0.55</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">Acknowledgments</head><p>I would like to extend my sincere thanks to the organizing team for providing the private evaluation scores and reviewing this working note.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,285.51,394.03,10.91;9,112.66,299.06,238.73,10.91" xml:id="b0">
	<monogr>
		<author>
			<orgName type="collaboration" coords="9,112.66,285.51,49.86,10.91">WHO Team</orgName>
		</author>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/snakebite-envenoming" />
		<title level="m" coord="9,170.58,285.51,182.41,10.91">Who statistics on ophidian envenomation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,312.61,393.33,10.91;9,112.66,326.16,393.32,10.91;9,112.41,339.71,85.18,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,363.19,312.61,142.80,10.91;9,112.66,326.16,343.68,10.91">Risks of snakebite and challenges to seeking and providing treatment for agro-pastoral communities in tanzania</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-M</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Heitz-Tokpa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kreppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,464.72,326.16,41.27,10.91">PLoS one</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">280836</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,353.26,365.92,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Lifeclef</surname></persName>
		</author>
		<ptr target="https://www.imageclef.org/LifeCLEF2023" />
		<title level="m" coord="9,158.94,353.26,76.21,10.91">Lifeclef-snakeclef</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,366.81,394.53,10.91;9,112.66,380.36,395.01,10.91;9,112.30,393.91,393.68,10.91;9,112.66,407.46,394.53,10.91;9,112.66,421.01,393.33,10.91;9,112.66,434.55,289.43,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,383.16,393.91,122.82,10.91;9,112.66,407.46,171.35,10.91">Lifeclef 2023 teaser: Species identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ©au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HruÅº</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Moussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,392.71,421.01,113.28,10.91;9,112.66,434.55,38.01,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,448.10,393.33,10.91;9,112.66,461.65,393.33,10.91;9,112.66,475.20,107.76,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,342.27,448.10,163.72,10.91;9,112.66,461.65,201.93,10.91">Overview of snakeclef 2023: Snake identification in medically important scenarios</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,337.00,461.65,168.99,10.91;9,112.66,475.20,77.06,10.91">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,488.75,394.52,10.91;9,112.66,502.30,393.33,10.91;9,112.66,515.85,393.33,10.91;9,112.66,529.40,393.33,10.91;9,112.66,542.95,394.53,10.91;9,112.66,556.50,123.33,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,236.07,502.30,269.91,10.91;9,112.66,515.85,298.34,10.91">Overview of lifeclef 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.87,515.85,72.12,10.91;9,112.66,529.40,393.33,10.91;9,112.66,542.95,128.19,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 22-25, 2020. 2020</date>
			<biblScope unit="page" from="342" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,570.05,394.53,10.91;9,112.66,583.60,394.53,10.91;9,112.66,597.15,393.33,10.91;9,112.66,610.69,395.17,10.91;9,112.66,624.24,394.52,10.91;9,112.33,637.79,329.01,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,112.66,597.15,393.33,10.91;9,112.66,610.69,135.53,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HruÅº</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.76,610.69,236.08,10.91;9,112.66,624.24,389.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">September 18-23, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,651.34,394.03,10.91;9,112.66,664.89,73.52,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lifeclef-Hf</forename></persName>
		</author>
		<ptr target="https://huggingface.co/spaces/competitions/SnakeCLEF2023" />
		<title level="m" coord="9,174.25,651.34,85.10,10.91">Lifeclef snakeclef hf</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,359.35,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,158.94,86.97,59.60,10.91">snakeclef</title>
		<author>
			<persName coords=""><surname>Lifeclef</surname></persName>
		</author>
		<ptr target="https://www.imageclef.org/SnakeCLEF2023" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,394.53,10.91;10,112.66,114.06,393.32,10.91;10,112.66,127.61,373.51,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,112.66,114.06,393.32,10.91;10,112.66,127.61,235.47,10.91">Identifying the snake: First scoping review on practices of communities and healthcare providers confronted with snakebite across the world</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,356.79,127.61,41.47,10.91">PLoS one</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">229989</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,395.17,10.91;10,112.66,154.71,393.33,10.91;10,112.66,168.26,394.53,10.91;10,112.66,181.81,22.49,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,236.98,154.71,269.00,10.91;10,112.66,168.26,206.32,10.91">Snakebite and snake identification: empowering neglected communities and health-care providers with ai</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SalathÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,326.39,168.26,114.94,10.91">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="e203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,394.62,10.91;10,112.28,208.91,382.78,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,358.78,195.36,148.49,10.91;10,112.28,208.91,214.32,10.91">Overview of the snakeclef 2020: Automatic snake species identification challenge</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.00,208.91,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,265.83,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>De CastaÃ±eda</surname></persName>
		</author>
		<title level="m" coord="10,334.93,222.46,171.06,10.91;10,112.66,236.01,233.91,10.91">Overview of snakeclef 2021: Automatic snake species identification with country-level focus</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.33,10.91;10,112.66,263.11,201.64,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<title level="m" coord="10,297.11,249.56,208.88,10.91;10,112.66,263.11,169.72,10.91">Overview of snakeclef 2022: Automated snake species identification on a global scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,393.33,10.91;10,112.66,290.20,393.33,10.91;10,112.66,303.75,157.39,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,388.92,276.66,117.07,10.91;10,112.66,290.20,232.61,10.91">Discriminative histogram taxonomy features for snake species identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sugathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Raveendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.46,290.20,147.53,10.91;10,112.66,303.75,93.74,10.91">Human-Centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,393.33,10.91;10,112.66,330.85,394.61,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,141.85,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,336.92,317.30,169.07,10.91;10,112.66,330.85,148.83,10.91">Image classification for snake species using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A H</forename><surname>Zahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yaakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,283.07,330.85,224.20,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,21.62,10.91">Computational Intelligence in Information Systems: Proceedings of the Computational Intelligence in Information Systems Conference (CIIS 2016)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,393.32,10.91;10,112.66,385.05,393.33,10.91;10,112.41,398.60,60.01,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,446.56,371.50,59.43,10.91;10,112.66,385.05,348.36,10.91">Revealing the unknown: real-time recognition of galÃ¡pagos snake species using deep learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Khatod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Matijosaitiene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Gilkey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,469.28,385.05,36.71,10.91">Animals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,393.59,10.91;10,112.33,425.70,101.72,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,186.32,412.15,269.69,10.91">Impact of pretrained networks for snake species classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,482.24,412.15,24.01,10.91;10,112.33,425.70,71.82,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.32,10.91;10,112.66,452.79,394.53,10.91;10,112.66,466.34,34.86,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,380.81,439.25,125.17,10.91;10,112.66,452.79,67.24,10.91">Snake species identification and recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vasmatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kumbla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pimpalkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,225.32,452.79,226.18,10.91">IEEE Bombay Section Signature Conference (IBSSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.33,10.91;10,112.66,493.44,393.33,10.91;10,112.33,506.99,303.81,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,294.59,479.89,211.40,10.91;10,112.66,493.44,128.22,10.91">Image-based classification of snake species using convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Abdurrazaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suyanto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Utama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,286.93,493.44,219.06,10.91;10,112.33,506.99,194.84,10.91">International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,394.92,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,264.19,520.54,241.80,10.91;10,112.66,534.09,111.59,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,233.07,534.09,229.80,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,395.17,10.91;10,112.66,561.19,395.01,10.91;10,112.41,574.74,38.81,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,259.74,547.64,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,488.38,547.64,19.45,10.91;10,112.66,561.19,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,394.53,10.91;10,112.66,601.84,394.52,10.91;10,112.66,615.39,65.36,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,280.19,588.29,222.13,10.91">Snake image classification using siamese networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Welivita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Perera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.48,601.84,375.05,10.91">Proceedings of the 3rd International Conference on Graphics and Signal Processing</title>
		<meeting>the 3rd International Conference on Graphics and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,628.93,394.52,10.91;10,112.66,642.48,394.52,10.91;10,112.66,656.03,65.30,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,257.38,628.93,245.47,10.91">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.19,642.48,350.38,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4834" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,669.58,393.33,10.91;11,112.66,86.97,393.32,10.91;11,112.66,100.52,386.02,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,453.38,669.58,52.61,10.91;11,112.66,86.97,393.32,10.91;11,112.66,100.52,146.21,10.91">An artificial intelligence model to identify snakes from across the world: Opportunities and challenges for global health and herpetology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alcoba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ruiz De CastaÃ±eda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,267.43,100.52,143.35,10.91">PLoS neglected tropical diseases</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">10647</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,395.16,10.91;11,112.66,127.61,395.17,10.91;11,112.66,141.16,349.55,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="11,330.08,127.61,177.76,10.91;11,112.66,141.16,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,154.71,361.77,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="11,201.80,154.71,240.71,10.91">Snake detection and classification using deep learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sinnott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,394.53,10.91;11,112.66,181.81,393.32,10.91;11,112.66,195.36,394.53,10.91;11,112.66,208.91,22.69,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,246.68,181.81,259.30,10.91;11,112.66,195.36,265.73,10.91">Combination of image and location information for snake species identification using object detection and efficientnets</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boketta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Keibel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mense</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Michailutschenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Willemeit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.68,195.36,98.06,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,393.53,10.91;11,112.66,236.01,238.10,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="11,229.73,222.46,276.46,10.91;11,112.66,236.01,92.18,10.91">When large kernel meets vision transformer: A solution for snakeclef &amp; fungiclef</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,212.94,236.01,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.60,10.91;11,112.66,263.11,146.44,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m" coord="11,362.79,249.56,109.69,10.91">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,276.66,393.33,10.91;11,112.66,290.20,354.88,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="11,254.27,276.66,251.72,10.91;11,112.66,290.20,19.47,10.91">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,139.95,290.20,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,303.75,394.61,10.91;11,112.66,317.30,395.01,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="11,327.53,303.75,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,317.30,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,330.85,393.33,10.91;11,112.66,344.40,393.33,10.91;11,112.66,357.95,184.87,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="11,328.18,330.85,177.81,10.91;11,112.66,344.40,69.95,10.91">Equalized focal loss for dense long-tailed object detection</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,205.81,344.40,300.17,10.91;11,112.66,357.95,86.75,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6990" to="6999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,394.04,10.91;11,112.66,385.05,263.78,10.91" xml:id="b33">
	<monogr>
		<ptr target="https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler" />
		<title level="m" coord="11,112.66,371.50,53.15,10.91">pytorch2016</title>
		<imprint/>
	</monogr>
	<note>Weightedrandomsampling, Unknown year</note>
</biblStruct>

<biblStruct coords="11,112.66,398.60,393.33,10.91;11,112.66,412.15,169.07,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="11,200.81,398.60,134.59,10.91">Learning from imbalanced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,343.33,398.60,162.66,10.91;11,112.66,412.15,74.99,10.91">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,452.79,394.03,10.91;11,112.66,466.34,203.84,10.91" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="11,159.18,452.79,203.04,10.91">Fine grained object detection saliency method</title>
		<author>
			<persName coords=""><surname>Lifeclef</surname></persName>
		</author>
		<ptr target="https://github.com/chamidullinr/fine-grained-visual-recognition" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,479.89,393.33,10.91;11,112.66,493.44,393.33,10.91;11,112.66,506.99,215.99,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="11,408.51,479.89,97.48,10.91;11,112.66,493.44,236.79,10.91">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,378.12,493.44,127.86,10.91;11,112.66,506.99,75.67,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,520.54,394.53,10.91;11,112.66,534.09,346.82,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="11,178.42,520.54,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.29,534.09,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,547.64,394.53,10.91;11,112.66,561.19,393.33,10.91;11,112.66,574.74,232.00,10.91" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
		<title level="m" coord="11,291.05,561.19,214.93,10.91;11,112.66,574.74,49.76,10.91">Dinov2: Learning robust visual features without supervision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,588.29,394.62,10.91;11,112.66,601.84,184.61,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="11,220.72,588.29,132.25,10.91">Principal component analysis</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,361.58,588.29,145.70,10.91;11,112.66,601.84,105.75,10.91">Wiley interdisciplinary reviews: computational statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="433" to="459" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
