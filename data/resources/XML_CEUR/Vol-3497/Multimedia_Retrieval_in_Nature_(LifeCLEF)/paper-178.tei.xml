<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,451.18,17.04;1,72.02,96.20,451.08,17.04;1,72.02,116.96,161.97,17.04">Reading the Robot Mind -Presenting Internal Data Flow Within an AI for Classification of Bird Sounds in a Format Familiar to Subject Matter Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.02,149.86,74.98,10.80"><forename type="first">Paul</forename><surname>Nussbaum</surname></persName>
							<email>pnussbaum@ecpi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ECPI University</orgName>
								<address>
									<addrLine>800 Moorefield Park Drive</addrLine>
									<postCode>23236</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,451.18,17.04;1,72.02,96.20,451.08,17.04;1,72.02,116.96,161.97,17.04">Reading the Robot Mind -Presenting Internal Data Flow Within an AI for Classification of Bird Sounds in a Format Familiar to Subject Matter Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C2E00D44271FA10211C945C78AC5952</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>, Explain Ability, Quality Assurance 1</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An interactive Jupyter notebook <ref type="bibr" coords="1,268.49,212.41,12.28,9.50" target="#b0">[1]</ref> is built for the purpose of training and deploying a deep learning neural network artificial intelligence (AI) <ref type="bibr" coords="1,350.83,224.41,12.28,9.50" target="#b1">[2]</ref> to automatically identify birds from recorded audio <ref type="bibr" coords="1,194.21,236.53,11.16,9.50" target="#b2">[3]</ref>. The notebook allows the user to modify parameters along the training and classification (inference) pipeline and observe the results. As with traditional observation methods, the notebook lets users view visual representations (spectrograms, etc.) of input vectors for similar and different birds <ref type="bibr" coords="1,286.39,272.80,11.16,9.50" target="#b3">[4]</ref>. In addition to traditional methods, this notebook also presents data in its original format (audio recordings of birds). This is common practice for a field researcher or subject matter expert (SME) testing a microphone and recording system [5]they will want to listen to the recordings to see if they contain valid and sufficient information. The notebook <ref type="bibr" coords="1,188.21,321.04,12.28,9.50" target="#b5">[6]</ref> extends this intuitive and useful technique to individual neural network layers -working backwards towards a best estimate of the original input (referred to in this working note as "reading the robot mind"). The user can even provide just the "answer" (select a bird at the final output layer), and the reading the robot mind system will work backwards through the entire automated process and AI layers to let the SME hear a best approximation of what the AI has learned that bird sounds like.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The BirdCLEF 2023 challenge <ref type="bibr" coords="1,227.69,476.25,12.91,9.94" target="#b2">[3]</ref> is part of 2023 LifeCLEF <ref type="bibr" coords="1,362.11,476.25,12.78,9.94" target="#b6">[7]</ref> and involves identifying Eastern African bird species by sound. Specifically, the task is to develop computational solutions to process continuous audio data and recognize the species by their calls. The best entries to this contest challenge will be able to train reliable classifiers with limited training data. The training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been down sampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The goal of the notebook is therefore to develop this solution from this data.</p><p>The ability of AI systems to explain their classification conclusions (also called "explain ability" or "explainability") is gaining increased research focus, both to provide support for decisions <ref type="bibr" coords="1,473.86,577.53,11.70,9.94" target="#b7">[8]</ref>, as well as for "possibilities of exposing complex AI models to human users/operators in an interpretable and understandable ways." <ref type="bibr" coords="1,175.01,602.76,11.70,9.94" target="#b8">[9]</ref>. Towards the goals of the latter, this paper focuses on the information flow through a trained AI system, including ability to observe where important information may be discarded or corrupted in some way.</p><p>The notebook <ref type="bibr" coords="1,151.10,640.80,12.91,9.94" target="#b5">[6]</ref> lets a subject matter expert (SME) hear, visualize, and compare data every step and layer on the way. It allows comparison of samples from the same type of bird so the SME can see and hear if they are similar, as well as comparison of different birds so the SME can see and hear if they are different. These audio and visual recreations are available for the following steps in the AI system:</p><p>‚Ä¢ Segmentation, feature extraction (choice of Mel Spectrogram or MFCC, both having user selectable parameters), and conversion to image format with 8-bit quantization ‚Ä¢ Each layer of a Conv2d, MaxPool combo model, followed by Dense embedding and Dense softmax categorizer. This notebook provides the ability to work backwards through the system from any point and let the user see a best estimation of the features that were presented as input, as well as hear a best approximation of the bird recording from which they were extracted.</p><p>Finally, the reading the robot mind system allows the above analysis method to be used to select an arbitrary bird classification, and work backwards to an approximated input so that the SME can see and hear what the AI has learned that bird sounds like.</p><p>Note that due to the time and compute limitations imposed by the computing environment and contest rules provided, the notebook is divided into four public notebooks:</p><p>‚Ä¢ https://www.kaggle.com/code/pnussbaum/v15h-birdclef2023-mindreader -This notebook focuses on the Segmentation and Feature Extraction aspects of the AI solution, allowing users to make modifications and see and hear how much information is retained. ‚Ä¢ https://www.kaggle.com/code/pnussbaum/v15h-all-birdclef2023-mindreader -This notebook allows the user to use their final decision related to segmentation and feature extraction, and convert and save all the BirdClef2023 data into this format.</p><p>‚Ä¢ https://www.kaggle.com/code/pnussbaum/v16e-gpu-all-birdclef2023-mindreader -This notebook uses the final decisions noted above, and trains the entire AI for a longer period of time, achieving better accuracy, and saving the trained AI system. ‚Ä¢ https://www.kaggle.com/code/pnussbaum/v17b-all-birdclef2023-mindreader -This notebook brings all of this together for the sake of the contest submission and scoring. In the following sections, segmentation, feature extraction, quantization, AI model creation, training, and validation are discussed in detail. Also shown with formulas and examples are the aspects of the reading the robot mind system, including visualization of filters, recreation of input approximations based on outputs of intermediate and final layers of the AI system, and also the method whereby the output can be forced to an individual bird, and a best approximation of what that bird sounds like is created by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Segmentation, Feature Extraction, and Image Quantization Analysis</head><p>An automated segmentation algorithm is used, however, due to the rules of the BirdCLEF 2023 competition, the SME is not permitted to modify this algorithm <ref type="bibr" coords="2,352.94,551.13,11.77,9.94" target="#b2">[3]</ref>. After segmentation, the audio data is transformed into another domain (called feature extraction in this document), and finally saved as a two-dimensional grayscale image with 8-bit quantization. The quantization was chosen due to the power and memory constraints of the edge device that will be performing inference (bird classification) in the field.</p><p>The notebook allows the SME to try several Feature Extraction Algorithms and test the following:</p><p>‚Ä¢ Do the features visually look similar for the same bird, and different for different birds?</p><p>‚Ä¢ Is the similarity/difference enough to be able to visually classify which bird is which?</p><p>‚Ä¢ If the feature extraction algorithm is performed in reverse to recreate the inputted audio (or an approximation thereof, due to the lossy nature of feature extraction) -is the recreated sound clear enough for the SME to identify the bird? ‚Ä¢ If the answer is "no" to any of the above, allow fine tuning by the user The notebook demonstrates this with three different pre-selected methods, and also allows the SME to make further changes as they see fit. The pre-selected methods are all using the Librosa Python library feature extraction methods <ref type="bibr" coords="2,224.21,729.00,39.43,9.94">[10] [11]</ref> of:</p><p>1. Melspectrogram <ref type="bibr" coords="2,197.81,741.84,18.43,9.94" target="#b9">[10]</ref> with default settings and the number of mel scale frequency bands set to 16; yielding 16 features per frame.</p><p>2. Melspectrogram with default settings and the number of mel scale frequency bands set to 32; yielding 32 features per frame. 3. MFCC <ref type="bibr" coords="3,157.34,101.20,18.30,9.94" target="#b10">[11]</ref> with default settings, 64 mel scale frequency bands, and 16 final cepstral coefficients; yielding 16 features per frame. After feature extraction, all 5 second audio samples result in a 2-dimensinal image whose width is 313 frames and whose height is equal to the number of extracted features per frame, in grayscale format, with 8-bit quantization to a positive integer from 0 to 255.</p><p>Finally, a subset of samples from like birds, as well as a subset of samples of different birds are presented visually and via audio playback to the SME.   Once the SME is satisfied that the feature extraction algorithm contains sufficient quality information, all of the data is converted and saved to disk using the selected segmentation, feature extraction, and quantization method.</p><p>If the SME makes no changes, there is also a pre-selected method which uses option 2 above and therefore converts 16,941 audio files into 147,248 eight bit greyscale images of width 313 pixels by 32 pixels in height, representing 5 second audio segments, organized in sub-directories by bird classification, to be used in the training/validation pipeline described in the next section (also called "image files").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Layer-wise Analysis of the AI (Convolutional, Max Pooling, and Dense Layers)</head><p>A simple sequential (not residual) convolutional AI is used, with dimensions and architecture shown in the figure below. This was found to yield a satisfactory accuracy measure for the purposes of demonstrating the reading the robot mind system, while still falling within the running time and memory constraints imposed by the development platform and contest rules. The AI starts with a rescaling layer (converting 8-bit unsigned integers ranging from 0 to 255, to floating point numbers ranging from 0 to 1). After that, data augmentation layers are used to reduce overfitting, including a time shift (left to right) of up to 20% with 0 fill, and a Gaussian dropout multiplier randomly ranging from 0 to 10%. The remaining layers are sequentially calculated, and have dimensions and parameters shown below. Finally, not shown in in the image is the fact that all layers have a rectified linear unit (relu) activation except for the last dense layer, which uses a softened winner take all (softmax) activation for final bird classification.</p><p>Training of the AI is performed with a randomly selected test/training split of 80%/20% from the entire set of previously saved image files containing the extracted features. The training is run for 16 epochs for a total of 30,682 seconds (just over 8.5 hours) using the GPU P100 option yielding a final categorical cross entropy loss of 0.9117 on the training set and 1.4665 on the validation set, and final accuracy measures of 0.7790 on the training set and 0.7120 (or 71%) accuracy on the validation set. Below, the history of these values over the training epochs is graphed, showing some overfitting towards the later epochs, despite the data augmentation employed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing Filter Patches</head><p>The notebook allows visualization of the convolutional filter patches. Although this information is more useful to the AI programmer than it is to the SME, it is presented here since it is a step in the system of reading the robot mind. The algorithms used are similar to the "expansion" method <ref type="bibr" coords="6,485.02,381.67,17.01,9.94" target="#b11">[12]</ref>.</p><p>The first convolutional layer filter patches are visualized as simply the weights of each position in the filter.</p><p>P (l,n,x,y) | ùëô=1 = ùêπ ùëô,ùëõ,ùëùùëô,ùë•,ùë¶ | ùëô=1 Where</p><p>‚Ä¢ P is the image patch to be displayed ‚Ä¢ F is the filter in question, along with its weights ‚Ä¢ l is the convolutional layer (in this case, the first layer where l = 1)</p><p>‚Ä¢ pl is the channel/filter depth of the previous layer. In this case, there is no previous layer, and the color channel depth of the input is 1 (grayscale) so pl=1 ‚Ä¢ n is the filter in question, from 1 to the number of filters in that layer ‚Ä¢ x and y are the positions of the weights and/or image patch, and can range from 1 to the size of that filter patch (only square filters are used in this example) For subsequent layers, the visualizations are weighted sums of filter patches, with the weights coming from the layer being examined, and the filter patches coming from the cumulative prior layer visualizations. Note that since padding with zeroes was used, each subsequent convolutional layer will have a border. Where, in addition to the aforementioned variable definitions,</p><p>‚Ä¢ l is the convolutional layer, starting with 2 (since the first layer image patches were already calculated for layer 1) and going up to the number of convolutional layers in the AI ‚Ä¢ num_filtl is the number of filters for convolutional layer l ‚Ä¢ sizl is the size of the filters at convolutional layer l (only square filters are used in this example). ‚Ä¢ cbl is the cumulative border for layer l, found by adding the border size of previous convolutional layers. For example, if the filter size is 3x3, then the border size is 1. If the filter size is 5x5 then the border size is 2. The first eight filter patches for each convolutional layer are shown below. Observing the above figure, we can see that successive convolution layers manage successive levels of complexity. These are often described as edge detection first, then these are combined to form textures. Finally, the last convolution layers combine these textures to form patterns the AI is looking for in order to correctly identify birds.</p><p>Examining the filters, a programmer might be able to see if there are too many filters (many of them are similar), too many layers (filters are similar from layer to layer), and also specific patterns the CNN is looking for. Although filter visualization may be of use to the AI programmer, it can be a struggle for an SME who is not an AI programmer to understand this information, and so the reading the robot mind system will convert this information into a format familiar to the SME, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Using Filter Patches to Reconstruct Approximation of Input (for Convolutional and Max Pooling layers)</head><p>Using convolutional neural networks to extract patterns from image data necessarily involves loss of information. Each convolutional layer has only a limited number of filters. Even though that limited number of filters is greater than would be needed to form an ortho-normal basis set (each filter having all 0 weights except for a weight of one at a different individual pixel), the AI is not being trained as an auto-encoder, and is instead being trained to minimize the loss when classifying birds. Also, information is being lost through the use of the relu activation function, which makes all negative output values a 0. Most of all, information is being lost at the max pooling layers.</p><p>The notebook providing the reading the robot mind system attempts to present this reconstructed best approximation of the input in a manner common and usable by the SME. This includes using the same visualization and audio playback mechanisms described in the earlier section that allowed the SME to qualitatively measure the segmentation, feature extraction and quantization algorithms.</p><p>The below image shows a three examples from the image set, proceeding left to right, first showing the original input, and then the subsequent reconstruction of the input from the output of the seven convolutional layers. Looking at the last row of images in the above figure, the SME can see the original input on the left. The outputs of the first two convolutional seem to show a very good reconstruction of the original input, but the third convolutional layer shows a definite lossy blurring of the reconstructed input. This is due to the max pooling layer that preceded this convolutional layer. The SME can see a similar blurring at the last convolutional layer, which is also preceded by a max pooling layer. These blurring's represent a loss of data, and the SME may want to determine if too much information is lost.</p><p>To create the above reconstructions, first the AI is subdivided into many models, each ending with the output of a different convolutional layer. This is done after the training is completed, so the configuration and trained weights remain the same, just the final output is now one particular convolutional layer, instead of the bird classification output of the originally trained AI model.</p><p>Once the output of a particular convolutional layer is calculated for a particular input, an approximation of the original input is made from that output as per the following formula. Where, in addition to the aforementioned variable definitions,</p><formula xml:id="formula_0" coords="9,177.41,139.15,148.76,12.65">ùêºùê¥ ùëô = ‚àë ‚àë ‚àë ùëÇ ùëô,</formula><p>‚Ä¢ IAl is the Image Approximation of the input to the AI based on the output from layer l ‚Ä¢ P are the image patches for layer l, filter n, of width x and height y previously calculated and shown earlier.</p><p>‚Ä¢ O is the output of layer l, filter n, at position x, and y Using the reading the robot mind system, the SME can view the approximated input (visually as shown above) and can also hear and view the envelope of the reconstructed audio. Shown below is the reconstructed input based on the outputs of convolutional layers 2 and 3, discussed above, appearing before and after a max pooling layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8:</head><p>Information about data content presented to SME for qualitative analysis. Both left and right images show reconstructed input from the same audio segment, using the output of two different convolutional layers (left is before a max pooling layer, and right is afterwards) to reconstruct the input approximation. The SME is presented with the approximations of the extracted features input image, as well as a playback button and visual envelope of the audio.</p><p>When looking at the images above, the SME can see that quite a bit of information is discarded at the max pooling layer. When listening to the audio playback the SME can hear that it is possible to distinguish a bird call from the leftmost recreation, but it sounds like a very noisy signal to the right. The SME might be able to hear some aspects of a bird call (change of frequency, bursts of sound) but it is clearly not distinguishable as being a bird call, and it is quite impossible to use the audio to classify which bird is making the audio.</p><p>In this way, the SME can clearly see and hear that the AI solution is throwing away a great deal of information that would otherwise be used by the SME to classify birds. There are some possible conclusions from this information.</p><p>‚Ä¢ The AI is only trained on limited sounds from the real world, unlike the SME who has listened to many other audio sounds other than those from the training set, both relating to bird calls, as well as other everyday sounds the SME can categorize. These can include the sound of a person talking, the sound of crickets, etc. which are not specifically classified in the training set. There might be an opportunity to train the AI on other sounds like these. ‚Ä¢ The AI may be throwing away too much feature information, such as in the case of the max pooling layer. When max pooling is performed in the column direction, different frequency bands are effectively pooled, which may remove the ability to distinguish important frequency information that can be used to categorize the bird. There might be an opportunity to throw away less frequency information by reducing the max pooling in the column direction. ‚Ä¢ The AI may be throwing away too much time information, such as in the case when max pooling is performed in the row direction, different fast occurring events are effectively pooled, which may remove the ability to distinguish different events happening in sequence; information that can be used to categorize the bird. There might be an opportunity to throw away less time information by reducing the max pooling in the row direction. ‚Ä¢ As discussed above, the AI is purposely throwing away information to reduce training resources and minimize overfitting that might happen with a larger network. This reduction of dimensionality might be accomplished a different way, other than by max pooling, which could lead to less loss of information, as perceived by the SME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reconstructing an Approximation of Input from Dense Layer Outputs</head><p>The output of the dense layer just after the flatten layer also represents a loss of information, because the flatten layer has (using the example settings) has 159,744 values, whereas the output of the dense layer following this (also called the embedding layer) has only 128 neurons. This represents a great reduction in dimensionality, and therefore a great opportunity for loss of information.</p><p>To calculate this input approximation based on the output of this dense layer, we estimate the output of the prior convolutional layer by multiplying the output of the dense layer by it's trained weights, and then performing the flatten function in reverse. This gives us approximations of the output of that prior convolutional layer, after which the above formulas are used to reconstruct and approximation of the input.</p><p>The same can be done from the output of the last (classification) Dense layer, working backwards as noted above. Below is a figure showing what the SME can see and hear based on the output of the dense layers. Once again, we are using the same input data as described in the previous section. When looking at the images above, the SME can see that quite a bit of information is discarded by the dense layer. When listening to the audio playback the SME can hear a very noisy signal. The SME might be able to hear some aspects of a bird call (some specific frequencies, some recognizable frequency changes, bursts of sound that roughly mimic the bursts of sound from the original audio sample, etc.) but it is clearly not distinguishable as being a bird call, and it is quite impossible for the SME to use the audio to classify which bird is making the sound.</p><p>Not described here are additional details and possible conclusions that can be observed by performing the above using different inputs, as well as possibly selection of different hyperparameters (such as the volume and row wise max pooling parameters discussed earlier) as well as possibly entirely different AI architectures. The notebook leaves this to the user, allowing experimentation with different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Working Backwards from a Bird Classification (What Does This Bird Sound</head><p>Like?)</p><p>The reading the robot mind system also allows one more function that can prove useful to the SME who is helping the AI programmer improve the system. This function is the ability to specify a particular output (bird classification) and work backwards through the entire AI to recreate an approximation of the original audio input, even when no input is provided.</p><p>Instead of using an actual output of the final dense layer from a given input to work backwards and recreate an approximation of that input, the reading the robot mind system allows the SME to simply select a bird. This value is forced as an output of the last dense layer, by making all of the output values 0 except for the output value for that bird, which is set to 1 (similar to a 100% classification accuracy estimation of a particular bird that might be made by the automated AI system).</p><p>In the below figure, we show such an input estimation. The SME forces the output of the AI to be the first bird in our dataset whose feature extraction examples were shown earlier in  The SME can see that some of the aspects from the original bird samples are present. These include:</p><p>‚Ä¢ The central band of frequencies being loudest, similar to the same band of frequencies being the loudest in the original extracted features from this bird. ‚Ä¢ The time varying cadence of sounds, similar to the time varying sounds being present in the original samples. Nevertheless, there are a large number of distortions that show great differences to the SME from the original samples. These include:</p><p>‚Ä¢ So many other frequencies also having similar loudness to the "important" central ones, which make the vertical features extracted blurry, and the audio playback quite noisy. ‚Ä¢ A compressed version of the sound with regard to time, blurring the distinctions where different sounds start and end (horizontal blurring). ‚Ä¢ The final audio when listened to by the SME cannot be used to distinguish which bird is making the sound, and the noise and aforementioned distortions making the sound not recognizable as a bird call at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>The reading the robot mind system is implemented, allowing the SME to observe and qualitatively analyze the internal data flow of deep learning neural networks in a format familiar to them.</p><p>With this, the SME can understand where in the pipeline the most information is being discarded by the AI system, and possibly help the programmer make improvements in future systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,86.18,311.21,423.95,11.04;3,72.02,324.65,81.11,11.04;3,140.72,201.01,327.72,107.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 32 band Mel Scale Spectrogram showing similarities between audio samples from the same type of bird.</figDesc><graphic coords="3,140.72,201.01,327.72,107.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,86.18,456.07,436.65,11.04;3,72.02,469.51,100.93,11.04;3,139.97,345.86,329.10,107.50"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 32 band Mel Scale Spectrogram showing differences between audio samples from the different types of bird.</figDesc><graphic coords="3,139.97,345.86,329.10,107.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,86.18,335.93,436.70,11.04;4,72.02,349.37,102.15,11.04;4,165.85,72.00,277.50,261.73"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Audio envelope (In BLUE is the original audio, in ORANGE is the recreated audio) as well as audio playback tool.</figDesc><graphic coords="4,165.85,72.00,277.50,261.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,160.34,636.82,288.48,11.04;5,165.85,72.00,277.50,562.50"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model parameters for the simple CNN AI method used.</figDesc><graphic coords="5,165.85,72.00,277.50,562.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,86.18,284.69,405.35,11.04;6,86.20,160.54,421.40,121.90"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: History of training and validation loss and accuracy over the 16 epochs of training</figDesc><graphic coords="6,86.20,160.54,421.40,121.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,86.18,671.86,436.68,11.04;7,72.02,685.30,26.66,11.04;7,86.20,84.65,420.95,584.40"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Filter patch visualization for the first 8 filters from each of the 7 convolutional layers of the AI</figDesc><graphic coords="7,86.20,84.65,420.95,584.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,86.18,617.26,436.86,11.04;8,72.02,630.70,211.24,11.04;8,86.20,412.46,421.40,202.10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Three examples of Input reconstruction. Left is input, followed by reconstructions made from the outputs of convolutional layers 1 to 7.</figDesc><graphic coords="8,86.20,412.46,421.40,202.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,86.18,539.23,436.79,11.04;11,72.02,552.67,450.81,11.04;11,72.02,566.11,98.76,11.04;11,86.20,72.00,420.95,464.70"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Information about data content presented to SME for qualitative analysis showing a reconstructed input from the same audio segment discussed earlier, using the output of the dense layer after the flatten.</figDesc><graphic coords="11,86.20,72.00,420.95,464.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="12,445.06,248.81,78.12,11.04;12,72.02,262.25,429.94,11.04"><head>Figure 1 :</head><label>1</label><figDesc>32 bandMel Scale Spectrogram showing similarities between audio samples from the same type of bird..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,86.18,669.70,436.77,11.04;13,72.02,683.14,450.84,11.04;13,72.02,696.58,451.04,11.04;13,72.02,710.02,24.02,11.04;13,127.47,72.00,354.25,595.45"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Reconstructed Input from Forced Output (top) working backwards to prior dense layer (next row), then un-flattening and using aforementioned methods to work backwards through convolutional layers (recreated input image approximation, followed by audio envelope and playback tool).</figDesc><graphic coords="13,127.47,72.00,354.25,595.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,86.20,293.88,420.95,389.15"><head></head><label></label><figDesc></figDesc><graphic coords="9,86.20,293.88,420.95,389.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,206.45,123.48,210.70,41.41"><head></head><label></label><figDesc>ùëõ,ùë•-ùëêùëè ùëô ,ùë¶-ùëêùëè ùëô * ùëÉ ùëô,ùëõ,ùë•,ùë¶</figDesc><table coords="9,206.45,123.48,103.52,41.41"><row><cell>ùëõùë¢ùëö_ùëìùëñùëôùë° ùëô</cell><cell>ùë†ùëñùëß ùëô +ùëêùëè ùëô</cell><cell>ùë†ùëñùëß ùëô +ùëêùëè ùëô</cell></row><row><cell>ùëõ=1</cell><cell>ùë•=ùëêùëè ùëô</cell><cell>ùë¶=ùëêùëè ùëô</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to all of the SME's whose work before this have allowed all of this to be possible. Special thanks to my daughter, <rs type="person">Nicole Nussbaum</rs>, <rs type="affiliation">BS Biology</rs>, <rs type="affiliation">James Madison University</rs> and bird identification expert, who was instrumental in helping me understand what <rs type="institution">SME</rs>'s look for. Thanks also to <rs type="institution">ECPI University</rs> and to my family for allowing me the time to work on this software. Finally, thanks to all of the people who took the time to read this document, and who may find it interesting, insightful, fun, and perhaps even useful in improving future AI systems.</p></div>
<div><head>6.</head></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="14,123.98,545.61,396.93,9.94;14,109.82,558.33,181.59,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,173.97,545.61,281.94,9.94">Why Jupyter is data scientists&apos; computational notebook of choice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Perkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,467.98,545.61,28.59,9.94">Nature</title>
		<imprint>
			<biblScope unit="volume">563</biblScope>
			<biblScope unit="page" from="145" to="147" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,123.98,573.93,397.04,9.94;14,109.82,586.65,411.18,9.94;14,109.82,599.28,204.93,9.94" xml:id="b1">
	<monogr>
		<ptr target="https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks" />
		<title level="m" coord="14,158.15,573.93,362.87,9.94;14,109.82,586.65,50.08,9.94">AI vs. Machine Learning vs. Deep Learning v. Neural Networks: What&apos;s the Difference?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,123.98,614.88,396.40,9.94;14,109.82,627.60,411.04,9.94;14,109.82,640.08,243.89,10.06" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,415.37,614.88,105.01,9.94;14,109.82,627.60,265.84,9.94">Overview of BirdCLEF 2023: Automated bird species identification in Eastern Africa</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D T K H R H C F G H G H V W P R J A</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,390.94,627.60,129.92,9.94;14,109.82,640.08,211.03,10.06">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,123.98,655.92,396.80,9.94;14,109.82,668.52,45.86,9.94;14,175.04,668.52,345.95,9.94;14,109.82,681.12,23.00,9.94" xml:id="b3">
	<monogr>
		<ptr target="https://www.audubon.org/news/start-using-spectrograms-read-bird-songs-and-calls" />
		<title level="m" coord="14,206.55,655.92,236.69,9.94">Start using Spectrograms to Read Bird Songs and Calls</title>
		<imprint>
			<publisher>Audibon Society</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,123.98,696.84,396.52,9.94;14,109.82,709.44,45.86,9.94;14,175.04,709.44,345.95,9.94;14,109.82,722.16,102.05,9.94" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Acoustic</forename><surname>Nature</surname></persName>
		</author>
		<ptr target="https://www.audubon.org/news/start-using-spectrograms-read-bird-songs-and-calls" />
		<title level="m" coord="14,209.74,696.84,229.22,9.94">Best way to record birdsong: Gear guide and tips</title>
		<imprint>
			<date type="published" when="2020">2020. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,123.98,737.76,396.68,9.94;14,109.82,750.36,326.05,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,210.86,737.76,154.22,9.94">v15h BirdClef2023 Mindreader</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nussbaum</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/code/pnussbaum/v15h-birdclef2023-mindreader" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,76.13,396.35,10.04;15,109.82,88.84,410.92,9.94;15,109.82,101.44,411.12,9.94;15,109.82,114.16,381.20,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,319.95,88.84,200.79,9.94;15,109.82,101.44,331.18,9.94">Overview of LifeCLEF 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B L P S K H G B D D M J E C L T L R C M ≈† M H M S H G R P W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-P</forename><forename type="middle">V H K T D I E P B H M A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,462.94,101.44,58.00,9.94;15,109.82,114.16,348.61,9.94">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,129.76,396.60,9.94;15,109.82,142.48,411.19,9.94;15,109.82,155.08,24.84,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,265.64,129.76,254.94,9.94;15,109.82,142.48,93.99,9.94">Explainable AI for Early Detection of Health Changes Via Streaming Clustering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M K M S A M P W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,230.45,142.48,251.65,9.94">2022 IEEE International Conference on Fuzzy Systems</title>
		<meeting><address><addrLine>Padua</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,170.68,396.90,9.94;15,109.82,183.40,313.82,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,267.57,170.68,253.30,9.94;15,109.82,183.40,69.94,9.94">Explainable) Artificial Intelligence in Aerospace Safety-Critical Systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P A S A S</forename><surname>Sutthithatip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,203.69,183.40,124.28,9.94">IEEE Aerospace Conference</title>
		<meeting><address><addrLine>Big Sky. MT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,199.00,36.45,9.94;15,185.63,199.00,148.41,9.94;15,359.39,199.00,24.84,9.94;15,409.53,199.00,40.04,9.94;15,474.88,199.00,45.75,9.94;15,109.82,211.72,333.09,9.94;15,475.42,211.72,45.21,9.94;15,109.82,224.32,28.55,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,190.12,199.00,134.92,9.94">librosa.feature.melspectrogram</title>
		<author>
			<persName coords=""><surname>Librosa</surname></persName>
		</author>
		<ptr target="https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,239.92,36.45,9.94;15,197.86,239.92,99.70,9.94;15,335.04,239.92,24.84,9.94;15,397.31,239.92,40.05,9.94;15,474.79,239.92,45.86,9.94;15,109.82,252.67,363.34,9.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,202.19,239.92,86.70,9.94">librosa.feature.mfcc</title>
		<author>
			<persName coords=""><surname>Librosa</surname></persName>
		</author>
		<ptr target="https://librosa.org/doc/main/generated/librosa.feature.mfcc.html" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,123.98,268.27,396.87,9.94;15,109.82,280.99,284.70,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,196.79,268.27,94.06,9.94">Visualizing Weights</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00024.007</idno>
		<idno>10.23915/distill.00024.007</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,310.27,268.27,26.54,9.94">Distill</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
