<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,385.31,15.42;1,89.29,106.66,372.30,15.42">Optimizing Fine-Grained Fungi Classification for Diverse Application-Oriented Open-Set Metrics</title>
				<funder>
					<orgName type="full">Baden-Württemberg</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,56.86,11.96"><forename type="first">Stefan</forename><surname>Wolf</surname></persName>
							<email>stefan.wolf@iosb.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Fraunhofer IOSB</orgName>
								<orgName type="department" key="dep2">Institute of Optronics</orgName>
								<orgName type="department" key="dep3">System Technologies and Image Exploitation</orgName>
								<address>
									<addrLine>Fraunhoferstrasse 1</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Fraunhofer Center for Machine Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.36,134.97,73.27,11.96"><forename type="first">Jürgen</forename><surname>Beyerer</surname></persName>
							<email>juergen.beyerer@iosb.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision and Fusion Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology KIT</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Fraunhofer IOSB</orgName>
								<orgName type="department" key="dep2">Institute of Optronics</orgName>
								<orgName type="department" key="dep3">System Technologies and Image Exploitation</orgName>
								<address>
									<addrLine>Fraunhoferstrasse 1</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Fraunhofer Center for Machine Learning</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technologiefabrik</orgName>
								<address>
									<addrLine>Haid-und-Neu-Str. 7</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,385.31,15.42;1,89.29,106.66,372.30,15.42">Optimizing Fine-Grained Fungi Classification for Diverse Application-Oriented Open-Set Metrics</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C96512DE67D707B244476C33C9B08ADE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fungi classification</term>
					<term>Open-set classification</term>
					<term>FungiCLEF</term>
					<term>Vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained fungi species classification is an important task to support distinguishing edible and poisonous fungi and thus, reducing the risk of accidental poisoning. Therefore, the FungiCLEF 2023 challenge seeks to find the best solution for this task considering multiple metrics with each having a different application in focus like e.g., a low confusion of edible and poisonous fungi. We propose a method to approach the different metrics by exploiting modern deep learning networks, strong data augmentation and class-balanced training. The challenge assumes an open-set scenario which includes unknown classes during evaluation which we identify by a confidence thresholding approach. With our method, we achieved the 2 nd place in the challenge with good scores across all metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While fungi are a common food in many cultures, distinguishing edible fungi from poisonous ones can be difficult. Thus, an automatic system for identifying the species of fungi can support in saving lives by reducing the risk of eating poisonous fungi. However, as common to many fine-grained classification tasks, fungi species classification is a difficult task which is still not solved due to the low inter-class variance of similar looking species. We propose a method for fine-grained fungi classification that is capable of achieving a high classification accuracy while rejecting samples of classes which were not part of the training dataset. With this approach, we achieved the 2 nd place in the FungiCLEF 2023 <ref type="bibr" coords="1,290.59,533.21,12.69,10.91" target="#b0">[1]</ref> challenge, part of the LifeCLEF 2023 <ref type="bibr" coords="1,466.57,533.21,11.23,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,480.50,533.21,8.88,10.91" target="#b2">3]</ref> lab, that seeks to find the best approach for fine-grained fungi classification with an emphasis on open-set classification and reducing the chance of confusing edible with poisonous fungi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fine-grained classification of fungi species is approached by multiple studies. While Zieliński et al. <ref type="bibr" coords="2,113.44,124.83,12.89,10.91" target="#b3">[4]</ref> investigate the use of microscopic images for fine-grained fungi classification with a bag-of-words approach based on deep learning features, most works use wildlife images for the classification due to the easier applicability. While recent approaches are mostly based on deep learning architectures, multiple extensions for the task has been explored. Sulc et al. <ref type="bibr" coords="2,465.98,165.48,12.81,10.91" target="#b4">[5]</ref> apply multiple deep learning models in an esemble in order to improve the classification accuracy. Picek et al. <ref type="bibr" coords="2,137.60,192.57,12.69,10.91" target="#b5">[6]</ref> exploit additional metadata like location, habitat and substrate. Kiss and Czùni <ref type="bibr" coords="2,493.30,192.57,12.68,10.91" target="#b6">[7]</ref> evaluate multiple strategies of training deep learning models for improving the accuracy of mushroom type classification. In the context of the FungiCLEF 2022 <ref type="bibr" coords="2,403.41,219.67,13.00,10.91" target="#b7">[8]</ref> challenge, multiple methods have been explored that increase the accuracy of fine-grained fungi classification in an open-set scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method is based on the work of Wolf and Beyerer <ref type="bibr" coords="2,333.55,305.40,12.86,10.91" target="#b8">[9]</ref> and we explore the application of a newer Swin Transformer V2 <ref type="bibr" coords="2,219.14,318.95,17.96,10.91" target="#b9">[10]</ref> feature extraction backbone which shows advantageous for some of the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature extraction backbone</head><p>To extract the features, we explore a modern Swin Transformer V2 <ref type="bibr" coords="2,393.24,382.22,18.02,10.91" target="#b9">[10]</ref> backbone besides its predecessor Swin Transforer <ref type="bibr" coords="2,224.18,395.77,16.41,10.91" target="#b10">[11]</ref>. Swin Transformer V2 extends the Swin Transformer by multiple extensions to stabilize the training for larger number of parameters. I.e., these are a novel post normalization technique, a scaled cosine attention and a log-spaced continuous position bias technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification head</head><p>The feature extraction backbone provides a feature vector for each image. During training, the classification head is applied for each single image. During evaluation, the feature vectors of an observation are averaged and the classification head is applied on the averaged feature vector. As Wolf and Beyerer <ref type="bibr" coords="2,215.86,526.79,12.78,10.91" target="#b8">[9]</ref> have shown, this fusion strategy is advantageous compared to averaging the class-wise scores after the application of the classification head. The classification head itself consists of a linear layer predicting logits and a softmax activation layer resulting in normalized confidence scores. To identify samples of unknown classes, a threshold is applied on the maximum score. If the highest score is below the threshold, the sample is rejected as unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class-balancing</head><p>In order to cope with the large imbalance of the used datasets as common for fine-grained classification tasks, we apply a class-balancing strategy. I.e., we apply the data resampling scheme proposed by Gupta et al. <ref type="bibr" coords="3,239.80,86.97,18.07,10.91" target="#b11">[12]</ref> that increases the likelihood of samples of rare classes occurring in training. It is parameterized by an oversampling threshold that controls how rare a class must occur in the training set so that images of the class are resampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For training and evaluation, we use two datasets. The first is the Danish Fungi 2020 <ref type="bibr" coords="3,459.41,193.66,12.74,10.91" target="#b5">[6]</ref> dataset which includes 295,938 images from 1,604 different fungi species. All of these images are used for training. For the evaluation, we use the Danish Fungi 2021 <ref type="bibr" coords="3,348.40,220.76,12.70,10.91" target="#b7">[8]</ref> dataset with 59,420 observations and 118,675 images in total. Of these observations, 30,131 obersvations with 60,832 images are part of the official FungiCLEF 2023 <ref type="bibr" coords="3,250.28,247.86,12.99,10.91" target="#b0">[1]</ref> validation set which we use to calculate the reported metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training setup</head><p>All models are trained with MMPreTrain <ref type="bibr" coords="3,268.50,311.13,16.09,10.91" target="#b12">[13]</ref>, a classification framework based on PyTorch <ref type="bibr" coords="3,487.55,311.13,16.09,10.91" target="#b13">[14]</ref>. The weights are initialized from checkpoints pretrained on ImageNet-21k <ref type="bibr" coords="3,418.72,324.68,16.25,10.91" target="#b14">[15]</ref>. We employ an AdamW <ref type="bibr" coords="3,128.77,338.23,17.95,10.91" target="#b15">[16]</ref> optimizer with an initial learning rate of 1.25 • 10 -4 and a batch size of 128. For models which are to large to fit the VRAM, we reduce the batch size and scale the learning rate according to the linear scaling rule. Of the images, a random crop of the size of 8% to 100% of the original image is taken and resized to 384×384 pixels. Afterwards, the images are flipped horizontally with a 50% chance. In order to increase the generalization of the model, RandAugment <ref type="bibr" coords="3,157.52,405.98,18.07,10.91" target="#b16">[17]</ref> and random erasing <ref type="bibr" coords="3,272.00,405.98,18.06,10.91" target="#b17">[18]</ref> is applied. The images are normalized with the default settings of PyTorch. We apply a label smoothing loss <ref type="bibr" coords="3,366.05,419.52,18.07,10.91" target="#b18">[19]</ref> with a smoothing value of 0.1. For experiments involving class balancing, an oversample threshold of 10 -2 is applied. Additionally, the training duration is shortened from 24 epochs to 6 epochs to keep the total number of iterations consistent with oversampling. If not mentioned otherwise, class balancing is enabled for all experiments. The training is performed image-wise ignoring that images are part of observations. The models have been trained on 6 Nvidia V100, 4 Nvidia A100 or 4 Nvidia H100 depending on availability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation setup</head><p>During evaluation, each image is scaled to 438 pixels on the shorter edge and afterwards, a center crop of 384×384 pixels is taken. The normalization is applied similar to the training configuration. Each image of an observation is fed independently to the feature extraction backbone and all resulting feature vectors of an observation are averaged. Finally, the classification result is calculated by applying the linear classification layer onto the averaged feature vector followed by a softmax activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Metrics</head><p>For the experiments, we evaluate the five metrics from the challenge: The best threshold for each metric is marked with a point. The optimal threshold is highly dependent on the metric which should be optimized. While track 2 and 3 are profiting mostly from a low threshold below of 0.2, the F1 score and track 1 are optimized with a medium range threshold between 0.3 and 0.5. Track 4 can be optimized to an almost perfect value with a threshold of 1.0 meaning that no classification is taking place and all samples are classified as unknown samples.</p><p>• F1 Score: the macro-average of the class-wise F1 score.</p><p>• Track 1: Classification Error: the ratio of misclassified samples to total number of samples.</p><p>• Track 2: Cost for Poisonousness Confusion: the ratio of samples which are confused between edible and non-edible to the total number of samples whereby non-edible samples classified as edible are weighted 100×. • Track 3: User-Focused Loss: the sum of the track 1 and track 2 error. • Track 4: Classification Error with Special Cost for Unknown: similar to track 1, a ratio of misclassified samples to the total number of samples. However, samples with unknown classes classified as a known class are weighted 100× while samples of known classes which are classified as unknown are weighted 0.1×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Confidence threshold</head><p>First on, we analyze the impact of the applied confidence threshold due to the importance of properly choosing the confidence value for further experiments. The results for a Swin Base model are shown in Figure <ref type="figure" coords="4,213.44,656.03,3.81,10.91">1</ref>. Overall, they indicate that a single confidence threshold is not well suited for all metrics. The metrics can be roughly grouped in three sets. The first group are score is showing a drastically different behavior with a minimal error reached at a confidence threshold of 1.0. Thus, the minimal error corresponds to not executing any classification at all but to reject all samples and mark them as unknown. Therefore, optimizing the track 4 error is only sensible when another metric is optimized in parallel. While we report the best score for each metric but track 4 in the following experiments, for the track 4 score, we report the result for the confidence threshold with the highest F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Backbone architectures</head><p>Based on the results of Wolf and Beyerer <ref type="bibr" coords="5,274.25,403.12,11.46,10.91" target="#b8">[9]</ref>, we start with evaluating Swin Transformer <ref type="bibr" coords="5,488.03,403.12,17.95,10.91" target="#b10">[11]</ref> models. Additionally, we evaluate a Swin Transformer V2 <ref type="bibr" coords="5,344.86,416.67,17.79,10.91" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Window size for SwinV2</head><p>Since one extension of Swin Transformer V2 is an increased window size, we evaluate this parameter separately. While Swin Transformer employs a window size of 7x7 for all stages, Swin Transformer V2 halves the window size in the last stage. We report the window size for the first three stages. Swin Transformer V2 is commonly pretrained on ImageNet-21k with a window size of 12x12 and an image resolution of 192x192. We employ the same pretraining but we increase the image resolution to 384x384 for fine-tuning on the Danish Fungi dataset in order to pick up the fine details of the fungi important for distinguishing the species. For the first experiments, we keep the window size at 12x12 and afterwards, we evaluate increasing it to 24x24. The results are shown in Table <ref type="table" coords="6,272.73,456.61,3.76,10.91" target="#tab_2">2</ref>. Increasing the window size improves the F1 score and the track 4 error by a significant margin. Nonetheless, the track 1 and 3 errors show no significant change and the error of track 2 shows a slight increase. So, overall the choice of window size depends on the metric on which the user is focused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Class-balancing</head><p>To cope with the high class imbalance of the Danish Fungi dataset, we employ a simple class balancing scheme based on resampling. We evaluate the impact of the class balancing with a Swin Transformer Base model. We show the results in Table <ref type="table" coords="6,373.52,574.08,3.81,10.91" target="#tab_3">3</ref>. The results are mixed with class balancing providing an advantage for the F1 score and the track 4 error while having no significant impact on the error for track 1 and deteriorating the performance on track 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Margin for center crop</head><p>For image classification, a center crop is usually applied to the input image before classification. For coarse-grained image classification, this provides an advantage due to the important parts of the image mostly being centered while the background may contain distracting cues that might still be recognized due to the wide range of classes. However, for fine-grained fungi classification, the relevant fungus might not be centered if it spans a large area and distracting objects in the background are rather unlikely due to the focus of fungi during training. Thus, we evaluate removing the margin of the center crop. To keep the aspect ratio of the image intact, we still apply a center crop but only for the longer side of the image. The results are shown in Table <ref type="table" coords="7,115.02,397.65,3.66,10.91" target="#tab_4">4</ref>. While the results for both evaluations are quite close, they show a slight decrease for all metrics but the track 1 error which shows no significant change. So, performing a center crop with margin is also advantageous for fine-grained fungi classification. Likely, a large part of the images are showing mushrooms which are not spanning the whole image and thus, performing a center crop highlights the important part of the image containing a mushroom and provides a higher resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Results on test set</head><p>A set of models were selected as final submissions and evaluated on the competitions' private test set. The models were chosen to provide a wide variety of settings instead of only submitting the models performing best on the validation set. Particularly, a wide variety of confidence thresholds is chosen. All models were trained with class balancing. The results are shown in Table <ref type="table" coords="7,116.54,569.32,3.81,10.91" target="#tab_5">5</ref>. The best model and configuration heavily depends on the observed metric. For the F1 score and the track 2 and 3 errors, a SwinV2 Base model with a low confidence threshold performs best. For the track 1 and 4 errors, a Swin Large model with higher confidence thresholds performs better. However, evaluating a SwinV2 Base model with higher confidence thresholds might lead to even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel method for classification of fine-grained fungi species in an open-set scenario. The methods outperform previous methods by exploiting Swin Transformer V2 as modern feature extractor backbone at a similar model size. Our investigations show the difficulty of optimizing multiple metrics in parallel with the optimal design decision often being dependent on the considered metric. This insight emphasizes the importance of deep investigations considering multiple metrics and scenarios and the significance of proper requirements engineering for shifting from research to production. Future work might include the exploitation of metadata or more sophisticated fusion schemes of the images of an observation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,346.88,416.69,8.93;4,89.29,358.88,416.70,8.87;4,89.29,370.84,416.70,8.87;4,89.29,382.79,416.69,8.87;4,89.29,394.75,416.70,8.87;4,88.93,406.70,417.05,8.87;4,89.29,418.66,76.21,8.87"><head>2 :Figure 1 :</head><label>21</label><figDesc>Figure 1:The relevant metrics depending on the confidence threshold for the open-set recognition for a Swin Transformer Base model. The best threshold for each metric is marked with a point. The optimal threshold is highly dependent on the metric which should be optimized. While track 2 and 3 are profiting mostly from a low threshold below of 0.2, the F1 score and track 1 are optimized with a medium range threshold between 0.3 and 0.5. Track 4 can be optimized to an almost perfect value with a threshold of 1.0 meaning that no classification is taking place and all samples are classified as unknown samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,417.24,192.56"><head>Table 1</head><label>1</label><figDesc>Comparison of backbone architectures. We evaluate the Base and Large variants of Swin Transformer and Swin Transformer V2. While the SwinV2 Base model shows a slight advantage over the Swin Base model, the Swin Large model shows the best performance overall. Interestingly, it shows the highest error for the Track 4 metric even though it is the best model for all other metrics. In contrast to our expectation, the SwinV2 Large model is not outperforming the other models for any metric.</figDesc><table coords="5,89.29,169.92,416.94,113.13"><row><cell>Backbone</cell><cell cols="5">F1 Score Track 1 Track 2 Track 3 Track 4</cell></row><row><cell>Swin Base</cell><cell>55.80</cell><cell>0.344</cell><cell>0.180</cell><cell>0.544</cell><cell>2.705</cell></row><row><cell>Swin Large</cell><cell>56.05</cell><cell>0.339</cell><cell>0.153</cell><cell>0.518</cell><cell>2.842</cell></row><row><cell>SwinV2 Base W24</cell><cell>55.92</cell><cell>0.343</cell><cell>0.160</cell><cell>0.527</cell><cell>2.701</cell></row><row><cell cols="2">SwinV2 Large W24 55.23</cell><cell>0.343</cell><cell>0.170</cell><cell>0.534</cell><cell>2.771</cell></row><row><cell cols="6">track 2 and track 3 which are showing an optimal score for thresholds slightly below 0.2. The</cell></row><row><cell cols="6">F1 score and the track 1 score are optimized for a threshold between 0.3 and 0.5. The track 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.96,416.67,417.31,132.85"><head></head><label></label><figDesc>models. The results as shown in Table1indicate that the Swin Transformer V2 Base model outperforms the Swin Transformer counterpart by a slight margin. However, it can not outweigh the advantage of the higher capacity of the Swin Transformer Large model which performs best overall. Just for the track 4 error, the Swin Transformer Large model shows the worst results in this comparison. In contrast to our expectation, the SwinV2 Large model does not perform best. In fact, it does not outperform the other models for any metric. The reasons for the lack of performance needs to be figured out in future research. More generalization like data augmentation or stochastic depth might be required. Other possible reasons might be inappropriate hyper parameter selection for generalization techniques like the label smoothing value.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.93,90.49,417.23,93.78"><head>Table 2</head><label>2</label><figDesc>Comparison of the window size for SwinV2 Base. A larger window size shows a significant increase in terms of F1 score and a reduction of the track 4 error. However, all other metrics are either equal or worse with a larger window size.</figDesc><table coords="6,163.91,146.01,267.46,38.25"><row><cell cols="6">Window Size F1 Score Track 1 Track 2 Track 3 Track 4</cell></row><row><cell>12</cell><cell>54.72</cell><cell>0.345</cell><cell>0.154</cell><cell>0.529</cell><cell>2.774</cell></row><row><cell>24</cell><cell>55.92</cell><cell>0.343</cell><cell>0.160</cell><cell>0.527</cell><cell>2.701</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,206.53,417.22,81.83"><head>Table 3</head><label>3</label><figDesc>Evaluating the class balancing strategy. While the class balancing improves the F1 score and the track 4 error, the advantage for other metrics is unclear.</figDesc><table coords="6,158.65,250.10,277.98,38.25"><row><cell cols="6">Class balancing F1 Score Track 1 Track 2 Track 3 Track 4</cell></row><row><cell>No</cell><cell>55.59</cell><cell>0.344</cell><cell>0.157</cell><cell>0.527</cell><cell>2.787</cell></row><row><cell>Yes</cell><cell>55.80</cell><cell>0.344</cell><cell>0.180</cell><cell>0.544</cell><cell>2.705</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,88.99,310.61,417.24,105.74"><head>Table 4</head><label>4</label><figDesc>Evaluating the impact of removing the margin of the center crop in the image pre-processing. A center crop is still applied to provide a square image. The results show that no advantage can be gained by removing the commonly applied margin for the center crop even though it might be reasonable for large fungi spanning the whole image.</figDesc><table coords="6,175.67,378.09,243.94,38.25"><row><cell cols="6">Margin F1 Score Track 1 Track 2 Track 3 Track 4</cell></row><row><cell>Yes</cell><cell>55.92</cell><cell>0.343</cell><cell>0.160</cell><cell>0.527</cell><cell>2.701</cell></row><row><cell>No</cell><cell>55.56</cell><cell>0.342</cell><cell>0.170</cell><cell>0.535</cell><cell>2.712</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.99,90.49,417.00,199.17"><head>Table 5</head><label>5</label><figDesc>Evaluating multiple models and configurations on the test set. In general, the results indicate an advantage of the Swin Large model over the Swin Base model and Swin V2 over Swin. The impact of the margin of the cencter crop and the confidence threshold depends on the observed metric. Similar to the validation set, a high confidence threshold drastically improves the track 4 error while hurting all other metrics.</figDesc><table coords="7,101.78,167.70,391.71,121.95"><row><cell>Backbone</cell><cell cols="4">Threshold Epochs Margin F1</cell><cell cols="4">Track 1 Track 2 Track 3 Track 4</cell></row><row><cell>Swin Base</cell><cell>0.0</cell><cell>6</cell><cell>Yes</cell><cell cols="2">52.53 0.417</cell><cell>0.194</cell><cell>0.611</cell><cell>3.626</cell></row><row><cell>Swin Base</cell><cell>0.2</cell><cell>6</cell><cell>Yes</cell><cell cols="2">54.91 0.369</cell><cell>0.202</cell><cell>0.571</cell><cell>3.095</cell></row><row><cell>Swin Base</cell><cell>0.3</cell><cell>6</cell><cell>Yes</cell><cell cols="2">54.74 0.360</cell><cell>0.230</cell><cell>0.590</cell><cell>2.920</cell></row><row><cell>Swin Large</cell><cell>0.2</cell><cell>6</cell><cell>No</cell><cell cols="2">55.21 0.365</cell><cell>0.191</cell><cell>0.556</cell><cell>3.064</cell></row><row><cell>Swin Large</cell><cell>0.2</cell><cell>6</cell><cell>Yes</cell><cell cols="2">55.27 0.367</cell><cell>0.197</cell><cell>0.564</cell><cell>3.071</cell></row><row><cell>Swin Large</cell><cell>0.5</cell><cell>6</cell><cell>No</cell><cell cols="2">54.67 0.347</cell><cell>0.316</cell><cell>0.663</cell><cell>2.611</cell></row><row><cell>Swin Large</cell><cell>0.85</cell><cell>6</cell><cell>No</cell><cell cols="2">44.92 0.384</cell><cell>0.822</cell><cell>1.206</cell><cell>1.905</cell></row><row><cell cols="2">SwinV2 Base 0.08</cell><cell>9</cell><cell>Yes</cell><cell cols="2">54.97 0.367</cell><cell>0.191</cell><cell>0.558</cell><cell>3.077</cell></row><row><cell cols="2">SwinV2 Base 0.1</cell><cell>6</cell><cell>Yes</cell><cell cols="2">55.31 0.366</cell><cell>0.190</cell><cell>0.556</cell><cell>3.047</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors acknowledge support by the state of <rs type="funder">Baden-Württemberg</rs> through bwHPC.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://github.com/wolfstefan/fungi2023.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,323.38,393.33,10.91;8,112.66,336.93,373.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,302.76,323.38,203.22,10.91;8,112.66,336.93,68.09,10.91">Overview of fungiclef 2023: Fungi recognition beyond 1/0 cost</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,203.36,336.93,252.47,10.91">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,350.47,394.53,10.91;8,112.66,364.02,395.01,10.91;8,112.30,377.57,393.68,10.91;8,112.66,391.12,394.53,10.91;8,112.66,404.67,393.33,10.91;8,112.66,418.22,289.43,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,383.16,377.57,122.82,10.91;8,112.66,391.12,171.35,10.91">Lifeclef 2023 teaser: Species identification and prediction challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Moussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.71,404.67,113.28,10.91;8,112.66,418.22,38.01,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,431.77,394.53,10.91;8,112.66,445.32,394.53,10.91;8,112.66,458.87,393.33,10.91;8,112.66,472.42,395.17,10.91;8,112.66,485.97,394.52,10.91;8,112.33,499.52,329.01,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,112.66,458.87,393.33,10.91;8,112.66,472.42,135.53,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,271.76,472.42,236.08,10.91;8,112.66,485.97,389.92,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">September 18-23, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,513.06,393.33,10.91;8,112.66,526.61,393.98,10.91;8,112.66,540.16,42.79,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,483.13,513.06,22.86,10.91;8,112.66,526.61,303.43,10.91">Deep learning approach to describe and classify fungi microscopic images</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zieliński</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sroka-Oleksiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rymarczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piekarczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brzychczy-Włoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,424.87,526.61,39.04,10.91">PloS one</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">234806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,553.71,393.33,10.91;8,112.66,567.26,393.53,10.91;8,112.30,580.81,124.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,375.95,553.71,130.03,10.91;8,112.66,567.26,33.31,10.91">Fungi recognition: A practical use case</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,167.21,567.26,338.98,10.91;8,112.30,580.81,26.65,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,594.36,394.53,10.91;8,112.66,607.91,393.33,10.91;8,112.66,621.46,392.48,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,112.66,607.91,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,420.58,607.91,85.40,10.91;8,112.66,621.46,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,635.01,393.33,10.91;8,112.66,648.56,393.33,10.91;8,112.66,662.11,200.36,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,196.45,635.01,309.54,10.91;8,112.66,648.56,78.68,10.91">Mushroom image classification with cnns: A case-study of different learning strategies</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Czùni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,212.74,648.56,293.25,10.91;8,112.66,662.11,86.15,10.91">2021 12th International Symposium on Image and Signal Processing and Analysis (ISPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="165" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,395.17,10.91;9,112.66,114.06,232.29,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,339.05,86.97,110.41,10.91;9,480.07,86.97,25.92,10.91;9,112.66,100.52,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,365.49,100.52,142.34,10.91;9,112.66,114.06,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="9,112.66,127.61,393.33,10.91;9,112.66,141.16,181.25,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,203.02,127.61,302.96,10.91;9,112.66,141.16,35.11,10.91">Transformer-based fine-grained fungi classification in an open-set scenario</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.09,141.16,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,154.71,394.53,10.91;9,112.66,168.26,393.32,10.91;9,112.66,181.81,395.01,10.91;9,112.41,195.36,59.11,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,149.42,168.26,250.25,10.91">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,422.67,168.26,83.31,10.91;9,112.66,181.81,346.11,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,393.33,10.91;9,112.39,222.46,393.60,10.91;9,112.66,236.01,250.30,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,370.89,208.91,135.10,10.91;9,112.39,222.46,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.36,222.46,188.63,10.91;9,112.66,236.01,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,249.56,394.53,10.91;9,112.66,263.11,394.53,10.91;9,112.66,276.66,90.72,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,251.23,249.56,251.23,10.91">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.24,263.11,375.49,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,290.20,394.04,10.91;9,112.66,303.75,141.14,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmpretrain" />
		<title level="m" coord="9,189.39,290.20,223.65,10.91">Openmmlab&apos;s pre-training toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,317.30,394.53,10.91;9,112.66,330.85,393.33,10.91;9,112.66,344.40,350.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,261.26,330.85,244.72,10.91;9,112.66,344.40,67.64,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,188.20,344.40,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,357.95,393.33,10.91;9,112.66,371.50,394.53,10.91;9,112.66,385.05,103.61,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,346.64,357.95,159.35,10.91;9,112.66,371.50,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.08,371.50,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,398.60,393.33,10.91;9,112.66,412.15,107.17,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m" coord="9,238.15,398.60,182.94,10.91">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,425.70,395.17,10.91;9,112.66,439.25,393.32,10.91;9,112.66,452.79,325.92,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,302.02,425.70,205.81,10.91;9,112.66,439.25,171.71,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,307.43,439.25,198.55,10.91;9,112.66,452.79,237.36,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,466.34,394.62,10.91;9,112.66,479.89,394.53,10.91;9,112.41,493.44,27.76,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,322.30,466.34,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,479.89,265.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,506.99,393.33,10.91;9,112.66,520.54,393.33,10.91;9,112.66,534.09,147.08,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,344.36,506.99,161.63,10.91;9,112.66,520.54,83.10,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.60,520.54,289.39,10.91;9,112.66,534.09,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
