<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,398.86,15.42;1,89.29,106.66,303.83,15.42">Deep Learning for Large-Scale Plant Classification: NEUON Submission to PlantCLEF 2023</title>
				<funder>
					<orgName type="full">NEUON AI SDN. BHD.</orgName>
				</funder>
				<funder>
					<orgName type="full">Malaysia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,66.76,11.96"><forename type="first">Sophia</forename><surname>Chulif</surname></persName>
							<email>schulif@swinburne.edu.my</email>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology Sarawak Campus</orgName>
								<address>
									<postCode>93350</postCode>
									<region>Sarawak</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.97,134.97,93.04,11.96"><forename type="first">Yang</forename><forename type="middle">Loong</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarawak</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.00,134.97,61.22,11.96"><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<email>shlee@swinburne.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology Sarawak Campus</orgName>
								<address>
									<postCode>93350</postCode>
									<region>Sarawak</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,398.86,15.42;1,89.29,106.66,303.83,15.42">Deep Learning for Large-Scale Plant Classification: NEUON Submission to PlantCLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">06944A609FB6CE2FE310266EC017B33D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>multi-organ</term>
					<term>convolutional neural networks</term>
					<term>machine learning</term>
					<term>computer vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the methods and submissions made by our team to PlantCLEF 2023, an imagebased plant identification task involving 80,000 plant species. Like PlantCLEF 2022, the task aimed to identify species based on 26,868 multi-image plant observations from a test set of 55,306 images. Given a training dataset of 4 million photos, we trained several Inception-v4 and Inception-ResNet-v2 models and submitted ten runs. Compared to the previous year, we experimented with more data augmentations, different batching methods and trained separate organ models, specifically for the labels: bark, flower, fruit, habit, and leaf. Our highest-performing run, comprising several ensembled models, achieved a macro-averaged mean reciprocal rank of 0.61813, increasing from 0.6078 in last year's performance (PlantCLEF 2022) through increased data augmentations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate identification of plants on a global scale helps us understand and conserve Earth's biodiversity. It allows naturalists to study species diversity <ref type="bibr" coords="1,357.77,416.59,11.58,10.91" target="#b0">[1]</ref>, assess the impact of environmental changes, and predict ecological responses to climate change <ref type="bibr" coords="1,390.31,430.14,11.40,10.91" target="#b1">[2]</ref>. Besides this, it plays a vital role in agriculture <ref type="bibr" coords="1,193.35,443.69,11.33,10.91" target="#b2">[3]</ref>, food security <ref type="bibr" coords="1,271.72,443.69,11.33,10.91" target="#b3">[4]</ref>, and medicinal research <ref type="bibr" coords="1,393.81,443.69,11.33,10.91" target="#b4">[5]</ref>. In many ways, plants are beneficial to all life-form. Nevertheless, it remains a challenge to identify all plant species on Earth <ref type="bibr" coords="1,130.83,470.79,11.48,10.91" target="#b5">[6]</ref>. Global plant-scale identification involves processing massive variations of plant features, taxonomic information, and ecological attributes. Capturing subtle differences among these species may require more effort for machine algorithms and even for human experts. To tackle this problem, the PlantCLEF 2023 challenge <ref type="bibr" coords="1,330.49,511.43,11.40,10.91" target="#b6">[7,</ref><ref type="bibr" coords="1,344.62,511.43,8.99,10.91" target="#b7">8]</ref> was introduced to classify a large multi-image dataset with 80,000 plant species. Likewise, the same training and test sets from PlantCLEF 2022 <ref type="bibr" coords="1,163.20,538.53,12.93,10.91" target="#b8">[9]</ref> were provided. These datasets remain the largest plant dataset published to date and represent a realistic global plant identification task concerning a vast number of species, strongly unbalanced, partially incorrect identifications, duplications, and diversified images.</p><p>This paper describes our training strategies, submissions, and results obtained in PlantCLEF 2023. In short, our training strategies include increasing data augmentation, applying bal-anced batching, and employing multi-organ and single-organ training schemes. Firstly, data augmentation is a common technique to increase the training dataset by performing various transformations on the images to tackle the class-imbalance problem and to avoid overfitting <ref type="bibr" coords="2,89.29,127.61,16.09,10.91" target="#b9">[10]</ref>. In addition, it has been shown that simple augmentation methods can contribute considerably to the results, which are very good relative to more complex methods in some cases <ref type="bibr" coords="2,487.32,141.16,16.27,10.91" target="#b10">[11]</ref>. Therefore, we experimented with increased augmentation during training by applying more bi-cubic resizing, random hue, and random contrast to our dataset. Next, we employed classbalancing in our batching method to deal with the long-tail characteristics of the PlantCLEF 2023 dataset, in which many classes have significantly lower training samples and fewer classes have more training samples. Class re-balancing is easy to implement in principle among the various long-tailed learning, but it can lead to comparable or superior results <ref type="bibr" coords="2,402.24,222.46,16.35,10.91" target="#b11">[12]</ref>. Consequently, we limit the training images for a species to a maximum of 16 in an epoch in hopes of reducing any bias towards the majority classes and preventing poor performance on the underrepresented species. Lastly, multi-organ integrated training <ref type="bibr" coords="2,308.61,263.11,18.07,10.91" target="#b12">[13]</ref> has been shown to improve the model performance compared to single-organ plant classification. Therefore, we utilised the available manual and predicted organ tags to experiment with the multi-organ and single-organ training schemes to evaluate how they would perform in the context of PlantCLEF 2023.</p><p>From the above approaches, we find that the data augmentation method enhances our model the best and has helped us achieve a macro-averaged mean reciprocal rank (MA-MRR) of 0.61813, increasing from 0.6078 in last year's performance (PlantCLEF 2022), producing the second best results in the PlantCLEF 2023 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets</head><p>Two datasets were provided in PlantCLEF 2023, the training dataset and the test dataset. The training dataset was grouped into two: Trusted and Web. The Trusted dataset comprised 2,886,761 higher-quality images from academic sources and collaborative platforms. On the other hand, the Web dataset included 1,071,627 images from search engine queries suffering significant inaccuracies. The Trusted and Web datasets contain metadata such as Class, Order, Family, Genus, Species, manual tag, predicted tag, and predicted tag probability. Nevertheless, not all images were annotated with the manual and predicted tags. Meanwhile, the test dataset consists of 55,306 images categorised into 26,868 plant observations used for the challenge's observationlevel plant classification. Like the Trusted and Web datasets, the test dataset contained the manual tag metadata. In addition, we partitioned separate datasets from the training dataset provided to cater to our multi-organ and single-organ training schemes. Since not all images were annotated with tags, some images were not included in the single-organ training. We used the images annotated with the tags of "bark", "flower", "fruit", "habit", and "leaf". Furthermore, we excluded the tags with less than 0.7 in their predicted probability score. Table <ref type="table" coords="2,453.13,592.72,5.09,10.91" target="#tab_0">1</ref> shows the training datasets used for our multi-organ and single-organ training approaches. Note that we pre-processed the training datasets and removed all additional duplicate images sharing the same filename, resulting in a reduced training dataset size compared to the original training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Setup</head><p>Our models were set up using Tensorflow 1.12 and TF-Slim library with the hyperparameters described in Table <ref type="table" coords="3,173.06,457.58,3.75,10.91" target="#tab_1">2</ref>. Like our submissions to PlantCLEF 2022 <ref type="bibr" coords="3,366.18,457.58,16.28,10.91" target="#b13">[14]</ref>, our models were based on the Inception-v4 and Inception-ResNet-v2 architectures initialised on the weights pre-trained from ImageNet. We added a batch normalisation layer and five fully-connected layers after the final layers to cater to the taxonomy classifications: Class, Order, Family, Genus, and Species. Since multi-task classification has been shown to improve classification compared to a single label classification in our previous PlantCLEF submissions <ref type="bibr" coords="3,358.73,525.32,16.55,10.91" target="#b14">[15,</ref><ref type="bibr" coords="3,378.39,525.32,12.42,10.91" target="#b13">14]</ref>, likewise, we implement multi-task classification in our models catering to the five labels: Class, Order, Family, Genus, and Species. In contrast to our PlantCLEF 2022 submissions, we experimented with several new strategies: increased data augmentation, a balanced batching method, and a multi-organ and single-organ training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increased Data Augmentation</head><p>Our models were trained with random cropping, horizontal flipping, and colour distortion applied to our images. In contrast to our models from last year, we increased the data augmentations in our training by performing more bi-cubic resizing, random hue and random contrast to our images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Balanced Batching Method</head><p>In addition, we adopted a balanced batching method in training our models. We limit the selection of training images for a species in an epoch to a maximum of 16 samples. We chose 16 as it would be suitable since the lowest sample number in the training dataset is 1, while the highest sample number in the dataset is over 100. It aims to avoid bias towards any particular species and prevents poor performance on underrepresented species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-organ and Single-organ Training Scheme</head><p>Furthermore, we implemented two training schemes: multi-organ and single-organ. In multiorgan training, we trained our model with the entire dataset (like our submissions last year). Using all the available organ parts, the classification is trained on the species. On the other hand, in single-organ, we trained our model based on the individual organ tags provided as mentioned in Section 2, which includes Bark, Flower, Fruit, Habit, and Leaf. The classification of a single-organ model is based on the specific organ and their respective species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference Methods</head><p>The model predictions are obtained by the Softmax function as well as the feature embedding comparison method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax function</head><p>The softmax function is commonly used as the activation function in the output layer of a neural network for multi-class classification. It allows the network to produce class probabilities over multiple classes. It accepts a vector of input data and assigns a probability value to each class. The probability generated is a vector of numbers between 0 and 1 and these values would sum up to 1. The class with the highest probability indicates the most confident predicted class. The Softmax function is represented in Equation 1 where x represents the input vector, and ùëí ùë• denotes the exponential function raised to the power of x. The denominator, ‚àëÔ∏Ä ùëñ ùëí ùë• ùëñ , calculates the sum of the exponentiated values over all elements of the input vector.</p><formula xml:id="formula_0" coords="4,250.52,485.91,256.12,29.07">softmax(ùë•) = ùëí ùë• ‚àëÔ∏Ä ùëñ ùëí ùë• ùëñ<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature embedding comparison</head><p>This method refers to comparing the test image's features to the train images' features. It involves transforming the train and test images into lower-dimensional feature vectors (embeddings) using the learned representation obtained from the fully-connected layers of our model, then comparing them to measure their similarity. Essentially, this method include two processes: the feature dictionary generation and feature similarity comparison.</p><p>1. Feature dictionary generation First, we generated a list of train images (feature dictionary) to compare with the test images. Since the training data consists of over 3 million images and it is computationally intensive to compare all the photos, we only selected a maximum of ten images for each of the 80,000 plant species from the Trusted training dataset. Consequently, our feature dictionary is made from a list of 592,258 training images. These images are then transformed into feature embeddings and averaged according to their respective classes. Therefore, the feature dictionary is composed of 80,000 feature embeddings. Note that to obtain the feature embedding of each training image, we applied ten crops to the image and then averaged them to get a single feature embedding. The ten crops involve cropping the top-left, top-right, bottom-left, bottom-right, and centre, all of which are also horizontally flipped to get ten variations. The total feature embeddings of each species are then averaged over the total number of images it has in the list of the dictionary, making a feature dictionary of 80,000 feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feature similarity comparison</head><p>After acquiring the train feature dictionary of 80,000 feature embeddings, we obtain the test image embeddings by performing the same ten crops technique to the test images.</p><p>Since the test images' classes are unknown, the embeddings are averaged according to the test observation id instead of the species class id as in the train feature dictionary generation. Therefore, since there are 26,868 observation ids, the total test embeddings generated result in 26,868 test feature embeddings. Next, we used cosine similarity to measure the similarity between the train feature dictionary and the test feature embeddings. The calculated cosine similarity score is then transformed with Inverse Distance Weighting into probabilities for ranking the classes.</p><p>The weights for the transformed embedding vector were calculated as in Equation <ref type="formula" coords="5,500.81,359.31,5.17,10.91" target="#formula_1">2</ref>where ùëÉ ùëñ is the weight assigned to a specific test feature embedding, ùëë ùëñ is the distance between the test feature embedding and the dictionary feature embedding, ùëë ùëõ ùëò is the distance between the test feature embedding and the dictionary feature embedding ùëò, raised to the power of ùëõ, while ‚àëÔ∏Ä ùëò is the sum over all dictionary feature embeddings. Similar to the Softmax function method, the class with the highest probability indicates the most confident predicted class.</p><formula xml:id="formula_1" coords="5,265.49,472.78,241.14,34.96">ùëÉ ùëñ = ( 1 ùëë ùëñ ) ‚àëÔ∏Ä ùëò ( 1 ùëë ùëõ ùëò )<label>(2)</label></formula><p>4. Submissions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Variations</head><p>We submitted ten runs to PlantCLEF 2023, consisting of several models described in Table <ref type="table" coords="5,500.04,571.01,3.81,10.91" target="#tab_2">3</ref>. Essentially, they differ in their network architecture, data augmentation, batching method, and training data. The evaluation metric implemented in this challenge is the Macro Average (by species) Mean Reciprocal Rank (MA-MRR). The MA-MRR is a statistic measure for evaluating any process that generates a list of possible responses to an index of queries ordered according to the likelihood of correctness as defined in <ref type="bibr" coords="5,289.36,638.75,11.43,10.91" target="#b6">[7]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Discussion</head><p>Our runs are tabulated in Table <ref type="table" coords="6,231.39,597.90,3.74,10.91" target="#tab_3">4</ref>. Run 1, 2, 3, 5, 6, and 8 were based on the predictions from a single model, while Run 4, 7, 9, and 10 were based on several models ensembled. As expected, the ensembled models are more accurate than a single model prediction. Our highest-performing run (Run 9), an ensembled model, obtained an MA-MRR score of 0.61813. From the new strategies experimented with, we found that increasing data augmentation helped improve the models' accuracy. It is confirmed by comparing our run from PlantCLEF 2022 (Run 7 MA-MRR: 0.6078) and our current run (Run 9 MA-MRR: 0.61813), which differ by only adding the new model (Multi-IR-AUG), which is trained with increased augmentations. On the other hand, the balanced batching method did not help in improving the model's performance. Comparing Run 1 and 2, we see that the model without the balanced batching strategy (Run 1 MA-MRR: 0.54242) achieved a higher MA-MRR score compared to the model with the balanced batching method (Run 2 MA-MRR: 0.46606). Although it was not what we expected, it could be due to the reduction of training samples which may be inherently more important than others. Furthermore, the single-organ training scheme did not help in the predictions. Run 4, which comprised the single-organ models (bark, flower, fruit, habit, and leaf), performed the worst among all other runs based on the multi-organ training scheme. It suggests that training with multi-organ data is more effective than single-organ data as a combination of organs can provide more information to the model. Finally, we show that the predictions obtained by the feature embedding matching method (Run 3, 6, and 8 MA-MRR: 0.45242, 0.46476, 0.4591) are lower compared to the predictions obtained by the Softmax function method (Run 1 and 5 MA-MRR: 0.54242, 0.5504). It is likely due to the clearer decision boundary obtained when using the Softmax function, as this approach relies on the confidence of the network's prediction. On the other hand, feature embedding comparison might involve more nuanced comparisons between feature vectors, which can lead to a loss of discriminative information. This loss could result in reduced accuracy and lower performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We trained several Inception-v4 and Inception-ResNet-v2 models and submitted ten runs to PlantCLEF 2023. To improve our performance in this year's challenge, we tried several new strategies that differed from our previous submissions in PlantCLEF 2022. These strategies included three key approaches: increased data augmentation, a balanced batching method, and a multi-organ and single-organ training scheme. Among these approaches, increased data augmentation improved our model predictions by increasing our MA-MRR score from 0.6078 to 0.61813. The balanced batching method did not work out as intended but is possibly the result of the degradation of the head classes with many training samples at the expense of improving the tail classes with fewer training samples from under-sampling <ref type="bibr" coords="7,389.29,497.87,16.41,10.91" target="#b11">[12]</ref>. Moreover, the lower performance of the ensembled single-organ training models is likely due to the reduction of training data in the single-organ training scheme. Since not all training images were tagged with their organ details and we did not utilise the images with a predicted tag score of less than 0.7, there was a significant loss of information compared to using the entire dataset in multi-organ training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,98.17,372.90,4.63,8.87;6,130.96,372.90,227.26,8.87;6,455.44,372.90,29.99,8.87;6,98.17,390.34,4.63,8.87;6,130.96,390.34,227.26,8.87;6,455.44,390.34,29.99,8.87;6,98.17,407.77,9.27,8.87;6,130.96,407.77,188.11,8.87;6,455.44,407.77,29.99,8.87;6,98.17,425.20,4.63,8.87;6,130.96,425.20,58.14,8.87;6,455.44,425.20,25.35,8.87;6,98.17,442.63,4.63,8.87;6,130.96,442.63,58.14,8.87;6,455.44,442.63,29.99,8.87;6,98.17,460.07,4.63,8.87;6,130.96,460.07,67.60,8.87;6,455.44,460.07,29.99,8.87;6,98.17,477.50,4.63,8.87;6,130.96,477.50,187.27,8.87;6,455.44,477.50,29.99,8.87;6,98.17,494.93,4.63,8.87;6,130.96,494.93,187.27,8.87;6,455.44,494.93,25.35,8.87;6,98.17,512.36,4.63,8.87;6,130.96,512.36,187.27,8.87;6,455.44,512.36,29.99,8.87;6,98.17,529.80,4.63,8.87;6,130.96,529.80,313.84,8.87;6,130.96,541.75,197.31,8.87;6,455.44,529.80,29.99,8.87"><head>9</head><label></label><figDesc>Multi-I + Multi-IR + Multi-IR (Trusted) + Multi-IR-AUG 0.61813 7 Multi-I + Multi-IR + Multi-IR (Trusted) + Multi-IR-AUG 0-AUG-B-Bark + Single-IR-AUG-B-Flower + Single-IR-AUG-B-Fruit + Single-IR-AUG-B-Habit + Single-IR-AUG-B-Leaf 0.33926</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,355.15,129.65"><head>Table 1</head><label>1</label><figDesc>Details of our training datasets partitioned for training.</figDesc><table coords="3,151.13,122.10,293.02,98.03"><row><cell>Dataset</cell><cell>Source</cell><cell cols="2">No. of images No. of species</cell></row><row><cell>Multi-organ</cell><cell>Trusted</cell><cell>2,821,933</cell><cell>80,000</cell></row><row><cell>Multi-organ</cell><cell cols="2">Trusted &amp; Web 3,893,560</cell><cell>80,000</cell></row><row><cell>Bark Single-organ</cell><cell cols="2">Trusted &amp; Web 54,937</cell><cell>19,430</cell></row><row><cell cols="3">Flower Single-organ Trusted &amp; Web 358,969</cell><cell>45,346</cell></row><row><cell>Fruit Single-organ</cell><cell cols="2">Trusted &amp; Web 88,639</cell><cell>26,824</cell></row><row><cell>Habit Single-organ</cell><cell cols="2">Trusted &amp; Web 2,088,292</cell><cell>76,511</cell></row><row><cell>Leaf Single-organ</cell><cell cols="2">Trusted &amp; Web 89,001</cell><cell>21,771</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,242.39,307.88,129.65"><head>Table 2</head><label>2</label><figDesc>Details of our models' hyperparameters.</figDesc><table coords="3,198.41,274.01,198.46,98.03"><row><cell>Hyperparameter</cell><cell>Values</cell></row><row><cell>Batch Size</cell><cell>128</cell></row><row><cell>Input Image Size</cell><cell>299 √ó 299 √ó 3</cell></row><row><cell>Optimizer</cell><cell>Adam Optimizer</cell></row><row><cell>Initial Learning Rate</cell><cell>0.0001</cell></row><row><cell>Weight Decay</cell><cell>0.00004</cell></row><row><cell cols="2">Learning Dropout rate 0.2</cell></row><row><cell>Loss Function</cell><cell>Softmax Cross Entropy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,421.85,189.42"><head>Table 3</head><label>3</label><figDesc>Details of our trained models. "IR" refers to the Inception-ResNet-v2 model, while "'I" refers to the Inception-v4 model.</figDesc><table coords="6,95.27,134.06,415.57,145.85"><row><cell>Model</cell><cell cols="4">Network architecture augmentation batching Increased Balanced Training data</cell></row><row><cell>Multi-IR (Trusted)</cell><cell>IR</cell><cell>No</cell><cell>No</cell><cell>Multi-organ (Trusted)</cell></row><row><cell>Multi-I</cell><cell>I</cell><cell>No</cell><cell>No</cell><cell>Multi-organ (Trusted &amp; Web)</cell></row><row><cell>Multi-IR</cell><cell>IR</cell><cell>No</cell><cell>No</cell><cell>Multi-organ (Trusted &amp; Web)</cell></row><row><cell>Multi-IR-AUG</cell><cell>IR</cell><cell>Yes</cell><cell>No</cell><cell>Multi-organ (Trusted &amp; Web)</cell></row><row><cell>Multi-IR-AUG-B</cell><cell>IR</cell><cell>Yes</cell><cell cols="2">Balanced Multi-organ (Trusted &amp; Web)</cell></row><row><cell>Single-IR-AUG-B-Bark</cell><cell>IR</cell><cell>Yes</cell><cell cols="2">Balanced Single-organ (Trusted &amp; Web)</cell></row><row><cell cols="2">Single-IR-AUG-B-Flower IR</cell><cell>Yes</cell><cell cols="2">Balanced Single-organ (Trusted &amp; Web)</cell></row><row><cell>Single-IR-AUG-B-Fruit</cell><cell>IR</cell><cell>Yes</cell><cell cols="2">Balanced Single-organ (Trusted &amp; Web)</cell></row><row><cell>Single-IR-AUG-B-Habit</cell><cell>IR</cell><cell>Yes</cell><cell cols="2">Balanced Single-organ (Trusted &amp; Web)</cell></row><row><cell>Single-IR-AUG-B-Leaf</cell><cell>IR</cell><cell>Yes</cell><cell cols="2">Balanced Single-organ (Trusted &amp; Web)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,302.17,418.66,62.17"><head>Table 4</head><label>4</label><figDesc>Performance of our submitted runs. The ones with (Feature embedding matching) indicates the predictions were made by the feature embedding method. Otherwise, they were obtained using the Softmax funtion.</figDesc><table coords="6,98.17,355.47,68.77,8.87"><row><cell>Run</cell><cell>Model(s)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The resources of this project is supported by <rs type="funder">NEUON AI SDN. BHD.</rs>, <rs type="funder">Malaysia</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,393.33,10.91;8,112.66,124.83,228.79,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,250.49,111.28,255.50,10.91;8,112.66,124.83,69.87,10.91">Global patterns of fern species diversity: An evaluation of fern data in gbif</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,190.38,124.83,67.13,10.91">Plant Diversity</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="135" to="140" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,138.38,393.33,10.91;8,112.66,151.93,131.56,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,234.04,138.38,233.14,10.91">Plants and climate change: complexities and surprises</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Parmesan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Hanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,475.09,138.38,30.90,10.91;8,112.66,151.93,42.56,10.91">Annals of botany</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="849" to="864" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,165.48,393.33,10.91;8,112.66,179.03,393.33,10.91;8,112.33,192.57,29.19,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,383.69,165.48,122.30,10.91;8,112.66,179.03,274.14,10.91">Plant biodiversity promotes sustainable agriculture directly and via belowground effects</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Domeignoz-Horta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Loaiza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-L</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,397.50,179.03,108.49,10.91">Trends in Plant Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,206.12,394.53,10.91;8,112.66,219.67,393.33,10.91;8,112.66,233.22,314.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,205.12,219.67,300.87,10.91;8,112.66,233.22,172.39,10.91">Dry beans (phaseolus vulgaris l.) as a vital component of sustainable agriculture and food security-a review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Uebersax</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Porch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heitholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Osorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kamfwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Snapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,293.53,233.22,71.36,10.91">Legume Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,246.77,393.33,10.91;8,112.66,260.32,363.51,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,422.53,246.77,83.45,10.91;8,112.66,260.32,157.69,10.91">An ethnobotanical study of medicinal plants in kinmen</title>
		<author>
			<persName coords=""><forename type="first">S.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,279.46,260.32,118.76,10.91">Frontiers in Pharmacology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">681190</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,273.87,393.33,10.91;8,112.66,287.42,343.24,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,220.70,273.87,285.29,10.91;8,112.66,287.42,53.13,10.91">Plant taxonomy: A historical perspective, current challenges, and perspectives</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rouhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gaudeul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,174.14,287.42,225.92,10.91">Molecular plant taxonomy: Methods and protocols</title>
		<imprint>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,300.97,393.32,10.91;8,112.66,314.52,393.33,10.91;8,112.33,328.07,58.71,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,241.41,300.97,264.57,10.91;8,112.66,314.52,59.83,10.91">Overview of plantclef 2023: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.94,314.52,322.04,10.91">Working Notes of CLEF -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,341.62,394.53,10.91;8,112.33,355.17,395.33,10.91;8,112.30,368.71,394.97,10.91;8,112.66,382.26,393.33,10.91;8,112.66,395.81,393.33,10.91;8,112.66,409.36,118.05,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,391.39,368.71,115.88,10.91;8,112.66,382.26,393.33,10.91;8,112.66,395.81,19.81,10.91">Overview of lifeclef 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,157.67,395.81,348.32,10.91;8,112.66,409.36,44.89,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,422.91,393.32,10.91;8,112.66,436.46,206.67,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,241.41,422.91,264.57,10.91;8,112.66,436.46,60.93,10.91">Overview of plantclef 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,181.51,436.46,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,450.01,394.52,10.91;8,112.48,463.56,146.07,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,255.77,450.01,247.04,10.91">A survey on image data augmentation for deep learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.48,463.56,82.43,10.91">Journal of big data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,477.11,393.33,10.91;8,112.66,490.66,393.33,10.91;8,112.66,504.21,137.19,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,303.68,477.11,202.31,10.91;8,112.66,490.66,161.01,10.91">A preliminary study on data augmentation of deep learning for image classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,295.56,490.66,210.43,10.91;8,112.66,504.21,69.56,10.91">Proceedings of the 11th Asia-Pacific Symposium on Internetware</title>
		<meeting>the 11th Asia-Pacific Symposium on Internetware</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,517.76,393.33,10.91;8,112.33,531.30,294.82,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,312.51,517.76,161.71,10.91">Deep long-tailed learning: A survey</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,484.09,517.76,21.90,10.91;8,112.33,531.30,262.90,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,544.85,393.33,10.91;8,112.66,558.40,393.33,10.91;8,112.66,571.95,342.33,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,187.56,544.85,318.43,10.91;8,112.66,558.40,38.41,10.91">A multi-organ plant identification method using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSESS.2017.8342935</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,173.06,558.40,332.93,10.91;8,112.66,571.95,72.55,10.91">2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,585.50,394.61,10.91;8,112.66,599.05,302.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,259.75,585.50,247.52,10.91;8,112.66,599.05,156.28,10.91">A global-scale plant identification using deep learning: Neuon submission to plantclef 2022</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,277.41,599.05,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,612.60,393.33,10.91;8,112.66,626.15,393.58,10.91;8,112.33,639.70,101.72,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,410.21,612.60,95.77,10.91;8,112.66,626.15,340.20,10.91">Plant identication on amazonian and guiana shield flora: Neuon submission to lifeclef 2019 plant</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Al Monnaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,481.58,626.15,24.66,10.91;8,112.33,639.70,71.82,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
