<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.76,319.30,14.99">Classic Approaches to Bird Song Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,116.13,104.26,10.54"><forename type="first">Mihai-Dimitrie</forename><surname>Minuț</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University &quot;Alexandru Ioan Cuza&quot; of Iasi</orgName>
								<address>
									<addrLine>Street General Henri Mathias Berthelot 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iași, Iași</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,184.65,116.13,97.00,10.54"><forename type="first">Cristian</forename><surname>Simionescu</surname></persName>
							<email>cristian@nexusmedia.ro</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University &quot;Alexandru Ioan Cuza&quot; of Iasi</orgName>
								<address>
									<addrLine>Street General Henri Mathias Berthelot 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iași, Iași</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.97,116.13,64.30,10.54"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University &quot;Alexandru Ioan Cuza&quot; of Iasi</orgName>
								<address>
									<addrLine>Street General Henri Mathias Berthelot 16</addrLine>
									<postCode>700483</postCode>
									<settlement>Iași, Iași</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.76,319.30,14.99">Classic Approaches to Bird Song Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67D493F5463FD1AE06B134A00820D582</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sound classification</term>
					<term>Mel Spectrograms</term>
					<term>1/2D Convolutions</term>
					<term>EfficientNetV2</term>
					<term>Noise</term>
					<term>Augmentation 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Processing bird audio data and building classifier algorithms lead to different insights about the bird species, in turn improving what is known about avian biodiversity. The research for audio processing and audio classification has been ongoing for some time and with major advances. Paired together with the many existing species and sound types of birds it makes the task of bird song classification complex. This work explores popular audio methods to preprocess and augment two types of extracted features, audio and spectrogram images, by making use of deep learning techniques such as pretraining, fine-tuning, and transfer learning. Results feature averagely good models while showcasing the effect of certain data modifications or classification training techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This work represents the working note and the details from participating in the BirdClef2023 competition, which is taking place as part of the LifeClef2023 branch <ref type="bibr" coords="1,388.55,404.74,11.68,9.66" target="#b0">[1]</ref>. The competition goal is to construct classification algorithms that will be used to recognize and attribute the bird species for a given audio signal as input <ref type="bibr" coords="1,196.34,430.03,11.68,9.66" target="#b1">[2]</ref>. The main focus of this working note is the deep learning experiments based on both types of data: audio time series and image spectrogram data; paired together with different preprocessing and augmentation techniques for both image and signal data. The best results feature models based on 1D CNN architectures trained and fine-tuned on audio signal data that make use of background noise as augmentation; achieving 0.63186 and 0.74384 private and public scores respectively. The rest of the paper includes the details related to the activity submitted by us regarding sending the runs in this competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Solution Objectives</head><p>The large amounts of audio data and the BirdClef2023 competition requirements for the model evaluation, under 2 hours and CPU only runs, imposed quite strict restrictions on what an individual can submit, making very deep and complex models impractical <ref type="bibr" coords="1,359.32,573.61,11.68,9.66" target="#b1">[2]</ref>. By taking the restrictions and the development environment, along with the team size of one person and the late registration to the competition (last month), the goals are not set to be on the overpromising side. As such, the proposed objectives of this work are to:</p><p>• establish a preprocessing and augmentation pipeline for the dataset;</p><p>• experiment on simpler model architectures trained from scratch;</p><p>• experiment with finetuning and transfer learning on pre-trained architectures, such as the EfficientNetv2 <ref type="bibr" coords="1,139.86,662.16,11.90,9.66" target="#b2">[3]</ref>;</p><p>• cover experiments on both signal time series data and spectrogram image data; • explore with Gaussian and Normally distributed background noise <ref type="bibr" coords="2,387.74,87.64,12.82,9.66" target="#b3">[4]</ref>  <ref type="bibr" coords="2,403.31,87.64,11.90,9.66" target="#b4">[5]</ref>;</p><p>• construct a final model pipeline with the best-found insights; Some of the selected experiments were chosen to test and experiment with past popular techniques applied to audio time series and spectrogram data. A good example would be the time series and spectrogram image augmentation results presented in <ref type="bibr" coords="2,311.12,138.23,11.68,9.66" target="#b5">[6]</ref>, which present Time Warping and Scaling as being among the best augmentation methods for audio time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Exploration</head><p>The dataset provided by the competition after registration contains 264 folders representing each class through an ID, with distinct amounts containing audio data files with varying durations. Each directory contains audio recordings saved in the OGG digital format. By exploring the dataset, some issues are identified: the audio file class imbalance (Fig. <ref type="figure" coords="2,327.69,231.21,3.97,9.66">1</ref>), different noise levels between audio files (Fig. <ref type="figure" coords="2,95.82,243.86,3.97,9.66" target="#fig_1">3</ref>), and empty segments longer than 5 seconds (Fig. <ref type="figure" coords="2,325.09,243.86,3.97,9.66" target="#fig_5">2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.Audio Data Frame Cutting</head><p>The initial audio file data is not in the correct format that allows for easy processing, so it needs to be cut into 5 seconds pieces with a sample rate of 32,000, implying a resulting dataset of samples of shape (160,000). Among the considered techniques for cutting up data from Figure <ref type="figure" coords="3,457.54,346.54,4.12,9.66" target="#fig_2">4</ref>, the one that would bring the most quality would be using bird sound detection <ref type="bibr" coords="3,368.68,359.19,11.68,9.66" target="#b6">[7]</ref>. The compromise between time and quality was to take the simple window shift without overlap because it would contain windows with the bird calls and a smaller quantity of empty fragments compared to the window shift with the overlap method. By applying the aforementioned method, the balance of data has suffered even further, with some classes having as little as 1 window of 5 seconds while others have over 1,000 inputs worth of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.Dataset Splitting</head><p>The setup for evaluation is done by splitting the training data by a ratio of 70:30 between train and evaluation, as a swifter alternative to the better-performing K-folds cross-validation technique <ref type="bibr" coords="3,488.11,477.47,11.68,9.66" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.Feature extraction</head><p>From the initial raw audio frames, by applying the Short-time Fourier Transform <ref type="bibr" coords="3,452.95,519.86,12.82,9.66" target="#b8">[9]</ref> and the Mel Scale transformation the log-mel features can be extracted <ref type="bibr" coords="3,333.06,532.51,16.85,9.66" target="#b9">[10]</ref>, which can be further transformed into the Mel spectrogram image. To obtain a square image shape with size (128, 128), the following parameters for the algorithm were used: 128 Number of Mels, the competition sample rate of 32,000/second, and a hop length equal to ⌈ 160000 / 128 ⌉.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.Dataset Augmentation</head><p>The augmentation was done both on the preprocessing side for the spectrogram data but only dynamically on batch load during training. The three used techniques for augmentation, which are shown to work well on audio data <ref type="bibr" coords="3,244.73,638.14,11.68,9.66" target="#b5">[6]</ref>, are Time Warping, Scaling, and Shifting (a subclass of permutation with one cut). Data scaling is a form of augmentation that reduces or decreases the values of a random time window of the time series <ref type="bibr" coords="3,273.20,663.44,11.68,9.66" target="#b5">[6]</ref>. The magnitude of the scaling was uniformly chosen between -0.1 and 0.1. Time Warping dilates or contracts random windows of time that are predominantly used as a technique for time series augmentation <ref type="bibr" coords="3,357.96,688.73,11.68,9.66" target="#b5">[6]</ref>. As can be seen in Fig. <ref type="figure" coords="3,474.27,688.73,4.58,9.66" target="#fig_3">5</ref> the effect on the spectrogram data is pretty damaging which is why it was omitted from the spectrogram preprocessing pipeline; this augmentation technique is still safe to use in the case of working directly with the time series data.  Shifting, a subclass of permutation, is an augmentation technique that cuts the time series in two and rearranges the order resulting in pieces <ref type="bibr" coords="4,269.23,625.24,11.68,9.66" target="#b5">[6]</ref>. Two types of shifting were applied: shifting the time series by a factor between -0.1 and 0.1, and cutting directly in half, and reordering the two pieces. The impact of the shifting on the spectrogram image data was minimal in most cases due to the bird song not being constant during the audio. Still, on the signal time series data counterpart, the discontinuity of values among the time axis was pretty visible and clear to affect the data quality (see Fig. <ref type="figure" coords="4,479.30,675.84,3.97,9.66" target="#fig_4">6</ref>).</p><p>The algorithm used to augment the dataset in the preprocessing phase is the one described in Algorithm 1. The goal was to obtain at least 20 distinct input samples per class and favor species with less than 264 input audio frames and generate more data for them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.Class Weights</head><p>After the augmentation of the spectrograms, the least represented classes had at least 20 samples but still nothing in comparison to 8,000 others; the time series data being unaffected. The problem with weights being too high or low is that during training this will cause the loss to overshoot and explode or vanish certain errors, making the learning process much harder. The solution applied was to smooth out the weights for both the spectrogram and the signal data samples, the result is shown in Fig. <ref type="figure" coords="6,92.16,100.28,4.12,9.66" target="#fig_6">7</ref>. The process for smoothing the data is described in Algorithm 2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.6.Mixup</head><p>The mixup operation was done directly on the audio signals, and then the spectrogram feature extraction was applied to the final combined signal to obtain instead of the original, the mixed-up spectrogram. Usually, Mixup is done during training between random samples <ref type="bibr" coords="6,441.80,688.32,11.68,9.66" target="#b8">[9]</ref>, but given the circumstances of time and resources available, it was decided to build a dataset generator using the mixup method. Additionally, to address the remaining imbalance within the dataset the mixup dataset generator also got provided with the initial class weights and target class weights for the resulting dataset which further balanced the data amount per bird species.</p><p>The target mixup generated dataset has a size of 100,000 and minimum label items of 264. The current label weights is the one presented in Figure <ref type="figure" coords="7,306.16,87.64,9.16,9.66" target="#fig_2">40</ref>, the target label weights are the ones in Figure <ref type="figure" coords="7,72.00,100.28,5.50,9.66" target="#fig_6">7</ref> and the data input is the augmented data, as previously mentioned in the augmentation section for spectrograms. The total amount of data needed per label was given by the target class weights multiplied by the dataset size and finally clipped by the minimum requirement of 264 items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.7.Partitioning</head><p>The final dataset data was separated into multiple partition files, which contained the respective training and validation data in order. One last step applied was to also shuffle the data in order to break the order between the dataset species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.8.Preprocessing Pipeline</head><p>There were two final configurable preprocessing pipelines for building the datasets used to train the models submitted to the competition, which can be seen in Fig. <ref type="figure" coords="7,373.24,273.60,4.28,9.66" target="#fig_7">8</ref>: one for images and another one for signals. In Fig. <ref type="figure" coords="7,160.16,286.25,4.12,9.66" target="#fig_7">8</ref>, all of the experimental preprocessing steps were included: with green the kept ones and red the discarded ones. Validation data is represented by the orange color while training by the violet one. The final product of the pipelines is the variations of the signal dataset, variations of the spectrogram dataset, and the final training class weights. The final datasets containing spectrograms were generated both with and without any augmentation to cover all cases when experimenting. A considerable observation would be that the augmentation that will happen during the training process is not shown in the below schema, and it will be included in the Deep Learning Experiments chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep Learning Experiments</head><p>Common denominators among all the experiments are the Adam optimizer with a starting learning rate of 0.00001-0.000001, the Binary Crossentropy Loss function, and the ROC AUC metric used to evaluate the model on training and validation data. The PR AUC metric is also used as a measurement in some of the experiments. One common feature is the method of loading the data partitions which was done by prefetching and loading at least 2 partitions ahead, which improved the training times for both the image and time series datasets. Initial testing was done with the batch size with values from {16, 32, 64, 128, 256} and 64 items per batch achieved the shortest train epoch duration in both scenarios: image and time series data. All of the models were scheduled to train up to 100 epochs and used a method to save and keep track of the best validation-performing model at each epoch. If no improvements to the validation metrics were made for more than 20 epochs the training would be stopped. The experiments tables are made to highlight the most significant changes in the training process and one entry from those tables might be in reality a series of tests concluding into that result. There were 3 model types with which were experimented: 2D convolutional models trained from scratch on spectrogram data, 1D convolutional models for signal data trained from scratch, and experiments with transfer learning and fine-tuning the EfficientNet V2 B0.</p><p>The final submission was made by taking the best three already submitted models (Table <ref type="table" coords="7,480.77,619.56,4.12,9.66">4</ref>, indices 6, 7, and 8, focus on public score) and fine-tuning them in 3 steps of 3 epochs each, on the validation data. Fine-tuning was done only on the last dense layers with a much lower learning rate. After the 3 fine tuning iterations, the best model was the one using the base 1D convolutional architecture paired together with some uniform background noise (indices 8-11 from Table <ref type="table" coords="7,388.90,670.16,3.97,9.66">4</ref>).  The model submissions were made using the Kaggle web platform<ref type="foot" coords="9,386.36,524.26,3.30,5.80" target="#foot_0">2</ref> for uploading the models and running the notebooks. An important note is that only one author kaggle account was used in preparing and submitting competition runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment Results</head><p>The augmentation methods that were used, Time Warping, Scaling, and Shifting as described in <ref type="bibr" coords="9,72.00,605.85,11.68,9.66" target="#b5">[6]</ref>, did not seem to impact the training that much, sometimes even being detrimental; with no strong evidence in the case of spectrograms due to the case of faulty data. Among all of the training experiments, multiple activation functions have been tried and the ones with the most consistent good results were Sigmoid, Relu, and LeakyRelu. By consulting all the experiments, the fine-tuning techniques always helped and provided models with an easier way to improve. Another, not as highlighted, technique is the class weights applied to the loss, which helped combat the data imbalance, especially during the 1D convolutional training sessions. The best submission made (index 9 from Table <ref type="table" coords="9,135.86,694.39,4.58,9.66">4</ref>) was a convolutional 1D model built and trained from scratch by using a variety of deep learning techniques combined with added background uniform noise; which was also fine-tuned on the validation dataset as a last successful step at improving the quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4. Model variants submitted to the competition through Kaggle and their final results</head><p>Training and experimenting with both the waveform of audio data and spectrogram image data helps in gathering (Tables <ref type="table" coords="10,191.45,372.56,4.58,9.66" target="#tab_0">1</ref><ref type="table" coords="10,196.03,372.56,4.58,9.66" target="#tab_1">2</ref><ref type="table" coords="10,200.60,372.56,4.58,9.66" target="#tab_2">3</ref>) a variety of helpful insights regarding the portability and consistency of the used training and pretraining methods. It also provides a solid ground for training more complex ensemble models that make use of both types of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future Work</head><p>There are many possible directions for further explorations, coming from both explored and unexplored ideas. An interesting idea that some participants used during the competition was to complement the existing dataset with external bird audio data. This proved to be one of the key points in obtaining higher-quality classification models. A future iteration of the dataset could potentially fix the problem either by applying transformations to entire data files or by finding a way to exclude empty audio frames altogether. There are many possible directions for further explorations, coming from both explored and unexplored ideas. An interesting idea that some participants used during the competition 24 was to complement the existing dataset with external bird audio data. This proved to be one of the key points in obtaining higher-quality classification models. Because of the data issue, the spectrograms' convolutional models were compromised. A future iteration of the dataset could potentially fix the problem either by applying transformations to entire data files or by finding a way to exclude empty audio frames altogether.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Looking back to the original goals and comparing them to the current state, the dataset-building pipeline has been finalized and, besides the problem in the mel spectrogram image pipeline, it helped in building training-ready datasets. The performed experiments added value through the insights they offered and allowed in the end to build better valid submissions. Most techniques did not manage to achieve a good performance, while others seemed to always improve the results; a case being the added background noise. The classic and simple model architectures turned out to be able to learn to distinguish among the bird species and even showed much more potential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,132.59,521.29,344.34,9.66;2,73.50,267.67,438.00,248.25"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure 1. Audio files per bird species contained within the competition dataset</figDesc><graphic coords="2,73.50,267.67,438.00,248.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,86.51,249.12,422.27,9.66"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Audio frames from different recordings showcasing the background noise discrepancy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,93.51,393.12,422.50,9.66;4,73.50,73.50,437.25,314.25"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Possible solutions for extracting audio frame samples from the initial audio recordings</figDesc><graphic coords="4,73.50,73.50,437.25,314.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,78.03,587.29,439.21,9.66;4,73.50,416.93,435.00,165.00"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Mel Spectrogram of a plain bird signal audio (left) and augmented with Time Warp (right)</figDesc><graphic coords="4,73.50,416.93,435.00,165.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,88.47,339.87,432.58,9.66;5,195.53,352.51,204.22,9.66;5,41.90,73.50,494.07,261.00"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Audio signal shifted circularly at the middle point and leaving a big discontinuity in data making the data less usable as plain waveform</figDesc><graphic coords="5,41.90,73.50,494.07,261.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,81.81,253.21,431.67,9.66"><head>Algorithm 2 .</head><label>2</label><figDesc>Method used to directly scale and smooth the training weights for the model training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,76.77,620.64,441.73,9.66;6,73.50,277.02,451.50,338.25"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Resulting class weights used for to increase loss during training for misrepresented species</figDesc><graphic coords="6,73.50,277.02,451.50,338.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,111.84,732.12,385.85,9.66;8,74.02,744.76,447.23,9.66;8,176.14,757.41,242.99,9.66;8,87.75,73.50,360.00,653.25"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Data preprocessing pipeline which includes the steps used to obtain the Image Spectrogram, Audio Waveform Datasets and training class weights (red marks unperformant methods while green marks the ones used within final pipelines)</figDesc><graphic coords="8,87.75,73.50,360.00,653.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="1,0.00,190.72,596.00,460.55"><head></head><label></label><figDesc></figDesc><graphic coords="1,0.00,190.72,596.00,460.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,73.50,86.15,444.75,186.75"><head></head><label></label><figDesc></figDesc><graphic coords="9,73.50,86.15,444.75,186.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,73.50,275.08,448.50,150.44"><head></head><label></label><figDesc></figDesc><graphic coords="9,73.50,275.08,448.50,150.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,87.75,86.15,429.78,283.08"><head></head><label></label><figDesc></figDesc><graphic coords="10,87.75,86.15,429.78,283.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.25,74.99,326.78,9.66"><head>Table 1 .</head><label>1</label><figDesc>Experiments with self made and trained 2D convolutional models</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,103.88,263.91,387.53,9.66"><head>Table 2 .</head><label>2</label><figDesc>Transfer Learning and Fine-tuning Experiments on EfficientNetv2B0 backbone</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.25,415.34,326.78,9.66"><head>Table 3 .</head><label>3</label><figDesc>Experiments with self made and trained 1D convolutional models</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="9,91.75,761.11,209.23,8.78"><p>https://www.kaggle.com/competitions/birdclef-2023</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,740.06,414.55,9.66;10,108.00,752.70,415.21,9.66;11,108.00,74.99,414.91,9.66;11,108.00,87.63,415.25,9.66;11,108.00,100.28,414.85,9.66;11,108.00,112.93,120.75,9.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,419.73,74.99,103.18,9.66;11,108.00,87.63,415.25,9.66;11,108.00,100.28,21.63,9.66">Overview of LifeCLEF 2023: evaluation of ai models for the identification and prediction of birds, plants, snakes and fungi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hrúz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,155.55,100.28,367.30,9.66;11,108.00,112.93,45.33,9.66">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,125.58,415.11,9.66;11,108.00,138.23,414.81,9.66;11,108.00,150.88,414.74,9.66;11,108.00,163.53,111.74,9.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,255.58,138.23,267.23,9.66;11,108.00,150.88,137.03,9.66">Overview of BirdCLEF 2023: Automated bird species identification in Eastern Africa</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Reers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cherutich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,252.93,150.88,269.81,9.66;11,108.00,163.53,79.69,9.66">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,176.18,414.92,9.66;11,108.00,188.83,100.15,9.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,225.91,176.18,231.61,9.66">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/2104.00298</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,470.88,176.18,24.19,9.66">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,201.48,414.71,9.66;11,108.00,214.13,415.25,9.66;11,108.00,226.77,24.73,9.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,372.23,201.48,150.48,9.66;11,108.00,214.13,186.24,9.66">Birdclef 2021: building a birdcall segmentation model based on weak labels</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Shugaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tanahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,316.60,214.13,201.31,9.66">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,239.42,414.56,9.66;11,108.00,252.07,303.76,9.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,298.91,239.42,223.66,9.66;11,108.00,252.07,145.52,9.66">Classification of complicated urban forest acoustic scenes with deep learning models</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,266.11,252.07,30.19,9.66">Forests</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">206</biblScope>
			<date type="published" when="2023-01">Jan 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,264.72,415.11,9.66;11,108.00,277.37,340.10,9.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,255.43,264.72,267.68,9.66;11,108.00,277.37,151.59,9.66">An empirical survey of data augmentation for time series classification with neural networks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Iwana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,271.92,277.37,48.63,9.66">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2021-07">07 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,290.02,414.59,9.66;11,108.00,302.67,414.85,9.66;11,108.00,315.32,414.90,9.66;11,108.00,327.96,415.22,9.66;11,108.00,340.61,414.71,9.66;11,108.00,353.26,151.86,9.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,255.42,290.02,267.17,9.66;11,108.00,302.67,391.33,9.66">Tuc media computing at birdclef 2021: Noise augmentation strategies in bird sound classification in combination with densenets and resnets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="11,108.00,315.32,414.90,9.66;11,108.00,327.96,26.72,9.66">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="11,361.02,340.61,137.50,9.66">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">F</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2021</date>
			<biblScope unit="volume">2936</biblScope>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,116.00,365.91,407.24,9.66;11,108.00,378.56,415.19,9.66;11,108.00,391.21,68.70,9.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,323.59,365.91,199.65,9.66;11,108.00,378.56,293.20,9.66">An efficient data partitioning to improve classification performance while keeping parameters interpretable</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">N</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,413.12,378.56,49.38,9.66">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016-08">08 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,403.86,759.76,9.66;11,174.37,416.51,348.73,9.66;11,108.00,429.16,68.70,9.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,285.05,403.86,582.71,9.66;11,174.37,416.51,141.91,9.66">Time-frequency feature representation using energy ćoncentration: An overview of recent advances</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sejdic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Djurovi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,328.61,416.51,109.92,9.66">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="183" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.50,441.80,393.35,9.66;11,108.00,454.45,142.50,9.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,208.66,441.80,240.94,9.66">Mel frequency cepstral coefficients for music modeling</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,477.79,441.80,21.26,9.66">Ismir</title>
		<meeting><address><addrLine>Plymouth, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,109.13,467.10,754.93,9.66;11,176.99,479.75,152.68,9.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,379.09,467.10,435.69,9.66">mixup: Beyond empirical risk ḿinimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>abs/1710.09412</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,176.99,479.75,24.19,9.66">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
