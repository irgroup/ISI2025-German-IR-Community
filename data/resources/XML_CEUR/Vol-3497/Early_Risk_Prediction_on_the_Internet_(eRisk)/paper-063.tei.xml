<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,121.02,15.42;1,311.20,84.74,147.04,15.42;1,89.29,106.66,363.62,15.42;1,89.29,128.58,83.93,15.43;1,89.29,150.91,353.37,11.96">SINAI at eRisk</title>
				<funder ref="#_HPeRhde">
					<orgName type="full">FEDER</orgName>
				</funder>
				<funder ref="#_AzhWZgx">
					<orgName type="full">Plan Nacional I+D+i</orgName>
				</funder>
				<funder ref="#_TN8fzH8">
					<orgName type="full">Andalusian Regional Government</orgName>
				</funder>
				<funder ref="#_5T6SBCb">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Consumer Affairs of the Spanish Government</orgName>
				</funder>
				<funder ref="#_EyEADkq #_c32MDWV">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,176.82,136.14,11.96"><forename type="first">Alba</forename><surname>Mar√≠a M√°rmol-Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">SINAI</orgName>
								<orgName type="department" key="dep3">CEATIC</orgName>
								<orgName type="institution">Universidad de Ja√©n</orgName>
								<address>
									<postCode>23071</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.62,176.82,131.13,11.96"><forename type="first">Flor</forename><forename type="middle">Miriam</forename><surname>Plaza-Del-Arco</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bocconi University</orgName>
								<address>
									<addrLine>Via Sarfatti 25</addrLine>
									<postCode>20100</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.45,176.82,102.50,11.96"><forename type="first">Arturo</forename><surname>Montejo-R√°ez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">SINAI</orgName>
								<orgName type="department" key="dep3">CEATIC</orgName>
								<orgName type="institution">Universidad de Ja√©n</orgName>
								<address>
									<postCode>23071</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,121.02,15.42;1,311.20,84.74,147.04,15.42;1,89.29,106.66,363.62,15.42;1,89.29,128.58,83.93,15.43;1,89.29,150.91,353.37,11.96">SINAI at eRisk</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">06978A542C58F62795C03955B8E38587</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Early risk prediction</term>
					<term>Gambling detection</term>
					<term>Natural Language Processing</term>
					<term>Transformers</term>
					<term>LSTM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, one of the proposed tasks has been addressed: Task 2 on the early detection of signs of pathological gambling. The approach presented in Task 2 is based on pre-trained models from Transformers architecture with comprehensive preprocessing data and data balancing techniques. Moreover, we integrate Long-short Term Memory (LSTM) architecture with automodels from Transformers. In this Task, our team has been ranked in seventh position, with an F1 score of 0.126, out of 49 participant submissions and achieves the highest values in recall metrics and metrics related to early detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The large amount of content posted daily on social media has made them a significant source of data for the early detection of mental disorders and risky behaviours. The eRisk@CLEF 2023 lab <ref type="bibr" coords="1,89.29,450.52,12.81,10.91" target="#b0">[1]</ref> focuses on early risk prediction on the Internet and its goal is to promote the development of automatic systems for the detection of mental disorders such as depression, self-harm or eating disorders. In this edition, three tasks have been proposed:</p><p>‚Ä¢ Task 1: Search for symptoms of depression. It is a new task consisting of ranking sentences from a collection of user writings according to their relevance to a depression symptom. Then, the participants will have to provide rankings for the 21 symptoms of depression from the BDI Questionnaire ‚Ä¢ Task 2: Early Detection of Signs of Pathological Gambling. It involves sequentially processing writings and detecting as early as possible the first signs of pathological CLEF 2023: Conference and Labs of the Evaluation Forum, September 18-21, 2023, Thessaloniki, Greece amarmol@ujaen.es (A. M. M√°rmol-Romero); flor.plaza@unibocconi.it (F. M. Plaza-del-Arco); amontejo@ujaen.es (A. Montejo-R√°ez) 0000-0001-7952-4541 (A. M. M√°rmol-Romero); 0000-0002-3020-5512 (F. M. Plaza-del-Arco); 0000-0002-8643-2714 (A. <ref type="bibr" coords="1,103.38,647.48,53.57,8.97">Montejo-R√°ez)</ref> gambling. It is a continuation of Task 1 proposed for eRisk 2021 and for eRisk 2022, but the difference is that in this edition a larger amount of training data has been provided. The training data of this edition comprises all test users of the 2021 and 2022 tasks.</p><p>‚Ä¢ Task 3: Measuring the severity of the signs of Eating Disorders. Its aim is to estimate a user's level of disordered eating from his or her history of posts. For this purpose, for each user, a standard eating disorder questionnaire (EDE-Q) has to be filled in.</p><p>Currently, our research group SINAI<ref type="foot" coords="2,265.58,178.28,3.71,7.97" target="#foot_0">1</ref> is working on the PRECOM project<ref type="foot" coords="2,432.69,178.28,3.71,7.97" target="#foot_1">2</ref> focused on the early detection of gambling addiction risk behaviour. Therefore, our interest in developing systems as those expected to answer eRisk tasks is high, as it is a perfect playground to test our approaches.</p><p>In this sense, our main goals are not only to produce systems reporting high performance but to understand the best methods and approaches that can be applied in similar scenarios. It is not our aim to put as many features and as many systems as possible all together in an ensemble of predictors to gain the top ranking position, but rather to find out the best approaches that can be applied to our project's objectives. The design of online and monitoring tools, as is requested in Task 1, along with the ability to understand user's disorder, the main pursuit in Task 3, fully matches our research interests.</p><p>This work presents the participation of our research group, the SINAI team, in Task 2: Early Detection of Signs of Pathological Gambling.</p><p>The rest of the paper is organized as follows. Section 2 describes the details of our participation in Task 2. It is divided into subsections in which, first, we introduce what the task consists of, the data provided and the evaluation measures used. Secondly, the system developed and the methodology used is presented. Thirdly, the experimental setup is detailed. Subsequently, the results obtained and a discussion of them are presented. Finally, Section 3 shows the conclusions obtained after participation in the eRisk lab and the perspectives for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 2: Early Detection of Signs of Pathological Gambling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Task description</head><p>This particular task focuses on identifying signs of gambling addiction at an early stage by analyzing posts from social media in the exact order they were published. The participating systems were required to read the posts, process them, and generate a response to proceed to the next set of posts. The dataset used for this task consists of 33,719 posts from 103 individuals who were categorized as having a positive association with gambling addiction, as well as approximately 1 million posts from 2,071 individuals who were not categorized as addicts <ref type="bibr" coords="2,490.26,571.07,11.43,10.91" target="#b1">[2]</ref>.</p><p>The task is approached from two different perspectives: as a binary decision problem and as a ranking (regression) decision problem. In the binary decision problem, the posts need to be classified as either positive (label 1, indicating the presence of addiction) or negative (label 0, indicating no addiction). The earlier the system detects the presence of addiction, the better it performs, as reflected by the evaluation metrics proposed by the organizers, namely ERDE and ùêπ ùëôùëéùë°ùëíùëõùëêùë¶ , along with well-known measures such as precision, recall, and F1 scores. In the ranking decision problem, instead of assigning binary labels, the system computes a score representing the estimated risk of developing a gambling addiction. Various metrics commonly used in information retrieval, such as P@10 or NDCG, are employed to evaluate this alternative perspective of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">System and methods</head><p>To tackle this task, we adopted a supervised learning approach. Our models were trained using the provided training dataset from the eRisk organizers, which combined data from the years 2021 and 2022. This dataset consists of a time series of posts authored by various users. Our main contributions in this task have been the data processing work done as well as the handling of the unbalanced data and the development of a new system that integrates a transformer with an LSTM network.</p><p>We performed two different methods of pre-processing on the training data: one comprehensive method and another lighter method tailored to the specific system structure where it is employed. The lighter method simply replaces URLs with tags, while the comprehensive method involves a series of steps. Here is a detailed explanation of the extensive processing we conducted:</p><p>‚Ä¢ We replaced all numeric HTML entities in the post string with their corresponding Unicode character equivalents. ‚Ä¢ Users, URLs, and emails mentioned in the messages were identified and replaced with #USER, #URL, and #EMAIL, respectively. ‚Ä¢ We removed any empty characters and replaced special Unicode white-space characters that may have been present in the text. ‚Ä¢ Emojis were substituted with strings using the emoji library <ref type="foot" coords="3,384.12,435.49,3.71,7.97" target="#foot_2">3</ref> .</p><p>‚Ä¢ If more than seven identical punctuation marks appeared consecutively, they were reduced to only three instances. ‚Ä¢ Markdown formatting was removed from the text.</p><p>‚Ä¢ We standardized the use of double and single quotes in the text string, replacing curved quotes and curved single quotes with straight double and single quotes, respectively.</p><p>For our participation in eRisk Task 2, we trained a total of five models. Two systems utilized the XLM-Roberta-Large <ref type="bibr" coords="3,201.50,548.11,13.00,10.91" target="#b2">[3]</ref> model, while the other three employed the RoBERTa-Large <ref type="bibr" coords="3,492.99,548.11,12.99,10.91" target="#b3">[4]</ref> linguistic model. Four of these systems followed the architecture depicted in Figure <ref type="figure" coords="3,463.00,561.66,3.74,10.91" target="#fig_0">1</ref>.</p><p>In these four systems, although the models used and the datasets differed, they followed the same structure. The prediction process for Task 2 involved post extraction. All posts by each user were concatenated, and if the concatenated posts exceeded 500 words, the subject was split into sub-subjects. This ensured that the maximum word limit was not exceeded. The concatenated posts served as the input to either the RoBERTa-Large automodel or the XLM-RoBERTa Large automodel. Subsequently, a Feed Forward Neural Network (FFNN) with a single hidden layer was applied to generate the final predictions. The initial layer of the FFNN had an input dimension of 1024 from the embeddings vector generated by RoBERTa. The output size of this layer was set to 256. During the learning process, these outputs were passed through a dropout layer with a probability of 0.5. Finally, a ReLU activation function was applied before feeding the last FFNN with 256 inputs and 2 outputs.</p><p>However, the most innovative system among the others is the one that combines the RoBERTa-Large model with the LSTM architecture, as illustrated in Figure <ref type="figure" coords="4,378.02,348.96,3.74,10.91" target="#fig_1">2</ref>.</p><p>Regarding the system that integrates RoBERTa-Large and the LSTM architecture, the process begins by concatenating the last 50 posts from each subject. This is because we have studied this number is the most appropriate according to this type of dataset as we saw in our last year participation <ref type="bibr" coords="4,149.38,403.16,11.45,10.91" target="#b4">[5]</ref>. This concatenation is subsequently split into rounds, representing the input to the RoBERTa-Large automodel. This approach allows us to simulate the sequencing that will occur during the testing phase. The model's output is then passed to an LSTM layer. The output of the LSTM layer is further processed by a Feed Forward Neural Network (FFNN) with a single hidden layer, generating the final predictions. The initial layer of the FFNN has an input dimension of 1024, and the output of this layer is of size 256. These outputs are subjected to a dropout layer (with a probability of 0.5) during the learning process. Finally, a ReLU activation function is applied before feeding the last FFNN with 256 inputs and 2 outputs. Moreover, the cell state and hidden state are passed to the next round in each batch.</p><p>The systems were implemented using Python packages such as scikit-learn <ref type="bibr" coords="4,429.32,525.10,11.28,10.91" target="#b5">[6]</ref>, Transformers <ref type="bibr" coords="4,89.29,538.65,11.43,10.91" target="#b6">[7]</ref>, and PyTorch <ref type="bibr" coords="4,166.06,538.65,11.43,10.91" target="#b7">[8]</ref>.</p><p>The training was performed on a 2xGPU NVIDIA V100 server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Experimental setup</head><p>First, all the parameters established in these systems and presented in this document are the result of a previous hyperparameter optimisation search carried out with Optuna<ref type="foot" coords="4,439.59,613.72,3.71,7.97" target="#foot_3">4</ref> , including the dataset selected for data balancing. This means that given a dataset with a number of subjects, these subjects were divided into further subjects due to the sequence length limit given by the Transformers models. For this new set, only a part of the subjects belonging to the control group was chosen due to the initial imbalance of the data. Table <ref type="table" coords="5,344.18,443.17,3.69,10.91">1</ref>, shows the information concerning the search space. However, for the latest system that includes the LSTM (run 4), the parameters were chosen according to what our hardware architecture and training time limit allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Search space in the hyperparameter optimisation process. In this case, with the variable "Data" we refer to the number of sub-subjects belonging to the control group we are working with (in the order from the first generated to the last). After the search for hyperparameters, in the learning phase, a maximum length of 512 tokens per document was set. For runs 0 to 3, the document is the concatenation of an undetermined number of posts whose sum of their words is no bigger than 500 and the optimizer used was AdamW. For run four, the document is the concatenation of the last 50 posts of each subject and the optimizer used was also AdamW. In table 2 there is a summary of hyperparameters and models used in each run. ‚Ä¢ Run 0. This run utilized a binary classification model based on RoBERTa-Large. The document used for training was the concatenation of an undetermined number of posts, where the sum of their words did not exceed 500. However, due to the unbalanced nature of the provided data, only the first 15 sub-subjects generated by our preprocessing, with no risk of gambling pathology, and all the sub-subjects generated with the risk of gambling pathology were considered. The training dataset consisted of 14.78% positive subjects and 85.22% negative subjects. The model's output score determined whether a user was considered a potential gambler or not. Users with a score lower than 0.5 were not considered potential gamblers, while those with a score equal to or higher than 0.5 were classified as potential gamblers. ‚Ä¢ Run 1. Similar to Run 0, this run employed a binary classification model based on RoBERTa-Large. The training dataset used the same structure as the other systems, considering the first 10 sub-subjects generated by our preprocessing with no risk of suffering gambling pathology, and all the sub-subjects generated with the risk of gambling pathology. The training dataset for this run contained 18.84% positive subjects and 81.16% negative subjects. ‚Ä¢ Run 2. In this run, the model used was XLM-RoBERTa-Large, while maintaining the other settings from Run 0. ‚Ä¢ Run 3. Similar to Run 1, this run utilized the XLM-RoBERTa-Large model instead of RoBERTa-Large, while keeping the remaining configurations the same. ‚Ä¢ Run 4. This run involved a binary classification model based on RoBERTa-Large and the LSTM architecture. The document used for training differed from the other systems due to the longer training time required. Only the last 50 concatenated posts were considered, with the only preprocessing step being the replacement of URLs with #URL. Additionally, 500 negative subjects were randomly selected from the unbalanced training dataset. The training dataset for this run comprised 245 positive subjects (the sum of positive subjects from the previous year's data) and 500 negative subjects. The model's output score determined whether a user was considered a potential gambler or not. Users with a score lower than 0.5 were not considered potential gamblers, while those with a score equal to or higher than 0.5 were classified as potential gamblers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Results and discussion</head><p>Salient results have been achieved with the approaches explored by our team. From the reported results provided by the organizers, we have extracted our scores, which are shown in Tables <ref type="table" coords="7,500.93,121.08,5.06,10.91" target="#tab_2">3</ref> and<ref type="table" coords="7,108.44,134.63,3.74,10.91">4</ref>. In decision-based evaluation, the value of the Recall score obtained by our Runs 0, 1, 2 and 3 (1.000) is the highest among all submissions by participants (49 submissions were reported in total). Our ERDE values are the second and fifteenth highest (0.020 and 0.028). In terms of ùë†ùëùùëíùëíùëë and ùëôùëéùë°ùëíùëõùëêùë¶ ùëá ùëÉ , we also reach top values for run 0 and run 2. Thus, it seems that a larger unbalance of the data and a larger number of epochs contribute slightly to the improvement in prediction. In general, the systems developed demonstrate a high capacity to correctly identify the majority of positive subjects at a fast speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Results of SINAI team for Task 2 in ranking-based evaluation 1 writing 100 writing 500 writing Run P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 0 1.00 1.00 0.72 1.00 1.00 0.88 1.00 1.00 0.85 1 1.00 1.00 0.73 1.00 1.00 0.90 1.00 1.00 0.85 2 1.00 1.00 0.71 1.00 1.00 0.87 1.00 1.00 0.84 3 1.00 1.00 0.72 1.00 1.00 0.89 1.00 1.00 0.86 4 0.80 0.86 0.53 0.90 0.94 0.56 0.70 0.80 0.47</p><p>Regarding ranking-based evaluation (see Table <ref type="table" coords="7,321.19,548.52,3.65,10.91">4</ref>), our team reaches one of the highest positions. High NDCG and P@k values indicate that the systems are capable of providing relevant and quality recommendations.</p><p>The differences in the systems developed according to the metrics appear to be minimal. This makes sense since the systems have almost the same structure. The only different system presented is used in run 4, which has slightly lower values than the rest of the systems, which can be understood by the resource limitations that have been taken into account due to the time required for execution. However, it is the one that we believe has the most promising results with the most research time invested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusions and future work</head><p>This paper describes our participation as SINAI team in Task 2 of the eRisk@CLEF 2023 edition. The former is the continuation of the first edition in 2021 and the second edition in 2022 and aims to detect signs of pathological gambling as soon as possible. For Task 2, we have developed only classification models using state-of-the-art pre-trained language models based on Transformers. Besides, we explored LSTM architecture's integration with this model type.</p><p>In future work, we plan to analyze in depth the use of the LSTM architecture for this kind of problem as the main objective is early detection in a sequential way. We plan to test different training strategies with the integration of a transformers model together with the LSTM architecture. In addition, we expect to continue to work on data processing and the imbalance present in the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,194.20,416.69,8.93;4,89.00,206.20,418.65,8.87;4,89.29,218.16,416.70,8.87;4,89.29,230.11,368.21,8.87;4,89.29,84.19,416.70,102.58"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture for the binary decision task involving the RoBERTa-Large model or the XLM-RoBERTa Large model. The input representation is based on RoBERTa-Large tokenization or XLM-RoBERTa Large tokenization. Each embedding of the model output is passed through a feed-forward neural network with a hidden layer, generating a binary decision and a score as the logits.</figDesc><graphic coords="4,89.29,84.19,416.70,102.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,356.16,416.69,8.93;5,89.29,368.16,416.69,8.87;5,89.29,380.12,416.69,8.87;5,89.29,392.07,418.22,8.87;5,89.29,84.19,416.71,264.54"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architecture for the binary decision task involving the RoBERTa model and LSTM architecture. The input representation consists of RoBERTa-Large tokenization of each subject's message in each round. Each embedding of the model output is passed through an LSTM layer. The output of the LSTM is then fed into a feedforward neural network with one hidden layer, producing a binary decision.</figDesc><graphic coords="5,89.29,84.19,416.71,264.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,152.95,401.19,97.13"><head>Table 2</head><label>2</label><figDesc>Summary of hyperparameters and models used in each run</figDesc><table coords="6,102.60,180.98,387.59,69.09"><row><cell cols="5">Run Batch size Learning rate Weight decay Epochs</cell><cell>Model</cell></row><row><cell>Run 0</cell><cell>8</cell><cell>1e-5</cell><cell>0.01</cell><cell>3</cell><cell>RoBERTa-Large</cell></row><row><cell>Run 1</cell><cell>8</cell><cell>1e-5</cell><cell>0.01</cell><cell>2</cell><cell>RoBERTa-Large</cell></row><row><cell>Run 2</cell><cell>8</cell><cell>1e-5</cell><cell>0.01</cell><cell>3</cell><cell>XLM-RoBERTa-Large</cell></row><row><cell>Run 3</cell><cell>8</cell><cell>1e-5</cell><cell>0.01</cell><cell>2</cell><cell>XLM-RoBERTa-Large</cell></row><row><cell>Run 4</cell><cell>8</cell><cell>1e-6</cell><cell>0</cell><cell>7</cell><cell>RoBERTa-Large &amp; LSTM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,161.19,390.09,94.51"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="7,89.29,173.20,389.80,82.51"><row><cell cols="6">Results of SINAI team for Task 2 in decision-based evaluation</cell><cell></cell></row><row><cell>Run</cell><cell>ùëÉ</cell><cell>ùëÖ</cell><cell>ùêπ 1</cell><cell cols="4">ùê∏ùëÖùê∑ùê∏ 5 ùê∏ùëÖùê∑ùê∏ 50 ùëôùëéùë°ùëíùëõùëêùë¶ ùë°ùëù ùë†ùëùùëíùëíùëë ùëôùëéùë°ùëíùëõùëêùë¶ ùë§ ùêπ 1</cell></row><row><cell>0</cell><cell cols="3">0.115 1.000 0.206</cell><cell>0.029</cell><cell>0.021</cell><cell>1.0</cell><cell>1.000</cell><cell>0.206</cell></row><row><cell>1</cell><cell cols="3">0.124 1.000 0.221</cell><cell>0.028</cell><cell>0.020</cell><cell>2.0</cell><cell>0.996</cell><cell>0.220</cell></row><row><cell>2</cell><cell cols="3">0.108 1.000 0.195</cell><cell>0.030</cell><cell>0.022</cell><cell>1.0</cell><cell>1.000</cell><cell>0.195</cell></row><row><cell>3</cell><cell cols="3">0.126 1.000 0.224</cell><cell>0.029</cell><cell>0.020</cell><cell>2.0</cell><cell>0.996</cell><cell>0.223</cell></row><row><cell>4</cell><cell cols="3">0.092 0.981 0.168</cell><cell>0.044</cell><cell>0.027</cell><cell>3.0</cell><cell>0.992</cell><cell>0.166</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,660.08,75.48,8.97"><p>https://sinai.ujaen.es</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,671.04,88.49,8.97"><p>https://precom.ujaen.es/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,92.57,671.03,111.94,8.97"><p>https://pypi.org/project/emoji/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,92.57,671.02,69.48,8.97"><p>https://optuna.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by <rs type="projectName">WeLee</rs> project (<rs type="grantNumber">1380939</rs>, <rs type="funder">FEDER</rs> <rs type="grantNumber">Andaluc√≠a 2014-2020</rs>) funded by the <rs type="funder">Andalusian Regional Government</rs>, and projects <rs type="projectName">CONSENSO</rs> (<rs type="grantNumber">PID2021-122263OB-C21</rs>), <rs type="projectName">MODERATES</rs> (<rs type="grantNumber">TED2021-130145B-I00</rs>), <rs type="projectName">SocialTOX</rs> (<rs type="grantNumber">PDC2022-133146-C21</rs>) funded by <rs type="funder">Plan Nacional I+D+i</rs> from the <rs type="funder">Spanish Government</rs>, and project <rs type="projectName">PRECOM</rs> (<rs type="grantNumber">SUBV-00016</rs>) funded by the <rs type="funder">Ministry of Consumer Affairs of the Spanish Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_HPeRhde">
					<idno type="grant-number">1380939</idno>
					<orgName type="project" subtype="full">WeLee</orgName>
				</org>
				<org type="funded-project" xml:id="_TN8fzH8">
					<idno type="grant-number">Andaluc√≠a 2014-2020</idno>
					<orgName type="project" subtype="full">CONSENSO</orgName>
				</org>
				<org type="funded-project" xml:id="_EyEADkq">
					<idno type="grant-number">PID2021-122263OB-C21</idno>
					<orgName type="project" subtype="full">MODERATES</orgName>
				</org>
				<org type="funded-project" xml:id="_c32MDWV">
					<idno type="grant-number">TED2021-130145B-I00</idno>
					<orgName type="project" subtype="full">SocialTOX</orgName>
				</org>
				<org type="funding" xml:id="_AzhWZgx">
					<idno type="grant-number">PDC2022-133146-C21</idno>
				</org>
				<org type="funded-project" xml:id="_5T6SBCb">
					<idno type="grant-number">SUBV-00016</idno>
					<orgName type="project" subtype="full">PRECOM</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.59,404.67,398.40,10.91;8,107.59,418.22,399.60,10.91;8,107.59,431.77,398.40,10.91;8,107.59,445.32,289.43,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,347.60,404.67,21.45,10.91;8,397.55,404.67,108.44,10.91;8,107.59,418.22,177.04,10.91">Depression, pathological gambling, and eating disorder challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.00,431.77,113.99,10.91;8,107.59,445.32,38.01,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
	<note>erisk</note>
</biblStruct>

<biblStruct coords="8,107.59,458.87,398.40,10.91;8,107.59,472.42,400.07,10.91;8,107.59,485.97,399.60,10.91;8,107.59,499.52,274.59,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,308.95,458.87,197.03,10.91;8,107.59,472.42,63.80,10.91">Overview of eRisk 2023: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,198.93,472.42,304.53,10.91;8,107.59,485.97,394.97,10.91">Proceedings of the 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting>the 14th International Conference of the CLEF Association, CLEF 2023<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,107.59,513.06,399.59,10.91;8,107.59,526.61,399.60,10.91;8,107.59,540.16,393.49,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,237.68,526.61,265.45,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzm√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1911.02116" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,553.71,400.25,10.91;8,107.59,567.26,398.40,10.91;8,107.26,580.81,196.41,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<title level="m" coord="8,136.34,567.26,263.59,10.91">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,594.36,399.60,10.91;8,107.59,607.91,398.68,10.91;8,107.59,621.46,373.64,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>M√°rmol-Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Jim√©nez-Zafra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del Arco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Molina-Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Mart√≠n-Valdivia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montejo-R√°ez</surname></persName>
		</author>
		<title level="m" coord="8,298.60,607.91,207.67,10.91;8,107.59,621.46,341.72,10.91">Sinai at erisk@ clef 2022: Approaching early detection of gambling and eating disorders with natural language processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,635.01,399.60,10.91;8,107.59,648.56,399.60,10.91;8,107.41,662.11,258.91,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,316.51,648.56,185.67,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,107.41,662.11,164.82,10.91">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,86.97,399.60,10.91;9,107.59,100.52,400.08,10.91;9,107.59,114.06,398.40,10.91;9,107.59,127.61,398.40,10.91;9,107.59,141.16,398.40,10.91;9,107.59,154.71,400.08,10.91;9,107.59,168.26,197.48,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,335.20,114.06,170.79,10.91;9,107.59,127.61,90.74,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="9,224.75,127.61,281.24,10.91;9,107.59,141.16,398.40,10.91;9,107.59,154.71,47.51,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,181.81,399.60,10.91;9,107.59,195.36,399.60,10.91;9,107.59,208.91,399.60,10.91;9,107.59,222.46,398.40,10.91;9,107.59,236.01,399.11,10.91;9,107.59,249.56,385.97,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,370.22,208.91,136.97,10.91;9,107.59,222.46,177.74,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="9,308.13,222.46,197.86,10.91;9,107.59,236.01,37.09,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
