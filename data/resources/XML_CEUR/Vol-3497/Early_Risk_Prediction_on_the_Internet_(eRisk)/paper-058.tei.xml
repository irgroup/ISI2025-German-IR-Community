<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,369.51,15.42;1,88.69,106.66,359.87,15.42;1,89.29,128.58,123.83,15.43;1,213.12,125.54,5.85,10.48;1,89.29,150.91,200.45,11.96">Representation Exploration and Deep Learning Applied to the Early Detection of Pathological Gambling Risks ⋆ Notebook for the eRisk Lab at CLEF 2023</title>
				<funder ref="#_quQTYw9">
					<orgName type="full">European Comission (FEDER)</orgName>
				</funder>
				<funder ref="#_KHX5Yhn">
					<orgName type="full">Misiones Euskampus</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union &quot;NextGenerationEU&quot;/PRTR</orgName>
				</funder>
				<funder ref="#_W9Xh4x3">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_sty2VB5">
					<orgName type="full">Basque Government</orgName>
				</funder>
				<funder ref="#_jR3N7b6">
					<orgName type="full">EXTEPA</orgName>
				</funder>
				<funder ref="#_F8teW2m #_6Gu4hWA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,176.82,78.38,11.96"><forename type="first">Xabier</forename><surname>Larrayoz</surname></persName>
							<email>xlarrayoz001@ikasle.ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
								<address>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.44,176.82,65.80,11.96"><forename type="first">Nuria</forename><surname>Lebeña</surname></persName>
							<email>nuria.lebena@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
								<address>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.87,176.82,79.53,11.96"><forename type="first">Arantza</forename><surname>Casillas</surname></persName>
							<email>arantza.casillas@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
								<address>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.39,176.82,58.09,11.96"><forename type="first">Alicia</forename><surname>Pérez</surname></persName>
							<email>alicia.perez@ehu.eus</email>
							<affiliation key="aff0">
								<orgName type="department">HiTZ Center -Ixa</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
								<address>
									<postCode>20080</postCode>
									<settlement>Donostia</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,369.51,15.42;1,88.69,106.66,359.87,15.42;1,89.29,128.58,123.83,15.43;1,213.12,125.54,5.85,10.48;1,89.29,150.91,200.45,11.96">Representation Exploration and Deep Learning Applied to the Early Detection of Pathological Gambling Risks ⋆ Notebook for the eRisk Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">DC6127781283E155599F63252F2D08DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Early risk prediction</term>
					<term>Natural Language Processing</term>
					<term>Class imbalance</term>
					<term>Deep learning</term>
					<term>Mental health</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of Task 2 (Early Detection of Signs of Pathological Gambling) from the CLEF 2023 eRisk Workshop is to analyze social media users' messages for early warning signs of pathological gambling. Given that Pathological Gamblers are a small set compared to the Control group, we propose the utilization of a neural network incorporating a customized loss function to effectively tackle the challenge of class imbalance. In our proposed loss function it is possible to adjust the penalty for false positives and false negatives, increasing the penalty for the critical false negatives. Our proposed solution demonstrates robustness, achieving one of the highest recall rates while maintaining a competitive precision. Furthermore, our system introduces a range of potential variations that warrant further investigation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The different editions of the eRisk workshop serve as a meeting point where different methodologies and practical approaches for early detection of different types of health risks are proposed. The various tasks focus on the textual analysis of posts and messages from social media users. Social media plays a crucial role in the early diagnosis of mental illnesses and other health problems. Many people use social media regularly posting their inner thoughts and daily experiences, sometimes showing different symptoms and signs of mental illnesses and therefore being indicative of potential mental disorders. Given that in many cases mental health is still perceived as taboo in our society, many patients don't receive the necessary medical care, as a consequence, social media emerges as a good data source to analyze mental illnesses and be able to help potential patients.</p><p>In this article, we present a system to address Task 2 of the eRisk Workshop 2023: Early Detection of Signs of Gambling Addiction <ref type="bibr" coords="1,288.86,570.47,11.59,10.91" target="#b0">[1]</ref>. The approach is based first on generating vector representations of user messages through sentence embedding, and then on detecting positive messages using deep learning-based methods. Additionally, an original loss function is introduced to deal with imbalanced classes or cases where false negatives have a significant impact.</p><p>The rest of the article is structured as follows: Section 2 provides an overview of previous work related to the task considered and the techniques used in this study. Section 3 describes the tackled task, including the available dataset and evaluation metrics, while the developed system is presented in Section 4. Section 5 demonstrates the configuration of the variants used for the competition. The results obtained are compared with those of other participating systems and discussed in Section 6. Finally, Section 7 presents the main conclusions and future lines of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In recent years, the use of AI to address mental health-related tasks has gained greater presence. AI, and in particular NLP, has proven to be a powerful tool in the detection of mental disorders. In previous studies, NLP has been used on electronic health records to assist in the identification of suicidal behaviors <ref type="bibr" coords="2,183.77,308.18,11.36,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,197.86,308.18,7.57,10.91" target="#b2">3]</ref>, achieving an accuracy of 0.47.</p><p>In this context, most of the methods used for the detection of mental disorders are based on traditional Machine Learning techniques, such as SVM, AdaBoost or Decision Trees. The recent interest in Deep Learning has shown a better performance <ref type="bibr" coords="2,356.38,348.83,11.57,10.91" target="#b3">[4]</ref>. However, as stated by Zhang et al. <ref type="bibr" coords="2,113.24,362.38,12.84,10.91" target="#b3">[4]</ref> a large part of the solutions proposed are concentrated on a few mental disorders. As one of the most prevalent disorders in the world, the absence of research on the identification of gambling addiction stands out.</p><p>The SMM4H <ref type="bibr" coords="2,160.34,403.03,11.54,10.91" target="#b4">[5]</ref>, CLEF and CLPsych <ref type="bibr" coords="2,267.42,403.03,12.96,10.91" target="#b5">[6]</ref> competitions focus on the application of machine learning techniques in the field of mental health. In 2022, the winner of the CLPsych competition achieved 68.9% accuracy in detecting mood changes in tweets. Years earlier, in 2019, a similar rate was recorded in identifying users at risk of suicide based on their messages.</p><p>For several years, one of the tasks to be addressed in CLEF eRisk has been the early detection of pathological gambling risks. By sequentially processing user interactions on social networks, the system was to detect the first signs of pathological gambling as early as possible.</p><p>Very different approaches were used in the previous edition. The BLUE group, proposed to train a BERT classifier, using an additional dataset generated from some Reddit mental health communities. The UNED-NLP <ref type="bibr" coords="2,233.82,524.97,12.99,10.91" target="#b6">[7]</ref> team, participated with a system that relied on Approx Nearest Neighbors techniques to detect positive messages. The SINAI <ref type="bibr" coords="2,401.07,538.52,12.82,10.91" target="#b7">[8]</ref> group came up with a design based on language features. Using the last 50 user messages, a vector was obtained which was complemented with message features such as number of words, lexical diversity and sentence complexity. Finally it was passed through a feed-forward neural network (FFNN) model. Similarly, in our work we propose a FFNN feed by a semantic representation of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>The dataset provided in this edition, is the combination of the data used in the two previous editions (eRisk 2021 and 2022). It is composed of a number of XML files, each of which contains a number of posts made by 4,427 users of social networks. It includes 2,298,412 messages overall. Labels are user level and they design them as either gamblers (1) or control users (0).</p><p>In Figure <ref type="figure" coords="3,140.90,114.06,3.66,10.91" target="#fig_0">1</ref>, it is evident that both editions exhibit a significantly imbalanced dataset, indicating a non-uniform data distribution. Specifically, the proportion of players classified as gambling addicts in their respective sets does not exceed 7% or 4%. Previous research in supervised classification has highlighted class imbalance, referred to as class skew or class imbalance, as a critical factor that significantly impedes the capacity of inference algorithms to learn and accurately generalize the minority class. In order to enhance the message quality during the training process, a filtering approach was employed. Specifically, users with fewer than 10 messages and the top 20 users with the highest presence in the dataset were excluded. This strategic exclusion aimed to mitigate the potential noise introduced by these users, thus improving the overall training efficacy.</p><p>The test set officially employed to assess the challenge, described in Table <ref type="table" coords="3,437.40,494.42,3.81,10.91" target="#tab_0">1</ref>, is sent to the participants iteratively through a connection to a server. The total number of users is 2174, of which 103 are compulsive gamblers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>Given a sequence of consecutive 𝑙 posts post messages written by subject 𝑘, denoted as t 𝑘 = (𝑡 1 𝑘 , 𝑡 2 𝑘 , . . . , 𝑡 𝑙 𝑘 ) with 𝑡 𝑗 𝑘 being the 𝑗-th post (i.e. text) in the succession, the aim is to get a subjectlevel classification label (𝑢 ^𝑘) to distinguish Pathological Gamblers from Control subjects. To that end, in our approach each post (𝑡 𝑗 𝑘 ) is processed and a post-level label (𝑐 𝑗 𝑘 ) computed by means of the architecture presented in Figure <ref type="figure" coords="4,299.34,165.48,3.81,10.91" target="#fig_1">2</ref>. With this information is attained, next, the user-level label. The processes involved and training strategies applied are detailed in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text pre-processing</head><p>Each of the posts has two parts, a title and a body. We combined both parts to create a single message. We conducted the following steps in order to preprocess the posts:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Conversion to lowercase • Characters cleaning • Stopwords removal</head><p>Lemmatization and stemming were also incorporated prior to LDA in order to improve the generated topics. Table <ref type="table" coords="5,197.31,100.52,5.17,10.91" target="#tab_1">2</ref> shows the versions of a text after applying different preprocessing techniques. The last version would be necessary in the case of applying LDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Post-level vector representation</head><p>We need to get each text-post, 𝑡 𝑗 𝑘 , converted into a fixed-size numeric vector, x 𝑗 𝑘 = (𝑥 𝑗 𝑘1 , 𝑥 𝑗 𝑘2 , . . . , 𝑥 𝑗 𝑘𝑁 ) ∈ R 𝑁 that would serve as the input to the FFNN. Note that 𝑡 𝑗 𝑘 ∈ Σ * being Σ the input vocabulary. Two main strategies were explored in order to get a numeric representation (x 𝑗 𝑘 ) given a post (𝑡 𝑗 𝑘 ): encoding and LDA. We can either use just one strategy or both and make use of the concatenated representation leading, thus, to a longer vector representation, as in <ref type="bibr" coords="5,111.44,325.25,10.45,10.91" target="#b0">(1)</ref>, in which the vectorization by means of the encoder led to an 𝑁 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 = 𝑛-dimensional array, 𝑣 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝑡 </p><p>In what follows, details are given of each strategy explored to obtain the representation, the encoder in section 4.2.1 and LDA in section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Post-level encoder</head><p>We wondered whether Dynamic Aggregation of Network (DAN) or Sentence-BERT (SBERT) would generate a better semantic representation of the texts. Models such as the Universal Sentence Encoder (USE) <ref type="bibr" coords="5,200.12,561.19,11.50,10.91" target="#b8">[9]</ref>, Sentence-BERT (SBERT) <ref type="bibr" coords="5,330.74,561.19,17.98,10.91" target="#b9">[10]</ref> and Transformer-based Pretrained Language Models (PML), allow a complex and global representation taking into account word interactions and relations. SBERT is a variation of the traditional BERT <ref type="bibr" coords="5,409.48,588.29,17.94,10.91" target="#b10">[11]</ref> model that incorporates Siamese and triplet lattice structures. Such structures enable learning the similarities and contrasts between various inputs. USE, on the other hand, bases its architecture on convolutional and recurrent neural networks. There are two USE variants, the most widely used is based on transformers. Nevertheless the variant known as the Dynamic Aggregation of Network (DAN) that makes use of the dynamic aggregation of networks approach to enhance the outcome has shorter computing time.</p><p>We represented the posts using DAN and SBERT in order to compare their performance. The vector of size (𝑁 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 ) generated by DAN encoder is 512 while SBERT works with a vector of size 384.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Topic modeling</head><p>Latent Dirichlet Allocation (LDA) is a probabilistic model able to identify the latent topics in the posts. It enables to extract the topic distribution of each post and we used it as additional features to represent the post. We configured LDA to extract 20 topics from the posts, leading to a representation x 𝑗 𝑘 ∈ R 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Post-level classification</head><p>The estimated post-label-confidence 𝑦 𝑗 𝑘 ∈ R is an intermediate scalar obtained in our system interpreted as the confidence score that the post 𝑡 𝑗 𝑘 contains traits of language related to Pathological Gambling. When 𝑦 𝑗 𝑘 takes positive values, the intermediate post-level label 𝑐 𝑗 𝑘 is assigned to class 1; otherwise, it is assigned to class 0, that is, 𝑔(𝑧) = 𝑠𝑖𝑔𝑛(𝑧).</p><p>Note that the 𝑓 (•) transformation is attained by means of the FFNN network. These processes are formally summarized in (2) and graphically depicted in Figure <ref type="figure" coords="6,384.39,324.61,3.74,10.91" target="#fig_1">2</ref>.</p><formula xml:id="formula_1" coords="6,186.07,347.41,319.92,29.11">𝑔 ∘ 𝑓 : R 𝑁 -→ R -→ {0, 1} x 𝑗 𝑘 -→ 𝑓 (x 𝑗 𝑘 ) = 𝑦 𝑗 𝑘 -→ 𝑔(𝑦 𝑗 𝑘 ) = 𝑐 𝑗 𝑘<label>(2)</label></formula><p>With regard to the practical details of the FFNN network, we would like to mention that it includes two layers and contains a total of 49538 to 65922 parameters, depending on the encoder employed, respectively, SBERT or DAN. If LDA is used, there are 2560 additional parameters.</p><p>In what the training process is concerned, the FFNN was trained on 5 epochs and setting the learning rate to 5 × 10 -5 . Besides, in the training stage the dropout was set to 0.2 and AdamW optimizer was employed as it was proven effective <ref type="bibr" coords="6,322.16,463.76,16.41,10.91" target="#b11">[12]</ref>. An iterative training approach was implemented where, in each epoch, all the messages from a user are sequentially processed to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Subject-level classification</head><p>User gold label 𝑢 𝑘 is the user label (either 'Control' or 'Pathological Gamblers') for 𝑘-th subject as in the gold-standard, that is, the expected label for the subject. With the information attained at post-level (as stated in section 4.3), the user-level label is estimated (̂︁ 𝑢 𝑘 ). The performance of the system is, indeed, assessed based on the difference between predicted (̂︁ 𝑢 𝑘 ) and expected (𝑢 𝑘 ) subject-level labels.</p><p>Note, however, that there is a subtlety in the arrangement of the task: not all the post of the user 𝑘 are given jointly, instead, the posts are presented to the system in sequentially, one by one in their turn. That is, for user 𝑘 in the time-stamp 1 we merely count on the post 𝑡 1 𝑘 , while by the 𝑙-th time-stamp we would have seen a sequence of 𝑙 posts, (𝑡 </p><formula xml:id="formula_2" coords="7,153.56,240.85,352.42,30.57">ℎ : Σ * × (Σ * ) 𝑙 × R 𝑙 × {0, 1} 𝑙 -→ {0, 1} (𝑡 𝑙+1 𝑘 , t 𝑘 , y 𝑘 , ̂︀ c 𝑘 ) -→ ℎ(𝑡 𝑙+1 𝑘 |t 𝑘 , y 𝑘 , ̂︀ c 𝑘 ) = ̂︁ 𝑢 𝑘 𝑙+1<label>(3)</label></formula><p>In our approach, however, the user-level label ̂︀ 𝑢 𝑙+1 𝑘 is estimated as in (4). That is, we computed the user-level label relying merely on the current post-level label and disregarding the historic information. Needless to say, future efforts could be devoted to leverage ℎ(•) exploiting all the information available.</p><formula xml:id="formula_3" coords="7,184.37,345.70,321.62,17.02">︁ 𝑢 𝑘 𝑙+1 = ℎ(𝑡 𝑙+1 𝑘 |t 𝑘 , y 𝑘 , ̂︀ c 𝑘 ) = 𝑔(𝑓 (𝑣(𝑡 𝑙+1 𝑘 ))) = ̂︀ 𝑐 𝑙+1 𝑘 (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Silver-standards explored as post-level reference</head><p>Since gold-labels are provided at user level and we trained our FFNN using posts, we needed to get the posts labeled for the training. That is, in the training stage the estimated post-label confidence (𝑦 𝑗 𝑘 ) must be compared to a desired or expected confidence (y ′ 𝑘 ) the underlying issue rests on the fact that the post-level confidence is not given. This silver standard is shown in Figure <ref type="figure" coords="7,120.10,451.92,5.03,10.91" target="#fig_2">3</ref> As the reference post-label confidence, y ′ 𝑘 = (𝑦 ′ 1 𝑘 , . . . , 𝑦 ′ 𝑙 𝑘 ), in this work we explored two alternative silver-standard assignation strategies:</p><p>• User-based message labeling (UBL): Consists of assigning each post the label of the user. That is, if a user is positive all it's posts will be labeled as positive, that is, all the components in this array, 𝑦 ′ 𝑙 𝑘 , equal to 𝑢 𝑘 . • Approximated Nearest Neighbors (ANN): Posts are labeled using an iterative labeling approach. First, each post is labeled with the label of the user. Then using ANN technique, the labels are reassigned, giving to the closer posts the same labels, getting y ′ 𝑘 as in <ref type="bibr" coords="7,492.44,560.07,11.42,10.91" target="#b6">[7]</ref>.</p><p>The heuristic post-level references employed have a deep impact in the training stage and, needless to say, should be selected carefully. Future work can be addressed in alternative reference assignment strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Loss function across post-level labels to improve user-level label</head><p>As mentioned in section 3, the data, far from being uniform, exhibit a noticeable imbalance. As a consequence, the neural network can become biased and achieve low precision in the minority class. To address this problem, there are various strategies to alter the class proportion through over-sampling and under-sampling. Following an approach closer to assigning class weights, we have chosen to apply a loss function based on cross-entropy during the training of the neural network. The strategy followed in our work is sketched in Figure <ref type="figure" coords="8,430.76,175.28,3.74,10.91" target="#fig_2">3</ref>. With the sequence of posts from user 𝑘 a sequence of confidence scores is computed by the model, post by post and y 𝑘 obtained. With this, as mentioned in section 4.3, a sequence of post-level labels are computed, that is, ̂︀ c 𝑘 . In the training stage, the system estimates a user-level label taking into account all the post-level labels, as in <ref type="bibr" coords="8,336.30,458.31,10.51,10.91" target="#b4">(5)</ref>, meaning that a positive post-level label in the sequence suffices to classify the user as positive. In the training stage, the user-level label is estimated comprising all the posts-level labels from the user. The estimated user-level label can be compared to the ground-truth provided (𝑢 𝑘 ) to update the model in the training stage.</p><formula xml:id="formula_4" coords="8,203.84,535.23,302.15,30.47">̂︁ 𝑢 𝑘 = {︃ 1, if ∃𝑖 1 ≤ 𝑖 ≤ 𝑙 : 𝑐 𝑖 𝑘 = 1 0, otherwise<label>(5)</label></formula><p>In the training stage, the sequence of labels computed (y 𝑘 ) are compared with the silverlabels proposed as reference (that is, the y ′ 𝑘 labels presented in section 4.5). This comparison is quantitatively seized as the loss by means of the Cross Entropy Loss function, 𝐻(y ′ 𝑘 , y 𝑘 ), as implemented in PyTorch <ref type="bibr" coords="8,200.63,615.55,16.18,10.91" target="#b11">[12]</ref>. The user-dependant loss is seized by a weight factor that allows for a penalty as in <ref type="bibr" coords="8,172.42,629.09,10.48,10.91" target="#b5">(6)</ref>.</p><formula xml:id="formula_5" coords="8,231.56,640.88,274.43,14.15">𝐿𝑜𝑠𝑠 𝑘 = 𝑊 𝑢𝑠𝑒𝑟 𝑘 • 𝐻(y ′ 𝑘 , y 𝑘 )<label>(6)</label></formula><p>Our training approach does not penalize equally false positives and false negatives, indeed the weight factor employed by our team is given in <ref type="bibr" coords="9,302.49,86.97,10.48,10.91" target="#b6">(7)</ref>.</p><formula xml:id="formula_6" coords="9,218.81,110.72,287.18,49.40">𝑊 𝑢𝑠𝑒𝑟 𝑘 = ⎧ ⎪ ⎨ ⎪ ⎩ 4 if ̂︁ 𝑢 𝑘 = 0 ∧ 𝑢 𝑘 = 1 2 if ̂︁ 𝑢 𝑘 = 1 ∧ 𝑢 𝑘 = 0 1 if ̂︁ 𝑢 𝑘 = 𝑢 𝑘<label>(7)</label></formula><p>In the experiments, a penalty of 2 was used for false positives and 4 for false negatives. The critical false negatives were penalized by doubling the loss in those cases where the system incorrectly predicted a positive instance. These values (1, 2 and 4) have been determined through a sensitivity analysis and based on the objective of prioritizing false positives over false negatives in an attempt not to miss Pathological Gamblers.</p><p>There is room for improvement in the training stage. On the one hand, the user-level labeling strategy, i.e. ( <ref type="formula" coords="9,149.89,251.17,3.50,10.91" target="#formula_4">5</ref>), for instance, could be computed taking the time-stamp into account and not just the sequence of post-labels, however, the function proposed is computationally cheap and suited to tackle class-imbalance. On the other hand, the loss function and the penalty weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Practical details</head><p>Preliminary experiments were conducted combining all the variants mentioned in the methodological framework, varying: the post level vector representation (involving SBERT, DAN and LDA as mentioned in section 4.2); alternative silver-standard approaches to get a post-level ground-truth (UBL and ANN as detailed in section 4.5); the data-set partition employed to train the system (exploring the 2021 or 2022 and sets as mentioned in section 3). user-penalty weights in the training stage (developed in section 4.6). These preliminary experiments were conducted on a partition of the provided data split by year and also combined (see Figure <ref type="figure" coords="9,496.73,439.16,3.65,10.91" target="#fig_0">1</ref>). These experiments led us to select the parametrization e.g. the penalty weights presented in expression 7 and the same modified loss function. We found that DAN outperformed SBERT and this is why we, eventually, discarded that encoding.</p><p>As a result, in our team, a total of 5 runs were submitted with the configurations detailed in Table <ref type="table" coords="9,114.76,506.90,3.66,10.91" target="#tab_4">3</ref>. Given the limitation on the number of runs to submit, greater diversity in configurations was prioritized over variants using SBERT. Additionally, a combined model was used, which utilizes all the mentioned variants to create a single result. The decision made by the combined model is determined through an OR operation. Accordingly, it is, both necessary and sufficient, for one of the variants to estimate a positive user-level label for this combined approach to estimate positive. T he motivation behind this combined approach is to leverage the Recall even at the expense of certain false positives, which are preferable to false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>In Table <ref type="table" coords="9,126.36,637.93,3.66,10.91">4</ref>, we can see some of the best results from the competition in the main task. This allows us to compare our proposed system, Xabi_EHU, with other submissions. Among all the system variations, the first configuration has achieved competitive scores in all metrics, with a strong dominance in Recall. In an environment where false negatives have a significant impact, having a high Recall is of great interest in real-world applications, compensating for small differences in other measures. On the other hand, as seen during development, the use of LDA has resulted in a general performance loss for the system, while training based on more precise labeling has led to a decrease in the system's precision. Overall, these results suggest that there are multiple effective approaches to addressing the problem. Note that, while somehow simplistic, the UBL approach resulted in sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Decision-based evaluation for Task 2. Our Team is denoted as Xabi_EHU and the details of each run were presented in Table <ref type="table" coords="10,188.44,382.50,3.41,8.87" target="#tab_4">3</ref>. Regarding the additional task of user ranking, Table <ref type="table" coords="10,342.25,606.34,5.17,10.91">5</ref> shows the performance of the top teams in that task. As can be seen, prioritizing better Recall has led to inferior performance compared to other teams. Without additional information, using the class probability itself as the user's risk level may not be a good indicator for what the competition intended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Ranking-based evaluation for Task 2. Our team is denoted as Xabi_EHU.</p><p>1 writing 100 writing 500 writing 1000 writing Team Run P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 Xabi_EHU 0 1.00 1.00 0.57 1.00 1.00 0.50 0.80 0.88 0.41 0.90 0.94 0.41 OBSER-MENH 0 1.00 1.00 0.64 1.00 1.00 0.55 1.00 1.00 0.48 1.00 1.00 0.50 ELiRF-UPV 0 1.00 1.00 0.59 1.00 1.00 0.91 1.00 1.00 0.95 1.00 1.00 0.94 UNSL 1 1.00 1.00 0.57 1.00 1.00 0.78 1.00 1.00 0.67 1.00 1.00 0.70 NLP-UNED-2 3 1.00 1.00 0.59 1.00 1.00 0.92 1.00 1.00 0.95 1.00 1.00 0.93</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Our participation focuses on early detection of signs of pathological gambling. Given a sequence of posts in a sequence, one by one, the aim is to estimate, employing as fewer posts as possible, the subject-level label (either Control or Pathological Gambler). With the data-set highly skewed, and being the target Pathological Gambler the minority group, we struggled to find robust models and focused on maximizing the recall not to miss pathological gamblers. However, this led us to the creation of a specific loss function. The data imbalance has also prevented us from using ML-based architectures, as any attempts to generalize the data and achieve satisfactory performance had failed. This effect can be observed in the metric values separated by classes, where results close to 1 are obtained for the negative class, unlike the case of the positive class.</p><p>Our approach was designed with the aim to avoid over-fitting. Basically, the runs submitted employed DAN encoder and an FFNN. The training was enhanced by means of a loss function defined by user. The user-level label is estimated by means of a post-level label, and this strategy required us to figure out, heuristically, the post-level label reference to train the system. Data imbalance has been a challenge throughout the development process and, as expected, the model obtained better results for the majority class (Control users).</p><p>The proposed system has achieved competitive performance in the tasks of binary classification and ranking-based classification.</p><p>Since the model development has focused exclusively on the main task of binary classification, the performance on the ranking task is lower.</p><p>Needless to say, there is room for improvement in the proposed approach. We feel motivated to keep exploring variants to improve the user-level label estimation employed all the pieces of information available at each stage re-defining (3) not in the simple manner we did in <ref type="bibr" coords="11,463.36,569.06,10.44,10.91" target="#b3">(4)</ref>. There are other core-issues to bear in mind, such as the definition of a reference for the post-level confidence that are worth exploring. In any case, the approach proposed is versatile. This same design can be extrapolated to other mental disorders, even with texts in other languages, using the corresponding encoder. The weights of the modified loss function will vary according to the class balance, but with an unaltered loss function, the model could be competitive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,398.62,416.69,8.93;3,89.29,410.62,226.03,8.87;3,110.13,204.52,375.02,187.51"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Class distribution of the training sets made available in eRisk 2021 and 2022. Pos label stands for Pathological Gambler and Neg for Control subjects.</figDesc><graphic coords="3,110.13,204.52,375.02,187.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,479.19,399.07,9.15;4,488.36,476.44,3.30,6.12;4,488.36,484.24,4.24,6.12;4,493.26,479.46,12.72,8.87;4,89.29,492.86,136.92,8.87;4,226.22,489.85,3.30,6.12;4,226.22,497.64,8.37,6.12;4,235.09,492.59,27.83,8.74;4,262.92,489.85,3.30,6.12;4,262.92,497.64,10.71,6.12;4,274.88,492.86,231.10,8.87;4,89.29,506.27,341.60,8.87;4,430.90,503.25,3.30,6.12;4,430.90,511.04,4.24,6.12;4,435.80,506.27,70.19,8.87;4,89.29,518.22,42.55,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Design of the model for the binary post-level classification. The post 𝑗 of a user 𝑘 (𝑡 𝑗 𝑘 ) is represented as a numeric array (𝑥 𝑗 𝑘1 , . . . , 𝑥 𝑗 𝑘𝑁 ). The input of the FFNN network is the concatenation of the vector generated by the Encoder and the LDA model. The output of the FFNN (𝑐 𝑗 𝑘 ) is the estimated post-label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,376.06,416.70,8.93;8,89.29,388.06,318.78,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System training approach. Cross Entropy is employed to compute the loss function and update the model for a given user. A zoom in the model was given in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,550.52,416.99,109.09"><head>Table 1</head><label>1</label><figDesc>Task 2 Pathological gambling. Main statistics of test collection for task 2 early detection of signs of pathological gambling</figDesc><table coords="3,323.78,590.52,138.54,8.96"><row><cell>Pathological Gamblers</cell><cell>Control</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,142.46,416.99,59.20"><head>Table 2</head><label>2</label><figDesc>Comparison of the original text and the preprocessed versions, where Preprocessing refers to applying the transformations mentioned in the previous list, and Lem. &amp; Stem. generates the base form.</figDesc><table coords="5,95.10,182.26,405.08,19.39"><row><cell>Original</cell><cell>Preprocessing</cell><cell>Lemmatization &amp; stemming</cell></row><row><cell cols="2">Having no money; worst part of recovering money worst part recovering</cell><cell>money worst part recover</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.97,336.23,418.86,125.86"><head></head><label></label><figDesc>The text, represented as an array of fixed size with the dimension (𝑁 ) depending on the representation used (either encoding or LDA or both). The resulting vector representation x 𝑗 𝑘 ∈ R 𝑁 is, indeed, the input for the classifier.</figDesc><table /><note coords="5,159.97,336.23,3.42,6.99;5,159.97,345.06,4.41,6.99;5,165.09,339.53,29.26,9.57;5,194.36,336.23,3.42,6.99;5,194.36,345.06,11.21,6.99;5,206.06,339.53,11.09,9.57;5,217.15,336.23,3.42,6.99;5,217.15,345.06,11.21,6.99;5,228.85,339.53,30.47,9.57;5,259.33,336.23,3.42,6.99;5,259.33,345.06,12.11,6.99;5,271.94,338.80,235.89,11.42;5,89.29,355.45,88.90,11.42;5,178.19,352.88,3.42,6.99;5,178.19,361.71,4.41,6.99;5,183.31,356.18,29.26,9.57;5,212.58,352.88,3.42,6.99;5,212.58,361.71,22.93,6.99;5,236.01,356.18,11.09,9.57;5,247.09,352.88,3.42,6.99;5,247.09,361.71,22.93,6.99;5,270.52,356.18,30.47,9.57;5,301.00,352.88,3.42,6.99;5,301.00,361.71,13.70,6.99;5,316.04,356.18,6.64,9.57;5,129.74,433.04,22.65,9.57;5,152.39,430.54,4.23,6.99;5,167.09,433.04,35.42,10.18;5,202.51,430.54,6.72,6.99;5,148.07,449.57,3.94,9.57;5,152.01,446.27,3.42,6.99;5,152.01,455.10,4.41,6.99;5,167.09,449.57,41.40,9.57;5,208.49,446.27,3.42,6.99;5,208.49,455.10,4.41,6.99;5,213.61,449.57,29.26,9.57;5,242.87,446.27,3.42,6.99;5,242.87,455.10,11.21,6.99;5,254.58,449.57,11.09,9.57;5,265.66,446.27,3.42,6.99;5,265.66,455.10,11.21,6.99;5,277.37,449.57,30.47,9.57;5,307.85,446.27,3.42,6.99;5,307.85,455.10,12.11,6.99;5,320.46,449.57,11.09,9.57;5,331.54,446.27,3.42,6.99;5,331.54,455.10,22.93,6.99;5,354.97,449.57,11.09,9.57;5,366.05,446.27,3.42,6.99;5,366.05,455.10,22.93,6.99;5,389.48,449.57,30.47,9.57;5,419.96,446.27,3.42,6.99;5,419.96,455.10,13.70,6.99;5,435.00,449.54,25.41,9.60;5,460.41,446.27,3.42,6.99;5,460.41,455.10,4.41,6.99"><p>𝑗 𝑘 ) = (𝑥 𝑗 𝑘,1 , 𝑥 𝑗 𝑘,2 , . . . , 𝑥 𝑗 𝑘,𝑛 ) and the LDA yielded an 𝑁 𝐿𝐷𝐴 = 𝑁 -𝑛 + 1 dimensional array 𝑣 𝐿𝐷𝐴 (𝑡 𝑗 𝑘 ) = (𝑥 𝑗 𝑘,𝑛+1 , 𝑥 𝑗 𝑘,𝑛+2 , . . . , 𝑥 𝑗 𝑘,𝑁 ). 𝑣 : Σ * -→ R 𝑁 𝑡 𝑗 𝑘 -→ 𝑣(𝑡 𝑗 𝑘 ) = (𝑥 𝑗 𝑘,1 , 𝑥 𝑗 𝑘,2 , . . . , 𝑥 𝑗 𝑘,𝑛 , 𝑥 𝑗 𝑘,𝑛+1 , 𝑥 𝑗 𝑘,𝑛+2 , . . . , 𝑥 𝑗 𝑘,𝑁 ) = x 𝑗 𝑘</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,90.49,417.17,135.01"><head>Table 3</head><label>3</label><figDesc>Submitted Runs: Description of the configurations explored. The second column refers to the encoding strategy (explained in section 4.2.1), LDA refers to the incorporation of LDA in the post-level vector representation (as in 4.2.2), Label indicates the labeling used in training (Sec. 4.5), and finally the edition of the training set used (Sec. 3).</figDesc><table coords="10,160.75,154.39,271.28,71.10"><row><cell cols="4">Run DAN vs SBERT LDA involved Label</cell><cell>Train</cell></row><row><cell>0</cell><cell>DAN</cell><cell>No</cell><cell>UBL</cell><cell>2021</cell></row><row><cell>1</cell><cell>DAN</cell><cell>Yes</cell><cell>UBL</cell><cell>2021</cell></row><row><cell>2</cell><cell>DAN</cell><cell>No</cell><cell>ANN</cell><cell>2021</cell></row><row><cell>3</cell><cell>DAN</cell><cell>No</cell><cell>UBL</cell><cell>2021 ∪ 2022</cell></row><row><cell>4</cell><cell></cell><cell>𝑂𝑅 3 𝑖=0 (𝑅𝑢𝑛 𝑖 )</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,113.44,396.35,365.90,193.59"><head>.961 0.900 0.030 0.012 8.0 0.973 0.875</head><label></label><figDesc></figDesc><table coords="10,113.44,396.35,365.90,193.59"><row><cell>Team</cell><cell>Run</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>𝐸𝑅𝐷𝐸 5</cell><cell>𝐸𝑅𝐷𝐸 50</cell><cell>latencyTP</cell><cell>speed</cell><cell>latency-weighted F1</cell></row><row><cell>ELiRF-UPV</cell><cell>0</cell><cell cols="8">1.000 0.883 0.938 0.026 0.010 4.0 0.988 0.927</cell></row><row><cell cols="10">Xabi_EHU 0.846 0Xabi_EHU 0 1 0.89 0.864 0.877 0.035 0.017 12.0 0.957 0.839</cell></row><row><cell>Xabi_EHU</cell><cell>2</cell><cell>0.79</cell><cell cols="7">0.913 0.847 0.036 0.015 13.0 0.953 0.807</cell></row><row><cell>Xabi_EHU</cell><cell>3</cell><cell cols="8">0.829 0.942 0.882 0.033 0.013 12.0 0.957 0.844</cell></row><row><cell>Xabi_EHU</cell><cell>4</cell><cell cols="8">0.756 0.961 0.846 0.031 0.013 8.00 0.973 0.823</cell></row><row><cell>UNSL</cell><cell>2</cell><cell cols="8">0.752 0.854 0.800 0.048 0.013 14.0 0.949 0.759</cell></row><row><cell>BioNLP-IISERB</cell><cell>0</cell><cell>0.933</cell><cell>0.68</cell><cell cols="6">0.787 0.038 0.037 62.0 0.766 0.603</cell></row><row><cell>NLP-UNED-2</cell><cell>1</cell><cell cols="8">0.957 0.883 0.919 0.034 0.016 13.0 0.953 0.876</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was elaborated within the framework of <rs type="projectName">LOTU</rs> (<rs type="grantNumber">TED2021-130398B-C22</rs>) funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033</rs>, <rs type="funder">European Comission (FEDER)</rs>, and by the <rs type="funder">European Union "NextGenerationEU"/PRTR</rs>. Besides, this work was partially funded by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> (<rs type="grantNumber">DOTT-HEALTH/PAT-MED PID2019-106942RB-C31</rs>); by the <rs type="funder">Basque Government</rs> (<rs type="grantNumber">IXA IT-1570-22</rs>, Predoctoral Grant <rs type="grantNumber">PRE-2022-1-0069</rs>) and <rs type="grantName">Ikasiker grants</rs> published in the <rs type="grantNumber">11/07/2022 BOPV</rs>; and by <rs type="funder">EXTEPA</rs> within <rs type="funder">Misiones Euskampus</rs> <rs type="grantNumber">2.0</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_F8teW2m">
					<idno type="grant-number">TED2021-130398B-C22</idno>
					<orgName type="project" subtype="full">LOTU</orgName>
				</org>
				<org type="funding" xml:id="_quQTYw9">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033</idno>
				</org>
				<org type="funding" xml:id="_W9Xh4x3">
					<idno type="grant-number">DOTT-HEALTH/PAT-MED PID2019-106942RB-C31</idno>
				</org>
				<org type="funding" xml:id="_sty2VB5">
					<idno type="grant-number">IXA IT-1570-22</idno>
				</org>
				<org type="funding" xml:id="_6Gu4hWA">
					<idno type="grant-number">PRE-2022-1-0069</idno>
					<orgName type="grant-name">Ikasiker grants</orgName>
				</org>
				<org type="funding" xml:id="_jR3N7b6">
					<idno type="grant-number">11/07/2022 BOPV</idno>
				</org>
				<org type="funding" xml:id="_KHX5Yhn">
					<idno type="grant-number">2.0</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,237.65,393.61,10.91;12,112.66,251.20,393.33,10.91;12,112.66,264.75,393.53,10.91;12,112.66,278.30,236.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,354.99,237.65,151.28,10.91;12,112.66,251.20,112.14,10.91">Overview of erisk 2023: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martín-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.41,251.20,258.58,10.91;12,112.66,264.75,348.13,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,291.85,393.33,10.91;12,112.66,305.40,393.33,10.91;12,112.66,318.95,395.01,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,447.65,291.85,58.34,10.91;12,112.66,305.40,393.33,10.91;12,112.66,318.95,263.35,10.91">Identification of suicidal behavior among psychiatrically hospitalized adolescents using natural language processing and machine learning of electronic health records</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,383.99,318.95,37.85,10.91">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">211116</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,332.50,393.33,10.91;12,112.66,346.05,393.32,10.91;12,112.66,359.59,393.32,10.91;12,112.66,373.14,147.88,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,448.12,332.50,57.86,10.91;12,112.66,346.05,393.32,10.91;12,112.66,359.59,393.32,10.91;12,112.66,373.14,92.94,10.91">Novel use of natural language processing (nlp) to predict suicidal ideation and psychiatric symptoms in a text-based mental health intervention in madrid, Computational and mathematical methods in medicine</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Progovac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Baca-Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,386.69,395.17,10.91;12,112.66,400.24,394.51,10.91;12,112.66,416.23,110.72,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,303.94,386.69,203.90,10.91;12,112.66,400.24,177.16,10.91">Natural language processing applied to mental illness detection: a narrative review</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schoene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-022-00589-7</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,300.89,400.24,96.87,10.91">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,427.34,394.53,10.91;12,112.34,440.89,394.84,10.91;12,112.66,454.44,393.33,10.91;12,112.66,467.99,393.32,10.91;12,112.66,481.54,393.33,10.91;12,112.66,495.09,394.53,10.91;12,112.66,508.64,297.60,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,340.12,454.44,165.87,10.91;12,112.66,467.99,316.22,10.91">Overview of the seventh social media mining for health applications (#SMM4H) shared tasks at COLING 2022</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenbacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Banda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Davydova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Estrada</forename><surname>Zavala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leddin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Magge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodriguez-Esteban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gonzalez-Hernandez</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.smm4h-1.54" />
	</analytic>
	<monogr>
		<title level="m" coord="12,451.97,467.99,54.01,10.91;12,112.66,481.54,393.33,10.91;12,112.66,495.09,256.97,10.91">Proceedings of The Seventh Workshop on Social Media Mining for Health Applications, Workshop &amp; Shared Task, Association for Computational Linguistics</title>
		<meeting>The Seventh Workshop on Social Media Mining for Health Applications, Workshop &amp; Shared Task, Association for Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="221" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,522.18,394.53,10.91;12,112.66,535.73,393.33,10.91;12,112.66,549.28,393.33,10.91;12,112.66,562.83,393.33,10.91;12,112.66,576.38,388.42,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,279.54,535.73,226.44,10.91;12,112.66,549.28,204.33,10.91">Overview of the clpsych 2022 shared task: Capturing moments of change in longitudinal user posts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zirikly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Inkster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leintz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.clpsych-1.16</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,340.02,549.28,165.97,10.91;12,112.66,562.83,245.01,10.91">Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</title>
		<meeting>the Eighth Workshop on Computational Linguistics and Clinical Psychology<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="184" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,589.93,393.32,10.91;12,112.66,603.48,393.32,10.91;12,112.66,617.03,178.54,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,351.55,589.93,154.44,10.91;12,112.66,603.48,319.30,10.91">Uned-nlp at erisk 2022: Analyzing gambling disorders in social media using approximate nearest neighbors</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fabregat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martínez-Romo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.11,603.48,50.87,10.91;12,112.66,617.03,147.84,10.91">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,630.58,394.52,10.91;12,112.66,644.13,393.60,10.91;13,112.66,86.97,393.33,10.91;13,112.14,100.52,300.31,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,302.86,644.13,203.41,10.91;13,112.66,86.97,343.02,10.91">Sinai at erisk@clef 2022: Approaching early detection of gambling and eating disorders with natural language processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Mármol-Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Jiménez-Zafra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del-Arco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Molina-González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Martín-Valdivia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montejo-Ráez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,478.96,86.97,27.03,10.91;13,112.14,100.52,100.46,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3180</biblScope>
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,114.06,395.17,10.91;13,112.66,127.61,394.52,10.91;13,112.66,141.16,122.77,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m" coord="13,383.43,127.61,119.11,10.91">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,394.53,10.91;13,112.66,168.26,122.77,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="13,219.42,154.71,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,181.81,393.33,10.91;13,112.66,195.36,363.59,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="13,353.43,181.81,152.55,10.91;13,112.66,195.36,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,208.91,394.53,10.91;13,112.66,222.46,394.52,10.91;13,112.66,236.01,394.53,10.91;13,112.66,249.56,393.32,10.91;13,112.66,263.11,394.03,10.91;13,112.66,276.66,385.97,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,371.36,236.01,135.83,10.91;13,112.66,249.56,175.42,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="13,310.67,249.56,195.31,10.91;13,112.66,263.11,36.87,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
