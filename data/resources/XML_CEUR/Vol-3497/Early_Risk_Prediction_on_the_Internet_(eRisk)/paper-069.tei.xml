<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,371.01,15.42;1,89.29,106.66,87.64,15.43;1,89.29,129.00,200.45,11.96">uOttawa at eRisk 2023: Search for Symptoms of Depression Notebook for the eRisk Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,154.90,53.41,11.96"><forename type="first">Yuxi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,155.70,154.90,66.28,11.96"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
							<email>diana.inkpen@uottawa.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<addrLine>800 King Edward</addrLine>
									<postCode>K1N 6N5</postCode>
									<settlement>Ottawa</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,371.01,15.42;1,89.29,106.66,87.64,15.43;1,89.29,129.00,200.45,11.96">uOttawa at eRisk 2023: Search for Symptoms of Depression Notebook for the eRisk Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">43D318B907679005F70862FBA050C36B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>depression detection</term>
					<term>social media analysis</term>
					<term>information retrieval</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the University of Ottawa's participation in Task 1 of the eRisk 2023 shared task at CLEF 2023. As early intervention of depression becomes more and more important, we are striving to build a system to search for depression symptoms. By participating, we could evaluate the effectiveness of our search techniques and identify areas for improvement. Our methods focused on extracting relevant sentences for each symptom in the Beck's Depression Inventory questionnaire and providing a ranking for further investigation. To rank the sentences, we represented them as neural embedding vectors, then we computed their cosine similarity to query embedding vectors. We constructed one query for each of the 21 symptoms of interest, based on the corresponding question and possible answers in the questionnaire.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media has become an essential part of everyone's daily life, people use it as a platform to express their feelings about almost everything. Since depression has become a prevalent mental health issue, early detection of symptoms could greatly improve the chances of proper treatment. Traditional methods of detection, usually human-led, are expensive to conduct and might be individually biased. Our team, as a participant in Task 1 of the eRisk 2023 shared task at CLEF 2023, is aiming to design a method to analyze social media sentences and then help identify potential symptoms of depression as well as support early intervention.</p><p>We considered the task as a search/information retrieval task, where user-written sentences are stored as documents. The 21 questions from the Beck's Depression Inventory (BDI) questionnaire were transformed into 21 queries. The aim of the task is to retrieve the top-1000 relevant sentences for each query, and also compute their rankings (rank 1 being the most relevant).</p><p>Several text embedding methods were used for transfer learning, including contextual text embedding methods such as DistilBERT, and distributional word embedding method GloVe. Combined with a semantic distance measure cosine similarity, our system extracts relevant sentences for each query and provides a rank for the top-1000 sentences for each of 21 questions/symptoms in the BDI questionnaire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">General Requirements</head><p>We focused on Task 1 of the eRisk 2023: Early Risk Prediction on the Internet <ref type="bibr" coords="2,420.20,469.28,11.28,10.91" target="#b0">[1]</ref>. Participants are given files in the TREC format containing the sentences of each user (subject). Each document has a document ID number as well as the text of the document. The aim of the task is to extract the top-1000 relevant sentences for each of 21 symptoms in the BDI questionnaire and provide rankings for the extracted sentences. The statistics of the dataset are shown in Table <ref type="table" coords="2,467.82,523.48,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluations</head><p>For each designed method, the results are saved in separate files, which will then be submitted for evaluation. The format of result files is shown through an example in Figure <ref type="figure" coords="2,447.41,586.75,3.74,10.91">1</ref>. The sentences were annotated by the shared task organizers with the help of human annotators. There were three annotators, two computer scientists with background in this research area and one psychologist. Based on the relevance of the sentences to the 21 symptoms in the BDI questionnaire, the annotators were guided to label sentences into 2 categories: relevant or irrelevant.</p><p>There are 2 types of evaluations, according to the provided relevance judgements (qrels): majority (using majority voting among the available human judgements) and unanimity. The performance is evaluated with 4 standard information retrieval metrics: Average Precision (AP), R-Precision (R-PREC), Precision at 10, and NDCG at 1000. More details including the resulting number of relevant sentences could be found in the overview paper <ref type="bibr" coords="3,392.68,141.16,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Information retrieval is usually the task that, given a query from a system user, the system searches and returns a ranked list of documents that are matching or are related to the specified query. Therefore we employed information retrieval techniques.</p><p>In a typical information retrieval system, an "index" is used to store for each term a list of documents containing the term. This inverted index is first constructed and then it is used for ranking based on some metrics (similarity formula). A search engine collects the documents before the information retrieval step and needs frequent updates. The index needs to be updated too. In our case, the collection of documents is provided and it is static. Since we do not need to update our collection and the system is not designed for frequent searches, we did not construct the inverted index. At this stage, the search space was restricted to a set of documents at hand. To accelerate the calculation of contextual representations, we selected keywords from the questions in the BDI questionnaire for filtering out unrelated documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Document Acquisition</head><p>All the documents, which in our case are more than 4 million sentences collected from social media and provided in the CLEF eRisk 2023 shared task, are downloaded and stored on an online storage platform Google Drive. Since we are not crawling documents from external sources, no crawler is needed. Before the pre-processing steps, the system connects to the Google Drive and extracts all the documents from the files in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Normalization and Text Processing</head><p>The sentences are stored in files containing documents with DOCNO (document number) and TEXT (textual content). The sentences are extracted, additional information related to the data format is discarded. Depending on the model being used, different pre-processing steps were applied to the texts: for obtaining word embeddings through GloVe <ref type="bibr" coords="3,383.08,543.13,11.28,10.91" target="#b1">[2]</ref>, we applied tokenization, lowercasing, stopword and punctuation removal; when getting vector representations using transformer-based models, we filtered out sentences that did not contain symptom-related keywords, used transfer learning and did not apply those pre-processing methods (we allowed the specific tokenization used by each contextual embedding model). The normalization and processing steps are applied on both documents and queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Searching with Contextual Representations</head><p>We used transformer-based models to obtain contextual representations of documents and queries, which is a type of embedding that looks at all the words in a sentence in the same time <ref type="bibr" coords="4,89.29,533.61,11.40,10.91" target="#b2">[3]</ref>. We filtered out the documents that did not contain certain keywords, to reduce the size of the dataset to accelerate computation. These keywords were picked from the 21 questions in the BDI questionnaire. All of the documents (4,264,693 sentences) were loaded for processing, 111,982 sentences were kept after filtering, and 4,152,711 sentences were filtered out using keywords. The queries we built for each question used both the text of the questions and the text of the possible answers. Information about the keywords and the queries we used is shown in Table <ref type="table" coords="4,127.71,614.91,3.74,10.91" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">DistilBERT with Cosine Similarity</head><p>We used DistilBERT <ref type="bibr" coords="5,185.30,107.54,11.58,10.91" target="#b3">[4]</ref>, a distilled version of BERT with a smaller model and competitive performance. It is faster to train, and lighter to load. After the vector representations of sentences and queries were collected, the cosine similarity was used for calculating semantic similarity between the query and the document (sentence in our case). The ranking of document relevance was then saved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">RoBERTa with Cosine Similarity</head><p>RoBERTa is an improved version of BERT, with a more carefully designed pretraining <ref type="bibr" coords="5,492.22,211.06,11.58,10.91" target="#b4">[5]</ref>. Similar with the method using DistilBERT, we used the cosine similarity to compute the text similarity and the record ranks for 21 the questions (queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Universal Sentence Encoder with Cosine Similarity</head><p>The Universal Sentence Encoder is a text encoder that directly encodes sentences into vectors. It is specifically designed for transfer learning of various types of NLP tasks. The encoder based on the transformer architecture was trained in the following way: the word representations acquired through the transformer were converted to a fixed-length encoding vector by summing the element-wise representations at each word position, and then the vector was divided by the square root of the length of the sentence to reduce sentence length effects. The inputs to the encoder are lowercased strings that tokenized using Penn Treebank Tokenizer (PTB), and the outputs are 512 dimensional vector representations. Since the model was designed to be of general purpose, multi-task learning was conducted <ref type="bibr" coords="5,318.41,395.88,11.28,10.91" target="#b5">[6]</ref>. The model has good performance with minimal training data <ref type="bibr" coords="5,189.55,409.43,11.47,10.91" target="#b6">[7]</ref>. We used the model to obtain embeddings of queries and sentences, and calculated cosine distance between them to obtain rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Searching with Distributional Word Representations</head><p>We used GloVe to get distributional embeddings of sentences and queries. Unlike transformers, GloVe creates co-occurrence matrices of texts, and then applies matrix factorization on the global matrix to shapes with various dimensionalities. As mentioned before, traditional pre-processing steps such as tokenization, lowercasing, stopword and punctuation removal were conducted on sentences. After data were cleaned, we chose 2 versions of GloVe: density of 50-dimension and 100-dimension. The GloVe embeddings were acquired for both documents and queries and then used for cosine similarity calculations. Ranks based on similarities are saved, as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>The expected criteria for sentence relevance judgements are introduced through examples. In Figure <ref type="figure" coords="5,119.46,626.18,3.66,10.91" target="#fig_0">2</ref>, some examples from the task overview paper <ref type="bibr" coords="5,327.44,626.18,12.69,10.91" target="#b0">[1]</ref> are given, to illustrate topic relevance. Participated systems are evaluated using the majority-based qrels and unanimity-based qrels. In total, 10 teams participated and 37 system runs were submitted for this shared task. The results of the majority voting evaluation for the 5 runs submitted by us are presented in Table <ref type="table" coords="5,499.87,666.82,3.66,10.91" target="#tab_2">3</ref>, Judging for symptom: Loss of Energy I cannot control my energy these days.</p><p>Relevant (1) My sister has no energy at all. Irrelevant (0) The book was about a highly energetic man.</p><p>Irrelevant (0) I feel more tired than usual.</p><p>Relevant (1) The football team is named Top Energy.</p><p>Irrelevant (0) I am totally lonely.</p><p>Irrelevant (0) I've just recharged my batteries.</p><p>Relevant (1) I am lost.</p><p>Irrelevant (0)  and the unanimity evaluation for the runs are shown in Table <ref type="table" coords="6,356.93,376.76,3.66,10.91" target="#tab_3">4</ref>. Our team ranked 3rd among the 10 participating teams, and our best performance was achieved by the method that employed the Universal Sentence Encoder with Cosine Similarity (USESim).</p><p>The results show that, overall, the universal text representation USE performed better than the other contextual representation techniques such as DistilBERT, for this task. Also, the contextual representation methods performed better on the metrics Precision at 10 and NDCG at 1000, compared to the distributed representation methods based on GloVe. A much larger search space was applied when using the method with GloVe (GloveSim and Glove100Sim) since all the sentences were checked for similarity (we did not filter out sentences in this method since the computation was fast enough). We think a lower performance could be due to the removal of stopwords when using the GloVe-based methods, for example, pronouns that are referring the participant in the discourse (the agent) were removed, but they could contain relevant information.</p><p>The performance of Glove100Sim is better than GloveSim which were providing embeddings with densities of 100 and 50 dimensions separately. This could demonstrate again the value of having more information and features being encoded for documents, provided that the vectors are not sparse.</p><p>Similar to situations met by many other teams, our methods performed generally worse on unanimity-based evaluations than majority-based evaluations. We consider the reason to be the stricter (but more convincing) nature of evaluation with unanimity-based qrels on relevance judgements of sentences.</p><p>In Table <ref type="table" coords="6,138.32,661.30,4.98,10.91" target="#tab_4">5</ref> and Table <ref type="table" coords="6,191.27,661.30,3.67,10.91" target="#tab_5">6</ref>, we compare our results with the best results from the shared task, for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper presented several methods for searching relevant sentences for symptoms in the BDI questionnaire. We showed that contextual representations, especially universal text representations perform better than distributed representations; we showed that when used for acquiring embeddings for documents, Universal Sentence Encoder gives better results and would also simplify the calculation since no need to combine word representations to form a single vector for a document. We also showed that transfer learning with pretrained models could be used for even detailed differentiation within a domain such as depression.</p><p>In the future, we consider distilling and enhancing our symptom-related keyword collections, and fine-tuning our filtering steps, so that more informative sentences could be retained. Since we conducted the experiments in a limited time, we expect the results to be better if more configurations are tested and more sentences are consumed by systems. Also for the USE, since multiple versions of models trained with different goals are available, we are considering experimenting with other variants of the model. Further pre-training the model on more specific and related corpus is also considerable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,211.17,173.26,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of sentence relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,404.44,243.43"><head>Table 1</head><label>1</label><figDesc>Statistics of the dataset</figDesc><table coords="2,323.21,119.88,37.30,8.87"><row><cell>Quantity</cell></row></table><note coords="2,89.29,324.99,176.32,8.93"><p>Figure 1: An example of generated results.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,408.53,368.75"><head>Table 2</head><label>2</label><figDesc>Queries and Keywords for Each Question</figDesc><table coords="4,96.92,122.10,400.60,229.54"><row><cell>Question</cell><cell>Keywords</cell><cell></cell><cell>Query</cell></row><row><cell>Q1</cell><cell cols="2">sadness, sad, unhappy</cell><cell>Sadness. I feel sad unhappy cannot stand it.</cell></row><row><cell>Q2</cell><cell cols="2">pessimism, discouraged, hope-</cell><cell>Pessimism. I feel discouraged about my future is</cell></row><row><cell></cell><cell>less</cell><cell></cell><cell>hopeless and will get worse.</cell></row><row><cell>Q3</cell><cell>failure, fail</cell><cell></cell><cell>Past Failure. I have failed.</cell></row><row><cell>Q4</cell><cell>pleasure, enjoy</cell><cell></cell><cell>Loss of Pleasure. I don't enjoy things.</cell></row><row><cell>Q5</cell><cell>guilty</cell><cell></cell><cell>Guilty Feelings. I feel guilty.</cell></row><row><cell>Q6</cell><cell>punishment, punish</cell><cell></cell><cell>Punishment Feelings. I am being punished.</cell></row><row><cell>Q7</cell><cell cols="2">confidence, disappointed</cell><cell>Self-Dislike. I have lost confidence. I am disap-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pointed in myself.</cell></row><row><cell>Q8</cell><cell cols="2">criticalness, critical, criticize,</cell><cell>Self-Criticalness. I criticize myself blame myself for</cell></row><row><cell></cell><cell>blame, fault</cell><cell></cell><cell>my faults.</cell></row><row><cell>Q9</cell><cell>suicidal, suicide, kill</cell><cell></cell><cell>Suicidal Thoughts or Wishes. I kill myself.</cell></row><row><cell>Q10</cell><cell>crying, cry</cell><cell></cell><cell>Crying. I cry.</cell></row><row><cell>Q14</cell><cell>worthlessness,</cell><cell>worthless,</cell><cell>Worthlessness. I feel worthless not useful.</cell></row><row><cell></cell><cell>worthwhile, useful</cell><cell></cell><cell></cell></row></table><note coords="4,96.92,296.25,16.26,7.22;4,150.55,294.95,104.78,8.87;4,287.52,294.95,198.53,8.87;4,96.92,308.20,16.26,7.22;4,150.55,306.91,76.87,8.87;4,287.52,306.91,174.05,8.87;4,96.92,320.16,16.26,7.22;4,150.55,318.86,343.01,8.87;4,96.92,356.02,16.26,7.22;4,150.55,354.73,69.71,8.87;4,287.52,354.73,178.47,8.87;4,96.92,367.98,16.26,7.22;4,150.55,366.68,59.04,8.87;4,287.52,366.68,208.35,8.87;4,287.52,378.64,45.65,8.87;4,96.92,391.89,16.26,7.22;4,150.55,390.59,106.59,8.87;4,287.52,390.59,101.73,8.87;4,96.92,403.84,16.26,7.22;4,150.55,402.55,74.14,8.87;4,287.52,402.55,208.32,8.87;4,96.92,415.80,16.26,7.22;4,150.55,414.50,109.91,8.87;4,287.52,414.50,209.88,8.87;4,287.52,426.46,78.89,8.87;4,96.92,439.71,16.26,7.22;4,150.55,438.41,94.37,8.87;4,287.52,438.41,177.99,8.87;4,96.92,451.66,16.26,7.22;4,150.55,450.37,12.92,8.87;4,287.52,450.37,201.88,8.87"><p>Q11 agitation, agitate, restless Agitation. I am restless or agitated keep moving. Q12 interest, interested Loss of Interest. It's hard to get interested. Q13 indecisiveness, decision, decide Indecisiveness. I find it difficult to make decisions. Q15 energy, energetic Loss of Energy. I don't have enough energy. Q16 sleep, sleeping Changes in Sleeping Pattern. I sleep more or less than usual. Q17 irritability, irritable, angry Irritability. I am irritable. Q18 appetite, food, eat Changes in Appetite. My appetite is greater or less. Q19 concentration, concentrate Concentration Difficulty. It's hard to keep my mind. I can't concentrate. Q20 tiredness, fatigue, tired Tiredness or Fatigue. I am tired or fatigued. Q21 sex Loss of Interest in Sex. I am less interested in sex.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,244.32,329.74,105.74"><head>Table 3</head><label>3</label><figDesc>Results for submitted 5 runs (majority voting)</figDesc><table coords="6,174.06,275.94,244.67,74.12"><row><cell>Run</cell><cell>AP</cell><cell cols="3">R-PREC P at 10 NDCG at 1000</cell></row><row><cell>USESim</cell><cell>0.160</cell><cell>0.248</cell><cell>0.600</cell><cell>0.382</cell></row><row><cell cols="2">Glove100Sim 0.017</cell><cell>0.052</cell><cell>0.195</cell><cell>0.105</cell></row><row><cell>RobertaSim</cell><cell>0.033</cell><cell>0.080</cell><cell>0.329</cell><cell>0.150</cell></row><row><cell>GloveSim</cell><cell>0.011</cell><cell>0.038</cell><cell>0.162</cell><cell>0.075</cell></row><row><cell>BertSim</cell><cell>0.084</cell><cell>0.150</cell><cell>0.505</cell><cell>0.271</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,329.74,105.74"><head>Table 4</head><label>4</label><figDesc>Results for submitted 5 runs (unanimity)</figDesc><table coords="7,174.06,122.10,244.67,74.12"><row><cell>Run</cell><cell>AP</cell><cell cols="3">R-PREC P at 10 NDCG at 1000</cell></row><row><cell>USESim</cell><cell>0.139</cell><cell>0.232</cell><cell>0.438</cell><cell>0.380</cell></row><row><cell>GloveSim</cell><cell>0.008</cell><cell>0.028</cell><cell>0.110</cell><cell>0.063</cell></row><row><cell cols="2">Glove100Sim 0.011</cell><cell>0.042</cell><cell>0.110</cell><cell>0.092</cell></row><row><cell>RobertaSim</cell><cell>0.025</cell><cell>0.068</cell><cell>0.190</cell><cell>0.140</cell></row><row><cell>BertSim</cell><cell>0.070</cell><cell>0.130</cell><cell>0.357</cell><cell>0.260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,218.48,366.47,93.78"><head>Table 5</head><label>5</label><figDesc>Our results compared to the best results in the shared task (majority voting)</figDesc><table coords="7,137.32,250.10,318.15,62.16"><row><cell>Metric</cell><cell>Run</cell><cell cols="3">Rank (Out of 37) Our best Best in shared task</cell></row><row><cell>AP</cell><cell>USESim</cell><cell>5</cell><cell>0.160</cell><cell>0.319</cell></row><row><cell>R-PREC</cell><cell>USESim</cell><cell>6</cell><cell>0.248</cell><cell>0.375</cell></row><row><cell>P@10</cell><cell>USESim</cell><cell>7</cell><cell>0.600</cell><cell>0.861</cell></row><row><cell cols="2">NDCG@1000 USESim</cell><cell>6</cell><cell>0.382</cell><cell>0.596</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.99,334.52,366.47,131.40"><head>Table 6</head><label>6</label><figDesc>Our results compared to the best results in the shared task (unanimity)</figDesc><table coords="7,89.29,366.14,366.17,99.78"><row><cell>Metric</cell><cell>Run</cell><cell cols="3">Rank (Out of 37) Our best Best in shared task</cell></row><row><cell>AP</cell><cell>USESim</cell><cell>5</cell><cell>0.139</cell><cell>0.293</cell></row><row><cell>R-PREC</cell><cell>USESim</cell><cell>5</cell><cell>0.232</cell><cell>0.360</cell></row><row><cell>P@10</cell><cell>USESim</cell><cell>7</cell><cell>0.438</cell><cell>0.709</cell></row><row><cell cols="2">NDCG@1000 USESim</cell><cell>6</cell><cell>0.380</cell><cell>0.615</cell></row><row><cell>the four metrics.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> <rs type="institution">(NSERC)</rs> for supporting our research. We also want to thank the <rs type="institution">CLEF eRisk 2023</rs>, for providing a great opportunity for us to explore the task of early risk detection.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.59,244.87,398.40,10.91;8,107.59,258.42,399.59,10.91;8,107.59,271.96,398.40,10.91;8,107.59,285.51,399.60,10.91;8,107.59,299.06,244.99,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,346.12,244.87,159.86,10.91;8,107.59,258.42,182.02,10.91">eRisk 2023: Depression, Pathological Gambling, and Eating Disorder Challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_67</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.00,271.96,113.99,10.91;8,107.59,285.51,38.61,10.91">Advances in Information Retrieval</title>
		<title level="s" coord="8,153.21,285.51,157.39,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Switzerland, Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,312.61,399.69,10.91;8,107.59,326.16,398.40,10.91;8,107.26,339.71,400.40,10.91;8,107.59,353.26,314.57,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,278.89,312.61,208.31,10.91">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://aclanthology.org/D14-1162.doi:10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="8,107.59,326.16,398.40,10.91;8,107.26,339.71,238.62,10.91">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,366.81,399.59,10.91;8,107.20,380.36,400.63,10.91;8,107.59,393.91,399.11,10.91;8,107.59,407.46,251.12,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,107.20,380.36,114.26,10.91">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="s" coord="8,246.19,380.36,236.05,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,421.01,399.60,10.91;8,107.59,434.55,399.58,10.91;8,107.59,448.10,196.96,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,292.07,421.01,215.11,10.91;8,107.59,434.55,120.41,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.01108</idno>
		<idno type="arXiv">arXiv:1910.01108[cs</idno>
		<ptr target="http://arxiv.org/abs/1910.01108.doi:10.48550/arXiv.1910.01108" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,461.65,399.60,10.91;8,107.23,475.20,400.04,10.91;8,107.24,488.75,388.46,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,163.44,475.20,266.28,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692.doi:10.48550/arXiv.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,502.30,400.24,10.91;8,107.59,515.85,400.24,10.91;8,107.59,529.40,399.60,10.91;8,107.59,542.95,96.88,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-Y. Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1803.11175</idno>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<ptr target="http://arxiv.org/abs/1803.11175.doi:10.48550/arXiv.1803.11175" />
		<title level="m" coord="8,399.82,515.85,108.00,10.91;8,107.59,529.40,22.50,10.91">Universal Sentence Encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,556.50,400.24,10.91;8,107.59,570.05,399.69,10.91;8,107.59,583.60,399.69,10.91;8,107.59,597.15,399.60,10.91;8,107.59,610.69,400.01,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,319.43,570.05,169.07,10.91">Universal Sentence Encoder for English</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-Y. Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2029</idno>
		<ptr target="https://aclanthology.org/D18-2029.doi:10.18653/v1/D18-2029" />
	</analytic>
	<monogr>
		<title level="m" coord="8,107.59,583.60,399.69,10.91;8,107.59,597.15,307.66,10.91">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
