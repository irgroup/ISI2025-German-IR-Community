<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.26,240.09,15.39;1,329.38,82.23,6.25,10.68">BFH-AMI at eRisk</title>
				<funder>
					<orgName type="full">Inventus Bern Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,114.59,98.51,10.68"><forename type="first">Ghofrane</forename><surname>Merhbene</surname></persName>
							<email>ghofrane.merhbene@bfh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Intelligence</orgName>
								<orgName type="institution">Berner Fachhochschule BFH</orgName>
								<address>
									<addrLine>HÃ¶heweg 80</addrLine>
									<postCode>2502</postCode>
									<settlement>Biel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.46,114.59,100.75,10.68"><forename type="first">Alexandre</forename><forename type="middle">R</forename><surname>Puttick</surname></persName>
							<email>alexandre.puttick@bfh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Intelligence</orgName>
								<orgName type="institution">Berner Fachhochschule BFH</orgName>
								<address>
									<addrLine>HÃ¶heweg 80</addrLine>
									<postCode>2502</postCode>
									<settlement>Biel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.22,114.59,106.32,10.68"><forename type="first">Mascha</forename><surname>Kurpicz-Briki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Intelligence</orgName>
								<orgName type="institution">Berner Fachhochschule BFH</orgName>
								<address>
									<addrLine>HÃ¶heweg 80</addrLine>
									<postCode>2502</postCode>
									<settlement>Biel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.26,240.09,15.39;1,329.38,82.23,6.25,10.68">BFH-AMI at eRisk</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6EAFF790C2F2BE47CC09D0AFC2D41A22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Early Detection System</term>
					<term>Natural Language Processing</term>
					<term>Machine Learning</term>
					<term>Eating Disorder</term>
					<term>Mental Health</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mental health problems are a rising problem of today's society. Methods of machine learning and natural language processing provide interesting new possibilities for psychology and psychiatry. In particular, eating disorders (ED) are widespread and can be life-threatening if untreated. This paper describes the approach to Task 3 of the eRisk 2023 challenge of the BFH-AMI team. The task concerned the prediction of patients' answers to the Eating Disorder Examination Questionnaire (EDE-Q) based on their social media writing history. In our approach, we used a logistic regression model that was fed with a combination of user and question embeddings from the GPT-2 Large model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Eating disorders (EDs) represent a severe and potentially life-threatening mental health condition, especially if left untreated. They encompass a range of complex conditions characterized by disturbances in eating behaviors, distorted body image, and major psychological distress. The impact of these disorders is widespread, affecting millions of individuals worldwide. For instance, research conducted in 2015 revealed that anorexia, which is a common type of eating disorder, had already affected more than 2.9 million people <ref type="bibr" coords="1,359.98,416.02,11.59,9.74" target="#b0">[1]</ref>. Such statistics highlight the magnitude of the issue and emphasize the urgent need for effective intervention and treatment strategies. Early detection and severity assessment of signs associated with EDs is paramount for effective intervention. Traditionally, the assessment of the severity of EDs has heavily relied on clinical evaluations which are known to often be time-consuming and labor-intensive. This resourceintensive assessment can be facilitated with computational approaches that can provide efficient pre-assessments on the severity of EDs. The CLEF eRisk 1 Challenge is an academic research competition that encourages participants to develop text-based innovative solutions toward understanding health-related data. In its third task of the 2023 edition, the focus was on using Natural Language Processing (NLP) techniques to assess the severity levels of ED symptoms. To solve the task, the participants of the challenge were asked to design systems for predicting responses to an eating disorder questionnaire for different patients, based on a history of their postings from social media. The Eating Disorder Examination Questionnaire (EDE-Q)<ref type="foot" coords="2,252.36,98.96,4.06,7.79" target="#foot_0">2</ref>  <ref type="bibr" coords="2,259.61,101.64,12.86,9.74" target="#b1">[2]</ref> was used to collect comprehensive and reliable data regarding participants' eating behaviors, body image concerns, and psychological distress associated with their eating disorder. The questionnaire covers multiple domains, including dietary restraint, eating concerns, shape concerns, and weight concerns. In this paper, we document the approach of our research team BFH-AMI in this third task. Our aim was to leverage state-of-the-art NLP techniques to develop an efficient methodology for automatically detecting the severity of the signs of EDs. In the long term, technologies such as the one developed in this challenge can be further enhanced and validated as clinical tools. Such tools can support clinical professionals in their tasks and provide them with additional new insights based on data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Traditional approaches conducted by clinical professionals, such as psychologists and therapists, to assessing people's emotions and traits through survey questionnaires and interviews have limitations in terms of cost, time, and scalability. However, recent advancements in NLP techniques offer promising new options to address these challenges and support the clinical professionals. Recent research has delved into various approaches to automate the identification of eating disorders. For example, LÃ³pez-Ãšbeda et al. <ref type="bibr" coords="2,317.46,349.47,12.69,9.74" target="#b2">[3]</ref> explored a range of strategies, including different machine learning techniques. They conducted experiments using five supervised learning models on a Spanish Anorexia dataset and achieved an F1-score of over 0.9 using Support Vector Machines (SVM) and Multilayer Perceptron (MLP). Other studies explored alternative techniques, such as Convolutional Neural Networks (CNN) and Short Term Memory (LSTM), e.g., <ref type="bibr" coords="2,109.44,417.22,11.43,9.74" target="#b3">[4]</ref>. Moreover, the exploration of recent NLP technologies, like BERT <ref type="bibr" coords="2,381.27,430.77,12.87,9.74" target="#b4">[5]</ref> embeddings, has demonstrated promise in predicting questionnaire responses by leveraging text from social media and survey questions, as demonstrated by Vu et al. <ref type="bibr" coords="2,330.26,457.86,13.00,9.74" target="#b5">[6]</ref> using a novel technique developed to address this task. By analyzing participants' social media texts and the text of the survey questions they are asked, the researchers used BERT to represent both the participants and the survey questions as embedding vectors. This enabled the prediction of responses for both new participants and new questions not seen during training. This method offers the possibility to study new participants or new questions without the constraints of costly data collection. The proposed approach not only facilitates novel practical applications but also contributes to the advancement of psychological theory. Furthermore, the success of the model suggests a promising NLP-powered alternative to the resource-intensive use of traditional assessment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Data</head><p>During the training phase of the challenge, the eRisk team provided the entire history of writings and corresponding answers to the Eating Disorder Examination Questionnaire (EDE-Q) for a specific set of training users. This allowed the participants to train their systems using the provided data. The EDE-Q questionnaire consists of 28 items out of which only questions 1-12 and 19-28 were considered for the purpose of this competition. The questionnaire is designed to assess the range and severity of characteristics associated with a diagnosis of an eating disorder and it includes four sub-scales: Restraint, Eating Concern, Shape Concern, and Weight Concern, as well as a global score.</p><p>The training set consisted of 28 subjects. Each subject had a history of postings from the social media platform Reddit<ref type="foot" coords="3,186.95,399.99,4.06,7.79" target="#foot_1">3</ref> as well as their answers to the EDE-Q questionnaire, the latter of which serving as ground truth labels for the task. This combined data allows for a comprehensive examination and analysis of the subjects' online interactions and self-reported information. During the test stage, the writing history of a new set of users was provided. However, the test set did not include the answers to the questionnaire. Using the trained models, participants of the task had to generate predictions for the EDE-Q questionnaire. The testing set consisted of the writing history of 46 subjects on Reddit and was structured in a similar manner to the training set. The statistics for the training and test data are presented in Table <ref type="table" coords="3,382.09,511.07,5.07,9.74" target="#tab_0">1</ref> and<ref type="table" coords="3,409.04,511.07,30.56,9.74">Table 2</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Metrics</head><p>The evaluation of system performance in this third task is based on several measures of effectiveness. These measures have been defined by the organizers as follows:</p><p>â€¢ Mean Zero-One Error (MZOE): To measure the average error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Mean Absolute Error (MAE):</head><p>To measure the deviation of the model's predictions from the actual values.</p><p>â€¢ Macroaveraged Mean Absolute Error (MAEmacro): Is similar to MAE. It is the mean absolute difference for each class independently and then averages them across all classes.</p><p>Here a class is defined as the set of all questions ğ‘„ ğ‘– whose true answer is equal to ğ‘– âˆˆ {0, 1, â€¦ , 6} MZOE, MAE, and MAEmacro each calculate a single score for every user, and the reported score is the average of all these values.</p><p>The measures presented below are derived from aggregated scores obtained from the questionnaires:</p><p>Restraint Subscale (RS):</p><formula xml:id="formula_0" coords="4,203.40,242.10,186.82,31.92">ğ‘…ğ‘€ğ‘†ğ¸(ğ‘“ , ğ‘ˆ ) = âˆš âˆ‘ ğ‘¢ ğ‘– âˆˆğ‘ˆ (ğ‘… RS (ğ‘¢ ğ‘– ) -ğ‘“ RS (ğ‘¢ ğ‘– )) 2 |ğ‘ˆ |</formula><p>where </p><formula xml:id="formula_1" coords="4,195.80,653.78,202.02,31.92">ğ‘…ğ‘€ğ‘†ğ¸(ğ‘“ , ğ‘ˆ ) = âˆš âˆ‘ ğ‘¢ ğ‘– âˆˆğ‘ˆ (ğ‘… GED (ğ‘¢ ğ‘– ) -ğ‘“ GED (ğ‘¢ ğ‘– )) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|ğ‘ˆ |</head><p>A global score can be calculated by adding the scores of the four subscales scores and then dividing the resulting total by 4. Additional details and information regarding the specific evaluation metrics employed during the evaluation phase can be accessed in the Overview of eRisk competition <ref type="bibr" coords="5,425.81,128.74,11.43,9.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head><p>Our main task is to assess to what extent the characteristics linked to the diagnosis of EDs in the questionnaire are reflected in a history of user writings. To accomplish this, we generated various embeddings for both user posts and questions. These embeddings were then combined and used as input for a logistic regression model. The process of generating the embeddings is described in detail below.</p><p>It is important to note that a single run was submitted to the challenge. Our submission involved a logistic regression model with an L2-regularization parameter = 1/50, which was chosen after fine-tuning using a hyperparameter search with values [1, 1/10, 1/20, 1/30, 1/40, 1/50]. All other models use L2-regularization parameters = 1. In the final model, the embeddings were obtained using GPT-2, combined with a method for extracting the most relevant user sentences via cosine similarity. This was the result of several incremental improvements detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Embeddings</head><p>To generate both the question embeddings as well as the user embeddings, we tried the method detailed below using two different models. This method was built upon techniques used for a similar task in <ref type="bibr" coords="5,155.32,398.95,11.56,9.74" target="#b5">[6]</ref>:</p><p>â€¢ BERT Large (uncased) <ref type="bibr" coords="5,227.27,419.11,12.69,9.74" target="#b4">[5]</ref> is a pre-trained language model with 336 million parameters.</p><p>The "uncased" aspect means that the model treats capitalization as irrelevant and converts all text to lowercase during training. This allows for better generalization across different cases of the same word. â€¢ GPT-2 Large <ref type="bibr" coords="5,180.66,474.01,13.00,9.74" target="#b7">[8]</ref> is a pre-trained language model with 774 million parameters, which</p><p>gives it an impressive ability to generate coherent and contextually relevant text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Embeddings</head><p>To compute the embeddings for the 22 questions from the EDE-Q questionnaire, we used one of the two pre-trained models described above. To begin, we extracted hidden vectors from the last four layers of the model corresponding to each word in the input text, which contain valuable information regarding the semantic representation of the questions. We averaged these four embedding vectors over all of the words in a given question, resulting in four vectors representing that question. These four vectors were concatenated to obtain the final question embeddings. This technique, yielded in embedding vectors of dimension 4096 in the case of BERT Large and 5120 in the case of GPT-2 Large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Writings Embeddings</head><p>Method 1, Chunk Embeddings: Because some user posts were too long to feed into the model, we concatenated all user posts together (in chronological order), and broke the results text into chunks of text corresponding of length ğ‘› tokens, where ğ‘› is the maximum input sequence length of the model (512 for BERT Large and 1024 for GPT-2 Large). The embeddings for each text chunk were computed in a manner identical to the one used to obtain question embeddings described above. Afterwards, these chunk embeddings were averaged to obtain user embeddings.</p><p>Method 2, Sentence Extraction: Since users' writing histories contained many posts that were not relevant for the task, we attempted to derive a method for extracting those sentences that were most relevant for the prediction of the corresponding user's degree of ED symptoms. To do this, we computed sentence embeddings for every sentence written by a given user in the same manner as for the question and text-block embeddings described above. These were compared to the 'topic' vector obtained by averaging all of the question embeddings to obtain a single vector. We extracted the 20 user-written sentences that were closest to this topic vector with respect to cosine similarity and averaged the corresponding sentence embeddings to obtain user embeddings. Note that this technique was only applied using the GPT-2 large model, and not the BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline</head><p>To assess the performance of our model, we relied on a simple baseline approach for comparison. In this baseline method, for each question, the prediction is made by taking the average of all the users' answers from the training data. This served as a basic benchmark against which we could measure the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Table <ref type="table" coords="6,115.92,426.78,5.10,9.74" target="#tab_2">3</ref> summarizes our results when evaluating our different approaches using 10-fold crossvalidation on the training data. Each sample consisted of the concatenation of the user embedding and question embedding pair, labeled by the user's response to the corresponding question. For each 10-fold split, a 7-class (responses from 0 to 6) logistic regression classifier was trained on nine folds and tested on the remaining fold. The evaluation metrics were computed by averaging over the ten folds. The GPT-2 model with sentence extraction outperformed all other models, which is why it was chosen to be submitted to the competition. The performance results on the test data across all the metrics described in Section 4 are presented in Table <ref type="table" coords="6,342.87,535.17,3.74,9.74" target="#tab_3">4</ref>. Although our model performed considerably better than the baseline model during development, it only outperformed the baseline according to the MZOE metric on the test data. Having only submitted one run, it is difficult to discern the cause for this, but random chance associated to a very small training set (ğ‘› = 28 users) maybe have played a role. We also observed significant differences in formatting between training and test data, which may have negatively affected performance. Given formatting differences and the inability to troubleshoot on test data, we cannot rule out simple implementation errors. Qualitative analysis of the sentences extracted using our cosine similarity based criteria suggest that the method indeed extracts sentences speaking about mental and physical health, food, weight etc., although plenty of less relevant sentences were also extracted, and we did not cross-check our method to gauge if the most important sentences were indeed extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper documents the participation of our team BFH-AMI in the task 3 of the eRisk@CLEF 2023 edition. We investigated the severity of the signs of eating disorders, by developing a model that automatically generates responses to questions from the EDE-Q questionnaire, based on the user's writings on social media provided in anonymous form by the organizers. In our proposed approach, we used a logistic regression model that was fed with a combination of user and question embeddings extracted from the GPT-2 Large model. The performance metrics demonstrate that there is substantial room for improvement across various evaluation criteria. Future work could investigate the following directions:</p><p>â€¢ More powerful language models: Larger language models could be employed in an attempt to capture more semantic information in user and question embeddings. However, experiments carried out using the 1.3 billion parameter version of GPT-Neo <ref type="bibr" coords="7,458.46,561.19,12.92,9.74" target="#b8">[9]</ref> did not yield significantly better results, although we did not have time to make a thorough comparison.</p><p>â€¢ Improved sentence extraction: Our sentence extraction method was based on comparison to a topic vector obtained by averaging all questions. In general, the more text that is averaged into an embedding vector of fixed length, the more the distinguishing features can become smoothed out. Therefore, it might be preferable to average only questions corresponding to a specific dimension of the EDE-Q, or even extract sentences separately for each question. We also observed that many of the extracted sentences were also questions. This is not so relevant for the task, but is a feature that was likely encoded in the question embeddings and carried over to the most similar user sentences. In the future, this could be avoided by rephrasing each question into an analogous first person statement such as "I have been deliberately trying to limit the amount of food I eat to influence my shape or weight. " Instead of taking the top 20 sentences, one might instead take only the sentences above a certain similarity threshold. This would allow flexibility in the amount of sentences extracted for each user, given relatively few when a user does not write about relevant topics and many when the user does. â€¢ Improved embedding methods: Our methods relied on averaging over many words and sentences, following the methods in <ref type="bibr" coords="8,277.99,211.39,11.36,9.74" target="#b5">[6]</ref>. For transformer models like the ones used here, in cases where the input text is not too long, the embeddings obtained from only the final word in the sequence should contain information about the context preceeding that word. As mentioned above, averaging could have an undesired smoothing effect on the embedding vectors, and it may be preferable to either use only such "last word" embeddings or devise other strategies (such as sentence extraction), for decreasing the amount of text aggregated into each embedding vector. Furthermore, while we always constructed embedding vectors using the final four layers of the models, this number four could also be considered a hyperparameter and adjusted for ideal performance. â€¢ More deep learning: In our methods, we only used deep learning models for feature extraction to then feed into a classical machine learning classifier (logistic regression). Fine-tuning weights within the large language models could improve performance, although, with so few data samples, there is a high danger of over-fitting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.67,291.05,175.90"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,88.99,102.62,291.05,163.94"><row><cell>Training data statistics</cell><cell></cell></row><row><cell>Nb. of Subjects</cell><cell>28</cell></row><row><cell>Min. Nb. of posts per Subject</cell><cell>12</cell></row><row><cell>Max. Nb. of posts per Subject</cell><cell>1143</cell></row><row><cell cols="2">Avg. Nb. of characters per Post 184.33</cell></row><row><cell>Table 2</cell><cell></cell></row><row><cell>Test data statistics</cell><cell></cell></row><row><cell>Nb. of Subjects</cell><cell>46</cell></row><row><cell>Min. Nb. of posts per Subject</cell><cell>5</cell></row><row><cell>Max. Nb. of posts per Subject</cell><cell>1161</cell></row><row><cell cols="2">Avg. Nb. of characters per Post 223.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.89,281.81,417.10,359.35"><head></head><label></label><figDesc>ğ‘ˆ is the user set, ğ‘… RS represents the real subscale ED score for user ğ‘¢ ğ‘– , and ğ¹ RS represents the estimated subscale ED score for user ğ‘¢ ğ‘– . The reported RMSE is the average over all RMSE values (mean RMSE over all users). ğ‘¢ ğ‘– âˆˆğ‘ˆ (ğ‘… WCS (ğ‘¢ ğ‘– ) -ğ‘“ WCS (ğ‘¢ ğ‘– ))2   |ğ‘ˆ |where ğ‘… WCS represents the real weight concern ED score for user ğ‘¢ ğ‘– , and ğ¹ WCS represents the estimated weight concern ED score for user ğ‘¢ ğ‘– . The reported RMSE is the average over all RMSE values (mean RMSE over all users).</figDesc><table coords="4,100.20,528.46,181.34,112.71"><row><cell>Weight Concern Subscale (WCS):</cell></row><row><cell>ğ‘…ğ‘€ğ‘†ğ¸(ğ‘“ , ğ‘ˆ ) = âˆ‘ Global ED (GED): âˆš</cell></row></table><note coords="4,100.20,322.46,153.56,9.74;4,198.02,357.21,63.84,9.74;4,264.90,367.24,9.35,9.74;4,275.44,346.93,9.24,9.74;4,284.68,351.37,14.86,9.09;4,300.01,345.06,95.58,13.47;4,329.82,364.86,11.87,9.74;4,88.89,384.78,417.09,11.50;4,89.29,398.33,416.69,11.50;4,89.02,411.88,152.49,9.74;4,100.20,425.42,149.95,9.74;4,198.65,460.13,63.84,9.74;4,265.52,470.17,9.35,9.74;4,276.07,449.85,9.24,9.74;4,285.31,454.30,14.86,9.09;4,300.64,447.98,94.32,13.47;4,329.82,467.78,11.87,9.74;4,88.89,487.81,417.09,11.50;4,89.29,501.36,416.69,11.50;4,89.02,514.91,155.74,9.74"><p>Eating Concern Subscale (ECS): ğ‘…ğ‘€ğ‘†ğ¸(ğ‘“ , ğ‘ˆ ) = âˆš âˆ‘ ğ‘¢ ğ‘– âˆˆğ‘ˆ (ğ‘… ECS (ğ‘¢ ğ‘– ) -ğ‘“ ECS (ğ‘¢ ğ‘– )) 2 |ğ‘ˆ | where ğ‘… ECS represents the real eating concern ED score for user ğ‘¢ ğ‘– , and ğ‘… ECS represents the estimated eating concern ED score for user ğ‘¢ ğ‘– . The reported RMSE is the average over all RMSE values (mean RMSE over all users. Shape Concern Subscale (SCS): ğ‘…ğ‘€ğ‘†ğ¸(ğ‘“ , ğ‘ˆ ) = âˆš âˆ‘ ğ‘¢ ğ‘– âˆˆğ‘ˆ (ğ‘… SCS (ğ‘¢ ğ‘– ) -ğ‘“ SCS (ğ‘¢ ğ‘– )) 2 |ğ‘ˆ | where ğ‘… SCS represents the real shape concern ED score for user ğ‘¢ ğ‘– , and ğ¹ SCS ) represents the estimated shape concern ED score for user ğ‘¢ ğ‘– . The reported RMSE is the average over all RMSE values (mean RMSE over all users).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,563.21,361.41,93.80"><head>Table 3</head><label>3</label><figDesc>Performance over training data using 10-fold cross validation</figDesc><table coords="6,144.88,594.81,305.52,62.21"><row><cell>Model</cell><cell cols="3">MZOE ğ‘€ğ´ğ¸ macro GED</cell></row><row><cell>Baseline average</cell><cell>0.96</cell><cell>2.10</cell><cell>1.96</cell></row><row><cell>GPT-2 with sentence embeddings (L2 = 1/50)</cell><cell>0.73</cell><cell>1.30</cell><cell>1.37</cell></row><row><cell>GPT-2 with chunk embeddings (L2 = 1)</cell><cell>0.78</cell><cell>1.50</cell><cell>1.61</cell></row><row><cell>BERT with chunk embeddings (L2 = 1)</cell><cell>0.78</cell><cell>1.70</cell><cell>2.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.67,418.65,105.76"><head>Table 4</head><label>4</label><figDesc>Performance over test data obtained by the Logistic regression and GPT-2 large sentence based embeddings</figDesc><table coords="7,119.62,134.22,356.04,62.21"><row><cell></cell><cell cols="4">MAE MZOE ğ‘€ğ´ğ¸ macro GED</cell><cell>RS</cell><cell>ECS</cell><cell>SCS WCS</cell></row><row><cell>Baseline all 0s</cell><cell>2.419</cell><cell>0.674</cell><cell>2.803</cell><cell cols="2">3.207 2.138 3.221 3.028 2.682</cell></row><row><cell>Baseline all 6s</cell><cell>3.581</cell><cell>0.834</cell><cell>3.995</cell><cell cols="2">3.839 4.814 3.650 3.950 3.318</cell></row><row><cell cols="2">Baseline average 2.091</cell><cell>0.859</cell><cell>1.957</cell><cell cols="2">2.391 1.592 2.398 2.162 2.002</cell></row><row><cell>BFH-AMI</cell><cell>2.407</cell><cell>0.719</cell><cell>2.729</cell><cell cols="2">3.169 2.597 2.854 2.923 2.144</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,92.46,671.96,222.07,8.01"><p>https://www.corc.uk.net/media/1273/ede-q_quesionnaire.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,92.46,671.93,90.34,8.01"><p>https://www.reddit.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the <rs type="funder">Inventus Bern Foundation</rs> in our research in the field of augmented intelligence for the detection of eating disorders.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.59,506.13,398.39,9.74;8,107.59,519.68,398.41,9.74;8,107.59,533.23,400.08,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,170.99,506.13,334.99,9.74;8,107.59,519.68,398.41,9.74;8,107.59,533.23,87.77,9.74">Global, regional, and national incidence, prevalence, and years lived with disability for 310 diseases and injuries, 1990-2015: a systematic analysis for the global burden of disease study 2015</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vos</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0140-6736(16)31678-6</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,202.56,533.23,47.99,9.74">The Lancet</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="page" from="1545" to="1602" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,546.78,342.84,9.74" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Fairburn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,288.19,546.78,127.24,9.74">Eating Disorder Examination</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">0D</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,560.33,400.24,9.74;8,107.59,573.88,400.24,9.74;8,107.59,587.43,399.60,9.74;8,107.23,600.98,398.76,9.74;8,107.59,614.53,105.93,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,485.50,560.33,22.32,9.74;8,107.59,573.88,188.52,9.74">Detecting anorexia in Spanish tweets</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>LÃ³pez Ãšbeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza Del Arco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>DÃ­az Galiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Urena Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-056-4_077</idno>
		<ptr target="https://aclanthology.org/R19-1077.doi:10.26615/978-954-452-056-4_077" />
	</analytic>
	<monogr>
		<title level="m" coord="8,322.20,573.88,185.63,9.74;8,107.59,587.43,326.80,9.74">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="655" to="663" />
		</imprint>
	</monogr>
	<note>Martin</note>
</biblStruct>

<biblStruct coords="8,107.59,628.08,399.60,9.74;8,107.59,641.63,398.39,9.74;8,107.59,655.17,399.60,9.74;8,107.59,668.72,311.72,9.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,256.66,628.08,64.05,9.74">TUA1 at erisk</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper_121.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,185.14,641.63,320.83,9.74;8,107.59,655.17,25.85,9.74">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,384.29,655.17,122.89,9.74;8,107.59,668.72,21.79,9.74">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Soulier</surname></persName>
		</editor>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-10">2018. September 10-14, 2018. 2125. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,88.09,398.39,9.74;9,107.59,101.64,293.87,9.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,351.90,88.09,154.08,9.74;9,107.59,101.64,181.08,9.74">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,326.44,102.88,18.39,8.14">r X i v</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8 1 0</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,115.19,400.23,9.74;9,107.59,128.74,398.39,9.74;9,107.59,142.29,398.40,9.74;9,107.59,155.84,400.07,9.74;9,107.34,169.38,209.19,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,298.72,115.19,209.10,9.74;9,107.59,128.74,324.87,9.74">Predicting responses to psychological questionnaires from participants&apos; social media posts and question text embeddings</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdurahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.137</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.137.doi:10.18653/v1/2020.findings-emnlp.137" />
	</analytic>
	<monogr>
		<title level="m" coord="9,456.44,128.74,49.54,9.74;9,107.59,142.29,398.40,9.74;9,107.59,155.84,45.65,9.74">Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1512" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,182.93,398.68,9.74;9,107.59,196.48,398.39,9.74;9,107.59,210.03,328.72,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,352.34,182.93,153.93,9.74;9,107.59,196.48,113.45,9.74">Overview of erisk 2023: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>MartÃ­n Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.35,196.48,261.63,9.74;9,107.59,210.03,47.12,9.74">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,223.58,398.40,9.74;9,107.59,237.13,253.81,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,410.78,223.58,95.21,9.74;9,107.59,237.13,141.16,9.74">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,256.93,237.13,56.95,9.74">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,250.68,400.24,9.74;9,107.59,264.23,400.08,9.74;9,107.59,277.78,391.71,9.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,321.40,250.68,186.42,9.74;9,107.59,264.23,169.39,9.74">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715.doi:10.5281/zenodo.5297715" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using these metadata</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
