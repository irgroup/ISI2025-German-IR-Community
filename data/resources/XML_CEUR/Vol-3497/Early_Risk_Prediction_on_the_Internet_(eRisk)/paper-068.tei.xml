<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.30,388.10,15.42;1,89.29,107.22,151.17,15.42">Strategies to Harness the Transformers&apos; Potential: UNSL at eRisk 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,135.53,94.45,11.96"><forename type="first">Horacio</forename><surname>Thompson</surname></persName>
							<email>hjthompson@unsl.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de San Luis (UNSL)</orgName>
								<address>
									<addrLine>Ej√©rcito de Los Andes 950</addrLine>
									<postCode>5700</postCode>
									<settlement>San Luis</settlement>
									<region>C.P</region>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Consejo Nacional de Investigaciones Cient√≠ficas y T√©cnicas (CONICET)</orgName>
								<address>
									<settlement>San Luis</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.67,135.53,76.31,11.96"><forename type="first">Leticia</forename><surname>Cagnina</surname></persName>
							<email>lcagnina@unsl.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de San Luis (UNSL)</orgName>
								<address>
									<addrLine>Ej√©rcito de Los Andes 950</addrLine>
									<postCode>5700</postCode>
									<settlement>San Luis</settlement>
									<region>C.P</region>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Consejo Nacional de Investigaciones Cient√≠ficas y T√©cnicas (CONICET)</orgName>
								<address>
									<settlement>San Luis</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.26,135.53,88.49,11.96"><forename type="first">Marcelo</forename><surname>Errecalde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de San Luis (UNSL)</orgName>
								<address>
									<addrLine>Ej√©rcito de Los Andes 950</addrLine>
									<postCode>5700</postCode>
									<settlement>San Luis</settlement>
									<region>C.P</region>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.30,388.10,15.42;1,89.29,107.22,151.17,15.42">Strategies to Harness the Transformers&apos; Potential: UNSL at eRisk 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C2571FC7A1D577CB35C070F6DF83E26D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transformers</term>
					<term>Information Retrieval</term>
					<term>Prompting</term>
					<term>Early Risk Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three finetuned models followed by decision policy according to criteria defined by an early detection framework. One model presented extended vocabulary with important words to the addressed domain. In the last task, we obtained good performances considering the decision-based metrics, ranking-based metrics, and runtime. In this work, we explore different ways to deploy the predictive potential of Transformers in eRisk tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Early Risk Prediction on the Internet (eRisk) laboratory proposes solving different challenges to explore evaluation methodologies, effectiveness metrics, and practical applications for risk detection in social networks. Through its editions <ref type="bibr" coords="1,356.22,496.90,11.49,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,370.69,496.90,7.52,10.91" target="#b1">2,</ref><ref type="bibr" coords="1,381.19,496.90,7.52,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,391.69,496.90,7.52,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,402.19,496.90,7.52,10.91" target="#b4">5,</ref><ref type="bibr" coords="1,412.69,496.90,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="1,423.19,496.90,7.52,10.91" target="#b6">7,</ref><ref type="bibr" coords="1,433.69,496.90,7.65,10.91" target="#b7">8]</ref>, several tasks have been proposed on different domains, promoting the participating teams to propose innovative solutions that solve the tasks in the best possible way. Our research group has actively participated in eRisk editions with notable contributions <ref type="bibr" coords="1,343.35,537.55,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="1,357.44,537.55,12.55,10.91" target="#b9">10,</ref><ref type="bibr" coords="1,372.72,537.55,12.55,10.91" target="#b10">11,</ref><ref type="bibr" coords="1,387.99,537.55,12.55,10.91" target="#b11">12,</ref><ref type="bibr" coords="1,403.28,537.55,12.32,10.91" target="#b12">13]</ref>. In the 2023 edition <ref type="bibr" coords="1,89.29,551.10,11.42,10.91" target="#b7">[8]</ref>, a new task was introduced: Task 1, which involved searching for symptoms of depression in a collection of user writings. Task 2 was a continuation of the 2022 edition of the problem on early risk detection of pathological gambling. Finally, Task 3 consisted of estimating the severity level of signs of eating disorders.</p><p>The neural architectures known as Transformers proposed by Vaswani et al. <ref type="bibr" coords="2,427.03,86.97,17.75,10.91" target="#b13">[14]</ref> have caused a true revolution in the artificial intelligence field. Numerous studies have shown the performance of Transformers to solve a wide variety of natural language processing tasks, with models such as BERT <ref type="bibr" coords="2,129.57,127.61,16.29,10.91" target="#b14">[15]</ref>, GPT-2 <ref type="bibr" coords="2,183.53,127.61,16.29,10.91" target="#b15">[16]</ref>, and GPT-3 <ref type="bibr" coords="2,256.73,127.61,16.29,10.91" target="#b16">[17]</ref>. Motivated by the relevant role of Transformers, we have proposed to address this year's tasks by applying approaches based on this architecture.</p><p>Our research group participated in Tasks 1 and 2, focusing on strategies that take advantage of the predictive power of Transformers. For Task 1, we presented three proposals. The first two were based on measuring the similarity of embeddings extracted from pre-trained models. We represented the writings of the collection and the symptoms of the BDI Questionnaire using verbs, adjectives, and nouns. From these terms, we obtained word embeddings using a BERT model adjusted for depression tasks, which allowed us to measure the closeness between the writings and the symptoms considering the task domain and then obtain the final rankings. The third solution consisted of applying one of the most attractive approaches currently known as prompting, which takes advantage of the predictive power of a pre-trained language model, adapting it to a particular task <ref type="bibr" coords="2,225.61,276.66,16.18,10.91" target="#b17">[18]</ref>. We applied the Fixed-prompt LM Tuning technique <ref type="bibr" coords="2,474.80,276.66,17.84,10.91" target="#b18">[19]</ref> by fitting the RoBERTa model <ref type="bibr" coords="2,209.59,290.20,17.82,10.91" target="#b19">[20]</ref> on samples containing pre-defined prompts. Since labeled data were unavailable, we created a dataset of 200 samples per symptom using ChatGPT <ref type="bibr" coords="2,467.72,303.75,16.37,10.91" target="#b20">[21]</ref>. We created prompts by concatenating each sample to a simple template with slots to fill. Then, we tuned the RoBERTa model to solve the missing word on these prompts, continuing its previous training. We evaluated the texts of the collection by predicting the prompts, and we associated the responses of the model with the probability of belonging to each one of the symptoms. Finally, we improved the ranking of two of the 21 symptoms with a multiclass classifier obtained by fine-tuning on the created dataset. For Task 2, we used an early detection framework <ref type="bibr" coords="2,486.76,385.05,16.34,10.91" target="#b21">[22]</ref>, which led us to remarkable results in previous editions <ref type="bibr" coords="2,341.12,398.60,16.56,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,360.52,398.60,12.42,10.91" target="#b12">13]</ref>. The method defines that, to solve an early detection problem, it is necessary to consider two components: one dedicated to solving a user classification problem (classification with partial information or CPI), and the other involves a decision policy to decide when to stop evaluating a user (deciding the moment of classification or DMC). On this occasion, we presented three proposals, for which we applied the BERT model with some variants (CPI component) and defined a decision policy based on the history of predictions that a model performs during user evaluation (DMC component).</p><p>Our main contribution in this edition was the application of different strategies based on Transformers. We applied novel techniques that may be of interest to works related to information retrieval. For the early detection problem, combining fine-tuned models with a decision policy based on a historic rule allowed us to maximize performance and obtain good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 1: Search for symptoms of depression</head><p>From a collection of user posts, the task consisted of building rankings of relevant posts considering the 21 depression symptoms of the BDI Questionnaire. A relevant sentence for a symptom S was defined as the user-generated text that provides information about their condition related to S. According to official data, the collection contains 3,807,115 sentences extracted from 3,107 users. Teams could submit up to five proposals, each with 21 rankings of up to 1,000 writings in descending order by score. Our research group put forward three solutions: two based on the similarity of contextualized embeddings and one employing the prompting technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Similarity-based proposals</head><p>The strategies consisted of measuring the closeness of the writings to each of the symptoms.</p><p>Sentences filter. Due to the large number of samples in the collection, a filter was initially applied to reduce the search space. For this, we analyzed the sentiment of the texts with the VADER processor <ref type="bibr" coords="3,171.18,193.72,16.29,10.91" target="#b22">[23]</ref>, and we selected those whose negative polarity score was greater than 0. This fact allowed us to reduce the search space from almost 4 million writings to 1 million, which meant that 25% had some indication of negativity.</p><p>Preprocessing steps. Characters were converted to lowercase, while Unicode and HTML codes were transformed into their corresponding symbols. Web pages and numbers were replaced by the weblink and number tokens, respectively. Repeated words and spaces were also removed, and emojis were manually replaced with text representations related to the symptoms.</p><p>Symptoms and writings representation. One way to understand a text is to direct the analysis toward important words that describe its semantics <ref type="bibr" coords="3,358.39,307.79,16.22,10.91" target="#b23">[24]</ref>. To represent each symptom, ten verbs, ten adjectives, and ten nouns were chosen, which were manually selected based on the information provided in the BDI Questionnaire. To represent writings, we used the Spacy parser to extract verbs, adjectives, and nouns. Stopwords were discarded, except those words that were present in the symptoms. The next step was to represent the symptoms and writings through word embeddings considering the context of the addressed domain. We employed a transformer-based approach instead of non-contextualized methods such as Word2Vec <ref type="bibr" coords="3,487.92,389.08,18.07,10.91" target="#b24">[25]</ref> and FastText <ref type="bibr" coords="3,150.34,402.63,16.41,10.91" target="#b25">[26]</ref>. Thus, a context was defined for each word, and a language model was used to extract its embedding vector. For instance, to represent feeling in the context of the symptom sadness, it was defined as feeling is linked to the symptom sadness. Subsequently, embeddings were obtained using the last layer of a BERT model tuned on samples from users with depression<ref type="foot" coords="3,159.82,455.07,3.71,7.97" target="#foot_0">1</ref> .</p><p>Embeddings similarity. The verbs, adjectives, and nouns of the texts were compared with the verbs, adjectives, and nouns of the symptoms using cosine similarity, obtaining a table of scores as follows:</p><formula xml:id="formula_0" coords="3,166.36,522.14,262.55,57.47">‚àÄ text T, symptom S: ‚àÄ V ùëá in verbs(T), V ùëÜ in verbs(S): context_V ùëá = "V ùëá is linked to the symptom S" emb_V ùëá = extract_embedding(BERT, context_V ùëá ) context_V ùëÜ = "V ùëÜ is linked to the symptom S"</formula><p>emb_V ùëÜ = extract_embedding(BERT, context_V ùëÜ ) similarity = similarity(emb_V ùëá , emb_V ùëÜ ) Repeat for adjectives and nouns Summary of scores. To summarize the values of a text according to symptoms, two strategies were considered:</p><p>‚Ä¢ Similarity-MAX: The score of each text is summarized considering the verb, adjective, and noun with the maximum similarity, that is, those closest to the symptom, and then the three values are averaged. ‚Ä¢ Similarity-AVG: The score of each text is summarized considering the verb, adjective, and noun with the best average, and then the values obtained are averaged. Unlike the previous one, the word that, on average, was closest to the symptom is taken to reach the final score.</p><p>Final rankings. For both strategies, the final ranking was created by ordering each symptom by score and extracting the first 1,000 writings. Figures <ref type="figure" coords="4,338.33,245.53,5.10,10.91" target="#fig_0">1</ref> and<ref type="figure" coords="4,365.40,245.53,5.10,10.91" target="#fig_1">2</ref> show the graphic scheme and the implementation in a tabular format of similarity-based proposals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Prompting-based proposals</head><p>Prompting aims to reformulate the original task as a masked language problem and take advantage of the predictive models' ability to complete the missing words of an input text. The method is divided into two steps: 1) create prompts, where a text fragment with slots to fill (template) is added to each original sentence that the model must complete; 2) derive the final answer, where the prediction of the model is mapped to an adequate answer for the original task. Finding suitable prompts can be challenging as it influences the solution's success. It can be solved by manually searching for the best prompt <ref type="bibr" coords="5,327.72,141.16,17.96,10.91" target="#b26">[27]</ref> or by automatic techniques such as soft-prompting <ref type="bibr" coords="5,155.27,154.71,16.30,10.91" target="#b27">[28,</ref><ref type="bibr" coords="5,174.16,154.71,12.23,10.91" target="#b28">29]</ref>. On the other hand, Fixed-prompt LM Tuning is based on fixing a prompt and continuing the training of the language model to improve its predictions. In a few-shot scenario, large language models are generally used due to the lack of labeled samples but can only be accessed via an API interface. Because of this, and according to Task 1, we decided to use a small and adjustable model based on Transformers, applying the prompting paradigm through the Fixed-prompt LM Tuning technique.</p><p>Sentences filter and preprocessing steps. As in the similarity-based proposals, writings that presented negative scores were selected to reduce the search space with the VADER processor, and the same preprocessing was also applied. Language Model Tuning. The RoBERTa pre-trained model was imported and tuned based on the masked language problem as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset creation</head><p>‚àÄ sample &lt;T, S&gt; label = T + "This is linked to S" prompt = T + "This is linked to MASK" pred = RoBERTa(prompt) loss = CrossEntropy(pred, label) ...</p><p>In this way, the model learned to complete missing words, considering the predefined prompt and the samples associated with the symptoms, directing the model's answers toward the task to be solved. For instance, for the sample &lt;"I've been feeling sad all week", sadness&gt;, the prompt "I've been feeling sad all week. This is linked to MASK" is formed; then, the model should predict that the best word for MASK is sadness.</p><p>Evaluation of the writings. The texts of the collection were evaluated using the prompting scheme. We defined the prompt using the same template with which the model was tuned. We also created a words dictionary (verbalizer) for mapping model predictions to the final output, including the words used to create the dataset. In this way, the tuned model was used to predict the probability that the writings were linked to each symptom.</p><p>Preliminary ranking. A table of results was obtained with the first 1,000 writings ordered by probability in decreasing order for each symptom.</p><p>When inspecting the writings obtained in the rankings, we observed that samples not related to depression were found in two of the 21 symptoms: worthlessness and loss of energy. For the former, finance and economic crisis were the main topics, while for the latter, they were renewable energy, and oil, among others. On the other hand, the ranking of indecisiveness and fatigue had potential writings of worthlessness and loss of energy, respectively. Because the probabilities are correlated, the possibility of belonging to one symptom affects the rest. Therefore, we improved the quality of these two symptoms with an extension of the proposal, re-evaluating the writings using a classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning RoBERTa for worthlessness y loss-energy.</head><p>A multiclass classification problem was formulated considering the labels: worthlessness, loss-energy, and others. The training set was defined as follows: for worthlessness and loss-energy, we used samples extracted from the previously created dataset; for others, the remaining symptoms were used, excluding sentences with the indecisiveness and fatigue labels. Besides, we added texts related to monetary value loss.</p><p>The others class attracted those samples that should not be part of worthlessness and loss-energy.</p><p>Then, fine-tuning was applied to the RoBERTa model on this training set.</p><p>Classification of the writings. The texts were evaluated by the classification model, recording the probability of each prediction in a new table. The ranking for worthlessness and loss-energy was created, and we selected the first 1,000 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final ranking (Prompting-Classifier).</head><p>The final proposal consisted of 19 symptom rankings using prompting and 2 rankings using the classifier explained above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Results</head><p>For evaluating the teams, a writing pool for each symptom was built by selecting the first 50 sentences from all proposals (in total, 37). The organizers carried out a labeling process with three assessors that manually chose which writings were relevant <ref type="bibr" coords="6,415.13,471.00,11.58,10.91" target="#b7">[8]</ref>, resulting in two evaluation schemes: majority voting (if 2/3 agreed) and unanimity (if 3/3 agreed). Table <ref type="table" coords="6,470.97,484.55,4.97,10.91" target="#tab_1">1</ref> shows an extract of the result of the labeling process. Tables <ref type="table" coords="7,132.36,86.97,5.17,10.91">2</ref> and<ref type="table" coords="7,160.73,86.97,5.17,10.91">3</ref> show the results obtained, considering the majority voting and unanimity schemes. The best results were achieved by Formula-ML. The mean, median, and performance distribution of all proposals (Figure <ref type="figure" coords="7,249.23,114.06,4.17,10.91" target="#fig_3">3</ref>) show that most teams maintained a considerably lower performance than Formula-ML. Prompting-Classifier stands out among our proposals with a performance close to that of most teams, considering the AP, R-PREC, and NDCG@1000 metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Ranking-based evaluation for Task 1. Results are reported according to the metrics Average Precision (AP), R-Precision (R-PREC), Precision at 10 (P@10), and NDCG at 1000 (NDCG@1000), for the majority voting scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Ranking-based evaluation for Task 1 according to the unanimity scheme. Several factors may have influenced the general performance of the models, such as the labeling process or the way of evaluating them. As observed in Table <ref type="table" coords="7,395.00,516.89,3.68,10.91" target="#tab_1">1</ref>, the number of relevant sentences was considerably less than the original number of samples extracted from all the proposals, and, in turn, distant values are observed between majority voting and unanimity for each symptom. This fact could indicate that the assessors had different opinions when interpreting sentences, probably due to the concept of relevance. On the other hand, it would be important to consider the performance of the models at the symptom level since there were significant differences in the number of relevant sentences between symptoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 2: Early Detection of Signs of Pathological Gambling</head><p>The goal was to detect, as early as possible, users that showed signs of pathological gambling. The challenge was divided into two stages: a training stage, where the participants experimented  with data extracted from previous editions, and a test stage, where a client application interacted with a server, defining an early environment. This last process was divided into rounds in which the client requested the next post of users and, according to the number of predictive models, evaluated them and returned a response to the server.</p><p>Early risk detection can be analyzed as a multi-objective problem, where the challenge is to find an adequate balance between the precision in identifying risky users and the minimum time required for that decision to be reliable. In <ref type="bibr" coords="8,313.84,558.96,16.41,10.91" target="#b12">[13]</ref>, our research group applied the early classification framework <ref type="bibr" coords="8,197.85,572.51,17.76,10.91" target="#b21">[22]</ref> by using a BERT model with extended vocabulary (CPI component) and a decision policy based on a historic rule (DMC component). In this edition, we presented three proposals, improving the CPI and DMC components to maximize the performance of the final models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>Table <ref type="table" coords="9,115.21,107.54,4.97,10.91">4</ref> shows the detail of the corpora available to solve the task. The eRisk2021 and eRisk2022 corpora were used to train the models, as well as UNSL2021_train and UNSL2021_valid created in <ref type="bibr" coords="9,100.11,134.63,16.08,10.91" target="#b11">[12]</ref>. The eRisk2023 corpus was used for the organizers to evaluate the participating models. It contains 4.7% of positive users compared to 7% and 3.9% of eRisk2021 and eRisk2022, respectively. Furthermore, the number of words per post was considerably higher than in previous editions, which may be a relevant factor for the models' performance when evaluating longer posts. On the other hand, the UNSL2021_train and UNSL2022_valid corpora have considerably fewer posts than the eRisk corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Details of the corpora used for Task 2. The corpora of the different eRisk editions are shown, as well as the corpora created by our team in previous editions. The number of users (total, positives, and negatives) and the number of posts of each corpus are reported. The median, minimum, and maximum number of posts per user and words per post in each corpus are detailed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CPI components: Models</head><p>Each model was trained and validated by combining different corpora with an 85/15 split. We used the BERT model applying the fine-tuning process to adjust it to the classification task. A limitation of the BERT architecture is that it only supports 512 input tokens. Thus, we improved the posts extracted from each user by selecting those posts with some indication of negativity by the VADER processor, and the first 512 tokens were taken. We also used a scheduler to automatically adjust the Learning Rate during fine-tuning, improving the convergence and performance of the model. Finally, the best model for each proposal was chosen considering the F1 metric over the positive class (F 1 +).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNSL#0: Classic BERT model</head><p>Training set. Combination of the eRisk2021, UNSL2021_train, and UNSL2021_valid corpora. Extraction of posts with some indication of negativity. Preprocessing steps. Characters were converted to lowercase, while Unicode and HTML codes were transformed into their corresponding symbols. Web pages and numbers were replaced by the weblink and number tokens, respectively. Repeated words and spaces were also removed. Hyperparameters for fine-tuning. Architecture = 'BERT-based-uncased', optimizer = 'AdamW', LR = 3E-5, scheduler = 'LinearSchedulerWarmup', batch_size = 8, and n_epochs = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNSL#1: BERT model with an extended vocabulary</head><p>We extended the BERT vocabulary using important words to the addressed domain extracted from an external model. The SS3 model <ref type="bibr" coords="10,277.41,86.97,18.07,10.91" target="#b29">[30]</ref> was trained to classify users on the available corpora, and we selected the first 40 words according to the confidence values on the positive class.</p><p>Training set, preprocessing steps, and hyperparameters for fine-tuning. The same as the UNSL#0 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNSL#2: Classic BERT model on all available data</head><p>Considering the same hyperparameters of the UNSL#0 model, the fine-tuning process was applied using all available data (eRisk2021, eRisk2022, UNSL2021_train, and UNSL2021_valid).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DMC component: Decision Policy</head><p>The best decision policy for the models described above was evaluated using a mock server <ref type="foot" coords="10,501.02,232.62,3.71,7.97" target="#foot_1">2</ref> . This tool simulates the eRisk challenge through the rounds of posts and answers submissions, and then it calculates the final results according to the decision and ranking-based metrics. It was useful since the performance of CPI models can drastically change when evaluated in an early environment. A client application was defined to manage the interaction with the server. When it receives a round of posts, the system preprocesses the writings, invokes the predictive models (CPI), and applies a decision policy (DMC). To take advantage of the 512 input tokens that the BERT architecture admits, the application uses the last N=10 posts (posts window), linking the current post with previous posts. With the mock server, the client application, and the predictive models, different decision policies were evaluated using the F 1 + and F ùëôùëéùë°ùëíùëõùëêùë¶ metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision policy based on a historic rule</head><p>The historic rule defines that if the current prediction and the last M predictions exceed a threshold (limit probability to predict a positive user), the client application must issue a risky user alarm; otherwise, it is necessary to continue the user evaluation. In addition, the rule has the min_delay parameter, which defines the moment when it will start to apply. We obtained that the best parameters were threshold = 0.7, M = 10, and min_delay = 10.</p><p>Performance in an early environment. The models were evaluated using the eRisk2022 corpus. UNSL#0: F 1 + = 0.88 and F ùëôùëéùë°ùëíùëõùëêùë¶ = 0.83; UNSL#1: F 1 + = 0.84 and F ùëôùëéùë°ùëíùëõùëêùë¶ = 0.81. UNSL#2 was not tested as it was trained with all the corpora, including eRisk2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>Table <ref type="table" coords="10,114.80,544.37,4.97,10.91" target="#tab_5">5</ref> shows the results obtained by our team according to the decision-based metrics. The best results were achieved by ELiRF-UPV#0, as well as other proposals had good results. Considering the average level among all the teams, our models achieved remarkable results in the F 1 , ERDE 50 , and F ùëôùëéùë°ùëíùëõùëêùë¶ metrics. The UNSL#1 and UNSL#2 models showed similar performance, outperforming UNSL#0 on the same metrics. Regarding ERDE 5 , the three models obtained the same performance as the mean among all teams. Regarding the ranking-based metrics, the teams that achieved the best results considering 1, 100, 500, and 1000 posts were ELiRF-UPV, NLP-UNED-2, OBSER-MENH, and UNSL. As can be seen in Table <ref type="table" coords="11,165.95,270.32,3.81,10.91" target="#tab_6">6</ref>, our team obtained the best results for the P@10 and NDCG@10 metrics. For NDCG@100, acceptable values were obtained, mainly with 100 posts. Besides, as in the decision-based metrics, UNSL#1 and UNSL#2 achieved similar results, outperforming UNSL#0. Finally, Table <ref type="table" coords="11,164.04,536.80,5.17,10.91">7</ref> shows the total time spent by each team to solve the task. Our team was the fastest, presenting three models with a final delay of 1 day and 2 hours, followed by the ELiRF-UPV's model (almost 11 hours of difference). The rest of the teams used five models to solve the task, with delays ranging from 4 to 54 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Total time spent by each team for Task 2. The team name, number of models, and number of user posts processed are shown. The teams are displayed according to the total time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this article, the UNSL team solved Tasks 1 and 2 of the eRisk 2023 Laboratory. For Task 1, we applied different approaches, obtaining better results with the proposal based on prompting.</p><p>Although we did not get the best results for this task, it would be interesting to improve these approaches considering the criteria used in the labeling process. For Task 2, we obtained outstanding results in all evaluation metrics, applying a model with extended vocabulary and a decision policy based on a historic rule. Our proposals harnessed the predictive potential of Transformers, demonstrating that these architectures can be used in information retrieval tasks and problems of early risk detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,409.65,416.69,8.93;4,89.29,421.61,417.29,8.96;4,89.29,433.56,61.73,8.96;4,89.29,281.11,166.72,103.19"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphic scheme of similarity-based proposals. The distance of a text is observed considering its verb and the verbs of each one of the symptoms (S0, S1,..., S20) for (a) Similarity-MAX and (b) Similarity-AVG.</figDesc><graphic coords="4,89.29,281.11,166.72,103.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,567.27,416.70,8.93;4,89.29,579.23,416.69,8.96;4,89.29,591.18,61.73,8.96;4,89.29,471.03,416.70,88.81"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tabular scheme of the similarity-based proposals implementation. The similarities between text T and the symptoms are saved in a scores table and summarized according to Similarity-MAX and Similarity-AVG.</figDesc><graphic coords="4,89.29,471.03,416.70,88.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,89.29,416.48,417.29,8.93;8,89.29,428.43,416.69,8.96;8,88.99,440.44,101.58,8.87;8,151.80,247.12,291.69,144.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance distribution of the proposals presented by all teams for Task 1, considering (a) Majority voting and (b) Unanimity. The performances of our team are shown with X marks. The mean (dashed line) is included.</figDesc><graphic coords="8,151.80,247.12,291.69,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.93,527.11,417.05,144.98"><head>Table 1</head><label>1</label><figDesc>Extract from the count of relevant sentences for each symptom of the BDI Questionnaire. Original: a writing pool with the first 50 sentences extracted from proposals of all teams. Majority voting: 2 of 3 assessors agreed. Unanimity: 3 of 3 agreed.</figDesc><table coords="6,158.04,579.09,279.20,93.00"><row><cell>Symptom</cell><cell cols="3">Original Majority voting Unanimity</cell></row><row><cell>Sadness</cell><cell>1110</cell><cell>318</cell><cell>179</cell></row><row><cell>Pessimism</cell><cell>1150</cell><cell>325</cell><cell>104</cell></row><row><cell>Past Failure</cell><cell>973</cell><cell>300</cell><cell>160</cell></row><row><cell>Loss of Pleasure</cell><cell>1013</cell><cell>204</cell><cell>97</cell></row><row><cell>Guilty Feelings</cell><cell>829</cell><cell>143</cell><cell>83</cell></row><row><cell>Punishment Feelings</cell><cell>1079</cell><cell>50</cell><cell>21</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,88.99,90.49,418.09,155.52"><head>Table 5</head><label>5</label><figDesc>Decision-based evaluation results for Task 2. The best team taking into account the F 1 , ERDE 5 , ERDE 50 , and F ùëôùëéùë°ùëíùëõùëêùë¶ is shown (values in bold), as well as the mean and median values of the results report for CLEF eRisk 2023. The second-best teams are also included.</figDesc><table coords="11,100.92,140.21,392.68,105.79"><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell cols="4">ERDE 5 ERDE 50 latencyTP speed F ùëôùëéùë°ùëíùëõùëêùë¶</cell></row><row><cell>UNSL#0</cell><cell cols="3">0.752 0.767 0.760</cell><cell>0.048</cell><cell>0.017</cell><cell>15.0</cell><cell>0.945</cell><cell>0.718</cell></row><row><cell>UNSL#1</cell><cell cols="3">0.79 0.806 0.798</cell><cell>0.048</cell><cell>0.014</cell><cell>13.0</cell><cell>0.953</cell><cell>0.761</cell></row><row><cell>UNSL#2</cell><cell cols="3">0.752 0.854 0.800</cell><cell>0.048</cell><cell>0.013</cell><cell>14.0</cell><cell>0.949</cell><cell>0.759</cell></row><row><cell>ELiRF-UPV#0</cell><cell cols="3">1.000 0.883 0.938</cell><cell>0.026</cell><cell>0.010</cell><cell>4.0</cell><cell>0.988</cell><cell>0.927</cell></row><row><cell cols="4">NLP-UNED-2#1 0.957 0.883 0.919</cell><cell>0.034</cell><cell>0.016</cell><cell>13.0</cell><cell>0.953</cell><cell>0.876</cell></row><row><cell cols="4">NLP-UNED-2#4 0.764 0.883 0.819</cell><cell>0.033</cell><cell>0.010</cell><cell>13.0</cell><cell>0.953</cell><cell>0.781</cell></row><row><cell>Mean</cell><cell cols="3">0.390 0.796 0.367</cell><cell>0.048</cell><cell>0.035</cell><cell>19.12</cell><cell>0.932</cell><cell>0.362</cell></row><row><cell>Median</cell><cell cols="3">0.092 0.903 0.125</cell><cell>0.047</cell><cell>0.042</cell><cell>8.00</cell><cell>0.973</cell><cell>0.162</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,88.99,326.43,416.99,193.97"><head>Table 6</head><label>6</label><figDesc>Ranking-based evaluation results for Task 2. Results are reported according to the three classification metrics obtained after processing 1, 100, 500, and 1000 posts, respectively.</figDesc><table coords="11,172.81,366.42,249.65,153.97"><row><cell>Ranking</cell><cell>Metric</cell><cell cols="3">UNSL#0 UNSL#1 UNSL#2</cell></row><row><cell></cell><cell>P@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>1 post</cell><cell>NDCG@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>NDCG@100</cell><cell>0.46</cell><cell>0.57</cell><cell>0.55</cell></row><row><cell></cell><cell>P@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>100 posts</cell><cell>NDCG@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>NDCG@100</cell><cell>0.70</cell><cell>0.78</cell><cell>0.75</cell></row><row><cell></cell><cell>P@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>500 posts</cell><cell>NDCG@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>NDCG@100</cell><cell>0.64</cell><cell>0.67</cell><cell>0.69</cell></row><row><cell></cell><cell>P@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell cols="2">1000 posts NDCG@10</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>NDCG@100</cell><cell>0.64</cell><cell>0.70</cell><cell>0.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,92.26,671.00,277.69,8.97"><p>Available in: https://huggingface.co/BitanBiswas/depression-detection-bert.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="10,92.26,671.02,224.71,8.97"><p>Available in: https://github.com/jmloyola/erisk_mock_server.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,473.44,394.61,10.91;12,112.66,486.99,394.52,10.91;12,112.66,500.54,45.01,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,223.60,473.44,264.55,10.91">A test collection for research on depression and language use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,486.99,291.91,10.91">Proc. of Conference and Labs of the Evaluation Forum (CLEF 2016)</title>
		<meeting>of Conference and Labs of the Evaluation Forum (CLEF 2016)<address><addrLine>Evora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,514.09,393.33,10.91;12,112.66,527.64,393.32,10.91;12,112.66,541.19,319.84,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,278.10,514.09,22.17,10.91;12,329.82,514.09,176.17,10.91;12,112.66,527.64,154.10,10.91">Clef lab on early risk prediction on the internet: experimental foundations</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,289.99,527.64,215.99,10.91;12,112.66,541.19,188.80,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
	<note>erisk</note>
</biblStruct>

<biblStruct coords="12,112.66,554.74,393.33,10.91;12,112.66,568.29,393.53,10.91;12,112.66,581.84,221.51,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,287.49,554.74,218.50,10.91;12,112.66,568.29,34.35,10.91">Overview of erisk: early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,177.78,568.29,328.41,10.91;12,112.66,581.84,90.47,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="343" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,595.39,393.33,10.91;12,112.66,608.93,394.61,10.91;12,112.41,622.48,394.78,10.91;12,112.66,636.03,297.11,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,286.21,595.39,219.78,10.91;12,112.66,608.93,51.44,10.91">Overview of erisk 2019 early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,187.09,608.93,320.18,10.91;12,112.41,622.48,294.08,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 10th International Conference of the CLEF Association, CLEF 2019</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="340" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,649.58,394.61,10.91;13,112.28,86.97,394.91,10.91;13,112.66,100.52,389.00,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,275.02,649.58,21.86,10.91;12,325.80,649.58,161.55,10.91">Self-harm and depression challenges</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.28,86.97,390.46,10.91">Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-04-14">2020. April 14-17, 2020. 2020</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="557" to="563" />
		</imprint>
	</monogr>
	<note>erisk</note>
</biblStruct>

<biblStruct coords="13,112.66,114.06,393.61,10.91;13,112.66,127.61,393.33,10.91;13,112.66,141.16,269.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,354.99,114.06,151.28,10.91;13,112.66,127.61,110.69,10.91">Overview of erisk 2021: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,245.65,127.61,260.33,10.91;13,112.66,141.16,138.12,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="324" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,393.61,10.91;13,112.66,168.26,393.33,10.91;13,112.66,181.81,394.52,10.91;13,112.66,195.36,303.82,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,354.99,154.71,151.28,10.91;13,112.66,168.26,112.14,10.91">Overview of erisk 2022: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,247.41,168.26,258.58,10.91;13,112.66,181.81,348.32,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 5-8, 2022. 2022</date>
			<biblScope unit="page" from="233" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,393.61,10.91;13,112.66,222.46,393.33,10.91;13,112.66,236.01,393.53,10.91;13,112.66,249.56,233.60,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,352.14,208.91,154.12,10.91;13,112.66,222.46,112.47,10.91">Overview of eRisk 2023: Early Risk Prediction on the Internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,247.81,222.46,258.17,10.91;13,112.66,236.01,348.05,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 14th International Conference of the CLEF Association, CLEF 2023</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,263.11,393.33,10.91;13,112.39,276.66,394.80,10.91;13,112.66,290.20,22.69,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,462.65,263.11,43.34,10.91;13,112.39,276.66,269.20,10.91">Temporal variation of terms as concept space for early risk prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Funez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J G</forename><surname>Ucelay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Cagnina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Clef (working notes</note>
</biblStruct>

<biblStruct coords="13,112.66,303.75,394.53,10.91;13,112.66,317.30,376.21,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Funez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J G</forename><surname>Ucelay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Cagnina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y G√≥mez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<title level="m" coord="13,175.15,317.30,159.42,10.91;13,360.25,317.30,98.72,10.91">Unsl&apos;s participation at erisk 2018 lab</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>CLEF (Working Notes)</note>
</biblStruct>

<biblStruct coords="13,112.66,330.85,393.33,10.91;13,112.66,344.40,393.33,10.91;13,112.66,357.95,56.77,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,338.00,330.85,167.99,10.91;13,112.66,344.40,290.71,10.91">Unsl at erisk 2019: a unified approach for anorexia, self-harm and depression detection in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y G√≥mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,434.93,344.40,71.06,10.91;13,112.66,357.95,26.87,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,394.62,10.91;13,112.28,385.05,393.70,10.91;13,112.66,398.60,119.73,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,424.33,371.50,56.88,10.91;13,112.28,385.05,293.71,10.91">A comparison of three early alert policies for early risk detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Cagnina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.20,385.05,70.78,10.91;13,112.66,398.60,26.87,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="992" to="1021" />
		</imprint>
	</monogr>
	<note>Unsl at erisk</note>
</biblStruct>

<biblStruct coords="13,112.66,412.15,393.33,10.91;13,112.26,425.70,188.05,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<title level="m" coord="13,348.93,412.15,157.06,10.91;13,112.26,425.70,156.13,10.91">Unsl at erisk 2022: Decision policies with history for early classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,439.25,395.17,10.91;13,112.66,452.79,393.33,10.91;13,112.33,466.34,29.19,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,484.04,439.25,23.79,10.91;13,112.66,452.79,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,264.71,452.79,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="13,112.66,479.89,393.33,10.91;13,112.66,493.44,363.59,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="13,353.43,479.89,152.55,10.91;13,112.66,493.44,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,506.99,393.33,10.91;13,112.66,520.54,253.81,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,412.10,506.99,93.89,10.91;13,112.66,520.54,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,262.00,520.54,56.95,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,534.09,394.53,10.91;13,112.66,547.64,393.32,10.91;13,112.66,561.19,266.90,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,274.48,547.64,169.72,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,452.29,547.64,53.69,10.91;13,112.66,561.19,172.82,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,574.74,393.71,10.91;13,112.66,588.29,393.33,10.91;13,112.66,601.84,104.03,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,358.17,574.74,148.20,10.91;13,112.66,588.29,310.91,10.91">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,431.32,588.29,74.67,10.91;13,112.66,601.84,35.31,10.91">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,615.39,393.59,10.91;13,112.66,628.93,146.44,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="13,194.23,615.39,277.81,10.91">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,642.48,394.53,10.91;13,112.30,656.03,393.68,10.91;13,112.66,669.58,107.17,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="13,173.53,656.03,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,86.97,371.29,10.91" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Chatgpt</forename><surname>Openai</surname></persName>
		</author>
		<ptr target="https://github.com/openai/chatgpt" />
		<imprint>
			<date type="published" when="2021-05-29">2021. May 29, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,100.52,393.33,10.91;14,112.66,114.06,393.33,10.91;14,112.66,127.61,394.53,10.91;14,112.66,141.16,70.43,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,424.04,100.52,81.94,10.91;14,112.66,114.06,153.73,10.91">Learning when to classify for early text classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes Y Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,289.14,114.06,216.85,10.91;14,112.66,127.61,39.32,10.91">Computer Science-CACIC 2017: 23rd Argentine Congress</title>
		<meeting><address><addrLine>La Plata, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">October 9-13, 2017. 2018</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers 23</note>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,393.33,10.91;14,112.66,181.81,158.66,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,206.21,154.71,299.77,10.91;14,112.66,168.26,71.87,10.91">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,206.53,168.26,299.46,10.91;14,112.66,181.81,24.53,10.91">Proceedings of the international AAAI conference on web and social media</title>
		<meeting>the international AAAI conference on web and social media</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,195.36,393.33,10.91;14,112.66,208.91,332.06,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">N D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<title level="m" coord="14,447.51,195.36,58.47,10.91;14,112.66,208.91,149.17,10.91">A structured self-attentive sentence embedding</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,222.46,393.33,10.91;14,112.39,236.01,230.07,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="14,298.68,222.46,207.31,10.91;14,112.39,236.01,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,249.56,393.33,10.91;14,112.66,263.11,395.01,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,331.70,249.56,174.29,10.91;14,112.66,263.11,49.97,10.91">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,170.36,263.11,261.21,10.91">Transactions of the association for computational linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,393.33,10.91;14,112.66,290.20,306.03,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="14,462.16,276.66,43.83,10.91;14,112.66,290.20,123.97,10.91">Language models as knowledge bases?</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,303.75,393.60,10.91;14,112.66,317.30,146.44,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06599</idno>
		<title level="m" coord="14,187.83,303.75,285.62,10.91">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,330.85,393.33,10.91;14,112.66,344.40,209.60,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<title level="m" coord="14,275.37,330.85,230.61,10.91;14,112.66,344.40,27.32,10.91">The power of scale for parameter-efficient prompt tuning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,357.95,393.53,10.91;14,112.66,371.50,393.33,10.91;14,112.26,385.05,168.62,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,346.48,357.95,159.71,10.91;14,112.66,371.50,317.52,10.91">A text classification framework for simple and effective early depression detection over social media streams</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y G√≥mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,438.66,371.50,67.33,10.91;14,112.26,385.05,79.61,10.91">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
