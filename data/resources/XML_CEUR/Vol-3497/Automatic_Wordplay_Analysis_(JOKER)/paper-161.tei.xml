<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.76,450.72,14.99;1,72.00,98.51,139.81,14.99;1,72.00,139.41,203.63,10.54">CLEF 2023 JOKER Task 1, 2, 3: Pun Detection, Pun Interpretation, and Pun Translation Notebook for the Joker Lab at CLEF 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,165.86,78.32,10.54"><forename type="first">Antonela</forename><surname>Prnjak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Split</orgName>
								<address>
									<addrLine>Ruđera Boškovića 31</addrLine>
									<settlement>Split</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.91,165.86,83.65,10.54"><forename type="first">Dennis</forename><forename type="middle">R</forename><surname>Davari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University od Kiel</orgName>
								<address>
									<addrLine>Christian-Albrechts-Platz 4</addrLine>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.29,165.86,79.00,10.54"><forename type="first">Kristina</forename><surname>Schmitt</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Malta</orgName>
								<orgName type="institution" key="instit2">University of Malta</orgName>
								<address>
									<postCode>MSD 2080</postCode>
									<settlement>Msida</settlement>
									<country key="MT">Malta</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.76,450.72,14.99;1,72.00,98.51,139.81,14.99;1,72.00,139.41,203.63,10.54">CLEF 2023 JOKER Task 1, 2, 3: Pun Detection, Pun Interpretation, and Pun Translation Notebook for the Joker Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">886118FC5454C96BF2D8EBEF992CCDF5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wordplay</term>
					<term>Pun</term>
					<term>Humor</term>
					<term>Detection</term>
					<term>Interpretation</term>
					<term>Translation 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans' approach towards language is incomparable as they use words creatively to express their thoughts and emotions in unique ways and to add humor, novelty, and aesthetic value to communication. Along these lines, a pun is a type of wordplay that exploits multiple meanings or similar sounds of different words for humorous or rhetorical effect and is a common notion in languages across the globe. This report focuses on the detection, interpretation, and translation of existing puns in a given data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>At present, machine translation is utilized by all professional translators as a foundation for comprehending the overall context of a text. However, one of the difficulties in translation nowadays is decoding and recoding humor, which is commonly considered to be untranslatable or challenging to translate. Indeed, conveying the humorous aspect necessitates comprehensive knowledge of the target language and culture. Humor is frequently created by puns in a language, leading to difficulties in translating structural ambiguity. Despite this, there is still much to be done in terms of humor translation, and there is currently minimal research into the machine translation of humor. In this study, the primary focus is on automatic humor translation from Franch to English and vice versa, utilizing artificial intelligence techniques. Firstly, attention is given to the provided data for this purpose, and efforts are made to clean them. The second task entails classifying word sets by providing semantic interpretations.</p><p>Research into pun translation by AI is still in its early stages and there are a limited number of studies on this topic. However, some recent studies have highlighted the challenges involved in accurately translating puns across languages.</p><p>For instance, a 2018 study by Li et al. <ref type="bibr" coords="1,258.82,608.38,12.82,9.66" target="#b0">[1]</ref> found that existing machine translation models often fail to identify and preserve the puns in a source text during the translation process, leading to errors and loss of meaning. The authors suggested that new models should incorporate information about the cultural context of the puns and the intended audience to improve their performance.</p><p>Similarly, a 2020 study by Shao et al. <ref type="bibr" coords="1,260.96,658.97,12.82,9.66" target="#b1">[2]</ref> noted that puns often involve wordplay and ambiguity, which can be difficult for machine translation models to handle. The authors proposed a new approach that uses semantic role labeling and bilingual word embeddings to identify the relevant words and meanings of puns and demonstrated improved accuracy in pun translation for Chinese-English language pairs. Additionally, Farghal and Obiedat <ref type="bibr" coords="2,241.89,100.28,12.82,9.66" target="#b2">[3]</ref> proposed a computational model for the translation of puns that takes into account the linguistic and cultural aspects of the source and target languages. The proposed model was evaluated through a set of experiments and achieved promising results, showing that it can be a useful tool for the translation of puns.</p><p>Overall, these studies suggest that pun translation by AI remains a challenging task, requiring more research into developing models that can better capture the nuances and cultural references of puns across languages.</p><p>The CLEF Joker task, a subtask of the CLEF conference and evaluation forum, aims to evaluate algorithms for the automatic detection of humor in text, and is a crucial step towards developing more accurate and reliable natural language processing (NLP) tools. The task involves a large corpus of humorous and non-humorous texts in various genres, including news articles, social media posts, and jokes. The participants are required to develop algorithms that can accurately distinguish between the two categories. The task is challenging due to the diverse and complex nature of humor, which can vary significantly across different genres and cultures. Moreover, humor can be expressed in many forms, such as wordplay, sarcasm, irony, and satire, making it difficult to capture all the nuances of humor in a text.</p><p>In this paper, we present a comprehensive review of the solved tasks for the CLEF 2023 Joker lab competition <ref type="bibr" coords="2,128.76,315.32,24.33,9.66">[4][5]</ref>. Task 1, which seeks to detect puns in the data set, task 2, which consists of pun interpretation, and task 3, which aims to produce a translated version of the pun into Spanish, were dealt with. Different models were used to execute the tasks while extracting the relevant words/phrases from the original data set. The models' performance was taken into account when ranking them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>To introduce the task and the data, we draw upon the work of Liana Ermakova, Tristan Miller, Anne-Gwenn Bosser, Victor Manuel Palma Preciado, Grigori Sidorov, and Adam Jatowt in their paper titled "Science for Fun: The CLEF 2023 JOKER Track on Automatic Wordplay Analysis" published in the Proceedings of the 45th European Conference on Information Retrieval (ECIR 2023), held in Dublin, Ireland from April 2 to 6, 2023 <ref type="bibr" coords="2,264.49,474.25,11.68,9.66" target="#b3">[4]</ref>. The tasks at hand are in multilingual texts (English, French, and Spanish), which involves automatic processing of wordplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Task 1 -Detecting puns and pun locations</head><p>A pun is a form of wordplay where a word or phrase is used in a way that evokes multiple meanings or exploits the similarity in pronunciation of different words or phrases. The detection of puns involves identifying instances in text where such wordplay is present. In the context of pun detection, the main goal is to distinguish between texts that contain puns and texts that do not. This task is typically framed as a binary classification problem, where the output is a prediction of whether a given text contains a pun or not. In addition to detecting puns, the task of pun location aims to identify the specific words or phrases within a text that constitute the pun. This can be a more fine-grained task that involves not only determining the presence of wordplay but also pinpointing the exact location of the pun within the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Data description</head><p>The dataset provided for this task includes texts in English, French, and Spanish. The aim is to distinguish between texts that contain puns and texts that do not. The data consists of both positive and negative examples. Positive examples are short jokes, specifically one-liners, each containing a single pun. These positive examples are drawn from previously constructed corpora as well as collections that may not have been used in previous shared tasks. Negative examples are generated through data augmentation techniques. Positive examples are manually or semi-automatically edited in such a way that the wordplay is lost, but most of the rest of the meaning remains intact.</p><p>The train and test data are provided in JSON format, with each instance having the following fields:</p><p>• "id": A unique identifier for each instance.</p><p>• "text": The text of the instance, which may or may not contain wordplay To train and evaluate the models, a relevance judgments (qrels) file is provided to help compare the model's output with the correct results. Qrels files include the following fields:</p><p>• "id": A unique identifier corresponding to the input file. • "wordplay": Indicates whether the text contains wordplay (values: "yes" or "no"). Here is an example of a qrel file:</p><p>[{"id":"en_135","wordplay":"yes"}, {"id":"en_7942","wordplay":"no"}]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Methods</head><p>12 methods in total were used for this task. FastText, TF-IDF Ridge, Multinomial Naive Bayes, Multi-Layer Perceptron, Simple T5, BLOOM, for pun detection -binary classification task, and random word as well as last word in the text, BLOOM, SimpleT5, OpenAI, AI21 for pun location. Two types of vectorizers are used: TF-IDF vectorizer and Count vectorizer. These vectorizers are commonly used in natural language processing tasks to convert text data into numerical feature vectors that machine learning models can process.</p><p>1. TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer measures the importance of a word in a document relative to a collection of documents. It assigns a weight to each word based on its frequency in the document and its rarity in the entire corpus. The TF-IDF score is calculated using the formula: TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF). TF measures how often a word appears in a document, while IDF measures the rarity of the word across the entire corpus. The TF-IDF vectorizer converts the text data into a matrix where each row represents a document and each column represents a unique word, with the cell values representing the TF-IDF scores. 2. The Count vectorizer, also known as the Bag-of-Words model, represents text data by counting the frequency of each word in a document. It creates a vocabulary of unique words from the corpus and generates a matrix where each row represents a document, each column represents a unique word, and the cell values represent the count of each word in the corresponding document.</p><p>The FastText model is a text classification and representation learning algorithm that has gained popularity due to its efficiency and effectiveness in processing large volumes of text data. The code begins by installing the FastText library and preparing the data. To prepare the data for the FastText model, the labels of dataframe are prefixed with 'label' to conform to FastText's input format. The model is trained and predicted column contains tuples where the predicted label is the first tuple element.</p><p>The TF-IDF Ridge model combines the TF-IDF vectorization technique with the Ridge classifier. TF-IDF is a numerical representation of text documents that reflects the importance of words in a document corpus. The Ridge classifier is a linear classifier that performs regularization to prevent overfitting. We use the TfidfVectorizer from scikit-learn to convert the text data into TF-IDF feature vectors. The RidgeClassifier is then trained on the transformed data using the fit method. The model parameters include the tolerance (tol) for convergence and the solver algorithm (sparse_cg) used for optimization.</p><p>Multinomial Naive Bayes is a probabilistic classifier that assumes independence among the features and follows the multinomial distribution. Similar to the TF-IDF Ridge model, the text data is transformed into TF-IDF feature vectors using the TfidfVectorizer. The MultinomialNB classifier is then trained on the TF-IDF vectors using the fit method. The model learns the probabilities of each class and predicts the most probable class for new instances. MLP (Multi-Layer Perceptron) represents a multi-layer perceptron neural network. MLP is a feedforward neural network with one or more hidden layers. The text data is transformed into TF-IDF feature vectors using the TfidfVectorizer, similar to the previous models. The MLPClassifier is then trained on the TF-IDF vectors using the fit method. The model parameters include the random_state for reproducibility and the maximum number of iterations for convergence.</p><p>The Simple T5 model is used for text generation based on the T5 model architecture. It leverages the power of the T5 model, which is a transformer-based model that can perform various natural language processing tasks, including text generation, translation, summarization, and more. An instance of the SimpleT5 class is created and the model is then loaded using the from_pretrained method, specifying the model type as "t5" and the base model size as "t5-base". The data is prepared by renaming the columns of the data and test dataframes to 'source_text' and 'target_text', respectively. The data is split into training and validation sets using the train_test_split function. The model is trained using the train method, which takes the training and validation dataframes as input. Other parameters such as source_max_token_len, target_max_token_len, batch_size, max_epochs, use_gpu, outputdir, early_stopping_patience_epochs, and precision are specified to control the training process. Once the model is trained, it is loaded using the load_model method to select the model with the best scores based on the lowest validation loss. The predict method is used to generate predictions for the 'source_text' column of the test dataframe. The BLOOM model Application Programming Interface (API) is used to generate predictions based on a given prompt and sentence. The prompt is constructed by concatenating the prompt text with the given sentence, and the BLOOM API is called with the constructed prompt. The generated text is extracted from the API response and returned. On this, and later large language models (LLM), because of the token limit, we could not do the testing on all data but small portions of it. Finally, the unique values in the 'wordplay' column are printed, checking the predicted wordplay categories are 'yes' and 'no'. For the second part of task, data preparation was needed. It includes text cleaning techniques to remove unwanted characters, punctuation, and stopwords. The following steps are performed:</p><p>1. Importing necessary libraries and downloading required resources from NLTK (Natural Language Toolkit), including stopwords, tokenizers… 2. Defining the function which takes a text input and performs the following steps:</p><p>• Tokenization -splitting the text into individual words • Removing punctuation • Removing non-alphabetic tokens • Removing stopwords -common words that do not contribute much to the meaning of the text) These text cleaning steps help preprocess the text by removing noise and irrelevant information, making it more suitable for wordplay detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Task 2 -Pun interpretation</head><p>The task of pun interpretation involves identifying and understanding the multiple meanings associated with wordplay. Wordplay often presents a challenge due to its inherent ambiguity, requiring systems to recognize and disambiguate between different interpretations. The difficulty lies in capturing the intended humorous or clever aspects of the pun, which may rely on linguistic nuances, context, or cultural references. The provided examples showcase different types of puns that can occur. Some common types of puns include:</p><p>• Homographic Puns: These puns rely on words that are spelled the same but have different meanings. For example, "interest" can refer to both curiosity and financial interest, leading to a play on words in the sentence "I used to be a banker but I lost interest." • Homophonic Puns: These puns depend on words that sound alike but have different meanings. An example is the pun "proceeds went from the sacred to the propane," where "proceeds" sounds like "prophets" and "propane" sounds like "profane." • Homonymic Puns: These puns involve words that are both homographic and homophonic, having the same spelling and pronunciation but different meanings. An example could be a sentence like "Time flies like an arrow; fruit flies like a banana," where "flies" can refer to both the action of flying and the insect. Interpreting puns requires a deep understanding of language, context, and word associations. The task emphasizes the importance of recognizing the various meanings associated with the puns and providing appropriate interpretations in the form of synonyms or hypernyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Data description</head><p>As the tasks before, we were provided with training and test data in JSON format, containing unique identifiers (id) and the text of wordplay instances. Additionally, qrels are provided, including fields such as id, location (the portion of text containing the wordplay), and interpretation (the correct interpretations of the pun). Basic data preprocessing was done. Duplicates were dropped from the "test" dataframe based on the 'id_en' column because we needed only one example of data interpretation. Example: {"id": "en_135", "location": "Pharaohs", "interpretation": "Pharaoh;Pharaoh of Egypt/fair" }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Methods</head><p>We used various models related to data processing and language generation using different natural language processing models. Each of the following model had two prompts. First one to find the pun word and the second one to interpret it.</p><p>LLMs we used were AI21, OpenAI and BLOOM for which we defined a function generate_from_prompt that sends a request to API endpoints to generate text based on a given prompt and a sentence. It uses the requests library for making the API call. A subset of the test dataframe was used and to each sentence in the text_en column we applied the mentioned function.</p><p>The SimpleT5 code segment involves using the SimpleT5 library to train a T5 model. The model is loaded using model.from_pretrained("t5", "t5-base"). The dataframe is preprocessed to have two columns: "source_text" and "target_text". The model is trained using the train function, passing the preprocessed data as the training dataframe. This task was considered to be the most complicated one and thus has the least models trained. Again, because of the token limit we could not do the testing on all data but small portions of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Task 3 -Translation of English Punning Jokes into Spanish</head><p>The objective of this task is to translate English punning jokes into Spanish while maintaining the original wordplay's form and meaning. The translations should aim to implement a pun-to-pun strategy, following the principles outlined in Delabastita's typology of pun translation strategies <ref type="bibr" coords="6,506.99,284.51,11.68,9.66" target="#b5">[6]</ref>. The goal is to preserve the humorous and linguistic aspects of the original jokes in the translated versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Data description</head><p>Training and test set contain English punning jokes and their corresponding translations into Spanish. These datasets are structured in JSON and include the following fields:</p><p>• id_en: A unique identifier for each instance of the source wordplay in English. • text_en: The text of the English punning joke.</p><p>Training data had additional field text_es with translation of the wordplay into Spanish. Since traditional vocabulary overlap metrics such as BLEU are not suitable for evaluating wordplay translations, evaluation process will consider various features, including but not limited to: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Methods</head><p>This section demonstrates the use of different neural network models (BLOOM, AI21, OpenAI) for English-to-Spanish translation. Each section loads the data, defines a prompt, queries the respective model, processes the translations, and saves the results in JSON files for further analysis or use.</p><p>Same as for the tasks before, we used the BLOOM model API to generate predictions based on a given prompt and sentence. Prompt is sent to the Hugging Face API with the input sentence to generate the translated text, and the results are stored in the part of dataframe used because of token number limitation. The generated translations are cleaned and processed to extract the desired Spanish text.</p><p>Next method used was AI21 model. The code follows a similar structure to the BLOOM section, but it uses the ai21 API to generate translations. The translations are obtained by providing the prompt and input sentence to the AI21 model. OpenAI model uses method that defines a prompt and uses the openai API method to generate translations. Again, the translations are obtained by providing the prompt and input sentence to the OpenAI model. For SimpleT5 we created an instance of the SimpleT5 model. The model is loaded from the "t5-base" pretrained model.</p><p>The dataframe columns are renamed to match the expected input format of SimpleT5 and the prefix "Spanish translate: " is added to the source text to specify the task needed to be done. For given source text, translations were provided using the trained model.</p><p>The EasyNMT library is installed, and three instances of the EasyNMT model are created using different translation model. For Opus_MT translation model is named 'opus-mt', mbart50_m2m uses the model's name 'mbart50_m2m', and for m2m_100_418M, the model's name should be 'm2m_100_418M'. Sentences from the test data are translated from English to Spanish using the model and s translations are stored in the text_es column of the test dataframe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We used twelve different models in total for these three tasks of recognizing puns in text and evaluated their performance using both subjective and objective methods. For the subjective evaluation, we had one annotator who manually assessed the models' performance for each task. The annotator ranked the models based on their performance for each task, and we report the ranking for each task below. For the objective evaluation, we used recall, F1 score, and precision as our evaluation metrics. We calculated these metrics for all twelve models for each of the four tasks, but we were not provided with any thresholds or benchmarks. Therefore, we only report the scores we obtained for each model for each task. analyze the performance of the models for T1.1. The model with the highest total precision (0.3076), recall (0.8307), F1 score (0.4489), and accuracy (0.4816) is SimpleT5. On the other hand, BLOOM had the lowest scores across all metrics, including precision (0.0833), recall (0.0012), F1 score (0.0024), and accuracy (0.7427). This indicates that the BLOOM model struggled the model performed the worst, with very low scores across all metrics. It's worth noting that the NB model achieved a very high recall score, but had lower precision and F1 scores, suggesting it may have been better at identifying true positives but also had a higher false positive rate.</p><p>For Task 2, the objective evaluation metrics were calculated, providing insights into the performance of the models. Here are the results: the models are listed along with their respective counts, total accuracy, and part accuracy for Task 1.2. Here's an explanation of the metrics:</p><p>• Count: Represents the number of instances or samples considered for evaluation.</p><p>• Total Accuracy: Indicates the overall accuracy of the model in classifying the instances correctly. • Part Accuracy: Refers to the accuracy of the model for a specific subset or category of instances. Based on the provided results, the model with the highest total accuracy and part accuracy is SimpleT5. It achieved an impressive total accuracy and part accuracy of approximately 0.7992, indicating its strong performance in recognizing puns. On the other hand, the random pun prediction performed comparatively poorer, with a total accuracy and part accuracy of around 0.1394.</p><p>Without additional information on thresholds and benchmarks, it's difficult to definitively say which model is the best or worst. However, based on the overall performance across all metrics, SimpleT5 appears to be the top performer while BLOOM and random predicting struggles the most in preformed tasks. However, further research is needed to determine how these models perform on larger and more diverse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>CLEF Joker 2023 tasks have provided an opportunity to delve into the challenging domain of pun detection and interpretation across multiple languages. The three specific tasks; the detection of puns in English (Task 1), the location and interpretation of puns in English, French, or Spanish (Task 2), and the translation of puns from English to French or Spanish (Task 3), have presented unique challenges and opportunities for the development of machine learning models.</p><p>For Task 1, the detection of puns in English, we have explored various approaches such as statistical models, and more advanced machine learning techniques. These efforts have led to the development of models that can identify linguistic patterns and contextual cues indicative of puns. However, the task remains complex due to the subtlety and ambiguity of puns, which often require deep understanding of language nuances and contextual information.</p><p>In Task 2, focusing on the location and interpretation of puns in English, the challenge lies in accurately identifying the specific words or phrases that constitute the pun and comprehending their intended multiple meanings. Machine learning models have been developed to leverage the available language resources and parallel corpora to capture the linguistic nuances and cultural references associated with puns in different languages. These models have shown promising results, although further research is needed to enhance their robustness and adaptability to varying linguistic contexts.</p><p>Task 3, involving the translation of puns from English to Spanish, has brought forth the intricacies of cross-lingual pun comprehension. Machine translation models have been explored to bridge the linguistic and cultural gaps between languages and faithfully convey the humor inherent in puns. This task requires not only accurate translation but also the preservation of the pun's underlying wordplay and humor, posing a considerable challenge for machine learning systems.</p><p>Overall, the development of machine learning models for the CLEF Joker 2023 tasks has advanced our understanding of pun detection, interpretation, and translation. These models have demonstrated their potential to automate the identification and comprehension of puns across languages, laying the foundation for further research in humor understanding and natural language processing. However, there are still areas for improvement.</p><p>The CLEF Joker 2023 tasks have provided valuable insights and benchmarks for evaluating the performance of machine learning models in the realm of humor detection and interpretation. The continued exploration of these tasks will foster advancements in computational humor understanding and contribute to the development of more sophisticated NLP systems capable of handling humor in diverse languages and contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,194.25,365.91,54.03,9.66;4,241.48,378.56,63.89,9.66;4,241.48,391.21,76.70,9.66;4,241.48,403.86,132.89,9.66;4,241.48,416.51,129.02,9.66;4,241.48,429.16,66.33,9.66;4,241.48,441.81,74.89,9.66;4,241.48,454.45,71.45,9.66;4,241.48,467.10,96.09,9.66;4,241.48,479.75,163.40,9.66;4,241.48,492.40,62.98,9.66;4,227.23,505.05,3.66,9.66;4,212.77,517.16,183.99,7.91"><head>Code 1</head><label>1</label><figDesc>Parameters used for SimpleT5 train model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,158.25,638.97,258.23,8.78;4,158.25,650.47,63.33,8.78;4,158.25,673.46,189.89,8.78;4,158.25,684.96,61.38,8.78;4,158.25,707.96,206.00,8.78;4,158.25,719.46,63.33,8.78;4,158.25,730.96,10.00,8.78;4,248.93,742.19,111.67,7.91"><head></head><label></label><figDesc>prompt = '''Sentence: Dentists don't like a hard day at the orifice. Wordplay: YES Sentence: Shock me, say something intelligent! Wordplay: NO Sentence: Give me a haircut, Tom said barbarously. Wordplay: YES … Code 2 Prompt used for LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,72.00,359.86,451.28,237.80"><head>Table 1</head><label>1</label><figDesc>Manually rated models In this table, the models are listed with their rankings for each task. A dash (-) indicates that the model was not used nor ranked for that particular task. The numbers represent the rankings, where lower numbers indicate better performance.</figDesc><table coords="7,105.00,381.50,365.48,176.61"><row><cell>Model</cell><cell>Task 1.1</cell><cell>Task 1.2</cell><cell>Task 2</cell><cell>Task 3</cell></row><row><cell>SimpleT5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell></row><row><cell>RidgeClass</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NB (Naïve Bayes)</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FastText</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MLP</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Multilayer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>perceptron)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLOOM</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>3</cell></row><row><cell>OpenAI</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell></row><row><cell>AI21</cell><cell>-</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell>Last word</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell>Random word</cell><cell>-</cell><cell>6</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,663.35,405.09,94.30"><head>Table 2</head><label>2</label><figDesc>Results for Task1.1</figDesc><table coords="7,90.00,684.99,405.09,72.66"><row><cell>Model</cell><cell>Total Precision</cell><cell>Total Recall</cell><cell>Total F1</cell><cell>Total Accuracy</cell></row><row><cell>BLOOM</cell><cell>0.0833</cell><cell>0.0012</cell><cell>0.0024</cell><cell>0.7427</cell></row><row><cell>FastText</cell><cell>0.2588</cell><cell>0.8294</cell><cell>0.3945</cell><cell>0.3528</cell></row><row><cell>MLP</cell><cell>0.2905</cell><cell>0.7293</cell><cell>0.4155</cell><cell>0.4785</cell></row><row><cell>NB</cell><cell>0.2598</cell><cell>0.9543</cell><cell>0.4085</cell><cell>0.2975</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,86.25,233.93,376.78,125.10"><head>Table 3</head><label>3</label><figDesc>Results for Task1.<ref type="bibr" coords="8,350.52,233.93,3.92,7.91" target="#b1">2</ref> </figDesc><table coords="8,86.25,255.57,376.78,103.46"><row><cell>Model</cell><cell>Total Accuracy</cell><cell>Part Accuracy</cell></row><row><cell>AI21</cell><cell>0.0133</cell><cell>0.9412</cell></row><row><cell>BLOOM</cell><cell>0.00996</cell><cell>0.7059</cell></row><row><cell>OpenAI</cell><cell>0.0124</cell><cell>0.8824</cell></row><row><cell>SimpleT5</cell><cell>0.7992</cell><cell>0.7992</cell></row><row><cell>lastWord</cell><cell>0.5444</cell><cell>0.5444</cell></row><row><cell>random</cell><cell>0.1394</cell><cell>0.1394</cell></row><row><cell>In this table,</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,100.50,360.41,422.68,9.66;9,100.50,373.05,413.02,9.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,281.83,360.41,154.35,9.66">A neural model for pun generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,456.80,360.41,66.38,9.66;9,100.50,373.05,336.09,9.66">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3938" to="3947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.50,385.70,422.72,9.66;9,100.50,398.35,350.46,9.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,344.78,385.70,178.44,9.66;9,100.50,398.35,207.64,9.66">The Automatic Pun Recognition and Interpretation System: Design and Applications</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,315.42,398.35,55.59,9.66">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="49968" to="49977" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.50,411.00,422.20,9.66;9,100.50,423.65,368.78,9.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,261.72,411.00,256.28,9.66">Towards a computational model for the translation of puns</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Farghal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Obiedat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,100.50,423.65,303.75,9.66">Journal of King Saud University-Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.50,436.30,422.69,9.66;9,100.50,448.95,422.28,9.66;9,100.50,461.60,422.73,9.66;9,100.50,474.25,422.77,9.66;9,100.50,486.90,56.48,9.66;9,180.72,486.90,17.71,9.66;9,221.43,486.90,13.73,9.66;9,258.15,486.90,72.73,9.66;9,353.88,486.90,30.23,9.66;9,407.11,486.90,51.38,9.66;9,481.48,486.90,41.22,9.66;9,100.50,499.54,206.20,9.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,268.75,448.95,254.03,9.66;9,100.50,461.60,143.61,9.66">Science for Fun: The CLEF 2023 JOKER Track on Automatic Wordplay Analysis</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne-Gwenn</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Palma</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-28241-6_63" />
	</analytic>
	<monogr>
		<title level="m" coord="9,273.84,461.60,249.39,9.66;9,100.50,474.25,243.50,9.66;9,100.50,486.90,56.48,9.66;9,180.72,486.90,17.71,9.66;9,221.43,486.90,10.30,9.66">Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023</title>
		<meeting><address><addrLine>Dublin, Ireland; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2023-04-02">2023. April 2-6, 2023</date>
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct coords="9,100.50,512.19,422.38,9.66;9,100.50,524.84,422.35,9.66;9,100.50,537.49,422.66,9.66;9,100.50,550.14,422.47,9.66;9,100.50,562.79,422.15,9.66;9,100.50,575.44,422.42,9.66;9,100.50,588.09,250.17,9.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,311.78,537.49,211.38,9.66;9,100.50,550.14,213.61,9.66">Overview of JOKER@CLEF 2022: Automatic Wordplay and Humour Translation Workshop</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne-Gwenn</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudine</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Élise</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaëlle</forename><surname>Le Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sílvia</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radia</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Boccou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albin</forename><surname>Digue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aurianne</forename><surname>Damoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoît</forename><surname>Jeanjean</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-13643-6_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-13643-6_27" />
	</analytic>
	<monogr>
		<title level="m" coord="9,340.06,550.14,182.91,9.66;9,100.50,562.79,422.15,9.66;9,100.50,575.44,19.79,9.66">Experimental IR Meets Multilinguality, Multimodality, and Interaction: 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2022-09-05">2022. September 5-8, 2022</date>
			<biblScope unit="page" from="447" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.50,600.74,422.51,9.66;9,100.50,613.38,298.51,9.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,187.02,600.74,73.46,9.66">Focus on the Pun</title>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Delabastita</surname></persName>
		</author>
		<idno type="DOI">10.1075/target.6.2.07del</idno>
		<ptr target="https://doi.org/10.1075/target.6.2.07del" />
	</analytic>
	<monogr>
		<title level="j" coord="9,273.12,600.74,226.03,9.66">Target. International Journal of Translation Studies</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
