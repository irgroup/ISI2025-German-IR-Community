<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,62.38,17.04;1,156.32,75.44,366.79,17.04;1,72.02,96.20,256.53,17.04;1,72.02,126.46,214.81,10.80">NLPalma</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,152.98,70.18,10.80"><forename type="first">Victor</forename><surname>Manuel</surname></persName>
						</author>
						<author>
							<persName coords="1,145.22,152.98,74.98,10.80"><forename type="first">Palma</forename><surname>Preciado</surname></persName>
							<email>c.palma.p0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Bretagne Occidentale</orgName>
								<address>
									<settlement>HCTI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.45,152.98,119.03,10.80"><forename type="first">Carolina</forename><forename type="middle">Palma</forename><surname>Preciado</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.15,152.98,76.35,10.80"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
							<email>sidorov@cic.ipn.mx</email>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Politécnico Nacional de México</orgName>
								<address>
									<addrLine>Gustavo A. Madero</addrLine>
									<settlement>Ciudad de México</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,62.38,17.04;1,156.32,75.44,366.79,17.04;1,72.02,96.20,256.53,17.04;1,72.02,126.46,214.81,10.80">NLPalma</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F78B45E69BAE6222D7E6525DBDFBFA1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>wordplay</term>
					<term>humorism</term>
					<term>pun</term>
					<term>wordplay classification</term>
					<term>wordplay translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The following work has the purpose of describing the participation in the JOKER 2023 track in the classification Task 1 in which it is asked to classify sentences that contain wordplay and the translation Task 3 in which this same type of sentence has to be translated into Spanish trying to maintain the wordplay and the sense in a certain way tropicalizing the sentence to Spanish language from English. Given the constant use of models such as GPT and T5, it was decided to take the direction of language models, making a slight finetuning of BLOOMZ &amp; mT5 models with which to address the problem of translation with a simple prompt and on the other hand the use of BERT as a model proposed for the classification task. The results were mixed since in the manual review of the BLOOMZ &amp; mT5 translated sentences not many were found to contain the Spanish pun, in the case of classification somewhat similar results were obtained given the structure of the dataset. It is believed that given their performance the models can still be optimized and improve the accuracy with which they classify, in the case of the translations perhaps another methodology could be chosen to improve the results obtained in this task, in general, the results are satisfactory for a first approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The main objective of the work is to find robust methods, to achieve good results in the tasks provided, specifically for Task 1 and Task 3. In Task 1, the model must be able to classify between those sentences that contain wordplay and those that do not contain wordplay and therefore would not contain humor. This task is based on the Spanish dataset of JOKER 2023 <ref type="bibr" coords="1,396.55,525.33,11.69,9.94" target="#b0">[1]</ref>.</p><p>In the case of Task 3, the objective was to correctly translate an English pun into Spanish in such a way that the produced translation also contains the pun. In this case, the translated pun should also make sense and be adapted to something appropriate within the context. In a certain way adapting the term to something according to what could be expected in the translation, since the terms that form the wordplay usually carry certain homophone elements that when translated do not have a homologous.</p><p>It is important to note that sometimes a direct translation may not have an exact equivalent for a wordplay, but it is expected that the model is able to see beyond the simple literal translation of term to term and find the connection between the wordplay.</p><p>The dataset used contains Spanish and English puns, which were fed to a large language model to obtain translations that capture the correct type of pun humor or, at the very least maintain some humor in the resulting translation. Additionally, for the classification task a BERT-like model was employed, since in other instances of humor, this model has demonstrated a certain level of effectiveness. Therefore, this time we aim to test if it can really be used for different cases and if its use can be generalized.</p><p>An important aspect of language-based models, especially BLOOMZ &amp; mT5 <ref type="bibr" coords="2,454.66,112.60,12.91,9.94" target="#b1">[2]</ref> is they are optimized to follow the next token predicted, in this case, following a prompt with instructions oriented to the translation task. It is believed that such models could be useful to maintain the wordplay in the translation. Although this type of tasks can become a little complicated since certain terms used in different languages can make it difficult how to perceive the wordplay of the original sentence to the translated one. This issue is later encountered, as these challenges can normally arise within a model.</p><p>The use of homophones can complicate the task even more, as there may not be a similar word in the translation that matches or has the same intention, altering in a certain way the interpretation of the joke with the pun, thus changing the original meaning. It could be said that the pun is maintained, but not the meaning. With the understanding that most puns become partial translations of what the original sentence proposes, we can say that there are different phenomena present in the translation of puns whether they are homophones, homographs, or this comes from a regional homophony of the sentence.</p><p>The task of classification presents a different challenge because puns can be confused by those sentences that unintentionally exhibit similar characteristics to a pun. Understanding the type of sentences that our model needs to screen can give us a broader idea of what needs to be done. In this case, the classification of puns appears to include certain sentences that emulate puns, but in reality, they are not. This can hinder and confuse the model, as it introduces a certain amount of noise with which we must deal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach to the task</head><p>The tasks proposed by JOKER in CLEF 2023 primarily focus on the automatic analysis of puns. In this edition, a corpus consisting not only of English and French text but also of Spanish was available. Based on previous successful works <ref type="bibr" coords="2,233.68,410.71,12.13,9.94" target="#b3">[4,</ref><ref type="bibr" coords="2,248.45,410.71,8.00,9.94" target="#b4">5]</ref>, it was decided to use the transformers approach <ref type="bibr" coords="2,475.18,410.71,11.79,9.94" target="#b2">[3]</ref>. Due to time constraints, of the three available tasks, only Task 1, involving wordplay detection, and Task 3, focusing on translation, were executed. It should be noted that tasks were performed solely on Spanish and English texts from the training corpus provided by JOKER.</p><p>The first task focused on pun detection, which can be solved as a binary classification task since it has yes and no labels that indicate whether a text is a pun or not. Initially, it was considered to train a multilingual model such as multilingual BERT <ref type="bibr" coords="2,285.35,486.69,11.94,9.94" target="#b6">[7]</ref>, which is pre-trained in 104 languages. However, due to the low performance obtained during evaluation, it was decided to keep the model but apply a separate approach.</p><p>As a result, two models were trained: one specifically for English and the other for a mixture of English-Spanish, both utilizing the same BERT architecture <ref type="bibr" coords="2,344.23,537.21,11.70,9.94" target="#b5">[6]</ref>. In the end, the models were trained with BERT and multilingual BERT. For this task, the model was loaded from the Hugging Face transformers module with the help of the Ktrain <ref type="bibr" coords="2,289.22,562.53,12.97,9.94" target="#b7">[8]</ref> wrapper, which facilitated the process of loading and fine-tuning the model in a fast and simple manner.</p><p>Due to the fact that transformers do not require extensive preprocessing, the training process was relatively straightforward. However, during the fine-tuning phase, it was necessary to experiment with different parameters to achieve the best results in validation accuracy and loss. In the end, the best models were saved in a Keras H5 format for future reference.</p><p>For the training of the BERT-like models, the following parameters were utilized: a batch size of 8, 3 training epochs, and a learning rate of 7e-6 for the BERT model trained on English data. On the other hand, the parameters for BERT-multilingual trained on English-Spanish data were a batch size of 32, 3 training epochs, and a learning rate of 5e-5. Considering the text length from the JOKER corpus, a maximum length of 70 tokens was declared.</p><p>To develop Task 3, consisting of the translation of word plays from English to Spanish, BLOOMZ &amp; mT5 was implemented, which is a more robust version of BLOOM, offering an improvement in task performance. This model can follow human instructions through the use of a prompt such as the examples used in the experimentation: "Translate into Spanish:", "Translate the text into Spanish:", "Translate into Spanish the next text:", and "Translate into Spanish the next sentence:". It is worth mentioning that the result obtained depends on the prompt used. In this case, four passes or runs were made on the text to achieve a better result, each pass corresponding to one of the provided prompts.</p><p>These iterations were necessary because in some passes the results were not favorable, either because the model fail to produce a translation (resulting in blank output), generating incomplete translation, or because the model produced more text than necessary (generating new text beyond the necessary translation).</p><p>The queries were conducted using the Hugging Face API, specifically the Inference API <ref type="bibr" coords="3,477.94,150.52,11.79,9.94" target="#b8">[9]</ref>, which facilitates the execution of queries using machine learning models. As mentioned, multiple iterations were performed due to the results obtained in each run, some sets of wordplays were either not translated or did not yield a proper result. It was required to try different prompts to get the best possible result, yet some puns failed to generate a translation, resulting in a blank query even after multiple iterations.</p><p>Although the generation of the translation was fully automated, a sample of the results were manually reviewed to identify potential errors. These errors were taken into consideration to determine which texts needed a re-runs. For instance, in the case of the pun "What do you call a doctor who treats retired soldiers? A veteran-arian." the first two attempts did not produce any results. It was in the third run that the text was successfully translated to "¿Qué es un doctor que trata a soldados retirados? Un veterano-ario.". Another example is the sentence "Recent survey revealed 6 out of 7 dwarf's aren't happy." which was generated on the second pass: "Una reciente encuesta reveló que 6 de cada 7 enanos no son felices."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Resources employed</head><p>Among the resources used to train and evaluate the models, Google's Colab environment was employed. This platform enables Python programming and execution, while also providing easier use of GPU. The use of GPU resources allowed for faster execution in the performance of the described tasks, by enabling multiple simultaneous computations. The server used has the following specifications: GPU NVIDIA-SMI 525.85.12, CUDA v12.0, and 25 of RAM.</p><p>As part of the resources, it was decided to use a combination of Google Colaboratory and Petals <ref type="bibr" coords="3,504.82,423.43,18.32,9.94" target="#b9">[10]</ref> for Task 3 translation, since we did not have the computational capacity to handle models with parameter size 10B, which requires a significant amount of computation beyond our capabilities. The use of Petals becomes indispensable for those jobs that do not have enough computational capacity, as it leverages the benefits of crowd computing to make inferences with large models in a relatively easy/semi-efficient way. The platform offers the flexibility of an API and the power of PyTorch, for seamless integration in the required tasks.</p><p>On the other hand, for other BERT-like models that are less demanding in the use of resources, it is necessary to consider the data volume and the desired width of tokenization. In the experimentation process BETO <ref type="bibr" coords="3,142.44,537.21,17.00,9.94" target="#b10">[11]</ref>, BERT, and BERT-multilingual models were tested for the classification task, likewise, the training set was taken under three different approaches:</p><p>1. Spanish-only dataset. 2. English-only dataset. 3. And a mixture of English-Spanish data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>As can be seen in the tables below, the probabilities of each of the tasks and a brief explanation of the cases are provided. Some of the sentences demonstrate a clear example of a pun, while others are not so straightforward in terms of their intended meaning. This variability in pun structures contributes to the challenge of accurately identifying puns. It is expected that the model may have some confusion in finding the characteristics between sentences that lack a pun's connotation but exhibit high similarity to those that do. This type of concern seems to be overlooked by the model to some extent. It is logical to evaluate the data set to get some insights into how the model works, which is exactly what is presented in the following tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1: Detection of puns</head><p>In this section, the results obtained for the binary classification of English and Spanish puns are presented. As the test set labels were not provided, some inferences about the wordplays are made to explain the model performance when evaluating this type of sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English corpus</head><p>The BERT model trained on the English corpus consisting of 5,292 wordplays, was divided into a training and validation set in a 80:20 distribution. Subsequently, the model was used to evaluate the test set provided by JOKER, which comprised 3,183 sentences. The probability of a text containing a wordplay was obtained, and the top 5 wordplays and non-wordplay were identified.</p><p>In Table <ref type="table" coords="4,126.84,239.44,4.15,9.94">1</ref>, it can be seen that a positive result indicating a wordplay involves a specific structure, where a sentence contains references in pairs (the words indicating this event are marked in bold letters). The highest results obtained a probability of around 0.99, where three of the puns with the highest probability of being wordplay refer to a pun referring to lawyers, as an example:</p><p>The lawyer asked a loaded question about guns.</p><p>The pun talks about a loaded question, and it is easily understood that the play on the words lies in the relationship between "loaded" and "guns". This example provides a small idea of how the model classifies such positive instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Top positive cases for wordplay classification of the English corpus Pun Probability She was only a lawyer's daughter, but what a will to break. 0.99865 She was only an Attorney's daughter, but what a will to break. 0.99862 The lawyer asked a loaded question about guns. 0.99860 Once ice cream was invented the problem was licked. 0.99860 Our Boy Scouts' knot -tying class went off without a hitch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.99859</head><p>Regarding the results with a low probability of being puns, that indicate a non-wordplay sentence. It can be observed that, unlike the previous table, these examples are not as obvious in terms of finding the wordplay. Therefore, if even for humans it is challenging to identify this word matching, it can be even more difficult for the model to understand the nuances that these sentences have with the use of puns. The results in Table <ref type="table" coords="4,193.61,555.21,5.52,9.94">2</ref> show how these sentences do not contain wordplay, indicating that the model correctly classified them as non-wordplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 Top negative cases for wordplay classification of the English corpus</head><p>Pun Probability What's the name of that street in Paris? asked Tom quietly. 0.00065 I wouldn't marry you if you were the only woman on earth, said Tom quietly.</p><p>0.00066 Give me some pre -packed cheese slices, said Tom professionally. 0.00068 ''Let's take a vacation in the south of France,'' said Tom loudly. 0.00068 Can you read music? the bandleader asked calmly.</p><p>0.00068</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish corpus</head><p>The multilingual BERT model used to classify the Spanish test set (2,241 sentences) presents distinct curiosities compared to the problems encounter in the English dataset. This may be attributed to the fact that incorporating a mixture of datasets strengthened the model's ability to infer the type of wordplays, but more so the type of redundant wordplays that was able to correctly classify. The example below shows a sentence that was accurately classified as a wordplay: ¿Sabes cuánto pago de alquiler por la frutería? -No, ¿cuánto? -Pimientos euros.</p><p>In this instance, it is observed how the model was able to infer alquiler (rent), and connected in to the word "euros" which corresponds to the word Pimientos (bell peppers). The intended meaning is to convey Pimientos to quinientos (five hundred) in relation to a monetary amount, a very similar word to the word pimientos. This wordplay is easy to understand, unlike other types of puns that are presented in Table <ref type="table" coords="5,111.23,226.48,4.14,9.94">3</ref>, like the sentence: Hey Jesús. -¿Salió cara la cena? No, salió mala.</p><p>To understand this pun, relate to religion, humans typically require prior knowledge about the subject. It can be challenging to identify, as it often involves cultural references. However, it is among the results with the highest probability of being a wordplay, which is correctly classified. As seen in the table, the confidence in the probability is lower than that of the English model, ranging between 0.80 percent in the probability that it will be a wordplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Top positive cases for wordplay classification of the Spanish corpus Pun Probability Los jugadores de baloncesto, famosos por su mal carácter, pillan muchos rebotes 0.82717 ¿Sabes cuánto pago de alquiler por la frutería? -No, ¿cuánto? -Pimientos euros. 0.81454 Ejecutivo agresivo busca monedas antiguas para partirles la cara. 0.80890 Ejecutivo agresivo busca monedas antiguas para partirles la marca. 0.80807 Hey Jesús. -¿Salió cara la cena? No, salió mala. 0.80444</p><p>In the case of the examples with lower probability, it can be noted that the presence of sentence structures where the word order is altered can make it more difficult to find the wordplay. This is evident in the example provided in Table <ref type="table" coords="5,220.51,505.17,4.34,9.94" target="#tab_0">4</ref>:</p><p>Un Señor Ruiz... que un ruiseñor.</p><p>The use of Señor (Mr.) and then the surname Ruiz may create a pun that refers to a type of bird, the ruiseñor (nightingale bird), but the model was unable to recognize the relationship between these two statements, where only the order of the pun was altered. The other four sentences do not contain a pun, this shows that most of the non-wordplay text where correctly identify as such. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 3: Translation of puns</head><p>Translations posed an extremely interesting case due to the fact that translating a term that makes a play of words in English may be difficult to interpret in Spanish. For a translation to fulfill the same objective as its counterpart in Spanish, it is extremely important to pay attention to certain structures. For example, the use of proper names that refer to a particular characteristic can be difficult to understand when they are translated into Spanish.</p><p>Understanding these nuances often becomes the pivotal part that modifies the behavior of what one wants to translate but that on other occasions can play in favor of the translation by making sense in the other language. Without forgetting that normally these words are quite related to the internal structure of the word in both languages.</p><p>These observations can be seen in Table <ref type="table" coords="6,267.12,219.64,4.14,9.94">5</ref>, where different results are presented. The first sentence "If there's one person you don't want to interrupt in the middle of a sentence it's a judge." although correctly translated to "Si hay una persona que no quieres interrumpir en medio de una oración, es un juez." the pun is not maintained, resulting in a literal translation.</p><p>On the contrary, in the case of the sentence "I tried to learn how to drive a stick shift but couldn't locate the manual." that was correctly translated to "Intenté aprender a conducir con una caja de cambios manual pero no pude encontrar el manual.", maintained not only the sense, but also the funniness of the wordplay (as indicated in bold letters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Pun translation from English to Spanish English Spanish If there's one person you don't want to interrupt in the middle of a sentence it's a judge.</p><p>Si hay una persona que no quieres interrumpir en medio de una oración, es un juez.</p><p>I tried to learn how to drive a stick shift but couldn't locate the manual.</p><p>Intenté aprender a conducir con una caja de cambios manual pero no pude encontrar el manual. The bride's best friend is so proud, she's practically made of honor.</p><p>La mejor amiga de la novia está tan orgullosa que parece que fuera la madrina. Some rappers are good but others are Ludacris.</p><p>Algunos raperos son buenos, pero otros son Ludacris. When the fog burns off it won't be mist.</p><p>Cuando se disipe la niebla, no será vapor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In conclusion, we can say that the approaches implemented to cope with the classification and translation of text with puns is not misguided, perhaps more work could be done to strengthen and further explore and improve the results in these tasks. Especially in the case of translation, as the aspect of translating while maintaining the original context of the language to another makes this type of task a very interesting one to delve into, and it calls for continued research to find better methods and techniques.</p><p>Moreover, the integration of more advanced language models, such as BLOOMZ &amp; mT5, showcased the potential of the use of prompts to enhance the translation of puns, which are instructions carried out by humans but this itself influences the result obtained. Regarding the classification of wordplays, the utilization of wordplays with BERT-like showed good results but they could be improved with better fine-tuning and exploring different variations of BERT-like architectures. Overall, this study obtained good results that demonstrate the potential of transformer-based models and language models in the classification and translation of puns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.02,618.46,441.26,109.68"><head>Table 4</head><label>4</label><figDesc>Top negative cases for wordplay classification of the Spanish corpus</figDesc><table coords="5,91.58,647.62,421.70,80.52"><row><cell>Pun</cell><cell>Probability</cell></row><row><cell>Mi nombre es Rob. Robo</cell><cell>0.05724</cell></row><row><cell>Un Señor Ruiz... que un loro.</cell><cell>0.08994</cell></row><row><cell>Mi nombre es Ladino. Soy ladrón.</cell><cell>0.10061</cell></row><row><cell>Un Señor Ruiz... que un ruiseñor.</cell><cell>0.10311</cell></row><row><cell>Mi nombre es Rob. Soy un ladrón.</cell><cell>0.11668</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,90.02,105.76,432.91,9.94;7,90.02,118.36,433.06,9.94;7,90.02,131.08,432.99,9.94;7,90.02,143.68,432.84,9.94;7,90.02,156.40,433.03,9.94;7,90.02,169.00,432.91,9.94;7,90.02,181.72,277.68,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,238.21,118.36,284.87,9.94;7,90.02,131.08,36.75,9.94">Overview of JOKER -CLEF-2023 track on Automatic Wordplay Analysis</title>
		<author>
			<persName coords=""><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne-Gwenn</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Palma</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,187.97,156.40,330.88,9.94;7,90.02,169.00,393.98,9.94">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="7,149.41,181.72,134.24,9.94">Special issue: Digital Libraries</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="1996">2023. 2023. 1996</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,90.02,194.32,433.01,9.94;7,90.02,206.92,432.87,9.94;7,90.02,219.64,432.78,9.94;7,90.02,232.24,407.87,9.94" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2211.01786</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2211.01786" />
		<title level="m" coord="7,358.73,219.64,164.08,9.94;7,90.02,232.24,90.89,9.94">Crosslingual Generalization through Multitask Finetuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct coords="7,90.02,244.96,432.73,9.94;7,90.02,257.59,433.23,9.94;7,90.02,270.19,273.35,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,188.21,257.59,109.96,9.94">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1706.03762v5" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
		<respStmt>
			<orgName>Cornell University ; Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">En arXiv</note>
</biblStruct>

<biblStruct coords="7,90.02,282.91,432.98,9.94;7,90.02,295.51,15.94,9.94;7,124.21,295.51,46.40,9.94;7,188.87,295.51,33.08,9.94;7,240.19,295.51,8.52,9.94;7,266.94,295.51,36.06,9.94;7,321.27,295.51,50.29,9.94;7,389.80,295.51,26.87,9.94;7,434.90,295.51,31.14,9.94;7,484.30,295.51,38.75,9.94;7,90.02,308.23,212.79,9.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,241.55,282.91,281.46,9.94;7,90.02,295.51,15.94,9.94;7,124.21,295.51,46.40,9.94;7,188.87,295.51,33.08,9.94;7,240.19,295.51,8.52,9.94;7,266.94,295.51,36.06,9.94;7,321.27,295.51,50.29,9.94;7,389.80,295.51,26.87,9.94;7,434.90,295.51,31.14,9.94;7,484.30,295.51,33.91,9.94">LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mahurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Patil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.semeval-1.108</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.semeval-1.108" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.02,320.83,433.23,9.94;7,90.02,333.43,432.91,9.94;7,90.02,346.15,307.79,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,312.70,320.83,210.56,9.94;7,90.02,333.43,346.77,9.94">Carolina Palma Preciado Assessing Wordplay-Pun classification from JOKER dataset with pretrained BERT humorous models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Palma</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,443.79,333.43,79.15,9.94;7,90.02,346.15,153.24,9.94">JokeR: Automatic Wordplay and Humour Translation</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1828" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.02,358.75,433.09,9.94;7,90.02,371.47,432.75,9.94;7,90.02,384.07,139.41,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,389.21,358.75,133.91,9.94;7,90.02,371.47,248.21,9.94">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.02,396.67,432.83,9.94;7,90.02,409.39,165.75,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,325.43,396.67,190.83,9.94">How Multilingual is Multilingual BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1493</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1493" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.02,421.99,433.11,9.94;7,90.02,434.73,391.96,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,199.86,421.99,289.04,9.94">ktrain: A Low-Code Library for Augmented Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Maiya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10703</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,352.47,434.73,39.69,9.94">BLOOM</title>
		<imprint>
			<date type="published" when="2020">2020. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>BigScience Workshop. Revision 4ab0472</note>
</biblStruct>

<biblStruct coords="7,90.02,447.33,433.14,9.94;7,90.02,459.93,433.04,9.94;7,90.02,472.65,198.08,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,90.02,447.33,129.66,9.94;7,345.31,459.93,177.76,9.94;7,90.02,472.65,69.30,9.94">Spanish PreTrained BERT Model and Evaluation Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,178.69,472.65,81.88,9.94">PML4DC at ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Inference API -Hugging Face</note>
</biblStruct>

<biblStruct coords="7,108.02,485.25,414.84,9.94;7,90.02,497.97,433.15,9.94;7,90.02,510.57,309.93,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,238.43,497.97,279.63,9.94">Petals: Collaborative Inference and Fine-tuning of Large Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Borzunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chumachenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Samygin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2209.01188</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2209.01188" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct coords="7,108.02,523.29,415.00,9.94;7,90.02,535.89,432.66,9.94;7,90.02,548.49,281.94,9.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,450.94,523.29,72.08,9.94;7,90.02,535.89,258.14,9.94">SPANISH PRE-TRAINED BERT MODEL AND EVALUATION DATA</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<ptr target="https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>workshop paper at PML4DC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
