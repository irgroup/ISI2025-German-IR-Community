<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,412.12,15.42;1,88.78,106.66,333.62,15.42">Innsbruck @ JOKER2023 Task 1: Data Augmentation Techniques for Humor Recognition in Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,66.73,11.96"><forename type="first">Stefan</forename><surname>Reicho</surname></persName>
							<email>stefan.reicho@student.uibk.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.67,134.97,63.85,11.96"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
							<email>adam.jatowt@uibk.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<postCode>6020</postCode>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,412.12,15.42;1,88.78,106.66,333.62,15.42">Innsbruck @ JOKER2023 Task 1: Data Augmentation Techniques for Humor Recognition in Text</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D1C32CCCEB2E78BAE98ACE5F24105E7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data augmentation</term>
					<term>humor detection</term>
					<term>JOKER</term>
					<term>wordplay</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the use of data augmentation techniques and their impact on pun detection in English, leveraging the official JOKER Task 1 -English Pun Detection dataset. The paper reports on various data augmentation strategies such as synonym replacement, back-translation, paraphrasing, shortening, and extension, and their respective impact on pun detection performance. An important aspect of the study is ensuring that the augmentation techniques retain the humorous effect present in the original text. The results from the experiments suggest however that the augmentation techniques did not substantially enhance pun detection capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humor is a crucial aspect of human communication that lends nuance to interactions and enhances our understanding of complex situations. One of the primary forms of humor in English is punning or wordplay, characterized by the use of words in such a way that multiple meanings or effects occur. The ability to detect puns in text is an interesting albeit challenging task in Natural Language Processing (NLP).</p><p>This paper presents our empirical study of the impact of various data augmentation techniques on the performance of pun detection, based on the official JOKER Task 1 -English Pun Detection dataset <ref type="bibr" coords="1,122.55,463.47,11.28,10.91" target="#b0">[1]</ref>. In particular, we explore techniques such as synonym replacement, back-translation, paraphrasing, shortening, and extension. Our study utilizes the transformer-based BERT <ref type="bibr" coords="1,489.31,477.02,16.68,10.91" target="#b1">[2]</ref> model as the underlying classifier for the pun detection task.</p><p>We present a detailed discussion on the various data augmentation techniques used, as well as the measures taken to ensure the quality of augmented data. Despite the multiple techniques employed, our study suggests that the data augmentation methods we apply have not significantly enhanced pun detection capabilities. However, this paper offers some insights into pun detection and the potential influence of data augmentation techniques, providing a foundation for future research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dataset</head><p>The primary source of data for this research was the official JOKER Task 1 -English Pun Detection dataset <ref type="bibr" coords="2,169.67,145.80,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pun Detection Classifier</head><p>The pun detection task was performed using a classifier model built on the transformer architecture. The choice for the underlying pre-trained model was the BERT model, specifically the 'bert-base-uncased' variant. This model was selected for its high effectiveness in handling a wide range of NLP tasks, including text classification.</p><p>The BERT model was fine-tuned using the Adam optimizer <ref type="bibr" coords="2,368.40,249.72,12.91,10.91" target="#b2">[3]</ref> with a learning rate of 1e-5 and a weight decay of 0.01. Our training procedure used a batch size of 32 and was conducted over two epochs.</p><p>To measure the performance of the model, the CrossEntropyLoss function was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data Augmentation</head><p>To further improve the ability to successfully classify sentences containing puns, without tuning the classifier, data augmentation could be applied.</p><p>Although applying different data augmentation techniques to humorous data containing wordplay can be challenging without losing the humorous effect, we decided to take this path and perform experiments in order to understand to what extent different data augmentation technologies would be applicable for wordplay detection.</p><p>In the following, we describe a few simple techniques that were implemented and experimented with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Synonym replacement</head><p>Data augmentation by substitution involves replacing parts of the original data to create slightly different versions, yet keeping the underlying meaning the same. One popular substitution technique is synonym replacement, where certain words are replaced by their synonyms. For example the sentence "The quick brown fox jumps over the fence" could become "The fast brown fox jumps over the fence", by replacing "quick" with one of its synonyms "fast".</p><p>Synonym replacement can be implemented in a variety of ways. Two popular approaches are Wordnet-based <ref type="bibr" coords="2,159.54,565.56,12.90,10.91" target="#b3">[4]</ref> and Word2vec-based <ref type="bibr" coords="2,271.05,565.56,12.90,10.91" target="#b4">[5]</ref> synonym augmentation. To implement synonym replacement, this project uses the TextAugment <ref type="bibr" coords="2,298.54,579.11,12.68,10.91" target="#b5">[6]</ref> library, which provides easy-to-use methods for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Wordnet-based synonym augmentation</head><p>TextAugment's Wordnet-based augmentation algorithm chooses 'r' words from a sentence and replaces them with one of their synonyms 's', where 'r' and 's' are randomly selected. <ref type="bibr" coords="2,469.29,655.54,12.76,10.91" target="#b5">[6]</ref> Note that the synonyms selected by Wordnet are context insensitive. This might lead to less accurate replacements if a word strongly depends on its context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Word2vec-based synonym augmentation</head><p>Word2vec-based augmentation uses a different approach by utilizing a word embedding model. Words that are close to each other in the model's vector space are likely to have a similar meaning, but must not strictly be synonyms. In contrast to Wordnet-based augmentation, it can handle words based on their context, making it possible to suggest more suitable synonyms in some situations.</p><p>As for the embedding model, we are using the pre-trained Google News Word2vec model, which was trained upon roughly 100 billion words from different Google News articles. However, we encountered an incompatibility issue between the TextAugment library and the latest gensim version (v4.3.1). Starting with gensim version 4.0.0, the 'wv' attribute was removed from the 'KeyedVectors' class. To resolve this breaking change, we had to slightly modify the TextAugment library as a workaround.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4.">Paraphrasing by back-translation</head><p>Another family of data augmentation techniques is augmentation by paraphrasing. In this case, instead of only substituting certain parts of the text, the entire text is modified by rewording or restructuring it (by paraphrasing). By paraphrasing, the example from before: "The quick brown fox jumps over the fence" could become "The fence is jumped over by the quick brown fox".</p><p>Back-translation is a type of paraphrasing, in which text is translated from its original language to some other language and then back-translated to its original language. Often, this results in a reworded or restructured sentence while retaining its original meaning.</p><p>For the translation of sentences, we used the MarianMTModel <ref type="bibr" coords="3,388.22,443.06,11.59,10.91" target="#b6">[7]</ref>, which was trained on a large dataset of texts. First, each sentence is translated from English to German using the pre-trained model 'Helsinki-NLP/opus-mt-en-de', and then back-translated from German to English using the 'Helsinki-NLP/opus-mt-de-en' model. After, the same was done for English and French using the 'Helsinki-NLP/opus-mt-en-fr' and 'Helsinki-NLP/opus-mt-fr-en' models. This results in 2 new augmented samples for every input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5.">Paraphrasing using PEGASUS</head><p>PEGASUS <ref type="bibr" coords="3,136.03,560.13,12.70,10.91" target="#b7">[8]</ref> stands for Pre-training with Extracted Gap-sentences for Abstractive Summarization and is used for tasks like text summarization. However, it is also capable of rephrasing text while retaining its original meaning. This makes PEGASUS not only suitable for summarization but also for data augmentation.</p><p>For our experiments, we use the 'tuner007/pegasus_paraphrase' model, a variant of PEGASUS, which was fine-tuned specifically for the task of paraphrasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6.">Shortening</head><p>In this approach, a text or sentence is reduced in length. One way to achieve this is by removing unnecessary details, i.e., the least important parts of the text. However, removing a word from a sentence, in our case, a sentence that might contain pun wordplay, without altering its meaning or humorous effect, turns out to be quite challenging. Every word in a sentence containing wordplay might contribute to its meaning.</p><p>To implement augmentation by shortening we applied the Rapid Automatic Keyword Extraction (RAKE) algorithm <ref type="bibr" coords="4,186.11,188.83,13.50,10.91" target="#b8">[9]</ref>. RAKE selects the key words and phrases of a sentence by analyzing the frequency of word appearance and its co-occurrence with other words in the text. By removing some of the words that were not identified as key words, a new shortened sentence can be formed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.7.">Extension</head><p>Augmentation via extension is another widely used technique. Since there are only new words added, but none removed, it could be considered one of the safer techniques in regards to not altering the text's meaning.</p><p>In our implementation, we use GPT-2 <ref type="bibr" coords="4,272.17,319.45,18.02,10.91" target="#b9">[10]</ref> to generate a new additional sentence based on the original text. This sentence is then randomly appended either at the beginning or the end of the original text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Improving augmented data quality</head><p>When applying any of those techniques to humorous texts containing wordplay, special care must be taken to not lose the humorous effect. In the experiments, it was inevitable that there were samples generated in which the humorous effect was lost. Therefore, after augmentation, we filter out / reject some generated samples that do not meet certain criteria.</p><p>The following filters were used:</p><p>• Duplicate Filter: If a generated text sample replicates the original word for word, it is discarded. Having duplicate data in the dataset can lead to overfitting. Thus, it is important to prevent augmentation techniques from generating duplicate samples. • Last Words Filter: Most of the time the punning word is one of the last two words in the sample <ref type="bibr" coords="4,167.17,528.54,16.19,10.91" target="#b10">[11]</ref>. By requiring every augmented sample to end with the same two words, it's possible to filter out generated samples that are likely to have lost their punning word in the process of augmentation. • Similarity Filter: Filtering out generated samples that are semantically "too far" from the original sample can also ensure a higher quality of the entire augmented dataset.</p><p>Here it is done by using Sentence-BERT <ref type="bibr" coords="4,301.13,597.64,18.07,10.91" target="#b11">[12]</ref> (SBERT), more specifically using the 'all-MiniLM-L6-v2' <ref type="bibr" coords="4,186.87,611.19,17.93,10.91" target="#b12">[13]</ref> model, which is able to generate sentence embeddings that capture semantic information. These embeddings are then used to determine the similarity between the original and augmented samples. The similarity is calculated using the cosine distance. If this similarity measure falls below a certain threshold, then the generated sample will not be included in the augmented set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Test set evaluation</head><p>Table <ref type="table" coords="5,116.27,244.30,5.17,10.91" target="#tab_0">1</ref> shows a summary of the predictions submitted to JOKER. For comparison, measures such as Precision, Recall, F1 score, and Accuracy are used. The Innsbruck_DS_r1 run utilizes the original dataset, which has not been augmented. The relatively low pun detection accuracy of approximately 37% raises concerns, especially considering the subsequent experiments detailed in the next section. These experiments, exclusively conducted on the training data, exhibited a significantly higher accuracy rate of around 72%. To put this into perspective, even a strategy of random guessing in a binary classification task would theoretically result in an accuracy of around 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Further experiments on train set</head><p>In addition to the work we submitted to JOKER, which involved making predictions on the test set, we conducted more experiments using only the training set, which we further divided into testing and training subsets via the application of K-Fold Cross Validation with k = 4.</p><p>Table <ref type="table" coords="5,127.74,429.52,5.17,10.91" target="#tab_1">2</ref> provides evaluations of different datasets generated using various DA techniques. The technique "None" implies an unaltered dataset and can be used as a standard for other techniques to be compared against. Note that due to the randomness in machine learning processes, we noticed fluctuations of around 1 percent in the results. Therefore, it is challenging to determine if any of those techniques provide a significant advantage for data augmentation in detecting wordplay.</p><p>According to the evaluation table, back-translation exhibits a notable increase in size, because every sample is once Back-translated from German and French. Here, adding filters helps to significantly reduce the data size, but it seems to have almost no effect on performance.</p><p>Synonym replacement via WordNet and Word2Vec yields similar performance. The use of WordNet with filters seems to produce the highest accuracy of 74%.</p><p>Pegasus paraphrase technique's unfiltered values are missing, because it generated so many samples (including bad quality ones) which were too many to train the classifier on our setup. But after applying filters, and reducing the size this technique also does not show an increased performance.</p><p>The application of filters in the extension technique results in the highest recall value (92.61%), contributing to the best F1 score in the table (80.07%), but with only a minor improvement in accuracy. Overall, synonym replacement (WordNet) with filters seems to be the most balanced technique in terms of performance and accuracy. Yet, if recall and F1 score are of higher importance, extension with filters could be a preferable choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In conclusion, we found that the accuracy of detecting puns on the JOKER Task 1 test data was much lower compared to our experiments where we divided the training data into separate training and testing groups. One explanation could be that training and test data have different origins. Our attempts to enhance the performance via data augmentation approaches specific to humorous text and pun wordplay did not yield substantial improvements in pun detection capabilities. It is worth mentioning that larger language models, such as GPT-4 might have a superior accuracy on pun detection. Therefore, it may be an area worth exploring for further enhancement of pun detection techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,384.98,81.83"><head>Table 1 Task</head><label>1</label><figDesc></figDesc><table coords="5,110.15,102.49,363.82,69.82"><row><cell>1 -Pun Detection in English</cell><cell></cell><cell></cell><cell></cell></row><row><cell>run ID</cell><cell>#</cell><cell cols="3">Precision Recall F1</cell><cell>Accuracy</cell></row><row><cell>Innsbruck_DS_r1</cell><cell cols="2">3,183 27.32</cell><cell>86.89</cell><cell>41.57 37.92</cell></row><row><cell>Innsbruck_DS_synonym</cell><cell cols="2">3,183 27.15</cell><cell>86.89</cell><cell>41.37 37.41</cell></row><row><cell cols="3">Innsbruck_DS_backtranslation 3,183 27.35</cell><cell>84.91</cell><cell>41.38 38.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,356.41,201.39"><head>Table 2</head><label>2</label><figDesc>Data Augmentation Techniques Overview</figDesc><table coords="6,149.88,122.10,295.52,169.77"><row><cell>Technique</cell><cell>Size</cell><cell cols="3">Precision Recall F1</cell><cell>Accuracy</cell></row><row><cell>None</cell><cell>5,293</cell><cell>73.09</cell><cell>83.89</cell><cell cols="2">77.87 72.47</cell></row><row><cell>Back-translation</cell><cell cols="2">15,879 71.80</cell><cell>88.84</cell><cell cols="2">79.41 73.17</cell></row><row><cell>+ Filters</cell><cell>8,722</cell><cell>69.96</cell><cell>89.78</cell><cell>78.6</cell><cell>72.2</cell></row><row><cell>Synonym (Wordnet)</cell><cell cols="2">10,586 72.10</cell><cell>90.03</cell><cell cols="2">80.05 73.86</cell></row><row><cell>+ Filters</cell><cell>9,587</cell><cell>72.99</cell><cell>88.97</cell><cell cols="2">80.16 74.32</cell></row><row><cell cols="3">Synonym (Word2vec) 10,586 72.20</cell><cell>89.02</cell><cell cols="2">79.71 73.63</cell></row><row><cell>+ Filters</cell><cell>7,776</cell><cell>72.96</cell><cell>87.75</cell><cell cols="2">79.65 73.70</cell></row><row><cell>Pegasus Paraphrase</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+ Filters</cell><cell cols="2">18,604 70.96</cell><cell>87.51</cell><cell cols="2">78.36 72.81</cell></row><row><cell>Shortening</cell><cell cols="2">10,586 70.85</cell><cell>91.32</cell><cell cols="2">79.74 72.97</cell></row><row><cell>+ Filters</cell><cell cols="2">10,546 72.38</cell><cell>87.15</cell><cell cols="2">79.02 73.06</cell></row><row><cell>Extension</cell><cell>8,525</cell><cell>71.19</cell><cell>89.92</cell><cell cols="2">79.46 73.09</cell></row><row><cell>+ Filters</cell><cell>8,471</cell><cell>70.52</cell><cell cols="3">92.61 80.07 73.21</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,557.77,393.33,10.91;6,112.66,571.32,394.53,10.91;6,112.66,584.87,393.33,10.91;6,112.33,598.42,393.65,10.91;6,112.66,611.96,393.33,10.91;6,112.66,625.51,395.00,10.91;6,112.66,639.06,187.11,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,472.07,557.77,33.92,10.91;6,112.66,571.32,322.23,10.91">Science for fun: The CLEF 2023 JOKER track on automatic wordplay analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M P</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-28241-6_63</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,143.77,598.42,362.22,10.91;6,112.66,611.96,92.97,10.91">Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023</title>
		<title level="s" coord="6,125.53,626.53,153.77,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023-06">April 2-6. 2023</date>
			<biblScope unit="volume">13982</biblScope>
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Part III</note>
</biblStruct>

<biblStruct coords="6,112.66,652.61,393.33,10.91;6,112.66,666.16,311.37,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="6,326.58,652.61,179.40,10.91;6,112.66,666.16,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,86.97,395.01,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="7,227.57,86.97,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,100.52,393.32,10.91;7,112.66,114.06,354.27,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,380.15,100.52,125.83,10.91;7,112.66,114.06,101.89,10.91">Introduction to wordnet: An on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,223.00,114.06,165.07,10.91">International journal of lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,127.61,393.33,10.91;7,112.39,141.16,230.07,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="7,298.68,127.61,207.31,10.91;7,112.39,141.16,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,154.71,393.33,10.91;7,112.66,168.26,395.17,10.91;7,112.66,181.81,198.38,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,215.34,154.71,290.65,10.91;7,112.66,168.26,35.77,10.91">Improving short text classification through global augmentation methods</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sefara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.16,168.26,335.66,10.91;7,112.66,181.81,67.89,10.91">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,195.36,394.53,10.91;7,112.66,208.91,393.33,10.91;7,112.66,222.46,394.52,10.91;7,112.28,236.01,395.00,10.91;7,112.66,249.56,294.08,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,448.32,208.91,57.66,10.91;7,112.66,222.46,148.63,10.91">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-4020</idno>
		<ptr target="https://aclanthology.org/P18-4020.doi:10.18653/v1/P18-4020" />
	</analytic>
	<monogr>
		<title level="m" coord="7,284.95,222.46,217.38,10.91">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,263.11,393.33,10.91;7,112.66,276.66,393.33,10.91;7,112.66,290.20,393.32,10.91;7,112.66,303.75,394.03,10.91;7,112.66,317.30,72.36,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,269.04,263.11,236.95,10.91;7,112.66,276.66,135.36,10.91">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/zhang20ae.html" />
	</analytic>
	<monogr>
		<title level="m" coord="7,396.38,276.66,109.60,10.91;7,112.66,290.20,213.32,10.91;7,401.95,291.22,104.03,9.72;7,112.66,304.77,75.65,9.72">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="7,112.66,330.85,393.33,10.91;7,112.66,344.40,275.90,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,297.33,330.85,208.66,10.91;7,112.66,344.40,45.96,10.91">Automatic keyword extraction from individual documents</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,167.65,344.40,165.07,10.91">Text mining: applications and theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,357.95,393.33,10.91;7,112.66,371.50,253.81,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,412.10,357.95,93.89,10.91;7,112.66,371.50,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,262.00,371.50,56.95,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,385.05,393.33,10.91;7,112.66,398.60,393.33,10.91;7,112.66,412.15,393.32,10.91;7,112.66,425.70,51.08,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,322.32,385.05,183.67,10.91;7,112.66,398.60,184.14,10.91">The joker corpus: English-french parallel data for multilingual wordplay recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,318.98,398.60,187.00,10.91;7,112.66,412.15,331.26,10.91">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>SIGIR, ACM Press</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,439.25,394.53,10.91;7,112.66,452.79,122.77,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="7,219.42,439.25,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,466.34,393.33,10.91;7,112.66,479.89,386.09,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10957</idno>
		<title level="m" coord="7,337.13,466.34,168.86,10.91;7,112.66,479.89,256.00,10.91">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
