<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,362.81,15.42;1,88.69,106.66,388.90,15.42">Overview of JOKER 2023 Automatic Wordplay Analysis Task 2 -Pun Location and Interpretation</title>
				<funder ref="#_GsuZ69S">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">La Maison des sciences de l&apos;homme en Bretagne</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,79.43,11.96"><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Bretagne Occidentale</orgName>
								<address>
									<settlement>HCTI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.35,134.97,67.33,11.96"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Austrian Research Institute for Artificial Intelligence (OFAI)</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.33,134.97,101.14,11.96"><forename type="first">Anne-Gwenn</forename><surname>Bosser</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Lab-STICC CNRS UMR 6285</orgName>
								<orgName type="institution">École Nationale d&apos;Ingénieurs de Brest</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.11,134.97,70.21,11.96"><forename type="first">Victor</forename><surname>Manuel</surname></persName>
						</author>
						<author>
							<persName coords="1,454.32,134.97,30.00,11.96;1,89.29,148.92,42.14,11.96"><forename type="first">Palma</forename><surname>Preciado</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Bretagne Occidentale</orgName>
								<address>
									<settlement>HCTI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Centro de Investigacion en Computacion (CIC)</orgName>
								<orgName type="institution">Instituto Politécnico Nacional (IPN)</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,149.37,148.92,75.90,11.96"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Centro de Investigacion en Computacion (CIC)</orgName>
								<orgName type="institution">Instituto Politécnico Nacional (IPN)</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.28,148.92,63.85,11.96"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Innsbruck</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,362.81,15.42;1,88.69,106.66,388.90,15.42">Overview of JOKER 2023 Automatic Wordplay Analysis Task 2 -Pun Location and Interpretation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8B7024C95C5FEDEDAE4F477A0699F5B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>wordplay</term>
					<term>puns</term>
					<term>computational humour</term>
					<term>wordplay interpretation</term>
					<term>wordplay detection</term>
					<term>pun location</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an overview of Task 2 of the JOKER-2023 track on automatic wordplay analysis. The goal of the JOKER track series is to bring together linguists, translators, and computer scientists to foster progress in the automatic interpretation, generation, and translation of wordplay. Task 2 is focussed on pun location and interpretation. Automatic pun interpretation is important for advancing natural language understanding, enabling humor generation, aiding in translation and cross-linguistic understanding, enhancing information retrieval, and contributing to the field of computational creativity. In this overview, we present the general setup of the shared task we organized as part of the CLEF-2023 evaluation campaign, the participants' approaches, and the quantitative results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents details and results of Task 2 of the JOKER-2023 Track on automatic wordplay analysis 1 which was held as part of the 14th Conference and Labs of the Evaluation Forum (CLEF 2023) 2 . JOKER-2023 is the second edition of the JOKER track which was started in 2022 <ref type="bibr" coords="1,89.29,485.98,11.34,10.91" target="#b0">[1]</ref>. Task 2 specifically focusses on the identification and interpretation of puns. The other two tasks in the track, on pun detection and translation, are covered in separate papers <ref type="bibr" coords="1,466.26,499.53,11.49,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,480.48,499.53,7.77,10.91" target="#b2">3]</ref>; an overall overview of the track <ref type="bibr" coords="1,220.36,513.08,12.84,10.91" target="#b3">[4]</ref> is also available.</p><p>Pun interpretation refers to the process of understanding and deciphering puns. A pun is a form of wordplay, usually humorous, that exploits multiple meanings of a word, or words with similar sounds but different meanings. Automatic pun interpretation plays a role in advancing natural language understanding, enabling humour generation, aiding in translation and crosslinguistic comprehension, enhancing information retrieval, and contributing to the field of computational creativity. Interpreting a pun involves two steps: recognizing the word or phrase carrying multiple meanings (pun location), and then identifying those meanings (pun interpretation). Pun interpretation systems need to identify potential sources of ambiguity in the context and narrow down the possible interpretations.</p><p>Pun comprehension, when done by humans, involves recognizing that there is a pun, and then understanding it, thereby finding humour in the unexpected or clever connection between the different meanings of the words involved. Automatic pun interpretation refers to the use of computational techniques and algorithms to automatically analyze and understand puns without human intervention. Some systems analyze the words involved in the pun, including their meanings and relationships with other words based on lexical resources such as dictionaries, thesauri, or semantic networks such as WordNet <ref type="bibr" coords="2,307.83,493.49,11.34,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,321.90,493.49,7.56,10.91" target="#b5">6]</ref>. Systems can consider the surrounding context of the pun to gain a better understanding of the intended meaning. This can involve analyzing the broader text or discourse in which the pun appears, including syntactic and semantic features.</p><p>Humour is an important aspect of interpersonal interactions and our social behavior. Humour can depend on subjective factors, which makes its automatic processing challenging. Thus, dealing with humour, even in its written form, becomes a rather complex task even if at first sight the problem may seem trivial. Wordplay processing is a specific task in a broader area of automatic humour processing which involves detection, classification, generation, or translation of humour.</p><p>The paper discusses two distinct subtasks of JOKER-2023: Task 2.1 on pun location in English, French, and Spanish; and Task 2.2 on pun interpretation in English. Each subtask is presented individually, covering various aspects such as the objectives, data collection process, evaluation metrics, approaches used by the participants, and the corresponding results. In total, nine teams submitted 64 runs in total for Task 2; the breakdown across teams, languages, and subtasks is given in Table <ref type="table" coords="3,154.97,114.06,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 2.1: Pun location</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Task description</head><p>Pun location (Task 2.1) is a finer-grained version of pun detection. The goal is to identify the words that carry the double meaning in a text which is known a priori to contain a pun. The double meaning here produces the humorous effect of wordplay. For example, the first of the following sentences contains a pun where the word propane evokes the similar-sounding word profane, although the latter is not included in the sentence explicitly, while the second sentence contains a pun exploiting two distinct meanings of the word interest:</p><p>• (1) When the church bought gas for their annual barbecue, proceeds went from the sacred to the propane. • (2) I used to be a banker but I lost interest.</p><p>Note that for the pun detection task which is the basis of Task 1 of JOKER-2023 Track, the correct answer for these two instances would be "true". Now, for the pun location task, the correct answers are respectively "propane" and "interest". System performance is reported in terms of accuracy for this subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data</head><p>The pun location data is drawn from the positive examples of JOKER Task 1 <ref type="bibr" coords="3,433.62,431.24,11.43,10.91" target="#b1">[2,</ref><ref type="bibr" coords="3,447.78,431.24,7.62,10.91" target="#b3">4]</ref>, with each text being accompanied by an annotation that reproduces the word being punned upon, as described above. These positive examples for the pun detection task were short jokes in English, French, and Spanish with single puns. A detailed description of the English and French pun detection and location datasets can be found in our SIGIR 2023 resource paper <ref type="bibr" coords="3,439.56,485.44,11.43,10.91" target="#b6">[7]</ref>.</p><p>The statistics on data per language are given in Table <ref type="table" coords="3,343.24,498.99,3.79,10.91" target="#tab_1">2</ref>. These statistics present the actual numbers used for the assessment, representing the effective figures for both the test and training data. Nevertheless, when providing files to the participants, we included the training data within the input of the test file. Incorporating the training data within the dataset, for which participants were required to make predictions, enables a comprehensive evaluation of the systems' performance on both the training and testing data. By incorporating the training data, we can assess the systems' ability to generalize to unseen test data by observing their performance on familiar training examples.</p><p>The test and training data sets were provided to participants as JSON or delimited text files with fields containing the text of the punning joke and a unique ID. For training data for the pun location task, there is an additional field reproducing the pun word. System output is expected as a JSON or delimited text file with fields for the run ID, text ID, the pun word, and a boolean flag indicating whether the run is manual or automatic. [{"id":"en_135", "text":"Cleopatra was the Pharaohs one of all."}, {"id":"en_226", "text":"At a flower show the first prize is often a bloom ribbon."} ]</p><p>Qrels. We provide training data as JSON or TSV qrels files with the following fields: id a unique identifier from the input file location the portion of the text containing the wordplay Example of a qrel file:</p><p>[{"id":"en_135","location":"Pharaohs"}, {"id":"en_226", "location":"bloom"} ]</p><p>Output format. Systems were expected to submit their results in a TREC-style JSON or TSV file with the following fields:</p><p>run_id run ID starting with &lt;team_id&gt;_&lt;task_id&gt;_&lt;method_used&gt; -e.g., UBO_task_2.1_-TFIDF manual flag indicating if the run is manual 0,1 id a unique identifier from the input file location the portion of the text containing the wordplay Example of an output file:</p><p>[{"run_id":"team1_task_2.1_TFIDF", "manual":0, "id":"en_135", "location":"Pharaohs"}, {"run_id":"team1_task_2.1_TFIDF", "manual":0, "id":"en_226", "location":"bloom"}]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Participants' approaches</head><p>Eight teams participated in Task 2.1:</p><p>1. The AKRaNLU team participants <ref type="bibr" coords="5,272.99,313.43,12.99,10.91" target="#b4">[5]</ref> employ the token classification method with a tagging schema that relies on assigning a tag of 1 to every pun word and 0 to every word that is not a punning word. 2. The MiCroGerk team <ref type="bibr" coords="5,216.65,355.43,12.99,10.91" target="#b7">[8]</ref> chose an large language model (LLM) approach for Task 2.1, using T5 (SimpleT5), BLOOM, and models from OpenAI and AI21. They also submitted a baseline that uses last word in the sentence as a prediction, as well as a random baseline.</p><p>It is noteworthy that the BLOOM model presented the worst results compared to the others. 3. The Smroltra team <ref type="bibr" coords="5,204.54,424.53,13.00,10.91" target="#b5">[6]</ref> observed that models based on GPT-3, SpaCy, T5, and BLOOM showed very good performance when it came to Spanish and English, while for French the results were worse. This was particularly the case for SpaCy, which is believed to be not as developed for French as for English. 4. TeamCAU <ref type="bibr" coords="5,164.28,480.09,12.70,10.91" target="#b8">[9]</ref> used various LLMs. T5 showed good results in comparison to BLOOM and models from AI21 (albeit for partial runs only). 5. FastText, Ridge, Naive Bayes, SimpleT5, and SimpleTransformersT5 were used by the participants of ThePunDetectives team <ref type="bibr" coords="5,292.23,522.09,16.23,10.91" target="#b9">[10]</ref>. They found the best results to be produced by the pre-trained models. In particular, T5 achieved good performance, as predicted by the authors. 6. For the location tasks, the UBO team <ref type="bibr" coords="5,282.95,564.09,17.91,10.91" target="#b10">[11]</ref> opted to use T5 (SimpleT5). 7. The Croland team <ref type="bibr" coords="5,199.42,579.00,17.91,10.91" target="#b11">[12]</ref> used GPT-3. 8. The Les_miserables team (who did not submit a system description paper) submitted two baseline runs, one where the system selects the final word of the sentence as the pun location, and another run that randomly predicts words; they also submitted a run using the T5 (SimpleT5) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Results</head><p>The eight teams submitted 47 runs in total for all three languages. Table <ref type="table" coords="6,421.56,107.54,5.17,10.91" target="#tab_2">3</ref> reports the participants' results for wordplay location tasks in English, French, and Spanish on the test set. This table provides a comprehensive overview of the participants' systems' performances and their respective scores or metrics achieved in locating wordplay instances in the given languages. As some participants submitted only partial runs, we provide two sets of accuracy scores: those labeled A are based on the total number of instances in the test set, while those labeled A * are calculated only using the actual number of attempted instances (#). Accuracy scores for pun location in English and Spanish (A ≈ 80) are roughly twice as good as those for French (A ≈ 40). For comparison, the predictions made on the last word for test sets in English and Spanish are around 50% while for French this score goes almost up to 30%. Thus, the improvement for French over a very simple baseline is rather not high. The significant improvement over this simple last-word baseline for English and Spanish could be explained by the fact that participants used large language models (e.g., GPT-3 or BLOOM) that might have included in their training data some of the same puns found in our corpus. By contrast, the French wordplay data was largely constructed by us and not previously published online.</p><p>Table <ref type="table" coords="6,127.41,310.77,5.15,10.91" target="#tab_3">4</ref> shows the results of the participants for wordplay location in English, French, and Spanish on the training set. We observe a significant difference between the best performance on the test and training data for French obtained by T5 model trained on our data as well as the results of the best-performing team AKRaNLU for French. These results suggest overfitting issues. The performance of a few models is comparable to or even lower than the predictions made by returning the last word in the text.</p><p>Several teams submitted runs by applying the same methods yet implemented, trained, or fine-tuned differently. Significant disparities can be observed, even in the last-word baseline outcomes (such as Les_miserables_word and MiCroGerk_lastWord), which could be attributed to variations in tokenization methods. Distinguishing variations in prediction accuracy are also noticeable between differently trained T5 models as well as prompt-based language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 2.2: Pun interpretation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task description</head><p>In this task, systems must describe the semantics -i.e., the two meanings -of the pun. In JOKER-2023, these semantic annotations are in the form of a pair of lemmatized word sets. Following the practice used in lexical substitution datasets, these word sets contain the synonyms (or if absent, then hypernyms) of the two words involved in the pun, except for any synonyms/hypernyms that happen to share the same spelling with the pun as written.</p><p>For example, for the punning joke introduced in Example 1 above, the word sets are {gas, fuel} and {profane}, and for Example 2, the word sets are {involvement} and {fixed charge, fixed cost, fixed costs}.</p><p>Results for Task 2.2 are scored as the average score for each of punning word senses. Systems need to guess only one word for each sense of the pun; a guess is considered correct if it matches any of the words in the gold-standard set. For example, a system guessing {fuel}, {profane} would </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data</head><p>For the English pun interpretation data, we manually annotated each pun according to its senses in WordNet 3.1 and then automatically extracted the synonyms (or if there were none, the hypernyms) of those words to form the two word sets. In some cases, one or both of the senses of the pun was not present in WordNet, or WordNet contained neither synonyms nor hypernyms for the annotated senses. (This was particularly the case with adjectives and adverbs, which WordNet does not arrange into a hypernymic hierarchy.) In these cases, we sourced the synonym/hypernym sets from human annotators. For the French data, we used a simplified version of the annotation made in JOKER-2022 <ref type="bibr" coords="8,300.08,550.98,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input format</head><p>The base data for test and training are provided in JSON and CSV formats with the following fields: id a unique identifier text the text of the instance of wordplay Input example:</p><p>[{"id":"en_135", "text":"Cleopatra was the Pharaohs one of all."}, {"id":"en_226", "text":"At a flower show the first prize is often a bloom ribbon."} ]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qrels</head><p>We provide training data in the format of JSON or TSV qrels files with the following fields:</p><p>id a unique identifier from the input file location the portion of the text containing the wordplay interpretation synonyms or hypernyms of the two meanings of the wordplay Example of a qrel file:</p><p>[{"id":"en_135", "location":"Pharaohs", "interpretation":"Pharaoh;Pharaoh of Egypt / fair"}, {"id":"en_226", "location":"bloom", "interpretation":"blossom;flower / bluish;blue;blueish"} ]</p><p>Output format Systems were expected to provide their results as a TREC-style JSON or TSV format with the following fields:</p><p>run_id run ID starting with &lt;team_id&gt;_&lt;task_id&gt;_&lt;method_used&gt; -e.g., UBO_task_2.2_-BLOOM manual flag indicating if the run is manual 0,1 id a unique identifier from the input file location the portion of the text containing the wordplay interpretation synonyms or hypernyms of the wordplay meanings</p><p>Example of an output file:</p><p>[{"run_id":"team1_task_2.2_manual", "manual":1, "id":"en_135", "location":"Pharaohs", "interpretation":"Pharaoh;Pharaoh of Egypt / fair"}, {"run_id":"team1_task_2.2_manual", "manual":1, "id":"en_226", "location":"bloom", "interpretation":"blossom;flower / bluish;blue;blueish"}]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Participants' approaches</head><p>The teams' approaches were as follows:</p><p>1. For pun interpretation, the AKRaNLU team participants <ref type="bibr" coords="10,362.63,309.05,12.68,10.91" target="#b4">[5]</ref> used the results from the pun location subtask to disambiguate the appropriate senses of the pun word based on the sentence content and find two synonyms for those senses, sourced from WordNet, that were most similar to sentence embedding. 2. The MiCroGerk team <ref type="bibr" coords="10,213.84,364.60,12.81,10.91" target="#b7">[8]</ref> submitted four runs for the interpretation task based on LLMs, such as T5 (SimpleT5), BLOOM, and models from OpenAI and AI21. 3. The Smroltra team <ref type="bibr" coords="10,203.61,393.05,12.97,10.91" target="#b5">[6]</ref> submitted six runs based on GPT-3 and BLOOM, SpaCy, T5 and their combinations with WordNet for location prediction. 4. The UBO team <ref type="bibr" coords="10,183.33,421.51,17.75,10.91" target="#b10">[11]</ref> applied T5 (SimpleT5) to predict the interpretation of puns in English. 5. The UBO-RT team <ref type="bibr" coords="10,201.56,436.41,17.92,10.91" target="#b12">[13]</ref> post-edited output of ChatGPT (C&amp;O in Tables <ref type="table" coords="10,433.96,436.41,5.08,10.91" target="#tab_4">5</ref> and<ref type="table" coords="10,460.92,436.41,3.58,10.91" target="#tab_5">6</ref>). A zeroshot strategy was used in their approach and the analysis of the results reveals quite poor capabilities of ChatGPT in interpreting puns, especially those involving homophonic components. 6. The Croland team <ref type="bibr" coords="10,199.42,491.96,17.91,10.91" target="#b11">[12]</ref> used GPT-3. 7. The Les_miserables team (who did not submit a system description paper) submitted a run using the T5 (SimpleT5) model to predict pun interpretation in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>We show in Table <ref type="table" coords="10,170.02,570.14,5.02,10.91" target="#tab_4">5</ref> the results of the pun interpretation task for English. We do not report the results for French as only two teams submitted runs for this language, one of which was heavily post-processed manually. This means it is impossible to draw any conclusions on the French dataset.</p><p>For the English pun interpretation task, seven teams submitted 15 runs in total. As we expected, the majority of participants opted to use LLMs, resulting in the generation of partial runs due to the efficiency constraints associated with these models. Only five runs out of the total number of submissions involved the entire testing data <ref type="bibr" coords="10,369.49,664.99,12.08,10.91" target="#b0">(1,</ref><ref type="bibr" coords="10,381.57,664.99,16.11,10.91">192)</ref>, hence the comparison is somewhat difficult. Nevertheless, when focussing on the full runs, we observe that the maximum accuracy was obtained by the team Les_miserables who used the T5 model, achieving 47.4%. A similar result was also obtained by the UBO team who also applied the T5 model. For comparison, a baseline approach with SpaCy gives 19.76% accuracy. This underscores the utility of large language models for the interpretation of wordplay. The best partial score was obtained by the UBO-RT team who used the post-processed results generated by ChatGPT. But even this heavily manually post-processed run obtained only 70%. Additionally, for completeness, Table <ref type="table" coords="11,275.15,437.33,5.17,10.91" target="#tab_4">5</ref> provides results of participating systems for the training data. Performance on the training data remains low, which may suggest the overall difficulty of this task. On the other hand, results of some models such us T5 and SpaCy exhibited notably superior performance on the training set, which suggests the possibility of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have described Task 2 of the JOKER track at CLEF 2023, consisting of pun location and interpretation challenges. We extended our previously described dataset <ref type="bibr" coords="11,492.99,550.15,13.00,10.91" target="#b6">[7]</ref> by introducing semantic annotation for wordplay in English and French. Furthermore, we constructed a corpus for pun location in Spanish.</p><p>Multiple teams submitted runs using similar methods, but with variations in implementation, training, or fine-tuning approaches for both subtasks. These variations entailed large differences in the performance of the systems.</p><p>Our results in general suggest that wordplay location is still a challenge for LLMs despite their recent significant advances. Interestingly, we found that the results for the French language when it comes to pun location are quite low (half of the scores of English and Spanish) which we attribute to different data creation procedures. Some of the puns found in our corpus for English and Spanish might have been "known" by large language models that were used by participants, as the data were sourced from the web for these languages. On the other hand, French data was novel and largely constructed by us. This calls for the future to construct wordplay datasets from scratch rather than sourcing humour from external sources like the web.</p><p>The results suggest the overfitting problem for models trained on our data (e.g., via the SimpleT5 library). The difference between training and test data observed for the prompt-based models is small as they are not actually trained on our data, but might be influenced by examples from the training set used in the prompts.</p><p>Automatic pun interpretation was the second subtask of Task 2. It is quite a challenging task due to the inherent ambiguity and creativity of puns, yet it fits into the recent focus on explainable AI and explainable/interpretable decision systems. Puns often rely on cultural knowledge, background information, and linguistic subtleties that can be difficult to capture computationally. Still, researchers continue to explore various approaches, including rule-based methods, machine learning models, and deep learning techniques to improve automatic pun interpretation systems. Our results data indicate that the T5 model performs well for this task. However, we could compare the results only for English language, as for French there were only two runs that were not fully automatic. We received many partial runs due to token/time constraints of LLMs and, therefore, apart from effectiveness, the efficiency of the approaches should be considered in future research. The results suggest the difficulty of pun interpretations at least in the particular settings that we use in subtask 2.2 (reliance on WordNet synonyms and hypernyms).</p><p>Additional information on the track is available on the JOKER website: http://www. joker-project.com/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,322.17,200.16"><head>Table 1</head><label>1</label><figDesc>Statistics on submitted runs by task</figDesc><table coords="2,184.11,122.10,227.05,168.54"><row><cell>Team</cell><cell cols="2">Task 2.1:</cell><cell></cell><cell cols="3">Task 2.2: Total</cell></row><row><cell></cell><cell cols="2">Location</cell><cell></cell><cell cols="2">Interpret.</cell><cell></cell></row><row><cell></cell><cell cols="4">EN FR ES EN</cell><cell>FR</cell><cell></cell></row><row><cell>Croland</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>4</cell></row><row><cell>Les_miserables</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>-</cell><cell>10</cell></row><row><cell>MiCroGerk</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>10</cell></row><row><cell>Smroltra</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>6</cell><cell>-</cell><cell>18</cell></row><row><cell>TeamCAU</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3</cell></row><row><cell>ThePunDetectives</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5</cell></row><row><cell>UBO</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>4</cell></row><row><cell>UBO-RT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>2</cell></row><row><cell>AKRaNLU</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>8</cell></row><row><cell>Total</cell><cell cols="3">25 11 11</cell><cell>15</cell><cell>2</cell><cell>64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,90.49,418.28,190.75"><head>Table 2</head><label>2</label><figDesc>Dataset statistics for Task 2</figDesc><table coords="4,89.29,119.88,417.99,161.35"><row><cell cols="2">Language Train</cell><cell>Test</cell></row><row><cell>English</cell><cell cols="2">2,315 1,205</cell></row><row><cell>French</cell><cell cols="2">2,000 4,655</cell></row><row><cell>Spanish</cell><cell>876</cell><cell>960</cell></row><row><cell cols="3">Input format. The base data is provided in JSON and CSV formats with the following fields:</cell></row><row><cell>id a unique identifier</cell><cell></cell><cell></cell></row><row><cell>text the text of the instance of wordplay</cell><cell></cell><cell></cell></row><row><cell>Input example:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,416.99,449.09"><head>Table 3</head><label>3</label><figDesc>Results for Task 2.1 (pun location) on the test data</figDesc><table coords="7,272.68,122.10,194.79,8.87"><row><cell>EN</cell><cell>FR</cell><cell>ES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,411.02,433.85"><head>Table 4</head><label>4</label><figDesc>Results for Task 2.1 (pun location) on the training data</figDesc><table coords="8,284.16,122.06,184.66,7.98"><row><cell>EN</cell><cell>FR</cell><cell>ES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.99,90.49,346.27,225.29"><head>Table 5</head><label>5</label><figDesc>Results for Task 2.2 (pun interpretation) on the test data</figDesc><table coords="11,160.01,122.10,275.25,193.67"><row><cell>run</cell><cell>count</cell><cell cols="2">score part_score</cell></row><row><cell>C&amp;O_task_2.2_Chat GPT</cell><cell>92</cell><cell>5.45%</cell><cell>70.65%</cell></row><row><cell>Croland_task_2_EN_GPT3</cell><cell>29</cell><cell>0.08%</cell><cell>3.45%</cell></row><row><cell>Les_miserables_simplet5</cell><cell cols="2">1,192 47.40%</cell><cell>47.40%</cell></row><row><cell>MiCroGerk_task_2_EN_AI21</cell><cell>11</cell><cell>0.46%</cell><cell>50.00%</cell></row><row><cell>MiCroGerk_task_2_EN_BLOOM</cell><cell>2</cell><cell>0.04%</cell><cell>25.00%</cell></row><row><cell>MiCroGerk_task_2_EN_OpenAI</cell><cell>11</cell><cell>0.34%</cell><cell>36.36%</cell></row><row><cell>MiCroGerk_task_2_EN_SimpleT5</cell><cell>39</cell><cell>1.59%</cell><cell>48.72%</cell></row><row><cell>Smroltra_task_2_Bloom</cell><cell>32</cell><cell>0.59%</cell><cell>21.88%</cell></row><row><cell>Smroltra_task_2_GPT3</cell><cell>32</cell><cell>0.59%</cell><cell>21.88%</cell></row><row><cell>Smroltra_task_2_GPT3_WN</cell><cell>32</cell><cell>1.09%</cell><cell>40.63%</cell></row><row><cell>Smroltra_task_2_SimpleT5_WN</cell><cell cols="2">1192 41.44%</cell><cell>41.44%</cell></row><row><cell>Smroltra_task_2_bloom_WN</cell><cell>32</cell><cell>0.80%</cell><cell>29.69%</cell></row><row><cell>Smroltra_task_2_spaCy_WN</cell><cell cols="2">1,192 19.76%</cell><cell>19.76%</cell></row><row><cell>UBO_task_2.2_SimpleT5</cell><cell cols="2">1,192 46.85%</cell><cell>46.85%</cell></row><row><cell cols="3">akranlu_task_2.2_sentembwordnet 1,192 39.77%</cell><cell>39.77%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,90.49,346.06,225.29"><head>Table 6</head><label>6</label><figDesc>Results for Task 2.2 (pun interpretation) on the training data</figDesc><table coords="12,160.23,122.10,274.82,193.67"><row><cell>run</cell><cell>count</cell><cell cols="2">score part_score</cell></row><row><cell>C&amp;O_task_2.2_Chat GPT</cell><cell>201</cell><cell>6.57%</cell><cell>75.62%</cell></row><row><cell>Croland_task_2_EN_GPT3</cell><cell>61</cell><cell>0.22%</cell><cell>8.20%</cell></row><row><cell>Les_miserables_simplet5</cell><cell cols="2">2,315 52.66%</cell><cell>52.66%</cell></row><row><cell>MiCroGerk_task_2_EN_AI21</cell><cell>30</cell><cell>0.76%</cell><cell>58.33%</cell></row><row><cell>MiCroGerk_task_2_EN_BLOOM</cell><cell>7</cell><cell>0.19%</cell><cell>64.29%</cell></row><row><cell>MiCroGerk_task_2_EN_OpenAI</cell><cell>30</cell><cell>0.35%</cell><cell>26.67%</cell></row><row><cell>MiCroGerk_task_2_EN_SimpleT5</cell><cell>119</cell><cell>4.34%</cell><cell>84.45%</cell></row><row><cell>Smroltra_task_2_Bloom</cell><cell>68</cell><cell>0.82%</cell><cell>27.94%</cell></row><row><cell>Smroltra_task_2_GPT3</cell><cell>68</cell><cell>0.30%</cell><cell>10.29%</cell></row><row><cell>Smroltra_task_2_GPT3_WN</cell><cell>68</cell><cell>1.34%</cell><cell>45.59%</cell></row><row><cell>Smroltra_task_2_SimpleT5_WN</cell><cell cols="2">2,315 53.50%</cell><cell>53.50%</cell></row><row><cell>Smroltra_task_2_bloom_WN</cell><cell>68</cell><cell>1.06%</cell><cell>36.03%</cell></row><row><cell>Smroltra_task_2_spaCy_WN</cell><cell cols="2">2,315 52.74%</cell><cell>52.74%</cell></row><row><cell>UBO_task_2.2_SimpleT5</cell><cell cols="2">2,315 67.15%</cell><cell>67.15%</cell></row><row><cell cols="3">akranlu_task_2.2_sentembwordnet 2,315 48.10%</cell><cell>48.10%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project has received a government grant managed by the <rs type="funder">National Research Agency</rs> under the program "<rs type="programName">Investissements d'avenir" integrated into France 2030</rs>, with the Reference <rs type="grantNumber">ANR-19-GURE-0001</rs>. JOKER is supported by <rs type="funder">La Maison des sciences de l'homme en Bretagne</rs>. For their help and support in the first Spanish pun translation contest, we thank <rs type="person">Carolina Palma Preciado</rs>, <rs type="person">Leopoldo Jesús Gutierrez Galeano</rs>, <rs type="person">Khatima El Krirh</rs>, <rs type="person">Nathalie Narváez Bruneau</rs>, and <rs type="person">Rachel Kinlay</rs>. We also thank all other colleagues and students who participated in data construction, the translation contests, and the CLEF JOKER track.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GsuZ69S">
					<idno type="grant-number">ANR-19-GURE-0001</idno>
					<orgName type="program" subtype="full">Investissements d&apos;avenir&quot; integrated into France 2030</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,251.20,394.53,10.91;13,112.66,264.75,393.33,10.91;13,112.48,278.30,395.34,10.91;13,112.66,291.85,394.53,10.91;13,112.66,305.40,395.17,10.91;13,112.66,318.95,393.59,10.91;13,112.28,332.50,394.91,10.91;13,112.66,346.05,302.06,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,448.09,264.75,57.90,10.91;13,112.48,278.30,327.40,10.91">Overview of JOKER@CLEF 2022: Automatic wordplay and humour translation workshop</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Élise</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boccou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Digue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Damoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jeanjean</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-13643-6_27</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,288.21,305.40,219.62,10.91;13,112.66,318.95,393.59,10.91;13,112.28,332.50,84.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction: Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,307.53,333.51,151.28,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
			<biblScope unit="page" from="447" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,359.59,393.33,10.91;13,112.48,373.14,350.25,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="13,451.47,359.59,54.52,10.91;13,112.48,373.14,281.59,10.91">Overview of JOKER 2023 automatic wordplay analysis task 1 -pun detection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M P</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,386.69,393.33,10.91;13,112.48,400.24,366.34,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,451.47,386.69,54.52,10.91;13,112.48,400.24,297.82,10.91">Overview of JOKER 2023 Automatic Wordplay Analysis Task 3 -Pun translation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M P</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,413.79,393.33,10.91;13,112.48,427.34,395.35,10.91;13,112.66,440.89,394.53,10.91;13,112.66,454.44,393.32,10.91;13,112.66,467.99,311.65,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,451.47,413.79,54.52,10.91;13,112.48,427.34,269.66,10.91">Overview of JOKER -CLEF-2023 Track on Automatic Wordplay Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M P</forename><surname>Preciado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,182.30,454.44,323.68,10.91;13,112.66,467.99,76.73,10.91">CLEF&apos;23: Proceedings of the Fourteenth International Conference of the CLEF Association</title>
		<title level="s" coord="13,196.65,467.99,155.05,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,481.54,393.33,10.91;13,112.66,495.09,245.08,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,167.78,481.54,115.82,10.91;13,311.31,481.54,194.68,10.91;13,112.66,495.09,175.91,10.91">Using sentence embeddings and multilingual models to detect and interpret wordplay</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Dsilva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>AKRaNLU @ CLEF JOKER</note>
</biblStruct>

<biblStruct coords="13,112.66,508.64,394.62,10.91;13,112.66,522.18,393.32,10.91;13,112.66,535.73,115.02,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,206.46,508.64,300.82,10.91;13,112.66,522.18,304.63,10.91">Does AI have a sense of humor? CLEF 2023 JOKER tasks 1, 2 and 3: Using BLOOM, GPT, SimpleT5, and more for pun detection, location</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dadić</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>interpretation and translation</note>
</biblStruct>

<biblStruct coords="13,112.66,549.28,393.33,10.91;13,112.66,562.83,393.33,10.91;13,112.41,576.38,393.57,10.91;13,112.66,589.93,394.51,10.91;13,112.66,603.48,137.88,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,338.10,549.28,167.89,10.91;13,112.66,562.83,231.66,10.91">The JOKER Corpus: English-French parallel data for multilingual wordplay recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3539618.3591885</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,370.97,562.83,135.02,10.91;13,112.41,576.38,393.57,10.91;13,112.66,589.93,38.77,10.91">SIGIR &apos;23: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="13,112.66,617.03,393.33,10.91;13,112.66,630.58,221.40,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,277.31,617.03,24.66,10.91;13,328.65,617.03,177.34,10.91;13,112.66,630.58,64.41,10.91">JOKER Task 1, 2, 3: pun detection, pun interpretation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prnjak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Davari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>CLEF. and pun translation</note>
</biblStruct>

<biblStruct coords="13,112.66,644.13,395.17,10.91;13,112.66,657.68,324.94,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,224.64,644.13,283.19,10.91;13,112.66,657.68,235.23,10.91">Exploring Humor in Natural Language Processing: A Comprehensive Review of JOKER Tasks at CLEF Symposium</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lieberum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,393.33,10.91;14,112.66,100.52,310.17,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="14,303.73,86.97,202.26,10.91;14,112.66,100.52,241.65,10.91">CLEF 2023 JOKER Tasks 2 and 3: using NLP models for pun location, interpretation and translation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ohnesorge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Á</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plichta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,393.33,10.91;14,112.66,127.61,194.15,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,168.64,114.06,337.35,10.91;14,112.66,127.61,125.63,10.91">UBO Team @ CLEF JOKER 2023 track for Task 1, 2 and 3 -applying AI models in regards to pun translation</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dubreuil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,141.16,393.15,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,288.28,141.16,42.12,10.91">CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Komorowska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Čatipović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vujica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,338.40,141.16,98.44,10.91">JOKER Working Notes</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,186.49,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,281.42,154.71,24.20,10.91;14,331.36,154.71,174.62,10.91;14,112.66,168.26,117.91,10.91">JOKER Task 2: using Chat GPT for pun location and interpretation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Brunelière</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Salina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>CLEF</note>
</biblStruct>

<biblStruct coords="14,112.66,181.81,393.33,10.91;14,112.66,195.36,262.54,10.91" xml:id="b13">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,181.81,393.33,10.91;14,112.66,195.36,26.38,10.91">Proceedings of the Working Notes of CLEF 2023: Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="14,147.04,195.36,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting>the Working Notes of CLEF 2023: Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
