<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,396.37,15.42;1,89.29,106.66,83.15,15.43">Biomedical Question Answering with Transformer Ensembles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.54,134.97,82.86,11.96"><forename type="first">Jason</forename><surname>Rauchwerk</surname></persName>
							<email>jrauchwe@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.32,134.97,69.63,11.96"><forename type="first">Parth</forename><surname>Rajwade</surname></persName>
							<email>prajwade@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.89,134.97,79.60,11.96"><forename type="first">Tanay</forename><surname>Gummadi</surname></persName>
							<email>tgummadi@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.42,134.97,57.53,11.96"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,148.92,85.87,11.96"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
							<email>teruko@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,396.37,15.42;1,89.29,106.66,83.15,15.43">Biomedical Question Answering with Transformer Ensembles</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">512E77705B52EECC8C0E8F16C1E7C5DC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical question answering</term>
					<term>transformer models</term>
					<term>ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in natural language processing, specifically transformers, have shown great promise in improving the performance of question-answering systems. However, we observe that a single transformer model may not achieve sufficient accuracy and reliability to meet the stringent requirements of biomedical question answering. Based on our participation in the BioASQ Challenge, we present a comprehensive approach for biomedical question answering using transformers, integrating an end-to-end data processing pipeline with the UMLS Metamap and different ensembling techniques. Our findings suggest that transformer ensembles achieve significant performance improvements when compared to individual models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rapid growth of on-line biomedical text has stimulated the research and development of robust, specialized language models that provide reliable, high-accuracy responses to queries posed against the medical literature. For example, the PubMed database contains more than 35 million citations and abstracts of biomedical articles 1 . To overcome the challenge of inadequate contextual representation when matching queries in the biomedical domain, researchers have turned to transformer-based language models such as BERT <ref type="bibr" coords="1,368.60,461.28,11.58,10.91" target="#b0">[1]</ref>, which have demonstrated remarkable efficacy in capturing contextual information from large corpora. Various adaptations of BERT, namely Med-BERT <ref type="bibr" coords="1,222.12,488.37,11.58,10.91" target="#b1">[2]</ref>, SciBERT <ref type="bibr" coords="1,283.19,488.37,11.58,10.91" target="#b2">[3]</ref>, and Clinical-BERT <ref type="bibr" coords="1,389.79,488.37,13.00,10.91" target="#b3">[4]</ref> have been specifically designed to address the need for context-aware biomedical language models.</p><p>In this paper, we explore the hypothesis that an ensemble of transformer models can perform better than a single transformer alone for specific bioinformatic tasks. We tested our hypothesis by participating in the eleventh edition of the BioASQ Challenge <ref type="bibr" coords="1,382.37,542.57,11.51,10.91" target="#b4">[5]</ref>, specifically focusing on Phase B of Task 11b <ref type="bibr" coords="1,179.75,556.12,11.35,10.91" target="#b5">[6]</ref>. Our primary objective is to deliver "exact" answers for yes/no, factoid, and list question types. The BioASQ Challenge consists of four rounds of test sets, providing participants with the opportunity to submit up to five systems for each test set. The organizers provide the dataset for Task 11b <ref type="bibr" coords="2,237.81,100.52,12.99,10.91" target="#b6">[7]</ref> in the form of a single training set and four test sets for each evaluation round. We submitted a total of four systems across three of the test sets. This allowed us to explore various approaches and methodologies, enhancing our understanding of the problem space.</p><p>After analyzing the performance of different systems in previous editions of the BioASQ Challenge, we decided to ensemble BioBERT <ref type="bibr" coords="2,301.44,168.26,12.99,10.91" target="#b7">[8]</ref> and BioM-Electra <ref type="bibr" coords="2,402.72,168.26,13.00,10.91" target="#b8">[9]</ref> for factoid and list questions. For yes/no questions, we employ BioM-Electra <ref type="bibr" coords="2,355.60,181.81,16.42,10.91" target="#b9">[10]</ref>. We use the Unified Medical Language System (UMLS) MetaMap tool<ref type="foot" coords="2,275.32,193.61,3.71,7.97" target="#foot_0">2</ref> for preprocessing data, and for synonym removal during post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There has been significant prior work done for question-answering in the biomedical domain. Following the advent of BERT <ref type="bibr" coords="2,230.95,281.08,11.58,10.91" target="#b0">[1]</ref>, Lee et al. <ref type="bibr" coords="2,295.81,281.08,13.00,10.91" target="#b7">[8]</ref> introduced BioBERT, a language modeling approach that initializes a BERT model (pretrained on Wikipedia and BookCorpus) and continues pretraining using masked-language modeling (MLM) and next sentence prediction (NSP) on PubMed abstracts and PubMed Central (PMC) full-text articles. BlueBERT <ref type="bibr" coords="2,413.82,321.73,17.75,10.91" target="#b10">[11]</ref> follows a similar approach but finds performance improvements, in the clinical domain, by pre-training on PubMed abstracts and MIMIC-III clinical notes. However, Gu et al. <ref type="bibr" coords="2,380.57,348.83,17.76,10.91" target="#b11">[12]</ref> find the aforementioned mixed-domain pretraining objective to be inferior to domain-specific pretraining from scratch given the difference in vocabulary from the initial BERT model and the later biomedical context.</p><p>PubMedBERT <ref type="bibr" coords="2,164.71,389.48,17.87,10.91" target="#b11">[12]</ref> is a new BERT model, trained from scratch using PubMed abstracts, that outperforms BioBERT and BlueBERT; the authors attribute the performance improvements to having an in-domain vocabulary which the architecture can model completely in order to fully optimize for in-domain data. Jeong et al. <ref type="bibr" coords="2,308.98,430.13,18.07,10.91" target="#b12">[13]</ref> propose a sequential transfer learning method for fine-tuning biomedical models on intermediate datasets, before fine-tuning on the specific biomedical task; this helps to improve performance due to data scarcity for the final task. Specifically, the authors show a significant F1 gain by training on MNLI <ref type="bibr" coords="2,432.73,470.77,17.84,10.91" target="#b13">[14]</ref> and SQuAD <ref type="bibr" coords="2,89.29,484.32,17.84,10.91" target="#b14">[15]</ref> before BioASQ, and unifying context-length distributions between fine-tuning tasks. Ting et al. <ref type="bibr" coords="2,115.31,497.87,18.07,10.91" target="#b15">[16]</ref> present a method using BioBERT to generate snippets for ideal answers (BioASQ Task B), and then using these snippets to predict exact answers for factoid/list questions.</p><p>BioM-ELECTRA and BioM-ALBERT <ref type="bibr" coords="2,263.71,524.97,17.90,10.91" target="#b9">[10]</ref> are variants of ELECTRA and ALBERT pretrained on PubMed abstracts. They subsequently fine-tune on a combination of SQuAD and MNLI, and finally the BioASQ dataset <ref type="bibr" coords="2,233.31,552.07,16.41,10.91" target="#b16">[17]</ref>, achieving SOTA on BioASQ 10b for list questions <ref type="bibr" coords="2,487.15,552.07,16.41,10.91" target="#b17">[18]</ref>. BioLinkBERT <ref type="bibr" coords="2,150.99,565.62,17.75,10.91" target="#b18">[19]</ref> takes an alternative approach in adding an additional pretraining objective of document relation prediction, in order to learn contextual linked concepts between documents in the form of PubMed hyperlinks. Given limited resources, we selected BioBERT, the most commonly cited baseline model for bioinformatics <ref type="bibr" coords="2,307.56,606.27,11.28,10.91" target="#b7">[8]</ref>, and BioM-ELECTRA, the best-performing model on the most recent BioASQ challenge <ref type="bibr" coords="2,288.47,619.81,17.91,10.91" target="#b17">[18]</ref> as the two baselines for our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">List and Factoid Questions</head><p>Following the methodology of Alrowili and Vijay-Shanker <ref type="bibr" coords="3,351.23,132.25,11.40,10.91" target="#b8">[9]</ref>, we merge factoid and list questions to overcome the limited number of training examples. We split the lists into multiple factoid questions and search the golden snippets for the spans that contain an exact string match for the answer. Because there are multiple snippets for each question, this creates many snippet-answer pairs for each original question. Each of these pairs are rewritten into the SQuAD format to be fed into our models. We use BioBERT and BioM-ELECTRA for these questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Yes/No Questions</head><p>We treated yes/no questions as a binary classification problem. We concatenate all of the golden snippets to create a paragraph and feed this context and the question to the model. We did not attempt to answer yes/no questions in our first batch, but submitted a model for the second and third batches to better compare our performance with submissions from previous years. We started with DistilBERT <ref type="bibr" coords="3,197.41,425.98,16.19,10.91" target="#b19">[20]</ref>, BioBERT, and BioM-ELECTRA for these questions. However, we chose BioM-ELECTRA for our systems because of its superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset Preprocessing</head><p>Snippets that come from different articles may use different names or acronyms to refer to the same concept. For instance, the protein "transforming growth factor alpha" is variously referred to as "transforming growth factor alpha", "transforming growth factor", "TGF𝛼", and "TGF-𝛼". We use to the MetaMap tool to ensure that all answers in the snippets are properly identified. MetaMap queries the UMLS Metathesaurus (curated by the National Library of Medicine<ref type="foot" coords="3,486.76,571.57,3.71,7.97" target="#foot_1">3</ref> ) to determine the canonical form for each biomedical term. We run snippets through MetaMap to expand all acronyms and abbreviations before finding the answer spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Synonym Postprocessing</head><p>We also utilize UMLS MetaMap to remove synonyms from the model's predicted candidate answer list. In the Metathesaurus, each entity has a Concept Unique Identifier (ConceptUI), which is shared among all names that can refer to the same entity. Our system sorts candidate answers by their confidence scores and greedily constructs an answer set while making sure that all final answers have unique ConceptUIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ensembling</head><p>We believe that ensembling models (which was also proposed by Alrowili and Vijay-Shanker <ref type="bibr" coords="4,493.30,211.46,12.68,10.91" target="#b8">[9]</ref> as a future prospect), specifically BioBERT and BioM-ELECTRA, can combine their strengths and form a better system. We performed a grid search to discover the weighting schemes that maximize F1 score (for list questions) and MRR (for factoid questions). Our ensembling weights were (0.004, 0.996) when maximizing F1 and (0.037, 0.963) when maximizing MRR for BioBERT and BioM-ELECTRA, respectively. We use these weights to compute a linear combination of weights and confidence scores for each predicted answer. We rerank and filter the candidate answers to return our ensembled predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BioASQ Task 11b Systems</head><p>We performed an 80-20 split on the training set for our validation purposes (internal testing). The results are shown in Table <ref type="table" coords="4,228.47,378.48,3.74,10.91" target="#tab_0">1</ref>. We participated in the BioASQ Challenge under the team name 'AsqAway', submitting 4 systems to the task. Our systems are described in Table <ref type="table" coords="4,334.96,546.52,3.68,10.91" target="#tab_1">2</ref>. The BioASQ test performance of our systems is described in Tables <ref type="table" coords="4,226.66,560.07,7.58,10.91" target="#tab_2">3,</ref><ref type="table" coords="4,236.96,560.07,3.79,10.91">4</ref>, and 5. The evaluation datasets are small and there is high variance in model performance across the batches, making it difficult to compare them directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>One of our initial observations was that the models returned a large number of probable answers, despite having set probability thresholds. While this meant that all possible answers were being covered in most of the cases, there were a large number of false positives which led to a drop in  precision. Upon further observations, we noticed that both acronyms and their expanded forms were included in the training data. This formed the motivation behind the UMLS Preprocessing step as described in Section 4.1.</p><p>Another observation was that a lot of the answers returned by our models were synonyms of each other. Since the challenge requires the systems to remove synonyms in the candidate answers, we performed the Synonym Postprocessing step as described in Section 4.2. We present a quantitative analysis of our results, based on UMLS Preprocessing and Synonym Postprocessing in Tables <ref type="table" coords="5,199.14,643.22,4.97,10.91" target="#tab_3">6</ref> and<ref type="table" coords="5,225.56,643.22,4.97,10.91" target="#tab_4">7</ref> respectively. Figures <ref type="figure" coords="5,325.20,643.22,4.97,10.91">2</ref> and<ref type="figure" coords="5,351.61,643.22,4.97,10.91">3</ref> show a qualitative example of the same. The number of answers returned is reduced significantly while maintaining accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future Work</head><p>We anticipate significant opportunities for improvement both on the data and modeling side.</p><p>As the model is currently only pre-trained on full-text articles and abstracts, there are instances where our architecture chooses an adjacent but incorrect medical term (low precision) or returns a nonsensical answer given the sparseness of the correct term in the full-text (low recall). The former case is more prevalent among our experiments than the latter and we hypothesize adding titles to the pre-training procedure can improve both precision and recall; precision is improved as the title acts effectively as a distillation for the context and recall is improved in the form of providing additional context for the model to train upon. Similarly, in the training procedure, we can leverage a combination of the abstract from the provided documents rather than solely relying on snippets as context. Moradi and Samwald <ref type="bibr" coords="7,197.90,114.06,17.96,10.91" target="#b20">[21]</ref> find vulnerabilities in BioBERT when exposed to word-level and character-level noise; we corroborate this observation in instances where training data has key medical terms misspelled or misused. Adversarial training offers robustness to such errors: Jia and Liang present the "AddSent" model-independent procedure from <ref type="bibr" coords="7,405.03,154.71,18.06,10.91" target="#b21">[22]</ref> and Du et al. <ref type="bibr" coords="7,487.92,154.71,18.07,10.91" target="#b22">[23]</ref> finds performance gains in the context of BioASQ. However, the alternative "AddAny" procedure by Jia and Liang <ref type="bibr" coords="7,164.14,181.81,17.91,10.91" target="#b21">[22]</ref> can also be implemented for more rigorous examples.</p><p>Using larger models (e.g. Large, X-Large, XX-Large variants) for BioM-ELECTRA and BioM-ALBERT empirically leads to incremental performance gains <ref type="bibr" coords="7,370.16,208.91,16.42,10.91" target="#b9">[10]</ref>, however don't address a strata of error. Alternative models such as LinkBERT <ref type="bibr" coords="7,335.28,222.46,18.07,10.91" target="#b18">[19]</ref> show similar performance as the Bio-M variants from preliminary experimentation. Changing the order of finetuning procedures mentioned by Jeong et al. <ref type="bibr" coords="7,206.58,249.56,17.95,10.91" target="#b12">[13]</ref> in swapping the ordering of MNLI and SQuAD prior to tuning for the BioASQ data has potential for marginal gains.</p><p>We hypothesize the use of adversarial methods to have the most promise in delivering performance improvements, followed by the use of alternative architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we address the challenge of generating accurate information retrieval systems for biomedical information, specifically focusing on Phase B of Task 11b in the eleventh BioASQ Challenge. Given the complex and sensitive nature of biomedical data, we adopt a novel approach that involves ensembling state-of-the-art transformer models that have previously performed well in BioASQ challenges, along with implementing data processing techniques based on UMLS MetaMap. Our efforts aim to contribute towards the development of highly precise answers for the list and factoid question types. Our approach yields promise for data-oriented techniques towards improving performance on the task. Our code files are publicly available in a GitHub repository <ref type="foot" coords="7,135.37,455.47,3.71,7.97" target="#foot_2">4</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,305.96,178.45,8.93;3,89.29,236.26,416.64,57.14"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A flowchart of the entire pipeline</figDesc><graphic coords="3,89.29,236.26,416.64,57.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,492.76,91.62,8.93;6,89.29,313.45,200.01,166.75"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: With UMLS</figDesc><graphic coords="6,89.29,313.45,200.01,166.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,405.04,333.56,109.09"><head>Table 1</head><label>1</label><figDesc>Results on validation dataset (internal testing)</figDesc><table coords="4,170.23,433.13,252.32,81.00"><row><cell>System</cell><cell cols="2">Yes/No F1 Acc.</cell><cell>Factoid MRR</cell><cell>List F1</cell></row><row><cell>Distilbert</cell><cell cols="2">0.8657 0.7708</cell><cell>-</cell><cell>-</cell></row><row><cell>BioBERT</cell><cell cols="4">0.8790 0.8063 0.6488 0.4914</cell></row><row><cell>BioM-ELECTRA</cell><cell cols="4">0.9430 0.9130 0.6862 0.5504</cell></row><row><cell>Ensemble 1 (max. F1)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.6910 0.5374</cell></row><row><cell>Ensemble 2 (max. MRR)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.6969 0.5374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,325.15,85.18"><head>Table 2</head><label>2</label><figDesc>Our BioASQ Task 11b Systems</figDesc><table coords="5,178.64,118.58,235.51,57.09"><row><cell>System</cell><cell>Yes/No Model</cell><cell>List and Factoid Model</cell></row><row><cell cols="2">AsqAway_1 BioM-ELECTRA</cell><cell>BioBERT</cell></row><row><cell cols="2">AsqAway_2 BioM-ELECTRA</cell><cell>BioM-ELECTRA</cell></row><row><cell cols="2">AsqAway_3 BioM-ELECTRA</cell><cell>Ensemble 1 (max. F1)</cell></row><row><cell cols="3">AsqAway_4 BioM-ELECTRA Ensemble 2 (max. MRR)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,195.57,333.56,328.45"><head>Table 3</head><label>3</label><figDesc>Results from BioASQ Task11B Batch 1</figDesc><table coords="5,88.99,222.74,333.56,301.28"><row><cell>System</cell><cell cols="3">Yes/No F1 Acc.</cell><cell>Factoid MRR</cell><cell>List F1</cell></row><row><cell>BioBERT</cell><cell></cell><cell>-</cell><cell>-</cell><cell>0.3158 0.4595</cell></row><row><cell>BioM-ELECTRA</cell><cell></cell><cell>-</cell><cell>-</cell><cell>0.3947 0.4804</cell></row><row><cell>Ensemble 1 (max. F1)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>0.3947 0.5106</cell></row><row><cell cols="2">Ensemble 2 (max. MRR)</cell><cell>-</cell><cell>-</cell><cell>0.4211 0.5106</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results from BioASQ Task11B Batch 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell cols="3">Yes/No F1 Acc.</cell><cell>Factoid MRR</cell><cell>List F1</cell></row><row><cell>BioBERT</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.4545 0.1756</cell></row><row><cell>BioM-ELECTRA</cell><cell cols="4">0.8693 0.8750 0.4545 0.2327</cell></row><row><cell>Ensemble 1 (max. F1)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.4773 0.2329</cell></row><row><cell>Ensemble 2 (max. MRR)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.4773 0.2329</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results from BioASQ Task11B Batch 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell cols="3">Yes/No F1 Acc.</cell><cell>Factoid MRR</cell><cell>List F1</cell></row><row><cell>BioBERT</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.3154 0.4290</cell></row><row><cell>BioM-ELECTRA</cell><cell cols="4">0.9091 0.8750 0.4423 0.4813</cell></row><row><cell>Ensemble 1 (max. F1)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.4615 0.4431</cell></row><row><cell>Ensemble 2 (max. MRR)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.4615 0.4431</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,90.49,320.03,85.18"><head>Table 6</head><label>6</label><figDesc>Model Performance with and without UMLS Preprocessing</figDesc><table coords="6,183.77,118.58,225.25,57.09"><row><cell></cell><cell cols="2">With UMLS</cell><cell cols="2">Without UMLS</cell></row><row><cell>System</cell><cell cols="2">Preprocessing</cell><cell cols="2">Preprocessing</cell></row><row><cell></cell><cell>Factoid</cell><cell>List</cell><cell>Factoid</cell><cell>List</cell></row><row><cell>BioBERT</cell><cell cols="4">0.6488 0.4914 0.5968 0.4435</cell></row><row><cell cols="5">BioM-ELECTRA 0.6862 0.5504 0.6026 0.4670</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,88.99,195.57,324.22,85.18"><head>Table 7</head><label>7</label><figDesc>Model Performance with and without Synonym Postprocessing</figDesc><table coords="6,179.58,223.66,233.63,57.09"><row><cell></cell><cell cols="4">With Synonym Without Synonym</cell></row><row><cell>System</cell><cell cols="2">Postprocessing</cell><cell cols="2">Postprocessing</cell></row><row><cell></cell><cell>Factoid</cell><cell>List</cell><cell>Factoid</cell><cell>List</cell></row><row><cell>BioBERT</cell><cell cols="3">0.6488 0.4914 0.5866</cell><cell>0.4430</cell></row><row><cell cols="4">BioM-ELECTRA 0.6862 0.5504 0.6254</cell><cell>0.4808</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,92.57,670.98,302.14,8.97"><p>https://www.nlm.nih.gov/research/umls/implementation_resources/metamap.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,92.57,670.93,95.02,8.97"><p>https://www.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,92.57,671.01,135.69,8.97"><p>https://github.com/parthsr5/asqaway</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,515.85,393.33,10.91;7,112.66,529.40,363.59,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="7,353.43,515.85,152.55,10.91;7,112.66,529.40,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,542.95,393.33,10.91;7,112.66,556.50,393.33,10.91;7,112.66,570.05,93.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,293.81,542.95,212.18,10.91;7,112.66,556.50,332.09,10.91">Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rasmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,456.29,556.50,49.69,10.91;7,112.66,570.05,40.46,10.91">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,583.60,393.60,10.91;7,112.66,597.15,146.44,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m" coord="7,234.39,583.60,239.83,10.91">Scibert: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,610.69,393.33,10.91;7,112.66,624.24,394.53,10.91;7,112.39,637.79,141.72,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,180.83,610.69,325.16,10.91;7,112.66,624.24,82.31,10.91">Clinical-bert: Vision-language pre-training for radiograph diagnosis and reports generation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,220.68,624.24,282.25,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,86.97,394.53,10.91;8,112.66,100.52,393.33,10.91;8,112.66,114.06,393.33,10.91;8,112.66,127.61,393.33,10.91;8,112.66,141.16,306.11,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,234.48,100.52,271.51,10.91;8,112.66,114.06,307.20,10.91">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,445.02,114.06,60.97,10.91;8,112.66,127.61,393.33,10.91;8,112.66,141.16,252.02,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,112.66,154.71,393.32,10.91;8,112.66,168.26,393.33,10.91;8,112.66,181.81,107.76,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,349.27,154.71,156.72,10.91;8,112.66,168.26,104.82,10.91">Overview of BioASQ Tasks 11b and Synergy11 in CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,241.61,168.26,264.38,10.91;8,112.66,181.81,77.06,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,195.36,393.32,10.91;8,112.66,208.91,326.44,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,357.16,195.36,148.82,10.91;8,112.66,208.91,189.95,10.91">BioASQ-QA: A manually curated corpus for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,311.68,208.91,64.68,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,393.33,10.91;8,112.66,236.01,393.98,10.91;8,112.41,249.56,48.96,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,361.64,222.46,144.35,10.91;8,112.66,236.01,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,394.29,236.01,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,263.11,395.17,10.91;8,112.66,276.66,393.32,10.91;8,112.66,290.20,124.68,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,259.17,263.11,248.66,10.91;8,112.66,276.66,267.49,10.91">Exploring biomedical question answering with biomtransformers at bioasq10b challenge: Findings and techniques</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,402.98,276.66,103.01,10.91;8,112.66,290.20,93.98,10.91">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,303.75,393.33,10.91;8,112.66,317.30,393.33,10.91;8,112.66,330.85,179.48,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,252.53,303.75,253.46,10.91;8,112.66,317.30,154.27,10.91">Biom-transformers: building large biomedical language models with bert, albert and electra</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,289.27,317.30,216.71,10.91;8,112.66,330.85,91.61,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,393.33,10.91;8,112.66,357.95,393.58,10.91;8,112.33,371.50,29.19,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,214.91,344.40,291.08,10.91;8,112.66,357.95,248.36,10.91">Transfer learning in biomedical natural language processing: an evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,385.05,394.53,10.91;8,112.66,398.60,394.53,10.91;8,112.28,412.15,332.27,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,112.66,398.60,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.28,412.15,268.63,10.91">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,425.70,393.33,10.91;8,112.66,439.25,393.57,10.91;8,112.33,452.79,29.19,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,390.94,425.70,115.05,10.91;8,112.66,439.25,239.70,10.91">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,466.34,393.33,10.91;8,112.66,479.89,327.40,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m" coord="8,289.72,466.34,216.27,10.91;8,112.66,479.89,145.31,10.91">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,493.44,393.33,10.91;8,112.66,506.99,280.07,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m" coord="8,324.88,493.44,181.11,10.91;8,112.66,506.99,98.46,10.91">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,520.54,393.33,10.91;8,112.66,534.09,393.33,10.91;8,112.33,547.64,29.19,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,314.08,520.54,191.91,10.91;8,112.66,534.09,393.33,10.91">Ncu-iisr/as-gis: Using bertscore and snippet score to improve the performance of pretrained language model in bioasq 10b phase b</title>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-H</forename><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,561.19,393.33,10.91;8,112.66,574.74,239.73,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,224.92,561.19,281.06,10.91;8,112.66,574.74,26.96,10.91">Large biomedical question answering models with albert and electra</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.88,574.74,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,588.29,395.17,10.91;8,112.66,601.84,306.53,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
		<title level="m" coord="8,259.17,588.29,248.66,10.91;8,112.66,601.84,274.61,10.91">Exploring biomedical question answering with biomtransformers at bioasq10b challenge: Findings and techniques</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,615.39,393.32,10.91;8,112.66,628.93,201.71,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="8,268.20,615.39,237.78,10.91;8,112.66,628.93,19.99,10.91">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15827</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,642.48,394.53,10.91;8,112.66,656.03,243.23,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="8,302.07,642.48,205.12,10.91;8,112.66,656.03,113.82,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,669.58,393.33,10.91;9,112.66,86.97,395.01,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,225.17,669.58,280.81,10.91;9,112.66,86.97,156.58,10.91">Improving the robustness and accuracy of biomedical language models through adversarial training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Samwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,277.32,86.97,148.31,10.91">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">104114</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,100.52,394.53,10.91;9,112.66,114.06,173.79,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="9,188.74,100.52,313.64,10.91">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07328</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,127.61,393.33,10.91;9,112.66,141.16,393.61,10.91;9,112.66,154.71,116.67,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,276.89,127.61,229.10,10.91;9,112.66,141.16,156.33,10.91">Improving biomedical question answering by data augmentation and model weighting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,277.68,141.16,228.59,10.91;9,112.66,154.71,84.75,10.91">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
