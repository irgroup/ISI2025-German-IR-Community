<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,408.23,15.42;1,88.69,106.66,407.83,15.42;1,89.29,128.58,164.13,15.43;1,89.29,150.91,212.38,11.96">Fusion @ BioASQ MedProcNER: Transformer-based Approach for Procedure Recognition and Linking in Spanish Clinical Text Notebook for the BioASQ Lab at CLEF 2023</title>
				<funder>
					<orgName type="full">ESI Funds</orgName>
				</funder>
				<funder ref="#_qA3ANnC">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_VrJKrM9">
					<orgName type="full">European Union-NextGenerationEU</orgName>
				</funder>
				<funder ref="#_ePMJg34">
					<orgName type="full">of the Republic of Bulgaria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,77.67,11.96"><forename type="first">Sylvia</forename><surname>Vassileva</surname></persName>
							<email>svasileva@fmi.uni-sofia.bg</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Informatics</orgName>
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.60,176.82,97.91,11.96"><forename type="first">Georgi</forename><surname>Grazhdanski</surname></persName>
							<email>ggrazhdans@uni-sofia.bg</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Informatics</orgName>
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.15,176.82,83.50,11.96"><forename type="first">Svetla</forename><surname>Boytcheva</surname></persName>
							<email>svetla@uni-sofia.bg</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Informatics</orgName>
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ontotext</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.93,176.82,66.55,11.96"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
							<email>koychev@fmi.uni-sofia.bg</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Informatics</orgName>
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,408.23,15.42;1,88.69,106.66,407.83,15.42;1,89.29,128.58,164.13,15.43;1,89.29,150.91,212.38,11.96">Fusion @ BioASQ MedProcNER: Transformer-based Approach for Procedure Recognition and Linking in Spanish Clinical Text Notebook for the BioASQ Lab at CLEF 2023</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1D92EC15FD268ADEA2A4C5F8E71F72A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named entity recognition (NER)</term>
					<term>Entity linking</term>
					<term>Biomedical NLP</term>
					<term>Clinical terms extraction</term>
					<term>Clinical terms linking</term>
					<term>Spanish clinical NER</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents an approach for solving subtasks 1 and 2 from the MedProcNER shared task, part of the BioASQ challenge -detecting and linking procedures in Spanish medical documents. The system consists of separate named entity recognition and entity linking modules and uses different BERT-based models for each module. For the NER subtask, we pre-train a Spanish RoBERTa model with additional Spanish clinical data and fine-tune the model for token classification. After the entities are detected, they are passed to the entity linker which uses the cross-lingual SapBERT XLMR-large to generate entity and mention representations and generates the candidate entity using cosine similarity. When evaluated with the test set, our system shows 0.71 F1 score on the NER subtask and 0.53 F1 score on the linking subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents our approach to BioASQ Task 3 -MedProcNER: Spanish Medical Procedures Named Entity Recognition, Linking, and Indexing Shared Task ( <ref type="bibr" coords="1,370.43,469.84,10.99,10.91" target="#b0">[1]</ref>, <ref type="bibr" coords="1,387.79,469.84,10.99,10.91" target="#b1">[2]</ref>). The challenge focuses on the concept of a clinical procedure -'a set of actions, interventions, or treatments that are carried out by healthcare professionals to diagnose, treat, or manage a patient's medical condition' 1 . Automatic identification and normalization of clinical procedure mentions in unstructured text is crucial for knowledge discovery, facilitating clinical research, building, and integrating systems that ultimately improve the quality of provided healthcare. There are three subtasks 2 :</p><p>‚Ä¢ Clinical Procedure Recognition -a named entity recognition task where mentions of clinical procedures must be identified in unstructured Spanish clinical case texts. ‚Ä¢ Clinical Procedure Normalization -assigning SNOMED CT codes to the clinical procedure mentions identified in the NER subtask. ‚Ä¢ Clinical Procedure-based Document Indexing -assigning SNOMED CT codes to the full clinical report texts, so that they could be indexed.</p><p>We take part in the first two subtasks -Clinical Procedure Recognition, and Procedure Normalization. The system performs named entity recognition for procedures using a pretrained Spanish RoBERTa model <ref type="bibr" coords="2,230.40,207.04,11.28,10.91" target="#b2">[3]</ref>, and entity linking using cross-lingual SapBERT XLMR-large <ref type="bibr" coords="2,89.29,220.59,11.37,10.91" target="#b3">[4]</ref>. We perform several different experiments to compare the performance of different clinical BERT-based language models, including models trained in English, Spanish, or cross-lingual. We investigate the effect of additional pre-training of the Spanish RoBERTa model using taskspecific Spanish data from the procedures gazetteer, as well as the effect of input preprocessing, and we find performance improvements when using both. We also attempt to train an Adapter over the BioM-ALBERT-Large model, which will adapt the English model to Spanish, however, the results shown are lower than the fully trained Spanish RoBERTa model.</p><p>The code related to this task is available on GitHub<ref type="foot" coords="2,328.00,313.68,3.71,7.97" target="#foot_0">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Subtask 1 -Named Entity Recognition</head><p>Named entity recognition (NER) is a fundamental subtask in multiple NLP challenges for Spanish biomedical and clinical texts. Deep neural models are predominantly used to tackle the NER task. In the DisTEMIST <ref type="bibr" coords="2,196.80,422.12,12.88,10.91" target="#b4">[5]</ref> task, the best-performing NER model, developed by Moscato et al. <ref type="bibr" coords="2,89.29,435.67,11.54,10.91" target="#b5">[6]</ref>, is based on PlanTL-GOB-ES/roberta-base-biomedical-clinical-es <ref type="bibr" coords="2,398.14,435.67,11.54,10.91" target="#b2">[3]</ref>, with a classification head on top. Xiong et al. <ref type="bibr" coords="2,200.40,449.22,12.72,10.91" target="#b6">[7]</ref> view the NER task as a machine reading comprehension (question answering) problem by using the definition of the entity as the question, and part of the clinical text as the segment, and training a joint model for the NER and entity linking subtasks. Xiong et al. <ref type="bibr" coords="2,114.38,489.87,13.00,10.91" target="#b7">[8]</ref> achieve the best NER result in the PharmaCoNER <ref type="bibr" coords="2,357.79,489.87,13.00,10.91" target="#b8">[9]</ref> task using a BERT-based <ref type="bibr" coords="2,487.92,489.87,18.07,10.91" target="#b9">[10]</ref> model and conditional random fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Subtask 2 -Entity Linking</head><p>Entity linking (EL) is a crucial task for extracting structured data from clinical texts. It usually relies on the result from NER and consists of two steps -generating candidates from a knowledge base of entities, and ranking and selecting the best candidate. The EL approach can take advantage of the global context information to inform the linking decisions based on neighboring linked entities. A common approach used for biomedical entity linking is using deep neural networks to generate entity representations in an embedding space and searching for the closest entities based on a distance metric like cosine similarity. SapBERT and cross-lingual SapBERT were pre-trained using UMLS and have shown very good results for linking for both English and other languages ( <ref type="bibr" coords="3,169.19,100.52,15.88,10.91" target="#b10">[11]</ref>, <ref type="bibr" coords="3,192.26,100.52,11.25,10.91" target="#b3">[4]</ref>). In the case of Spanish biomedical entity linking, the best model on the DisTEMIST dataset used an ensemble of cross-lingual SapBERT and TF-IDF vectorizer based on character n-gram features for candidate generation which scored F1 0.5657 <ref type="bibr" coords="3,457.57,127.61,16.09,10.91" target="#b11">[12]</ref>. Other participants in the competition used FastText embeddings and approximate nearest neighbor similarity and scored F1 0.4987 <ref type="bibr" coords="3,225.40,154.71,16.09,10.91" target="#b12">[13]</ref>. Due to the lack of large corpora of biomedical training data in many languages, approaches using exact match and surface form similarity are still used, for example, several papers in the CanTEMIST competition used the Levenshtein distance metric ( <ref type="bibr" coords="3,92.89,195.36,15.71,10.91" target="#b13">[14]</ref>, <ref type="bibr" coords="3,115.25,195.36,15.71,10.91" target="#b14">[15]</ref>).</p><p>In some cases, EL and NER can be combined and solved as sequence labeling task, as long as the entity identifier is composed of different hierarchical components, like the tumor codes (ICD-O-3) in CanTEMIST. The highest scoring systems in CanTEMIST use sequence labeling and predict the different tumor code components for each word ( <ref type="bibr" coords="3,379.06,249.56,15.71,10.91" target="#b15">[16]</ref>, <ref type="bibr" coords="3,401.42,249.56,15.71,10.91" target="#b16">[17]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MedProcNER Dataset</head><p>The MedProcNER corpus <ref type="bibr" coords="3,206.39,329.15,18.07,10.91" target="#b17">[18]</ref> contains 1,000 clinical case reports in Spanish annotated with procedure mentions and normalized to SNOMED CT codes. The corpus consists of a fully annotated train set, a smaller test set, as well as a gazetteer of SNOMED-CT codes and different aliases. Statistics about the train and test sets are shown in Table <ref type="table" coords="3,380.23,369.80,3.74,10.91" target="#tab_0">1</ref>.</p><p>The train set contains 4,857 annotated entities, 1,630 unique entity codes, 3,086 unique entity mentions, 106 nested mentions, and 510 abbreviations. The majority of entities have only one SNOMED-CT code, but there are 51 mentions which have no code selected, and 125 which have more than one code. There are very few ambiguous mentions which are labeled with different codes based on the context -a total of 12.</p><p>The Spanish SNOMED-CT gazetteer contains a total of 234,675 aliases for terms in multiple categories, including procedures, substance, clinical drug, and others. The most important category for this task is procedures. The gazetteer contains 61,714 unique SNOMED-CT codes of procedures and 94,133 total different aliases for procedure terms. The average number of procedure aliases per code is 1.52.</p><p>The corpus has a test set of 250 documents for testing purposes and the annotated version is not public as of the moment of writing this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Language Pre-training Dataset</head><p>We experimented with two different pre-training datasets:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">MedProcNER Gazetteer Dataset</head><p>The dataset consists of all of the terms in the initial version of the MedProcNER gazetteer, each as a separate example. This was the dataset of choice for our pre-trained PlanTL-GOB-ES/roberta-base-biomedical-clinical-es model submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Custom Spanish Medical Procedure Dataset</head><p>This dataset was compiled to include clinical procedure term descriptions from CIE-10-ES 2022 Procedimientos Tabla de Referencia <ref type="foot" coords="4,252.83,245.09,3.71,7.97" target="#foot_1">4</ref> , and all term descriptions from the Spanish release of SNOMED CT <ref type="bibr" coords="4,150.57,260.39,17.76,10.91" target="#b18">[19]</ref> which contain 'procedimiento' and do not contain 'RETIRADO' (deprecated).</p><p>It also includes all sentences from the MedProcNER train and test files, and all unique values in the COMPONENT, SYSTEM, and METHOD_TYP 5 columns of the Spanish (Spain) linguistic variant of LOINC v2.74. Since this set is larger than the gazetteer one, it is only used for training a language adapter for one of the models -a less computationally expensive alternative to full pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unified Medical Language System Procedures (UMLS) Procedures</head><p>For the purposes of entity linking, we have extracted all procedures from UMLS (2023AA edition) <ref type="bibr" coords="4,126.55,391.41,17.92,10.91" target="#b19">[20]</ref> which are in Spanish, have a Spanish SNOMED CT code, and that code is part of the gazetteer procedure codes. We extracted 80,681 procedure aliases from UMLS, however, about half of them were already present in the MedProcNER gazetteer. The number of new aliases extracted from UMLS was 45,010. We construct the entity-linking knowledge base by combining the UMLS and the gazetteer procedures. The total number of procedure aliases in the knowledge base is 125,691.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>The system performs named-entity recognition and entity linking of procedures in Spanish clinical texts. The overall system architecture is shown on Figure <ref type="figure" coords="4,389.40,531.33,5.17,10.91" target="#fig_0">1</ref> and consists of separate steps for named-entity recognition and entity linking of the procedures.</p><p>After preprocessing, the NER module identifies the spans in the text which contain procedures and they are passed to the Entity Linking module to predict their SNOMED-CT identifiers. As a result, a list of procedure spans and their corresponding identifiers is returned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing</head><p>Due to the fact that more than 33% of the clinical case texts exceed the 512 token limit of our models, we split the texts into sentences, and use each sentence as a separate example for fine-tuning. We use the SPACCC Sentence Splitter<ref type="foot" coords="5,323.67,333.97,3.71,7.97" target="#foot_2">6</ref> . We also replace any number with the 'NUMBER' literal, and remove any punctuation, special characters, and symbols matching the following regex:</p><formula xml:id="formula_0" coords="5,89.29,386.57,248.96,9.27">[~:\'+\[\\@^{%(\-"*|,&amp;&lt;'}._=\]!&gt;;?#$)/¬Æ¬©‚Ñ†]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Subtask 1 -Named Entity Recognition</head><p>For the NER subtask, we fine-tune a large transformer model with a classification head on the preprocessed MedProcNER train dataset, postprocessing the results to reconstruct the final procedure from tokens. The classification head labels correspond to the 3 classes in the IOB2 annotation scheme <ref type="bibr" coords="5,176.47,475.72,16.25,10.91" target="#b20">[21]</ref>.</p><p>The process of pre-training the model using the MedProcNER gazetteer and then fine-tuning it on the MedProcNER train set is shown on figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Language Model Pre-training</head><p>We pre-train the model using the masked language model objective and the standard BERT architecture <ref type="bibr" coords="5,146.93,565.69,18.06,10.91" target="#b9">[10]</ref> for two epochs using the HuggingFace Transformers library 7 . We use the default value for the probability of masking a token -15%. The MedProcNER gazetteer dataset from Section 3.2 is used to pre-train the model on Spanish medical procedure vocabulary. Table <ref type="table" coords="5,117.10,606.34,5.17,10.91" target="#tab_1">2</ref> shows the hyperparameter values used for pre-training. They are the same as the hyperparameters used for pre-training RoBERTa (base) in the original paper <ref type="bibr" coords="5,430.85,619.89,16.25,10.91" target="#b21">[22]</ref>.</p><p>The resulting pre-trained language model is used for the downstream NER task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification Model Selection</head><p>We experiment with the following models for the token classifier:</p><p>‚Ä¢ PlanTL-GOB-ES/roberta-base-biomedical-clinical-es <ref type="bibr" coords="6,381.62,399.16,13.00,10.91" target="#b2">[3]</ref> -a RoBERTa-based language model, trained on a large Spanish biomedical-clinical corpus of more than 1B tokens. Systems based on this model have achieved competitive results on previous Spanish biomedical-clinical tasks. We further pre-train the model on the Gazetteer dataset. ‚Ä¢ CLIN-X-ES <ref type="bibr" coords="6,172.90,468.26,17.91,10.91" target="#b22">[23]</ref> -a cross-lingual language model, based on XLM-RoBERTa (large), pretrained on the MeSpEN <ref type="bibr" coords="6,228.24,481.81,18.07,10.91" target="#b23">[24]</ref> dataset, and Spanish clinical documents from the Scielo archive 8 . We use CLIN-X-ES to explore the cross-lingual knowledge transfer capabilities of a multilingual model in the clinical domain. No additional pre-training is done. ‚Ä¢ BioM-BERT-Large <ref type="bibr" coords="6,211.43,523.81,18.07,10.91" target="#b24">[25]</ref> -a BERT-based model (with ELECTRA architecture) for the biomedical domain, pre-trained exclusively on an English corpus -PubMed Abstracts + PMC + general domain vocab (EN Wiki + Books). It achieves state-of-the-art results on certain bio text classification tasks such as ChemProt. With no further pre-training, BioM BERT serves as a good baseline, proving that cross-lingual knowledge transfer in the biomedical-clinical context is possible to a certain extent even with a monolingual model, likely due to the Latin and Greek etymology of clinical terms. ‚Ä¢ BioM-ALBERT-Large <ref type="bibr" coords="6,223.19,620.01,17.92,10.91" target="#b24">[25]</ref> -an ALBERT-based model, pre-trained on English PubMed abstracts only. We train a language adapter with the Multiple ADapters for Cross-lingual transfer (MAD-X) architecture which has shown very good results for cross-lingual adaptation <ref type="bibr" coords="7,168.05,86.97,16.41,10.91" target="#b25">[26]</ref>. We train the adapter layers on the MLM objective using the Custom Spanish Medical Procedure dataset from Section 3.2 for 1 epoch. The core idea is to augment the original model with a new set of parameters, by inserting bottleneck feedforward layers in each layer of the transformer model, and only train these new parameters on the masked language modeling objective for Spanish. This not only speeds up the training process, but also improves modularity and reusability, as multiple adapters can be composed for different tasks, and the adapter parameters are saved and shared separately. Table <ref type="table" coords="7,143.67,181.81,5.17,10.91" target="#tab_2">3</ref> shows the hyperparameter values used when training the adapter. We use the default value for masking probability -15%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Postprocessing</head><p>Since our models are token classifiers, to get the final clinical procedure mentions in the required format for the NER task, we must merge the token class predictions into word class predictions.</p><p>To do this, we use the aggregation capabilities of the Hugging Face token classification pipeline <ref type="foot" coords="7,500.63,370.76,3.71,7.97" target="#foot_3">9</ref> , setting the aggregation strategy to 'first'. This groups the tokens of a word, while preserving word boundaries. For instance, 'resonancia magn√©tica' may be tokenized as 'resonancia', 'magn√©t' and '##ica'. These three tokens may be classified as B-PROCEDIMIENTO, I-PROCEDIMIENTO, and O. The chosen aggregation strategy would override the classification of the last token in order to preserve word boundaries when merging. As a result, 'resonancia magn√©tica' (and not 'resonancia magn√©t') is recognized as a clinical procedure mention <ref type="foot" coords="7,400.05,452.05,7.41,7.97" target="#foot_4">10</ref> . This approach might be beneficial for models pre-trained exclusively on an English corpus since it is more likely that they misclassify suffixes of out-of-vocabulary words. Note this could only be applied to word-based models, where there is the notion of a word boundary. For the RoBERTa Biomedicalclinical model and the BioM-ALBERT-Large model, we include an additional postprocessing step -removing relatively rare leading and trailing whitespace characters and unclosed parenthesis occurrences which are a result of the preprocessing, e.g. ' PAAF)' is transformed into 'PAAF'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Subtask 2 -Entity Linking</head><p>For the task of Entity Linking, we construct a knowledge base (KB) using the MedProcNER train set and gazetteer described in Section 3.1, which contains procedure SNOMED-CT codes from the task gazetteer and all their aliases from the train set. For all entries in the KB, we create representations using the SapBERT XLMR-large model <ref type="bibr" coords="7,368.04,625.28,11.49,10.91" target="#b3">[4]</ref>. The SapBERT XLMR-large model was pre-trained on UMLS data for all available languages including Spanish using a self-alignment objective so that phrases that represent a concept in different languages are grouped closely in the embedding space. Spanish is the second most common language in UMLS 2020AB and the dataset used to pre-train the model consists of 10.7% Spanish terms. The model achieves 56.4% F1 score on the Spanish version of the WikiMed dataset <ref type="bibr" coords="8,407.48,127.61,11.43,10.91" target="#b3">[4]</ref>.</p><p>We create an entity representation for the input span and use cosine similarity search in the knowledge base. The system selects the entity with the highest cosine similarity. The implementation uses the NeMO library 11 adapted to use the SapBERT XLMR-large model. The NeMO library implements the approach for entity linking described by Liu et al <ref type="bibr" coords="8,475.04,181.81,16.41,10.91" target="#b10">[11]</ref>. It uses models pre-trained using the self-alignment objective like SapBERT and generates linking candidates using cosine similarity search. Future enhancements of the system can support predicting the NIL object (i.e. NO_CODE), or multiple predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Train and Validation Datasets</head><p>To form the training and validation sets for fine-tuning, we first split all of the clinical case texts in the MedProcNER train dataset, then 80% of the sentences are randomly selected for training, and the remaining 20% are used for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>We used micro-averaged precision, recall, and F1-score as metrics for both subtasks.</p><p>Since no official evaluation library was provided, we modified the DisTEMIST evaluation script by replacing the gazetteer with the MedProcNER gazetteer, and updating the entity type to 'PROCEDIMIENTO' <ref type="bibr" coords="8,194.13,419.52,11.43,10.91" target="#b5">[6]</ref>. The results of the different classifiers on the validation set and the test set are presented in Table <ref type="table" coords="8,128.07,632.94,3.78,10.91" target="#tab_3">4</ref>. Being a monolingual model, highly specialized in the clinical context, the Spanish Biomedical-Clinical RoBERTa outperforms the rest of the models on both datasets, in terms of F1. The multilingual CLIN-X-ES shows competitive performance as well. Interestingly, despite being trained exclusively on English texts, both BioM models perform relatively well on the test set, with BioM-BERT Large having a slight lead, perhaps because it is trained on a more diverse dataset than the BioM-ALBERT. Adam optimizer parameter values (ùúñ, ùõΩ 1 , and ùõΩ 2 ) are the same for all models -the HuggingFace trainer defaults -1e-8, 0.9, 0.999 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Subtask 1 -Named Entity Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">NER Model Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Effect of Further Pre-training</head><p>We further pre-trained the Spanish RoBERTa-Biomedical-Clinical for two epochs on the Gazetteer terms dataset. This led to a stable performance improvement, as it conditioned the model to the target clinical procedure terms that may appear in the clinical case texts. Table <ref type="table" coords="9,89.29,230.82,5.07,10.91" target="#tab_4">5</ref> compares the performance on the validation set, with and without pre-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Effect of Preprocessing</head><p>Preprocessing the clinical report texts by removing special characters, and replacing numbers with the 'NUMBER' literal, has a positive impact on the performance of the RoBERTa-Biomedical-Clinical model, allowing it to better generalize, ignoring dates, patient age, and other numeric mentions that, in the context of a clinical procedure, only add noise. On the other hand, Roman numerals are preserved, as they are part of some disease names (e.g. diabetes tipo II). Table <ref type="table" coords="9,500.85,437.76,5.13,10.91" target="#tab_5">6</ref> shows that preprocessing improves recall without significantly affecting precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Language Adapter Applicability</head><p>In an attempt to allow BioM models, which are trained exclusively on English data, to adapt to the specifics of the Spanish language without causing catastrophic forgetting of their representations of biomedical terms, we pre-trained a language adapter for the BioM-ALBERT model. Adapters <ref type="bibr" coords="9,89.29,642.48,17.91,10.91" target="#b26">[27]</ref> offer a less computationally expensive alternative to traditional full pre-training and finetuning. It involves introducing a small number of new parameters (relative to the size of the original model), and training just the new ones, while keeping the rest of the model frozen. Adapters have successfully been applied to biomedical NLP tasks <ref type="bibr" coords="10,370.10,176.80,16.09,10.91" target="#b27">[28]</ref>. Our adapter has the MAD-X language adapter architecture and was trained on the Custom Spanish Medical Procedure dataset with the masked language modeling objective for 1 epoch. Although this did not lead to a significant performance improvement (only affecting precision), it might be an approach worth exploring on a larger dataset, and training for more epochs. Table <ref type="table" coords="10,380.89,231.00,5.01,10.91" target="#tab_6">7</ref> compares the performance of adapted, and base BioM-ALBERT-Large models on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Hyperparameter values used during fine-tuning of the NER classifier models. RoBERTa values are identical to those used in the original paper <ref type="bibr" coords="10,270.76,297.52,10.54,8.87" target="#b2">[3]</ref>. CLIN-X-ES values are the same as the RoBERTa ones, apart from the batch size, as the model had to fit in memory. BioM-BERT-Large values are a result of a hyperparameter search using Tune <ref type="bibr" coords="10,234.32,321.43,14.92,8.87" target="#b28">[29]</ref>. BioM-ALBERT-Large values are from the original paper <ref type="bibr" coords="10,484.85,321.43,14.92,8.87" target="#b24">[25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Subtask 2 -Entity Linking</head><p>For the entity linking task, we perform an experiment to compare three different SapBERT flavors for generating the mention representation. We report the accuracy at @1, @5, @10, and @25. In order to test the Entity Linking independently, we take the annotated train set for subtask 1 and predict the SNOMED-CT code for each mention. The knowledge base used for this validation consists of the procedures from the gazetteer (Section 3.3), UMLS (Section 3.1), and the mentions from the train set which are not part of the validation set. The results of the experiment for SapBERT <ref type="bibr" coords="10,295.03,541.43,16.30,10.91" target="#b10">[11]</ref>, SapBERT XLMR <ref type="bibr" coords="10,393.85,541.43,11.48,10.91" target="#b3">[4]</ref>, and SapBERT XLMRlarge <ref type="bibr" coords="10,114.27,554.97,12.84,10.91" target="#b3">[4]</ref> are shown in table <ref type="table" coords="10,214.63,554.97,3.74,10.91" target="#tab_8">9</ref>.</p><p>The original SapBERT has the lowest score, explained by the fact that it was trained only with English terms on PubMed texts, while the cross-lingual versions are trained on UMLS terms.</p><p>The best result @1 is achieved using SapBERT XLMR and SapBERT XLMR-large scores just 0.93% less than the smaller model which is not statistically significant (Chi-square = 0.0871, p=0.7678&gt;0.05 for the 968 validation set samples). For the purposes of the final system, we decided to use the large model since it showed better results in the original paper <ref type="bibr" coords="10,453.98,649.82,11.43,10.91" target="#b3">[4]</ref>.</p><p>The SapBERT XLMR-large model shows an accuracy @25 of approximately 0.84, so future enhancements of the system can use it as a candidate generator and use a separate ranking function to select the best candidate and increase the likelihood of finding the correct one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Overall System</head><p>We perform a comparison of the different NER models in the end-to-end system performance. We measure F1 for both the NER and EL tasks. We use two different dataset scenarios for comparison:</p><p>‚Ä¢ Using a split of the train dataset -80% of data used for training, 20% used for validation testing; ‚Ä¢ Using the full train dataset for training and the test set for testing;</p><p>The results of the experiment on the validation dataset are shown in table 10, and table <ref type="table" coords="11,495.89,369.56,10.35,10.91" target="#tab_10">11</ref> shows the results from the test dataset evaluated by the organizers <ref type="bibr" coords="11,393.16,383.10,11.55,10.91" target="#b1">[2]</ref>. The highest score on the validation NER model is achieved using the BioM-BERT-Large model, while the highest EL score is achieved using the CLIN-X-ES + SapBERT XLMR-Large model. Overall, most of the models produce very close results and the top-scoring models change when evaluating on the test set. The best-performing model for both NER and EL on the test set is the Pre-trained Spanish RoBERTa which scores 0.71 on NER and 0.53 on the EL task.</p><p>The reason why the entity linking model shows lower results on the validation set may be explained by the fact that the KB it uses has all the validation mentions explicitly removed to account for a worst-case scenario. A more detailed comparison may be performed once the test set annotations become available. where each token is tagged with only one of {B, I, O}. Consequently, a mention that is part of another mention, is not identified. For instance, the following mention 'tomograf√≠a axial computarizada (TAC) abd√≥mino-p√©lvica' contains a nested mention 'TAC' that is not recognised by our model. Since about 2% of the training data consists of nested mentions, the model is not able to recognize them correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2.">Subtask 2 -Entity Linking</head><p>As the linking module uses the results from the NER module, successful linking is limited by the correctly identified mentions. Several factors contribute to entity-linking errors:</p><p>‚Ä¢ Zero or multiple codes matching the mention -the model supports the prediction of a single entity from the knowledge base, so mentions that should be linked with zero or multiple codes will not be predicted correctly. The validation set contains only 30 mentions which correspond to NO_CODE or multiple codes, accounting for approximately 3% of the validation set errors. ‚Ä¢ Missing entities in the KB cannot be predicted by the model. About 5.5% of the mentions in the validation set do not have a record in the validation KB for the same entity code.</p><p>A common error is suggesting an entity that is close to the target entity like a parent or entity term which has common (sub)-words, as shown in table <ref type="table" coords="13,346.36,165.03,8.53,10.91" target="#tab_11">12</ref>. For example, when the query is "somatometr√≠a (somatometry)", the system predicts the code for "medici√≥n de somatomedina (somatomedin measurement)" due to the common sub-word "somato". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The proposed system shows satisfactory results on the named entity recognition, and entity linking subtasks of the MedProcNER challenge, exploring various approaches, with a focus on the impact of further pre-training and adapting on the performance of both monolingual (Spanish or English), and multilingual models. The system was evaluated on the test set and scored 0.71 F1 on the NER task and 0.53 F1 on the entity linking task. By comparison, the best system in the competition has shown an F1 score of 0.7985 on the NER task and 0.5707 on the entity linking task.</p><p>The conducted experiments suggest that further conditioning a specialized monolingual Spanish model via training on a focused dataset, such as one featuring gazetteer terms, leads to a stable performance improvement on the NER task. Large cross-lingual models proved to be an optimal choice for entity linking, achieving competitive results.</p><p>As further work, additional training data could be generated by replacing procedure mentions with synonyms. Also, introducing machine translation and alignment as a step in the NER pipeline could be beneficial, as it would allow utilizing state-of-the-art monolingual transformer models that are trained on English biomedical-clinical datasets. Last, but not least, it would be worth looking into alternative strategies for constructing entity mentions from token classification results, as the approach we employed, although simple, might be limiting in some cases (e.g. longer mentions). For the entity linking task further work can explore using the SapBERT XLMR-Large model to generate candidates and training a model to rank them. Also, exploring how to take advantage of the SNOMED-CT structure and entity relationships can be useful to provide additional information for linking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,207.31,416.69,8.93;5,89.29,219.32,418.23,8.87;5,89.29,231.27,416.70,8.87;5,89.29,243.23,39.87,8.87;5,176.16,256.51,239.96,6.97;5,89.29,84.19,416.69,115.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: End-to-end architecture of the system -the preprocessed clinical case text is passed to a token classifier. Classified tokens are merged into the clinical procedure mentions in the postprocessing step. Finally, an entity linker uses the identified mentions as input and assigns the corresponding SNOMED CT codes. BERT image: https://www.iconspng.com/image/146543/sesame-street-bert-standing</figDesc><graphic coords="5,89.29,84.19,416.69,115.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,215.15,416.69,8.93;6,89.29,227.15,115.24,8.87;6,176.16,242.66,239.96,6.97;6,110.13,84.19,375.03,124.37"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Language model pre-training using the Gazetteer terms pre-training dataset from Section 3.2 and subsequent fine-tuning. BERT image: https://www.iconspng.com/image/146543/sesame-street-bert-standing</figDesc><graphic coords="6,110.13,84.19,375.03,124.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,561.40,313.13,72.81"><head>Table 1</head><label>1</label><figDesc>MedProcNER dataset metrics.</figDesc><table coords="3,183.81,589.02,218.31,45.18"><row><cell>Metric</cell><cell>Train</cell><cell>Test</cell></row><row><cell>Documents</cell><cell>750</cell><cell>250</cell></row><row><cell>Sentences</cell><cell>11,884</cell><cell>3,986</cell></row><row><cell>Tokens</cell><cell>323,192</cell><cell>107,551</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,269.75,411.73,61.96"><head>Table 2</head><label>2</label><figDesc>Hyperparameter values used during pre-training of Spanish RoBERTa-Biomedical-Clinical.</figDesc><table coords="6,95.50,298.49,405.23,33.23"><row><cell>Learning</cell><cell>Weight</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Warmup</cell><cell>Batch</cell><cell>Gradient Accu-</cell><cell>Epochs</cell></row><row><cell>Rate</cell><cell>Decay</cell><cell>ùúñ</cell><cell>ùõΩ 1</cell><cell>ùõΩ 2</cell><cell>Ratio</cell><cell>Size</cell><cell>mulation Steps</cell><cell></cell></row><row><cell>6e-4</cell><cell>0.01</cell><cell>1e-6</cell><cell>0.9</cell><cell>0.98</cell><cell>0.048</cell><cell>80</cell><cell>100</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,231.88,411.73,61.96"><head>Table 3</head><label>3</label><figDesc>Hyperparameter values used during training of a language adapter for BioM-ALBERT-Large.</figDesc><table coords="7,95.50,260.62,405.23,33.23"><row><cell>Learning</cell><cell>Weight</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Warmup</cell><cell>Batch</cell><cell>Gradient Accu-</cell><cell>Epochs</cell></row><row><cell>Rate</cell><cell>Decay</cell><cell>ùúñ</cell><cell>ùõΩ 1</cell><cell>ùõΩ 2</cell><cell>Ratio</cell><cell>Size</cell><cell>mulation Steps</cell><cell></cell></row><row><cell>1.76e-3</cell><cell>0</cell><cell>1e-8</cell><cell>0.9</cell><cell>0.999</cell><cell>0</cell><cell>32</cell><cell>256</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,502.92,410.05,96.72"><head>Table 4</head><label>4</label><figDesc>Subtask 1 results on the validation and test sets.</figDesc><table coords="8,95.95,530.55,403.09,69.09"><row><cell>Model</cell><cell>Val</cell><cell>Val</cell><cell>Val</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell></row><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Pre-trained Spanish RoBERTa</cell><cell>0.6944</cell><cell>0.6887</cell><cell cols="2">0.6915 0.7165</cell><cell>0.7143</cell><cell>0.7154</cell></row><row><cell>CLIN-X-ES</cell><cell>0.6985</cell><cell>0.6841</cell><cell cols="2">0.6912 0.7047</cell><cell>0.6916</cell><cell>0.6981</cell></row><row><cell>BioM-BERT Large</cell><cell>0.6581</cell><cell>0.6317</cell><cell cols="2">0.6447 0.6894</cell><cell>0.6599</cell><cell>0.6743</cell></row><row><cell cols="2">BioM-ALBERT Large + Adapter 0.6703</cell><cell>0.6098</cell><cell cols="2">0.6387 0.6928</cell><cell>0.6264</cell><cell>0.6580</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,259.16,417.00,73.23"><head>Table 5</head><label>5</label><figDesc>Comparison of the performance of Spanish RoBERTa-Biomedical-Clinical on the validation set, with and without additional pre-training.</figDesc><table coords="9,135.31,299.15,321.87,33.24"><row><cell>Model</cell><cell>Val Precision</cell><cell>Val Recall</cell><cell>Val F1</cell></row><row><cell>Spanish RoBERTa</cell><cell>0.688</cell><cell>0.6755</cell><cell>0.6817</cell></row><row><cell>Pre-trained Spanish RoBERTa</cell><cell>0.6944</cell><cell>0.6887</cell><cell>0.6915</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,479.65,416.99,71.01"><head>Table 6</head><label>6</label><figDesc>Effect of preprocessing on the performance of Spanish RoBERTa-Biomedical-Clinical on the validation set.</figDesc><table coords="9,102.71,517.42,350.22,33.24"><row><cell>Model</cell><cell>Val Precision</cell><cell>Val Recall</cell><cell>Val F1</cell></row><row><cell>Spanish RoBERTa</cell><cell>0.6897</cell><cell>0.6513</cell><cell>0.6699</cell></row><row><cell>Spanish RoBERTa + Preprocessing</cell><cell>0.6880</cell><cell>0.6755</cell><cell>0.6817</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,88.98,90.49,368.27,61.97"><head>Table 7</head><label>7</label><figDesc>Adapted BioM-ALBERT-Large performance compared to base model on the validation set.</figDesc><table coords="10,102.71,119.22,350.22,33.24"><row><cell>Model</cell><cell>Val Precision</cell><cell>Val Recall</cell><cell>Val F1</cell></row><row><cell>BioM-ALBERT-Large</cell><cell>0.6566</cell><cell>0.6135</cell><cell>0.6343</cell></row><row><cell>BioM-ALBERT-Large + Adapter</cell><cell>0.6703</cell><cell>0.6098</cell><cell>0.6387</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,88.99,129.52,419.32,85.88"><head>Table 9</head><label>9</label><figDesc>Subtask 2 SapBERT model comparison results.</figDesc><table coords="11,95.27,158.26,28.86,8.93"><row><cell>Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,88.99,531.61,414.02,109.78"><head>Table 10</head><label>10</label><figDesc>End-to-end comparison results on the validation dataset.</figDesc><table coords="11,95.45,560.34,407.56,81.05"><row><cell>Model NER</cell><cell>Model EL</cell><cell>Validation</cell><cell>Validation</cell></row><row><cell></cell><cell></cell><cell>F1 NER</cell><cell>F1 EL</cell></row><row><cell>BioM-BERT-Large</cell><cell>SapBERT XLMR-Large</cell><cell>0.7055</cell><cell>0.4514</cell></row><row><cell>BioM-BERT-Large + Preprocessing</cell><cell>SapBERT XLMR-Large</cell><cell>0.6576</cell><cell>0.4644</cell></row><row><cell>CLIN-X-ES</cell><cell>SapBERT XLMR-Large</cell><cell>0.6912</cell><cell>0.4845</cell></row><row><cell>Pre-trained Spanish RoBERTa</cell><cell>SapBERT XLMR-Large</cell><cell>0.6915</cell><cell>0.4838</cell></row><row><cell>BioM-ALBERT Large + Adapter</cell><cell>SapBERT XLMR-Large</cell><cell>0.6387</cell><cell>0.4402</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,88.99,90.49,418.83,402.10"><head>Table 11</head><label>11</label><figDesc>End-to-end comparison results on the test dataset.In this section, we look into the errors of our best NER model (a further pre-trained Spanish RoBERTa-Biomedical Clinical) on the validation set. The following types of errors are identified:‚Ä¢ Redundant context -for short mentions, instead of only identifying the mention itself, the model outputs a phrase containing the mention, thus adding unnecessary context. For example, in the sentence 'Reacci√≥n de Mantoux negativa. ' our model outputs the phrase 'Reacci√≥n de Mantoux' instead of the ground truth -'Mantoux'. This is also the case when multiple separate mentions are listed as one, e.</figDesc><table coords="12,89.29,119.22,410.79,137.71"><row><cell>Model NER</cell><cell>Model EL</cell><cell>Test F1</cell><cell>Test F1</cell></row><row><cell></cell><cell></cell><cell>NER</cell><cell>EL</cell></row><row><cell>BioM-BERT-Large</cell><cell>SapBERT XLMR-Large</cell><cell>0.6769</cell><cell>0.5293</cell></row><row><cell>BioM-BERT-Large + Preprocessing</cell><cell>SapBERT XLMR-Large</cell><cell>0.6743</cell><cell>0.5216</cell></row><row><cell>CLIN-X-ES</cell><cell>SapBERT XLMR-Large</cell><cell>0.6981</cell><cell>0.5283</cell></row><row><cell>Pre-trained Spanish RoBERTa</cell><cell>SapBERT XLMR-Large</cell><cell>0.7154</cell><cell>0.5369</cell></row><row><cell>BioM-ALBERT Large + Adapter</cell><cell>SapBERT XLMR-Large</cell><cell>0.6580</cell><cell>0.51870</cell></row><row><cell>5.6. Error Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.6.1. Subtask 1 -Named Entity Recognition</cell><cell></cell><cell></cell></row></table><note coords="12,330.40,357.02,176.78,10.91;12,116.56,370.57,368.31,10.91;12,107.28,385.47,400.55,10.91;12,116.56,399.02,390.61,10.91;12,116.56,413.59,390.51,9.72;12,116.56,426.12,389.42,10.91;12,116.56,439.67,389.42,10.91;12,114.72,453.22,391.55,10.91;12,116.56,466.77,331.90,10.91;12,107.28,481.67,399.90,10.91"><p><p>g. 'Mantoux, electrocardiograma (ECG), radiograf√≠a de t√≥rax... ' are identified as a single mention, instead of 3 separate ones.</p>‚Ä¢ Partitioned mentions -parts of a single true procedure mention are identified as separate mentions. For example, in the sentence 'El paciente se intervino quir√∫rgicamente, realiz√°ndose una cistoprostatectom√≠a radical con linfadenectom√≠a m√°s derivaci√≥n tipo Indiana y cierre del defecto de pared abdominal con fascia lata.' our model predicts two separate mentions 'cistoprostatectom√≠a radical' and 'linfadenectom√≠a', instead of the truth -'cistoprostatectom√≠a radical con linfadenectom√≠a'. Overall, the model has a difficulty identifying long clinical procedure mentions, such as surgery descriptions. ‚Ä¢ Nested mentions are not identified -this is an inherent problem of the token classifier,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="13,88.99,220.78,410.14,169.60"><head>Table 12</head><label>12</label><figDesc>Examples of entity linking errors.</figDesc><table coords="13,94.72,249.52,404.42,140.86"><row><cell>Query</cell><cell>Predicted</cell><cell>True</cell><cell>Predicted text</cell><cell>True text</cell></row><row><cell></cell><cell>code</cell><cell>code</cell><cell></cell><cell></cell></row><row><cell>estudio radiogr√°fico</cell><cell cols="3">363679005 363680008 procedimiento de diag-</cell><cell cols="2">procedimiento radiogr√°-</cell></row><row><cell>(radiographic study)</cell><cell></cell><cell></cell><cell>n√≥stico por im√°genes (di-</cell><cell cols="2">fico (radiographic proce-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>agnostic imaging proce-</cell><cell>dure)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dure)</cell><cell></cell></row><row><cell>somatometr√≠a (so-</cell><cell cols="3">104936007 54709006 medici√≥n de somatome-</cell><cell cols="2">medici√≥n corporal (body</cell></row><row><cell>matometry)</cell><cell></cell><cell></cell><cell>dina (somatomedin mea-</cell><cell>measurement)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>surement)</cell><cell></cell></row><row><cell>antibi√≥tico (antibi-</cell><cell cols="3">58427002 281789004 medici√≥n de antibi√≥tico</cell><cell cols="2">tratamiento con an-</cell></row><row><cell>otic)</cell><cell></cell><cell></cell><cell>(antibiotic measurement)</cell><cell>tibi√≥ticos</cell><cell>(antibiotic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>treatment)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,92.57,671.04,187.10,8.97"><p>https://github.com/svassileva/clef2023-medprocner</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,92.57,659.97,188.88,8.97"><p>CIE-10-ES 2022 Procedimientos Tabla de Referencia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,92.57,660.07,96.03,8.97"><p>SPACCC Sentence Splitter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="7,92.57,660.08,154.91,8.97"><p>Hugging Face token classification pipeline</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4" coords="7,95.35,671.04,308.87,8.97"><p>Another example of using the 'first' strategy from the Hugging Face documentation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is partially funded by Project <rs type="grantNumber">UNITe BG05M2OP001-1.001-0004</rs> funded by the <rs type="programName">OP "Science and Education for Smart Growth</rs>", co-funded by the <rs type="funder">EU</rs> through the <rs type="funder">ESI Funds</rs>, and partially financed by the <rs type="funder">European Union-NextGenerationEU</rs>, through the <rs type="programName">National Recovery and Resilience Plan</rs> <rs type="funder">of the Republic of Bulgaria</rs>, project No <rs type="grantNumber">BG-RRP-2.004-0008</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qA3ANnC">
					<idno type="grant-number">UNITe BG05M2OP001-1.001-0004</idno>
					<orgName type="program" subtype="full">OP &quot;Science and Education for Smart Growth</orgName>
				</org>
				<org type="funding" xml:id="_VrJKrM9">
					<orgName type="program" subtype="full">National Recovery and Resilience Plan</orgName>
				</org>
				<org type="funding" xml:id="_ePMJg34">
					<idno type="grant-number">BG-RRP-2.004-0008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,285.51,394.53,10.91;14,112.66,299.06,393.33,10.91;14,112.66,312.61,393.33,10.91;14,112.66,326.16,393.33,10.91;14,112.66,339.71,306.11,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,234.48,299.06,271.51,10.91;14,112.66,312.61,307.20,10.91">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farr√©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,445.02,312.61,60.97,10.91;14,112.66,326.16,393.33,10.91;14,112.66,339.71,252.02,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="14,112.66,353.26,394.53,10.91;14,112.66,366.81,393.33,10.91;14,112.66,380.36,393.33,10.91;14,112.66,393.91,136.29,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,232.45,366.81,273.54,10.91;14,112.66,380.36,125.38,10.91">Overview of MedProcNER task on medical procedure detection and entity linking at BioASQ</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farr√©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasc√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,280.63,380.36,225.35,10.91;14,112.66,393.91,105.59,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,407.46,394.53,10.91;14,112.28,421.01,393.70,10.91;14,112.66,434.55,338.04,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Carrino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-Estap√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Guti√©rrez-Fandi√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Llop-Palao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>P√†mies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<title level="m" coord="14,256.23,421.01,249.76,10.91;14,112.66,434.55,308.23,10.91">Biomedical and clinical language models for spanish: On the benefits of domain-specific pretraining in a mid-resource scenario</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,448.10,393.53,10.91;14,112.66,461.65,395.01,10.91;14,112.66,475.20,38.81,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,290.35,448.10,215.84,10.91;14,112.66,461.65,173.66,10.91">Learning domain-specialised representations for cross-lingual biomedical entity linking</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vuliƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,310.59,461.65,149.47,10.91">Proceedings of ACL-IJCNLP 2021</title>
		<meeting>ACL-IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,488.75,394.52,10.91;14,112.28,502.30,395.00,10.91;14,112.28,515.85,394.91,10.91;14,112.66,529.40,393.33,10.91;14,112.66,542.95,336.31,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,362.24,502.30,145.03,10.91;14,112.28,515.85,394.91,10.91;14,112.66,529.40,166.29,10.91">Overview of distemist at bioasq: Automatic detection and normalization of diseases from clinical texts: results, methods, evaluation and multilingual resources</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasc√≥</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farr√©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,302.03,529.40,203.96,10.91;14,112.66,542.95,248.29,10.91">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="179" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,556.50,393.61,10.91;14,112.66,570.05,393.33,10.91;14,112.66,583.60,353.23,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,290.32,556.50,215.95,10.91;14,112.66,570.05,186.23,10.91">Biomedical spanish language models for entity recognition and linking at bioasq distemist</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moscato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Postiglione</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sperl√¨</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,321.51,570.05,184.48,10.91;14,112.66,583.60,265.21,10.91">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,597.15,393.61,10.91;14,112.66,610.69,393.33,10.91;14,112.66,624.24,393.33,10.91;14,112.66,637.79,242.38,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,328.75,597.15,177.53,10.91;14,112.66,610.69,137.66,10.91">A joint model for medical named entity recognition and normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,279.39,610.69,226.60,10.91;14,112.66,624.24,393.33,10.91;14,112.66,637.79,154.75,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) co-located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020)</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020) co-located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,651.34,393.71,10.91;14,112.66,664.89,393.33,10.91;15,112.66,86.97,395.01,10.91;15,112.66,100.52,320.50,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,498.63,651.34,7.73,10.91;14,112.66,664.89,202.02,10.91">A deep learning-based system for pharmaconer</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5706</idno>
		<ptr target="https://aclanthology.org/D19-5706.doi:10.18653/v1/D19-5706" />
	</analytic>
	<monogr>
		<title level="m" coord="14,339.61,664.89,166.38,10.91;15,112.66,86.97,316.87,10.91">Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics</title>
		<meeting>the 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,394.53,10.91;15,112.66,127.61,395.17,10.91;15,112.66,141.16,394.53,10.91;15,112.28,154.71,395.00,10.91;15,112.66,168.26,295.83,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,112.66,127.61,395.17,10.91;15,112.66,141.16,52.89,10.91">PharmaCoNER: Pharmacological substances, compounds and proteins named entity recognition track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marimon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Intxaurrondo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5701</idno>
		<ptr target="https://aclanthology.org/D19-5701.doi:10.18653/v1/D19-5701" />
	</analytic>
	<monogr>
		<title level="m" coord="15,197.86,141.16,309.33,10.91;15,112.28,154.71,194.47,10.91">Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics</title>
		<meeting>the 5th Workshop on BioNLP Open Shared Tasks, Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.33,10.91;15,112.66,195.36,311.37,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="15,326.58,181.81,179.40,10.91;15,112.66,195.36,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,208.91,393.53,10.91;15,112.66,222.46,393.32,10.91;15,112.28,236.01,393.71,10.91;15,112.33,249.56,154.49,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,368.29,208.91,137.90,10.91;15,112.66,222.46,149.39,10.91">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,285.39,222.46,220.59,10.91;15,112.28,236.01,393.71,10.91;15,112.33,249.56,56.35,10.91">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4228" to="4238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,393.61,10.91;15,112.66,276.66,393.33,10.91;15,112.66,290.20,394.52,10.91;15,112.66,303.75,55.16,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,256.90,263.11,249.36,10.91;15,112.66,276.66,319.61,10.91">Hpi-dhc @ bioasq distemist: Spanish biomedical entity linking with pre-trained transformers and cross-lingual candidate retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-P</forename><surname>Schapranow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,452.94,276.66,53.05,10.91;15,112.66,290.20,364.17,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="244" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,317.30,393.33,10.91;15,112.66,330.85,393.33,10.91;15,112.14,344.40,395.53,10.91;15,112.66,357.95,38.81,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,319.26,317.30,186.73,10.91;15,112.66,330.85,287.26,10.91">Diag√ëoza: a natural language processing tool for automatic annotation of clinical free text with snomed-ct</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">F L M</forename><surname>-R. Matic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Bernik</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tovornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,423.35,330.85,82.64,10.91;15,112.14,344.40,347.34,10.91">Proceedings of the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<meeting>the Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,371.50,393.32,10.91;15,112.66,385.05,373.72,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="15,281.31,371.50,224.67,10.91;15,112.66,385.05,239.28,10.91">NLNDE at CANTEMIST: neural sequence labeling and parsing approaches for clinical concept extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Str√∂tgen</surname></persName>
		</author>
		<idno>CoRR abs/2010.12322</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,398.60,393.33,10.91;15,112.66,412.15,394.62,10.91;15,112.66,425.70,160.91,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,458.71,398.60,47.28,10.91;15,112.66,412.15,374.20,10.91">Extracting neoplasms morphology mentions in spanish clinical cases through word embeddings</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>L√≥pez-√öbeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>D√≠az-Galiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Mart√≠n-Valdivia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A U</forename><surname>L√≥pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,425.70,72.06,10.91">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="324" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,439.25,393.33,10.91;15,112.66,452.79,393.33,10.91;15,112.66,466.34,319.36,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,361.81,439.25,144.18,10.91;15,112.66,452.79,158.42,10.91">A joint model for medical named entity recognition and normalization</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">C X W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Y</forename><forename type="middle">N</forename><surname>Ying Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,293.44,452.79,212.55,10.91;15,112.66,466.34,94.37,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020)</title>
		<title level="s" coord="15,214.08,466.34,129.93,10.91">CEUR Workshop Proceedings</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,479.89,393.33,10.91;15,112.66,493.44,394.52,10.91;15,112.66,506.99,80.57,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,293.40,479.89,103.85,10.91">Vicomtech at cantemist</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Aitor Garc√≠a-Pablos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naiara</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,440.88,479.89,65.10,10.91;15,112.66,493.44,250.46,10.91">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020)</title>
		<title level="s" coord="15,370.24,493.44,132.15,10.91">CEUR Workshop Proceedings</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,520.54,394.61,10.91;15,112.66,534.09,394.62,10.91;15,112.66,547.64,328.01,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="15,352.49,520.54,154.78,10.91;15,112.66,534.09,337.77,10.91">MedProcNER/ProcTEMIST Corpus: Gold Standard annotations for Clinical Procedures Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>L√≥pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7929830</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7929830.doi:10.5281/zenodo.7929830" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,561.19,394.53,10.91;15,112.66,574.74,395.01,10.91;15,112.66,588.29,262.69,10.91" xml:id="b18">
	<monogr>
		<author>
			<orgName type="collaboration" coords="15,464.11,561.19,32.31,10.91">IHTSDO</orgName>
		</author>
		<ptr target="https://confluence.ihtsdotools.org/display/RMT/October+2022+Spanish+edition+release" />
		<title level="m" coord="15,112.66,561.19,340.56,10.91;15,112.66,574.74,114.89,10.91">International Health Terminology Standards Development Organisation</title>
		<meeting><address><addrLine>SNOMED CT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>SNOMED Clinical Terms</note>
</biblStruct>

<biblStruct coords="15,112.66,601.84,393.33,10.91;15,112.66,615.39,394.04,10.91;15,112.66,628.93,178.84,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,190.92,601.84,315.07,10.91;15,112.66,615.39,50.91,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/nar/nar32.html#Bodenreider04" />
	</analytic>
	<monogr>
		<title level="j" coord="15,181.37,615.39,85.61,10.91">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,642.48,266.87,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="15,235.87,642.48,112.58,10.91">Named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ganapathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,656.03,395.17,10.91;15,112.66,669.58,395.01,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="15,137.85,669.58,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.66,100.52,393.33,10.91;16,112.66,114.06,397.48,10.91;16,112.36,128.84,162.63,9.27" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,300.84,86.97,205.14,10.91;16,112.66,100.52,319.19,10.91">iCLIN-x/i: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Str√∂tgen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btac297</idno>
		<ptr target="https://doi.org/10.1093%2Fbioinformatics%2Fbtac297.doi:10.1093/bioinformatics/btac297" />
	</analytic>
	<monogr>
		<title level="j" coord="16,440.91,100.52,65.08,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="3267" to="3274" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,141.16,393.33,10.91;16,112.66,154.71,393.33,10.91;16,112.66,168.26,393.33,10.91;16,112.66,181.81,228.65,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,452.89,141.16,53.10,10.91;16,112.66,154.71,393.33,10.91;16,112.66,168.26,212.33,10.91">The mespen resource for english-spanish medical machine translation and terminologies: Census of parallel corpora, glossaries and term translations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Intxaurrondo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marimon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,347.52,168.26,158.46,10.91;16,112.66,181.81,119.98,10.91">LREC MultilingualBIO: Multilingual Biomedical Text Processing</title>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,195.36,393.32,10.91;16,112.26,208.91,393.73,10.91;16,112.66,222.46,395.01,10.91;16,112.66,236.01,258.95,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,219.86,195.36,286.12,10.91;16,112.26,208.91,153.47,10.91">BioM-transformers: Building large biomedical language models with BERT, ALBERT and ELECTRA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanker</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.bionlp-1.24" />
	</analytic>
	<monogr>
		<title level="m" coord="16,289.99,208.91,216.00,10.91;16,112.66,222.46,276.72,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics</title>
		<meeting>the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,249.56,395.17,10.91;16,112.33,263.11,393.65,10.91;16,112.66,276.66,394.53,10.91;16,112.66,290.20,397.48,10.91;16,112.36,304.98,169.07,9.27" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="16,290.15,249.56,217.68,10.91;16,112.33,263.11,120.98,10.91">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vuliƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.617.doi:10.18653/v1/2020.emnlp-main.617" />
	</analytic>
	<monogr>
		<title level="m" coord="16,254.63,263.11,251.35,10.91;16,112.66,276.66,390.21,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,317.30,393.61,10.91;16,112.66,330.85,276.08,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>R√ºckl√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<title level="m" coord="16,346.92,317.30,159.35,10.91;16,112.66,330.85,146.20,10.91">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,344.40,395.17,10.91;16,112.66,357.95,395.17,10.91;16,112.66,371.50,394.52,10.91;16,112.66,385.05,394.62,10.91;16,112.66,398.60,173.66,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="16,250.86,357.95,256.97,10.91;16,112.66,371.50,12.78,10.91">Cross-lingual knowledge transfer for clinical phenotyping</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kyparissidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giannakoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Loeser</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.95" />
	</analytic>
	<monogr>
		<title level="m" coord="16,148.48,371.50,353.76,10.91">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="900" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,412.15,393.33,10.91;16,112.66,425.70,348.63,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05118</idno>
		<title level="m" coord="16,425.67,412.15,80.32,10.91;16,112.66,425.70,233.30,10.91">Tune: A research platform for distributed model selection and training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
