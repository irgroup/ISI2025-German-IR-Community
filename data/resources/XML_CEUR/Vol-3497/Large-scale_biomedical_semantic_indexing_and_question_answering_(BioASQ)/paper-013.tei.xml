<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.05,399.66,15.39;1,88.69,106.97,84.79,15.39">Semi-Supervised Training for Biomedical Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.29,71.77,10.68"><forename type="first">Dimitra</forename><surname>Panou</surname></persName>
							<email>panou@fleming.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Fundamental Biomedical Science</orgName>
								<orgName type="institution">Biomedical Sciences Research Center &quot;Alexander Fleming&quot;</orgName>
								<address>
									<addrLine>34 Fleming Street</addrLine>
									<postCode>16672</postCode>
									<settlement>Vari</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.72,136.29,71.20,10.68"><forename type="first">Martin</forename><surname>Reczko</surname></persName>
							<email>reczko@fleming.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Fundamental Biomedical Science</orgName>
								<orgName type="institution">Biomedical Sciences Research Center &quot;Alexander Fleming&quot;</orgName>
								<address>
									<addrLine>34 Fleming Street</addrLine>
									<postCode>16672</postCode>
									<settlement>Vari</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.05,399.66,15.39;1,88.69,106.97,84.79,15.39">Semi-Supervised Training for Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F7C8E32E5030C3D46E481523C91E3DCA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answering</term>
					<term>Semi-supervised learning</term>
					<term>BioASQ</term>
					<term>GANBERT</term>
					<term>large language models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently introduced semi-supervised method GANBERT for finetuning large language models [1] has been applied for document relevance prediction in biomedical question answering. The additional use of unlabeled texts during training enhances the robustness of the prediction and outperforms our previous transformer ELECTROLBERT <ref type="bibr" coords="1,241.50,246.02,9.36,8.01" target="#b1">[2]</ref>. The initial document selection phase used both for ELECTROLBERT and GANBERT has been improved using BM25 combined with RM3 query expansion with optimized parameters. Both systems were continuously improved during the BioASQ11 [3] competition and in the last batch, GANBERT ranked as the 3 𝑟𝑑 team for document prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One major bottleneck in the development of robust question answering systems is the lack of large volumes of high quality question answer pairs provided by human experts. Though transfer learning by finetuning pretrained large language models (LLMs) alleviates this problem <ref type="bibr" coords="1,89.29,432.54,11.28,9.74" target="#b4">[5]</ref>, the limited data jeopardizes finetuning through overfitting. A recently suggested remedy <ref type="bibr" coords="1,493.30,432.54,12.69,9.74" target="#b0">[1]</ref> transfers the successful paradigm of semi-supervised learning used in Generative Adversarial Networks (GAN) for image processing <ref type="bibr" coords="1,265.17,459.64,12.96,9.74" target="#b5">[6]</ref> to the finetuning of LLMs. GANBERT extends the fine-tuning of BERT with unlabeled data using GAN framework, where a 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑜𝑟(𝐺) is trained to produce samples of the internal BERT representation resembling the distribution over the unlabeled data, and a 𝑑𝑖𝑠𝑐𝑟𝑖𝑚𝑖𝑛𝑎𝑡𝑜𝑟(𝐷) that is trained to distinguish samples of the generator from the real instances. By generating only the internal representation of text, GANBERT avoids the generation of "fake" text instances. It is an effective semi-supervised method that can improve the generalization capability. Using vast amounts of unlabeled texts during training, the scope of the language model can be expanded to facilitate the use of alternative formulations for the same semantic content. In the original GANBERT paper <ref type="bibr" coords="1,378.93,568.03,12.97,9.74" target="#b0">[1]</ref> tests were performed for news topic classification, question conceptual class prediction, sentiment analysis and text genre classification. Two GANBERT variants were later successfully used for predicting he checkworthiness of potential fake news in tweets <ref type="bibr" coords="2,284.64,101.64,11.39,9.74" target="#b6">[7]</ref>. In <ref type="bibr" coords="2,315.32,101.64,11.39,9.74" target="#b7">[8]</ref>, the noise generation in GANBERT was optimized for the task of discriminating correct paraphrases of Spanish texts. In the following we describe optimized document selection and the application of GANBERT for document relevance prediction in biomedical question answering in the BioASQ11 competition <ref type="bibr" coords="2,472.43,142.29,11.53,9.74" target="#b8">[9]</ref>. We also provide details for the additional predictions with our ELECTROLBERT algorithm <ref type="bibr" coords="2,481.36,155.84,12.93,9.74" target="#b1">[2]</ref> in the same competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BM25 and RM3 hyperparameter optimization</head><p>To identify documents relevant for a question, we replace the TF/IDF method with the widely used BM25 <ref type="bibr" coords="2,144.15,241.56,16.42,9.74" target="#b9">[10]</ref>. BM25 has two parameters 𝑘1 and 𝑏. 𝑘1 is intuitively related to the rate of increase in a document's score from matching an additional occurrence of a term, where smaller 𝑘1 provides a faster increase. The parameter 𝑏 controls the extent of document-length normalisation. The search is combined with RM3 <ref type="bibr" coords="2,319.22,282.21,16.42,9.74" target="#b10">[11]</ref>, a classic pseudo-relevance feedback based query expansion model, to find related concepts. RM3 has three parameters, 𝑡𝑒𝑟𝑚𝑠 is the number of query expansion terms, 𝑑𝑜𝑐𝑠 is the number of top-ranked documents to obtain the expansion terms and 𝑞𝑤 defines the weight of the original query. The efficient Python implementation in the package Pyserini is used <ref type="bibr" coords="2,311.63,336.40,16.42,9.74" target="#b11">[12]</ref>. A gridsearch on these parameters to optimize the mean average precision (𝑀𝐴𝑃) of the top 10 returned documents for the BioASQ11 training set provided the values that were used in all four batches of BioASQ11. A random search optimizing the average 𝑀𝐴𝑃 of the top 10 returned documents for the 240 questions in the first three batches of BioASQ11 indicates potential improvements. The optimized parameters shown in table 1 clearly outperform the default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training, validation and test data</head><p>For finetuning GANBERT, all pairs of a question and its correct documents provided in the training set for BioASQ11 are used for the 'relevant' class. As introduced in the ELECTROLBERT training <ref type="bibr" coords="2,127.83,489.87,11.46,9.74" target="#b1">[2]</ref>, the negative examples for the 'non-relevant' class are generated using a range of false positives from the initial document selection phase to better discriminate the relevant documents obtained. All questions of the relevance training set were processed with BM25 and RM3 using the settings marked with B3+4:EB0-4 in table 1 to select 1000 relevant documents for each question. The documents were ranked according to their score and all documents between rank 100 and 150 were used as negative examples, excluding potential positive examples in these ranks. The values of the start and end rank positions for the negative set were optimized by retraining and maximizing the mean average precision measured on all batches of BioASQ10. For the unlabeled set, all pairs of a question and its ideal answer and all related snippets from the BioASQ10 training set were used. As a validation set, the top 100 documents scored with BM25 and RM3 (settings again as in B3+4:EB0-4) of the 240 questions in the first three batches of BioASQ11 was used. A final independent test was made on the 90 questions of batch 4 of BioASQ11. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GANBERT finetuning and hyperparameter optimization</head><p>The adaptation of the GANBERT architecture introduced in <ref type="bibr" coords="3,356.91,475.60,12.83,9.74" target="#b0">[1]</ref> for document relevance classification is shown in figure <ref type="figure" coords="3,210.70,489.15,3.81,9.74" target="#fig_1">1</ref>. Using the labeled and unlabeled data described in the previous section for finetuning and employing the large pretrained BERT model provided with the GANBERT implementation in the path for the real data (provided by the authors of GANBERT at https://github.com/crux82/ganbert), all relevant hyperparameters for GANBERT are optimized by multiple finetunings while monitoring the performance on the first three batches of BioASQ11 as shown in table <ref type="table" coords="3,233.41,556.90,3.81,9.74">2</ref>. All GANBERT models perform substantially better when compared to the standard BERT model and the performance of GANBERT is quite stable for the different hyperparameter settings, also for variations as suggested in <ref type="bibr" coords="3,390.43,583.99,12.69,9.74" target="#b7">[8]</ref> in the noise generation part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In table <ref type="table" coords="3,125.09,656.17,5.01,9.74" target="#tab_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Hyperparameter optimization for question answering GANBERT models using the 𝑀𝐴𝑃 averaged over the first three batches of BioASQ11 (𝑀𝐴𝑃 𝑏123 ). The final test uses the 𝑀𝐴𝑃 of BioASQ11 batch4 (𝑀𝐴𝑃 𝑏4 ). Unless specified, the sequence length for prediction is 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 175. All models are finetuned for 32000 steps. 𝐿𝑅 denotes the learning rate, 𝑆𝐿𝐸𝑁 the sequence length during finetuning and 𝐿𝐴𝐵𝐸𝐿_𝑀𝐴𝑆𝐾 controls the ratio between the number of labeled and unlabeled examples. The bold model is GANBERT3, submitted as ELECTOLBERT-4 in batch4. 0.3289 0.2220 batch 4 in the BioASQ10 competition described in <ref type="bibr" coords="5,314.81,88.09,11.49,9.74" target="#b1">[2]</ref>. The models marked with 'large model' use the large architecture in <ref type="bibr" coords="5,222.06,101.64,11.59,9.74" target="#b1">[2]</ref>, where pretraining was continued for 30 million steps and finetuning was performed with the labeled part of the training set for GANBERT for 180000 steps. It can be observed that the sequence length for predictions converged to an optimal value of 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 175 during the competition. With the optimized first phase document selection, it also became evident that the transformers in the second phase focus on the final ranking of the results and the number of documents was gradually reduced from 𝑛𝑑𝑜𝑐𝑠 = 11500 to 𝑛𝑑𝑜𝑐𝑠 = 10. In batch 4, the names of the systems ELECTROLBERT- <ref type="bibr" coords="5,386.31,182.93,15.19,9.74" target="#b0">[1,</ref><ref type="bibr" coords="5,401.50,182.93,10.13,9.74" target="#b1">2,</ref><ref type="bibr" coords="5,411.63,182.93,10.13,9.74" target="#b2">3,</ref><ref type="bibr" coords="5,421.76,182.93,10.13,9.74" target="#b3">4]</ref> are used for the different GANBERT submissions. Query expansion using Roccio's methond <ref type="bibr" coords="5,463.43,398.74,9.88,5.34" target="#b13">[14]</ref> base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 400, 𝑛𝑑𝑜𝑐𝑠 = 300 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>Our suggested GANBERT version for document relevance prediction has shown promising performance, defeating our previous algorithm ELECTROLBERT. As can be seen at the published BioASQ11 results, both algorithms perform better than some of the other systems that seem to employ ChatGPT <ref type="bibr" coords="6,207.57,153.05,16.36,9.74" target="#b14">[15]</ref>. One obvious extension would be the replacement of BERT in the path processing the real data with ELECTROLBERT. This would also lead to the use of a more appropriate scientific vocabulary, as the BERT model provided with the GANBERT implementation uses a general purpose vocabulary. It should also be noted that the size of the unlabeled data set in this study is relatively small due to generation of this using only text available with the BioASQ datasets and our limited computational resources. One way to increase this could be the use of random segments from Pubmed abstracts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,132.82,656.17,375.00,9.74;3,89.29,669.72,416.69,9.74;4,191.50,227.11,8.64,12.46;4,131.23,192.00,8.64,12.46;4,126.92,233.50,17.29,12.46;4,272.64,221.83,31.71,12.46;4,298.19,131.97,7.31,12.46;4,165.16,116.17,28.61,12.46;4,226.79,116.73,9.31,12.46;4,367.72,176.82,8.64,12.46;4,139.67,170.07,46.58,12.46;4,136.44,234.52,8.20,10.97;4,133.37,247.51,15.21,10.97;4,137.04,192.54,8.20,10.97;4,137.08,206.89,7.61,10.97;4,438.74,156.71,42.58,12.46;4,438.74,178.54,66.53,12.46;4,438.74,200.09,38.58,12.46"><head></head><label></label><figDesc>the performances of our document relevance submissions for the BioASQ11 competition are listed. All submissions marked with 'base model' use the ELECTROLBERT model of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,301.37,416.68,8.91;4,89.29,313.32,416.87,8.91;4,89.29,325.28,416.70,8.91;4,89.29,337.23,416.69,8.91;4,89.29,349.19,279.75,8.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:The GANBERT architecture for question answering: The 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑜𝑟 G generates a set of fake representations F given a random distribution. These and the unlabeled U and labeled L vector representations computed by BERT are used as input for the 𝑑𝑖𝑠𝑐𝑟𝑖𝑚𝑖𝑛𝑎𝑡𝑜𝑟 D. The labeled examples are classified into documents relevant (R) and non-relevant (NR) for a question Q. The real data should be discriminated from the fake representations via the 'is real?' output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,128.64,425.55,4.63,8.91;5,154.78,425.55,28.05,8.91;5,222.37,425.55,28.34,8.91;5,316.80,425.55,4.63,8.91;5,156.13,437.51,25.35,8.91;5,198.19,437.51,76.69,8.91;5,316.81,437.51,4.63,8.91;5,365.85,440.09,101.56,6.42;5,156.13,449.46,25.35,8.91;5,198.19,449.46,76.69,8.91;5,316.81,449.46,4.63,8.91;5,365.85,452.05,101.56,6.42;5,156.13,461.42,25.35,8.91;5,198.19,461.42,76.69,8.91;5,316.81,461.42,4.63,8.91;5,365.23,464.00,102.79,6.42;5,156.13,473.37,25.35,8.91;5,198.19,473.37,76.69,8.91;5,316.81,473.37,4.63,8.91;5,364.46,475.96,104.34,6.42;5,156.13,485.33,25.35,8.91;5,198.19,485.33,76.69,8.91;5,316.81,485.33,4.63,8.91;5,365.23,487.91,102.79,6.43;5,128.64,502.77,4.63,8.91;5,154.78,502.77,28.05,8.91;5,222.37,502.77,28.34,8.91;5,316.80,502.77,4.63,8.91;5,156.13,514.72,25.35,8.91;5,199.35,514.72,74.39,8.91;5,316.81,514.72,4.63,8.91;5,365.28,517.30,102.70,6.42;5,156.13,526.68,25.35,8.91;5,199.11,526.68,74.39,8.91;5,316.81,526.68,4.63,8.91;5,365.28,529.26,102.70,6.42;5,156.13,538.63,25.35,8.91;5,199.15,538.63,74.39,8.91;5,316.81,538.63,4.63,8.91;5,365.28,541.21,102.70,6.43;5,156.13,550.59,25.35,8.91;5,199.12,550.59,74.39,8.91;5,316.81,550.59,4.63,8.91;5,365.28,553.17,102.70,6.43;5,156.13,562.54,25.35,8.91;5,198.19,562.54,76.69,8.91;5,316.81,562.54,4.63,8.91;5,365.85,565.13,101.56,6.42"><head></head><label></label><figDesc>𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 175, 𝑛𝑑𝑜𝑐𝑠 = 60</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.67,416.99,332.91"><head>Table 1</head><label>1</label><figDesc>BM25 &amp; RM3 parameter optimization. 𝑘1 &amp; 𝑏 are parameters of BM25 and the variables 𝑡𝑒𝑟𝑚𝑠 (Expansion terms), 𝑑𝑜𝑐𝑠 (number of top-ranked documents) and 𝑞𝑤 (Original query weight) are parameters of RM3 model. 𝑛𝑎𝑠𝑛𝑜𝑘 specifies the number of questions (total 240) with at least one correct document. 𝑛𝑑𝑜𝑐𝑜𝑘 specifies the number of correctly identified documents (max. 647). In the column "used for", Bx denotes the BioASQ11 test batch x, and EBy denotes the system ELECTOLBERTy.</figDesc><table coords="3,141.72,170.08,309.34,253.49"><row><cell>𝑘1</cell><cell>𝑏</cell><cell cols="7">𝑡𝑒𝑟𝑚𝑠 𝑑𝑜𝑐𝑠 𝑞𝑤 𝑀𝐴𝑃 𝑟123 𝑛𝑎𝑛𝑠𝑜𝑘 𝑛𝑑𝑜𝑐𝑜𝑘 used for</cell></row><row><cell cols="2">1.2 0.75</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2734</cell><cell>128</cell><cell>192</cell><cell>defaults</cell></row><row><cell>0.4</cell><cell>0.3</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2625</cell><cell>138</cell><cell>201</cell><cell>B1:EB2+3</cell></row><row><cell>1.1</cell><cell>0.0</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2752</cell><cell>125</cell><cell>195</cell><cell>B1:EB0+1</cell></row><row><cell>0.4</cell><cell>0.3</cell><cell>17</cell><cell>14</cell><cell>0.6</cell><cell>0.2906</cell><cell>134</cell><cell>198</cell><cell>B2:EB0,2+3</cell></row><row><cell cols="2">0.30 0.31</cell><cell>16</cell><cell>16</cell><cell>0.8</cell><cell>0.2932</cell><cell>135</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.7</cell><cell>0.2936</cell><cell>138</cell><cell>205</cell><cell>B3+4:EB0-4</cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.9</cell><cell>0.2940</cell><cell>129</cell><cell>199</cell><cell></cell></row><row><cell cols="2">0.30 0.31</cell><cell>20</cell><cell>16</cell><cell>0.8</cell><cell>0.2952</cell><cell>134</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.45 0.36</cell><cell>20</cell><cell>21</cell><cell>0.8</cell><cell>0.2980</cell><cell>134</cell><cell>203</cell><cell></cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.8</cell><cell>0.2981</cell><cell>134</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.60 0.37</cell><cell>17</cell><cell>16</cell><cell>0.8</cell><cell>0.2983</cell><cell>130</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.50 0.33</cell><cell>20</cell><cell>25</cell><cell>0.7</cell><cell>0.2987</cell><cell>135</cell><cell>206</cell><cell></cell></row><row><cell cols="2">0.45 0.37</cell><cell>15</cell><cell>22</cell><cell>0.7</cell><cell>0.2992</cell><cell>137</cell><cell>205</cell><cell></cell></row><row><cell cols="2">0.45 0.31</cell><cell>17</cell><cell>20</cell><cell>0.7</cell><cell>0.2993</cell><cell>135</cell><cell>201</cell><cell></cell></row><row><cell cols="2">0.40 0.38</cell><cell>15</cell><cell>24</cell><cell>0.8</cell><cell>0.2999</cell><cell>131</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.40 0.30</cell><cell>18</cell><cell>21</cell><cell>0.7</cell><cell>0.3000</cell><cell>135</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.55 0.34</cell><cell>19</cell><cell>23</cell><cell>0.7</cell><cell>0.3002</cell><cell>138</cell><cell>206</cell><cell></cell></row><row><cell cols="2">0.60 0.34</cell><cell>14</cell><cell>18</cell><cell>0.8</cell><cell>0.3003</cell><cell>131</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.35 0.34</cell><cell>18</cell><cell>25</cell><cell>0.7</cell><cell>0.3004</cell><cell>138</cell><cell>209</cell><cell></cell></row><row><cell cols="2">0.35 0.37</cell><cell>17</cell><cell>26</cell><cell cols="2">0.7 0.3011</cell><cell>138</cell><cell>210</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,222.11,416.99,182.95"><head>Table 3</head><label>3</label><figDesc>BioASQ11 document relevance prediction performance measured as mean average precision (𝑀𝐴𝑃). The column 'model details' specifies the type of the transformer architecture, the sequence length during prediction and the number of documents to be ranked. The model GANBERT4 was trained for twice the number of steps as GANBERT3.</figDesc><table coords="5,119.48,289.56,352.10,115.51"><row><cell>batch</cell><cell>𝑀𝐴𝑃</cell><cell>system</cell><cell>per team rank</cell><cell>model details</cell></row><row><cell>1</cell><cell>0.4590</cell><cell>bioinfo-0</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell cols="2">0.3875 ELECTROLBERT-2,3</cell><cell>4</cell><cell>base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 200, 𝑛𝑑𝑜𝑐𝑠 = 11500</cell></row><row><cell></cell><cell cols="2">0.3732 ELECTROLBERT-0,1</cell><cell>4</cell><cell>base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 250, 𝑛𝑑𝑜𝑐𝑠 = 11500</cell></row><row><cell>2</cell><cell>0.3852</cell><cell>bioinfo-4</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>0.3252</cell><cell>ELECTROLBERT-2</cell><cell>4</cell><cell>base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 175, 𝑛𝑑𝑜𝑐𝑠 = 6750</cell></row><row><cell></cell><cell>0.2942</cell><cell>ELECTROLBERT-0</cell><cell>4</cell><cell>base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 250, 𝑛𝑑𝑜𝑐𝑠 = 6750</cell></row><row><cell></cell><cell>0.2781</cell><cell>ELECTROLBERT-3</cell><cell>4</cell><cell>base model, 𝑆𝐿𝐸𝑁 𝑝𝑟𝑒𝑑𝑖𝑐𝑡 = 275, 𝑛𝑑𝑜𝑐𝑠 = 6750</cell></row><row><cell></cell><cell>0.2513</cell><cell>ELECTROLBERT-1</cell><cell>4</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>GPU computations were offered by HYPATIA, the Cloud infrastructure of the Greek ELIXIR node.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,365.15,393.32,9.74;6,112.33,378.70,393.64,9.74;6,112.66,392.25,393.32,9.74;6,112.66,405.79,395.01,9.74;6,112.66,419.34,177.35,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,316.06,365.15,189.91,9.74;6,112.33,378.70,231.79,9.74">Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan-Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.191</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.191.doi:10.18653/v1/2020.acl-main.191" />
	</analytic>
	<monogr>
		<title level="m" coord="6,367.83,378.70,138.15,9.74;6,112.66,392.25,393.32,9.74;6,112.66,405.79,46.02,9.74">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2114" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,432.89,395.16,9.74;6,112.66,446.44,394.52,9.74;6,112.66,459.99,394.52,9.74;6,112.66,473.54,131.93,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,167.84,432.89,339.98,9.74;6,112.66,446.44,62.48,9.74">ELECTROLBERT: Combining Replaced Token Detection and Sentence Order Prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Reczko</surname></persName>
		</author>
		<ptr target="nbn:de:0074-3180-7" />
	</analytic>
	<monogr>
		<title level="m" coord="6,200.89,446.44,300.91,9.74">Proc. of CLEF 2022: Conference and Labs of the Evaluation Forum</title>
		<meeting>of CLEF 2022: Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,487.09,394.53,9.74;6,112.66,500.64,393.32,9.74;6,112.66,514.19,393.32,9.74;6,112.66,527.74,393.32,9.74;6,112.66,541.29,306.11,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,234.29,500.64,271.69,9.74;6,112.66,514.19,306.98,9.74">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,445.01,514.19,60.97,9.74;6,112.66,527.74,393.32,9.74;6,112.66,541.29,252.02,9.74">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,112.66,554.84,393.32,9.74;6,112.66,568.38,393.32,9.74;6,112.66,581.93,107.76,9.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,349.28,554.84,156.70,9.74;6,112.66,568.38,104.71,9.74">Overview of BioASQ Tasks 11b and Synergy11 in CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,241.90,568.38,264.08,9.74;6,112.66,581.93,77.06,9.74">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,595.48,393.32,9.74;6,112.33,609.03,395.33,9.74;6,112.66,622.58,155.09,9.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,320.45,595.48,185.53,9.74;6,112.33,609.03,190.47,9.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805.doi:10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,636.13,394.53,9.74;6,112.34,649.68,395.48,9.74;7,112.66,88.09,394.53,9.74;7,112.66,101.64,132.13,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,160.26,649.68,122.75,9.74">Generative Adversarial Nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,370.04,88.09,32.18,9.74">NIPS&apos;14</title>
		<title level="s" coord="6,306.44,649.68,201.38,9.74;7,112.66,88.09,210.89,9.74">Proceedings of the 27th International Confer-ence on Neural Information Processing Systems -</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,115.19,393.33,9.74;7,112.66,128.74,393.58,9.74;7,112.33,142.29,29.19,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,264.20,115.19,241.78,9.74;7,112.66,128.74,274.17,9.74">Fraunhofer SIT at CheckThat! 2022: Semi-Supervised Ensemble Classification for Detecting Check-Worthy Tweets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">N</forename><surname>Grieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,397.22,128.74,109.03,9.74">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,155.84,393.32,9.74;7,112.28,169.38,394.09,9.74;7,112.66,182.93,91.05,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,393.98,155.84,112.00,9.74;7,112.28,169.38,186.50,9.74">an Adversarial Learning Architecture for Paraphrase Identification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan-Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,322.04,169.38,93.33,9.74">Proc. of IberLEF 2022</title>
		<meeting>of IberLEF 2022<address><addrLine>A Coruña, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">September 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,196.48,393.32,9.74;7,112.66,210.03,326.44,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,357.16,196.48,148.82,9.74;7,112.66,210.03,189.95,9.74">BioASQ-QA: A manually curated corpus for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,311.68,210.03,64.68,9.74">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,223.58,393.32,9.74;7,112.66,237.13,211.57,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,331.03,223.58,174.95,9.74;7,112.66,237.13,33.91,9.74">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,156.19,237.13,89.18,9.74">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,250.68,393.32,9.74;7,112.28,264.23,393.70,9.74;7,112.66,277.78,395.01,9.74;7,112.41,291.33,371.50,9.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,226.03,250.68,151.52,9.74">Relevance Based Language Models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383972</idno>
		<ptr target="https://doi.org/10.1145/383952.383972.doi:10.1145/383952.383972" />
	</analytic>
	<monogr>
		<title level="m" coord="7,401.50,250.68,104.48,9.74;7,112.28,264.23,393.70,9.74;7,112.66,277.78,85.10,9.74">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,304.88,393.33,9.74;7,112.33,318.43,393.64,9.74;7,112.66,331.98,362.38,9.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,359.15,304.88,146.83,9.74;7,112.33,318.43,359.89,9.74">Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Nogueira</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2102.10073" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,345.52,393.32,9.74;7,112.66,359.07,322.04,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,430.69,345.52,75.28,9.74;7,112.66,359.07,235.28,9.74">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,371.65,359.07,31.36,9.74">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,372.62,394.52,9.74;7,112.39,386.17,250.80,9.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,235.28,372.62,153.21,9.74">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,417.70,372.62,84.09,9.74">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,399.72,348.13,9.74" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chatgpt</forename><surname>Openai</surname></persName>
		</author>
		<ptr target="https://chat.openai.com/chat" />
		<title level="m" coord="7,201.59,399.72,96.10,9.74">Large language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
