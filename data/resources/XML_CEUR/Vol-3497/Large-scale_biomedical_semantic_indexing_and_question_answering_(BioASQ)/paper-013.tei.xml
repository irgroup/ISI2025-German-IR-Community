<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,85.05,399.66,15.39;1,88.69,106.97,84.79,15.39">Semi-Supervised Training for Biomedical Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,136.29,71.77,10.68"><forename type="first">Dimitra</forename><surname>Panou</surname></persName>
							<email>panou@fleming.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Fundamental Biomedical Science</orgName>
								<orgName type="institution">Biomedical Sciences Research Center &quot;Alexander Fleming&quot;</orgName>
								<address>
									<addrLine>34 Fleming Street</addrLine>
									<postCode>16672</postCode>
									<settlement>Vari</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.72,136.29,71.20,10.68"><forename type="first">Martin</forename><surname>Reczko</surname></persName>
							<email>reczko@fleming.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Fundamental Biomedical Science</orgName>
								<orgName type="institution">Biomedical Sciences Research Center &quot;Alexander Fleming&quot;</orgName>
								<address>
									<addrLine>34 Fleming Street</addrLine>
									<postCode>16672</postCode>
									<settlement>Vari</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,85.05,399.66,15.39;1,88.69,106.97,84.79,15.39">Semi-Supervised Training for Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F7C8E32E5030C3D46E481523C91E3DCA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answering</term>
					<term>Semi-supervised learning</term>
					<term>BioASQ</term>
					<term>GANBERT</term>
					<term>large language models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently introduced semi-supervised method GANBERT for finetuning large language models [1] has been applied for document relevance prediction in biomedical question answering. The additional use of unlabeled texts during training enhances the robustness of the prediction and outperforms our previous transformer ELECTROLBERT <ref type="bibr" coords="1,241.50,246.02,9.36,8.01" target="#b1">[2]</ref>. The initial document selection phase used both for ELECTROLBERT and GANBERT has been improved using BM25 combined with RM3 query expansion with optimized parameters. Both systems were continuously improved during the BioASQ11 [3] competition and in the last batch, GANBERT ranked as the 3 ğ‘Ÿğ‘‘ team for document prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One major bottleneck in the development of robust question answering systems is the lack of large volumes of high quality question answer pairs provided by human experts. Though transfer learning by finetuning pretrained large language models (LLMs) alleviates this problem <ref type="bibr" coords="1,89.29,432.54,11.28,9.74" target="#b4">[5]</ref>, the limited data jeopardizes finetuning through overfitting. A recently suggested remedy <ref type="bibr" coords="1,493.30,432.54,12.69,9.74" target="#b0">[1]</ref> transfers the successful paradigm of semi-supervised learning used in Generative Adversarial Networks (GAN) for image processing <ref type="bibr" coords="1,265.17,459.64,12.96,9.74" target="#b5">[6]</ref> to the finetuning of LLMs. GANBERT extends the fine-tuning of BERT with unlabeled data using GAN framework, where a ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘œğ‘Ÿ(ğº) is trained to produce samples of the internal BERT representation resembling the distribution over the unlabeled data, and a ğ‘‘ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘šğ‘–ğ‘›ğ‘ğ‘¡ğ‘œğ‘Ÿ(ğ·) that is trained to distinguish samples of the generator from the real instances. By generating only the internal representation of text, GANBERT avoids the generation of "fake" text instances. It is an effective semi-supervised method that can improve the generalization capability. Using vast amounts of unlabeled texts during training, the scope of the language model can be expanded to facilitate the use of alternative formulations for the same semantic content. In the original GANBERT paper <ref type="bibr" coords="1,378.93,568.03,12.97,9.74" target="#b0">[1]</ref> tests were performed for news topic classification, question conceptual class prediction, sentiment analysis and text genre classification. Two GANBERT variants were later successfully used for predicting he checkworthiness of potential fake news in tweets <ref type="bibr" coords="2,284.64,101.64,11.39,9.74" target="#b6">[7]</ref>. In <ref type="bibr" coords="2,315.32,101.64,11.39,9.74" target="#b7">[8]</ref>, the noise generation in GANBERT was optimized for the task of discriminating correct paraphrases of Spanish texts. In the following we describe optimized document selection and the application of GANBERT for document relevance prediction in biomedical question answering in the BioASQ11 competition <ref type="bibr" coords="2,472.43,142.29,11.53,9.74" target="#b8">[9]</ref>. We also provide details for the additional predictions with our ELECTROLBERT algorithm <ref type="bibr" coords="2,481.36,155.84,12.93,9.74" target="#b1">[2]</ref> in the same competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BM25 and RM3 hyperparameter optimization</head><p>To identify documents relevant for a question, we replace the TF/IDF method with the widely used BM25 <ref type="bibr" coords="2,144.15,241.56,16.42,9.74" target="#b9">[10]</ref>. BM25 has two parameters ğ‘˜1 and ğ‘. ğ‘˜1 is intuitively related to the rate of increase in a document's score from matching an additional occurrence of a term, where smaller ğ‘˜1 provides a faster increase. The parameter ğ‘ controls the extent of document-length normalisation. The search is combined with RM3 <ref type="bibr" coords="2,319.22,282.21,16.42,9.74" target="#b10">[11]</ref>, a classic pseudo-relevance feedback based query expansion model, to find related concepts. RM3 has three parameters, ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  is the number of query expansion terms, ğ‘‘ğ‘œğ‘ğ‘  is the number of top-ranked documents to obtain the expansion terms and ğ‘ğ‘¤ defines the weight of the original query. The efficient Python implementation in the package Pyserini is used <ref type="bibr" coords="2,311.63,336.40,16.42,9.74" target="#b11">[12]</ref>. A gridsearch on these parameters to optimize the mean average precision (ğ‘€ğ´ğ‘ƒ) of the top 10 returned documents for the BioASQ11 training set provided the values that were used in all four batches of BioASQ11. A random search optimizing the average ğ‘€ğ´ğ‘ƒ of the top 10 returned documents for the 240 questions in the first three batches of BioASQ11 indicates potential improvements. The optimized parameters shown in table 1 clearly outperform the default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training, validation and test data</head><p>For finetuning GANBERT, all pairs of a question and its correct documents provided in the training set for BioASQ11 are used for the 'relevant' class. As introduced in the ELECTROLBERT training <ref type="bibr" coords="2,127.83,489.87,11.46,9.74" target="#b1">[2]</ref>, the negative examples for the 'non-relevant' class are generated using a range of false positives from the initial document selection phase to better discriminate the relevant documents obtained. All questions of the relevance training set were processed with BM25 and RM3 using the settings marked with B3+4:EB0-4 in table 1 to select 1000 relevant documents for each question. The documents were ranked according to their score and all documents between rank 100 and 150 were used as negative examples, excluding potential positive examples in these ranks. The values of the start and end rank positions for the negative set were optimized by retraining and maximizing the mean average precision measured on all batches of BioASQ10. For the unlabeled set, all pairs of a question and its ideal answer and all related snippets from the BioASQ10 training set were used. As a validation set, the top 100 documents scored with BM25 and RM3 (settings again as in B3+4:EB0-4) of the 240 questions in the first three batches of BioASQ11 was used. A final independent test was made on the 90 questions of batch 4 of BioASQ11. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GANBERT finetuning and hyperparameter optimization</head><p>The adaptation of the GANBERT architecture introduced in <ref type="bibr" coords="3,356.91,475.60,12.83,9.74" target="#b0">[1]</ref> for document relevance classification is shown in figure <ref type="figure" coords="3,210.70,489.15,3.81,9.74" target="#fig_1">1</ref>. Using the labeled and unlabeled data described in the previous section for finetuning and employing the large pretrained BERT model provided with the GANBERT implementation in the path for the real data (provided by the authors of GANBERT at https://github.com/crux82/ganbert), all relevant hyperparameters for GANBERT are optimized by multiple finetunings while monitoring the performance on the first three batches of BioASQ11 as shown in table <ref type="table" coords="3,233.41,556.90,3.81,9.74">2</ref>. All GANBERT models perform substantially better when compared to the standard BERT model and the performance of GANBERT is quite stable for the different hyperparameter settings, also for variations as suggested in <ref type="bibr" coords="3,390.43,583.99,12.69,9.74" target="#b7">[8]</ref> in the noise generation part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In table <ref type="table" coords="3,125.09,656.17,5.01,9.74" target="#tab_2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Hyperparameter optimization for question answering GANBERT models using the ğ‘€ğ´ğ‘ƒ averaged over the first three batches of BioASQ11 (ğ‘€ğ´ğ‘ƒ ğ‘123 ). The final test uses the ğ‘€ğ´ğ‘ƒ of BioASQ11 batch4 (ğ‘€ğ´ğ‘ƒ ğ‘4 ). Unless specified, the sequence length for prediction is ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 175. All models are finetuned for 32000 steps. ğ¿ğ‘… denotes the learning rate, ğ‘†ğ¿ğ¸ğ‘ the sequence length during finetuning and ğ¿ğ´ğµğ¸ğ¿_ğ‘€ğ´ğ‘†ğ¾ controls the ratio between the number of labeled and unlabeled examples. The bold model is GANBERT3, submitted as ELECTOLBERT-4 in batch4. 0.3289 0.2220 batch 4 in the BioASQ10 competition described in <ref type="bibr" coords="5,314.81,88.09,11.49,9.74" target="#b1">[2]</ref>. The models marked with 'large model' use the large architecture in <ref type="bibr" coords="5,222.06,101.64,11.59,9.74" target="#b1">[2]</ref>, where pretraining was continued for 30 million steps and finetuning was performed with the labeled part of the training set for GANBERT for 180000 steps. It can be observed that the sequence length for predictions converged to an optimal value of ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 175 during the competition. With the optimized first phase document selection, it also became evident that the transformers in the second phase focus on the final ranking of the results and the number of documents was gradually reduced from ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 11500 to ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 10. In batch 4, the names of the systems ELECTROLBERT- <ref type="bibr" coords="5,386.31,182.93,15.19,9.74" target="#b0">[1,</ref><ref type="bibr" coords="5,401.50,182.93,10.13,9.74" target="#b1">2,</ref><ref type="bibr" coords="5,411.63,182.93,10.13,9.74" target="#b2">3,</ref><ref type="bibr" coords="5,421.76,182.93,10.13,9.74" target="#b3">4]</ref> are used for the different GANBERT submissions. Query expansion using Roccio's methond <ref type="bibr" coords="5,463.43,398.74,9.88,5.34" target="#b13">[14]</ref> base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 400, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 300 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>Our suggested GANBERT version for document relevance prediction has shown promising performance, defeating our previous algorithm ELECTROLBERT. As can be seen at the published BioASQ11 results, both algorithms perform better than some of the other systems that seem to employ ChatGPT <ref type="bibr" coords="6,207.57,153.05,16.36,9.74" target="#b14">[15]</ref>. One obvious extension would be the replacement of BERT in the path processing the real data with ELECTROLBERT. This would also lead to the use of a more appropriate scientific vocabulary, as the BERT model provided with the GANBERT implementation uses a general purpose vocabulary. It should also be noted that the size of the unlabeled data set in this study is relatively small due to generation of this using only text available with the BioASQ datasets and our limited computational resources. One way to increase this could be the use of random segments from Pubmed abstracts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,132.82,656.17,375.00,9.74;3,89.29,669.72,416.69,9.74;4,191.50,227.11,8.64,12.46;4,131.23,192.00,8.64,12.46;4,126.92,233.50,17.29,12.46;4,272.64,221.83,31.71,12.46;4,298.19,131.97,7.31,12.46;4,165.16,116.17,28.61,12.46;4,226.79,116.73,9.31,12.46;4,367.72,176.82,8.64,12.46;4,139.67,170.07,46.58,12.46;4,136.44,234.52,8.20,10.97;4,133.37,247.51,15.21,10.97;4,137.04,192.54,8.20,10.97;4,137.08,206.89,7.61,10.97;4,438.74,156.71,42.58,12.46;4,438.74,178.54,66.53,12.46;4,438.74,200.09,38.58,12.46"><head></head><label></label><figDesc>the performances of our document relevance submissions for the BioASQ11 competition are listed. All submissions marked with 'base model' use the ELECTROLBERT model of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,301.37,416.68,8.91;4,89.29,313.32,416.87,8.91;4,89.29,325.28,416.70,8.91;4,89.29,337.23,416.69,8.91;4,89.29,349.19,279.75,8.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:The GANBERT architecture for question answering: The ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘œğ‘Ÿ G generates a set of fake representations F given a random distribution. These and the unlabeled U and labeled L vector representations computed by BERT are used as input for the ğ‘‘ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘šğ‘–ğ‘›ğ‘ğ‘¡ğ‘œğ‘Ÿ D. The labeled examples are classified into documents relevant (R) and non-relevant (NR) for a question Q. The real data should be discriminated from the fake representations via the 'is real?' output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,128.64,425.55,4.63,8.91;5,154.78,425.55,28.05,8.91;5,222.37,425.55,28.34,8.91;5,316.80,425.55,4.63,8.91;5,156.13,437.51,25.35,8.91;5,198.19,437.51,76.69,8.91;5,316.81,437.51,4.63,8.91;5,365.85,440.09,101.56,6.42;5,156.13,449.46,25.35,8.91;5,198.19,449.46,76.69,8.91;5,316.81,449.46,4.63,8.91;5,365.85,452.05,101.56,6.42;5,156.13,461.42,25.35,8.91;5,198.19,461.42,76.69,8.91;5,316.81,461.42,4.63,8.91;5,365.23,464.00,102.79,6.42;5,156.13,473.37,25.35,8.91;5,198.19,473.37,76.69,8.91;5,316.81,473.37,4.63,8.91;5,364.46,475.96,104.34,6.42;5,156.13,485.33,25.35,8.91;5,198.19,485.33,76.69,8.91;5,316.81,485.33,4.63,8.91;5,365.23,487.91,102.79,6.43;5,128.64,502.77,4.63,8.91;5,154.78,502.77,28.05,8.91;5,222.37,502.77,28.34,8.91;5,316.80,502.77,4.63,8.91;5,156.13,514.72,25.35,8.91;5,199.35,514.72,74.39,8.91;5,316.81,514.72,4.63,8.91;5,365.28,517.30,102.70,6.42;5,156.13,526.68,25.35,8.91;5,199.11,526.68,74.39,8.91;5,316.81,526.68,4.63,8.91;5,365.28,529.26,102.70,6.42;5,156.13,538.63,25.35,8.91;5,199.15,538.63,74.39,8.91;5,316.81,538.63,4.63,8.91;5,365.28,541.21,102.70,6.43;5,156.13,550.59,25.35,8.91;5,199.12,550.59,74.39,8.91;5,316.81,550.59,4.63,8.91;5,365.28,553.17,102.70,6.43;5,156.13,562.54,25.35,8.91;5,198.19,562.54,76.69,8.91;5,316.81,562.54,4.63,8.91;5,365.85,565.13,101.56,6.42"><head></head><label></label><figDesc>ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 175, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 60</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.67,416.99,332.91"><head>Table 1</head><label>1</label><figDesc>BM25 &amp; RM3 parameter optimization. ğ‘˜1 &amp; ğ‘ are parameters of BM25 and the variables ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  (Expansion terms), ğ‘‘ğ‘œğ‘ğ‘  (number of top-ranked documents) and ğ‘ğ‘¤ (Original query weight) are parameters of RM3 model. ğ‘›ğ‘ğ‘ ğ‘›ğ‘œğ‘˜ specifies the number of questions (total 240) with at least one correct document. ğ‘›ğ‘‘ğ‘œğ‘ğ‘œğ‘˜ specifies the number of correctly identified documents (max. 647). In the column "used for", Bx denotes the BioASQ11 test batch x, and EBy denotes the system ELECTOLBERTy.</figDesc><table coords="3,141.72,170.08,309.34,253.49"><row><cell>ğ‘˜1</cell><cell>ğ‘</cell><cell cols="7">ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  ğ‘‘ğ‘œğ‘ğ‘  ğ‘ğ‘¤ ğ‘€ğ´ğ‘ƒ ğ‘Ÿ123 ğ‘›ğ‘ğ‘›ğ‘ ğ‘œğ‘˜ ğ‘›ğ‘‘ğ‘œğ‘ğ‘œğ‘˜ used for</cell></row><row><cell cols="2">1.2 0.75</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2734</cell><cell>128</cell><cell>192</cell><cell>defaults</cell></row><row><cell>0.4</cell><cell>0.3</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2625</cell><cell>138</cell><cell>201</cell><cell>B1:EB2+3</cell></row><row><cell>1.1</cell><cell>0.0</cell><cell>10</cell><cell>10</cell><cell>0.5</cell><cell>0.2752</cell><cell>125</cell><cell>195</cell><cell>B1:EB0+1</cell></row><row><cell>0.4</cell><cell>0.3</cell><cell>17</cell><cell>14</cell><cell>0.6</cell><cell>0.2906</cell><cell>134</cell><cell>198</cell><cell>B2:EB0,2+3</cell></row><row><cell cols="2">0.30 0.31</cell><cell>16</cell><cell>16</cell><cell>0.8</cell><cell>0.2932</cell><cell>135</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.7</cell><cell>0.2936</cell><cell>138</cell><cell>205</cell><cell>B3+4:EB0-4</cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.9</cell><cell>0.2940</cell><cell>129</cell><cell>199</cell><cell></cell></row><row><cell cols="2">0.30 0.31</cell><cell>20</cell><cell>16</cell><cell>0.8</cell><cell>0.2952</cell><cell>134</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.45 0.36</cell><cell>20</cell><cell>21</cell><cell>0.8</cell><cell>0.2980</cell><cell>134</cell><cell>203</cell><cell></cell></row><row><cell cols="2">0.40 0.31</cell><cell>20</cell><cell>16</cell><cell>0.8</cell><cell>0.2981</cell><cell>134</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.60 0.37</cell><cell>17</cell><cell>16</cell><cell>0.8</cell><cell>0.2983</cell><cell>130</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.50 0.33</cell><cell>20</cell><cell>25</cell><cell>0.7</cell><cell>0.2987</cell><cell>135</cell><cell>206</cell><cell></cell></row><row><cell cols="2">0.45 0.37</cell><cell>15</cell><cell>22</cell><cell>0.7</cell><cell>0.2992</cell><cell>137</cell><cell>205</cell><cell></cell></row><row><cell cols="2">0.45 0.31</cell><cell>17</cell><cell>20</cell><cell>0.7</cell><cell>0.2993</cell><cell>135</cell><cell>201</cell><cell></cell></row><row><cell cols="2">0.40 0.38</cell><cell>15</cell><cell>24</cell><cell>0.8</cell><cell>0.2999</cell><cell>131</cell><cell>200</cell><cell></cell></row><row><cell cols="2">0.40 0.30</cell><cell>18</cell><cell>21</cell><cell>0.7</cell><cell>0.3000</cell><cell>135</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.55 0.34</cell><cell>19</cell><cell>23</cell><cell>0.7</cell><cell>0.3002</cell><cell>138</cell><cell>206</cell><cell></cell></row><row><cell cols="2">0.60 0.34</cell><cell>14</cell><cell>18</cell><cell>0.8</cell><cell>0.3003</cell><cell>131</cell><cell>202</cell><cell></cell></row><row><cell cols="2">0.35 0.34</cell><cell>18</cell><cell>25</cell><cell>0.7</cell><cell>0.3004</cell><cell>138</cell><cell>209</cell><cell></cell></row><row><cell cols="2">0.35 0.37</cell><cell>17</cell><cell>26</cell><cell cols="2">0.7 0.3011</cell><cell>138</cell><cell>210</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,222.11,416.99,182.95"><head>Table 3</head><label>3</label><figDesc>BioASQ11 document relevance prediction performance measured as mean average precision (ğ‘€ğ´ğ‘ƒ). The column 'model details' specifies the type of the transformer architecture, the sequence length during prediction and the number of documents to be ranked. The model GANBERT4 was trained for twice the number of steps as GANBERT3.</figDesc><table coords="5,119.48,289.56,352.10,115.51"><row><cell>batch</cell><cell>ğ‘€ğ´ğ‘ƒ</cell><cell>system</cell><cell>per team rank</cell><cell>model details</cell></row><row><cell>1</cell><cell>0.4590</cell><cell>bioinfo-0</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell cols="2">0.3875 ELECTROLBERT-2,3</cell><cell>4</cell><cell>base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 200, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 11500</cell></row><row><cell></cell><cell cols="2">0.3732 ELECTROLBERT-0,1</cell><cell>4</cell><cell>base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 250, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 11500</cell></row><row><cell>2</cell><cell>0.3852</cell><cell>bioinfo-4</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>0.3252</cell><cell>ELECTROLBERT-2</cell><cell>4</cell><cell>base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 175, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 6750</cell></row><row><cell></cell><cell>0.2942</cell><cell>ELECTROLBERT-0</cell><cell>4</cell><cell>base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 250, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 6750</cell></row><row><cell></cell><cell>0.2781</cell><cell>ELECTROLBERT-3</cell><cell>4</cell><cell>base model, ğ‘†ğ¿ğ¸ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ = 275, ğ‘›ğ‘‘ğ‘œğ‘ğ‘  = 6750</cell></row><row><cell></cell><cell>0.2513</cell><cell>ELECTROLBERT-1</cell><cell>4</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>GPU computations were offered by HYPATIA, the Cloud infrastructure of the Greek ELIXIR node.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,365.15,393.32,9.74;6,112.33,378.70,393.64,9.74;6,112.66,392.25,393.32,9.74;6,112.66,405.79,395.01,9.74;6,112.66,419.34,177.35,9.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,316.06,365.15,189.91,9.74;6,112.33,378.70,231.79,9.74">Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan-Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.191</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.191.doi:10.18653/v1/2020.acl-main.191" />
	</analytic>
	<monogr>
		<title level="m" coord="6,367.83,378.70,138.15,9.74;6,112.66,392.25,393.32,9.74;6,112.66,405.79,46.02,9.74">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2114" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,432.89,395.16,9.74;6,112.66,446.44,394.52,9.74;6,112.66,459.99,394.52,9.74;6,112.66,473.54,131.93,9.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,167.84,432.89,339.98,9.74;6,112.66,446.44,62.48,9.74">ELECTROLBERT: Combining Replaced Token Detection and Sentence Order Prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Reczko</surname></persName>
		</author>
		<ptr target="nbn:de:0074-3180-7" />
	</analytic>
	<monogr>
		<title level="m" coord="6,200.89,446.44,300.91,9.74">Proc. of CLEF 2022: Conference and Labs of the Evaluation Forum</title>
		<meeting>of CLEF 2022: Conference and Labs of the Evaluation Forum<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,487.09,394.53,9.74;6,112.66,500.64,393.32,9.74;6,112.66,514.19,393.32,9.74;6,112.66,527.74,393.32,9.74;6,112.66,541.29,306.11,9.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,234.29,500.64,271.69,9.74;6,112.66,514.19,306.98,9.74">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-LÃ³pez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>FarrÃ©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,445.01,514.19,60.97,9.74;6,112.66,527.74,393.32,9.74;6,112.66,541.29,252.02,9.74">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,112.66,554.84,393.32,9.74;6,112.66,568.38,393.32,9.74;6,112.66,581.93,107.76,9.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,349.28,554.84,156.70,9.74;6,112.66,568.38,104.71,9.74">Overview of BioASQ Tasks 11b and Synergy11 in CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,241.90,568.38,264.08,9.74;6,112.66,581.93,77.06,9.74">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,595.48,393.32,9.74;6,112.33,609.03,395.33,9.74;6,112.66,622.58,155.09,9.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,320.45,595.48,185.53,9.74;6,112.33,609.03,190.47,9.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805.doi:10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,636.13,394.53,9.74;6,112.34,649.68,395.48,9.74;7,112.66,88.09,394.53,9.74;7,112.66,101.64,132.13,9.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,160.26,649.68,122.75,9.74">Generative Adversarial Nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,370.04,88.09,32.18,9.74">NIPS&apos;14</title>
		<title level="s" coord="6,306.44,649.68,201.38,9.74;7,112.66,88.09,210.89,9.74">Proceedings of the 27th International Confer-ence on Neural Information Processing Systems -</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,115.19,393.33,9.74;7,112.66,128.74,393.58,9.74;7,112.33,142.29,29.19,9.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,264.20,115.19,241.78,9.74;7,112.66,128.74,274.17,9.74">Fraunhofer SIT at CheckThat! 2022: Semi-Supervised Ensemble Classification for Detecting Check-Worthy Tweets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">N</forename><surname>Grieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,397.22,128.74,109.03,9.74">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,155.84,393.32,9.74;7,112.28,169.38,394.09,9.74;7,112.66,182.93,91.05,9.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,393.98,155.84,112.00,9.74;7,112.28,169.38,186.50,9.74">an Adversarial Learning Architecture for Paraphrase Identification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan-Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,322.04,169.38,93.33,9.74">Proc. of IberLEF 2022</title>
		<meeting>of IberLEF 2022<address><addrLine>A CoruÃ±a, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">September 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,196.48,393.32,9.74;7,112.66,210.03,326.44,9.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,357.16,196.48,148.82,9.74;7,112.66,210.03,189.95,9.74">BioASQ-QA: A manually curated corpus for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,311.68,210.03,64.68,9.74">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,223.58,393.32,9.74;7,112.66,237.13,211.57,9.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,331.03,223.58,174.95,9.74;7,112.66,237.13,33.91,9.74">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,156.19,237.13,89.18,9.74">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,250.68,393.32,9.74;7,112.28,264.23,393.70,9.74;7,112.66,277.78,395.01,9.74;7,112.41,291.33,371.50,9.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,226.03,250.68,151.52,9.74">Relevance Based Language Models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383972</idno>
		<ptr target="https://doi.org/10.1145/383952.383972.doi:10.1145/383952.383972" />
	</analytic>
	<monogr>
		<title level="m" coord="7,401.50,250.68,104.48,9.74;7,112.28,264.23,393.70,9.74;7,112.66,277.78,85.10,9.74">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,304.88,393.33,9.74;7,112.33,318.43,393.64,9.74;7,112.66,331.98,362.38,9.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,359.15,304.88,146.83,9.74;7,112.33,318.43,359.89,9.74">Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Nogueira</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2102.10073" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,345.52,393.32,9.74;7,112.66,359.07,322.04,9.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,430.69,345.52,75.28,9.74;7,112.66,359.07,235.28,9.74">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,371.65,359.07,31.36,9.74">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,372.62,394.52,9.74;7,112.39,386.17,250.80,9.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,235.28,372.62,153.21,9.74">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,417.70,372.62,84.09,9.74">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,399.72,348.13,9.74" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chatgpt</forename><surname>Openai</surname></persName>
		</author>
		<ptr target="https://chat.openai.com/chat" />
		<title level="m" coord="7,201.59,399.72,96.10,9.74">Large language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
