<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,258.61,15.42;1,89.29,107.08,401.52,11.96">Is ChatGPT a Biomedical Expert? Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,132.98,55.99,11.96"><forename type="first">Samy</forename><surname>Ateia</surname></persName>
							<email>samy.ateia@stud.uni-regensburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.92,132.98,78.61,11.96"><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
							<email>udo.kruschwitz@ur.de</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">bioasq CLEF</orgName>
								<orgName type="institution">SamyAteia</orgName>
								<address>
									<postCode>2023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,258.61,15.42;1,89.29,107.08,401.52,11.96">Is ChatGPT a Biomedical Expert? Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8B62904E9E6A01886A7F91892AEDCB58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-Shot Learning</term>
					<term>LLMs</term>
					<term>BioASQ</term>
					<term>GPT-4</term>
					<term>NER</term>
					<term>Question Answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We assessed the performance of commercial Large Language Models (LLMs) GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b Phase B, which is focused on answer generation, both models demonstrated competitive abilities with leading systems. Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets. Even without relevant snippets, their performance was decent, though not on par with the best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&amp;A setting on Factoid and List answers. In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance, but the models fell short compared to other systems. The code needed to rerun these experiments is available through GitHub 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently released ChatGPT models GPT-3.5-Turbo and GPT-4 <ref type="bibr" coords="1,376.75,388.31,13.00,10.91" target="#b0">[1]</ref> and their unprecedented zero-shot performance in a variety of tasks, sparked a surge in the development and application of LLMs. By participating in the eleventh CLEF BioASQ challenge <ref type="bibr" coords="1,390.09,415.41,11.58,10.91" target="#b1">[2]</ref>, we wanted to explore how well these systems perform in specialized domains and whether they can compete with expert fine-tuned systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">BioASQ Challenge</head><p>BioASQ is a series of large-scale biomedical challenges associated with the CLEF 2023 conference. Its 11th iteration comprises three tasks <ref type="bibr" coords="1,264.69,505.54,11.56,10.91" target="#b1">[2]</ref>:</p><p>1. Synergy On Biomedical Semantic QA For Developing Issues 2. Biomedical Semantic QA 3. MedProcNER On MEDical PROCedure Named Entity Recognition This paper focuses on the second and third tasks, the two tasks we participated in. The Biomedical Semantic QA task (Task B) is subdivided into Phase A (document retrieval and snippet extraction) and Phase B (Question Answering) <ref type="bibr" coords="1,334.31,605.82,11.43,10.91" target="#b2">[3]</ref>.</p><p>We will start with a brief overview of some related work in Section 2 before outlining the experimental setup in Section 3. Section 4 presents our methodology followed by a discussion of Results in Section 5. Finally, we will also touch on ethical issues (Section 6) and offer some conclusions (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To motivate our approach and contextualise our contribution we will briefly discuss related work on recently released generative pre-trained transformer models, have a look at few-shot and zero-shot learning and touch on professional search, i.e. search in a professional context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">GPT Models</head><p>Recently released generative pre-trained transformer (GPT) models GPT-4 and GPT-3.5-turbo are based on the transformer architecture <ref type="bibr" coords="2,273.80,276.61,12.70,10.91" target="#b3">[4]</ref> and pre-trained on the next token prediction task. These models are additionally fine-tuned with reinforcement learning from human feedback, which greatly improves their ability to follow instructions and the perceived utility of their generations <ref type="bibr" coords="2,143.78,317.26,11.36,10.91" target="#b4">[5]</ref>. OpenAI states that GPT-3.5-turbo is additionally optimized for chats, but does not disclose the exact training procedure used <ref type="foot" coords="2,294.05,329.06,3.71,7.97" target="#foot_0">1</ref> .</p><p>GPT-4 is the most recent and best performing model of OpenAI, which is, as of this writing, only programmatically accessible through closed beta API access <ref type="foot" coords="2,388.19,356.15,3.71,7.97" target="#foot_1">2</ref> . It exhibits human-level performance on various professional and academic benchmarks and can process images as well as text <ref type="bibr" coords="2,121.03,385.01,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Few and Zero-Shot Learning</head><p>These models improve over the earlier GPT-3 model which showed that in certain tasks sufficiently big LLMs can compete with fine-tuned transformer models using only few-shot learning, which greatly reduces the need for extensive training data <ref type="bibr" coords="2,351.11,461.83,11.43,10.91" target="#b5">[6]</ref>.</p><p>In the few-shot learning setting, the GPT models are prompted with a text that contains a few examples of the tasks at hand, for example, multiple question-answer pairs, and at the end only the current question for which an answer should be generated by the model. The model then ideally completes this text by writing the correct answer.</p><p>In the zero-shot learning setting, the model is not supplied with any examples but rather only a direct question or abstract task description and is ideally able to generate a useful completion that answers the question or solves the task <ref type="bibr" coords="2,339.55,556.68,11.43,10.91" target="#b6">[7]</ref>.</p><p>Zero-shot and few-shot learning is especially interesting for applications in specialized domains with no or sparse training data available. Prior work in the biomedical domain has shown that language models pre-trained on in-domain data outperform models pre-trained on open domain data <ref type="bibr" coords="2,170.70,610.87,12.43,10.91" target="#b7">[8]</ref>[9] <ref type="bibr" coords="2,195.57,610.87,16.58,10.91" target="#b9">[10]</ref>. In this work, we want to explore whether these new GPT models, that are extensively trained on vast amounts of open domain data, can compete with specialized fine-tuned models that are expected to participate in the challenge.</p><p>Even though these models are proprietary and neither the architecture nor the specific training process is known, several open-source alternatives have been developed such as OPT <ref type="bibr" coords="3,89.29,114.06,16.41,10.91" target="#b10">[11]</ref>, BLOOM <ref type="bibr" coords="3,152.68,114.06,16.42,10.91" target="#b11">[12]</ref>, or Pythia <ref type="bibr" coords="3,220.55,114.06,16.42,10.91" target="#b12">[13]</ref>. Projects based on these and other open source models are constantly improving, and some are already nearly reaching GPT-3.5-turbo level performance <ref type="bibr" coords="3,89.29,141.16,16.14,10.91" target="#b13">[14]</ref>. We therefore believe that studying these commercial models is valuable for establishing a baseline in zero-shot performance for upcoming open-source alternatives. These alternatives could potentially challenge state-of-the-art (SOTA) systems across a wide range of natural language processing (NLP) tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Professional Search</head><p>Professional search is search conducted in a work context <ref type="bibr" coords="3,355.67,231.13,16.41,10.91" target="#b14">[15]</ref>. This is an everyday activity for many professionals that comes with specific requirements which are different from the requirements of generic Web search <ref type="bibr" coords="3,255.69,258.23,16.41,10.91" target="#b15">[16]</ref>. The BioASQ challenge can be framed as a form of professional search in which the searchers are biomedical experts aiming to find answers to domain-specific questions.</p><p>Automatic query expansion plays a key part in many professional search contexts including search by healthcare information professionals, patent agents and recruitment professionals <ref type="bibr" coords="3,89.29,325.97,18.07,10.91" target="#b16">[17]</ref> as well as in conducting systematic reviews <ref type="bibr" coords="3,316.45,325.97,16.41,10.91" target="#b17">[18]</ref>. What is ultimately being submitted to the search system can turn out to be a fairly complex search strategy, a query involving domain-specific information based around Boolean operators. This is one of the motivations for us to explore automatic query expansion in our methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head><p>We describe the experimental setup of the two BioASQ tasks that we participated in, Task 11 B and MedProcNER. For Task 11 B a benchmark dataset with training and test biomedical questions in English along with reference answers was used that has been created based on questions by biomedical experts <ref type="bibr" coords="3,234.61,465.49,16.25,10.91" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task 11 B: Biomedical Semantic QA</head><p>For Phase A, the participating systems receive a list of biomedical questions such as "Which protein is targeted by Herceptin?" and should retrieve a list of up to 10 most relevant articles from the PubMed Annual Baseline Repository for 2023 <ref type="foot" coords="3,331.66,540.15,3.71,7.97" target="#foot_2">3</ref> . Additionally, the systems should also create a list of at most 10 most relevant snippets extracted from the previously retrieved article titles or abstracts. Participating systems are compared based on the Mean Average Precision (MAP) metric.</p><p>In Phase B, the participating systems receive the same questions as in Phase A, along with a set of gold (correct) articles and snippets selected by biomedical experts. They should then generate an ideal paragraph sized (at most 200 words) answer based on these snippets. The questions are also tagged with either Yes/No, Factoid, Summary, or List type indicating the format for an additional exact answer that should be created by these systems.</p><p>• Yes/no questions require the exact answer to be either "yes" or "no".</p><p>• Factoid question require the exact answer to be a list of up to 5 entity names or other short expressions ordered by decreasing confidence. • List questions require the exact answer to be a list of up to 100 entity names or similar short expressions. • Summary questions do not require an additional exact answer, only the ideal answer needs to be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MedProcNER: MEDical PROCedure Named Entity Recognition</head><p>The MedProcNER task <ref type="bibr" coords="4,200.28,222.05,18.02,10.91" target="#b19">[20]</ref> focuses on the detection and mapping of medical procedures in Spanish texts. It consists of three subtasks:</p><p>• In subtask 1, systems have to identify medical procedures from Spanish clinical reports.</p><p>• In subtask 2, systems have to map the medical procedures identified in subtask 1 to SNOMED CT codes <ref type="bibr" coords="4,206.65,289.56,16.25,10.91" target="#b20">[21]</ref>. • In subtask 3, systems have to assign SNOMED CT codes to the full clinical report for later indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model</head><p>We accessed GPT-3.5-turbo and GPT-4 through the OpenAI API <ref type="foot" coords="4,381.42,395.85,3.71,7.97" target="#foot_3">4</ref> . We used a simple system message to set the behavior of the model, which can be seen in Listing 1.</p><p>Listing 1: System Message You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.</p><p>This system message was then followed by task specific zero-shot prompts, including necessary information such as the questions, snippets, or retrieved article titles. More details on these prompts can be found in the subsection corresponding to the particular task. Prompt engineering has developed into a very active field and at this point we should note that there is scope for plenty of future work exploring more systematically the best way of prompting the system for the task at hand.</p><p>We experimented with a subset of the BioASQ training and development data to explore the system's behavior and evaluate the performance of individual modules.</p><p>Additional parameters that were sent in the API request to the models were temperature which controls the randomness of completion; frequency_penalty which discourages repetition of words or phrases; and presence_penalty which has a similar effect. We set temperature to 0 for all requests to have reproducible results over multiple runs.</p><p>As these models are currently non-deterministic, even with temperature set to 0, there is a residual randomness in the generations, which can lead to slightly different results in each run <ref type="foot" coords="5,501.11,98.76,3.71,7.97" target="#foot_4">5</ref> . We also conducted a limited test to roughly estimate the variance of the results by repeating five runs over the same 50 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Task 11 B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Phase A</head><p>Our approach used zero-shot learning for query expansion, query reformulation and reranking directly with the models. For document retrieval, we queried the eUtils API with a maxdate cutoff corresponding to the creation date of the relevant 2023 PubMed snapshot. The Entrez Programming Utilities (eUtils) API is a set of web applications provided by the National Center for Biotechnology Information (NCBI), which offers programmatic access to the various databases and functionalities of the NCBI resources, such as PubMed. We also used the sort by relevance option of PubMed and retrieved only the top 50 results for a given query.</p><p>We acknowledge that querying the live PubMed database with the corresponding date cutoff is not the same as searching through the downloaded static snapshot or using the search interface provided by the BioASQ organizers. Articles could be deleted or modified in PubMed, which could affect the reproducibility and comparability of the results with other systems. To estimate the impact of this approach, we looked up all articles that were included in the gold set provided in Phase B of the task after the challenge concluded and found that one out of the 899 referenced articles was no longer retrievable in PubMed <ref type="foot" coords="5,288.48,372.29,3.71,7.97" target="#foot_5">6</ref> .</p><p>We were most interested in the impact of the query expansion step and therefore conducted one run with and one without query expansion for both models, where we instead sent the question directly as a query to PubMed.</p><p>The exact steps were:</p><p>1. Query expansion 2. Search on PubMed 3. Query refinement only if no documents were found and one additional search on PubMed 4. Reranking of top 50 articles based on title string All of these steps were executed automatically in Python without manual intervention, the exact code used is available on GitHub 7 . The zero-shot learning prompt used for query expansion can be seen in Listing 2. Where the placeholder {question} was replaced by the question that was currently processed by the system. For query expansion, we set frequency_penalty to 0.5 and presence_penalty to 0.1. Some example query expansions for this prompt can be seen in Listing 3. Interestingly, these models seem to not only know what Boolean syntax is accepted by PubMed but also important Listing 2: Query Expansion Prompt {"role": "user", "content": f"""Expand this search query: '{question}' for PubMed by incorporating synonyms and additional terms that closely relate to the main topic and help reduce ambiguity. Assume that phrases are not stemmed; therefore, generate useful variations. Return only the query that can directly be used without any explanation text. Focus on maintaining the query's precision and relevance to the original question."""} internal fields such as MeSH Terms and the syntax on how to query on these fields, but these were not often used in the expanded queries. <ref type="foot" coords="6,288.72,235.17,3.71,7.97" target="#foot_7">8</ref>Listing 3: Query Expansion Example Question: What are the outcomes of ubiquitination? Expanded Query: ("ubiquitination" OR "ubiquitin modification" OR "ubiquitin conjugation" OR "ubiquitin pathway") AND ("outcomes" OR "effects" OR "consequences") Question: What is the incidence of Leigh syndrome? Expanded Query: ("Leigh syndrome"[MeSH Terms] OR "Leigh syndrome"[All Fields] OR " subacute necrotizing encephalomyelopathy"[All Fields]) AND ("incidence"[MeSH Terms] OR "incidence"[All Fields] OR "prevalence"[MeSH Terms] OR "prevalence"[All Fields])</p><p>For the optional query reformulation step, we used the prompt in Listing 4. This step was introduced after it became clear that some queries constructed by the models were overly specific and returned no results. The placeholder {question} in the prompt was replaced by the question that was currently processed by the system, and the placeholder {original_query} was replaced by the original expanded query that returned no results. For query reformulation, we set frequency_penalty to 0.6 and presence_penalty to 0.2. An example of a query reformulation that generated a slightly broader query that then led to some results can be seen in Listing 5. Additionally, terms added to the query are highlighted in gray.</p><p>Listing 4: Query Reformulation Prompt {"role": "user", "content": f"""Given that the following search query for PubMed has returned no documents, please generate a broader query that retains the original question's context and relevance. Assume that phrases are not stemmed; therefore, generate useful variations. Return only the query that can directly be used without any explanation text. Focus on maintaining the query's precision and relevance to the original question. Original question: '{ question}', Original query: '{original_query}'. """} Listing 5: Query Reformulation Example Question: Can skin picking phenotype present following methylphenidate treatment? Query: ("skin picking" OR "excoriation disorder" OR "dermatillomania" OR "compulsive skin picking") AND (phenotype OR presentation OR manifestation) AND ("methylphenidate treatment" OR "methylphenidate therapy" OR "methylphenidate administration") Reformulated Query: ("skin picking" OR "excoriation disorder" OR "dermatillomania" OR " compulsive skin picking") AND (phenotype OR presentation OR manifestation OR symptoms ) AND ("methylphenidate treatment" OR "methylphenidate therapy" OR " methylphenidate administration" OR "methylphenidate use" )</p><p>For the final reranking step, we took the titles of the top 50 returned articles as returned by the relevancy sort from PubMed and prompted the model to rerank these articles given the original question and return the top 10 articles. The prompt used for the reranking can be seen in Listing 6, where {articles_str} is replaced by the list of returned article titles, {question} is replaced by the question that is currently processed by the system, and {nr_of_articles} is replaced by the 10 or fewer articles if less relevant articles were returned by PubMed. For reranking, we set frequency_penalty to 0.3 and presence_penalty to 0.1.</p><p>Listing 6: Reranking Prompt {"role": "user", "content": f"{articles_str} \n\n Given these articles and the question: '{question }'. Rerank the articles based on their relevance to the question and return the top { nr_of_articles} most relevant articles as a comma separated list of their index ids. Don't explain your answer, return only this list, for example: '1, 2, 3, 4' "} The returned list was then mapped back to the articles retrieved from PubMed, and these were returned as the required output of Phase A.</p><p>We also explored the extraction of snippets for the Phase A task but abandoned it, as it required sending all abstracts of the 10 returned papers for processing to the model, which was especially expensive for the GPT-4 model because API usage is priced on token counts, and we were exploring these models on a limited budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Phase B</head><p>In Phase B, we used the gold (correct) snippets from the test set and sent them along with the question and description of the answer format to the model.</p><p>We also conducted a test where this grounding information in the form of relevant snippets was omitted and just the question and description of the answer format were sent to the models.</p><p>The prompts utilized for generating these answer types are listed as follows: for ideal answers, refer to Listing 7; for Yes/No answers, see Listing 8; for List answers, Listing 9; and for Factoid responses, see Listing 10.</p><p>Listing 7: Ideal Answer Prompt {"role": "user", "content": f""" {snippets}\n\n\ '{question['body']}'. Answer this question by returning a single paragraph-sized text ideally summarizing the most relevant information. The maximum allowed length of the answer is 200 words. The returned answer is intended to approximate a short text that a biomedical expert would write to answer the corresponding question (e.g., including prominent supportive information). """} Listing 8: Yes/No Answer Prompt {"role": "user", "content": f" {snippets}\n\n\ '{question['body']}'. You * must answer * only with lowercase 'yes' or 'no' even if you are not sure about the answer. "} Listing 9: Factoid Answer Prompt {"role": "user", "content": f" {snippets}\n\n\ '{question['body']}'. Answer this question by returning only a JSON string array of entity names, numbers, or similar short expressions that are an answer to the question, ordered by decreasing confidence. The array should contain at max 5 elements but can contain less. If you don't know any answer return an empty list. Return only this list, it must not contain phrases and * * must be valid JSON * * . "} Listing 10: List Answer Prompt {"role": "user", "content": f" {snippets}\n\n\ '{question['body']}'. Answer this question by only returning a JSON string array of entity names, numbers, or similar short expressions that are an answer to the question (e.g., the most common symptoms of a disease). The returned list will have to contain no more than 100 entries of no more than 100 characters each. If you don't know any answer return an empty list. Return only this list, it must not contain phrases and * * must be valid JSON * * . "} In all these prompts, {question['body']} is replaced by the question that is currently processed by the system, and {snippets} is replaced by the snippets provided by the test set.</p><p>For all answer types, we set frequency_penalty to 0.5. Presence_penalty was set to 0.3 for Yes/No answers, to 0.1 for both List and Factoid answers, and to 0.7 for the ideal answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MedProcNER</head><p>For the MedProcNER task, we translated all prompt templates, including the system prompt, to Spanish using and comparing deepL <ref type="foot" coords="8,246.23,629.56,3.71,7.97" target="#foot_8">9</ref> ) and ChatGPT <ref type="foot" coords="8,314.94,629.56,7.41,7.97" target="#foot_9">10</ref> . For substask 1, instead of using zero-shot Listing 11: MedProcNER Prompt conversation = [{'role': 'system', 'content': """Eres un asistente útil que extrae procedimientos médicos de textos médicos en español. Un procedimiento médico se refiere a cualquier acció n diagnóstica, terapéutica, médica o quirúrgica realizada en un paciente. Tu respuesta debe ser una lista de procedimientos en formato JSON válido. """}] for input, output in examples: conversation.append({'role': 'user', 'content': f'{input}'}) conversation.append({'role': 'assistant', 'content': json.dumps(output)}) conversation.append({'role': 'user', 'content': f"""Extraiga todos los procedimientos médicos del texto delimitado por tres comillas invertidas. Devuelve una lista vacía si no se menciona ninguno. {text}"""}) prompting as before, we instead explored the few-shot prompting approach, where we included three examples from the training set into the request sent to the OpenAI API. We also compared the performance of GPT-3.5-turbo and GPT-4.</p><p>The relevant Python code part that constructed the prompt can be seen in Listing 11. The examples list mentioned therein contained three examples taken from the training set.</p><p>For substask 2, we used the gazetteer file provided by the MedProcNER task organizers. We filtered the file for all SNOMED CT codes that were tagged as procedure and stemmed their terms, and used Levenshtein distance based fuzzy matching to find an entry for a procedure. The detailed code used for all tasks is available on the aforementioned GitHub repository.</p><p>For subtask 3, we just joined all SNOMED CT codes identified in subtask 2 for one document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The systems participating in the Biomedical Semantic Q&amp;A task were evaluated in four batches.</p><p>Results are reported for every batch. For readability, we only included the results of our systems and the top performing systems. The full result tables are publicly available on the BioASQ website 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Task 11 B Phase A</head><p>We participated with 4 systems in Task 11 B Phase A, the systems' names and their properties are listed as follows:</p><p>• UR-gpt4-zero-ret corresponds to GPT-4 with query expansion.</p><p>• UR-gpt3.5-turbo-zero corresponds to GPT-3.5-turbo with query expansion.</p><p>• UR-gpt4-simple corresponds to GTP-4 without query expansion.</p><p>• UR-gpt3.5-t-simple corresponds to GPT-3.5-turbo without query expansion.</p><p>11 http://participants-area.bioasq.org/results/11b/phaseA/</p><p>The following Table <ref type="table" coords="10,192.38,86.97,5.08,10.91" target="#tab_0">1</ref> shows the results of our systems participating in the 4 batches. MAP was the official metric to compare the systems. N stands for the number of participating systems in each batch. One observation is that GPT-4 achieved better results than GPT-3.5-turbo in all batches except batch 3. It seems to perform better in both query expansion and reranking without query expansion. Query expansion consistently improves the results for all models in all batches. It greatly improves recall in all batches, and in most batches, precision is also slightly increased except in batch 1, where it leads to decreased precision for GPT-3.5-turbo but an overall improved F1 score.</p><p>In general, our approach performs worse than most systems. This could be due to the fact that we do not do any embedding based neural retrieval, but instead only rely on the keywords created by the models in the query expansion step and the relevancy ranking provided by PubMed. The reranking window of only 50 article titles might also be too small, or the information provided by the titles is not sufficient for a more effective reranking. A thorough ablation study in future work could help explain the contribution of these individual factors to the overall system performance.</p><p>Using only query expansion in the retrieval phase and not having to do any embedding calculations during indexing does come with advantages for applying such an approach to existing or huge search use-cases where efficient reindexing with more advanced embedding based approaches might not be feasible. On the other hand, the used models do take several seconds to create results for both reranking and query expansion, which could limit their usefulness in classical enterprise-search use-cases if sub-second response times are expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Task 11 B Phase A</head><p>We participated with 4 systems in Task 11 B Phase B, the systems' names and their properties are listed as follows:</p><p>• UR-gpt4-zero-ret corresponds to GPT-4 grounded with snippets.</p><p>• UR-gpt3.5-turbo-zero corresponds to GPT-3.5-turbo grounded with snippets.</p><p>• UR-gpt4-simple corresponds to GTP-4 answering directly without reading snippets.</p><p>• UR-gpt3.5-t-simple corresponds to GPT-3.5-turbo answering directly without reading snippets.</p><p>We were not able to complete all runs in batches 1 and 2, which is why some results are missing. We report the results for each answer format (Yes/No, Factoid, List) separately in the following tables. For readability, we again only included the results of our systems and the top-performing systems, the full result tables are publicly available on the BioASQ website <ref type="foot" coords="11,492.20,263.27,7.41,7.97" target="#foot_10">12</ref> . In the Yes/No question format, our results indicate that GPT-4 surpasses GPT-3.5-turbo in both the grounded and ungrounded settings. For batches 1 and 3, the ungrounded GPT-4 system UR-gpt4-simple even showed a tendency to perform better than the grounded variant of GPT-3.5-turbo UR-gpt3.5-turbo-zero as can be seen in Table <ref type="table" coords="11,353.66,574.02,3.74,10.91" target="#tab_1">2</ref>.</p><p>In the Factoid question format, both grounded GPT-4 and grounded GPT-3.5-turbo achieved an MRR score of 0.5789 taking first and second place over all other systems. In the remaining batches, GPT-3.5-turbo stayed consistently in the top 6 systems, while GPT-4 only reached 11th and 13th place in batches 3 and 4. This mixed performance comparison between GPT-3.5-turbo and GPT-4 was also observed in the List question format, where GPT-3.5-turbo achieved 1st place in batch 2 but was behind GPT-4 in batches 3 and 4. The results for the Factoid question format are shown in Table <ref type="table" coords="12,209.32,605.13,5.07,10.91" target="#tab_2">3</ref> and the results for the List question format are shown in Table <ref type="table" coords="12,498.11,605.13,3.74,10.91" target="#tab_3">4</ref>. While GPT-4 seems to perform consistently better than GPT-3.5-turbo in the Yes/No question format, there is no clear winner in the more extractive Factoid and List formats.</p><p>Both models without grounding information from snippets were not able to compete with the top models but were often placed slightly below the average performing systems, which is still surprisingly good as in this setting the models need to rely only on the open-domain knowledge acquired during training for answering these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Task MedProcNER</head><p>In the MedProcNER task, GPT-4 performed better than GPT-3.5-turbo, but was not able to compete with the best performing system. The results are shown in Table <ref type="table" coords="13,440.59,163.46,3.81,10.91" target="#tab_4">5</ref>. Our simple gazetteer based entity linking and indexing approach performed poorly compared to the topperforming system. At the time of this writing, the performance of other systems involved in the task has not been published yet. Even though the few-shot NER approach did not compete with the top-performing system in the MedProcNER task, it still indicates that GPT-4 can be used for specialized domains in multilingual tasks while only using a minimal amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion and Future Work</head><p>The results from our participation in the BioASQ challenge indicate that current commercial GPT models GPT-3.5-turbo and GPT-4 can compete with other presumably fine-tuned leading systems in question answering in the biomedical domain, while only being zero-shot prompted with relevant snippets. Even without relevant snippets, just relying on the biomedical knowledge aquired during their pre-training, these models were performing better than some of the other systems participating in the task.</p><p>One big challenge in using zero-shot learning with these GPT models is prompt-engineering. It still seems to be more of an art than a science and requires considerable testing <ref type="bibr" coords="13,451.04,493.44,16.17,10.91" target="#b23">[24]</ref>. During system development, it became clear that the expanded queries in Task 11 B Phase A were sometimes too specific and did not return results. We tried to prompt the models to create broader queries that were not using as many phrase terms that are not stemmed in PubMed, but the overall system performance on our development set declined. We therefore experimented with using GPT-4 to come up with a better prompt by supplying it with the original prompt and the 5 worst-performing and 5 best-performing queries. The new prompt actually increased the performance of the system. This self prompt learning might be an interesting approach to investigate further in future work.</p><p>Nevertheless, the zero-shot learning approach makes the usage of these models very accessible, as it does not require thorough data preparation, knowledge about classical deep learning techniques, or advanced programming skills.</p><p>A prominent problem in these GPT models are so-called hallucinations <ref type="bibr" coords="13,434.54,656.03,16.42,10.91" target="#b24">[25]</ref>. These are unsupported or factually wrong statements in the responses. These problems might be espe-cially observable in the ideal answer setting. In future work, we want to conduct a thorough investigation of the factuality of the ideal answers and especially compare the grounded and ungrounded settings. This could provide error rate estimates that might be useful for generative search systems in specialized domains.</p><p>As noted earlier, these commercial models are not completely deterministic, even when the temperature parameter is set to 0. OpenAI states in their documentation: "OpenAI models are non-deterministic, meaning that identical inputs can yield different outputs. Setting the temperature parameter to 0 will make the outputs mostly deterministic, but a small amount of variability may remain. " <ref type="foot" coords="14,419.65,202.57,7.41,7.97" target="#foot_11">13</ref>We had concerns about the potential cascading effect of such residual non-determinism, especially in the context of query expansion. To estimate this variability, we performed a limited test by repeating the retrieval task from Task 11 B Phase A over 50 questions taken from the training set five times with the same model. Our test results showed minimal variance across metrics such as MAP, precision, recall, and F-measure, indicating that while variability exists, its impact is currently minimal, with broader investigations pending for future work.</p><p>This residual non-determinism in the model output also led to some instability in the system when we fully relied on the model returning the right output format for further processing. For example, in the Yes/No question format, the evaluation system of the BioASQ organizers expects the answers to be all lowercase, either "yes" or "no". The models often returned variants such as "Yes" or "Yes. " even if explicitly prompted not to do so. This necessitated an additional normalization post-processing step.</p><p>In the MedProcNER task, where we used few-shot learning, it seemed that the examples greatly assisted the model in returning the correct output format. We suspect that giving even just a few examples is a more effective way to guide the models towards the expected output format than explicitly describing the format in a zero-shot learning prompt.</p><p>Even if the models were outputting the right format, the overall system was still unstable due to the instability of the OpenAI API. In every run, there were at least 2-3 requests that failed due to internal server errors or the model being overloaded with requests. Thus, retry loops must be incorporated when accessing such external services.</p><p>As usage of these models is priced based on token count, some use-cases might not be financially feasible yet. Only running one evaluation batch with GPT-4 can cost around $10 in model usage. At the same time, the GPT-4 model was still much slower in answering requests than GPT-3.5-turbo. These two factors led us to not participate in the snippet generation task, as this task is especially demanding regarding both the amount of tokens to be processed in the prompt and generated as a response. In general, the economic barrier to using these commercial models may hinder some researchers due to the cost of usage. Also, over-reliance on these models might stifle innovation in other research areas.</p><p>We also conducted a limited test with grounding the query expansion by suggesting semantically related terms from the word embeddings supplied by the BioASQ organizers, but these terms led to queries that performed worse than just ungrounded ones. We did not investigate this approach thoroughly and leave it open for future work. Some of our results might indicate that the performance gap between presumably smaller (GPT-3.5-turbo) and more complex models (GPT-4) is narrower in the grounded extractive Q&amp;A setting, because GPT-3.5-turbo sometimes performed better than GPT-4 in answering Factoid or List questions in some of the batches. It would be interesting to see how model performance in this setting scales with model size, and to test whether the use of much smaller generative models is feasible. Some related work in other use-cases already showed promising results in this direction <ref type="bibr" coords="15,149.12,168.26,33.66,10.91">[26][27]</ref>. This might open up new possibilities for using these models in enterprise search settings where confidential data must remain on-premise <ref type="bibr" coords="15,377.53,181.81,16.25,10.91" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ethical Considerations</head><p>The use of large language models like GPT-3.5-Turbo and GPT-4 in biomedical tasks presents several ethical considerations.</p><p>First, we must address data privacy. While these models do not retain specific training examples, there is a remote possibility of them generating outputs resembling sensitive data, or sensitive data included in a prompt might be repeated and further processed in downstream tasks. This issue has to be addressed when employing these models in a real world biomedical context.</p><p>Second, as these models may produce factually incorrect outputs or "hallucinations" <ref type="bibr" coords="15,486.67,335.28,16.41,10.91" target="#b24">[25]</ref>, rigorous fact-checking mechanisms must be applied, especially when used in a biomedical context to prevent the spread of harmful misinformation.</p><p>Lastly, large language models operate as black-box algorithms, raising issues of interpretability, transparency, and accountability <ref type="bibr" coords="15,253.70,389.48,16.25,10.91" target="#b28">[29]</ref>.</p><p>In conclusion, the potential of large language models in biomedical tasks is significant, but the ethical implications of their deployment need careful attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We showed that in context learning, both zero-and few-shot, with recent LLMs trained on human feedback can compete with presumably fine-tuned state-of-the-art systems in some domain-specific questions answering tasks. Zero-and few-shot learning can greatly simplify and speed up the development of complex NLP or IR systems, which might be especially useful for research and prototyping. It also opens up the possibility to improve use-cases where fine-tuning is not feasible due to a lack of available training data.</p><p>Prompt engineering for these models poses challenges, and grounding the answer generation with the right context information is an interesting problem for current and future generative search systems research. Even though the currently offered GPT models have severe limitations regarding cost of usage, speed, and factuality, we see promising research towards making these types of models more affordable and accessible and improving their overall performance and factuality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,140.62,392.07,233.96"><head>Table 1</head><label>1</label><figDesc>Task 11 B Phase A, Batches 1-4</figDesc><table coords="10,114.21,167.71,366.85,206.88"><row><cell>Batch</cell><cell>Position</cell><cell>System</cell><cell cols="3">Precision Recall F-Measure</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.2118</cell><cell>0.6047</cell><cell>0.2774</cell><cell>0.4590</cell><cell>0.0267</cell></row><row><cell></cell><cell>19</cell><cell>UR-gpt4-zero-ret</cell><cell>0.1664</cell><cell>0.3352</cell><cell>0.1955</cell><cell>0.2657</cell><cell>0.0009</cell></row><row><cell>Batch 1</cell><cell>21</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.1488</cell><cell>0.2847</cell><cell>0.1782</cell><cell>0.2145</cell><cell>0.0009</cell></row><row><cell>N = 33</cell><cell>24</cell><cell>UR-gpt4-simple</cell><cell>0.1654</cell><cell>0.2508</cell><cell>0.1799</cell><cell>0.1809</cell><cell>0.0005</cell></row><row><cell></cell><cell>25</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.1600</cell><cell>0.2290</cell><cell>0.1734</cell><cell>0.1769</cell><cell>0.0003</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.1027</cell><cell>0.5149</cell><cell>0.1618</cell><cell>0.3852</cell><cell>0.0104</cell></row><row><cell>Batch 2</cell><cell>20</cell><cell>UR-gpt4-simple</cell><cell>0.0945</cell><cell>0.3011</cell><cell>0.1277</cell><cell>0.1905</cell><cell>0.0011</cell></row><row><cell>N = 33</cell><cell>21</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.1153</cell><cell>0.2977</cell><cell>0.1455</cell><cell>0.1736</cell><cell>0.0008</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.0800</cell><cell>0.4776</cell><cell>0.1320</cell><cell>0.3185</cell><cell>0.0049</cell></row><row><cell></cell><cell>21</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.1295</cell><cell>0.3258</cell><cell>0.1646</cell><cell>0.2048</cell><cell>0.0008</cell></row><row><cell>Batch 3</cell><cell>22</cell><cell>UR-gpt4-zero-ret</cell><cell>0.1086</cell><cell>0.2289</cell><cell>0.1303</cell><cell>0.1930</cell><cell>0.0003</cell></row><row><cell>N = 35</cell><cell>23</cell><cell>UR-gpt4-simple</cell><cell>0.1089</cell><cell>0.2102</cell><cell>0.1238</cell><cell>0.1727</cell><cell>0.0002</cell></row><row><cell></cell><cell>24</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.1078</cell><cell>0.1981</cell><cell>0.1217</cell><cell>0.1553</cell><cell>0.0002</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.0933</cell><cell>0.4292</cell><cell>0.1425</cell><cell>0.3224</cell><cell>0.0030</cell></row><row><cell></cell><cell>18</cell><cell>UR-gpt4-zero-ret</cell><cell>0.0791</cell><cell>0.1728</cell><cell>0.0933</cell><cell>0.1251</cell><cell>0.0002</cell></row><row><cell>Batch 4</cell><cell>19</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.0922</cell><cell>0.1956</cell><cell>0.1025</cell><cell>0.1139</cell><cell>0.0002</cell></row><row><cell>N = 27</cell><cell>20</cell><cell>UR-gpt4-simple</cell><cell>0.0785</cell><cell>0.1563</cell><cell>0.0864</cell><cell>0.1010</cell><cell>0.0002</cell></row><row><cell></cell><cell>21</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.0752</cell><cell>0.1319</cell><cell>0.0810</cell><cell>0.0912</cell><cell>0.0001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,88.99,294.03,371.74,223.01"><head>Table 2</head><label>2</label><figDesc>Task 11 B Phase B, Yes/No Questions Batches 1-4</figDesc><table coords="11,134.55,321.12,326.18,195.92"><row><cell>Batch</cell><cell>Position</cell><cell>System</cell><cell cols="3">Accuracy F1 Yes F1 No Macro F1</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.9583</cell><cell>0.9697 0.9333</cell><cell>0.9515</cell></row><row><cell>Batch1</cell><cell>8</cell><cell>UR-gpt4-zero-ret</cell><cell>0.9167</cell><cell>0.9412 0.8571</cell><cell>0.8992</cell></row><row><cell>N = 33</cell><cell>9</cell><cell>UR-gpt4-simple</cell><cell>0.9167</cell><cell>0.9412 0.8571</cell><cell>0.8992</cell></row><row><cell></cell><cell>13</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.8750</cell><cell>0.9091 0.8000</cell><cell>0.8545</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>1.0000</cell><cell>1.0000 1.0000</cell><cell>1.0000</cell></row><row><cell>Batch2</cell><cell>7</cell><cell>UR-gpt4-zero-ret</cell><cell>0.9583</cell><cell>0.9655 0.9474</cell><cell>0.9564</cell></row><row><cell>N = 42</cell><cell>12</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.9167</cell><cell>0.9333 0.8889</cell><cell>0.9111</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>1.0000</cell><cell>1.0000 1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>9</cell><cell>UR-gpt4-zero-ret</cell><cell>0.9167</cell><cell>0.9375 0.8750</cell><cell>0.9063</cell></row><row><cell>Batch3</cell><cell>12</cell><cell>UR-gpt4-simple</cell><cell>0.8750</cell><cell>0.9032 0.8235</cell><cell>0.8634</cell></row><row><cell>N = 47</cell><cell>14</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.8750</cell><cell>0.9091 0.8000</cell><cell>0.8545</cell></row><row><cell></cell><cell>21</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.7917</cell><cell>0.8485 0.6667</cell><cell>0.7576</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>1.0000</cell><cell>1.0000 1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>7</cell><cell>UR-gpt4-zero-ret</cell><cell>0.9286</cell><cell>0.8889 0.9474</cell><cell>0.9181</cell></row><row><cell>Batch4</cell><cell>14</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.9286</cell><cell>0.8571 0.9524</cell><cell>0.9048</cell></row><row><cell>N = 52</cell><cell>19</cell><cell>UR-gpt4-simple</cell><cell>0.7857</cell><cell>0.7273 0.8235</cell><cell>0.7754</cell></row><row><cell></cell><cell>29</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.4286</cell><cell>0.5000 0.3333</cell><cell>0.4167</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,88.99,90.49,359.51,233.96"><head>Table 3</head><label>3</label><figDesc>Task 11 B Phase B, Factoid Questions Batches 1-4</figDesc><table coords="12,146.77,117.57,301.74,206.88"><row><cell>Batch</cell><cell>Position</cell><cell>System</cell><cell cols="2">Strict Acc. Lenient Acc.</cell><cell>MRR</cell></row><row><cell></cell><cell>1</cell><cell>UR-gpt4-zero-ret</cell><cell>0.5789</cell><cell>0.5789</cell><cell>0.5789</cell></row><row><cell>Batch1</cell><cell>2</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.5263</cell><cell>0.6316</cell><cell>0.5789</cell></row><row><cell>N = 33</cell><cell>3</cell><cell>Next Competitor</cell><cell>0.5263</cell><cell>0.6316</cell><cell>0.5570</cell></row><row><cell></cell><cell>22</cell><cell>UR-gpt4-simple</cell><cell>0.2105</cell><cell>0.2632</cell><cell>0.2368</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.5455</cell><cell>0.6364</cell><cell>0.5909</cell></row><row><cell>Batch2</cell><cell>2</cell><cell>Next Competitor</cell><cell>0.5455</cell><cell>0.6364</cell><cell>0.5909</cell></row><row><cell>N = 42</cell><cell>3</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.5455</cell><cell>0.5909</cell><cell>0.5682</cell></row><row><cell></cell><cell>4</cell><cell>UR-gpt4-zero-ret</cell><cell>0.5455</cell><cell>0.5909</cell><cell>0.5682</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.4615</cell><cell>0.6538</cell><cell>0.5205</cell></row><row><cell></cell><cell>5</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.5000</cell><cell>0.5000</cell><cell>0.5000</cell></row><row><cell>Batch3</cell><cell>11</cell><cell>UR-gpt4-zero-ret</cell><cell>0.4615</cell><cell>0.5000</cell><cell>0.4808</cell></row><row><cell>N = 47</cell><cell>22</cell><cell>UR-gpt4-simple</cell><cell>0.2692</cell><cell>0.4615</cell><cell>0.3654</cell></row><row><cell></cell><cell>27</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.3077</cell><cell>0.3077</cell><cell>0.3077</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.6452</cell><cell>0.8710</cell><cell>0.7323</cell></row><row><cell>Batch4</cell><cell>6</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.6452</cell><cell>0.6452</cell><cell>0.6452</cell></row><row><cell>N = 52</cell><cell>13</cell><cell>UR-gpt4-zero-ret</cell><cell>0.5161</cell><cell>0.6129</cell><cell>0.5645</cell></row><row><cell></cell><cell>30</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.2581</cell><cell>0.2903</cell><cell>0.2742</cell></row><row><cell></cell><cell>33</cell><cell>UR-gpt4-simple</cell><cell>0.2258</cell><cell>0.2581</cell><cell>0.2366</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.99,344.29,359.51,223.01"><head>Table 4</head><label>4</label><figDesc>Task 11 B Phase B, List Questions Batches 1-4</figDesc><table coords="12,146.77,371.37,301.74,195.92"><row><cell>Batch</cell><cell>Position</cell><cell>System</cell><cell cols="2">Strict Acc. Lenient Acc.</cell><cell>MRR</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.7861</cell><cell>0.6668</cell><cell>0.7027</cell></row><row><cell>Batch1</cell><cell>2</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.6742</cell><cell>0.7249</cell><cell>0.6917</cell></row><row><cell>N = 33</cell><cell>8</cell><cell>UR-gpt4-zero-ret</cell><cell>0.6472</cell><cell>0.6530</cell><cell>0.6495</cell></row><row><cell></cell><cell>19</cell><cell>UR-gpt4-simple</cell><cell>0.4000</cell><cell>0.4014</cell><cell>0.3939</cell></row><row><cell></cell><cell>1</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.4598</cell><cell>0.4671</cell><cell>0.4316</cell></row><row><cell>Batch2</cell><cell>2</cell><cell>Next Competitor</cell><cell>0.5099</cell><cell>0.3577</cell><cell>0.3980</cell></row><row><cell>N = 42</cell><cell>4</cell><cell>UR-gpt4-zero-ret</cell><cell>0.3742</cell><cell>0.4369</cell><cell>0.3828</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.6519</cell><cell>0.6058</cell><cell>0.6049</cell></row><row><cell></cell><cell>3</cell><cell>UR-gpt4-zero-ret</cell><cell>0.5518</cell><cell>0.6597</cell><cell>0.5736</cell></row><row><cell>Batch3</cell><cell>9</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.5600</cell><cell>0.5140</cell><cell>0.5101</cell></row><row><cell>N = 47</cell><cell>24</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.2690</cell><cell>0.2385</cell><cell>0.2333</cell></row><row><cell></cell><cell>25</cell><cell>UR-gpt4-simple</cell><cell>0.2519</cell><cell>0.2343</cell><cell>0.2305</cell></row><row><cell></cell><cell>1</cell><cell>Top Competitor</cell><cell>0.7139</cell><cell>0.8061</cell><cell>0.7440</cell></row><row><cell>Batch4</cell><cell>2</cell><cell>UR-gpt4-zero-ret</cell><cell>0.6902</cell><cell>0.7818</cell><cell>0.7191</cell></row><row><cell>N = 52</cell><cell>10</cell><cell>UR-gpt3.5-turbo-zero</cell><cell>0.6090</cell><cell>0.6710</cell><cell>0.6196</cell></row><row><cell></cell><cell>21</cell><cell>UR-gpt4-simple</cell><cell>0.4440</cell><cell>0.4214</cell><cell>0.4127</cell></row><row><cell></cell><cell>26</cell><cell>UR-gpt3.5-t-simple</cell><cell>0.3944</cell><cell>0.3362</cell><cell>0.3470</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,88.99,232.40,366.57,74.02"><head>Table 5</head><label>5</label><figDesc>Comparison of F1 scores of different systems for NER, Entity Linking, and Indexing tasks.</figDesc><table coords="13,142.91,260.44,309.45,45.98"><row><cell>Task</cell><cell cols="3">Top Performing System F1 GPT-3.5-turbo F1 GPT-4 F1</cell></row><row><cell>NER</cell><cell>0.7985</cell><cell>0.3002</cell><cell>0.4814</cell></row><row><cell>EL</cell><cell>0.5707</cell><cell>0.1264</cell><cell>0.1976</cell></row><row><cell>Indexing</cell><cell>0.6242</cell><cell>0.1785</cell><cell>0.2695</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,92.57,660.08,228.59,8.97"><p>https://platform.openai.com/docs/model-index-for-researchers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,92.57,671.04,122.02,8.97"><p>https://openai.com/product/gpt-4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,92.57,671.04,190.96,8.97"><p>https://lhncbc.nlm.nih.gov/ii/information/MBR.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,92.57,671.01,215.33,8.97"><p>https://platform.openai.com/docs/guides/chat/introduction</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,92.57,649.05,179.82,8.97"><p>https://platform.openai.com/docs/models/gpt-3-5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,92.26,660.01,395.76,8.97"><p>Article from batch 4 that is no longer accessible in PubMed: http://www.ncbi.nlm.nih.gov/pubmed/36459075</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,92.57,670.97,136.11,8.97"><p>https://github.com/SamyAteia/bioasq</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,92.30,660.07,413.69,8.97;6,92.57,671.03,82.98,8.97"><p>The identification of suitable MeSH terms in structured queries for systematic reviews has been explored in detail elsewhere, e.g.<ref type="bibr" coords="6,148.28,671.03,13.50,8.97" target="#b21">[22,</ref><ref type="bibr" coords="6,164.02,671.03,11.53,8.97" target="#b22">23]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="8,92.57,660.06,135.84,8.97"><p>https://www.deepl.com/en/translator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="8,95.35,671.02,89.05,8.97"><p>https://chat.openai.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="11,95.35,671.02,199.79,8.97"><p>http://participants-area.bioasq.org/results/11b/phaseB/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="14,95.35,671.03,179.82,8.97"><p>https://platform.openai.com/docs/models/gpt-3-5</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank the organizers of the BioASQ challenge for setting up this challenge and supporting us during our participation. We are also grateful for the feedback and recommendations of the anonymous reviewers.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,197.00,274.23,10.91" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
</biblStruct>

<biblStruct coords="16,112.66,210.55,394.53,10.91;16,112.66,224.10,393.33,10.91;16,112.66,237.65,393.33,10.91;16,112.66,251.20,393.33,10.91;16,112.66,264.75,306.11,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,234.48,224.10,271.51,10.91;16,112.66,237.65,307.20,10.91">Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,445.02,237.65,60.97,10.91;16,112.66,251.20,393.33,10.91;16,112.66,264.75,252.02,10.91">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="16,112.66,278.30,393.32,10.91;16,112.66,291.85,393.33,10.91;16,112.66,305.40,107.76,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,349.27,278.30,156.72,10.91;16,112.66,291.85,104.82,10.91">Overview of BioASQ Tasks 11b and Synergy11 in CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,241.61,291.85,264.38,10.91;16,112.66,305.40,77.06,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,318.95,395.17,10.91;16,112.66,332.50,393.33,10.91;16,112.66,346.05,394.53,10.91;16,112.66,359.59,129.08,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,149.68,332.50,112.68,10.91">Attention is All You Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,286.16,332.50,219.82,10.91;16,112.66,346.05,232.30,10.91">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,373.14,394.53,10.91;16,112.66,386.69,393.33,10.91;16,112.66,400.24,383.49,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,225.45,386.69,280.54,10.91;16,112.66,400.24,37.18,10.91">Training language models to follow instructions with human feedback</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,158.41,400.24,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,413.79,393.33,10.91;16,112.66,427.34,394.52,10.91;16,112.66,440.89,116.06,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,148.95,413.79,175.52,10.91">Language Models Are Few-Shot Learners</title>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,345.92,413.79,160.06,10.91;16,112.66,427.34,283.77,10.91">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,454.44,393.71,10.91;16,112.66,467.99,395.00,10.91;16,112.66,481.54,346.85,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,356.46,454.44,149.91,10.91;16,112.66,467.99,322.93,10.91">Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
		<ptr target="https://doi.org/10.1145/3560815.doi:10.1145/3560815" />
	</analytic>
	<monogr>
		<title level="j" coord="16,444.00,467.99,63.67,10.91;16,112.66,481.54,22.48,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,495.09,393.33,10.91;16,112.66,508.64,394.53,10.91;16,112.41,522.18,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,355.95,495.09,150.04,10.91;16,112.66,508.64,251.54,10.91">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,372.87,508.64,64.30,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,535.73,394.61,10.91;16,112.66,549.28,393.33,10.91;16,112.66,562.83,395.17,10.91;16,112.66,576.38,132.19,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,234.41,535.73,253.35,10.91">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,549.28,393.33,10.91;16,112.66,562.83,395.17,10.91;16,112.66,576.38,33.90,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,589.93,394.52,10.91;16,112.66,603.48,394.53,10.91;16,112.28,617.03,337.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,112.66,603.48,390.05,10.91">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,112.28,617.03,268.63,10.91">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,630.58,394.53,10.91;16,112.30,644.13,394.88,10.91;17,112.33,86.97,395.33,10.91;17,112.66,102.96,97.35,7.90" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m" coord="17,229.06,86.97,246.98,10.91">OPT: Open Pre-trained Transformer Language Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,114.06,395.01,10.91;17,112.66,130.06,97.35,7.90" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="17,151.63,114.06,324.41,10.91">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,141.16,395.01,10.91;17,112.66,154.71,394.53,10.91;17,112.66,168.26,395.01,10.91;17,112.66,184.25,97.35,7.90" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Van Der Wal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01373</idno>
		<title level="m" coord="17,112.66,168.26,365.36,10.91">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,195.36,393.33,10.91;17,112.66,208.91,201.40,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14314</idno>
		<title level="m" coord="17,364.18,195.36,141.81,10.91;17,112.66,208.91,70.44,10.91">QLoRA: Efficient Finetuning of Quantized LLMs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,222.46,394.53,10.91;17,112.66,236.01,394.51,10.91;17,112.66,252.00,117.15,7.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="17,158.40,222.46,181.25,10.91">An Introduction to Professional Search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">I</forename><surname>Tait</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-12511-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-12511-4_1.doi:10.1007/978-3-319-12511-4_1" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="1" to="5" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,263.11,394.53,10.91;17,112.66,276.66,378.02,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,112.66,276.66,228.10,10.91">First international workshop on professional search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiggers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,349.11,276.66,57.63,10.91">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="153" to="162" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,290.20,202.45,10.91;17,330.85,290.20,176.98,10.91;17,112.66,303.75,139.38,10.91;17,270.64,303.75,237.02,10.91;17,112.66,317.30,395.00,10.91;17,112.66,333.29,281.33,7.90" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,330.85,290.20,176.98,10.91;17,112.66,303.75,134.96,10.91">Interactive query expansion for professional search applications</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<idno type="DOI">10.1177/02663821211034079</idno>
		<ptr target="https://doi.org/10.1177/02663821211034079" />
	</analytic>
	<monogr>
		<title level="j" coord="17,270.64,303.75,140.25,10.91">Business Information Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,344.40,393.33,10.91;17,112.66,357.95,393.32,10.91;17,112.33,371.50,395.33,10.91;17,112.66,385.05,268.20,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,317.40,344.40,188.59,10.91;17,112.66,357.95,201.71,10.91">Search strategy formulation for systematic reviews: Issues, challenges and opportunities</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shokraneh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.iswa.2022.200091</idno>
		<ptr target="https://doi.org/10.1016/j.iswa.2022.200091" />
	</analytic>
	<monogr>
		<title level="j" coord="17,322.88,357.95,170.05,10.91">Intelligent Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">200091</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,398.60,393.32,10.91;17,112.66,412.15,326.44,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,357.16,398.60,148.82,10.91;17,112.66,412.15,189.95,10.91">BioASQ-QA: A manually curated corpus for Biomedical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,311.68,412.15,64.68,10.91">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,425.70,394.53,10.91;17,112.66,439.25,393.33,10.91;17,112.66,452.79,393.33,10.91;17,112.66,466.34,136.29,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,232.45,439.25,273.54,10.91;17,112.66,452.79,125.38,10.91">Overview of MedProcNER task on medical procedure detection and entity linking at BioASQ</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gascó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,280.63,452.79,225.35,10.91;17,112.66,466.34,105.59,10.91">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,479.89,393.33,10.91;17,112.66,493.44,394.53,10.91;17,112.28,506.99,248.78,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,344.46,479.89,161.53,10.91;17,112.66,493.44,193.31,10.91">SNOMED clinical terms: overview of the development process and project status</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Q</forename><surname>Stearns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Spackman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,333.19,493.44,168.41,10.91">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<publisher>American Medical Informatics Association</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">662</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,520.54,393.33,10.91;17,112.66,534.09,393.33,10.91;17,112.66,547.64,395.01,10.91;17,112.66,561.19,362.24,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,335.54,520.54,170.45,10.91;17,112.66,534.09,103.75,10.91">Mesh term suggestion for systematic review literature search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Scells</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503516.3503530</idno>
		<ptr target="https://doi.org/10.1145/3503516.3503530.doi:10.1145/3503516.3503530" />
	</analytic>
	<monogr>
		<title level="m" coord="17,239.51,534.09,266.48,10.91;17,112.66,547.64,98.65,10.91">Proceedings of the 25th Australasian Document Computing Symposium, ADCS &apos;21</title>
		<meeting>the 25th Australasian Document Computing Symposium, ADCS &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,574.74,393.33,10.91;17,112.66,588.29,393.33,10.91;17,112.66,601.84,393.53,10.91;17,112.66,615.39,395.01,10.91;17,112.41,628.93,257.80,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,229.15,574.74,276.84,10.91;17,112.66,588.29,217.66,10.91">Mesh suggester: A library and system for mesh term suggestion for systematic review boolean query construction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3539597.3573025</idno>
		<ptr target="https://doi.org/10.1145/3539597.3573025.doi:10.1145/3539597.3573025" />
	</analytic>
	<monogr>
		<title level="m" coord="17,353.33,588.29,152.66,10.91;17,112.66,601.84,317.67,10.91">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM &apos;23</title>
		<meeting>the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM &apos;23<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1176" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,642.48,393.32,10.91;17,112.28,656.03,290.67,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Muresanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01910</idno>
		<title level="m" coord="17,401.52,642.48,104.47,10.91;17,112.28,656.03,160.49,10.91">Large Language Models Are Human-Level Prompt Engineers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,669.58,393.61,10.91;18,112.66,86.97,394.62,10.91;18,112.66,100.52,252.17,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,475.74,669.58,30.53,10.91;18,112.66,86.97,221.89,10.91">Survey of Hallucination in Natural Language Generation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3571730</idno>
		<ptr target="https://doi.org/10.1145/3571730.doi:10.1145/3571730" />
	</analytic>
	<monogr>
		<title level="j" coord="18,344.08,86.97,91.91,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,114.06,393.33,10.91;18,112.66,127.61,166.05,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="18,178.15,114.06,327.84,10.91;18,112.66,127.61,36.04,10.91">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07759</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,141.16,395.01,10.91;18,112.66,157.15,97.35,7.90" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="18,249.57,141.16,225.61,10.91">Large Language Models Are Reasoning Teachers</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10071</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,168.26,395.17,10.91;18,112.66,181.81,394.51,10.91;18,112.36,197.80,61.75,7.90" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="18,219.74,168.26,108.85,10.91">Searching the Enterprise</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hull</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000053</idno>
		<ptr target="http://dx.doi.org/10.1561/1500000053.doi:10.1561/1500000053" />
	</analytic>
	<monogr>
		<title level="j" coord="18,336.99,168.26,170.84,10.91;18,112.66,181.81,60.45,10.91">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,208.91,393.33,10.91;18,112.66,222.46,393.33,10.91;18,112.66,236.01,270.04,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="18,376.92,208.91,129.07,10.91;18,112.66,222.46,178.44,10.91">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,313.14,222.46,192.85,10.91;18,112.66,236.01,182.12,10.91">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</title>
		<meeting>the 2021 ACM conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
