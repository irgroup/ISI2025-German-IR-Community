<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,80.44,387.26,19.63;1,89.29,102.35,340.68,19.63">NCU-IISR: Prompt Engineering on GPT-4 to Stove Biological Problems in BioASQ 11b Phase B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,132.96,77.70,13.63"><forename type="first">Chun-Yu</forename><surname>Hsueh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.63,132.96,47.32,13.63"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.60,132.96,50.86,13.63"><forename type="first">Yu-Wei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.10,132.96,72.54,13.63"><forename type="first">Jen-Chieh</forename><surname>Han</surname></persName>
							<email>joyhan@cc.ncu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.29,132.96,100.69,13.63"><forename type="first">Wilailack</forename><surname>Meesawad</surname></persName>
							<email>wilailack.meesawad@g.ncu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,146.91,118.73,13.63"><forename type="first">Richard Tzong-Han</forename><surname>Tsai</surname></persName>
							<email>thtsai@csie.ncu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Humanities and Social Sciences</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,80.44,387.26,19.63;1,89.29,102.35,340.68,19.63">NCU-IISR: Prompt Engineering on GPT-4 to Stove Biological Problems in BioASQ 11b Phase B</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BFB6FAC18D4639A0AE9F323D514E49ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answer</term>
					<term>Large language models (LLMs)</term>
					<term>Generative Pre-trained Transformer</term>
					<term>Zeroshot</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our system applied in BioASQ 11b phase b. We showcase prompt engineering strategies and outline our experimental steps. Building upon the success of ChatGPT/GPT-4 in answer generation and the field of biology, we developed a system that utilizes GPT-4 to answer biomedical questions. The system leverages OpenAI's ChatCompletions API and combines Prompt Engineering methods to explore various prompts. In addition, we also attempted to incorporate GPT-4 into our system from last year, which combines a BERT-based model and BERTScore. However, the standalone GPT-4 method outperformed this approach by a large margin. Ultimately, in our submission, we adopted what we believe to be the optimal prompts and achieved the highest scores in the second batch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>BioASQ <ref type="bibr" coords="1,121.16,413.68,15.93,12.44" target="#b0">[1]</ref> has been organizing annual challenges in biomedical semantic indexing and question answering since 2013. This year, BioASQ Task 11b Phase B (QA task) <ref type="bibr" coords="1,415.07,427.22,12.89,12.44" target="#b1">[2]</ref> provides biomedical questions along with relevant snippets, and participants are required to generate either the exact answer or the ideal answer using these snippets. The training set for Task 11b Phase B consisted of 4,719 questions, including the test set from the previous year with gold annotations. In addition, it included 330 new test questions for evaluation. The questions were divided into four batches, with 75, 75, 90, 90 questions respectively. A team of biomedical experts from across Europe constructed all the questions and answers. The questions were categorized into four types: Yes/no, factoid, list, and summary. Three types of questions required exact answers: yes/no, factoid, and list. Participants were expected to submit the ideal answer to each question. In Task 11b, each participant could submit up to five results per batch.</p><p>Figure <ref type="figure" coords="1,131.00,562.72,5.07,12.44" target="#fig_0">1</ref> illustrates four examples of QA types for BioASQ Task 11b Phase B (QA task). Each instance of BioASQ QA consists of a question and PubMed abstract snippets relevant to the question. Thus, we framed the task as a query-based multi-document extraction (for the exact answer) and summarization (for the ideal answer). In the previous year, we achieved the highest result in generating ideal answers by using the BioBERT model in combination with linear regression <ref type="bibr" coords="2,138.04,126.04,11.43,12.44" target="#b2">[3]</ref>.</p><p>This year, we observed GPT-4's comprehension capabilities in the field of biology and its advantages in answer generation. We therefore used GPT-4 for answer generation. Specifically, in each batch we developed three or more systems. Particularly in System 1 and System 2, we investigated the impact of prompts on answer generation. We employed Prompt Engineering techniques to select the most suitable prompt. Both systems shared the same prompt, with the only difference being that System 1 utilized GPT-3.5 while System 2 utilized GPT-4. As for System 3, based on the results from the previous year's competition, we found that our research from last year performed well in generating ideal answers. Therefore, we improved upon the previous year's model and utilized its ideal answer for response generation. We relied on System 2's answers for exact answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Biomedical knowledge is often acquired by reading academic papers. This process is timeconsuming and labor-intensive, and it requires a high level of professional expertise. Biomedi-cal professionals cannot quickly obtain the required knowledge in a short period of time. The general public is also unable to acquire biomedical knowledge without expert assistance. QA in natural language processing tasks has the potential to solve these problems by providing direct answers to users' questions. This tests machine learning systems' ability to semantically understand, retrieve, and generate answers from existing text. Many QA models based on deep learning have been developed and applied in the past <ref type="bibr" coords="3,328.73,153.14,11.43,12.44" target="#b3">[4]</ref>.</p><p>Well-trained Large language models: Well-trained large language models have emerged as a powerful tool in natural language processing (NLP) tasks, particularly in question answering (QA). These language models, such as GPT-3 and GPT-4, are trained on vast amounts of text data and can understand and generate human-like responses.</p><p>In NLP QA tasks, well-trained large language models have shown remarkable performance, surpassing traditional methods and achieving state-of-the-art results. These models excel in comprehending complex questions and generating accurate and contextually relevant answers. They leverage their vast knowledge base to provide detailed explanations, supporting evidence, and even generate creative responses.</p><p>According to the GPT-4 Technical Report <ref type="bibr" coords="3,278.06,288.63,13.58,12.44" target="#b4">[5]</ref>, GPT-4 demonstrates a high level of understanding in the medical domain. This is evidenced by its 75% score on the Medical Knowledge Self-Assessment Program test. Additionally, it obtained an outstanding score of 5 in the AP Biology Exam, a feat accomplished by only 15% of the test takers. This indicates its strong performance in biology-related questions. Therefore, we anticipate that GPT-4 will also deliver favorable results in the BioASQ 11b Phase B task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Systems</head><p>We use different systems in different batch. The detailed configuration of each system can be seen in Table <ref type="table" coords="3,150.44,449.52,5.07,12.44" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>This year's competition provided 4,719 training data samples. Among them, there were 1,130 samples of the summary type, 1,417 samples of the factoid type, 901 samples of the list type, and 1,271 samples of the yes/no type. On average, each question consisted of 12 snippets, with an average length of 203 characters per snippet.</p><p>Considering the token limit imposed by OpenAI's API, we extracted only partial information from the snippets. Specifically, in the first two batches, we selected the first 5 snippets and truncated any excessively long sentences to 250 characters. In the subsequent two batches, we input all the snippets. However, for snippets exceeding 250/300 characters, we utilized the ChatCompletions API to perform summarization tasks. This ensured that sentence lengths remained within 250/300 characters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prompting</head><p>OpenAI's ChatCompletions API adheres to a predefined format, requiring specific fields for each message. In addition to the "text" field, the "role" field must be configured, which can be categorized as "system", "assistant", or "user".</p><p>• The system message: As an optional component, configures the assistant's behavior. It can alter the assistant's personality or furnish explicit instructions regarding its conduct throughout the conversation. • The user message: Convey requests or comments that require responses from the assistant. • The assistant message: Retain prior assistant responses, while also allowing developers to compose them as illustrative instances of desired behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Snippets:</head><p>We observed that ChatGPT incorporates past responses, and we aim to leverage this feature to achieve a similar effect of having ChatGPT read through snippets before answering questions. Therefore, for the snippets, we adopt the format of assistant messages, separating snippets from questions, and directly appending them before the questions. We do not include any additional prompts.</p><p>Questions: We experimented with various prompts to guide ChatGPT in generating the desired responses. Ultimately, we opted for a direct approach where ChatGPT generates responses in a fixed JSON format. This decision was driven by our observation that the Exact Answer and Ideal Answer often have a certain degree of overlap. By combining both questions in a single prompt, we encouraged ChatGPT to avoid generating completely unrelated responses. Additionally, imposing a fixed response format greatly improved data processing ef-ficiency. Across the four batches, we employed similar prompts without significant variations. Please refer to Table <ref type="table" coords="5,182.31,98.94,5.07,12.44">2</ref> for the details of the relevant prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The prompts using on ChatGPT Question Types or Tasks Prompts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Reply to the answer clearly and easily in less than 3 sentences. The first question is:{QUESTION_BODY}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes/No</head><p>You can only use JSON format to answer my questions. The format must be {"exact_answer":"", "ideal_answer":""}, where exact_answer should be "yes" or "no", and ideal_answer is a short conversational response starting with yes/no then follow on the explain. The first question is:{QUESTION_BODY} List You can only use JSON format to answer my questions. The format must be {"exact_answer":[], "ideal_answer":""}, where exact_answer is a list of precise key entities to answer the question, and ideal_answer is a short conversational response containing an explanation. The first question is:{QUESTION_BODY} Factoid You can only use JSON format to answer my questions. The format must be {"exact_answer":[], "ideal_answer":""}. where exact_answer is a list of precise key entities to answer the question. ideal_answer is a short conversational response containing an explanation. The first question is:{QUESTION_BODY} To summarize the snippets Conclusion and summarize this context in less than {MAX_SNIPPET_LEN} letters: {SNIPPET_BODY}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Strategy</head><p>In terms of prompt engineering, we can incorporate certain cues or guidelines, in accordance with the competition rules, to enhance the effectiveness of the responses. The following are some of the strategies we have employed:</p><p>• In yes/no type questions, we restrict ChatGPT to only provide responses of 'yes' or 'no. '</p><p>This approach ensures ChatGPT avoids ambiguous answers. • We enable ChatGPT to simultaneously respond to both an exact answer and an ideal answer. This approach prevents situations where there are starkly different responses between the two question-answer pairs. Additionally, simultaneous responses encourage ChatGPT to explain its exact answer within the ideal answer. While we do not have explicit experimental evidence, we believe that, similar to the concept of Chainof-Thought <ref type="bibr" coords="5,163.50,579.58,14.08,12.44" target="#b5">[6]</ref>, having the language model explain its own answers can enhance the accuracy of the responses. • When presenting JSON format, we use quotation marks and square brackets to represent strings and lists, respectively. We also provide additional textual descriptions to help ChatGPT understand the expected type of answer it should provide. • We have observed that the length of a code snippet can impact the length of the generated response. Therefore, in summary-type questions, we limit ChatGPT to providing answers in three sentences. This implicitly avoids excessively long responses without explicitly specifying a specific word count. This approach helps prevent ChatGPT artificially elongating short answers to the question or generating extremely long responses.</p><p>When formulating prompts, we intentionally avoid defining rules or restrictions in excessive detail or complexity. Doing so could potentially result in responses lacking diversity. Therefore, we leave some room for ChatGPT to explore and generate more varied answers, allowing creativity within certain boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Procedure</head><p>Prompt engineering is an experimental and iterative process that requires continuous experimentation, evaluation, and improvement. Depending on the specific task and dataset, different steps and combinations of methods may be necessary. The key is to adjust and optimize based on actual circumstances to achieve the best model outputs. In our experiment, we followed the following steps:</p><p>1. Definition: Confirm the specific task objectives and define the model's input and output.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Result</head><p>The final scores can be obtained from the BioASQ competition results page. These scores are categorized into Exact Answer (Table <ref type="table" coords="7,264.86,367.13,4.16,12.44" target="#tab_1">3</ref>) and Ideal Answer (Table <ref type="table" coords="7,393.29,367.13,3.57,12.44" target="#tab_2">4</ref>). In the Exact Answer category, we included an additional FIN Score, which utilizes the same final ranking score calculation method as the previous year. Although we do not yet have access to the Manual Scores in the Ideal Answer category, we found that the IISR-2 system in Batch 2 achieved the highest score in the FIN metric within the Exact Answer category. This suggests that if the final ranking score calculation remains the same as last year, we would secure the first position in Batch 2 Exact Answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion And Conclusions</head><p>In this year's competition, we observed widespread use of Generative Transformers. However, training a Generative Transformer model effectively is often challenging in typical scenarios. Therefore, we heavily rely on pre-trained large-scale language models that already demonstrate a certain level of generality. Our results in this competition reflect this observation, as the GPT model far exceeded our fine-tuned BioBert model. When most participants utilize OpenAI's API to generate results, it becomes crucial to guide ChatGPT in providing the expected answers. Specifically, the most critical aspect is how to provide key prompts without exceeding the token limit.</p><p>Our high performance in Batch 2 indirectly indicates our strategies' effectiveness. While it is difficult to precisely analyze which strategy contributed the most to the improvement in performance, the summarized explanation of our strategies includes: 1) Using the Assistant role to directly incorporate snippets, 2) Simultaneously addressing both Exact Answer and Ideal Answer, 3) Having ChatGPT respond in a fixed JSON format, and 4) Summarizing excessively long snippets before processing.</p><p>Despite our efforts, we have observed that some other teams performed better in this competition. Therefore, we have been reflecting on why there were disparities in performance despite using the same model. We believe that there are still many areas for improvement. For example, we can employ more scientific methods to determine which snippets should be referenced, conduct more rigorous validation and evaluation of experimental results, and even explore whether to use simple English words or include subject pronouns in the prompts.</p><p>By continuously seeking ways to enhance our approach, we aim to bridge the performance gap and achieve better results in future iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,579.97,254.81,11.36;2,151.80,285.05,291.68,284.85"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of QA types for BioASQ Task 11b Phase B</figDesc><graphic coords="2,151.80,285.05,291.68,284.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,103.64,314.28,324.93,12.44;6,103.64,329.11,278.38,12.44;6,103.64,343.95,245.52,12.44;6,103.64,358.78,402.35,12.44;6,116.56,372.33,40.56,12.44;6,103.64,387.16,404.02,12.44"><head>2 .</head><label>2</label><figDesc>Analysis: Analyze the characteristics and specifications of the dataset. 3. Design: Design a prompt that combines different strategies. 4. Evaluation: Examine and analyze the output results. 5. Optimization: Attempt to optimize the strategies and explore combinations of different methods. 6. Iteration: Repeat steps 3 to 5 continuously until satisfactory output results are achieved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.93,88.21,418.15,240.91"><head>Table 1</head><label>1</label><figDesc>All submitted systems' settings. BioBert's Model field represents that Exact Answer uses GPT-4 results, while Ideal Answer utilizes last year's method. In the Snippet Strategy field, split means to truncate the snippet directly, while summary means to summarize using the same model.</figDesc><table coords="4,147.96,143.72,299.35,185.41"><row><cell>Batch</cell><cell cols="4">System Name Model Snippet Length Snippet Strategy</cell></row><row><cell></cell><cell>IISR-1</cell><cell>GPT-3</cell><cell>250</cell><cell>split</cell></row><row><cell>Batch-1</cell><cell>IISR-2</cell><cell>GPT-4</cell><cell>250</cell><cell>split</cell></row><row><cell></cell><cell>IISR-3</cell><cell>BioBert</cell><cell>250</cell><cell>split</cell></row><row><cell></cell><cell>IISR-1</cell><cell>GPT-3</cell><cell>250</cell><cell>split</cell></row><row><cell>Batch-2</cell><cell>IISR-2</cell><cell>GPT-4</cell><cell>250</cell><cell>split</cell></row><row><cell></cell><cell>IISR-3</cell><cell>BioBert</cell><cell>250</cell><cell>split</cell></row><row><cell></cell><cell>IISR-1</cell><cell>GPT-3</cell><cell>250</cell><cell>summary</cell></row><row><cell>Batch-3</cell><cell>IISR-2</cell><cell>GPT-4</cell><cell>250</cell><cell>summary</cell></row><row><cell></cell><cell>IISR-3</cell><cell>BioBert</cell><cell>250</cell><cell>summary</cell></row><row><cell></cell><cell>IISR-1</cell><cell>GPT-3</cell><cell>250</cell><cell>summary</cell></row><row><cell></cell><cell>IISR-2</cell><cell>GPT-4</cell><cell>250</cell><cell>summary</cell></row><row><cell>Batch-4</cell><cell>IISR-3</cell><cell>BioBert</cell><cell>250</cell><cell>summary</cell></row><row><cell></cell><cell>IISR-4</cell><cell>GPT-3</cell><cell>300</cell><cell>summary</cell></row><row><cell></cell><cell>IISR-5</cell><cell>GPT-4</cell><cell>300</cell><cell>summary</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,89.02,422.02,418.06,239.99"><head>Table 3</head><label>3</label><figDesc>The Exact Answers test results on BioASQ. We define FIN scores as the average of Accuracy in Yes/No, MRR in Factoid, and F-Measure in List.</figDesc><table coords="6,100.53,464.64,394.22,197.36"><row><cell>Batch</cell><cell>System</cell><cell cols="6">Yes/No Acc maF1 SAcc LAcc MRR Precision Recall Factoid List</cell><cell>F1</cell><cell>FIN</cell></row><row><cell></cell><cell cols="5">IISR-1 0.917 0.906 0.421 0.421 0.421</cell><cell>0.719</cell><cell cols="2">0.667 0.684 0.674</cell></row><row><cell>Batch-1</cell><cell cols="5">IISR-2 0.958 0.952 0.526 0.526 0.526</cell><cell>0.642</cell><cell cols="2">0.570 0.597 0.694</cell></row><row><cell></cell><cell cols="2">IISR-3 0.708 0.415</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">IISR-1 1.000 1.000 0.500 0.546 0.523</cell><cell>0.486</cell><cell cols="2">0.331 0.368 0.630</cell></row><row><cell>Batch-2</cell><cell cols="5">IISR-2 1.000 1.000 0.546 0.636 0.591</cell><cell>0.510</cell><cell cols="2">0.358 0.398 0.663</cell></row><row><cell></cell><cell cols="5">IISR-3 0.583 0.368 0.546 0.636 0.591</cell><cell>0.510</cell><cell cols="2">0.358 0.398 0.524</cell></row><row><cell></cell><cell cols="5">IISR-1 0.917 0.906 0.423 0.423 0.423</cell><cell>0.515</cell><cell cols="2">0.458 0.459 0.600</cell></row><row><cell>Batch-3</cell><cell cols="5">IISR-2 0.917 0.911 0.385 0.423 0.404</cell><cell>0.652</cell><cell cols="2">0.606 0.605 0.642</cell></row><row><cell></cell><cell cols="5">IISR-3 0.625 0.384 0.385 0.423 0.404</cell><cell>0.652</cell><cell cols="2">0.606 0.605 0.554</cell></row><row><cell></cell><cell cols="5">IISR-1 1.000 1.000 0.387 0.419 0.403</cell><cell>0.717</cell><cell cols="2">0.648 0.671 0.691</cell></row><row><cell></cell><cell cols="5">IISR-2 0.929 0.918 0.419 0.419 0.419</cell><cell>0.640</cell><cell cols="2">0.662 0.636 0.661</cell></row><row><cell>Batch-4</cell><cell cols="5">IISR-3 0.286 0.222 0.419 0.419 0.419</cell><cell>0.242</cell><cell cols="2">0.184 0.197 0.301</cell></row><row><cell></cell><cell cols="5">IISR-4 1.000 1.000 0.419 0.419 0.419</cell><cell>0.667</cell><cell cols="2">0.575 0.596 0.672</cell></row><row><cell></cell><cell cols="5">IISR-5 0.929 0.918 0.452 0.452 0.452</cell><cell>0.713</cell><cell cols="2">0.684 0.681 0.687</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,89.02,88.21,350.91,216.04"><head>Table 4</head><label>4</label><figDesc>The Ideal Answers test results on BioASQ.</figDesc><table coords="7,155.35,118.84,284.58,185.41"><row><cell>Batch</cell><cell cols="5">System R-2 (Rec) R-2 (F1) R-SU4 (Rec) R-SU4 (F1)</cell></row><row><cell></cell><cell>IISR-1</cell><cell>0.378</cell><cell>0.314</cell><cell>0.361</cell><cell>0.290</cell></row><row><cell>Batch-1</cell><cell>IISR-2</cell><cell>0.415</cell><cell>0.329</cell><cell>0.403</cell><cell>0.309</cell></row><row><cell></cell><cell>IISR-3</cell><cell>0.448</cell><cell>0.406</cell><cell>0.439</cell><cell>0.396</cell></row><row><cell></cell><cell>IISR-1</cell><cell>0.339</cell><cell>0.287</cell><cell>0.340</cell><cell>0.282</cell></row><row><cell>Batch-2</cell><cell>IISR-2</cell><cell>0.355</cell><cell>0.301</cell><cell>0.352</cell><cell>0.290</cell></row><row><cell></cell><cell>IISR-3</cell><cell>0.339</cell><cell>0.306</cell><cell>0.336</cell><cell>0.295</cell></row><row><cell></cell><cell>IISR-1</cell><cell>0.381</cell><cell>0.345</cell><cell>0.383</cell><cell>0.342</cell></row><row><cell>Batch-3</cell><cell>IISR-2</cell><cell>0.378</cell><cell>0.323</cell><cell>0.378</cell><cell>0.317</cell></row><row><cell></cell><cell>IISR-3</cell><cell>0.376</cell><cell>0.331</cell><cell>0.364</cell><cell>0.314</cell></row><row><cell></cell><cell>IISR-1</cell><cell>0.350</cell><cell>0.342</cell><cell>0.345</cell><cell>0.331</cell></row><row><cell></cell><cell>IISR-2</cell><cell>0.331</cell><cell>0.300</cell><cell>0.332</cell><cell>0.293</cell></row><row><cell>Batch-4</cell><cell>IISR-3</cell><cell>0.333</cell><cell>0.340</cell><cell>0.314</cell><cell>0.317</cell></row><row><cell></cell><cell>IISR-4</cell><cell>0.345</cell><cell>0.336</cell><cell>0.339</cell><cell>0.326</cell></row><row><cell></cell><cell>IISR-5</cell><cell>0.345</cell><cell>0.321</cell><cell>0.341</cell><cell>0.309</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.59,265.96,399.60,12.44;8,107.59,279.51,398.39,12.44;8,107.59,293.06,398.39,12.44;8,107.59,306.61,400.23,12.44;8,107.59,320.16,296.96,12.44" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,236.11,279.51,269.87,12.44;8,107.59,293.06,295.54,12.44">Overview of bioasq 2023: The eleventh bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,432.85,293.06,73.12,12.44;8,107.59,306.61,400.23,12.44;8,107.59,320.16,242.88,12.44">Proceedings of the Fourteenth International Conference of the CLEF Association (CLEF</title>
		<meeting>the Fourteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,107.59,333.71,398.39,12.44;8,107.59,347.26,398.39,12.44;8,107.59,360.81,107.76,12.44" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,353.65,333.71,152.32,12.44;8,107.59,347.26,96.79,12.44">Overview of bioasq tasks 11b and synergy11 in clef2023</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,234.50,347.26,271.47,12.44;8,107.59,360.81,77.06,12.44">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,374.36,398.39,12.44;8,107.59,387.91,399.60,12.44;8,107.59,401.46,187.18,12.44" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,310.86,374.36,195.12,12.44;8,107.59,387.91,395.81,12.44">Ncu-iisr/as-gis: Using bertscore and snippet score to improve the performance of pretrained language model in bioasq 10b phase b</title>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,107.59,401.46,132.24,12.44">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,415.01,398.39,12.44;8,107.59,428.55,399.59,12.44;8,107.20,442.10,399.49,12.44;8,107.59,455.65,103.02,12.44" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,315.72,415.01,190.26,12.44;8,107.59,428.55,59.96,12.44">COVID-QA: A question answering dataset for COVID-19</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietsch</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.nlpcovid19-acl.18" />
	</analytic>
	<monogr>
		<title level="m" coord="8,194.66,428.55,312.51,12.44;8,107.20,442.10,190.34,12.44">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Association for Computational Linguistics</title>
		<meeting>the 1st Workshop on NLP for COVID-19 at ACL 2020, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,469.20,258.94,12.44" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Gpt-4 technical report</note>
</biblStruct>

<biblStruct coords="8,107.59,482.75,400.23,12.44;8,107.59,496.30,394.78,12.44" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m" coord="8,464.79,482.75,43.03,12.44;8,107.59,496.30,272.23,12.44">Chain-ofthought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
