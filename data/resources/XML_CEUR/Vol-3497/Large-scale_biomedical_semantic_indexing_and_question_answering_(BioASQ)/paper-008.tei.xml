<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,372.70,15.42;1,89.29,106.66,299.01,15.42;1,89.29,128.58,233.13,15.43;1,89.29,150.91,369.77,11.96">Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training University of Technology Sydney participation in BioASQ Task 11b Phase B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,176.82,53.84,11.96"><forename type="first">Dima</forename><surname>Galat</surname></persName>
							<email>dima.galat[@]student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney (UTS)</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,153.97,176.82,106.43,11.96"><forename type="first">Marian-Andrei</forename><surname>Rizoiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney (UTS)</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,372.70,15.42;1,89.29,106.66,299.01,15.42;1,89.29,128.58,233.13,15.43;1,89.29,150.91,369.77,11.96">Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training University of Technology Sydney participation in BioASQ Task 11b Phase B</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">96052FF54374FA00B40825EE1E43ED87</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>natural language processing</term>
					<term>biomedical summarization</term>
					<term>biomedical question answering</term>
					<term>transfer learning</term>
					<term>language modeling</term>
					<term>domain-specific pre-training</term>
					<term>BioASQ</term>
					<term>CEUR-WS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The fields of question-answering and summarization have witnessed significant advancements in recent years, with a shift from classification-based extractive approaches to the emergence of abstractive summarization models. This transition has been driven by the superior performance and enhanced generalization capabilities exhibited by abstractive models, effectively blurring the boundary between long-form question answering and summarization. This paper addresses the summarization challenge presented by BioASQ Task B Phase B in the biomedical domain, for which we propose a novel approach.</p><p>The healthcare sector holds immense potential for leveraging health research data sharing to enhance clinical care, informed decision-making, and scientific discovery <ref type="bibr" coords="1,404.85,540.57,11.28,10.91" target="#b0">[1]</ref>. Sharing biomedical and healthcare studies and research data with the wider public requires robust and efficient methods. Large pre-trained language models (LLMs) have emerged as promising candidates for this purpose. LLMs have the potential to store medical knowledge while accommodating variations in data and application tasks <ref type="bibr" coords="1,260.12,594.77,11.28,10.91" target="#b1">[2]</ref>. This paper aims to analyze the impact of the training process on LLMs' ability to store biomedical knowledge, explicitly focusing on their utilization for a question-answering and summarization task.</p><p>Traditionally, achieving state-of-the-art performance on natural language processing tasks involves a two-phase approach <ref type="bibr" coords="2,227.41,405.99,11.23,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,241.37,405.99,8.88,10.91" target="#b3">4]</ref> that is shown in blue in the top row of Fig. <ref type="figure" coords="2,442.45,405.99,3.75,10.91" target="#fig_0">1</ref>: pre-training the models on an extensive range of texts and topics, followed by task-specific fine-tuning <ref type="bibr" coords="2,89.29,433.09,11.23,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,103.12,433.09,7.43,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,113.15,433.09,7.49,10.91" target="#b1">2]</ref>. This approach has revolutionized various areas of natural language processing <ref type="bibr" coords="2,472.09,433.09,11.23,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,485.93,433.09,7.42,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,495.95,433.09,7.49,10.91" target="#b3">4]</ref>, with LLMs such as BERT, GPT, and BART demonstrating remarkable capabilities. However, pretraining models is a time-consuming and a resource-intensive process, and the literature lacks comprehensive insights into the performance of these models for domain-specific applications with limited data availability. Therefore, this study aims to address this gap by examining the performance of LLMs in the context of the BioASQ summarization task.</p><p>This paper investigates two open questions concerning biomedical domain questionanswering and text summarization tasks. Over the past five years, the biomedical domain has increasingly relied on in-domain pre-training and fine-tuning of BERT <ref type="bibr" coords="2,418.66,541.49,12.72,10.91" target="#b3">[4]</ref> for a wide range of datasets and benchmarks <ref type="bibr" coords="2,218.62,555.03,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,233.02,555.03,7.52,10.91" target="#b8">9,</ref><ref type="bibr" coords="2,243.45,555.03,12.59,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,258.95,555.03,12.59,10.91" target="#b10">11,</ref><ref type="bibr" coords="2,274.45,555.03,12.42,10.91" target="#b11">12]</ref>. In-domain pre-training has proven effective in enhancing performance for discriminatory biomedical tasks. However, BERT's architecture is not optimized for text generation tasks <ref type="bibr" coords="2,264.49,582.13,16.39,10.91" target="#b12">[13,</ref><ref type="bibr" coords="2,283.62,582.13,12.30,10.91" target="#b13">14]</ref>, lacking an autoregressive decoder to generate tokens based on previously generated ones. Consequently, BERT is suboptimal for generation tasks, necessitating exploring alternative approaches. Previous studies evaluating biomedical models across diverse tasks have not reported results on generation problems due to using non-autoregressive models <ref type="bibr" coords="2,208.90,636.33,16.08,10.91" target="#b9">[10]</ref>. The first question is is there a better-suited architecture for biomedical text generation tasks? A significant amount of research suggests that domainspecific pre-training significantly outperforms mixed-domain pre-training. However, we could not find any convincing evidence for supporting this belief when it comes to text generation problems <ref type="bibr" coords="3,132.71,100.52,11.29,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,146.73,100.52,7.45,10.91" target="#b8">9,</ref><ref type="bibr" coords="3,156.91,100.52,12.52,10.91" target="#b9">10,</ref><ref type="bibr" coords="3,172.16,100.52,12.52,10.91" target="#b10">11,</ref><ref type="bibr" coords="3,187.41,100.52,12.52,10.91" target="#b14">15,</ref><ref type="bibr" coords="3,202.66,100.52,12.52,10.91" target="#b15">16,</ref><ref type="bibr" coords="3,217.91,100.52,12.27,10.91" target="#b11">12]</ref>. The second question is do LLMs need to be pre-trained in domain to achieve optimal performance?</p><p>We answer the above two questions. To investigate the efficacy of domain-specific pre-training and fine-tuning for biomedical text generation, we propose an alternative three-step approach (shown in the bottom row of Fig. <ref type="figure" coords="3,237.16,154.71,3.54,10.91" target="#fig_0">1</ref>). In this approach, we initially train a general-domain LLM, followed by fine-tuning for a specific task in the general domain (text summarization) and subsequent fine-tuning for the target biomedical domain task. Contrary to established theories in the biomedical domain <ref type="bibr" coords="3,208.82,195.36,16.55,10.91" target="#b9">[10,</ref><ref type="bibr" coords="3,228.49,195.36,12.59,10.91" target="#b14">15,</ref><ref type="bibr" coords="3,244.20,195.36,12.42,10.91" target="#b15">16]</ref>, our findings suggest that having a large task-specific dataset can be more valuable than domain-specific pre-training for biomedical text generation tasks. This approach aligns with studies indicating that diverse pre-training objectives, larger and more diverse datasets, and tasks contribute to the robustness of the fine-tuning process even without domain adaptation <ref type="bibr" coords="3,237.17,249.56,16.43,10.91" target="#b13">[14,</ref><ref type="bibr" coords="3,256.33,249.56,12.32,10.91" target="#b16">17]</ref>.</p><p>We explore alternative architectures for biomedical text generation. In this study, we focus on BART <ref type="bibr" coords="3,135.61,276.66,11.59,10.91" target="#b4">[5]</ref>, a comprehensive architecture that incorporates pre-training objectives from both BERT <ref type="bibr" coords="3,142.10,290.20,12.99,10.91" target="#b3">[4]</ref> and GPT <ref type="bibr" coords="3,201.41,290.20,18.07,10.91" target="#b17">[18]</ref> models. BART has demonstrated state-of-the-art performance in abstractive dialogue, question-answering, and summarization tasks, making it particularly effective for text generation and comprehension. Our experimental results showcase the benefits and effectiveness of utilizing the BART architecture for transfer learning techniques in a context of a biomedical summarization task.</p><p>The main contributions of this work can be summarized as follows:</p><p>â€¢ Evaluating the advantages of domain-specific pre-training in the context of text generation tasks. â€¢ Evaluating the impact of task-specific training on improving text generation tasks.</p><p>â€¢ Assessing the performance of BART, an encoder with an auto-regressive decoder architecture, in the biomedical question answering task B of BioASQ 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We are looking for a LLM which has an architecture suitable for long-form question answering and has been trained on relevant in-domain data. There are several important model architectures and pre-training objectives used to optimize the models worth considering <ref type="bibr" coords="3,449.29,526.09,16.43,10.91" target="#b17">[18,</ref><ref type="bibr" coords="3,468.45,526.09,7.47,10.91" target="#b3">4,</ref><ref type="bibr" coords="3,478.65,526.09,7.57,10.91" target="#b4">5]</ref>. First, lets briefly mention BERT <ref type="bibr" coords="3,243.15,539.64,12.84,10.91" target="#b3">[4]</ref> in the context of text generation, since most biomedical Transformer-based <ref type="bibr" coords="3,174.86,553.18,12.69,10.91" target="#b2">[3]</ref> models still rely on this architecture. BERT does not have an autoregressive decoder, preventing it from generating text. Despite this fact, a well-known summarisation approach called PreSumm <ref type="bibr" coords="3,205.02,580.28,17.76,10.91" target="#b12">[13]</ref> uses this architecture by inserting additional tokens for teaching models which sentences should be included in the summary. We followed the process proposed by the authors while using a BioBERT <ref type="bibr" coords="3,267.04,607.38,13.00,10.91" target="#b8">[9]</ref> model; we first trained an extractive summariser, which did perform a little better on BioASQ data than a regular BERT trained the same way. Unfortunately, when training an abstractive summarization architecture, PreSumm <ref type="bibr" coords="3,453.07,634.48,17.75,10.91" target="#b12">[13]</ref> process uses a randomly initialised Transformer <ref type="bibr" coords="3,269.82,648.03,12.83,10.91" target="#b2">[3]</ref> for a decoder. It appears that there is a significant mismatch between this decoder and a BioBERT <ref type="bibr" coords="3,310.46,661.58,12.99,10.91" target="#b8">[9]</ref> encoder leading to unstable abstractive fine-tuning process and poor generation outputs in our experiments. Based on these findings, we have concluded that BERT is a wrong architecture to be using for text generation tasks.</p><p>BART <ref type="bibr" coords="4,128.99,114.06,12.68,10.91" target="#b4">[5]</ref> is an architecture that uses an encoder with an auto-regressive decoder, similarly to the original Transformer <ref type="bibr" coords="4,200.69,127.61,11.31,10.91" target="#b2">[3]</ref>. BART relies on an architecture which can be seen as generalising BERT (because it also uses a bi-directional encoder) and GPT <ref type="bibr" coords="4,373.76,141.16,18.06,10.91" target="#b18">[19]</ref> (because it also uses the left-to-right decoder). This model is using a masked language modeling objective (also known as denoising) introduced by BERT <ref type="bibr" coords="4,249.91,168.26,13.00,10.91" target="#b3">[4]</ref> and adds two additional denoising objectives (token deletion and sentence permutation). Authors conduct experiments that are focused on text generation, and show that denoising objectives are particularly well-suited for summarization tasks. Because it can be easily fine-tuned directly for generation tasks, authors achieved a remarkable success on a wide range of abstractive summarization and long-form question answering problems <ref type="bibr" coords="4,182.36,236.01,11.43,10.91" target="#b4">[5]</ref>.</p><p>BioBART <ref type="bibr" coords="4,143.68,249.56,17.76,10.91" target="#b14">[15]</ref> is a BART model pre-trained on PubMed [20] abstracts. Authors have reported that they have trained without one of the objectives proposed by BART, namely the sentence permutation, showing that models trained without this objective have a better performance. Overall, this is the only study that we are aware of that applies a LLM to a range of generation tasks and reports the results (another BioGPT <ref type="bibr" coords="4,288.54,303.75,17.75,10.91" target="#b15">[16]</ref> study we found has not reported any numeric results on text generation problems). We are also not completely convinced that some of the results, like those reported for a BioASQ task could not be a result of a random chance, since the differences in the scores are very small and there are a few possible sources of non-determinism in training and generation procedures we discuss later in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Contribution</head><p>In the biomedical domain, the majority of models we have reviewed are focused on the pretraining process, perhaps because pre-training data is readily available <ref type="bibr" coords="4,405.85,430.13,11.36,10.91" target="#b7">[8,</ref><ref type="bibr" coords="4,419.93,430.13,7.47,10.91" target="#b8">9,</ref><ref type="bibr" coords="4,430.13,430.13,12.55,10.91" target="#b9">10,</ref><ref type="bibr" coords="4,445.41,430.13,12.55,10.91" target="#b10">11,</ref><ref type="bibr" coords="4,460.69,430.13,12.55,10.91" target="#b14">15,</ref><ref type="bibr" coords="4,475.96,430.13,12.55,10.91" target="#b15">16,</ref><ref type="bibr" coords="4,491.24,430.13,12.32,10.91" target="#b11">12]</ref>. However, question answering and summarization are plagued by a lack of a large domain specific dataset for fine-tuning LLMs directly for text generation problems. More specifically, when we are looking at the biomedical text generation tasks, it's hard to find a large (and clean) sequence-to-sequence dataset for fine-tuning for a long-form question answering and summarization. BioASQ is the closest dataset currently available, however it is still a few orders of magnitude away from what we would require to fine-tune a LLM for a previously unseen generation task. Therefore, we conclude that this two-step fine-tuning process offers a limited utility for this problem.</p><p>Following a conventional transfer learning definition we use a task to refer to training on labeled data, seeking to transfer the knowledge from a source task and a source domain (ğ’¯ ğ‘† and ğ’Ÿ ğ‘† ) to a target task and a target domain (ğ’¯ ğ‘‡ and ğ’Ÿ ğ‘‡ ) <ref type="bibr" coords="4,344.24,579.17,16.30,10.91" target="#b19">[21,</ref><ref type="bibr" coords="4,363.18,579.17,12.23,10.91" target="#b20">22]</ref>. One of the common transfer learning scenarios involves learning the tasks sequentially, one after another; and we could also have an intermediate fine-tuning task making it a three-step fine tuning process, where a second step is only used to get a representation that is more suitable for the task of summarization in a biomedical domain. This means that an intermediate ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ (which could be both in/out domain) should lead to a performance improvement in ğ’¯ ğ‘‡ . This could be potentially useful, since task-domain specific data is hard to come by.</p><p>Since we need to perform text generation, a reasonable option is to train for an ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ which teaches the model to perform this task. Unfortunately, large question answering and summarization datasets like SQUAD <ref type="bibr" coords="5,249.23,114.06,17.76,10.91" target="#b21">[23]</ref> and CNN/DM <ref type="bibr" coords="5,331.96,114.06,17.76,10.91" target="#b22">[24]</ref> have nothing to do with biomedical domain, but because we need 10-100 times more biomedical summarization data than what we have available, we believe that task-specific datasets could offer just as much value as a domainspecific pre-training. We believe that CNN/DM is the most suitable (clean, large, easily available) task-specific dataset; especially because summaries there are typically closely related to source sentences, which is also the case with the BioASQ data. Moreover, lengths of summaries are similar to those in BioASQ. Therefore, we are interested in this task, even though a newsmedia domain would likely have completely different marginal probability distributions of generated text. This approach means that in addition to sequential transfer learning (two and three step fine-tuning processes described above), models competing with a two-step fine-tuning strategy would have to also adapt for the domain difference (i.e. differences in prior and conditional distributions). Second ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ worth considering is training on Pubmed [20] article-abstracts combinations. While these are not summaries in the stricter sense of the word, this is the closest domain-specific dataset that we could find, and we would like to understand if it adds useful information to a LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Models compared</head><p>We select LLMs that reduce the amount of overall training required. We select a mix of domainspecific pre-training and general pre-training datasets, and we attempt different ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ s to see how well the resulting models generalize to ğ’¯ ğ‘‡ , namely BioASQ Task 11b Phase B. Hence, the final list of LLMs we are considering are:</p><p>â€¢ BART -a baseline two-step LLM (without additional fine-tuning) used to establish a baseline for a general domain model without specialized domain knowledge or ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ fine-tuning â€¢ BioBART -a biomedical two-step LLM (without fine-tuning ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ), used to establish a baseline for an in-domain model â€¢ BART CNN -a baseline LLM three-step LLM with task-specific fine-tuning ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ but without any deep domain knowledge â€¢ BioBART CNN -a biomedical three-step LLM with task-specific fine-tuning ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ â€¢ BART CNN Pubmed -a general domain three-step LLM fine-tuned for ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ summarisation task, and then further fine-tuned on a domain-specific ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ dataset containing Pubmed articles</p><p>Based on the data available, we believe that these tasks and LLMs offer the greatest benefit for biomedical summarization, and we limit our selection to 5 models that will participate in the BioASQ competition. We are only considering large models because we want the model to analyze as much context as possible, and therefore having a large model helps to double the context length (1024 tokens vs. 512 tokens). We are using pre-trained BART, BioBART, and BART CNN models available via Huggingface<ref type="foot" coords="5,290.43,648.33,3.71,7.97" target="#foot_0">1</ref> ; and we are fine-tuning BART CNN on Pubmed data and BioBART on CNN data for one epoch each (our ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ). Subsequently, all models are fine-tuned on the latest complete BioASQ 11 dataset (ğ’¯ ğ‘‡ ) for five epochs using a 10-fold cross-validation process. We empirically chose the number of training epochs to maximize the final model scores. Using Pubmed (summarisation) data for fine-tuning BioBART after CNN training didn't offer advantages (Appendix A) and was excluded from the top five models under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Our experiments have revealed a substantial (over 10%) variation in ROUGE <ref type="bibr" coords="6,429.95,569.06,17.91,10.91" target="#b23">[25]</ref> score results based on a simple choice of a seed parameter for cross-validation. This indicates that the finetuning process is susceptible to changes in data. Future studies should consider which BioASQ information snippets are passed to the model as the input for summarization training. Working with small healthcare question-answering datasets can require a more careful knowledge extraction process <ref type="bibr" coords="6,173.13,636.81,16.25,10.91" target="#b24">[26]</ref>.</p><p>We have experimented with fine-tuning for up to 10 epochs on the ğ’¯ ğ‘‡ , and found that this problem consistently persists across a range of training scenarios. In-domain studies we have  reviewed show that the generation results can often differ by a minimal margin, significantly lower than the variation in scores we have observed in cross-validation.</p><p>To our knowledge, this research is the first to draw attention to this specific problem, and we decided to overcome this by repeating the 10-fold cross-validation training process ğ’¯ ğ‘‡ four times using a different seed value. Therefore, we effectively report the average of 400 runs for each model (95% t-test confidence interval is given in parentheses), with 100 runs for each seed choice (ten for each fold). We are primarily focused on SU4-F1 scores (Table <ref type="table" coords="7,426.69,427.78,4.12,10.91" target="#tab_0">1</ref>) since they have been shown to correlate with human scores the best <ref type="bibr" coords="7,323.53,441.33,16.22,10.91" target="#b25">[27]</ref>. However, ROUGE is recall-oriented; therefore, we also look at Recall results separately (Table <ref type="table" coords="7,344.66,454.88,3.57,10.91" target="#tab_1">2</ref>).</p><p>Our experiments (Section 5) suggest that LLMs without domain-specific pre-training show a better capacity for domain-specific text generation. This becomes particularly clear when comparing BART and BioBART results before any additional task-specific fine-tuning, suggesting that BioASQ data is not as similar to Pubmed pre-training data as we would expect based on other results reported on discriminatory tasks. Moreover, we believe that currently a non-domain specific CNN summarization task ğ’¯ ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ is required to accomplish the best results on a BioASQ task. Adding in-domain Pubmed data improves Recall; however, Pubmed data is unsuitable for training for a summarization task from scratch. ROUGE Recall scores (Section 5) show one notable difference, BART CNN has a higher recall, whereas BART CNN Pubmed has a higher precision, likely because the Pubmed training after the task-specific training introduces a task-specific vocabulary to the model.</p><p>Overall, LLMs have established some remarkable results in various practical applications. However, since LLMs require task-specific datasets to train to generate text, and such domainspecific datasets are scarce, we need to find ways to overcome these challenges. We have presented an approach that focuses on applications of transfer learning to a domain with limited task-specific training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this work, we have observed that task-specific data is critical for generating text in a biomedical domain. Based on our experiments, models without in-domain pre-training are better at summarizing BioASQ data. Unfortunately, our models have achieved fairly modest automated ROUGE scores during BioASQ 11 runs, and we are waiting for the final results to determine how the models have performed overall. The generation process is non-deterministic, and while the answers generated by the models appear sensible, we need better ways to evaluate the candidates.</p><p>We have discussed how transfer learning can overcome challenges with data availability. We see a lot of exciting possibilities for using generator models (more specifically paraphrasing, simplification, and rewriting models <ref type="bibr" coords="8,257.20,267.54,17.11,10.91" target="#b26">[28]</ref>) for creating synthetic training data, as well as for providing a differentiable loss function which allows sampling a wider space of possible answers without over-penalizing exploration. Abstractive summarization models are trained to generate specific gold sequences, even when they start making errors in the first steps (a problem known as exposure bias <ref type="bibr" coords="8,168.45,321.73,15.89,10.91" target="#b27">[29]</ref>). One recent improvement over BART proposes generating multiple candidates and comparing them, showing a new SOTA on several popular summarization datasets <ref type="bibr" coords="8,128.92,348.83,16.41,10.91" target="#b28">[30]</ref>. This could address a common shortcoming of autoregressive models, leading to further performance improvements. Another possibility that shows a significant promise would be generating synthetic data to augment BioASQ. This approach has recently shown good results in machine translation <ref type="bibr" coords="8,245.84,389.48,16.09,10.91" target="#b29">[31]</ref>, and we believe it can be used for other text-generation problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,267.97,417.05,8.93;2,89.29,279.97,416.70,8.87;2,89.29,291.93,416.69,8.87;2,89.29,303.88,416.69,8.87;2,89.29,315.84,416.70,8.87;2,89.29,327.79,224.10,8.87;2,150.55,84.19,291.68,176.35"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic representation of LLM usage in practice. The blue arrow in the top row represents the two-phase (training and then fine-tuning) transfer learning approach typically seen in literature. The middle and bottom rows represent the task-specific fine-tuning approach proposed in this paper. Our results suggest that in-domain pre-training does not improve the BioASQ summarization performance even when summarization training is introduced in the middle (second row). In this diagram, the orange area indicates in-domain training.</figDesc><graphic coords="2,150.55,84.19,291.68,176.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,357.27,418.35,8.93;6,89.29,369.28,416.70,8.87;6,89.29,381.23,416.69,8.87;6,89.29,393.19,416.70,8.87;6,89.29,405.14,192.80,8.87;6,119.29,84.19,354.21,265.66"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:We can see that BioBART outperforms regular BART, which can be attributed to the domainspecific vocabulary; however, the same vocabulary hurts its ability to do task-specific training on the out-of-domain CNN dataset. BART CNN shows a solid performance for a model that was only fine-tuned on a small BioASQ in-domain dataset, and adding an in-domain Pubmed dataset reduces the range of scores a little, making it the best overall model.</figDesc><graphic coords="6,119.29,84.19,354.21,265.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="11,120.54,259.31,354.21,265.66"><head></head><label></label><figDesc></figDesc><graphic coords="11,120.54,259.31,354.21,265.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="12,120.54,200.08,354.20,384.11"><head></head><label></label><figDesc></figDesc><graphic coords="12,120.54,200.08,354.20,384.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,90.49,287.98,103.52"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="7,89.29,102.49,287.68,91.52"><row><cell>ROUGE-SU4 F1 -Mean confidence intervals</cell><cell></cell><cell></cell></row><row><cell>model name</cell><cell cols="2">mean CI 95%</cell></row><row><cell>BioBART</cell><cell cols="2">0.383 (0.373, 0.394)</cell></row><row><cell>BART</cell><cell cols="2">0.384 (0.376, 0.392)</cell></row><row><cell>BioBART CNN</cell><cell>0.39</cell><cell>(0.382, 0.398)</cell></row><row><cell>BART CNN</cell><cell cols="2">0.396 (0.387, 0.406)</cell></row><row><cell cols="3">BART CNN Pubmed 0.396 (0.386, 0.405)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,216.26,287.98,103.53"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,89.29,228.26,287.68,91.52"><row><cell>ROUGE-SU4 Recall -Mean confidence intervals</cell><cell></cell><cell></cell></row><row><cell>model name</cell><cell cols="2">mean CI 95%</cell></row><row><cell>BioBART</cell><cell cols="2">0.398 (0.386, 0.409)</cell></row><row><cell>BART</cell><cell>0.4</cell><cell>(0.392, 0.409)</cell></row><row><cell>BioBART CNN</cell><cell cols="2">0.415 (0.407, 0.424)</cell></row><row><cell>BART CNN</cell><cell>0.42</cell><cell>(0.411, 0.429)</cell></row><row><cell cols="3">BART CNN Pubmed 0.422 (0.41, 0.434)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,92.57,671.04,110.81,8.97"><p>https://huggingface.co/models</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,461.65,393.33,10.91;8,112.66,475.20,348.44,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,322.02,461.65,183.97,10.91;8,112.66,475.20,160.07,10.91">Data acquisition, curation, and use for a continuously learning health system</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Krumholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Waldstreicher</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2016.12537</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,488.75,394.53,10.91;8,112.48,502.30,394.70,10.91;8,112.28,515.85,394.91,10.91;8,112.66,529.40,394.53,10.91;8,112.66,542.95,394.53,10.91;8,112.48,556.50,394.70,10.91;8,112.66,570.05,394.53,10.91;8,112.66,583.60,394.53,10.91;8,112.66,597.15,394.53,10.91;8,112.28,610.69,394.91,10.91;8,112.66,624.24,394.52,10.91;8,112.66,637.79,394.53,10.91;8,112.28,651.34,394.90,10.91;9,112.66,86.97,394.53,10.91;9,112.66,100.52,393.74,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Arx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Levent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Munyikwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nilforoshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ogut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Portelance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Roohani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>TramÃ¨r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m" coord="9,240.49,100.52,233.99,10.91">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,394.53,10.91;9,112.66,127.61,393.32,10.91;9,112.66,141.16,393.33,10.91;9,112.66,154.71,395.01,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,309.51,141.16,103.77,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
	</analytic>
	<monogr>
		<title level="m" coord="9,421.03,141.16,84.96,10.91;9,112.66,154.71,198.24,10.91">Advances in Neural Information Processing Systems 2017-Decem</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.33,10.91;9,112.66,181.81,395.01,10.91;9,112.66,195.36,66.92,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,353.43,168.26,152.55,10.91;9,112.66,181.81,187.25,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<ptr target="N19-1423" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,395.17,10.91;9,112.66,222.46,395.17,10.91;9,112.66,236.01,395.01,10.91;9,112.41,249.56,395.25,10.91;9,112.66,263.11,253.49,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,147.17,222.46,360.66,10.91;9,112.66,236.01,158.30,10.91">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703.doi:10.18653/v1/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m" coord="9,279.81,236.01,182.73,10.91">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,393.33,10.91;9,112.39,290.20,288.85,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<title level="m" coord="9,298.68,276.66,207.31,10.91;9,112.39,290.20,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,393.32,10.91;9,112.66,317.30,395.01,10.91;9,112.66,330.85,138.14,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,482.48,303.75,23.51,10.91;9,112.66,317.30,156.63,10.91">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1202</idno>
		<ptr target="http://arxiv.org/abs/1802.05365.doi:10.18653/v1/n18-1202" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,344.40,394.53,10.91;9,112.66,357.95,388.80,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.03323" />
		<title level="m" coord="9,112.66,357.95,189.66,10.91">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,371.50,393.33,10.91;9,112.66,385.05,393.33,10.91;9,112.33,398.60,394.84,10.91;9,112.66,414.59,38.01,7.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,406.10,371.50,99.89,10.91;9,112.66,385.05,316.05,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="http://arxiv.org/abs/1901.08746.doi:10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="9,439.07,385.05,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,425.70,394.53,10.91;9,112.66,439.25,393.33,10.91;9,112.33,452.79,196.41,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,112.66,439.25,393.33,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.15779" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,466.34,393.33,10.91;9,112.66,479.89,375.43,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,251.91,466.34,254.08,10.91;9,112.66,479.89,73.82,10.91">Biom-transformers: Building large biomedical language models with bert</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.24</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,193.26,479.89,75.84,10.91">albert and electra</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,393.33,10.91;9,112.66,506.99,397.48,10.91;9,112.66,522.98,73.62,7.90" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,283.81,493.44,222.18,10.91;9,112.66,506.99,144.82,10.91">Large-scale application of named entity recognition to biomedicine and epidemiology</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Reji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Bashir</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pdig.0000152</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,266.18,506.99,90.15,10.91">PLOS Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,395.01,10.91;9,112.66,547.64,228.00,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,193.90,534.09,202.00,10.91">Text summarization with pretrained encoders</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1387</idno>
		<ptr target="http://arxiv.org/abs/1908.08345.doi:10.18653/v1/d19-1387" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,561.19,394.53,10.91;9,112.66,574.74,395.01,10.91;9,112.66,588.29,164.49,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<title level="m" coord="9,112.66,574.74,362.09,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.84,393.32,10.91;9,112.66,615.39,382.61,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,329.48,601.84,176.50,10.91;9,112.66,615.39,168.61,10.91">Biobart: Pretraining and evaluation of a biomedical generative language model</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.bionlp-1.9</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,628.93,393.33,10.91;9,112.66,642.48,393.33,10.91;9,112.33,656.03,163.63,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,371.26,628.93,134.73,10.91;9,112.66,642.48,247.99,10.91">Biogpt: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bib/bbac409</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,370.78,642.48,121.82,10.91">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,669.58,393.33,10.91;10,112.66,86.97,394.91,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,402.10,669.58,103.88,10.91;10,112.66,86.97,169.79,10.91">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.244</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,393.61,10.91;10,112.66,114.06,176.64,10.91" xml:id="b17">
	<analytic>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.52,100.52,200.75,10.91;10,112.66,114.06,101.46,10.91">Gpt: Improving language understanding by generative pre-training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,394.93,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,353.88,127.61,152.11,10.91;10,112.66,141.16,78.66,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Alec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rewon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dario</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
		<ptr target="https://github.com/codelucas/newspaper" />
	</analytic>
	<monogr>
		<title level="j" coord="10,199.51,141.16,57.92,10.91">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.32,10.91;10,112.66,181.81,395.01,10.91;10,112.66,195.36,143.58,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,195.51,168.26,131.93,10.91">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2009.191</idno>
		<ptr target="http://ieeexplore.ieee.org/document/5288526/.doi:10.1109/TKDE.2009.191" />
	</analytic>
	<monogr>
		<title level="j" coord="10,335.80,168.26,170.18,10.91;10,112.66,181.81,76.60,10.91">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,320.89,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="10,154.21,208.91,249.38,10.91">Neural transfer learning for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,128.97,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,324.88,222.46,181.11,10.91;10,112.66,236.01,98.46,10.91">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,394.53,10.91;10,112.33,263.11,324.75,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>KoÄiskÃ½</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m" coord="10,112.33,263.11,193.65,10.91">Teaching machines to read and comprehend</title>
		<imprint>
			<date type="published" when="2015-01">2015. January, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,276.66,328.33,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="10,156.71,276.66,252.58,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.33,10.91;10,112.26,303.75,345.32,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<title level="m" coord="10,352.28,290.20,153.71,10.91;10,112.26,303.75,313.40,10.91">Interpretable multi-step reasoning with knowledge extraction on complex healthcare question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,317.30,393.33,10.91;10,112.66,330.85,395.01,10.91;10,112.66,344.40,164.49,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="10,224.52,317.30,281.47,10.91;10,112.66,330.85,364.86,10.91">Classification betters regression in query-based multi-document summarisation techniques for question answering: Macquarie university at bioasq7b</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Molla-Aliod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.00542" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,394.53,10.91;10,112.66,371.50,163.56,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="10,204.52,357.95,298.07,10.91">Alter: Auxiliary text rewriting tool for natural language generation</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-3003</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,385.05,393.33,10.91;10,112.26,398.60,268.69,10.91" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="10,306.82,385.05,199.17,10.91;10,112.26,398.60,137.96,10.91">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-01">2015. January, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,394.53,10.91;10,112.39,425.70,262.89,10.91" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="10,275.67,412.15,226.48,10.91">Brio: Bringing order to abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.207</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,395.01,10.91;10,112.66,452.79,191.55,10.91" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="10,217.71,439.25,215.52,10.91">Can synthetic translations improve bitext quality?</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Briakou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.326</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
